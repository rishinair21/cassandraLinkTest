{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Open-sourcing a 10x reduction in Apache Cassandra tail latency","alternative_id":9381,"content":"<p id=\"1d80\" class=\"graf graf--p graf-after--h3\">At Instagram, we have one of the world’s largest deployments of the Apache Cassandra database. We began using Cassandra in 2012 to replace Redis and support product use cases like fraud detection, Feed, and the Direct inbox. At first we ran Cassandra clusters in an AWS environment, but migrated them over to Facebook’s infrastructure when the rest of Instagram moved. We’ve had a really good experience with the reliability and availability of Cassandra, but saw room for improvement in read latency.<br /> <br />Last year Instagram’s Cassandra team started working on a project to reduce Cassandra’s read latency significantly, which we call Rocksandra. In this post, I will describe the motivation for this project, the challenges we overcame, and performance metrics in both internal and public cloud environments.</p><h3 id=\"d01d\" class=\"graf graf--h3 graf-after--p\">Motivation</h3><p id=\"c5aa\" class=\"graf graf--p graf-after--h3\">At Instagram, we use Apache Cassandra heavily as a general key value storage service. The majority of Instagram’s Cassandra requests are online, so in order to provide a reliable and responsive user experience for hundreds of millions of Instagram users, we have very tight SLA on the metrics. <br /> <br />Instagram maintains a 5–9s reliability SLA, which means at any given time, the request failure rate should be less than 0.001%. For performance, we actively monitor the throughput and latency of different Cassandra clusters, especially the P99 read latency. <br /> <br /> Here’s a graph that shows the client-side latency of one production Cassandra cluster. The blue line is the average read latency (5ms) and the orange line is the P99 read latency (in the range of 25ms to 60ms and changing a lot based on client traffic).</p><figure id=\"73e1\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*Scn1Nm33oukOJpUd4Ukszw.png\" data-width=\"1052\" data-height=\"668\" data-action=\"zoom\" data-action-value=\"1*Scn1Nm33oukOJpUd4Ukszw.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*Scn1Nm33oukOJpUd4Ukszw.png\" alt=\"image\" /></div></figure><figure id=\"0b14\" class=\"graf graf--figure graf-after--figure\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*ItBORNwCXce82ZNX6qf6Vg.png\" data-width=\"1052\" data-height=\"668\" data-action=\"zoom\" data-action-value=\"1*ItBORNwCXce82ZNX6qf6Vg.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*ItBORNwCXce82ZNX6qf6Vg.png\" alt=\"image\" /></div></figure><p id=\"0e33\" class=\"graf graf--p graf-after--figure\">After investigation, we found the JVM garbage collector (GC) contributed a lot to the latency spikes. We defined a metric called GC stall percentage to measure the percentage of time a Cassandra server was doing stop-the-world GC (Young Gen GC) and could not serve client requests. Here’s another graph that shows the GC stall percentage on our production Cassandra servers. It was 1.25% during the lowest traffic time windows, and could be as high as 2.5% during peak hours.</p><p id=\"14d6\" class=\"graf graf--p graf-after--p\">The graph shows that a Cassandra server instance could spend 2.5% of runtime on garbage collections instead of serving client requests. The GC overhead obviously had a big impact on our P99 latency, so if we could lower the GC stall percentage, we would be able to reduce our P99 latency significantly.</p><h3 id=\"87af\" class=\"graf graf--h3 graf-after--p\">Solution</h3><p id=\"5830\" class=\"graf graf--p graf-after--h3\">Apache Cassandra is a distributed database with it’s own LSM tree-based storage engine written in Java. We found that the components in the storage engine, like memtable, compaction, read/write path, etc., created a lot of objects in the Java heap and generated a lot of overhead to JVM. To reduce the GC impact from the storage engine, we considered different approaches and ultimately decided to develop a C++ storage engine to replace existing ones. <br /> <br />We did not want to build a new storage engine from scratch, so we decided to build the new storage engine on top of RocksDB. <br /> <br />RocksDB is an open source, high-performance embedded database for key-value data. It’s written in C++, and provides official API language bindings for C++, C, and Java. RocksDB is optimized for performance, especially on fast storage like SSD. It’s widely used in the industry as the storage engine for MySQL, mongoDB, and other popular databases.</p><h3 id=\"cb1c\" class=\"graf graf--h3 graf-after--p\">Challenges</h3><p id=\"4c10\" class=\"graf graf--p graf-after--h3\">We overcame three main challenges when implementing the new storage engine on RocksDB.<br /> <br />The first challenge was that Cassandra does not have a pluggable storage engine architecture yet, which means the existing storage engine is coupled together with other components in the database. To find a balance between massive refactoring and quick iterations, we defined a new storage engine API, including the most common read/write and streaming interfaces. This way we could implement the new storage engine behind the API and inject it into the related code paths inside Cassandra.<br /> <br />Secondly, Cassandra supports rich data types and table schema, while RocksDB provides purely key-value interfaces. We carefully defined the encoding/decoding algorithms to support Cassandra’s data model within RocksDB’s data structure and supported same-query semantics as original Cassandra. <br /> <br />The third challenge was about streaming. Streaming is an important component for a distributed database like Cassandra. Whenever we join or remove a node from a Cassandra cluster, Cassandra needs to stream data among different nodes to balance the load across the cluster. The existing streaming implementation was based on the details in the current storage engine. Accordingly, we had to decouple them from each other, make an abstraction layer, and re-implement the streaming using RocksDB APIs. For high streaming throughput, we now stream data into temp sst files first, and then use the RocksDB ingest file API to bulk load them into the RocksDB instance at once.</p><h3 id=\"a1f4\" class=\"graf graf--h3 graf-after--p\">Performance metrics</h3><p id=\"7a2f\" class=\"graf graf--p graf-after--h3\">After about a year of development and testing, we have finished a first version of the implementation and successfully rolled it into several production Cassandra clusters in Instagram. In one of our production clusters, the P99 read latency dropped from 60ms to 20ms. We also observed that the GC stalls on that cluster dropped from 2.5% to 0.3%, which was a 10X reduction!<br /> <br />We also wanted to verify whether Rocksandra would perform well in a public cloud environment. We setup a Cassandra cluster in an AWS environment using three i3.8 xlarge EC2 instances, each with 32 cores CPU, 244GB memory, and raid0 with 4 nvme flash disks. <br /> <br />We used <a href=\"https://github.com/Netflix/ndbench\" data-href=\"https://github.com/Netflix/ndbench\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">NDBench</a> for the benchmark, and the default table schema in the framework:</p><blockquote id=\"7425\" class=\"graf graf--blockquote graf-after--p\"><div><code class=\"markup--code markup--blockquote-code\">TABLE emp (</code><br /> <code class=\"markup--code markup--blockquote-code\">emp_uname text PRIMARY KEY,<br />emp_dept text,<br />emp_first text,<br />emp_last text</code><br /> <code class=\"markup--code markup--blockquote-code\">)</code></div></blockquote><p id=\"e302\" class=\"graf graf--p graf-after--blockquote\">We pre-loaded 250M 6KB rows into the database (each server stores about 500GB data on disk). We configured 128 readers and 128 writers in NDBench.<br /> <br />We tested different workloads and measured the avg/P99/P999 read/write latencies. As you can see, Rocksandra provided much lower and consistent tail read/write latency.</p><figure id=\"f950\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*Mpvc-jd61xmcrE4aEth4NA.png\" data-width=\"1132\" data-height=\"725\" data-action=\"zoom\" data-action-value=\"1*Mpvc-jd61xmcrE4aEth4NA.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*Mpvc-jd61xmcrE4aEth4NA.png\" alt=\"image\" /></div></figure><figure id=\"8947\" class=\"graf graf--figure graf-after--figure\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*zZO7xeU8fsWosWbkev873g.png\" data-width=\"1131\" data-height=\"724\" data-action=\"zoom\" data-action-value=\"1*zZO7xeU8fsWosWbkev873g.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*zZO7xeU8fsWosWbkev873g.png\" alt=\"image\" /></div></figure><p id=\"7f48\" class=\"graf graf--p graf-after--figure\">We also tested a read-only workload and observed that, at similar P99 read latency (2ms), Rocksandra could provide 10X higher read throughput (300K/s for Rocksandra vs. 30K/s for C* 3.0).</p><figure id=\"7652\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*E-2efj-mMo0dQWEvZyxn1g.png\" data-width=\"1483\" data-height=\"746\" data-action=\"zoom\" data-action-value=\"1*E-2efj-mMo0dQWEvZyxn1g.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*E-2efj-mMo0dQWEvZyxn1g.png\" alt=\"image\" /></div></figure><figure id=\"b56d\" class=\"graf graf--figure graf-after--figure\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*d5gs5SJzq6laocevBqA1Bg.png\" data-width=\"1359\" data-height=\"731\" data-action=\"zoom\" data-action-value=\"1*d5gs5SJzq6laocevBqA1Bg.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*d5gs5SJzq6laocevBqA1Bg.png\" alt=\"image\" /></div></figure><h3 id=\"d343\" class=\"graf graf--h3 graf-after--figure\">Future work</h3><p id=\"5bac\" class=\"graf graf--p graf-after--h3\">We have open sourced our <a href=\"https://github.com/Instagram/cassandra/tree/rocks_3.0\" data-href=\"https://github.com/Instagram/cassandra/tree/rocks_3.0\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">Rocksandra code base</a> and <a href=\"https://github.com/Instagram/cassandra-aws-benchmark\" data-href=\"https://github.com/Instagram/cassandra-aws-benchmark\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">benchmark framework</a>, which you can download from Github to try out in your own environment! Please let us know how it performs.<br /> <br />As our next step, we are actively working on the development of more C* features support, like secondary indexes, repair, etc. We are also working on a <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-13474\" data-href=\"https://issues.apache.org/jira/browse/CASSANDRA-13474\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">C* pluggable storage engine architecture</a> to contribute our work back to the Apache Cassandra community. <br /> <br />If you are in the Bay Area and are interested in learning more about our Cassandra developments, join us at our next meetup event <a href=\"https://www.meetup.com/Apache-Cassandra-Bay-Area/events/248376266/\" data-href=\"https://www.meetup.com/Apache-Cassandra-Bay-Area/events/248376266/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">here</a>.</p><p id=\"3171\" class=\"graf graf--p graf-after--p graf--trailing\"><em class=\"markup--em markup--p-em\">Dikang Gu is an infrastructure engineer at Instagram.</em></p>"}}]}},"pageContext":{"alternative_id":9381}}