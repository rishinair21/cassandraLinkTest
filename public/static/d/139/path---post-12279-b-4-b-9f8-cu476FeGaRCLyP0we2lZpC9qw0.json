{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Global Cloud — Active-Active and Beyond","alternative_id":12279,"content":"<section class=\"section section--body section--first\"><div class=\"section-divider\"><hr class=\"section-divider\" /></div><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><h1 id=\"ee40\" class=\"graf graf--h3 graf--leading graf--title\">Global Cloud — Active-Active and Beyond</h1><p id=\"3fbc\" class=\"graf graf--p graf-after--h3\">This is a continuing post on the Netflix architecture for Global Availability. In the past we talked about efforts like <a href=\"https://medium.com/@Netflix_Techblog/isthmus-resiliency-against-elb-outages-d9e0623484f3\" data-href=\"https://medium.com/@Netflix_Techblog/isthmus-resiliency-against-elb-outages-d9e0623484f3\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">Isthmus</a> and <a href=\"https://medium.com/@Netflix_Techblog/active-active-for-multi-regional-resiliency-c47719f6685b\" data-href=\"https://medium.com/@Netflix_Techblog/active-active-for-multi-regional-resiliency-c47719f6685b\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">Active-Active</a>:</p><div id=\"044d\" class=\"graf graf--mixtapeEmbed graf-after--p\"><a href=\"https://medium.com/@Netflix_Techblog/isthmus-resiliency-against-elb-outages-d9e0623484f3\" data-href=\"https://medium.com/@Netflix_Techblog/isthmus-resiliency-against-elb-outages-d9e0623484f3\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"https://medium.com/@Netflix_Techblog/isthmus-resiliency-against-elb-outages-d9e0623484f3\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">Isthmus — Resiliency against ELB outages</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">achieving multi-regional ELB resiliency</em>medium.com</a></div><div id=\"019e\" class=\"graf graf--mixtapeEmbed graf-after--mixtapeEmbed\"><a href=\"https://medium.com/@Netflix_Techblog/active-active-for-multi-regional-resiliency-c47719f6685b\" data-href=\"https://medium.com/@Netflix_Techblog/active-active-for-multi-regional-resiliency-c47719f6685b\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"https://medium.com/@Netflix_Techblog/active-active-for-multi-regional-resiliency-c47719f6685b\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">Active-Active for Multi-Regional Resiliency</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">With large scale and velocity there is increased chance of failure.</em>medium.com</a></div><p id=\"f34a\" class=\"graf graf--p graf-after--mixtapeEmbed\">We continue the story from where we left off at the end of the Active-Active project in 2013. We had achieved multi-regional resiliency for our members in the Americas, where the vast majority of Netflix members were located at the time. Our European members, however, were still at risk from a single point of failure.</p><figure id=\"d9b7\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*AJFbTcWbGFHqSJvV.\" data-width=\"499\" data-height=\"224\" src=\"https://cdn-images-1.medium.com/max/1600/0*AJFbTcWbGFHqSJvV.\" alt=\"image\" /></div></div></figure><p id=\"212c\" class=\"graf graf--p graf-after--figure\">Our expansion around the world since then, has resulted in a growing percentage of international members who were exposed to this single point of failure, so we set out to make our cloud deployment even more resilient.</p><h3 id=\"47dd\" class=\"graf graf--h3 graf-after--p\">Creating a Global Cloud</h3><p id=\"a10e\" class=\"graf graf--p graf-after--h3\">We decided to create a global cloud where we would be able to serve requests from any member in any AWS region where we are deployed. The diagram below shows the logical structure of our multi-region deployment and the default routing of member traffic to AWS region.</p><figure id=\"0193\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*gkGFwvOCDtZu3cp2c3wjvg.png\" data-width=\"389\" data-height=\"330\" src=\"https://cdn-images-1.medium.com/max/1600/1*gkGFwvOCDtZu3cp2c3wjvg.png\" alt=\"image\" /></div></div></figure><h3 id=\"0d85\" class=\"graf graf--h3 graf-after--figure\">Getting There</h3><p id=\"366a\" class=\"graf graf--p graf-after--h3\">Getting to the end state, while not disrupting our ongoing operations and the development of new features, required breaking the project down into a number of stages. From an availability perspective, removing AWS EU-West-1 as a single point of failure was the most important goal, so we started in the Summer of 2014 by identifying the tasks that we needed to execute in order to be able to serve our European members from US-East-1.</p><h4 id=\"6fa1\" class=\"graf graf--h4 graf-after--p\">Data Replication</h4><p id=\"b8d1\" class=\"graf graf--p graf-after--h4\">When we initially launched service in Europe in 2012, we made an explicit decision to build regional data islands for most, but not all, of the member related data. In particular, while a member’s subscription allowed them to stream anywhere that we offered service, information about what they watched while in Europe would not be merged with the information about what they watched while in the Americas. Since we figured we would have relatively few members travelling across the Atlantic, we felt that the isolation that these data islands created was a win as it would mitigate the impact of a region specific outage.</p><p id=\"5bb4\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">Cassandra</strong></p><p id=\"410b\" class=\"graf graf--p graf-after--p\">In order to serve our EU members a normal experience from US-East-1, we needed to replicate the data in the EU Cassandra island data sets to the Cassandra clusters in US-East-1 and US-West-2. We considered replicating this data into separate keyspaces in US clusters or merging the data with our Americas data. While using separate keyspaces would have been more cost efficient, merging the datasets was more in line with our longer term goal of being able to serve any member from any region as the Americas data would be replicated to the Cassandra clusters in EU-West-1.</p><p id=\"46a0\" class=\"graf graf--p graf-after--p\">Merging the EU and Americas data was more complicated than the replication work that was part of the 2013 Active-Active project as we needed to examine each component data set to understand how to merge the data. Some data sets were appropriately keyed such that the result was the union of the two island data sets. To simplify the migration of such data sets, the Netflix Cloud Database Engineering (CDE) team enhanced the Astyanax Cassandra client to support writing to two keyspaces in parallel. This dual write functionality was sometimes used in combination with another tool built by the CDE that could be used to forklift data from one cluster or keyspace to another. For other data sets, such as member viewing history, custom tools were needed to handle combining the data associated with each key. We also discovered one or two data sets in which there were unexpected inconsistencies in the data that required deeper analysis to determine which particular values to keep.</p><p id=\"f1cb\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">EVCache</strong></p><p id=\"00a8\" class=\"graf graf--p graf-after--p\">As described in the blog post on the Active-Active project, we built a mechanism to allow updates to EVCache clusters in one region to invalidate the entry in the corresponding cluster in the other US region using an SQS message. EVCache now supports both full replication and invalidation of data in other regions, which allows application teams to select the strategy that is most appropriate to their particular data set. Additional details about the current EVCache architecture are available in <a href=\"https://medium.com/@Netflix_Techblog/caching-for-a-global-netflix-7bcc457012f1\" data-href=\"https://medium.com/@Netflix_Techblog/caching-for-a-global-netflix-7bcc457012f1\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">a recent Tech Blog post</a>:</p><div id=\"04f0\" class=\"graf graf--mixtapeEmbed graf-after--p\"><a href=\"https://medium.com/@Netflix_Techblog/caching-for-a-global-netflix-7bcc457012f1\" data-href=\"https://medium.com/@Netflix_Techblog/caching-for-a-global-netflix-7bcc457012f1\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"https://medium.com/@Netflix_Techblog/caching-for-a-global-netflix-7bcc457012f1\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">Caching for a Global Netflix</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">#CachesEverywhere</em>medium.com</a></div><h4 id=\"342e\" class=\"graf graf--h4 graf-after--mixtapeEmbed\">Personalization Data</h4><p id=\"8bfc\" class=\"graf graf--p graf-after--h4\">Historically the personalization data for any given member has been pre-computed in only one of our AWS regions and then replicated to whatever other regions might service requests for that member. When a member interacted with the Netflix service in a way that was supposed to trigger an update of the recommendations, this would only happen if the interaction was serviced in the member’s “home” region, or its active-active replica, if any.</p><p id=\"b7e2\" class=\"graf graf--p graf-after--p\">This meant that when a member was serviced from a different region during a traffic migration, their personalized information would not be updated. Since there are regular, clock driven, updates to the precomputed data sets, this was considered acceptable for the first phase of the Global Cloud project. In the longer term, however, the precomputation system was enhanced to allow the events that triggered recomputation to be delivered across all three regions. This change also allowed us to redistribute the precomputation workload based on resource availability.</p><h4 id=\"5b40\" class=\"graf graf--h4 graf-after--p\">Handling Misrouted Traffic</h4><p id=\"b1eb\" class=\"graf graf--p graf-after--h4\">In the past, Netflix has used a variety of application level mechanisms to redirect device traffic that has landed in the “wrong” AWS region, due to DNS anomalies, back to the member’s “home” region. While these mechanisms generally worked, they were often a source of confusion due the differences in their implementations. As we started moving towards the Global Cloud, we decided that, rather than redirecting the misrouted traffic, we would use the same Zuul-to-Zuul routing mechanism that we use when failing over traffic to another region to transparently proxy traffic from the “wrong” region to the “home” region.</p><p id=\"9fcf\" class=\"graf graf--p graf-after--p\">As each region became capable of serving all members, we could then update the Zuul configuration to stop proxying the “misrouted” traffic to the member’s home region and simply serve it locally. While this potentially added some latency versus sticky redirects, it allowed several teams to simplify their applications by removing the often crufty redirect code. Application teams were given the guidance that they should no longer worry about whether a member was in the “correct” region and instead serve them the best response that they could give the locally available information.</p><h4 id=\"d6a2\" class=\"graf graf--h4 graf-after--p\">Evolving Chaos Kong</h4><p id=\"b9d9\" class=\"graf graf--p graf-after--h4\">With the Active-Active deployment model, our Chaos Kong exercises involved failing over a single region into another region. This is also the way we did our first few Global Cloud failovers. The following graph shows our traffic steering during a production issue in US-East-1. We steered traffic first from US-East-1 to US-West-2 and then later in the day to EU-West-1. The upper graph shows that the aggregate, global, stream starts tracked closely to the previous week’s pattern, despite the shifts in the amount of traffic being served by each region. The thin light blue line shows SPS traffic for each region the previous week and allows you to see the amount of traffic we are shifting.</p></div><div class=\"section-inner sectionLayout--outsetColumn\"><figure id=\"fc70\" class=\"graf graf--figure graf--layoutOutsetCenter graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*nRlYKs7dRoWcvYE0.\" data-width=\"1200\" data-height=\"288\" data-action=\"zoom\" data-action-value=\"0*nRlYKs7dRoWcvYE0.\" src=\"https://cdn-images-1.medium.com/max/2000/0*nRlYKs7dRoWcvYE0.\" alt=\"image\" /></div></div></figure></div><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"5b7c\" class=\"graf graf--p graf-after--figure\">By enhancing our traffic steering tools, we are now able to steer traffic from one region to both remaining regions to make use of available capacity. The graphs below show a situation where we evacuated all traffic from US-East-1, sending most of the traffic to EU-West-1 and a smaller portion to US-West-2.</p></div><div class=\"section-inner sectionLayout--outsetColumn\"><figure id=\"31e0\" class=\"graf graf--figure graf--layoutOutsetCenter graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*DOqQH3D_Xq-6o2jl.\" data-width=\"1200\" data-height=\"273\" data-is-featured=\"true\" data-action=\"zoom\" data-action-value=\"0*DOqQH3D_Xq-6o2jl.\" src=\"https://cdn-images-1.medium.com/max/2000/0*DOqQH3D_Xq-6o2jl.\" alt=\"image\" /></div></div></figure></div><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"d08a\" class=\"graf graf--p graf-after--figure\">We have done similar evacuations for the other two regions, each of them involving rerouted traffic being split between both remaining regions based on available capacity and minimizing member impact. For more details on the evolution of the Kong exercises and our Chaos philosophy behind them, <a href=\"https://medium.com/@Netflix_Techblog/chaos-engineering-upgraded-878d341f15fa\" data-href=\"https://medium.com/@Netflix_Techblog/chaos-engineering-upgraded-878d341f15fa\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">see our earlier post</a>:</p><div id=\"5e64\" class=\"graf graf--mixtapeEmbed graf-after--p\"><a href=\"https://medium.com/@Netflix_Techblog/chaos-engineering-upgraded-878d341f15fa\" data-href=\"https://medium.com/@Netflix_Techblog/chaos-engineering-upgraded-878d341f15fa\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"https://medium.com/@Netflix_Techblog/chaos-engineering-upgraded-878d341f15fa\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">Chaos Engineering Upgraded</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">Chaos Kong is the most destructive Chaos Monkey yet</em>medium.com</a></div><h3 id=\"bb44\" class=\"graf graf--h3 graf-after--mixtapeEmbed\">Are We Done?</h3><p id=\"e13e\" class=\"graf graf--p graf-after--h3\">Not even close. We will continue to explore new ways in which to efficiently and reliably deliver service to our millions of global members. We will report on those experiments in future updates here.</p><p id=\"bc23\" class=\"graf graf--p graf-after--p graf--trailing\"><em class=\"markup--em markup--p-em\">— Peter Stout on behalf of all the teams that contributed to the Global Cloud Project</em></p></div></div></section><section class=\"section section--body section--last\"><div class=\"section-divider\"><hr class=\"section-divider\" /></div><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"9cee\" class=\"graf graf--p graf--leading graf--trailing\"><em class=\"markup--em markup--p-em\">Originally published at </em><a href=\"http://techblog.netflix.com/2016/03/global-cloud-active-active-and-beyond.html\" data-href=\"http://techblog.netflix.com/2016/03/global-cloud-active-active-and-beyond.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\"><em class=\"markup--em markup--p-em\">techblog.netflix.com</em></a><em class=\"markup--em markup--p-em\"> on March 30, 2016.</em></p></div></div></section>"}}]}},"pageContext":{"alternative_id":12279}}