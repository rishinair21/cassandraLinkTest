{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"How Discord Stores Billions of Messages","alternative_id":12316,"content":"<div class=\"section-inner sectionLayout--fullWidth\"><figure id=\"7660\" class=\"graf graf--figure graf--layoutFillWidth graf-after--h3\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*8eA5bgAG9NmCJshhccMhZQ.png\" data-width=\"2500\" data-height=\"900\" src=\"https://cdn-images-1.medium.com/max/2000/1*8eA5bgAG9NmCJshhccMhZQ.png\" alt=\"image\" /></div></figure></div><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"d1b5\" class=\"graf graf--p graf-after--figure\">Discord continues to grow faster than we expected and so does our user-generated content. With more users comes more chat messages. In July, <a href=\"https://blog.discordapp.com/11-million-players-in-one-year/\" data-href=\"https://blog.discordapp.com/11-million-players-in-one-year/\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">we announced 40 million messages a day</a>, in December <a href=\"http://venturebeat.com/2016/12/08/discord-hits-25-million-users-and-releases-gamebridge-sdk-for-its-voice-chat/\" data-href=\"http://venturebeat.com/2016/12/08/discord-hits-25-million-users-and-releases-gamebridge-sdk-for-its-voice-chat/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">we announced 100 million</a>, and as of this blog post we are well past 120 million. We decided early on to store all chat history forever so users can come back at any time and have their data available on any device. This is a lot of data that is ever increasing in velocity, size, and must remain available. <em class=\"markup--em markup--p-em\">How do we do it? Cassandra!</em></p><h3 id=\"8d4c\" class=\"graf graf--h3 graf-after--p\">What we were doing</h3><p id=\"37d6\" class=\"graf graf--p graf-after--h3\">The original version of Discord was built in just under two months in early 2015. Arguably, one of the best databases for iterating quickly is MongoDB. Everything on Discord was stored in a single MongoDB replica set and this was intentional, but we also planned everything for easy migration to a new database (we knew we were not going to use MongoDB sharding because it is complicated to use and not known for stability). This is actually part of our company culture: build quickly to prove out a product feature, but always with a path to a more robust solution.</p><p id=\"05e8\" class=\"graf graf--p graf-after--p\">The messages were stored in a MongoDB collection with a single compound index on <code class=\"markup--code markup--p-code\">channel_id</code> and <code class=\"markup--code markup--p-code\">created_at</code>. Around November 2015, we reached 100 million stored messages and at this time we started to see the expected issues appearing: the data and the index could no longer fit in RAM and latencies started to become unpredictable. It was time to migrate to a database more suited to the task.</p><h3 id=\"d9b8\" class=\"graf graf--h3 graf-after--p\">Choosing the Right Database</h3><p id=\"6a39\" class=\"graf graf--p graf-after--h3\">Before choosing a new database, we had to understand our read/write patterns and why we were having problems with our current solution.</p><ul class=\"postList\"><li id=\"e2a4\" class=\"graf graf--li graf-after--p\">It quickly became clear that our reads were extremely random and our read/write ratio was about 50/50.</li><li id=\"4e27\" class=\"graf graf--li graf-after--li\">Voice chat heavy Discord servers send almost no messages. This means they send a message or two every few days. In a year, this kind of server is unlikely to reach 1,000 messages. The problem is that even though this is a small amount of messages it makes it harder to serve this data to the users. Just returning 50 messages to a user can result in many random seeks on disk causing disk cache evictions.</li><li id=\"0c8c\" class=\"graf graf--li graf-after--li\">Private text chat heavy Discord servers send a decent number of messages, easily reaching between 100 thousand to 1 million messages a year. The data they are requesting is usually very recent only. The problem is since these servers usually have under 100 members the rate at which this data is requested is low and unlikely to be in disk cache.</li><li id=\"415e\" class=\"graf graf--li graf-after--li\">Large public Discord servers send a lot of messages. They have thousands of members sending thousands of messages a day and easily rack up millions of messages a year. They almost always are requesting messages sent in the last hour and they are requesting them often. Because of that the data is usually in the disk cache.</li><li id=\"e87c\" class=\"graf graf--li graf-after--li\">We knew that in the coming year we would add even more ways for users to issue random reads: the ability to view your mentions for the last 30 days then jump to that point in history, viewing plus jumping to pinned messages, and full-text search. <em class=\"markup--em markup--li-em\">All of this spells more random reads!!</em></li></ul><p id=\"2d85\" class=\"graf graf--p graf-after--li\">Next we defined our requirements:</p><ul class=\"postList\"><li id=\"c0e2\" class=\"graf graf--li graf-after--p\"><strong class=\"markup--strong markup--li-strong\">Linear scalability — </strong>We do not want to reconsider the solution later or manually re-shard the data.</li><li id=\"9a9a\" class=\"graf graf--li graf-after--li\"><strong class=\"markup--strong markup--li-strong\">Automatic failover — </strong>We love sleeping at night and build Discord to self heal as much as possible.</li><li id=\"a6ca\" class=\"graf graf--li graf-after--li\"><strong class=\"markup--strong markup--li-strong\">Low maintenance — </strong>It should just work once we set it up. We should only have to add more nodes as data grows.</li><li id=\"b5ea\" class=\"graf graf--li graf-after--li\"><strong class=\"markup--strong markup--li-strong\">Proven to work — </strong>We love trying out new technology, but not too new.</li><li id=\"8a1c\" class=\"graf graf--li graf-after--li\"><strong class=\"markup--strong markup--li-strong\">Predictable performance</strong> <strong class=\"markup--strong markup--li-strong\">— </strong>We have alerts go off when our API’s response time 95th percentile goes above 80ms. We also do not want to have to cache messages in Redis or Memcached.</li><li id=\"10c1\" class=\"graf graf--li graf-after--li\"><strong class=\"markup--strong markup--li-strong\">Not a blob store — </strong>Writing thousands of messages per second would not work great if we had to constantly deserialize blobs and append to them.</li><li id=\"cfd9\" class=\"graf graf--li graf-after--li\"><strong class=\"markup--strong markup--li-strong\">Open source — </strong>We believe in controlling our own destiny and don’t want to depend on a third party company.</li></ul><p id=\"87e5\" class=\"graf graf--p graf-after--li\">Cassandra was the only database that fulfilled all of our requirements. We can just add nodes to scale it and it can tolerate a loss of nodes without any impact on the application. Large companies such as Netflix and Apple have thousands of Cassandra nodes. Related data is stored contiguously on disk providing minimum seeks and easy distribution around the cluster. It’s backed by DataStax, but still open source and community driven.</p><p id=\"024a\" class=\"graf graf--p graf-after--p\">Having made the choice, we needed to prove that it would actually work.</p><h3 id=\"7d98\" class=\"graf graf--h3 graf-after--p\">Data Modeling</h3><p id=\"aea7\" class=\"graf graf--p graf-after--h3\">The best way to describe Cassandra to a newcomer is that it is a KKV store. The two Ks comprise the primary key. The first K is the partition key and is used to determine which node the data lives on and where it is found on disk. The partition contains multiple rows within it and a row within a partition is identified by the second K, which is the clustering key. The clustering key acts as both a primary key within the partition and how the rows are sorted. You can think of a partition as an ordered dictionary. These properties combined allow for very powerful data modeling.</p><p id=\"1f84\" class=\"graf graf--p graf-after--p\">Remember that messages were indexed in MongoDB using <code class=\"markup--code markup--p-code\">channel_id</code> and <code class=\"markup--code markup--p-code\">created_at</code>? <code class=\"markup--code markup--p-code\">channel_id</code> became the partition key since all queries operate on a channel, but <code class=\"markup--code markup--p-code\">created_at</code> didn’t make a great clustering key because two messages can have the same creation time. Luckily every ID on Discord is actually a <a href=\"https://blog.twitter.com/2010/announcing-snowflake\" data-href=\"https://blog.twitter.com/2010/announcing-snowflake\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">Snowflake</a> (chronologically sortable), so we were able to use them instead. The primary key became <code class=\"markup--code markup--p-code\">(channel_id, message_id)</code>, where the <code class=\"markup--code markup--p-code\">message_id</code> is a Snowflake. This meant that when loading a channel we could tell Cassandra exactly where to range scan for messages.</p><p id=\"c72a\" class=\"graf graf--p graf-after--p\">Here is a simplified schema for our messages table (this omits about 10 columns).</p><figure id=\"a957\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"93f6\" class=\"graf graf--p graf-after--figure\">While Cassandra has schemas not unlike a relational database, they are cheap to alter and do not impose any temporary performance impact. We get the best of a blob store and a relational store.</p><p id=\"8f6b\" class=\"graf graf--p graf-after--p\">When we started importing existing messages into Cassandra we immediately began to see warnings in the logs telling us that partitions were found over 100MB in size. <em class=\"markup--em markup--p-em\">What gives?!</em> <em class=\"markup--em markup--p-em\">Cassandra advertises that it can support 2GB partitions!</em> Apparently, just because it can be done, it doesn’t mean it should. Large partitions put a lot of GC pressure on Cassandra during compaction, cluster expansion, and more. Having a large partition also means the data in it cannot be distributed around the cluster. It became clear we had to somehow bound the size of partitions because a single Discord channel can exist for years and perpetually grow in size.</p><p id=\"c176\" class=\"graf graf--p graf-after--p\">We decided to bucket our messages by time. We looked at the largest channels on Discord and determined if we stored about 10 days of messages within a bucket that we could comfortably stay under 100MB. Buckets had to be derivable from the <code class=\"markup--code markup--p-code\">message_id</code> or a timestamp.</p><figure id=\"c830\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"0952\" class=\"graf graf--p graf-after--figure\">Cassandra partition keys can be compounded, so our new primary key became <code class=\"markup--code markup--p-code\">((channel_id, bucket), message_id)</code>.</p><figure id=\"defd\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"1d29\" class=\"graf graf--p graf-after--figure\">To query for recent messages in the channel we generate a bucket range from current time to <code class=\"markup--code markup--p-code\">channel_id</code> (it is also a Snowflake and has to be older than the first message). We then sequentially query partitions until enough messages are collected. The downside of this method is that rarely active Discords will have to query multiple buckets to collect enough messages over time. In practice this has proved to be fine because for active Discords enough messages are usually found in the first partition and they are the majority.</p><p id=\"383e\" class=\"graf graf--p graf-after--p\">Importing messages into Cassandra went without a hitch and we were ready to try in production.</p><h3 id=\"7a52\" class=\"graf graf--h3 graf-after--p\">Dark Launch</h3><p id=\"724b\" class=\"graf graf--p graf-after--h3\">Introducing a new system into production is always scary so it’s a good idea to try to test it without impacting users. We setup our code to double read/write to MongoDB and Cassandra.</p><p id=\"f0e5\" class=\"graf graf--p graf-after--p\">Immediately after launching we started getting errors in our bug tracker telling us that <code class=\"markup--code markup--p-code\">author_id</code> was null. <em class=\"markup--em markup--p-em\">How can it be null? It is a required field!</em></p><h3 id=\"08bd\" class=\"graf graf--h3 graf-after--p\">Eventual Consistency</h3><p id=\"7964\" class=\"graf graf--p graf-after--h3\">Cassandra is an <a href=\"https://en.wikipedia.org/wiki/CAP_theorem\" data-href=\"https://en.wikipedia.org/wiki/CAP_theorem\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">AP</a> database which means it trades strong consistency for availability which is something we wanted. It is an anti-pattern to read-before-write (reads are more expensive) in Cassandra and therefore everything that Cassandra does is essentially an upsert even if you provide only certain columns. You can also write to any node and it will resolve conflicts automatically using “last write wins” semantics on a per column basis. <em class=\"markup--em markup--p-em\">So how did this bite us?</em></p></figure></figure></figure></div><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"13d2\" class=\"graf graf--p graf-after--figure\">In the scenario that a user edits a message at the same time as another user deletes the same message, we ended up with a row that was missing all the data except the primary key and the text since all Cassandra writes are upserts. There were two possible solutions for handling this problem:</p><ol class=\"postList\"><li id=\"930e\" class=\"graf graf--li graf-after--p\">Write the whole message back when editing the message. This had the possibility of resurrecting messages that were deleted and adding more chances for conflict for concurrent writes to other columns.</li><li id=\"67a9\" class=\"graf graf--li graf-after--li\">Figuring out that the message is corrupt and deleting it from database.</li></ol><p id=\"8441\" class=\"graf graf--p graf-after--li\">We went with the second option, which we did by choosing a column that was required (in this case <code class=\"markup--code markup--p-code\">author_id</code>) and deleting the message if it was null.</p><p id=\"7b03\" class=\"graf graf--p graf-after--p\">While solving this problem, we noticed we were being very inefficient with our writes. Since Cassandra is eventually consistent it cannot just delete data immediately. It has to replicate deletes to other nodes and do it even if other nodes are temporarily unavailable. Cassandra does this by treating deletes as a form of write called a “tombstone.” On read, it just skips over tombstones it comes across. Tombstones live for a configurable amount of time (10 days by default) and are permanently deleted during compaction when that time expires.</p><p id=\"8fca\" class=\"graf graf--p graf-after--p\">Deleting a column and writing null to a column are the exact same thing. They both generate a tombstone. Since all writes in Cassandra are upserts, that means you are generating a tombstone even when writing null for the first time. In practice, our entire message schema contains 16 columns, but the average message only has 4 values set. We were writing 12 tombstones into Cassandra most of the time for no reason. The solution to this was simple: only write non-null values to Cassandra.</p><h3 id=\"dbd1\" class=\"graf graf--h3 graf-after--p\">Performance</h3><p id=\"50d0\" class=\"graf graf--p graf-after--h3\">Cassandra is known to have faster writes than reads and we observed exactly that. Writes were sub-millisecond and reads were under 5 milliseconds. We observed this regardless of what data was being accessed, and performance stayed consistent during a week of testing. <em class=\"markup--em markup--p-em\">Nothing was surprising, we got exactly what we expected.</em></p><figure id=\"be55\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*MrdDaSA6ghOQQ7WyzqztcQ.png\" data-width=\"1634\" data-height=\"848\" data-action=\"zoom\" data-action-value=\"1*MrdDaSA6ghOQQ7WyzqztcQ.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*MrdDaSA6ghOQQ7WyzqztcQ.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\">Read/Write Latency via Datadog</figcaption></figure><p id=\"34d2\" class=\"graf graf--p graf-after--figure\">In line with fast, consistent read performance, here’s an example of a jump to a message from over a year ago in a channel with millions of messages:</p></div><div class=\"section-inner sectionLayout--insetColumn\"><h3 id=\"aa17\" class=\"graf graf--h3 graf-after--figure\">The Big Surprise</h3><p id=\"fd5c\" class=\"graf graf--p graf-after--h3\">Everything went smoothly, so we rolled it out as our primary database and phased out MongoDB within a week . It continued to work flawlessly…for about 6 months until that one day where Cassandra became unresponsive.</p><p id=\"7083\" class=\"graf graf--p graf-after--p\">We noticed Cassandra was running 10 second <em class=\"markup--em markup--p-em\">“stop-the-world”</em> GC constantly but we had no idea why. We started digging and found a Discord channel that was taking 20 seconds to load. The <a href=\"https://www.reddit.com/r/PuzzleAndDragons/\" data-href=\"https://www.reddit.com/r/PuzzleAndDragons/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">Puzzles &amp; Dragons Subreddit</a> public Discord server was the culprit. Since it was public we joined it to take a look. To our surprise, the channel had only 1 message in it. It was at that moment that it became obvious they deleted millions of messages using our API, leaving only 1 message in the channel.</p><p id=\"b84a\" class=\"graf graf--p graf-after--p\">If you have been paying attention you might remember how Cassandra handles deletes using tombstones (mentioned in <strong class=\"markup--strong markup--p-strong\">Eventual Consistency</strong>). When a user loaded this channel, even though there was only 1 message, Cassandra had to effectively scan millions of message tombstones (generating garbage faster than the JVM could collect it).</p><p id=\"5acc\" class=\"graf graf--p graf-after--p\">We solved this by doing the following:</p><ul class=\"postList\"><li id=\"4366\" class=\"graf graf--li graf-after--p\">We lowered the lifespan of tombstones from 10 days down to 2 days because we run <a href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsRepair.html#toolsRepair__description\" data-href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsRepair.html#toolsRepair__description\" class=\"markup--anchor markup--li-anchor\" rel=\"noopener\" target=\"_blank\">Cassandra repairs</a> (an anti-entropy process) every night on our message cluster.</li><li id=\"e1cd\" class=\"graf graf--li graf-after--li\">We changed our query code to track empty buckets and avoid them in the future for a channel. This meant that if a user caused this query again then at worst Cassandra would be scanning only in the most recent bucket.</li></ul><h3 id=\"805e\" class=\"graf graf--h3 graf-after--li\">The Future</h3><p id=\"dd65\" class=\"graf graf--p graf-after--h3\">We are currently running a 12 node cluster with a replica factor of 3 and will just continue to add new Cassandra nodes as needed. We believe this will continue to work for a long time but as Discord continues to grow there is a distant future where we are storing billions of messages per day. Netflix and Apple run clusters of hundreds of nodes so we know we can punt thinking too much about this for a while. However we like to have some ideas in our pocket for the future.</p><h4 id=\"6c38\" class=\"graf graf--h4 graf-after--p\">Near term</h4><ul class=\"postList\"><li id=\"2de1\" class=\"graf graf--li graf-after--h4\">Upgrade our message cluster from Cassandra 2 to Cassandra 3. Cassandra 3 has a <a href=\"http://www.datastax.com/2015/12/storage-engine-30\" data-href=\"http://www.datastax.com/2015/12/storage-engine-30\" class=\"markup--anchor markup--li-anchor\" rel=\"noopener\" target=\"_blank\">new storage format</a> that can reduce storage size by more than 50%.</li><li id=\"6986\" class=\"graf graf--li graf-after--li\">Newer versions of Cassandra are better at handling more data on a single node. We currently store nearly 1TB of compressed data on each node. We believe we can safely reduce the number of nodes in the cluster by bumping this to 2TB.</li></ul><h4 id=\"e79b\" class=\"graf graf--h4 graf-after--li\">Long term</h4><ul class=\"postList\"><li id=\"f493\" class=\"graf graf--li graf-after--h4\">Explore using <a href=\"http://www.scylladb.com/\" data-href=\"http://www.scylladb.com/\" class=\"markup--anchor markup--li-anchor\" rel=\"noopener\" target=\"_blank\">Scylla</a>, a Cassandra compatible database written in C++. During normal operations our Cassandra nodes are actually not using too much CPU, however at non peak hours when we run repairs (an anti-entropy process) they become fairly CPU bound and the duration increases with the amount of data written since the last repair. Scylla advertises significantly lower repair times.</li><li id=\"3008\" class=\"graf graf--li graf-after--li\">Build a system to archive unused channels to flat files on Google Cloud Storage and load them back on-demand. We want to avoid doing this one and don’t think we will have to do it.</li></ul><h3 id=\"05c6\" class=\"graf graf--h3 graf-after--li\">Conclusion</h3><p id=\"2468\" class=\"graf graf--p graf-after--h3\">It has now been just over a year since we made the switch and, despite <em class=\"markup--em markup--p-em\">“the big surprise,”</em> it has been smooth sailing. We went from over 100 million total messages to more than 120 million messages a day, with performance and stability staying consistent.</p><p id=\"3bc8\" class=\"graf graf--p graf-after--p\">Due to the success of this project we have since moved the rest of our live production data to Cassandra and that has also been a success.</p><p id=\"3f10\" class=\"graf graf--p graf-after--p\">In a follow-up to this post we will explore how we make billions of messages searchable.</p><p id=\"34ed\" class=\"graf graf--p graf-after--p graf--trailing\">We don’t have dedicated DevOps engineers yet (only 4 backend engineers), so having a system we don’t have to worry about has been great. <em class=\"markup--em markup--p-em\">We are hiring, so </em><a href=\"https://discordapp.com/jobs\" data-href=\"https://discordapp.com/jobs\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\"><em class=\"markup--em markup--p-em\">come join us</em></a><em class=\"markup--em markup--p-em\"> if this type of stuff tickles your fancy.</em></p></div>"}}]}},"pageContext":{"alternative_id":12316}}