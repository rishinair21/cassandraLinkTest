{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Apache Cassandra — The minimum internals you need to know","alternative_id":12991,"content":"<p id=\"e760\" class=\"graf graf--p graf-after--h4\">We will discuss two parts here; first, the database design internals that may help you compare between database’s, and second the main intuition behind auto-sharding/auto-scaling in Cassandra, and how to model your data to be aligned to that model for the best performance.</p><h3 id=\"3631\" class=\"graf graf--h3 graf-after--p\">Part 1: Database Architecture — Master-Slave and Masterless and its impact on HA and Scalability</h3><p id=\"2263\" class=\"graf graf--p graf-after--h3\">There are two broad types of HA Architectures <strong class=\"markup--strong markup--p-strong\">Master -slave </strong>and <strong class=\"markup--strong markup--p-strong\">Masterless</strong> or master-master architecture.</p><p id=\"d342\" class=\"graf graf--p graf-after--p\">Here is an interesting Stack Overflow QA that sums up quite easily one main trade-off with these two type of architectures.</p><blockquote id=\"14da\" class=\"graf graf--blockquote graf-after--p\"><div><strong class=\"markup--strong markup--blockquote-strong\">Q</strong>. -I’ve heard about <strong class=\"markup--strong markup--blockquote-strong\">two kind of database architectures.</strong></div></blockquote><blockquote id=\"3939\" class=\"graf graf--blockquote graf-after--blockquote\"><div><strong class=\"markup--strong markup--blockquote-strong\">master-master and master-slave</strong></div></blockquote><blockquote id=\"5193\" class=\"graf graf--blockquote graf-after--blockquote\"><div>Isn’t the master-master more suitable for today’s web cause it’s like Git, every unit has the whole set of data and if one goes down, it doesn’t quite matter.</div></blockquote><blockquote id=\"dac0\" class=\"graf graf--blockquote graf-after--blockquote\"><div><strong class=\"markup--strong markup--blockquote-strong\">A</strong>.- There’s a fundamental tension:</div></blockquote><blockquote id=\"5c66\" class=\"graf graf--blockquote graf-after--blockquote\"><div><strong class=\"markup--strong markup--blockquote-strong\">One copy: consistency is easy,</strong> but if it happens to be down everybody is out of the water, and if people are remote then may pay horrid communication costs. Bring portable devices, which may need to operate disconnected, into the picture and one copy won’t cut it.</div></blockquote><blockquote id=\"8918\" class=\"graf graf--blockquote graf-after--blockquote\"><div><strong class=\"markup--strong markup--blockquote-strong\">Master Slave: consistency is not too difficult because each piece of data has exactly one owning master. </strong>But then what do you do if you can’t see that master, some kind of postponed work is needed.</div></blockquote><blockquote id=\"b37f\" class=\"graf graf--blockquote graf-after--blockquote\"><div><strong class=\"markup--strong markup--blockquote-strong\">Master-Master: well if you can make it work then it seems to offer everything,</strong> no single point of failure, everyone can work all the time. <strong class=\"markup--strong markup--blockquote-strong\">Trouble is it <em class=\"markup--em markup--blockquote-em\">very</em> hard to preserve absolute consistency. </strong>See the <a href=\"https://en.wikipedia.org/wiki/Multi-master_replication\" data-href=\"https://en.wikipedia.org/wiki/Multi-master_replication\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"noreferrer noopener\" target=\"_blank\">wikipedia article</a> for more.</div></blockquote><blockquote id=\"df87\" class=\"graf graf--blockquote graf-after--blockquote\"><a href=\"https://stackoverflow.com/questions/3736969/master-master-vs-master-slave-database-architecture\" data-href=\"https://stackoverflow.com/questions/3736969/master-master-vs-master-slave-database-architecture\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://stackoverflow.com/questions/3736969/master-master-vs-master-slave-database-architecture</a></blockquote><p id=\"f27a\" class=\"graf graf--p graf-after--blockquote\">In<strong class=\"markup--strong markup--p-strong\"> master-slave</strong>, the master is the one which generally does the write and reads can be distributed across master and slave; the slave is like a hot standby. The main problem happens when there is an automatic switchover facility for HA when a master dies.</p><p id=\"3b7e\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">I will add a word here about database clusters</strong>. I used to work shortly in a project with a big Oracle RAC system, and have seen the problems related to maintaining it in the context of the data that scaled out with time. We needed Oracle support and also an expert in storage/SAN networking to balance disk usage. I am however no expert. With this disclaimer -Oracle RAC is said to be masterless,<strong class=\"markup--strong markup--p-strong\"> I will consider it to be a pseudo-master-slave architecture as there is a shared ‘master’ disk that is the basis of its architecture. </strong>This will mean that the slave (multi oracle instances in different nodes) can scale read, but when it comes to writing things are not that easy. Here is a quote from a better expert</p><blockquote id=\"d184\" class=\"graf graf--blockquote graf-after--p\"><div>I’ll start this blog post with a quick disclaimer. I’m what you would call a “born and raised” Oracle DBA. My first job, 15 years ago, had me responsible for administration and developing code on production Oracle 8 databases. Since then, I’ve had the opportunity to work as a database architect and administrator with all Oracle versions up to and including Oracle 12.2. Throughout my career, I’ve delivered a lot of successful projects using Oracle as the relational database componen….</div></blockquote><blockquote id=\"6107\" class=\"graf graf--blockquote graf-after--blockquote\"><div>Although you can scale read performance easily by adding more cluster nodes, s<strong class=\"markup--strong markup--blockquote-strong\">caling write performance is a more complex subject</strong>. Technically, Oracle RAC can scale writes and reads together when adding new nodes to the cluster, but attempts from multiple sessions to modify rows that reside in the same physical Oracle block (the lowest level of logical I/O performed by the database) can cause write overhead for the requested block and affect write performance. This is well known phenomena and why RAC-Aware applications are a real thing in the real world. …</div></blockquote><blockquote id=\"0934\" class=\"graf graf--blockquote graf-after--blockquote\"><div><strong class=\"markup--strong markup--blockquote-strong\">With the limitations for pure write scale-out, many Oracle RAC customers choose to split their RAC clusters into multiple “services,” </strong>which are logical groupings of nodes in the same RAC cluster.</div></blockquote><blockquote id=\"7e1f\" class=\"graf graf--blockquote graf-after--blockquote\"><div>Splitting writes from different individual “modules” in the application (that is, groups of independent tables) to different nodes in the cluster. This is also known as “application partitioning” (not to be confused with database table partitions).</div></blockquote><blockquote id=\"a69a\" class=\"graf graf--blockquote graf-after--blockquote\"><div>In extremely un-optimized workloads with high concurrency, directing all writes to a single RAC node and load-balancing only the reads.</div></blockquote><blockquote id=\"12df\" class=\"graf graf--blockquote graf-after--blockquote\"><a href=\"https://aws.amazon.com/blogs/database/amazon-aurora-as-an-alternative-to-oracle-rac/\" data-href=\"https://aws.amazon.com/blogs/database/amazon-aurora-as-an-alternative-to-oracle-rac/\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://aws.amazon.com/blogs/database/amazon-aurora-as-an-alternative-to-oracle-rac/</a></blockquote><p id=\"8d1f\" class=\"graf graf--p graf-after--blockquote\">Note that for scalability there can be clusters of master-slave nodes handling different tables, but that will be discussed later).</p><p id=\"46bb\" class=\"graf graf--p graf-after--p\">Obviously, this is done by a third node which is neither master or slave as it can only know if the master is gone down or not (NW down is also master down). This is essentially flawed.</p><h4 id=\"1c62\" class=\"graf graf--h4 graf-after--p\">The Split Brain Curse -High Availability in a Master-Slave auto failover System</h4><p id=\"d72b\" class=\"graf graf--p graf-after--h4\">In a master slave-based HA system where master and slaves run in different compute nodes (because there is a limit of vertical scalability), the Split Brain syndrome is a curse which does not have a good solution.</p><figure id=\"a775\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder\"><img class=\"graf-image\" data-image-id=\"1*RyzNZXf_b6WA04DOqeLVoA.png\" data-is-featured=\"true\" src=\"https://cdn-images-1.medium.com/max/1600/1*RyzNZXf_b6WA04DOqeLVoA.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"https://medium.com/stashaway-engineering/running-a-lagom-microservice-on-akka-cluster-with-split-brain-resolver-2a1c301659bd\" data-href=\"https://medium.com/stashaway-engineering/running-a-lagom-microservice-on-akka-cluster-with-split-brain-resolver-2a1c301659bd\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow\" target=\"_blank\">https://medium.com/stashaway-engineering/running-a-lagom-microservice-on-akka-cluster-with-split-brain-resolver-2a1c301659bd</a></figcaption></figure><p id=\"7bb4\" class=\"graf graf--p graf-after--figure\">The Split brain syndrome — if there is a network partition in a cluster of nodes, then which of the two nodes is the master, which is the slave? Depends on where the NW partition happens; It seems easy to solve, but unless there is some guarantee that the third node/common node has 100% connection reliability with other nodes, it is hard to resolve. We were using pgpool-2 and <a href=\"http://www.pgpool.net/mantisbt/view.php?id=227\" data-href=\"http://www.pgpool.net/mantisbt/view.php?id=227\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">this</a> was I guess one of the bugs that bit us. If you are the type who think that rare things do not happen, in the computer world, you will never walk in the street at any time a meteor could hit your head.</p><p id=\"b974\" class=\"graf graf--p graf-after--p\">Here is a short snippet with something that I was a part of trying to solve- analyzed by a colleague wrestling to make the pgpool work a few years back, basically automatic failover of a master-slave system.</p><blockquote id=\"b7ba\" class=\"graf graf--blockquote graf-after--p\"><div>· WE have two pgpool and two postgresql services configured as master and standby in to VMs. In case of postgresql, they are configured as active-standby. A virtual IP resides on the master pgpool and migrates to standby pgpool in case of failure.</div></blockquote><blockquote id=\"52f1\" class=\"graf graf--blockquote graf-after--blockquote\"><div>· In normal working, all DB calls pass through the master pgpool which redirects them to the master postgresql node.</div></blockquote><blockquote id=\"d5ed\" class=\"graf graf--blockquote graf-after--blockquote\"><div>· The standby postgresql keeps replicating data from the master node using WAL log synchronization.</div></blockquote><blockquote id=\"0056\" class=\"graf graf--blockquote graf-after--blockquote\"><div>· If the master postgresql node goes down/crashes, any of the two pgpool triggers a failover and promotes the standby postgresql as new master. When the old master node comes back up, it is brought as a standby node.</div></blockquote><blockquote id=\"d52f\" class=\"graf graf--blockquote graf-after--blockquote\"><div>Problem:</div></blockquote><blockquote id=\"3a35\" class=\"graf graf--blockquote graf-after--blockquote\"><div>With the current implementation, we have the following issues/short-comings:</div></blockquote><blockquote id=\"0bdb\" class=\"graf graf--blockquote graf-after--blockquote\"><div>· If the pgpool node that detects a master postgresql failure is on the same node, as the failed master, then it has to trigger a remote failover to the other postgresql node using SSH. If there is an SSH failure at that moment, the failover will fail, resulting in a standby-standby situation.</div></blockquote><blockquote id=\"4847\" class=\"graf graf--blockquote graf-after--blockquote\"><div>· If postgresql node gets detached from pgpool due to heavy load (this happens if pgpool is not able to make connections to postgresql), then there is no way to re-attach the node again. It has to be manually attached using repmgr library.</div></blockquote><blockquote id=\"6dfd\" class=\"graf graf--blockquote graf-after--blockquote\"><div>· In the above case, if the slave node detaches itself and master node goes down, then pgpool has no more nodes to trigger failover to. This again causes in a standby-standby scenario…</div></blockquote><p id=\"9f1b\" class=\"graf graf--p graf-after--blockquote\">It is not just a Postgres problem, a general google search (below) on this should throw up many problems most such software, Postgres, MySQL, Elastic Search etc.</p><p id=\"d3c8\" class=\"graf graf--p graf-after--p\"><a href=\"https://www.google.co.in/search?rlz=1C1CHBD_enIN781IN781&amp;ei=5uXqW6fgCcXhvAT075TACA&amp;q=high+availabillity+master+slave+and+the+split+brain+syndrome&amp;oq=high+availabillity+master+slave+and+the+split+brain+syndrome&amp;gs_l=psy-ab.3...613378.642087.0.642573.37.27.10.0.0.0.273.3278.2j23j1.26.0....0...1c.1.64.psy-ab..1.3.377...33i10k1.0.0MfIzWAb8CU\" data-href=\"https://www.google.co.in/search?rlz=1C1CHBD_enIN781IN781&amp;ei=5uXqW6fgCcXhvAT075TACA&amp;q=high+availabillity+master+slave+and+the+split+brain+syndrome&amp;oq=high+availabillity+master+slave+and+the+split+brain+syndrome&amp;gs_l=psy-ab.3...613378.642087.0.642573.37.27.10.0.0.0.273.3278.2j23j1.26.0....0...1c.1.64.psy-ab..1.3.377...33i10k1.0.0MfIzWAb8CU\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://www.google.co.in/search?rlz=high+availabillity+master+slave+and+the+split+brain+syndrome</a></p><p id=\"3255\" class=\"graf graf--p graf-after--p\">Before we leave this for those curious you can see here the mechanism from <strong class=\"markup--strong markup--p-strong\">Oracle RAC to tackle the split-brain (all master-slave architectures this will crop up but never in a true masterless system)</strong>-where they assume the<a href=\"https://www.red-gate.com/simple-talk/sql/oracle/split-brain-whats-new-in-oracle-database-12-1-0-2c/\" data-href=\"https://www.red-gate.com/simple-talk/sql/oracle/split-brain-whats-new-in-oracle-database-12-1-0-2c/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\"> common shared disk</a> is always available from all cluster; I don’t know in depth the RAC structure, but looks like a classical <a href=\"https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing\" data-href=\"https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">distributed computing fallacy</a> or a <a href=\"https://ask.slashdot.org/comments.pl?sid=168316&amp;cid=14038920\" data-href=\"https://ask.slashdot.org/comments.pl?sid=168316&amp;cid=14038920\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">single point of failure if not configured redundantly</a>; which on further reading, t<a href=\"https://community.oracle.com/thread/2541200?start=0&amp;tstart=0\" data-href=\"https://community.oracle.com/thread/2541200?start=0&amp;tstart=0\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">hey are recommending</a> to cover this part.</p><blockquote id=\"7917\" class=\"graf graf--blockquote graf-after--p\"><div>Voting disk needs to be mirrored, should it become unavailable, cluster will come down. Hence, you should maintain multiple copies of the voting disks on separate disk LUNs so that you eliminate a Single Point of Failure (SPOF) in your Oracle 11g RAC configuration. <a href=\"http://oracleinaction.com/voting-disk/\" data-href=\"http://oracleinaction.com/voting-disk/\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener noopener\" target=\"_blank\">http://oracleinaction.com/voting-disk/</a></div></blockquote><p id=\"8a0a\" class=\"graf graf--p graf-after--blockquote\">Another from a blog referred from Google Cloud Spanner page which captures sort of the essence o fthis problem.</p><blockquote id=\"ce85\" class=\"graf graf--blockquote graf-after--p\"><div>We use MySQL to power our website, which allows us to serve millions of students every month, but is difficult to scale up — we need our database to handle more writes than a single machine can process.<strong class=\"markup--strong markup--blockquote-strong\"><em class=\"markup--em markup--blockquote-em\"> There are many solutions to this problem, but these can be complex to run or require extensive refactoring of your application’s SQL queries</em></strong></div></blockquote><blockquote id=\"ca65\" class=\"graf graf--blockquote graf-after--blockquote\"><a href=\"https://quizlet.com/blog/quizlet-cloud-spanner\" data-href=\"https://quizlet.com/blog/quizlet-cloud-spanner\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener noopener\" target=\"_blank\">https://quizlet.com/blog/quizlet-cloud-spanner</a></blockquote><p id=\"e788\" class=\"graf graf--p graf-after--blockquote\">These type of scenarios are common and a lot of instances can be found of SW trying to fix this. You may want to steer clear of this; <strong class=\"markup--strong markup--p-strong\">the Database’s using the master-slave (with or without automatic failover) -</strong><a href=\"https://dzone.com/articles/the-mysql-high-availability-landscape-in-2017-part\" data-href=\"https://dzone.com/articles/the-mysql-high-availability-landscape-in-2017-part\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">MySQL</a>, Postgres, MongoDB, Oracle RAC(note MySQL recent <a href=\"https://dzone.com/articles/the-mysql-high-availability-landscape-in-2017-part\" data-href=\"https://dzone.com/articles/the-mysql-high-availability-landscape-in-2017-part\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">Cluster</a> seems to use master less concept (similar/based on Paxos) but with limitations, read <a href=\"https://severalnines.com/blog/comparing-oracle-rac-ha-solution-galera-cluster-mysql-or-mariadb\" data-href=\"https://severalnines.com/blog/comparing-oracle-rac-ha-solution-galera-cluster-mysql-or-mariadb\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">MySQL Galera Cluster</a>)</p><p id=\"a6f6\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">You may want to choose a database that support’s Master-less High Availability( also read Replication )</strong></p><ol class=\"postList\"><li id=\"139b\" class=\"graf graf--li graf-after--p\"><strong class=\"markup--strong markup--li-strong\">Apache Cassandra</strong></li></ol><blockquote id=\"663a\" class=\"graf graf--blockquote graf-after--li\"><div>Cassandra has a peer-to-peer (or “masterless”) distributed “ring” architecture that is <strong class=\"markup--strong markup--blockquote-strong\">elegant, easy to set up, and maintain</strong>.In Cassandra, all nodes are the same; there is no concept of a master node, with all nodes communicating with each other via a gossip protocol. <a href=\"https://www.datastax.com/wp-content/uploads/2012/09/WP-DataStax-MultiDC.pdf\" data-href=\"https://www.datastax.com/wp-content/uploads/2012/09/WP-DataStax-MultiDC.pdf\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener noopener\" target=\"_blank\">https://www.datastax.com/wp-content/uploads/2012/09/WP-DataStax-MultiDC.pdf</a></div></blockquote><p id=\"bca6\" class=\"graf graf--p graf-after--blockquote\">Apache Cassandra does not use Paxos yet has tunable consistency (sacrificing availability) without complexity/read slowness of Paxos consensus. ( It uses Paxos only for LWT. (Here is a<a href=\"https://docs.google.com/presentation/d/1y2lbLzSmZdd3OVzXpxAvmvWt4Hhn_WljQiU4x30Cst4/edit#slide=id.g1514cfd79b_0_1782\" data-href=\"https://docs.google.com/presentation/d/1y2lbLzSmZdd3OVzXpxAvmvWt4Hhn_WljQiU4x30Cst4/edit#slide=id.g1514cfd79b_0_1782\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\"> gentle introduction</a> which seems easier to follow than others (I do not know how it works))</p><p id=\"675a\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">2. Google Cloud Spanner?</strong></p><blockquote id=\"636e\" class=\"graf graf--blockquote graf-after--p\"><div>Spanner claims to be consistent and available Despite being a global distributed system, Spanner claims to be consistent and highly available, which implies there are no partitions and thus many are skeptical.1 Does this mean that Spanner is a CA system as defined by CAP? The short answer is “no” technically, but “yes” in effect and its users can and do assume CA. The purist answer is “no” because partitions can happen and in fact have happened at Google, and during (some) partitions, Spanner chooses C and forfeits A. It is technically a CP system. We explore the impact of partitions below.</div></blockquote><blockquote id=\"1a72\" class=\"graf graf--blockquote graf-after--blockquote\"><div><em class=\"markup--em markup--blockquote-em\">First, Google runs its own private global network. Spanner is not running over the public Internet — </em><strong class=\"markup--strong markup--blockquote-strong\"><em class=\"markup--em markup--blockquote-em\">in fact, every Spanner packet flows only over Google-controlled routers and links</em></strong><em class=\"markup--em markup--blockquote-em\"> (excluding any edge links to remote clients).</em></div></blockquote><blockquote id=\"0c1d\" class=\"graf graf--blockquote graf-after--blockquote\"><div><em class=\"markup--em markup--blockquote-em\">One subtle thing about </em><strong class=\"markup--strong markup--blockquote-strong\"><em class=\"markup--em markup--blockquote-em\">Spanner is that it gets serializability from locks</em></strong><em class=\"markup--em markup--blockquote-em\">, but it gets external consistency (similar to linearizability) </em><strong class=\"markup--strong markup--blockquote-strong\"><em class=\"markup--em markup--blockquote-em\">from TrueTime</em></strong></div></blockquote><blockquote id=\"e112\" class=\"graf graf--blockquote graf-after--blockquote\"><a href=\"https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45855.pdf\" data-href=\"https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45855.pdf\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45855.pdf</a></blockquote><p id=\"303e\" class=\"graf graf--p graf-after--blockquote\">3. <strong class=\"markup--strong markup--p-strong\">Cockroach</strong> <strong class=\"markup--strong markup--p-strong\">DB</strong> is an open source in-premise database of Cloud Spanner -that is Highly Available and strongly Consistent that uses Paxos type algorithm.</p><blockquote id=\"e03f\" class=\"graf graf--blockquote graf-after--p\"><div>Writes are serviced using the <a href=\"https://raft.github.io/\" data-href=\"https://raft.github.io/\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"noopener\" target=\"_blank\"><strong class=\"markup--strong markup--blockquote-strong\">Raft </strong>consensus algorithm</a>, a popular alternative to <a href=\"http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf\" data-href=\"http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"noopener\" target=\"_blank\">Paxos</a>. — <a href=\"https://www.cockroachlabs.com/docs/stable/strong-consistency.html\" data-href=\"https://www.cockroachlabs.com/docs/stable/strong-consistency.html\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://www.cockroachlabs.com/docs/stable/strong-consistency.html</a></div></blockquote><blockquote id=\"03b7\" class=\"graf graf--blockquote graf-after--blockquote\"><div>The main difference is that since CockroachDB does not have Google infrastructure to implement TrueTime API to synchronize the clocks across the distributed system, the consistency guarantee it provides is known as Serializability and not Linearizability (which Spanner provides). <a href=\"http://wp.sigmod.org/?p=2153\" data-href=\"http://wp.sigmod.org/?p=2153\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">http://wp.sigmod.org/?p=2153</a></div></blockquote><p id=\"9b1c\" class=\"graf graf--p graf-after--blockquote\">Cockroach DB maybe something to see as it gets more stable;</p><p id=\"e340\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">Scalability — Application Sharding and Auto-Sharding</strong></p><p id=\"f81c\" class=\"graf graf--p graf-after--p\">This directly takes us to the evolution of NoSQL databases. Database scaling is done via <a href=\"http://www.agildata.com/database-sharding/\" data-href=\"http://www.agildata.com/database-sharding/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">sharding</a>, the key thing is if sharding is automatic or manual. By manual, I mean that application developer do the custom code to distribute the data in code — <a href=\"https://www.javaworld.com/article/2073449/think-twice-before-sharding.html\" data-href=\"https://www.javaworld.com/article/2073449/think-twice-before-sharding.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">application level sharding</a>. Automatic sharding is done by NoSQL database like Cassandra whereas almost all older SQL type databases (MySQL, Oracle, Postgres) one need to do sharding manually.</p><p id=\"a300\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">Auto-sharding is a key feature that ensures scalability without complexity increasing in the code.</strong></p><p id=\"4fbf\" class=\"graf graf--p graf-after--p\">Here is a snippet from the net. It covers two parts, the disk I/O part (which I guess early designers never thought will become a bottleneck later on with more data-Cassandra designers knew fully well this problem and designed to minimize disk seeks), and the other which is more important touches on application level sharding.</p><blockquote id=\"3d9d\" class=\"graf graf--blockquote graf-after--p\"><div>Why doesn’t PostgreSQL naturally scale well?</div></blockquote><blockquote id=\"9c79\" class=\"graf graf--blockquote graf-after--blockquote\"><div>It comes down to the performance gap between RAM and disk.</div></blockquote><blockquote id=\"c5fb\" class=\"graf graf--blockquote graf-after--blockquote\"><div>But if the data is sufficiently large that we can’t fit all (similarly fixed-size) pages of our index in memory, then updating a random part of the tree can involve <strong class=\"markup--strong markup--blockquote-strong\">significant disk I/O as we read pages from disk into memory, modify in memory, and then write back out to dis</strong>k (when evicted to make room for other pages). And a relational database like PostgreSQL keeps an index (or other data structure, such as a <a href=\"https://en.wikipedia.org/wiki/B-tree\" data-href=\"https://en.wikipedia.org/wiki/B-tree\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"noopener\" target=\"_blank\">B-tree</a>) for each table index, in order for values in that index to be found efficiently. So, the problem compounds as you index more columns.</div></blockquote><blockquote id=\"3ed8\" class=\"graf graf--blockquote graf-after--blockquote\"><div><strong class=\"markup--strong markup--blockquote-strong\">In general, if you are writing a lot of data to a PostgreSQL table, at some point you’ll need partitioning. </strong><a href=\"https://blog.timescale.com/scaling-partitioning-data-postgresql-10-explained-cd48a712a9a1\" data-href=\"https://blog.timescale.com/scaling-partitioning-data-postgresql-10-explained-cd48a712a9a1\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"noopener\" target=\"_blank\">https://blog.timescale.com/scaling-partitioning-data-postgresql-10-explained-cd48a712a9a1</a></div></blockquote><p id=\"32b9\" class=\"graf graf--p graf-after--blockquote\">There is another part to this, and it relates to the master-slave architecture which means master is the one that writes and slaves just act as a standby to replicate and distribute reads. (More accurately, Oracle RAC or MongoDB Replication Sets are not exactly limited by only one master to write and multiple slaves to read from; but either<a href=\"https://www.oracle.com/technetwork/middleware/coherence/coherence-sample-chapter-132591.pdf\" data-href=\"https://www.oracle.com/technetwork/middleware/coherence/coherence-sample-chapter-132591.pdf\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\"> use a shared storage </a>and multiple masters -slave sets to write and read to, in case of Oracle RAC; and similar in case of MongoDB uses multiple replication sets with <a href=\"https://www.simplilearn.com/replication-and-sharding-mongodb-tutorial-video\" data-href=\"https://www.simplilearn.com/replication-and-sharding-mongodb-tutorial-video\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">each replication set being a master-slave </a>combination, but not using shared storage like Oracle RAC. Please see above where I mentioned the practical limits of a psuedo master-slave system like shared disk systems)</p><p id=\"df2e\" class=\"graf graf--p graf-after--p\">Let us now see how this automatic sharding is done by Cassandra and what it means to data Modelling.</p><h4 id=\"52df\" class=\"graf graf--h4 graf-after--p\">Part 2 : Cassandra Internals for Data Modelling</h4><h4 id=\"eb43\" class=\"graf graf--h4 graf-after--h4\">Cassandra Write — Intuition</h4><figure id=\"25cf\" class=\"graf graf--figure graf-after--h4\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"0*9GCxaoWtzo0dEIsp.png\" data-width=\"769\" data-height=\"358\" data-action=\"zoom\" data-action-value=\"0*9GCxaoWtzo0dEIsp.png\" src=\"https://cdn-images-1.medium.com/max/1600/0*9GCxaoWtzo0dEIsp.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\">Cassandra Write path Source</figcaption></figure><p id=\"5bac\" class=\"graf graf--p graf-after--figure\">Note the Memory and Disk Part. The flush from Memtable to SStable is one operation and the SSTable file once written is immutable (not more updates). Many people may have seen the above diagram and still missed few parts. SSTable flush happens periodically when memory is full. Commit log has the data of the commit also and is used for persistence and recovering in scenarios like power-off before flushing to SSTable. It is always written in append mode and read only on startup. Since SSTable is a different file and Commit log is a different file and since there is only one arm in a magnetic disk, this is the reason why the main guideline is to configure Commit log in a different disk (not even partition and SStable (data directory)i<strong class=\"markup--strong markup--p-strong\">n a separate disk.</strong></p><blockquote id=\"483d\" class=\"graf graf--blockquote graf-after--p\"><div>Cassandra performs very well on both spinning hard drives and solid state disks.<strong class=\"markup--strong markup--blockquote-strong\"> </strong>In both cases, Cassandra’s sorted immutable <strong class=\"markup--strong markup--blockquote-strong\">SSTables allow for linear reads, few seeks, and few overwrites,</strong> maximizing throughput for <strong class=\"markup--strong markup--blockquote-strong\">HDDs</strong> and lifespan of <strong class=\"markup--strong markup--blockquote-strong\">SSDs</strong> by avoiding<strong class=\"markup--strong markup--blockquote-strong\"> write amplification</strong>.</div></blockquote><blockquote id=\"4424\" class=\"graf graf--blockquote graf-after--blockquote\"><div>However, when using spinning disks, it’s important that the commitlog (<code class=\"markup--code markup--blockquote-code\">commitlog_directory</code>) be on one physical disk (not simply a partition, but a physical disk), and the data files (<code class=\"markup--code markup--blockquote-code\">data_file_directories</code>) be set to a<strong class=\"markup--strong markup--blockquote-strong\"> separate physical disk</strong>. By separating the commitlog from the data directory, <strong class=\"markup--strong markup--blockquote-strong\">writes can benefit from sequential appends</strong> to the commitlog without having to<strong class=\"markup--strong markup--blockquote-strong\"> seek around the platter</strong> as reads request data from various SSTables on disk. -<a href=\"http://cassandra.apache.org/doc/4.0/operating/hardware.html\" data-href=\"http://cassandra.apache.org/doc/4.0/operating/hardware.html\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">http://cassandra.apache.org/doc/4.0/operating/hardware.html</a></div></blockquote><p id=\"2fcb\" class=\"graf graf--p graf-after--blockquote\">Please, note that the SSTable file is immutable. This means that after multiple flushes there would be many SSTable. This would mean that read query may have to read multiple SSTables. Also, updates to rows are new insert’s in another SSTable with a higher timestamp and this also has to be reconciled with different SSTables for reading. To optimize there is something called periodic <strong class=\"markup--strong markup--p-strong\">compaction </strong>that is done where multiple SSTables are combined to a new SSTable file and the older is discarded.</p><p id=\"72b4\" class=\"graf graf--p graf-after--p\">Note that Delete’s are like updates but with a marker called Tombstone and are deleted during compaction. However, due to the complexity of the distributed database, there is additional safety (read complexity) added like gc_grace seconds to prevent Zombie rows. This is one of the reasons that Cassandra does not like frequent Delete.</p><p id=\"8258\" class=\"graf graf--p graf-after--p\">If you want to get an intuition behind compaction and how relates to very fast writes (LSM <a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlManageOndisk.html\" data-href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlManageOndisk.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">storage engine</a>) and you can read this more</p><blockquote id=\"7e8e\" class=\"graf graf--blockquote graf-after--p\"><div>These SSTables might contain outdated data — e.g., different SSTables might contain both an old value and new value of the same cell, or an old value for a cell later deleted. That is fine, as Cassandra uses timestamps on each value or deletion to figure out which is the most recent value. However, it is a waste of disk space. It also slows down reads: different SSTables can hold different columns of the same row, so a query might need to read from multiple SSTables to compose its result.</div></blockquote><blockquote id=\"0df1\" class=\"graf graf--blockquote graf-after--blockquote\"><div>For these reasons, compaction is needed. Compaction is the process of reading several SSTables and outputting <em class=\"markup--em markup--blockquote-em\">one</em> SSTable containing the merged, most recent, information.</div></blockquote><blockquote id=\"6db1\" class=\"graf graf--blockquote graf-after--blockquote\"><div><strong class=\"markup--strong markup--blockquote-strong\">This technique, of keeping sorted files and merging them, is a well-known one and often called </strong><a href=\"http://en.wikipedia.org/wiki/Log-structured_merge-tree\" data-href=\"http://en.wikipedia.org/wiki/Log-structured_merge-tree\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\"><strong class=\"markup--strong markup--blockquote-strong\">Log-Structured Merge (LSM) tree</strong></a><strong class=\"markup--strong markup--blockquote-strong\">.</strong></div></blockquote><blockquote id=\"a13d\" class=\"graf graf--blockquote graf-after--blockquote\"><a href=\"https://github.com/scylladb/scylla/wiki/SSTable-compaction-and-compaction-strategies\" data-href=\"https://github.com/scylladb/scylla/wiki/SSTable-compaction-and-compaction-strategies\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://github.com/scylladb/scylla/wiki/SSTable-compaction-and-compaction-strategies</a><div> + </div><a href=\"https://blog.pythian.com/proposal-for-a-new-cassandra-cluster-key-compaction-strategy/\" data-href=\"https://blog.pythian.com/proposal-for-a-new-cassandra-cluster-key-compaction-strategy/\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"noopener\" target=\"_blank\">others</a></blockquote><p id=\"dae5\" class=\"graf graf--p graf-after--blockquote\">This blog gives the <a href=\"https://medium.com/databasss/on-disk-io-part-3-lsm-trees-8b2da218496f\" data-href=\"https://medium.com/databasss/on-disk-io-part-3-lsm-trees-8b2da218496f\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">internals of LSM</a> if you are interested.</p><p id=\"2d0b\" class=\"graf graf--p graf-after--p\">We have skipped some parts here. One main part is <strong class=\"markup--strong markup--p-strong\">Replication</strong>. When we need to distribute the data across multi-nodes for data availability (read data safety), the writes have to be replicated to that many numbers of nodes as Replication Factor.</p><p id=\"8cb5\" class=\"graf graf--p graf-after--p\">Also when there are multiple nodes, which node should a client connect to?</p><p id=\"65f0\" class=\"graf graf--p graf-after--p\">It connects to any node that it has the IP to and it becomes the coordinator node <strong class=\"markup--strong markup--p-strong\">for the client.</strong></p><blockquote id=\"f44b\" class=\"graf graf--blockquote graf-after--p\"><div>The coordinator node is typically chosen by an algorithm which takes “network distance” into account. <strong class=\"markup--strong markup--blockquote-strong\">Any node can act as the coordinator, and at first, requests will be sent to the nodes which your driver knows abou</strong>t….The coordinator only stores data locally (on a write) if it ends up being one of the nodes responsible for the data’s token range --h<a href=\"https://stackoverflow.com/questions/32867869/how-cassandra-chooses-the-coordinator-node-and-the-replication-nodes\" data-href=\"https://stackoverflow.com/questions/32867869/how-cassandra-chooses-the-coordinator-node-and-the-replication-nodes\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener noopener noopener\" target=\"_blank\">ttps://stackoverflow.com/questions/32867869/how-cassandra-chooses-the-coordinator-node-and-the-replication-nodes</a></div></blockquote><h4 id=\"5d01\" class=\"graf graf--h4 graf-after--blockquote\">Role of PARTITION Key in Write</h4><p id=\"08de\" class=\"graf graf--p graf-after--h4\">Now let us see how the auto-sharding taking place. Suppose there are three nodes in a Cassandra cluster. Each node will own a particular token range.</p><p id=\"d3ac\" class=\"graf graf--p graf-after--p\">Assume a particular row is inserted. <strong class=\"markup--strong markup--p-strong\">Cassandra uses the PARTITION COLUMN Key value and feeds it a hash function which tells which of the bucket the row has to be written to.</strong></p><p id=\"d74e\" class=\"graf graf--p graf-after--p\">It uses the same function on the WHERE Column key value of the READ Query which also gives exactly the same node where it has written the row.</p><p id=\"099b\" class=\"graf graf--p graf-after--p\">A Primary key should be unique. More specifically a ParitionKey should be unique and all values of those are needed in the WHERE clause. (Cassandra does not do a Read before a write, so there is no constraint check like the Primary key of relation databases, it just updates another row)</p><blockquote id=\"f20e\" class=\"graf graf--blockquote graf-after--p\"><div><strong class=\"markup--strong markup--blockquote-strong\">The partition key has a special use in Apache Cassandra beyond showing the uniqueness of the record in the database -</strong><a href=\"https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key\" data-href=\"https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key</a></div></blockquote><p id=\"a945\" class=\"graf graf--p graf-after--blockquote\">The relation between PRIMARY Key and PARTITION KEY.</p><p id=\"28ab\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">PARTITION KEY == First Key in PRIMARY KEY, rest are clustering keys</strong></p><blockquote id=\"fab3\" class=\"graf graf--blockquote graf-after--p\"><div>Example 1: PARTITION KEY == PRIMARY KEY== videoid</div></blockquote><blockquote id=\"1176\" class=\"graf graf--blockquote graf-after--blockquote\"><div>CREATE TABLE videos (<br />…PRIMARY KEY (videoid)<br />);</div></blockquote><blockquote id=\"f2a5\" class=\"graf graf--blockquote graf-after--blockquote\"><div>Example 2: PARTITION KEY == userid, rest of PRIMARY keys are Clustering keys for ordering/sortig the columns</div></blockquote><blockquote id=\"345b\" class=\"graf graf--blockquote graf-after--blockquote\"><div>CREATE TABLE user_videos (<br /> PRIMARY KEY (userid, added_date, videoid)<br />);</div></blockquote><blockquote id=\"6302\" class=\"graf graf--blockquote graf-after--blockquote\"><div>Example 3: COMPOSITE PARTITION KEY ==(race_year, race_name)</div></blockquote><blockquote id=\"e58c\" class=\"graf graf--blockquote graf-after--blockquote\"><div>CREATE TABLE rank_by_year_and_name ( <br />PRIMARY KEY ((race_year, race_name), rank) <br />);</div></blockquote><p id=\"778b\" class=\"graf graf--p graf-after--blockquote\"><strong class=\"markup--strong markup--p-strong\">Now here is the main intuition. Part 1</strong></p><p id=\"5f8b\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">For writes to be distributed and scaled the partition key should be chosen so that it distributes writes in a balanced way across all nodes.</strong></p><p id=\"18f2\" class=\"graf graf--p graf-after--p\">But don’t you think it is common sense that if a query read has to touch all the nodes in the NW it will be slow. Yes, you are right; and that is what I wanted to highlight. Before that let us go shallowly into — Cassandra Read Path</p><figure id=\"d683\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"0*QGCqGKK5lOdBbpoe.png\" data-width=\"523\" data-height=\"291\" src=\"https://cdn-images-1.medium.com/max/1600/0*QGCqGKK5lOdBbpoe.png\" alt=\"image\" /></div></figure><p id=\"6d67\" class=\"graf graf--p graf-after--figure\"><strong class=\"markup--strong markup--p-strong\">Now here is the main intuition. Part 2</strong></p><p id=\"52a8\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">For reads to be NOT distributed across multiple nodes (that is fetched and combine from multiple nodes) a read triggered from a client query should fall in one partition </strong>(forget replication for simplicity)</p><p id=\"efb9\" class=\"graf graf--p graf-after--p\">This is illustrated beautifully in the diagram below</p><p id=\"2107\" class=\"graf graf--p graf-after--p\">You can see how the COMPOSITE PARTITION KEY is modeled<strong class=\"markup--strong markup--p-strong\"> so that writes are distributed across nodes and reads for particular state lands in one partition.</strong></p><figure id=\"230c\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder\"><img class=\"graf-image\" data-image-id=\"0*h5hj5vlKIlHatGf2.png\" src=\"https://cdn-images-1.medium.com/max/1600/0*h5hj5vlKIlHatGf2.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\">source <a href=\"http://db.geeksinsight.com/2016/07/19/cassandra-for-oracle-dbas-part-2-three-things-you-need-to-know/\" data-href=\"http://db.geeksinsight.com/2016/07/19/cassandra-for-oracle-dbas-part-2-three-things-you-need-to-know/\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener noopener\" target=\"_blank\">http://db.geeksinsight.com/2016/07/19/cassandra-for-oracle-dbas-part-2-three-things-you-need-to-know/</a></figcaption></figure><p id=\"fd77\" class=\"graf graf--p graf-after--figure\">To have a good read performance/fast query we need data for a query in one partition read one node.There is a balance between write distribution and read consolidation that you need to achieve, and you need to know your data and query to know that.</p><blockquote id=\"6688\" class=\"graf graf--blockquote graf-after--p\"><div>The point is, these two goals often conflict, so you’ll need to try to balance them.</div></blockquote><blockquote id=\"5567\" class=\"graf graf--blockquote graf-after--blockquote\"><div><strong class=\"markup--strong markup--blockquote-strong\">Conflicting Rules?</strong></div></blockquote><blockquote id=\"20fa\" class=\"graf graf--blockquote graf-after--blockquote\"><div>If it’s good to minimize the number of partitions that you read from, why not put everything in a single big partition? You would end up violating Rule #1, which is to spread data evenly around the cluster.</div></blockquote><blockquote id=\"86e3\" class=\"graf graf--blockquote graf-after--blockquote\"><div>The point is, these two goals often conflict, so you’ll need to try to balance them.</div></blockquote><blockquote id=\"1f5a\" class=\"graf graf--blockquote graf-after--blockquote\"><div>Model Around Your Queries</div></blockquote><blockquote id=\"8095\" class=\"graf graf--blockquote graf-after--blockquote\"><div>The way to minimize partition reads is to model your data to fit your queries. Don’t model around relations. Don’t model around objects. Model around your queries. Here’s how you do that -</div></blockquote><blockquote id=\"9f46\" class=\"graf graf--blockquote graf-after--blockquote\"><a href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\" data-href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener noopener\" target=\"_blank\"><strong class=\"markup--strong markup--blockquote-strong\">https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling</strong></a></blockquote><p id=\"0b28\" class=\"graf graf--p graf-after--blockquote\"><strong class=\"markup--strong markup--p-strong\">This is the most essential skill that one needs when doing modeling for Cassandra.</strong></p><blockquote id=\"fc7a\" class=\"graf graf--pullquote graf-after--p\"><a href=\"https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key\" data-href=\"https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key\" class=\"markup--anchor markup--pullquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\"><strong class=\"markup--strong markup--pullquote-strong\">https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key</strong></a></blockquote><p id=\"9e8d\" class=\"graf graf--p graf-after--pullquote graf--trailing\">A more detailed example of modeling the Partition key along with some explanation of how CAP theorem applies to Cassandra with tunable consistency is described in part 2 of this series<a href=\"https://hackernoon.com/using-apache-cassandra-a-few-things-before-you-start-ac599926e4b8\" data-href=\"https://hackernoon.com/using-apache-cassandra-a-few-things-before-you-start-ac599926e4b8\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener noopener noopener\" target=\"_blank\">https://hackernoon.com/using-apache-cassandra-a-few-things-before-you-start-ac599926e4b8</a></p>"}}]}},"pageContext":{"alternative_id":12991}}