{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Snap Cassandra to S3 with tablesnap","alternative_id":9483,"content":"<p>We were trying to find solution to backup and restore Cassandra hosted on AWS without loosing too much data . We tried using the native <strong>nodetool snapshot </strong>but we will loose data between the snapshots. </p><p>We found a tool which seems to do the job well <a href=\"https://github.com/JeremyGrosser/tablesnap\" target=\"_blank\" rel=\"nofollow noopener\">tablesnap</a> . This tool provides scripts which can help backup , restore SSTables. We can restore data from specific time in past. </p> \n<p>The following are the scripts which are part of this project.</p> \n<ul><li><strong>tablesnap : </strong> This script can be used for backup Cassandra data folder to S3</li> \n <li><strong>tableslurp :</strong> Used to restore the data files from S3</li> \n <li><strong>tablechop :</strong> Used to delete older data files from S3.</li> \n</ul><p><strong>tablesnap</strong> script monitors the Cassandra data folder continuously and copies any new files which are created to configured S3 bucket. This tool also creates a JSON file which lists all the files in the data folder at the time of snapshot providing option to restore at a specific time in past.</p> \n<p>The usage of the tool is as below</p> \n<pre spellcheck=\"false\">tablesnap [-h] -k AWS_KEY -s AWS_SECRET [-r] [-a] [-B] [-p PREFIX]\n                 [--without-index] [--keyname-separator KEYNAME_SEPARATOR]\n                 [-t THREADS] [-n NAME] [-e EXCLUDE | -i INCLUDE]\n                 [--listen-events {IN_MOVED_TO,IN_CLOSE_WRITE}]\n                 [--max-upload-size MAX_UPLOAD_SIZE]\n                 [--multipart-chunk-size MULTIPART_CHUNK_SIZE]\n                 bucket paths [paths ...]\n</pre> \n<p>This tool depends few basic principles of the Cassandra data </p> \n<ul><li>The data is stored as series of files.</li> \n <li>The SSTables are immutable structures and once created they are never modified.</li> \n <li>During compaction the old SSTable files are deleted &amp; a new file is created instead of updating the existing file.</li> \n <li>New SSTables are created in temporary folder and then moved into the data folder. The tool listens to IN_MOVED or CLOSE_WRITE events hence file will be consistent when uploaded to S3</li> \n</ul><p>The usage of the script is as below, in a cluster this needs to be configured on each node with respective node names.</p> \n<pre spellcheck=\"false\">tablesnap -B -a –r --aws-region ap-southeast-1 mybucket -n node1 /var/lib/cassandra/data/mykeyspace\n</pre> \n<p>The content of the JSON file is as shown below which lists all the files which are part of the snapshot. </p> \n<pre spellcheck=\"false\">{  \n   \"/var/lib/cassandra/data/mykeyspace/users-cf815f100f9711e78211dbd467a9ea7d\":[  \n      \"backups\",\n      \"lb-1- big-Index.db\",\n      \"lb-1- big-Digest.adler32\",\n      \"lb-1- big-Statistics.db\",\n      \"lb-1- big-CompressionInfo.db\",\n      \"lb-1- big-Data.db\",\n      \"lb-1- big-Summary.db\",\n      \"lb-1- big-Filter.db\",\n      \"lb-1- big-TOC.txt\"\n   ]\n</pre> \n<pre spellcheck=\"false\">}\n</pre> \n<p>In order to restore the data from the S3 , we can use <strong>tableslurp </strong>tool . We need to identify the snapshot which we need to recover and use the tool to download the data from S3 as below.</p> \n<pre spellcheck=\"false\">udo tableslurp -- aws-region ap-southeast- 1 mybucket -n node1\n/var/lib/cassandra/data/mykeyspace/users-cf815f100f9711e78211dbd467a9ea7d /var/lib/cassandra/data/mykeyspace/users-f234fgsdfsfdsfsdd\n --file lb-2-big-Data.db -o cassandra -g cassandra\n</pre> \n \n<p>In the above commands we are restoring the data on <strong>node1 </strong>which was backed up while SSTable <strong>lb-2-big-Data.db</strong> is created. </p> \n<p>To recover the entire cluster we need to shutdown new nodes &amp; recover respective data with <strong>tableslurp </strong>and then restart the nodes (seed nodes followed by others) . Once the cluster is up run <strong>nodetool repair </strong>to restore all the data. </p> \n<p>One additional step which we have to take care to avoid loosing data which is in <strong>memtable</strong> is by periodic <strong>nodetool flush</strong> which will help to avoid loosing data .</p>"}}]}},"pageContext":{"alternative_id":9483}}