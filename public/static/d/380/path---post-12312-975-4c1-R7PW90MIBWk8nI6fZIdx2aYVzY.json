{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Introduction to cstar","alternative_id":12312,"content":"<p>Spotify is a long time user of Apache Cassandra at very large scale. It is also a creative company which tries to open source most of the tools they build for internal needs. They released <a href=\"http://cassandra-reaper.io\">Cassandra Reaper</a> a few years ago to give the community a reliable way of repairing clusters, which we now love and actively maintain. Their latest open sourced tool for Cassandra is <a href=\"https://github.com/spotify/cstar\">cstar</a>, a <a href=\"https://github.com/ParallelSSH/parallel-ssh\">parallel-ssh</a> equivalent (distributed shell) that is Cassandra topology aware. At TLP, we love it already and are sure you soon will too.</p><h2 id=\"what-is-cstar\">What is cstar?</h2>\n<p>Running distributed databases requires good automation, especially at scale. But even with small clusters, running the same command or roll restarting a cluster can quickly get tedious.\nSure, you can use tools like dsh and pssh, but they run commands on all servers at the same time (or just a given number) and you need to keep a list of the nodes to connect to locally. Each time your cluster scales out/in or if nodes get replaced you need to update the list. If you forget to update you may run commands that won’t touch the whole cluster without noticing.</p>\n<p>All commands cannot run on all nodes at the same time either. For instance upgrading sstables, running cleanup, major compaction or restarting nodes will have an impact on either latencies or availability and require more granularity of execution.</p>\n<p><a href=\"https://github.com/spotify/cstar\">Cstar</a> doesn’t suffer any of the above problems. It will discover the topology of the cluster dynamically and tune concurrency based on replication settings. In addition, cstar will run from a single machine (not necessarily within the cluster) that has SSH access to all nodes in the cluster, and perform operations through SSH and SFTP.\nIt requires no dependency, other than nodetool, to be installed on the Cassandra nodes.</p>\n<h2 id=\"installing-cstar\">Installing cstar</h2>\n<p>You’ll need to have Python 3 and pip3 installed on your server/laptop and then follow the README instructions which will, in the simplest case, boil down to:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>pip3 install cstar\n</pre></div></div>\n<h3 id=\"running-cstar\">Running cstar</h3>\n<p>Cstar is built with Python 3 and offers a straightforward way to run simple commands or complex scripts on an Apache Cassandra cluster using a single contact point.</p>\n<p>The following command, for example, will perform a rolling restart of Cassandra in the cluster, one node at a time using the <code class=\"highlighter-rouge\">one</code> strategy:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>cstar run --command=\"sudo service cassandra restart\" --seed-host=&lt;contact_point_ip&gt; --strategy=one\n</pre></div></div>\n<p>During the execution, cstar will update progress with a clear and pleasant output:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre> +  Done, up      * Executing, up      !  Failed, up      . Waiting, up\n -  Done, down    / Executing, down    X  Failed, down    : Waiting, down\nCluster: Test Cluster\nDC: dc1\n+....\n....\n....\nDC: dc2\n+....\n....\n....\nDC: dc3\n*....\n....\n....\n2 done, 0 failed, 1 executing\n</pre></div></div>\n<p>If we want to perform cleanup with topology awareness and have only one replica at a time, running the command for each token range (leaving a quorum of unaffected replicas at RF=3), we can use the default <code class=\"highlighter-rouge\">topology</code> strategy:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>cstar run --command=\"nodetool cleanup\" --seed-host=&lt;contact_point_ip&gt; --strategy=topology\n</pre></div></div>\n<p>This way, we’ll have several nodes processing the command to minimize the overall time spent on the operation and still ensure low impact on latencies:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre> +  Done, up      * Executing, up      !  Failed, up      . Waiting, up\n -  Done, down    / Executing, down    X  Failed, down    : Waiting, down\nCluster: Test Cluster\nDC: dc1\n****.\n....\n....\nDC: dc2\n++++.\n****\n....\nDC: dc3\n+****\n....\n....\n5 done, 0 failed, 12 executing\n</pre></div></div>\n<p>Finally, if we want to run a command that doesn’t involve pressure on latencies and display the outputs locally, we can use strategy all and add the <code class=\"highlighter-rouge\">-v</code> flag to display the command outputs:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>cstar run --command=\"nodetool getcompactionthroughput\" --seed-host=&lt;contact_point_ip&gt; --strategy=all -v\n</pre></div></div>\n<p>Which will give us the following output:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre> +  Done, up      * Executing, up      !  Failed, up      . Waiting, up\n -  Done, down    / Executing, down    X  Failed, down    : Waiting, down\nCluster: Test Cluster\nDC: dc1\n*****\n****\n****\nDC: dc2\n*****\n****\n****\nDC: dc3\n*****\n****\n****\n0 done, 0 failed, 39 executing\nHost node1.mycompany.com finished successfully\nstdout:\nCurrent compaction throughput: 0 MB/s\nHost node21.mycompany.com finished successfully\nstdout:\nCurrent compaction throughput: 0 MB/s\nHost node10.mycompany.com finished successfully\nstdout:\nCurrent compaction throughput: 0 MB/s\n...\n...\nHost node7.mycompany.com finished successfully\nstdout:\nCurrent compaction throughput: 0 MB/s\nHost node18.mycompany.com finished successfully\nstdout:\nCurrent compaction throughput: 0 MB/s\n +  Done, up      * Executing, up      !  Failed, up      . Waiting, up\n -  Done, down    / Executing, down    X  Failed, down    : Waiting, down\nCluster: Test Cluster\nDC: dc1\n+++++\n++++\n++++\nDC: dc2\n+++++\n++++\n++++\nDC: dc3\n+++++\n++++\n++++\n39 done, 0 failed, 0 executing\nJob cff7f435-1b9a-416f-99e4-7185662b88b2 finished successfully\n</pre></div></div>\n<h2 id=\"how-cstar-does-its-magic\">How cstar does its magic</h2>\n<p>When you run a cstar command it will first connect to the seed node you provided and run a set of nodetool commands through SSH.</p>\n<p>First, nodetool ring will give it the cluster topology with the state of each node. By default, cstar will stop the execution if one node in the cluster is down or unresponsive.\nIf you’re aware that nodes are down and want to run a command nonetheless, you can add the <code class=\"highlighter-rouge\">--ignore-down-nodes</code> flag to bypass the check.</p>\n<p>Then cstar will list the keyspaces using nodetool cfstats and build a map of the replicas for all token ranges for each of them. This will allow it to identify which nodes contain the same token ranges, using nodetool describering, and apply the topology strategy accordingly. As shown before, the topology strategy will not allow two nodes that are replicas for the same token to run the command at the same time. If the cluster does not use vnodes, the topology strategy will run the command every RF node. If the cluster uses vnodes but is not using NetworkTopologyStrategy (NTS) for all keyspaces nor spreading across racks, chances are only one node will be able to run the command at once, even with the topology strategy.If both NTS and racks are in use, the topology strategy will run the command on a whole rack at a time.</p>\n<p>By default, cstar will process the datacenters in parallel, so 2 nodes being replicas for the same tokens but residing in different datacenters can be processed at the same time.</p>\n<p>Once the cluster has been fully mapped execution will start in token order.\nCstar is very resilient because it uploads a script on each remote node through SFTP and runs it using nohup. Each execution will write output (std and err) files along with the exit code for cstar to check on regularly. If the command is interrupted on the server that runs cstar, it can be resumed safely as cstar will first check if the script is still running or has finished already on each node that hasn’t gone through yet.<br />Note that interrupting the command on the cstar host will not stop it on the remote nodes that are already running it.<br />Resuming an interrupted command is done simply by executing : <code class=\"highlighter-rouge\">cstar continue &lt;job_id&gt;</code></p>\n<p>Each time a node finishes running the command cstar will check if the cluster health is still good and if the node is up. This way, if you perform a rolling restart and one of the nodes doesn’t come back up properly, although the exit code of the restart command is <code class=\"highlighter-rouge\">0</code>, cstar will wait indefinitely to protect the availability of the cluster. That is unless you specified a timeout on the job. In such a case, the job will fail.\nOnce the node is up after the command has run, cstar will look for the next candidate node in the ring to run the command.</p>\n<h2 id=\"a-few-handy-flags\">A few handy flags</h2>\n<h3 id=\"two-steps-execution\">Two steps execution</h3>\n<p>Some commands may be scary to run on the whole cluster and you may want to run them on a subset of the nodes first, check that they are in the expected state manually, and then continue the execution on the rest of the cluster.\nThe <code class=\"highlighter-rouge\">--stop-after=&lt;number-of-nodes&gt;</code> flag will do just that. Setting it to <code class=\"highlighter-rouge\">--stop-after=1</code> will run the command on a single node and exit. Once you’ve verified that you’re happy with the execution on that one node you can process the rest of the cluster using <code class=\"highlighter-rouge\">cstar continue &lt;job_id&gt;</code>.</p>\n<h3 id=\"retry-failed-nodes\">Retry failed nodes</h3>\n<p>Some commands might fail mid-course due to transient problems. By default, <code class=\"highlighter-rouge\">cstar continue &lt;job_id&gt;</code> will halt if there is any failed execution in the history of the job. In order to resume the job and retry the execution on the failed nodes, add the <code class=\"highlighter-rouge\">--retry-failed</code> flag.</p>\n<h3 id=\"run-the-command-on-a-specific-datacenter\">Run the command on a specific datacenter</h3>\n<p>To process only a specific datacenter add the <code class=\"highlighter-rouge\">--dc-filter=&lt;datacenter-name&gt;</code> flag. All other datacenters will be ignored by cstar.</p>\n<h3 id=\"datacenter-parallelism\">Datacenter parallelism</h3>\n<p>By default, cstar will process the datacenters in parallel. If you only want only one datacenter to process the command at a time, add the <code class=\"highlighter-rouge\">--dc-serial</code> flag.</p>\n<h3 id=\"specifying-a-maximum-concurrency\">Specifying a maximum concurrency</h3>\n<p>You can forcefully limit the number of nodes running the command at the same time, regardless of topology, by adding the <code class=\"highlighter-rouge\">--max-concurrency=&lt;number-of-nodes&gt;</code> flag.</p>\n<h3 id=\"wait-between-each-node\">Wait between each node</h3>\n<p>You may want to delay executions between nodes in order to give some room for the cluster to recover from the command. The <code class=\"highlighter-rouge\">--node-done-pause-time=&lt;time-in-seconds&gt;</code> flag will allow to specify a pause time that cstar will apply before looking for the next node to run the command on.</p>\n<h3 id=\"run-the-command-regardless-down-nodes\">Run the command regardless down nodes</h3>\n<p>If you want to run a command while nodes are down in the cluster add the <code class=\"highlighter-rouge\">--ignore-down-nodes</code> flag to cstar.</p>\n<h3 id=\"run-on-specific-nodes-only\">Run on specific nodes only</h3>\n<p>If the command is meant to run on some specific nodes only you can use either the <code class=\"highlighter-rouge\">--host</code> or the <code class=\"highlighter-rouge\">--host-file</code> flags.</p>\n<h3 id=\"control-the-verbosity-of-the-output\">Control the verbosity of the output</h3>\n<p>By default, cstar will only display the progress of the execution as shown above in this post. To get the output of the remote commands, add the <code class=\"highlighter-rouge\">-v</code> flag.\nIf you want to get more verbosity on the executions and get debug loggings use either <code class=\"highlighter-rouge\">-vv</code> (very verbose) or <code class=\"highlighter-rouge\">-vvv</code> (extra verbose).</p>\n<h2 id=\"you-havent-installed-it-already\">You haven’t installed it already?</h2>\n<p><a href=\"https://github.com/spotify/cstar\">Cstar</a> is the tool that all Apache Cassandra operators have been waiting for to manage clusters of all sizes. We were happy to collaborate closely with Spotify to help them open source it.\nIt has been built and matured at one of the most smart and successful start-ups in the world and was developed to manage hundreds of clusters of all sizes. It requires no dependency to be installed on the cluster and uses SSH exclusively. Thus, it will comply nicely with any security policy and you should be able to run it within minutes on any cluster of any size.</p>\n<p>We love cstar so much we are already working on integrating it with Reaper as you can see in the following video :</p>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-rYQxGw2Cnk\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>\n<p>We’ve seen in this blog post how to run simple one line commands with cstar, but there is much more than meets the eye. In an upcoming blog post we will introduce complex command scripts that perform operations like upgrading a Cassandra cluster, selectively clearing snapshots, or safely switching compaction strategies in a single cstar invocation.</p>"}}]}},"pageContext":{"alternative_id":12312}}