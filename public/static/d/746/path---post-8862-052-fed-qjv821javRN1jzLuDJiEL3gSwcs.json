{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"How to Setup a Highly Available Multi-AZ Cassandra Cluster on AWS EC2 - High Scalability -","alternative_id":8862,"content":"<div><img src=\"https://c1.staticflickr.com/9/8606/28622928606_10df5eb9e8_o.jpg\" alt=\"\" /></div><p dir=\"ltr\"><em>This is a guest post by Alessandro Pieri, Software Architect at <a href=\"https://getstream.io/?ref=hs\">Stream</a>. Try out this <a href=\"https://getstream.io/get_started/?ref=hs\">5 minute interactive tutorial</a> to learn more about Stream’s API.</em></p><p dir=\"ltr\">Originally built by Facebook in 2009, <a href=\"http://cassandra.apache.org/\">Apache Cassandra</a> is a free and open-source distributed database designed to handle large amounts of data across a large number of servers. At <a href=\"https://getstream.io/?ref=hs\">Stream</a>, we use Cassandra as the primary data store for our feeds. Cassandra stands out because it’s able to:</p><ul><li dir=\"ltr\">\n<p dir=\"ltr\">Shard data automatically</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\">Handle partial outages without data loss or downtime</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\">Scales close to linearly</p>\n</li>\n</ul><p dir=\"ltr\">If you’re already using Cassandra, your cluster is likely configured to handle the loss of 1 or 2 nodes. However, <strong>what happens when a full availability zone goes down</strong>?</p><p dir=\"ltr\">In this article <strong>you will</strong> <strong>learn how to setup Cassandra to survive a full availability zone outage</strong>. Afterwards, we will analyze how moving from a single to a multi availability zone cluster impacts availability, cost, and performance.</p><h2 dir=\"ltr\">Recap 1: What Are Availability Zones?</h2><p dir=\"ltr\">AWS operates off of geographically isolated locations called regions. Each region is composed of a small amount (usually 3 or 4) physically independent availability zones. Availability zones are connected with a low latency network, while regions are completely independent of each other, as shown in the diagram below:</p><p dir=\"ltr\"><img src=\"https://lh5.googleusercontent.com/taKgfxUy-J49NVs4lVTAQKc3o8DNj17r3ZJMniJh2dAD593cjXMiCH32LZbHtW5o6Vh0fmfD58cHxe-4B8kBbeNc0mJOwooUd2CF1Enem4KMl_sRZ2-1Xp0y411s8aeElLeRKvY\" alt=\"\" width=\"504\" height=\"267\" /></p><p dir=\"ltr\">In order to achieve high availability, AWS resources should be hosted in multiple availability zones. Hosting in multiple availability zones allows you to ensure that if one goes down, your app will stay up and running.</p><h2 dir=\"ltr\">Recap 2: Cassandra and High Availability</h2><p dir=\"ltr\">One of the primary benefits of Cassandra is that it automatically shards your data across multiple nodes. It even manages to scale almost linearly, so doubling the number of nodes give you nearly double the capacity.</p><p dir=\"ltr\"><img src=\"https://lh3.googleusercontent.com/vaSRqgcXK82r2gcFlbHhlWYXghP0iSyEdWWaS106h55FxUhRBdalycwbxsIZ_D5r79fr4MtyGcx987Mfuk-eRzRr00yQM6lPiFaglR9VhlBeCO7FcTl0DCt7yPSBwgZMZbQ1OBI\" alt=\"\" width=\"624\" height=\"205\" /></p><p dir=\"ltr\">Cassandra has a setting called “<a href=\"https://docs.datastax.com/en/cassandra/3.x/cassandra/architecture/archDataDistributeReplication.html\">replication factor</a>” that defines how many copies of your data should exist. If your replication factor is set to 1 and a node goes down, you will lose your data because it was only stored in 1 place. A replication factor of 3 will insure that your data is always stored on 3 different nodes, ensuring that your data is safe when a single node breaks down.</p><h2 dir=\"ltr\">Configuring Cassandra for multi AZ availability</h2><p dir=\"ltr\">Now that we’ve covered the basics, let’s explain how to setup Cassandra for multi-AZ availability.</p><p dir=\"ltr\">If you’re new to Cassandra and want to learn how to setup your own cluster, <a href=\"https://www.digitalocean.com/community/tutorials/how-to-run-a-multi-node-cluster-database-with-cassandra-on-ubuntu-14-04\">this article</a> is a good starting point. </p><h2>Part 1 - The Snitch</h2><p dir=\"ltr\">As a first step we have to make sure Cassandra knows which region and availability zone it’s in. This is handled by the “snitch”, which keeps track of the information related to the network topology. Cassandra provides several built-in snitches. The Ec2Snitch and Ec2MultiRegionSnitch work well for AWS. The Ec2Snitch is meant for a single region deployment, and the Ec2MultiRegionSnitch is meant for clusters that span multiple regions.</p><p dir=\"ltr\">Cassandra understands the concept of a data center and a rack. The EC2 snitches treat each EC2 region as a data center and the availability zone as the rack.</p><p dir=\"ltr\">You can change the Snitch setting in cassandra.yaml. Beware that changing the Snitch setting is a potentially destructive operations and should be planned with care. Read the <a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/operations/opsSwitchSnitch.html\">Cassandra documentation about changing the Snitch setting</a>.</p><table><colgroup><col width=\"*\" /></colgroup><tbody><tr><td><p dir=\"ltr\"># IF YOU CHANGE THE SNITCH AFTER DATA IS INSERTED INTO THE CLUSTER,</p>\n<p dir=\"ltr\"># YOU MUST RUN A FULL REPAIR, SINCE THE SNITCH AFFECTS WHERE REPLICAS</p>\n<p dir=\"ltr\"># ARE PLACED.</p>\n<p dir=\"ltr\">#</p>\n<p dir=\"ltr\"># Out of the box, Cassandra provides</p>\n<p dir=\"ltr\">...</p>\n<p dir=\"ltr\">#  - Ec2Snitch:</p>\n<p dir=\"ltr\">#    Appropriate for EC2 deployments in a single Region.  Loads Region</p>\n<p dir=\"ltr\">#    and Availability Zone information from the EC2 API. The Region is</p>\n<p dir=\"ltr\">#    treated as the Datacenter, and the Availability Zone as the rack.</p>\n<p dir=\"ltr\">#    Only private IPs are used, so this will not work across multiple</p>\n<p dir=\"ltr\">#    Regions.</p>\n<p dir=\"ltr\">#  - Ec2MultiRegionSnitch:</p>\n<p dir=\"ltr\">#    Uses public IPs as broadcast_address to allow cross-region</p>\n<p dir=\"ltr\">#    connectivity.  (Thus, you should set seed addresses to the public</p>\n<p dir=\"ltr\">#    IP as well.) You will need to open the storage_port or</p>\n<p dir=\"ltr\">#    ssl_storage_port on the public IP firewall.  (For intra-Region</p>\n<p dir=\"ltr\">#    traffic, Cassandra will switch to the private IP after</p>\n<p dir=\"ltr\">#    establishing a connection.)</p>\n<p dir=\"ltr\">#</p>\n<p dir=\"ltr\"># You can use a custom Snitch by setting this to the full class name</p>\n<p dir=\"ltr\"># of the snitch, which will be assumed to be on your classpath.</p>\n<p dir=\"ltr\">endpoint_snitch: Ec2Snitch </p>\n</td>\n</tr></tbody></table><p dir=\"ltr\"><em>The above is a snippet from cassandra.yaml</em></p><h2>Part 2 - The Replication Factor</h2><p dir=\"ltr\">The replication factor determines the number of replicas that should exist in the cluster. Replication strategy, also known as replica placement strategy, determines how replicas are distributed across the cluster. Both settings are keyspace properties.</p><p dir=\"ltr\">By default Cassandra uses the “SimpleStrategy” replication strategy. This strategy places replicas in the cluster ignoring which region or availability zone it’s in. The <a href=\"https://docs.datastax.com/en/cassandra/2.0/cassandra/architecture/architectureDataDistributeReplication_c.html\">NetworkTopologyStrategy</a> is rack aware and is designed to support multi-datacenter deployments.</p><table><colgroup><col width=\"*\" /></colgroup><tbody><tr><td><p dir=\"ltr\">CREATE KEYSPACE mykeyspace WITH replication = {</p>\n<p dir=\"ltr\">  'class': 'NetworkTopologyStrategy',</p>\n<p dir=\"ltr\">  'us-east': '3'</p>\n<p dir=\"ltr\">};</p>\n</td>\n</tr></tbody></table><p dir=\"ltr\">In the above code snippet we’ve declared a keyspace called “mykeyspace” with a NetworkReplicationStrategy which will place the replicas in the “us-east” datacenter only, with a replication factor of 3.</p><p dir=\"ltr\">To change an existing keyspace you can use the example below. Beware that changing the replication strategy of a running Cassandra’s cluster is a sensitive operation. <a href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_using/useUpdateKeyspaceRF.html\">Read the full documentation</a>.</p><table><colgroup><col width=\"*\" /></colgroup><tbody><tr><td><p dir=\"ltr\">ALTER KEYSPACE mykeyspace WITH REPLICATION = { </p>\n<p dir=\"ltr\">   'class' : 'NetworkTopologyStrategy', </p>\n<p dir=\"ltr\">   'us-east' : '3' </p>\n<p dir=\"ltr\">};</p>\n<br /></td>\n</tr></tbody></table><h2>Part 3 - Consistency levels</h2><p dir=\"ltr\">When you read or write from Cassandra, you have the ability to specify the “<a href=\"https://docs.datastax.com/en/cassandra/2.0/cassandra/dml/dml_config_consistency_c.html\">consistency level</a>” on the client-side. In other words, you can specify how many nodes in the Cassandra cluster are required to agree before a read or write request is valid.</p><p dir=\"ltr\">If you ask for a higher consistency level than Cassandra is able to answer with nodes in the local availability zone, it will query the other zones. To stay up during an availability zone outage, you need to use a consistency level that the remaining nodes are able to satisfy. The next section will discuss failure scenarios and consistency levels in more detail.</p><h2>Handling AZ(s) Outages</h2><p dir=\"ltr\">How a Cassandra cluster behaves when an availability zone goes down depends on several factors:</p><ul><li dir=\"ltr\">\n<p dir=\"ltr\">Scale of the failure (how many AZs are down)</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\">Number of AZs used by the cluster</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\">Replication factor</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\">Consistency level</p>\n</li>\n</ul><p dir=\"ltr\">Let’s have a look at the diagram below, which shows a couple of scenarios:</p><p dir=\"ltr\"><img src=\"https://lh5.googleusercontent.com/64YpnsN_mfrVsRDrL9ke12vVvX7RxdjMATjYnhuxy_6U7hYLyyN-v3X-alyo6NOCuIUiG8xg6Ewf0FLYJrTupY9X9KXLH2aWGwA_Kukaa6ffgxudVJhjv2RgUu680me92mZAYh8\" alt=\"\" width=\"624\" height=\"355\" /></p><p dir=\"ltr\"><em>Figure 2. How consistency level affects availability</em></p><p dir=\"ltr\">In the first scenario shown on the left we show a <strong>cluster running on 2 AZs with 6 nodes (3 per AZ) and a RF=2</strong>. When 1 AZ goes down, half of our cluster will be offline. With 2 AZs and a RF=2, we will have the guarantee that our entire dataset is still present on at least 1 node. As you can see in the table next to the cluster diagram, the outcome of a query depends on the requested consistency level. For example, a query with CL=ONE will succeed because we still have at least 1 node available. On the other hand, queries with higher CL requirements such as. QUORUM and ALL will always fail because they both require responses from 2 nodes.</p><p dir=\"ltr\">In the second scenario, <strong>we run Cassandra with 9 nodes on 3 different AZs and a replica factor of 3</strong>. With this deployment, our cluster is clearly more resilient in the event of 1 AZ failure. Cassandra will still be able to satisfy queries with CL=QUORUM.</p><p dir=\"ltr\">It is worth noting that in the event of an availability zone outage, the capacity left in service for the 2 clusters is different. With the first cluster setup you lose 50% of the capacity, while <strong>the second setup only affects 33% of the capacity</strong>.</p><h2>How Much Latency Is Introduced by a Multi-AZs Setup?</h2><p dir=\"ltr\">Estimating query latency introduced by the multi-AZ setup is not easy due to the nature of Cassandra and the number of factors that fluctuate in a cloud environment (e.g. network latency, disk I/O, host utilization, etc.).</p><p dir=\"ltr\">For our tests we used the <a href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsCStress_t.html\">cassandra-stress</a> tool to generate read and write load on clusters running on single and multiple AZs. In order to keep the variance as low as possible, and to lower the deviation on disk I/O, we used instances with ephemeral storage instead of network attached storage (EBS).</p><p dir=\"ltr\">We then came up with <strong>two test scenarios</strong>:</p><p dir=\"ltr\">The first used a cluster of 6 i2.xlarge instances (AWS network performance = “moderate”) and was running without enhanced networking:</p><table><colgroup><col width=\"324\" /><col width=\"151\" /><col width=\"149\" /></colgroup><tbody><tr><td><br /></td>\n<td>\n<p dir=\"ltr\">Median</p>\n</td>\n<td>\n<p dir=\"ltr\">95th percentile</p>\n</td>\n</tr><tr><td colspan=\"3\">\n<p dir=\"ltr\">WRITE</p>\n</td>\n</tr><tr><td>\n<p dir=\"ltr\">Single AZ</p>\n</td>\n<td>\n<p dir=\"ltr\">1.0</p>\n</td>\n<td>\n<p dir=\"ltr\">2.5</p>\n</td>\n</tr><tr><td>\n<p dir=\"ltr\">Multi AZ (3 AZs)</p>\n</td>\n<td>\n<p dir=\"ltr\">1.5</p>\n</td>\n<td>\n<p dir=\"ltr\">2.8</p>\n</td>\n</tr><tr><td colspan=\"3\">\n<p dir=\"ltr\">READ</p>\n</td>\n</tr><tr><td>\n<p dir=\"ltr\">Single AZ</p>\n</td>\n<td>\n<p dir=\"ltr\">1.0</p>\n</td>\n<td>\n<p dir=\"ltr\">2.6</p>\n</td>\n</tr><tr><td>\n<p dir=\"ltr\">Multi AZ (3 AZs)</p>\n</td>\n<td>\n<p dir=\"ltr\">1.5</p>\n</td>\n<td>\n<p dir=\"ltr\">23.5</p>\n</td>\n</tr></tbody></table><p dir=\"ltr\"><em>Table 1. Scenario 1: performance test Single AZ vs Multi AZ (time in milliseconds). Setup: Cassandra 2.0.15; RF=3; CL=1</em></p><p dir=\"ltr\"> </p><p dir=\"ltr\">The second scenario used a cluster of 6 i2.2xlarge (AWS network performance = “high”), with enhanced networking turned on:</p><table><colgroup><col width=\"321\" /><col width=\"153\" /><col width=\"150\" /></colgroup><tbody><tr><td><br /></td>\n<td>\n<p dir=\"ltr\">Median</p>\n</td>\n<td>\n<p dir=\"ltr\">95th percentile</p>\n</td>\n</tr><tr><td colspan=\"3\">\n<p dir=\"ltr\">WRITE</p>\n</td>\n</tr><tr><td>\n<p dir=\"ltr\">Single AZ</p>\n</td>\n<td>\n<p dir=\"ltr\">0.9</p>\n</td>\n<td>\n<p dir=\"ltr\">2.4</p>\n</td>\n</tr><tr><td>\n<p dir=\"ltr\">Multi AZ (3 AZs)</p>\n</td>\n<td>\n<p dir=\"ltr\">1.1</p>\n</td>\n<td>\n<p dir=\"ltr\">2.3</p>\n</td>\n</tr><tr><td colspan=\"3\">\n<p dir=\"ltr\">READ</p>\n</td>\n</tr><tr><td>\n<p dir=\"ltr\">Single AZ</p>\n</td>\n<td>\n<p dir=\"ltr\">0.7</p>\n</td>\n<td>\n<p dir=\"ltr\">1.5</p>\n</td>\n</tr><tr><td>\n<p dir=\"ltr\">Multi AZ (3 AZs)</p>\n</td>\n<td>\n<p dir=\"ltr\">1.0</p>\n</td>\n<td>\n<p dir=\"ltr\">1.9</p>\n</td>\n</tr></tbody></table><p dir=\"ltr\"><em>Table 2. Scenario 2: performance test Single AZ vs Multi AZ (time in milliseconds). Setup: Cassandra 2.0.15; RF=3; CL=1</em></p><p dir=\"ltr\"> </p><p dir=\"ltr\">Interesting enough, networking performance varies between the two instance types. <strong>When using i2.2xlarge with enhanced networking enabled, we saw very little difference between single AZ and multi AZ deployments</strong>. Therefor we recommend enabling enhanced networking and selecting an instance types with “high” network performance.</p><p dir=\"ltr\">Another interesting fact is that Cassandra reads are – to a certain extent – rack-aware. When coordinating a query, Cassandra nodes will route the request to the peer with lowest latency. This feature is called “<a href=\"http://www.datastax.com/dev/blog/dynamic-snitching-in-cassandra-past-present-and-future\">dynamic snitching</a>” and has been part of Cassandra since version 0.6.5.</p><p dir=\"ltr\">Thanks to <strong>dynamic snitching, most of the read queries to Cassandra do not “hit” nodes on different availability zones</strong> and give Cassandra some sort of rack-awareness. We could reproduce this behavior on our read tests as shown in the following chart:</p><p dir=\"ltr\"> <img src=\"https://lh6.googleusercontent.com/nfXiH0qO3JJNb6MjfQlqnvudZqStqgACx76nkLaELKpdZILASz02YEKlXB9uTzh3WjKYEVHUdv832Oz8B24qM0TFD6Yfvm3K2P95qjHmWwsclyzWJngxOBkn06mjm6A0VOgQKVQ\" alt=\"\" width=\"538\" height=\"409\" /></p><p dir=\"ltr\"><em>Figure 1. Number of local read-requests per node on multi-AZ setup. Replicas in the same AZ are preferred. Set up: 6 nodes cluster spanning across 3 Azs. Read are performed with Consistency Level=ONE</em></p><p dir=\"ltr\"> </p><p dir=\"ltr\">Figure 1 shows how 10M read requests are distributed across the cluster. As you can see, <strong>most requests are handled within the local availability zone</strong>.</p><p dir=\"ltr\">About enhanced networking: AWS offers enhanced networking on their most recent instance families. <strong>Using enhanced networking results in consistently lower inter-instance latency</strong>. For more information about this topic, please follow <a href=\"http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html\">this link</a>.</p><h2>Guidelines for Deciding the Number of Availability Zones to Use</h2><p dir=\"ltr\">Cassandra can be configured in such a way that every availability zone has at least 1 entire copy of the dataset. Cassandra refers to this scenario as making an AZ self-contained. To achieve this, you need to place your nodes across a number of AZs that is less or equal your replication factor. It is also recommended to have the same number of nodes running on every AZ.</p><p dir=\"ltr\">In general it is beneficial to have:</p><p dir=\"ltr\">Availability Zones &lt;= Replication Factor</p><p dir=\"ltr\">At <a href=\"https://getstream.io/\">Stream</a> <strong>we’ve chosen to use a replication factor of 3 with 3 different availability zones</strong>. This ensures that every availability zone has a copy of the data, and that we have enough capacity left to handle read and write requests in the unlikely event of an AZ outage.</p><h2 dir=\"ltr\">Conclusion</h2><p dir=\"ltr\">Cassandra is an amazing database. At Stream we rely heavily on it to keep the feeds running for tens of millions of end users. In short, we do so because Cassandra has the ability to:</p><ul><li dir=\"ltr\">\n<p dir=\"ltr\">Shard data automatically</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\">Handle instance failures without data loss or downtime</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\">Scale (almost) linearly</p>\n</li>\n</ul><p dir=\"ltr\">In this post we’ve explained how to configure Cassandra in a highly available Multi-AZ setup on AWS EC2. The costs and performance are almost identical to a single availability zone deployment. A few key takeaways:</p><ul><li dir=\"ltr\">\n<p dir=\"ltr\"><strong>Placing nodes across multiple availability zones makes your Cassandra cluster more available and resilient to availability zone outages</strong>.</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><strong>No additional storage is needed to run on multiple AZs and the cost increase is minimal</strong>. Traffic between AZs isn’t free but for most use cases this isn’t a major concern.</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><strong>A replication factor of 3 combined with using 3 availability zones is a good starting point for most use cases</strong>. This enables your Cassandra cluster to be self-contained.</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><strong>AWS has done a great job at keeping the latency between availability zones low</strong>. Especially if you use an instance with network performance set to “high” and have enhanced networking enabled. </p>\n</li>\n</ul>"}}]}},"pageContext":{"alternative_id":8862}}