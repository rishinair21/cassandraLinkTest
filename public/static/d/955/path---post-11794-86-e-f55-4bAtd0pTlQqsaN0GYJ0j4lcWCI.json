{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Yannael/kafka-sparkstreaming-cassandra","alternative_id":11794,"content":"<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>This Dockerfile sets up a complete streaming environment for experimenting with Kafka, Spark streaming (PySpark), and Cassandra. It installs</p>\n<ul><li>Kafka 0.10.2.1</li>\n<li>Spark 2.1.1 for Scala 2.11</li>\n<li>Cassandra 3.7</li>\n</ul><p>It additionnally installs</p>\n<ul><li>Anaconda distribution 4.4.0 for Python 2.7.10</li>\n<li>Jupyter notebook for Python</li>\n</ul>\n<p>Run container using <a href=\"https://hub.docker.com/r/yannael/kafka-sparkstreaming-cassandra\" rel=\"nofollow\">DockerHub image</a></p>\n<pre>docker run -p 4040:4040 -p 8888:8888 -p 23:22 -ti --privileged yannael/kafka-sparkstreaming-cassandra\n</pre>\n<p>See following video for usage demo.\n<br /><a href=\"https://www.youtube.com/watch?v=XxCFo7BzNQ8\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/61da6654e141eb080dce4055dbd84d7c47231037/687474703a2f2f696d672e796f75747562652e636f6d2f76692f587843466f37427a4e51382f302e6a7067\" alt=\"Demo\" width=\"480\" height=\"360\" border=\"10\" data-canonical-src=\"http://img.youtube.com/vi/XxCFo7BzNQ8/0.jpg\" style=\"text-align: center;\" /></a></p>\n<p>Note that any changes you make in the notebook will be lost once you exit de container. In order to keep the changes, it is necessary put your notebooks in a folder on your host, that you share with the container, using for example</p>\n<pre>docker run -v `pwd`:/home/guest/host -p 4040:4040 -p 8888:8888 -p 23:22 -ti --privileged yannael/kafka-sparkstreaming-cassandra\n</pre>\n<p>Note:</p>\n<ul><li>The \"-v <code>pwd</code>:/home/guest/host\" shares the local folder (i.e. folder containing Dockerfile, ipynb files, etc...) on your computer - the 'host') with the container in the '/home/guest/host' folder.</li>\n<li>Port are shared as follows:\n<ul><li>4040 bridges to Spark UI</li>\n<li>8888 bridges to the Jupyter Notebook</li>\n<li>23 bridges to SSH</li>\n</ul></li>\n</ul><p>SSH allows to get a onnection to the container</p>\n<pre>ssh -p 23 guest@containerIP\n</pre>\n<p>where 'containerIP' is the IP of th container (127.0.0.1 on Linux). Password is 'guest'.</p>\n<h3><a id=\"user-content-start-services\" class=\"anchor\" aria-hidden=\"true\" href=\"#start-services\"></a>Start services</h3>\n<p>Once run, you are logged in as root in the container. Run the startup_script.sh (in /usr/bin) to start</p>\n<ul><li>SSH server. You can connect to the container using user 'guest' and password 'guest'</li>\n<li>Cassandra</li>\n<li>Zookeeper server</li>\n<li>Kafka server</li>\n</ul><pre>startup_script.sh\n</pre>\n<h3><a id=\"user-content-connect-create-cassandra-table-open-notebook-and-start-streaming\" class=\"anchor\" aria-hidden=\"true\" href=\"#connect-create-cassandra-table-open-notebook-and-start-streaming\"></a>Connect, create Cassandra table, open notebook and start streaming</h3>\n<p>Connect as user 'guest' and go to 'host' folder (shared with the host)</p>\n<pre>su guest\n</pre>\n<p>Start Jupyter notebook</p>\n<pre>notebook\n</pre>\n<p>and connect from your browser at port host:8888 (where 'host' is the IP for your host. If run locally on your computer, this should be 127.0.0.1 or 192.168.99.100, check Docker documentation)</p>\n<h4><a id=\"user-content-start-kafka-producer\" class=\"anchor\" aria-hidden=\"true\" href=\"#start-kafka-producer\"></a>Start Kafka producer</h4>\n<p>Open kafkaSendDataPy.ipynb and run all cells.</p>\n<h4><a id=\"user-content-start-kafka-receiver\" class=\"anchor\" aria-hidden=\"true\" href=\"#start-kafka-receiver\"></a>Start Kafka receiver</h4>\n<p>Open kafkaReceiveAndSaveToCassandraPy.ipynb and run cells up to start streaming. Check in subsequent cells that Cassandra collects data properly.</p>\n<h4><a id=\"user-content-connect-to-spark-ui\" class=\"anchor\" aria-hidden=\"true\" href=\"#connect-to-spark-ui\"></a>Connect to Spark UI</h4>\n<p>It is available in your browser at port 4040</p>\n<p>The container is based on CentOS 6 Linux distribution. The main steps of the building process are</p>\n<ul><li>Install some common Linux tools (wget, unzip, tar, ssh tools, ...), and Java (1.8)</li>\n<li>Create a guest user (UID important for sharing folders with host!, see below), and install Spark and sbt, Kafka, Anaconda and Jupyter notbooks for the guest user</li>\n<li>Go back to root user, and install startup script (for starting SSH and Cassandra services), sentenv.sh script to set up environment variables (JAVA, Kafka, Spark, ...), spark-default.conf, and Cassandra</li>\n</ul><h3><a id=\"user-content-user-uid\" class=\"anchor\" aria-hidden=\"true\" href=\"#user-uid\"></a>User UID</h3>\n<p>In the Dockerfile, the line</p>\n<pre>RUN useradd guest -u 1000\n</pre>\n<p>creates the user under which the container will be run as a guest user. The username is 'guest', with password 'guest', and the '-u' parameter sets the linux UID for that user.</p>\n<p>In order to make sharing of folders easier between the container and your host, <strong>make sure this UID matches your user UID on the host</strong>. You can see what your host UID is with</p>\n<pre>echo $UID\n</pre>\n<h3><a id=\"user-content-clone-this-repository\" class=\"anchor\" aria-hidden=\"true\" href=\"#clone-this-repository\"></a>Clone this repository</h3>\n<pre>git clone https://github.com/Yannael/kafka-sparkstreaming-cassandra\n</pre>\n<h3><a id=\"user-content-build\" class=\"anchor\" aria-hidden=\"true\" href=\"#build\"></a>Build</h3>\n<p>From Dockerfile folder, run</p>\n<pre>docker build -t kafka-sparkstreaming-cassandra .\n</pre>\n<p>It may take about 30 minutes to complete.</p>\n<h3><a id=\"user-content-run\" class=\"anchor\" aria-hidden=\"true\" href=\"#run\"></a>Run</h3>\n<pre>docker run -v `pwd`:/home/guest/host -p 4040:4040 -p 8888:8888 -p 23:22 -ti --privileged kafka-sparkstreaming-cassandra\n</pre>\n</article>"}}]}},"pageContext":{"alternative_id":11794}}