{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Planet Cassandra","alternative_id":1663,"content":"<div class=\"post\" readability=\"34\">\n                        \n                        \n                        <div readability=\"14\">\n<p>Sorry about missing last week, but my birthday won out over\nworking: </p>\n<div align=\"center\" readability=\"6\"><img src=\"https://c1.staticflickr.com/5/4374/36013670130_91760fec5e.jpg\" alt=\"\"/>\n<p>Ouch! <a href=\"https://twitter.com/john_overholt/status/881951639904690176\">@john_overholt</a>:\nMy actual life is now a science exhibit about the primitive\nconditions of the past.</p>\n</div>\n<p>If you like this sort\nof <em>Stuff</em> then please support\nme <a href=\"https://www.patreon.com/toddhoff\">on\nPatreon</a>.</p>\n\n<ul><li><a href=\"https://www.pcper.com/news/Storage/FMS-2017-Intels-EDSFF-Ruler-SSD-Form-Factor-Details-Emerge-1-Petabyte-1U-Chassis\">\n1PB</a>: SSD in 1U chassis; <a href=\"https://www.youtube.com/watch?v=yLpDczIRJSQ\">90%</a>: savings\nusing EC2 Spot for containers; <a href=\"https://twitter.com/swardley/status/898085035072466944\">16</a>:\nforms of inertia; <a href=\"https://techcrunch.com/2017/08/17/alibaba-profit-doubles-to-2-1b/\">\n$2.1B</a>: Alibaba’s profit; <a href=\"https://dazeinfo.com/2017/08/15/app-downloads-worldwideq-app-store-google-play-q2-2017/\">\n22.6B</a>: app downloads in Q2; <a href=\"https://thestack.com/cloud/2017/04/05/google-unveils-espresso-for-the-peering-edge/\">\n25%</a>: Google generated internet traffic; <a href=\"http://spectrum.ieee.org/nanoclast/computing/hardware/a-true-random-number-generator-built-from-carbon-nanotubes-promises-better-security-for-flexible-electronics\">20\nby 20 micrometers</a>: quantum random number generators;\n<a href=\"https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv\">\n16</a>: lectures on Convolutional Neural Networks for Visual\nRecognition; <a href=\"https://archive.org/details/georgeblood\">25,000</a>: digitized\ngramophone records; <a href=\"http://highscalability.com/blog/a\">280%</a>: increase in IoT\nattacks; <a href=\"https://twitter.com/billmckibben/status/894542512580423680\">6.5%</a>:\nworld's GDP goes to subsidizing fossil fuel; <a href=\"http://www.jonkensy.com/832-tb-zfs-on-linux-project-cheap-and-deep-part-1/\">832\nTB</a>: ZFS on Linux;  <a href=\"https://www.wired.com/story/meet-alex-the-russian-casino-hacker-who-makes-millions-targeting-slot-machines\">$250,000</a>:\nweekly take from breaking slot machines; <a href=\"http://surprizingfacts.com/advanced-civilizations-can-build-a-galactic-internet-using-the-passage-of-planets-translation/\">\n30</a>: galatic message exchanges using artificial megastructures\nin 100,000 years; </li>\n<li><br/></li>\n<li>Quotable Quotes:\n<ul readability=\"181\"><li readability=\"4\">\n<p><a href=\"https://twitter.com/chris__martin/status/893951453269766144\">@chris__martin</a>:\nALIENS: we bring you a gift of reliable computing technol--</p>\n<p>HUMANS: oh no we have that already but JS is easier to hire\nfor</p>\n</li>\n<li><a href=\"https://twitter.com/rakyll/status/895132157630226432\">@rakyll</a>: \"You\nwoman, you like intern.\" I interned on F-16's flight computer. Even\nmy internship was 100x more legit than any job you will have.</li>\n<li><a href=\"https://twitter.com/codewisdom/status/897911593878511617\">@CodeWisdom</a>:\n\"Debugging is like being the detective in a crime movie where you\nare also the murderer.\" - Filipe Fortes</li>\n<li><a href=\"http://www.vulture.com/2017/08/william-gibson-archangel-apocalypses-dystopias.html\">\nWilliam Gibson</a>: what I find far more ominous is how seldom,\ntoday, we see the phrase “the 22nd century.” Almost never. Compare\nthis with the frequency with which the 21st century was evoked in\npopular culture during, say, the 1920s.</li>\n<li><a href=\"https://www.arador.com/ridiculous-bandwidth-costs-amazon-google-microsoft/\">\nArador</a>: Amazon EC2, Microsoft Azure and Google Gloud Platform\nare all seriously screwing their customers over when it comes to\nbandwidth charges. Every one of the big three has massive\nbuying power yet between them their average bandwidth price is 3.4x\nhigher than colocation facilities.</li>\n<li><a href=\"https://twitter.com/mattklein123/status/896586512757805058\">@mattklein123</a>:\nGood thread: my view: 1) most infra startups will fail. It's an\nawful business to be in (sorry all, much ❤️).</li>\n<li><a href=\"https://mondaynote.com/misunderstanding-apple-services-58b1aa248590\">\nJean-Louis Gassée</a>: With Services, Apple enjoys the\nbenefits of a virtuous circle: Hardware sales create Services\nrevenue opportunities; Services makes hardware more attractive and\n“stickier”. Like Apple Stores, Services are part of the ecosystem.\nSuch is the satisfying simplicity and robustness of Apple’s\nbusiness model.</li>\n<li><a href=\"https://news.ycombinator.com/item?id=14929128\">cardine</a>: The\nprice difference between Hetzner and AWS is large enough that it\ncould pay for 4x as much computational power (as much redundancy as\nyou'd ever need), three full time system admins (not that you'd\never need them), and our office lease... with plenty of money left\nover!</li>\n<li><a href=\"https://hackernoon.com/moving-on-9f1bb1e42331\">Brujo\nBenavides</a>: Communication is Key: Have you ever watched a movie\nor a soap opera and thought “If you would’ve just told her that, we\nwould’ve avoided 3 entire episodes, you moron!”. Happens to me all\nthe time. At Inaka we learned that the hard way.</li>\n<li><a href=\"https://twitter.com/f3ew/status/897245414537256961\">@f3ew</a>:\nDoesn't matter how many layers of stateless services you have in\nthe middle, the interesting ends have state.</li>\n<li><a href=\"https://news.ycombinator.com/item?id=14928750\">brianwawok</a>: My\ncloud cost is less than 5% of my bussiness costs using GCE. Would\nbe foolish to move it to lower my costs 2%.</li>\n<li readability=\"6\">\n<p><a href=\"https://twitter.com/f3ew/status/897245609400442880\">@f3ew</a>: Stateless\nservices are as relevant as routers. Pattern match, compute, push\nto next layer.</p>\n</li>\n<li readability=\"7\">\n<p><a href=\"http://5by5.tv/asymcar/40\">Horace Dediu</a>~ when you\noutsource you're taking knowledge out of your company, which ends\nup gutting it in terms of the value that is added </p>\n</li>\n<li readability=\"3\">\n<div readability=\"12\">\n<p><a href=\"https://www.amazon.com/dp/B01M9C1Y3S\">Jason\nCalacanis</a>: Google was the twelfth search engine. Facebook was\nthe tenth social network. iPad was the twentieth tablet. It’s not\nwho gets there first. It’s who gets there first when the market’s\nready.</p>\n</div>\n</li>\n<li readability=\"14\">\n<p><a href=\"https://news.ycombinator.com/item?id=14039755\">puzzle</a>: The\nB4 paper states multiple times that Google runs links at almost\n100% saturation, versus the standard 30-40%. That's accomplished\nthrough the use of SDN technology and, even before that, through\nstrict application of QoS.</p>\n</li>\n<li readability=\"4\">\n<p><a href=\"https://siliconangle.com/blog/2017/08/14/serverless-buzz-has-eclipsed-containers-say-analysts-awssummit/\">\n@stu</a>: Serverless has in many ways eclipsed the containers\ndiscussion for the hot buzz in the industry</p>\n</li>\n<li readability=\"4\">\n<p><a href=\"https://twitter.com/mjpt777/status/894540931042553856\">@mjpt777</a>: GC\nis a wonderful thing but I cannot help but feel it leaves the\ntypical developer even less prepared for distributed resource\nmanagement.</p>\n</li>\n<li readability=\"24\">\n<p><a href=\"https://news.ycombinator.com/item?id=14987617\">joaodlf</a>: Spark\nworks and does a good job, it has many features that I can see us\nuse in the future too. With that said, it's yet another piece of\ntech that bloats our stack. I would love to reduce our tech debt:\nWe are much more familiar with relational databases like MySQL and\nPostgres, but we fear they won't answer the analytics problems we\nhave, hence Cassandra and Spark. We use these technologies out of\nnecessity, not love for them.</p>\n</li>\n<li readability=\"16\">\n<p><a href=\"https://news.ycombinator.com/item?id=14983047\">tobyjsullivan</a>: No,\ndear author. Setting up the AWS billing alarm was the smartest\nthing you ever did. It probably saved you tens of thousands of\ndollars (or at least the headache associated with fighting Amazon\nover the bill). Developers make mistakes. It's part of the\njob. It's not unusual or bad in any way. A bad developer is one who\ndenies that fact and fails to prepare for it. A great developer is\none like the author.</p>\n</li>\n<li readability=\"9\">\n<p><a href=\"http://woz.posthaven.com/what-orms-have-taught-me-just-learn-sql\">Geoff\nWozniak</a>: Regardless of whether I find that stored procedures\naren't actually that evil or whether I keep using templated SQL, I\ndo know one thing: I won't fall into the \"ORMs make it easy\"\ntrap.</p>\n</li>\n<li readability=\"5\">\n<p><a href=\"https://twitter.com/BenedictEvans/status/893870187090386945\">@BenedictEvans</a>: Part\nof what distinguishes today’s big tech companies is a continual\npush against complacency. They saw the last 20 years and read the\nbooks</p>\n</li>\n<li readability=\"27\">\n<p><a href=\"http://fortune.com/2017/08/03/game-of-thrones-sophie-turner-role-social-media/\">\nJohn Patrick Pullen</a>: In the upcoming fall issue of Porter\nmagazine, the 21-yer-old X-Men: Apocalypse star said, \"I auditioned\nfor a project and it was between me and another girl who is a far\nbetter actress than I am, far better, but I had the followers, so I\ngot the job,\" according to The Telegraph. \"It’s not right, but it\nis part of the movie industry now.\"</p>\n</li>\n<li readability=\"21\">\n<p><a href=\"http://www.slate.com/articles/technology/future_tense/2017/08/the_19th_century_moral_panic_over_paper_technology.html\">\nRachel Adler</a>: Naturally, faster prints drove up demand for\npaper, and soon traditional methods of paper production couldn’t\nkeep up. The paper machine, invented in France in 1799 at the Didot\nfamily’s paper mill, could make 40 times as much paper per day as\nthe traditional method, which involved pounding rags into pulp by\nhand using a mortar and pestle.</p>\n</li>\n<li readability=\"7\">\n<p><a href=\"https://news.ycombinator.com/item?id=14990373\">pawelkomarnicki</a>:\nAs a person that can get the product from scratch to production and\nscale it, I can say I'm a full-stack developer. Can I feel mythical\nnow?</p>\n</li>\n<li readability=\"12\">\n<p><a href=\"https://www.deekit.com/braintree-to-stripe/\">Risto</a>: Before\nintegrating any payment flow make sure you understand the whole\nflow and the different payment states trialing -&gt; active -&gt;\nunpaid -&gt; cancelled. For Braintree there is a flow chart. For\nStripe there is one too. Both payment providers have REST API’s so\nmake sure to play through the payment flows before starting actual\ncoding.</p>\n</li>\n<li readability=\"8\">\n<p><a href=\"https://hackernoon.com/swarming-the-power-of-1m-evs-teslas-on-our-roads-cab1bfa1615\">\nSeyi Fabode</a>: I have 3 neighbors in close proximity who also\nhave solar panels on their roofs. And a couple of other neighbors\nwith electric cars. What says we can’t start our own mini-grid\nsystem between ourselves?</p>\n</li>\n<li readability=\"5\">\n<div readability=\"17\">\n<p><a href=\"https://news.ycombinator.com/item?id=14981592\">pixl97</a>: Muscles/limbs\nare only 'vastly' more efficient if you consider they have large\nnumbers of nano scale support systems constantly rebuilding them.\nSince we don't have nanobots, gears will be better for machines.\nAlso, nature didn't naturally develop a axle.</p>\n</div>\n</li>\n<li readability=\"9\">\n<p><a href=\"http://bravenewgeek.com/are-we-there-yet-the-go-generics-debate/\">Brave\nNew Greek</a>: I sympathize with the Go team’s desire to keep\nthe overall surface area of the language small and the complexity\nlow, but I have a hard time reconciling this with the existing\nbuilt-in generics and continued use of interface{} in the standard\nlibrary.</p>\n</li>\n<li readability=\"4\">\n<p><a href=\"https://twitter.com/jeffhollan/status/897837651339784192\">@jeffhollan</a>: Agree\nto a point. But where does PaaS become “serverless”? Feel should be\n‘infinite’ scale of dynamic allocation of resources + micro\nbill</p>\n</li>\n<li readability=\"10\">\n<p><a href=\"https://twitter.com/kcimc/status/893849886529003524\">@kcimc</a>: common\ntempos in 1M songs, 1959-2011: 120 bpm takes over in the late 80s,\nand bpms at multiples of 10 emerge in the mid 90s</p>\n</li>\n<li readability=\"6\">\n<p><a href=\"http://www.nature.com/news/how-to-map-the-circuits-that-define-us-1.22437\">\nHow to Map the Circuits That Define Us</a>: If neural circuits can\nteach one lesson, it is that no network is too small to yield\nsurprises — or to frustrate attempts at comprehension.</p>\n</li>\n<li readability=\"11\">\n<p><a href=\"https://twitter.com/orskov/status/895882341674926080\">@orskov</a>: In\nQ2, The Wall Street Journal had 1,270,000 daily digital-only\nsubscribers, a 34% increase compared to last year</p>\n</li>\n<li readability=\"6\">\n<p><a href=\"https://www.youtube.com/watch?v=h0962biiZa4&amp;lc=z120tpvy2zqujnszd04chtaxdnybcv5ppoo0k\">\nThrust Zone</a>: A panel including tech billionaire Elon Musk is\ndiscussing the fact that technology has progressed so much that it\nmay soon destroy us and they have to pass microphones to\ntalk.</p>\n</li>\n<li readability=\"5\">\n<p><a href=\"https://twitter.com/damonedwards/status/897928039480217600\">@damonedwards</a>: When\nwe are all running containers in public clouds, I’m really going to\nmiss datacenter folks one-upping each other on hardware\nspecs.</p>\n</li>\n<li readability=\"5\">\n<p><a href=\"https://twitter.com/benedictevans/status/896970740288724992\">@BenedictEvans</a>: 186\npage telecoms report from 1994. 5 pages on ‘videophones’: no\nmention of internet. 10 pages saying web will lose to VR. Nothing\non mobile</p>\n</li>\n<li readability=\"13\">\n<p><a href=\"https://www.edge.org/conversation/thomas_metzinger-benevolent-artificial-anti-natalism-baan\">\nThomas Metzinger</a>: The superintelligence concludes that\nnon-existence is in the own best interest of all future\nself-conscious beings on this planet. Empirically, it knows that\nnaturally evolved biological creatures are unable to realize this\nfact because of their firmly anchored existence bias. The\nsuperintelligence decides to act benevolently.</p>\n</li>\n<li readability=\"19\">\n<p><a href=\"https://jeremyeder.com/2017/07/25/docker-operations-slowing-down-on-aws-this-time-its-not-dns/\">\nJeremy Eder</a>: As with all public cloud, you can do whatever you\nwant…for a price.  BurstBalance is the creation of folks who\nwant you to get hooked on great performance (gp2 can run at 3000+\nIOPS), but then when you start doing something more than dev/test\nand run into these weird issues, you’re already hooked and you have\nno choice but to pay more for a service that is actually\nusable.</p>\n</li>\n<li readability=\"11\">\n<p><a href=\"https://www.bloomberg.com/news/features/2017-08-03/a-baccarat-binge-helped-launder-the-world-s-biggest-cyberheist\">\nKatz and Fan</a>: After all, the important thing for anyone looking\nto launder money through a casino isn’t to win. It’s to exchange\nmillions of dollars for chips you can swap for cool, untraceable\ncash at the end of the night.</p>\n</li>\n<li readability=\"21\">\n<p><a href=\"https://cacm.acm.org/magazines/2016/2/197420-the-verification-of-a-distributed-system/fulltext\">\nCaitie McCaffrey</a>: Verification in industry generally consists\nof unit tests, monitoring, and canaries. While this provides some\nconfidence in the system's correctness, it is not sufficient. More\nexhaustive unit and integration tests should be written. Tools such\nas random model checkers should be used to test a large subset of\nthe state space. In addition, forcing a system to fail via fault\ninjection should be more widely used. Even simple tests such as\nrunning kill −9 on a primary node have found catastrophic\nbugs.</p>\n</li>\n<li readability=\"12\">\n<p><a href=\"http://zupa.hu/thoughts/fpga-the-future-of-computing\">Zupa</a>:\nFPGAs give you most of the benefits of special-purpose processors,\nfor a fraction of the cost. They are about 10x slower, but that\nmeans an FPGA based bitcoin miner is still 100k times faster than a\nprocessor based one</p>\n</li>\n<li readability=\"27\">\n<p><a href=\"https://www.reddit.com/r/programming/comments/6se3lu/fpga_the_future_of_computing/dldlnb3/\">\nmenge101work</a>: I'm not sure if the implication is that our CPUs\nwill have gate arrays on chip with the generic CPU, that is an\ninteresting idea. But if they are not on chip, the gate array will\nnever be doing anything in a few clock cycles. It'll be more akin\nto going out to memory, the latency between a real memory load and\nan L1 or L2 cache hit is huge. (reference)</p>\n<p>Not to say that being able to do complex work on dedicated\nhardware won't still be fast, but the difference between on-die and\noff-die is a huge difference in how big of a change this could\nbe.</p>\n</li>\n<li readability=\"24\">\n<p><a href=\"https://news.ycombinator.com/item?id=14945033\">Animats</a>: This\narticle [<a href=\"https://www.coindesk.com/three-smart-contract-misconceptions/\">Why\nMany Smart Contract Use Cases Are Simply Impossible</a>] outlines\nthe basic problem. If you want smart contracts that do anything off\nchain, there have to be connections to trusted services that\nprovide information and take actions. If you have trusted services\navailable, you may not need a blockchain.The article points out\nthat you can't construct an ordinary loan on chain, because you\nhave no way to enforce paying it back short of tying up the loaned\nfunds tor the duration of the loan. Useful credit fundamentally\nrequires some way of making debtors pay up later. It's possible to\nconstruct various speculative financial products entirely on chain,\nand that's been done, but it's mostly useful for gambling, broadly\ndefined.</p>\n</li>\n<li readability=\"26\">\n<p><a href=\"https://news.ycombinator.com/item?id=14931441\">curun1r</a>: Your\ncharacterization of startup cloud costs is laughably outdated. With\ncredits for startups and the ability to go serverless, I've known\nstartups that didn't pay a dime for hosting their entire first year\ndespite reaching the threshold of hundreds of customers and over\n$1m ARR. One of my friends actually started doing some ML stuff on\nAWS because he wanted to use his remaining credits before they\nexpired and his production and staging workloads weren't going to\nget him there. I'd say it makes no sense to buy your 32gb,\n16-core single point of fail, waste $40/mo and half a day setting\nit up and then have to keep it running yourself when you can easily\nspin up an API in API Gateway/Lambda that dumps data into\ndynamo/simpledb and front it with a static site in S3. That setup\nscales well enough to be mentioned on HN without getting hugged to\ndeath and is kept running by someone else. And if it is, literally,\nfree for the first year, how is that not a no brainer?</p>\n</li>\n<li readability=\"22\">\n<p><a href=\"https://www.nytimes.com/2017/07/25/upshot/maybe-weve-been-thinking-about-the-productivity-slump-all-wrong.html\">\nNeil Irwin</a>: In this way of thinking about productivity,\ninventors and business innovators are always cooking up better ways\nto do things, but it takes a labor shortage and high wages to coax\nfirms to deploy the investment it takes to actually put those\ninnovations into widespread use. In other words, instead of\nworrying so much about robots taking away jobs, maybe we should\nworry more about wages being too low for the robots to even get a\nchance.</p>\n</li>\n</ul></li>\n</ul><p><b>Don't miss all that the Internet has to say on Scalability,\nclick below and become eventually consistent with all scalability\nknowledge</b> (which means this post has many more items to read so\nplease keep on reading)...</p>\n<div class=\"feedflare\"><a href=\"http://feeds.feedburner.com/~ff/HighScalability?a=6n8Lr-P1qd8:wzCC1Uw-qhc:yIl2AUoC8zA\">\n<img src=\"http://feeds.feedburner.com/~ff/HighScalability?d=yIl2AUoC8zA\" border=\"0\"/></a> <a href=\"http://feeds.feedburner.com/~ff/HighScalability?a=6n8Lr-P1qd8:wzCC1Uw-qhc:ZMKU6pOd71k\">\n<img src=\"http://feeds.feedburner.com/~ff/HighScalability?d=ZMKU6pOd71k\" border=\"0\"/></a></div>\n<img src=\"http://feeds.feedburner.com/~r/HighScalability/~4/6n8Lr-P1qd8\" height=\"1\" width=\"1\" alt=\"\"/></div>\n                    </div>\n                                        <div class=\"post\" readability=\"209\">\n                        \n                        \n                        <h2>1. Hello, World!</h2>\n<p>Hi, I’m Paul Brebner and this is a “Hello, World!” blog to\nintroduce myself and test everything out. I’m very excited to have\nstarted at Instaclustr last week as a Technology Evangelist.\n  </p>\n<p>One of the cool things to happen in my first week was that\nInstaclustr celebrated a significant milestone when it exceeded 1\nPetabyte (1PB) of data under management.  Is this a lot of\ndata? Well, yes! A Petabyte is 10^15 bytes, 1 Quadrillion bytes\n(which sounds more impressive!), or 1,000,000,000,000,000\nbytes.</p>\n<p>Also, I’m a Petabyte Person. Apart from my initials being\nPB, the human brain <a href=\"http://www.popsci.com.au/science/medicine/the-human-brain-could-store-10-times-more-memories-than-previously-thought,414008\">\nwas recently estimated to have 1PB of storage\ncapacity,</a> not bad for a 1.4kg meat computer! So in\nthis post, I’m going to introduce myself and some of what I’ve\nlearned about Instaclustr by exploring what that Petabyte\nmeans.</p>\n<p>Folklore has it that the 1st “Hello, World!” program was\nwritten in BCPL (the programming language that led to C), which I\nused to program a 6809 microprocessor I built in the early 1980’s.\nIn those days the only persistent storage I had were 8-inch floppy\ndisks. Each disk could hold 1MB of data, so you’d need 10^9\n(1,000,000,000) 8-inch floppy disks to hold 1PB of data.\n That’s a LOT of disks, and would weigh a whopping 31,000\ntonnes, as much as the German WWII Pocket Battleship the Admiral\nGraf Spee.</p>\n<p><a href=\"https://upload.wikimedia.org/wikipedia/commons/b/b0/Bundesarchiv_DVM_10_Bild-23-63-06%2C_Panzerschiff_%22Admiral_Graf_Spee%22.jpg\" target=\"_blank\" rel=\"noopener\"><img class=\"wp-image-6171 size-medium aligncenter\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/08/German-Battle-Ship-Admiral-Graf-Spee--300x198.jpg\" alt=\"the German WWII Pocket Battleship the Admiral Graf Spee\" width=\"300\" height=\"198\" srcset=\"https://www.instaclustr.com/wp-content/uploads/2017/08/German-Battle-Ship-Admiral-Graf-Spee--300x198.jpg 300w, https://www.instaclustr.com/wp-content/uploads/2017/08/German-Battle-Ship-Admiral-Graf-Spee--768x507.jpg 768w, https://www.instaclustr.com/wp-content/uploads/2017/08/German-Battle-Ship-Admiral-Graf-Spee--640x422.jpg 640w, https://www.instaclustr.com/wp-content/uploads/2017/08/German-Battle-Ship-Admiral-Graf-Spee--73x48.jpg 73w, https://www.instaclustr.com/wp-content/uploads/2017/08/German-Battle-Ship-Admiral-Graf-Spee--164x108.jpg 164w, https://www.instaclustr.com/wp-content/uploads/2017/08/German-Battle-Ship-Admiral-Graf-Spee-.jpg 800w\" sizes=\"(max-width: 300px) 100vw, 300px\"/></a></p>\n<h6>Source: <a href=\"https://en.wikipedia.org/wiki/German_cruiser_Admiral_Graf_Spee\" target=\"_blank\" rel=\"noopener\">Wikipedia<br/></a></h6>\n<p>1PB of data is also a lot in terms of my experience.\n I worked in R&amp;D for CSIRO and UCL in the areas of\nmiddleware, distributed systems, grid computing, and sensor\nnetworks etc between 1996-2007. During 2003 I was the software\narchitect for a CSIRO Grid cluster computing project with\n<a href=\"http://acomputerscientistlearnsaws.blogspot.com.au/2017/06/who-am-i-need-for-speed-and-scalability.html\">\nBig Astronomy Data – all 2TB of it.</a> 1PB is\n500,000 times more data than that.</p>\n<p>What do Instaclustr and the Large Hadron Collider have in\ncommon?</p>\n<p><img class=\"size-medium wp-image-6177 aligncenter\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/08/Large-Hadron-Collider-300x199.jpg\" alt=\"Large Hadron Collider Instaclustr\" width=\"300\" height=\"199\" srcset=\"https://www.instaclustr.com/wp-content/uploads/2017/08/Large-Hadron-Collider-300x199.jpg 300w, https://www.instaclustr.com/wp-content/uploads/2017/08/Large-Hadron-Collider-72x48.jpg 72w, https://www.instaclustr.com/wp-content/uploads/2017/08/Large-Hadron-Collider-163x108.jpg 163w, https://www.instaclustr.com/wp-content/uploads/2017/08/Large-Hadron-Collider.jpg 500w\" sizes=\"(max-width: 300px) 100vw, 300px\"/></p>\n<h6>Source: <a href=\"http://cms.cern/\">cms.cern</a></h6>\n<p>Apart from the similarity of the Instaclustr Logo and the\naerial shot of the LHC? (The circular pattern)</p>\n<p><img class=\"size-medium wp-image-6193 aligncenter\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/08/LHC-from-above-300x249.jpg\" alt=\"LHC from above\" width=\"300\" height=\"249\" srcset=\"https://www.instaclustr.com/wp-content/uploads/2017/08/LHC-from-above-300x249.jpg 300w, https://www.instaclustr.com/wp-content/uploads/2017/08/LHC-from-above-58x48.jpg 58w, https://www.instaclustr.com/wp-content/uploads/2017/08/LHC-from-above-130x108.jpg 130w, https://www.instaclustr.com/wp-content/uploads/2017/08/LHC-from-above.jpg 400w\" sizes=\"(max-width: 300px) 100vw, 300px\"/></p>\n<p><img class=\"aligncenter wp-image-6163 size-medium\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/08/Apache-Cassandra-Instaclustr-logo-Home-300x44.png\" alt=\"Instaclustr logo &amp; the Large Hadron Collider\" width=\"300\" height=\"44\" srcset=\"https://www.instaclustr.com/wp-content/uploads/2017/08/Apache-Cassandra-Instaclustr-logo-Home-300x44.png 300w, https://www.instaclustr.com/wp-content/uploads/2017/08/Apache-Cassandra-Instaclustr-logo-Home-327x48.png 327w, https://www.instaclustr.com/wp-content/uploads/2017/08/Apache-Cassandra-Instaclustr-logo-Home.png 565w\" sizes=\"(max-width: 300px) 100vw, 300px\"/></p>\n<h6>Source: <a href=\"http://d1068036.site.myhosting.com/VQuarkNet/LHCabove.jpg\">Sequim\nScience</a></h6>\n<p>The LHC “<a href=\"https://home.cern/about/computing\">Data Centre processes\nabout one petabyte of data every day</a>“.  More\nastonishing is that <a href=\"http://www.techrepublic.com/article/cern-we-generate-1pb-each-second-heres-what-thats-taught-us-about-big-data/\">\nthe LHC has a huge number of detectors which spit out data at\nthe astonishing rate of 1PB/s – most of which they don’t (and\ncan’t) store.</a></p>\n<h2>2. Not just Data Size</h2>\n<p>But there’s more to data than just the dimension of size.\nThere are large amounts of “dead” data lying around on archival\nstorage media which is more or less useless. But live dynamic data\nis extremely useful and is of enormous business value. So apart\nfrom being able to store 1PB of data (and more), you need to be\nable to: write data fast enough, keep track of what data you have,\nfind the data and use it, have 24×7 availability, and have\nsufficient processing and storage capacity on-demand to do useful\nthings as fast as possible (scalability and elasticity).  How\nis this possible? <a href=\"https://www.instaclustr.com/\"><b>Instaclustr</b></a><b>!</b></p>\n<p>The Instaclustr value proposition is:\n“<b>Reliability at scale.”</b></p>\n<p>Instaclustr solves the problem of “reliability at scale”\nvia a combination of Scalable Open Source Big Data technologies,\nPublic Cloud hosting, and Instaclustr’s own Managed Services\ntechnologies (including cloud provisioning, cluster monitoring and\nmanagement, automated repairs and backups, performance\noptimization, scaling and elasticity).</p>\n<p>How does this enable reliability at Petabyte data scale?\nHere’s just one example. If you have 1PB+ of data on multiple SSDs\nin your own data centre an obvious problem would be\n<i>data durability</i>.  1PB is a lot of\ndata, and SSDs are NOT 100% error free (the main problem is\n“quantum tunneling” which “drills holes” in SSDs and imposes a\nmaximum number of writes).  <a href=\"http://techreport.com/review/27909/the-ssd-endurance-experiment-theyre-all-dead\">These\npeople did the experiment</a> to see how many writes\nit took before different SSDs died, and the answer is in some cases\nit’s less than 1PB. So, SSDs will fail and you will lose data –\nunless you have taken adequate precautions and continue to do so.\nThe whole system must be designed and operated assuming failure is\nnormal.</p>\n<p>The Cassandra NoSQL database itself is designed for high\ndurability by replicating data on as many different nodes as you\nspecify (it’s basically a shared-nothing P2P distributed system).\n Instaclustr enhances this durability by providing automatic\nincremental backups (to S3 on AWS), multiple region deployments for\nAWS, automated repairs, and 24×7 monitoring and\nmanagement.</p>\n<h2>3. The Data Network Effect</h2>\n<p>1PB of data under management by Instaclustr is a\nnoteworthy milestone, but that’s not the end of the story. The\namount of data will GROW! The Network effect is what happens\nwhen the value of a product or service increases\nsubstantially with the number of other people using it. The classic\nexample is the telephone system.  A single telephone is\nuseless, but 100 telephones connected together (even manually by an\noperator!) is really useful, and so the network grows rapidly.\n <a href=\"http://mattturck.com/the-power-of-data-network-effects/\">Data\nNetwork effects</a> are related to the amount and\ninterconnectedness of data in a system.  </p>\n<p>For Instaclustr customers the Data Network effect is\nlikely to be felt in two significant ways.  Firstly, the\namount of data customers have in their Cassandra cluster will\nincrease – as it increases over time the value will increase\nsignificantly; also velocity: the growth of data, services, sensors\nand applications in the related ecosystems – e.g in the AWS\necosystem – will accelerate.  Secondly, the continued growth\nin clusters and data under management provides considerable more\nexperience, learned knowledge and capabilities for managing\nyour data and clusters.</p>\n<p>Here’s a prediction. I’ve only got 3 data points to go on.\nWhen Instaclustr started (in 2014) it was managing 0PB of data. At\nthe end of 2016 this had increased to 0.5PB, and in July 2017 we\nreached 1PB. Assuming the amount of data continues to double at the\nsame rate in the future, then the total data under management by\nInstaclustr may increase massively over the next 3 years</p>\n<p><img class=\"aligncenter wp-image-6233 size-full\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/08/Graph-of-months-vs-PB-Data.png\" alt=\"Graph of months vs PB Data Instaclustr\" width=\"553\" height=\"367\" srcset=\"https://www.instaclustr.com/wp-content/uploads/2017/08/Graph-of-months-vs-PB-Data.png 553w, https://www.instaclustr.com/wp-content/uploads/2017/08/Graph-of-months-vs-PB-Data-300x199.png 300w, https://www.instaclustr.com/wp-content/uploads/2017/08/Graph-of-months-vs-PB-Data-72x48.png 72w, https://www.instaclustr.com/wp-content/uploads/2017/08/Graph-of-months-vs-PB-Data-163x108.png 163w\" sizes=\"(max-width: 553px) 100vw, 553px\"/></p>\n<p>(Graph of Months vs PB of data).</p>\n<h2>4. What next?</h2>\n<p>I started as Technology Evangelist at Instaclustr (last\nweek). My background for the last 10 years has been a senior\nresearch scientist in NICTA (National ICT Australia, now merged\nwith CSIRO as Data61), and for the last few years at CTO/consultant\nwith a NICTA start-up specializing in performance engineering (via\nPerformance Data Analytics and Modelling). I’ve invented some\nperformance analytics techniques, commercialized them in a tool,\nand applied them to client problems, often in the context of Big\nData Analytics problems. The tool we developed actually used\nCassandra (but in a rather odd way, we needed to store very large\nvolumes of performance data for later analysis from a simulation\nengine, hours of data could be simulated and needed to be persisted\nin a few seconds and Cassandra was ideal for that use case). I’m\nalso an AWS Associate Solution Architect. Before all this, I was a\nComputer Scientist, Machine Learning researcher, Software Engineer,\nSoftware Architect, UNIX systems programmer, etc.  Some of\nthis may come in useful for getting up to speed with Cassandra\netc.</p>\n<p>Over the next few months, the plan is for me to understand\nthe Instaclustr technologies from “end user” and developer\nperspectives, develop and try out some sample applications, and\nblog about them. I’m going to start with the internal Instaclustr\nuse case for Cassandra, which is the heart of the performance data\nmonitoring tool (<a href=\"https://www.instaclustr.com/resource/instametrics/\">Instametrics</a>).\n Currently there’s lots of performance data stored in\nCassandra, but only a limited amount of analysis and prediction\nbeing done (just aggregation). It should be feasible and an\ninteresting learning exercise to do some performance analytics on\nthe Cassandra data performance data to predict interesting events\nin advance and take remedial action.</p>\n<p>I’m planning to start out with some basic use cases and\nprogramming examples for Cassandra (e.g. how do you connect to\nCassandra, how do you find out what’s in it, how do get data out of\nit, how do you do simple queries such as finding “outliers”, how do\nyou more complex Data Analytics (e.g. regression analysis, ML,\netc), etc. I’ll probably start out very simple. e.g. Cassandra\n&amp; Java on my laptop, having a look at some sample Cassandra\ndata, try some simple Data analytics, then move to more complex\nexamples as the need arises. E.g. Cassandra on Instaclustr, Java on\nAWS, Spark + MLLib, etc. I expect to make some mistakes and revise\nthings as I go. Most of all I hope to make it\ninteresting.</p>\n<p>Watch this space <img src=\"https://s.w.org/images/core/emoji/2.3/72x72/1f642.png\" alt=\"????\" class=\"wp-smiley\"/></p>\n\n<p>The post <a rel=\"nofollow\" href=\"https://www.instaclustr.com/paul-brebner-petabyte-person-joins-instaclustr-petabyte-company/\">\nPaul Brebner (the Petabyte Person) joins Instaclustr (the Petabyte\nCompany)</a> appeared first on <a rel=\"nofollow\" href=\"https://www.instaclustr.com/\">Instaclustr</a>.</p>\n                    </div>\n                                        <div class=\"post\" readability=\"49\">\n                        \n                        \n                        <p>A handy feature was silently added to <a href=\"http://cassandra.apache.org/\">Apache Cassandra</a>’s <code class=\"highlighter-rouge\">nodetool</code> just over a year ago. The\nfeature added was the <code class=\"highlighter-rouge\">-j</code>\n(jobs) option. This little gem controls the number of compaction\nthreads to use when running either a <code class=\"highlighter-rouge\">scrub</code>, <code class=\"highlighter-rouge\">cleanup</code>, or <code class=\"highlighter-rouge\">upgradesstables</code>. The option was added to\n<code class=\"highlighter-rouge\">nodetool</code> via <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-11179\">CASSANDRA-11179</a>\nto version 3.5. It has been back ported to Apache Cassandra\nversions 2.1.14, 2.2.6, and 3.5.</p>\n<p>If unspecified, <code class=\"highlighter-rouge\">nodetool</code>\nwill use 2 compaction threads. When this value is set to 0 all\navailable compaction threads are used to perform the operation.\nNote that the total number of available compaction threads is\ncontrolled by the <code class=\"highlighter-rouge\">concurrent_compactors</code> property in the\n<em>cassandra.yaml</em> configuration file. Examples of how it can\nbe used are as follows.</p>\n<div class=\"highlighter-rouge\" readability=\"6\">\n<pre>$ nodetool scrub -j 3\n$ nodetool cleanup -j 1\n$ nodetool upgradesstables -j 1 \n</pre></div>\n<p>The option is most useful in situations where disk space is\nscarce and a limited number of threads for the operation need to be\nused to avoid disk exhaustion.</p>\n                    </div>\n                                        <div class=\"post\" readability=\"525\">\n                        \n                        \n                        Introduction I’ve wanted to create a system which in\nits core uses event sourcing for quite a while - actually since\nI’ve read Martin Kleppmann’s Making\nSense of Stream Processing. The book is\nreally amazing, Martin tends to explain all concepts from basic\nbuilding blocks and in a really simple and understandable way. I\nrecommend it to everyone. The idea is to have a running Cassandra\ncluster and to evolve a system with no downtime in such a way that\nKafka is the Source of Truth with immutable facts. Every other\nsystem (in this case Cassandra cluster) should use these facts and\naggregate / transform them for its purpose. Also, since all facts\nare in Kafka, it should be easy to drop the whole database, index,\ncache or any other data system and recreate it from scratch\nagain. The\nfollowing diagrams should illustrate the system\nevolution.  Starting system\narchitecture    Target system\narchitecture   When observing the diagrams, it seems like\na pretty straightforward and trivial thing to do, but there’s more\nto it, especially when you want to do it with no\ndowntime. Evolution\nbreakdown I\ntried to break down the evolution process to a few conceptual steps\nand this is what I came up with: \nHave a mechanism to push each Cassandra\nchange to Kafka with timestamp\nStart collecting each Cassandra change to\ntemporary Kafka topic I need to start collecting before a\nsnapshot is taken, otherwise there will be a time window in which\nincoming changes would be lost, and it also needs to go to\ntemporary topic since there is data in the database which should be\nfirst in an ordered sequence of events\n Take the existing database\nsnapshot This one is\npretty straightforward \nStart reading data from the snapshot into\nthe right Kafka topic Since the data from the snapshot was\ncreated first, it should be placed first into\nKafka \nAfter the snapshot is read, redirect the\ndata from the temporary Kafka topic to the right Kafka topic, but\nmind the timestamp when the snapshot is taken This step is essential to be done\ncorrectly, and could be considered as the hardest part. Since\nchange event collecting started before the snapshot, there is a\npossibility that some events also exist in the snapshot as well\nand, to avoid inconsistencies, each event should be idempotent and\nI should try to be as precise as possible when comparing the event\ntimestamp with the snapshot timestamp\n Create a new Cassandra\ncluster/keyspace/table and Kafka stream to read from Kafka and\ninsert into this new Cassandra\ncluster/keyspace/table As a result, the new cassandra cluster\nshould be practically a copy/clone of the existing\none  Wait\nfor the temporary Kafka topic to deplete If I change the application to read from\nthe new cassandra right away, and Kafka temporary topic still\ndoesn’t catch up with system, there will be significant read delays\n(performance penalties) in the system. To make sure everything is\nin order, I think monitoring of time to propagate the change to the\nnew Cassandra cluster will help and if the number is decent (a few\nmilliseconds), I can proceed to the next\nstep \nChange the application to read from the new\ncassandra instead of old and still write to old\nSince everything is done within the\nno downtime context, the application is actually several instances\nof application on different nodes, and they won’t be changed\nsimultaneously, that would cause the downtime. I’d need to change\none at a time, while others are still having the old software\nversion. For this reason, the application still needs to write to\nthe old cassandra, since other application nodes are still reading\nfrom the old cassandra. \nWhen each application instance is updated,\nchange the application to write directly to Kafka right\ntopic Now each node,\none by one, can be updated with new application version which will\nwrite directly to Kafka. In parallel, old nodes will write to the\nold Cassandra which will propagate to Kafka topic, and new nodes\nwill write directly to the Kafka topic. When the change is\ncomplete, all nodes are writing directly to the Kafka topic and we\nare good to go. \nClean up At this point, the system writes to the\nright Kafka topic, the stream is reading from it and making inserts\ninto the new Cassandra. The old Cassandra and Kafka temporary topic\nare no longer necessary so it should be safe for me to remove\nthem.   Well, that’s the plan, so we’ll see\nwhether it is doable or not. There are a few motivating factors why\nI’ve chosen to evolve an existing system instead of building one\nthe way I want from scratch. \nIt is more challenging, hence more\nfun The need for\nevolving existing systems is the everyday job of software\ndevelopers; you don’t get a chance to build a system for a starting\nset of requirements with guarantee that nothing in it will ever\nchange (except for a college project,\nperhaps) When a\nsystem needs to change, you can choose two ways, to build a new one\nfrom scratch and when ready replace the old or to evolve the\nexisting. I’ve done the former a few times in my life, and it might\nseem as fun at the beginning, but it takes awfully long, with a lot\nof bug fixing, often ends up as a catastrophe and is always\nexpensive Evolving a\nsystem takes small changes with more control, instead of placing a\ntotally new system instead of the old.\nI’m a fan of Martin Fowler’s blog,\nEvolutionary\nDatabase Design fits\nparticularly nicely in this topic\n Since writing about this\nin a single post would render quite a huge post, I’ve decided to\nsplit it into a few, I’m still not sure how many, but I’ll start\nand see where it takes me. Bear with me.\nData model\nI’ll start with data model.\nActually, it is just one simple table, but it should be enough to\ndemonstrate the idea. The following CQL code describes the\ntable. CREATE\nTABLE IF NOT EXISTS movies_by_genre (title text,genre text,year int,rating\nfloat,duration int,director text,country text,PRIMARY KEY ((genre, year), rating,\nduration)) WITH CLUSTERING ORDER BY (rating DESC,\nduration ASC) The use\ncase for this table might not be that common, since the table is\nactually designed to have a complex primary key with at least two\ncolumns as a partition key and at least two clustering columns. The\nreason for that is it will leverage examples, since handling of a\ncomplex primary key might be needed for someone reading\nthis. In\norder to satisfy the first item from the Evolution breakdown, I\nneed a way to push each Cassandra change to Kafka with a timestamp.\nThere are a few ways to do it: Cassandra Triggers, Cassandra CDC,\nCassandra Custom Secondary Index and possibly some other ways, but\nI’ll investigate only the three mentioned.\nCassandra\nTriggers For\nthis approach I’ll use two Cassandra 3.11.0 nodes, two Kafka\n0.10.1.1 nodes and one Zookeeper 3.4.6. Every node will run in a\nseparate Docker container. I decided to use Docker since it keeps\nmy machine clean and it is easy to recreate\ninfrastructure. To create a trigger in Cassandra, ITrigger\ninterface needs to be implemented. The interface itself is pretty\nsimple: public\ninterface ITrigger {public\nCollection&lt;Mutation&gt; augment(Partition update);} And that’s all\nthere is to it. The interface has been changed since Cassandra 3.0.\nEarlier versions of Cassandra used the following\ninterface: public interface ITrigger {public Collection&lt;Mutation&gt; augment(ByteBuffer\npartitionKey, ColumnFamily update);} Before I dive into implementation, let’s\ndiscuss the interface a bit more. There are several important\npoints regarding the implementation that need to be honored and\nthose points are explained on the interface’s\njavadoc: \nImplementation of this interface should only\nhave a constructor without parameters\nITrigger implementation can\nbe instantiated multiple times during the server life time.\n(Depends on the number of times the trigger folder is\nupdated.) ITrigger\nimplementation should be stateless (avoid dependency on instance\nvariables).  Besides that, augment method is called\nexactly once per update and Partition object contains\nall relevant information about the update. You might notice that\nreturn type is not void but rather a collection\nof mutations. This way trigger can be implemented to perform some\nadditional changes when certain criteria are met. But since I just\nwant to propagate data to Kafka, I’ll just read the update\ninformation, send it to Kafka and return empty mutation collection.\nIn order not to pollute this article with a huge amount of code,\nI’ve created maven project which creates a JAR file, and the\nproject can be found here.\nI’ll try to explain the code in the\nproject. Firstly, there is a FILE_PATH constant, which\npoints to /etc/cassandra/triggers/KafkaTrigger.yml\nand this is where YAML configuration for trigger class needs to be.\nIt should contain configuration options for Kafka brokers and for\ntopic name. The file is pretty simple, since the whole file\ncontains just the following two lines:\nbootstrap.servers:\ncluster_kafka_1:9092,cluster_kafka_2:9092 topic.name:\ntrigger-topic I’ll come\nto that later when we build our docker images. Next, there is a\nconstructor which initializes the Kafka producer and ThreadPoolExecutor. I could\nhave done it without ThreadPoolExecutor, but the\nreason for it is that the trigger augment call is on\nCassandra’s write path and in that way it impacts Cassandra’s write\nperformances. To minimize that, I’ve moved trigger execution to\nbackground thread. This is doable in this case, since I am not\nmaking any mutations, I can just start the execution in another\nthread and return an empty list of mutations immediately. In case\nwhen the trigger needs to make a mutation based on partition\nchanges, that would need to happen in the same\nthread. Reading data from partition update in\naugment method is really a mess. Cassandra API is not that\nintuitive and I went through a real struggle to read all the\nnecessary information. There are a few different ways to update a\npartition in Cassandra, and these are ones I’ve\ncovered: \nInsert\nUpdate\nDelete of director\ncolumn Delete of\ntitle column Delete\nof both director and title columns\nDelete of row\nDelete range of rows for last clustering\ncolumn (duration between some values)\nDelete all rows for specific rating\nclustering column\nDelete range of rows for first clustering\ncolumn (rating between some values)\nDelete whole\npartition  A simplified algorithm would\nbe: if\n(isPartitionDeleted(partition)) {handle partition\ndelete;} else {if\n(isRowUpdated(partition)) {if\n(isRowDeleted(partition)) {handle row delete;} else {if (isCellDeleted(partition)) {handle cell delete;} else {handle\nupsert;}}} else if\n(isRangeDelete(partition)) {handle range delete;}} In\neach case, JSON is generated and sent to Kafka. Each message\ncontains enough information to recreate Cassandra CQL query from\nit. Besides\nthat, there are a few helper methods for reading the YAML\nconfiguration and that is all. In order to test everything, I’ve chosen\nDocker, as stated earlier. I’m using Cassandra\ndocker image with 3.11.0 tag. But since the JAR file and KafkaTrigger.yml need to be\ncopied into the docker container, there are two\noptions: \nUse Cassandra 3.11.0 image and docker cp\ncommand to copy the files into the\ncontainer Create a\nnew Docker image with files already in it and use that\nimage  The first option is not an option\nactually, it is not in the spirit of Docker to do such thing so I\nwill go with the second option. Create a cluster directory somewhere and a\ncassandra directory within it mkdir -p cluster/cassandra cluster directory will be\nneeded for later, now just create KafkaTrigger.yml in\ncassandra dir with the content I provided earlier. Also, the built\nJAR file (cassandra-trigger-0.0.1-SNAPSHOT.jar)\nneeds to be copied here. To build all that into Docker, I created a\nDockerfile with the following content:\nFROM cassandra:3.11.0COPY\nKafkaTrigger.yml /etc/cassandra/triggers/KafkaTrigger.ymlCOPY cassandra-trigger-0.0.1-SNAPSHOT.jar\n/etc/cassandra/triggers/trigger.jarCMD [\"cassandra\",\n\"-f\"] In console, just\nposition yourself in the cassandra directory and\nrun: docker\nbuild -t trigger-cassandra . That will create a docker image with name\ntrigger-cassandra.\nAll that is left is to create a\nDocker compose file, join all together and test it. The Docker\ncompose file should be placed in the  cluster directory. The\nreason for that is because Docker compose has a naming convention\nfor containers it creates, it is &lt;present_directory_name&gt;_&lt;service_name&gt;_&lt;order_num&gt;.\nAnd I already specified the Kafka domain names in KafkaTrigger.yml as cluster_kafka_1 and cluster_kafka_2, in case the\nDocker compose is run from another location, container naming would\nchange and KafkaTrigger.yml would need\nto be updated. My Docker compose file is located in\nthe cluster directory, it’s named cluster.yml and it looks\nlike this: version: '3.3'services:zookeeper:image:\nwurstmeister/zookeeper:3.4.6ports:-\n\"2181:2181\"kafka:image:\nwurstmeister/kafka:0.10.1.1ports:-\n9092environment:HOSTNAME_COMMAND:\n\"ifconfig | awk '/Bcast:.+/{print $$2}' | awk -F\\\":\\\" '{print\n$$2}'\"KAFKA_ADVERTISED_PORT: 9092KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181cassandra-seed:image: trigger-cassandraports:- 7199- 9042environment:CASSANDRA_CLUSTER_NAME: test-clustercassandra:image: trigger-cassandraports:- 7199- 9042environment:CASSANDRA_CLUSTER_NAME: test-clusterCASSANDRA_SEEDS: cassandra-seed The cluster contains the definition for\nZookeeper, Kafka and Cassandra with the exception that there are\ntwo Cassandra services. The reason for that is that one can be\nstandalone, but all others need a seed list. cassandra-seed will serve as\nseed, and cassandra as scalable\nservice. That way, I can start multiple instances of cassandra.\nHowever, to start multiple instances, it takes time, and it is not\nrecommended to have multiple Cassandra nodes in joining state. So,\nscale should be done one node at a time. That does not apply to\nKafka nodes. With the following command, I’ve got a running cluster\nready for use: docker-compose -f cluster.yml up -d --scale\nkafka=2 After that, I\nconnected to the Cassandra cluster with cqlsh and\ncreated the keyspace and table. To add a trigger to the table, you need to\nexecute the following command: CREATE TRIGGER kafka_trigger ON\nmovies_by_genre USING\n'io.smartcat.cassandra.trigger.KafkaTrigger'; In case you get the following\nerror: ConfigurationException: Trigger class\n'io.smartcat.cassandra.trigger.KafkaTrigger' doesn't\nexist There are several\nthings that can be wrong. The JAR file might not be loaded within\nthe Cassandra node; that should happen automatically, but if it\ndoesn’t you can try to load it with: nodetool reloadTriggers If the problem persists, it might be that\nthe configuration file is not at a proper location, but that can\nonly happen if you are using a different infrastructure setup and\nyou forgot to copy KafkaTrigger.yml to the\nproper location. Cassandra will show the same error even if class\nis found but there is some problem instantiating it or casting it\nto theITrigger interface. Also, make sure that you implemented the\nITrigger\ninterface from the right Cassandra version (versions of cassandra\nin the JAR file and of the cassandra node should\nmatch). If\nthere are no errors, the trigger is created properly. This can be\nchecked by executing the following CQL\ncommands: USE\nsystem_schema;SELECT * FROM triggers;\nResults\nI used kafka-console-consumer to see\nif messages end up in Kafka, but any other option is good enough.\nHere are a few things I tried and the results it gave\nme.  For most cases, not all of these mutations\nare used, usually it’s just insert, update and one kind of delete.\nHere I intentionally tried several ways since it might come in\nhandy to someone. In case you have a simpler table use case, you\nmight be able to simplify the trigger code as\nwell. What is\nalso worth noting is that triggers execute only on a coordinator\nnode; they have nothing to do with data ownership nor replication\nand the JAR file needs to be on every node that can become a\ncoordinator. Going a step\nfurther This\nis OK for testing purposes, but for this experiment to have any\nvalue, I will simulate the mutations to the cassandra cluster at\nsome rate. This can be accomplished in several ways, writing a\ncustom small application, using cassandra stress or using some\nother tool. Here at SmartCat, we have developed a tool for such\npurpose. That is the easiest way for me to create load on a\nCassandra cluster. The tool is called Berserker,\nyou can give it a try. To start with Berserker, I’ve downloaded\nthe latest version (0.0.7 is the latest at the moment of writing)\nfrom here.\nAnd I’ve created a configuration file named\nconfiguration.yml.  load-generator-configuration\nsection is used to specify all other configurations. There, for\nevery type of the configuration, name is specified in order for the\nBerserker to know which configuration parser to use in concrete\nsections. After that, a section for each configuration with parser\nspecific options and format is found. There are following sections\navailable: \ndata-source-configuration\nwhere data source which will generate data for worker is\nspecified rate-generator-configuration\nwhere should be specified how rate generator will be created and it\nwill generate rate. This rate is rate at which worker will\nexecute worker-configuration,\nconfiguration for worker\nmetrics-reporter-configuration,\nconfiguration for metrics reporting, currently only JMX and console\nreporting is supported  In this case, the data-source-configuration\nsection is actually a Ranger configuration format and can be found\nhere.\nAn important part for this article\nis the connection-points property\nwithin worker-configration. This\nwill probably be different every time Docker compose creates a\ncluster. To see your connection points run:\ndocker ps It should give you a similar\noutput:  There you can find port mapping for\ncluster_cassandra-seed_1 and\ncluster_cassandra_1\ncontainers and use it, in this case it is: 0.0.0.0:32779 and 0.0.0.0:32781.\nNow that everything is settled, just\nrun: java -jar\nberserker-runner-0.0.7.jar -c configuration.yml Berserker starts spamming the Cassandra\ncluster and in my terminal where kafka-console-consumer is\nrunning, I can see messages appearing, it seems everything is as\nexpected, at least for now. End\nThat’s all, next time I’ll talk\nabout Cassandra CDC and maybe custom secondary index. Hopefully, in\na few blog posts, I’ll have the whole idea tested and\nrunning.                    </div>\n                                        <div class=\"post\" readability=\"555\">\n                        \n                        \n                        <p>One of the common griefs Scala developers express when using the\nDataStax Java driver is the overhead incurred in almost every read\nor write operation, if the data to be stored or retrieved needs\nconversion from Java to Scala or vice versa.</p>\n<p>This could be avoided by using \"native\" Scala codecs. This has\nbeen occasionally solicited from the Java driver team, but such\ncodecs unfortunately do not exist, at least not officially.</p>\n<p>Thankfully, the <a href=\"http://docs.datastax.com/en/drivers/java/3.3/index.html?com/datastax/driver/core/TypeCodec.html\">\nTypeCodec</a> API in the Java driver can be easily extended. For\nexample, several convenience Java codecs are available in <a href=\"http://docs.datastax.com/en/developer/java-driver/3.3/manual/custom_codecs/extras/\">\nthe driver's extras package</a>.</p>\n<p>In this post, we are going to piggyback on the existing extra\ncodecs and show how developers can create their own codecs –\ndirectly in Scala.</p>\n<p><em>Note: all the examples in this post are available in\n<a href=\"https://github.com/datastax/java-driver-scala-extras\">this\nGithub repository</a>.</em></p>\n<h3>Dealing with Nullability</h3>\n<p>It can be tricky to deal with CQL types in Scala because CQL\ntypes are all nullable, whereas most typical representations of CQL\nscalar types in Scala resort to value classes, and these are\nnon-nullable.</p>\n<p>As an example, let's see how the Java driver deserializes, say,\nCQL <code>int</code>s.</p>\n<p>The <a href=\"https://github.com/datastax/java-driver/blob/3.3.0/driver-core/src/main/java/com/datastax/driver/core/TypeCodec.java#L736-L759\">\ndefault codec for CQL <code>int</code>s</a> converts such values to\n<code>java.lang.Integer</code> instances. From a Scala perspective,\nthis has two disadvantages: first, one needs to convert from\n<code>java.lang.Integer</code> to <code>Int</code>, and second,\n<code>Integer</code> instances are nullable, while Scala\n<code>Int</code>s aren't.</p>\n<p>Granted, the DataStax Java driver's <code>Row</code> interface\nhas a pair of methods named <a href=\"http://docs.datastax.com/en/drivers/java/3.3/com/datastax/driver/core/GettableByIndexData.html#getInt-int-\">\n<code>getInt</code></a> that deserialize CQL <code>int</code>s into\nJava <code>int</code>s, converting <code>null</code> values into\nzeroes.</p>\n<p>But for the sake of this demonstration, let's assume that these\nmethods did not exist, and all CQL <code>int</code>s were being\nconverted into <code>java.lang.Integer</code>. Therefore,\ndevelopers would yearn to have a codec that could deserialize CQL\n<code>int</code>s into Scala <code>Int</code>s while at the same\ntime addressing the nullability issue.</p>\n<p>Let this be the perfect excuse for us to introduce\n<code>IntCodec</code>, our first Scala codec:</p>\n<pre class=\"brush: scala; collapse: true; gutter: true; light: false; title: ; toolbar: true; notranslate\">\nimport java.nio.ByteBuffer\nimport com.datastax.driver.core.exceptions.InvalidTypeException\nimport com.datastax.driver.core.{DataType, ProtocolVersion, TypeCodec}\nimport com.google.common.reflect.TypeToken\nobject IntCodec extends TypeCodec[Int](DataType.cint(), TypeToken.of(classOf[Int]).wrap()) {\n  override def serialize(value: Int, protocolVersion: ProtocolVersion): ByteBuffer = \n    ByteBuffer.allocate(4).putInt(0, value)\n  override def deserialize(bytes: ByteBuffer, protocolVersion: ProtocolVersion): Int = {\n    if (bytes == null || bytes.remaining == 0) return 0\n    if (bytes.remaining != 4) throw new InvalidTypeException(\"Invalid 32-bits integer value, expecting 4 bytes but got \" + bytes.remaining)\n    bytes.getInt(bytes.position)\n  }\n  override def format(value: Int): String = value.toString\n  override def parse(value: String): Int = {\n    try {\n      if (value == null || value.isEmpty || value.equalsIgnoreCase(\"NULL\")) 0\n      else value.toInt\n    }\n    catch {\n      case e: NumberFormatException =&gt;\n        throw new InvalidTypeException( s\"\"\"Cannot parse 32-bits integer value from \"$value\"\"\"\", e)\n    }\n  }\n}\n</pre>\n<p>All we did so far is extend <code>TypeCodec[Int]</code> by\nfilling in the superclass constructor arguments (more about that\nlater) and implementing the required methods in a very similar way\ncompared to the driver's <a href=\"https://github.com/datastax/java-driver/blob/3.3.0/driver-core/src/main/java/com/datastax/driver/core/TypeCodec.java#L1412-L1452\">\nbuilt-in codec</a>.</p>\n<p>Granted, this isn't rocket science, but it will get more\ninteresting later. The good news is, this template is reproducible\nenough to make it easy for readers to figure out how to create\nsimilar codecs for every <code>AnyVal</code> that is mappable to a\nCQL type (<code>Boolean</code>, <code>Long</code>,\n<code>Float</code>, <code>Double</code>, etc... let your\nimagination run wild or just go for the <a href=\"https://github.com/datastax/java-driver-scala-extras/tree/master/src/main/scala/com/datastax/driver/extras/codecs/scala\">\nready-made solution</a>).</p>\n<p>(Tip: because of the automatic boxing/unboxing that occurs under\nthe hood, don't use this codec to deserialize simple CQL\n<code>int</code>s, and prefer instead the driver's built-in one,\nwhich will avoid this overhead; but you can use\n<code>IntCodec</code> to compose more complex codecs, as we will\nsee below – the more complex the CQL type, the more negligible the\noverhead becomes.)</p>\n<p>Let's see how this piece of code solves our initial problems: as\nfor the burden of converting between Scala and Java,\n<code>Int</code> values are now written <em>directly</em> with\n<code>ByteBuffer.putInt</code>, and read <em>directly</em> from\n<code>ByteBuffer.getInt</code>; as for the nullability of CQL\n<code>int</code>s, the issue is addressed just as the driver does:\n<code>null</code>s are converted to zeroes.</p>\n<p>Converting <code>null</code>s into zeroes might not be\nsatisfying for everyone, but how to improve the situation? The\ngeneral Scala solution for dealing with nullable integers is to map\nthem to <code>Option[Int]</code>. DataStax Spark Connector for\nApache Cassandra®'s <a href=\"http://datastax.github.io/spark-cassandra-connector/ApiDocs/2.0.2/spark-cassandra-connector/#com.datastax.spark.connector.CassandraRow\">\n<code>CassandraRow</code></a> class has exactly one such\nmethod:</p>\n<pre class=\"brush: scala; title: ; notranslate\">\ndef getIntOption(index: Int): Option[Int] = ...\n</pre>\n<p>Under the hood, it reads a <code>java.lang.Integer</code> from\nthe Java driver's <code>Row</code> class, and converts the value to\neither <code>None</code> if it's <code>null</code>, or to\n<code>Some(value)</code>, if it isn't.</p>\n<p>Let's try to achieve the same behavior, but using the composite\npattern: we first need a codec that converts from <em>any</em> CQL\nvalue into a Scala <code>Option</code>. There is no such built-in\ncodec in the Java driver, but now that we are codec experts, let's\nroll our own <code>OptionCodec</code>:</p>\n<pre class=\"brush: scala; collapse: true; gutter: true; light: false; title: ; toolbar: true; notranslate\">\nclass OptionCodec[T](\n    cqlType: DataType,\n    javaType: TypeToken[Option[T]],\n    innerCodec: TypeCodec[T])\n  extends TypeCodec[Option[T]](cqlType, javaType)\n    with VersionAgnostic[Option[T]] {\n  def this(innerCodec: TypeCodec[T]) {\n    this(innerCodec.getCqlType, TypeTokens.optionOf(innerCodec.getJavaType), innerCodec)\n  }\n  override def serialize(value: Option[T], protocolVersion: ProtocolVersion): ByteBuffer =\n    if (value.isEmpty) OptionCodec.empty.duplicate else innerCodec.serialize(value.get, protocolVersion)\n  override def deserialize(bytes: ByteBuffer, protocolVersion: ProtocolVersion): Option[T] =\n    if (bytes == null || bytes.remaining() == 0) None else Option(innerCodec.deserialize(bytes, protocolVersion))\n  override def format(value: Option[T]): String =\n    if (value.isEmpty) \"NULL\" else innerCodec.format(value.get)\n  override def parse(value: String): Option[T] =\n    if (value == null || value.isEmpty || value.equalsIgnoreCase(\"NULL\")) None else Option(innerCodec.parse(value))\n}\nobject OptionCodec {\n  private val empty = ByteBuffer.allocate(0)\n  def apply[T](innerCodec: TypeCodec[T]): OptionCodec[T] =\n    new OptionCodec[T](innerCodec)\n  import scala.reflect.runtime.universe._\n  def apply[T](implicit innerTag: TypeTag[T]): OptionCodec[T] = {\n    val innerCodec = TypeConversions.toCodec(innerTag.tpe).asInstanceOf[TypeCodec[T]]\n    apply(innerCodec)\n  }\n}\n</pre>\n<p>And voilà! As you can see, the class body is very simple (its\ncompanion object is not very exciting at this point either, but we\nwill see later how it could do more than just mirror the class\nconstructor). Its main purpose when deserializing/parsing is to\ndetect CQL <code>null</code>s and return <code>None</code> right\naway, <em>without even having to interrogate the inner codec</em>,\nand when serializing/formatting, intercept <code>None</code> so\nthat it can be immediately converted back to an empty\n<code>ByteBuffer</code> (the native protocol's representation of\n<code>null</code>).</p>\n<p>We can now combine our two codecs together,\n<code>IntCodec</code> and <code>OptionCodec</code>, and compose a\n<code>TypeCodec[Option[Int]]</code>:</p>\n<pre class=\"brush: scala; gutter: true; title: ; notranslate\">\nimport com.datastax.driver.core._\nval codec: TypeCodec[Option[Int]] = OptionCodec(IntCodec)\nassert(codec.deserialize(ByteBuffer.allocate(0), ProtocolVersion.V4).isEmpty)\nassert(codec.deserialize(ByteBuffer.allocate(4), ProtocolVersion.V4).isDefined)\n</pre>\n<h3>The problem with TypeTokens</h3>\n<p>Let's sum up what we've got so far: a\n<code>TypeCodec[Option[Int]]</code> that is the perfect match for\nCQL <code>int</code>s. But how to use it?</p>\n<p>There is nothing really particular with this codec and it is\nperfectly compatible with the Java driver. You can use it\nexplicitly, which is probably the simplest way:</p>\n<pre class=\"brush: scala; gutter: true; title: ; notranslate\">\nimport com.datastax.driver.core._\nval codec: TypeCodec[Option[Int]] = OptionCodec(IntCodec)\nval row: Row = ??? // some CQL query containing an int column\nval v: Option[Int] = row.get(0, codec)\n</pre>\n<p>But your application is certainly more complex than that, and\nyou would like to register your codec beforehand so that it gets\ntransparently used afterwards:</p>\n<pre class=\"brush: scala; gutter: true; title: ; notranslate\">\nimport com.datastax.driver.core._\n// first\nval codec: TypeCodec[Option[Int]] = OptionCodec(IntCodec)\ncluster.getConfiguration.getCodecRegistry.register(codec)\n// then\nval row: Row = ??? // some CQL query containing an int column\nval v: Option[Int] = row.get(0, ???) // How to get a TypeToken[Option[Int]]?\n</pre>\n<p>Well, before we can actually do that, we first need to solve one\nproblem: the <code>Row.get</code> method comes in a few overloaded\nflavors, and the most flavory ones accept a <a href=\"https://google.github.io/guava/releases/21.0/api/docs/com/google/common/reflect/TypeToken.html\">\n<code>TypeToken</code></a> argument; let's learn how to use them in\nScala.</p>\n<p>The Java Driver API, for historical reasons — but also, let's be\nhonest, due to the lack of alternatives – makes extensive usage of\n<a href=\"https://github.com/google/guava/wiki/ReflectionExplained\">Guava's\nTypeToken API</a> (if you are not familiar with the <a href=\"http://gafter.blogspot.fr/2006/12/super-type-tokens.html\">type\ntoken pattern</a> you might want to stop and read about it\nfirst).</p>\n<p>Scala has its own interpretation of the same reflective pattern,\nnamed <em><a href=\"http://docs.scala-lang.org/overviews/reflection/typetags-manifests.html\">\ntype tags</a></em>. Both APIs pursue identical goals – to\nconvey compile-time type information to the runtime – through\nvery different roads. Unfortunately, it's all but an easy path to\ntravel from one to the other, simply because there is <a href=\"https://stackoverflow.com/questions/28569619/get-java-reflection-representation-of-scala-type\">\nno easy bridge</a> between <code>java.lang.Type</code> and Scala's\n<a href=\"http://www.scala-lang.org/api/current/scala-reflect/scala/reflect/api/Types%24Type.html\">\nType</a>.</p>\n<p>Hopefully, all is not lost. As a matter of fact, creating a\nfull-fledged conversion service between both APIs is not a\npre-requisite: it turns out that Guava's <code>TypeToken</code>\nworks pretty well in Scala, and most classes get resolved just\nfine. <code>TypeToken</code>s in Scala are just a bit cumbersome to\nuse, and quite error-prone when instantiated, but that's something\nthat a helper object can facilitate.</p>\n<p>We are not going to dive any deeper in the troubled waters of\nScala reflection (well, at least not until the last chapter of this\ntutorial). It suffices to assume that the helper object we\nmentioned above really <a href=\"https://github.com/datastax/java-driver-scala-extras/blob/master/src/main/scala/com/datastax/driver/extras/codecs/scala/TypeTokens.scala\">\nexists</a>, and that it does the job of creating\n<code>TypeToken</code> instances while at the same time sparing the\ndeveloper the boiler-plate code that this operation usually\nincurs.</p>\n<p>Now we can resume our example and complete our code that reads a\nCQL <code>int</code> into a Scala <code>Option[Int]</code>, in the\nmost transparent way:</p>\n<pre class=\"brush: scala; gutter: true; title: ; notranslate\">\nimport com.datastax.driver.core._\nval tt = TypeTokens.optionOf(TypeTokens.int) // creates a TypeToken[Option[Int]]\nval row: Row = ??? // some CQL query containing an int column\nval v: Option[Int] = row.get(0, tt) \n</pre>\n<h3>Dealing with Collections</h3>\n<p>Another common friction point between Scala and the Java driver\nis the handling of CQL collections.</p>\n<p>Of course, the driver has built-in support for CQL collections;\nbut obviously, these map to typical Java collection types: CQL\n<code>list</code> maps to <code>java.util.List</code> (implemented\nby <code>java.util.ArrayList</code>), CQL <code>set</code> to\n<code>java.util.Set</code> (implemented by\n<code>java.util.LinkedHashSet</code>) and CQL <code>map</code> to\n<code>java.util.Map</code> (implemented by\n<code>java.util.HashMap</code>).</p>\n<p>This leaves Scala developers with two inglorious options:</p>\n<ol><li>Use the implicit <a href=\"http://www.scala-lang.org/api/current/scala/collection/JavaConverters%24.html\">\n<code>JavaConverters</code></a> object and deal with – gasp! –\nmutable collections in their code;</li>\n<li>Deal with custom Java-to-Scala conversion in their code, and\nface the consequences of conversion overhead (this is the choice\nmade by the already-mentioned Spark Connector for Apache\nCassandra®, because it has a very rich set of converters\navailable).</li>\n</ol><p>All of this could be avoided if CQL collection types were\ndirectly deserialized into Scala immutable collections.</p>\n<p>Meet <code>SeqCodec</code>, our third Scala codec in this\ntutorial:</p>\n<pre class=\"brush: scala; collapse: true; gutter: true; light: false; title: ; toolbar: true; notranslate\">\nimport java.nio.ByteBuffer\nimport com.datastax.driver.core.CodecUtils.{readSize, readValue}\nimport com.datastax.driver.core._\nimport com.datastax.driver.core.exceptions.InvalidTypeException\nclass SeqCodec[E](eltCodec: TypeCodec[E])\n  extends TypeCodec[Seq[E]](\n    DataType.list(eltCodec.getCqlType),\n    TypeTokens.seqOf(eltCodec.getJavaType))\n    with ImplicitVersion[Seq[E]] {\n  override def serialize(value: Seq[E], protocolVersion: ProtocolVersion): ByteBuffer = {\n    if (value == null) return null\n    val bbs: Seq[ByteBuffer] = for (elt &lt;- value) yield {\n      if (elt == null) throw new NullPointerException(\"List elements cannot be null\")\n      eltCodec.serialize(elt, protocolVersion)\n    }\n    CodecUtils.pack(bbs.toArray, value.size, protocolVersion)\n  }\n  override def deserialize(bytes: ByteBuffer, protocolVersion: ProtocolVersion): Seq[E] = {\n    if (bytes == null || bytes.remaining == 0) return Seq.empty[E]\n    val input: ByteBuffer = bytes.duplicate\n    val size: Int = readSize(input, protocolVersion)\n    for (_ &lt;- 1 to size) yield eltCodec.deserialize(readValue(input, protocolVersion), protocolVersion)\n  }\n  override def format(value: Seq[E]): String = {\n    if (value == null) \"NULL\" else '[' + value.map(e =&gt; eltCodec.format(e)).mkString(\",\") + ']'\n  }\n  override def parse(value: String): Seq[E] = {\n    if (value == null || value.isEmpty || value.equalsIgnoreCase(\"NULL\")) return Seq.empty[E]\n    var idx: Int = ParseUtils.skipSpaces(value, 0)\n    if (value.charAt(idx) != '[') throw new InvalidTypeException( s\"\"\"Cannot parse list value from \"$value\", at character $idx expecting '[' but got '${value.charAt(idx)}'\"\"\")\n    idx = ParseUtils.skipSpaces(value, idx + 1)\n    val seq = Seq.newBuilder[E]\n    if (value.charAt(idx) == ']') return seq.result\n    while (idx &lt; value.length) {\n      val n = ParseUtils.skipCQLValue(value, idx)\n      seq += eltCodec.parse(value.substring(idx, n))\n      idx = n\n      idx = ParseUtils.skipSpaces(value, idx)\n      if (value.charAt(idx) == ']') return seq.result\n      if (value.charAt(idx) != ',') throw new InvalidTypeException( s\"\"\"Cannot parse list value from \"$value\", at character $idx expecting ',' but got '${value.charAt(idx)}'\"\"\")\n      idx = ParseUtils.skipSpaces(value, idx + 1)\n    }\n    throw new InvalidTypeException( s\"\"\"Malformed list value \"$value\", missing closing ']'\"\"\")\n  }\n  override def accepts(value: AnyRef): Boolean = value match {\n    case seq: Seq[_] =&gt; if (seq.isEmpty) true else eltCodec.accepts(seq.head)\n    case _ =&gt; false\n  }\n}\nobject SeqCodec {\n  def apply[E](eltCodec: TypeCodec[E]): SeqCodec[E] = new SeqCodec[E](eltCodec)\n}\n</pre>\n<p>(Of course, we are talking here about\n<code>scala.collection.immutable.Seq</code>.)</p>\n<p>The code above is still vaguely ressemblant to the <a href=\"https://github.com/datastax/java-driver/blob/3.3.0/driver-core/src/main/java/com/datastax/driver/core/TypeCodec.java#L1759-L1864\">\nequivalent Java code</a>, and not very interesting <em>per se</em>;\nthe <code>parse</code> method in particular is not exactly a feast\nfor the eyes, but there's little we can do about it.</p>\n<p>In spite of its modest body, this codec allows us to compose a\nmore interesting <code>TypeCodec[Seq[Option[Int]]]</code> that can\nconvert a CQL <code>list&lt;int&gt;</code> <em>directly</em> into a\n<code>scala.collection.immutable.Seq[Option[Int]]</code>:</p>\n<pre class=\"brush: scala; gutter: true; title: ; notranslate\">\nimport com.datastax.driver.core._\ntype Seq[+A] = scala.collection.immutable.Seq[A]\nval codec: TypeCodec[Seq[Int]] = SeqCodec(OptionCodec(IntCodec))\nval l = List(Some(1), None)\nassert(codec.deserialize(codec.serialize(l, ProtocolVersion.V4), ProtocolVersion.V4) == l)\n</pre>\n<p>Some remarks about this codec:</p>\n<ol><li>This codec is just for the immutable <code>Seq</code> type. It\ncould be generalized into an <code>AbstractSeqCodec</code> in order\nto accept other mutable or immutable sequences. If you want to know\nhow it would look, <a href=\"https://github.com/datastax/java-driver-scala-extras/blob/master/src/main/scala/com/datastax/driver/extras/codecs/scala/AbstractSeqCodec.scala\">\nthe answer is here</a>.</li>\n<li>Ideally, <code>TypeCodec[T]</code> should have been made\ncovariant in <code>T</code>, the type handled by the codec (i.e.\n<code>TypeCodec[+T]</code>); unfortunately, this is not possible in\nJava, so <code>TypeCodec[T]</code> is in practice invariant in\n<code>T</code>. This is a bit frustrating for Scala implementors,\nas they need to choose the best upper bound for <code>T</code>, and\nstick to it for both input and output operations, just like we did\nabove.</li>\n<li>Similar codecs can be created to map CQL <code>set</code>s to\n<a href=\"http://www.scala-lang.org/api/current/scala/collection/immutable/Set.html\">\n<code>Set</code></a>s and CQL <code>map</code>s to <a href=\"http://www.scala-lang.org/api/current/scala/collection/immutable/Map.html\">\n<code>Map</code></a>s; again, we leave this as an exercise to the\nuser (and again, <a href=\"https://github.com/datastax/java-driver-scala-extras/blob/master/src/main/scala/com/datastax/driver/extras/codecs/scala/MapCodec.scala\">\nit is possible to cheat</a>).</li>\n</ol><h3>Dealing with Tuples</h3>\n<p>Scala tuples are an appealing target for CQL tuples.</p>\n<p>The Java driver does have a built-in codec for CQL tuples; but\nit translates them into <a href=\"http://docs.datastax.com/en/drivers/java/3.3/com/datastax/driver/core/TupleValue.html\">\n<code>TupleValue</code></a> instances, which are unfortunately of\nlittle help for creating Scala tuples.</p>\n<p>Luckily enough, <code>TupleCodec</code> inherits from <a href=\"http://docs.datastax.com/en/drivers/java/3.3/com/datastax/driver/core/TypeCodec.AbstractTupleCodec.html\">\n<code>AbstractTupleCodec</code></a>, a class that has been designed\nexactly with that purpose in mind: to be extended by developers\nwanting to map CQL tuples to more meaningful types than\n<code>TupleValue</code>.</p>\n<p>As a matter of fact, it is extremely simple to craft a codec for\n<a href=\"https://www.scala-lang.org/api/current/scala/Tuple2.html\"><code>Tuple2</code></a>\nby extending <code>AbstractTupleCodec</code>:</p>\n<pre class=\"brush: scala; collapse: true; gutter: true; light: false; title: ; toolbar: true; notranslate\">\nclass Tuple2Codec[T1, T2](\n    cqlType: TupleType, javaType: TypeToken[(T1, T2)],\n    eltCodecs: (TypeCodec[T1], TypeCodec[T2]))\n  extends AbstractTupleCodec[(T1, T2)](cqlType, javaType)\n    with ImplicitVersion[(T1, T2)] {\n  def this(eltCodec1: TypeCodec[T1], eltCodec2: TypeCodec[T2])(implicit protocolVersion: ProtocolVersion, codecRegistry: CodecRegistry) {\n    this(\n      TupleType.of(protocolVersion, codecRegistry, eltCodec1.getCqlType, eltCodec2.getCqlType),\n      TypeTokens.tuple2Of(eltCodec1.getJavaType, eltCodec2.getJavaType),\n      (eltCodec1, eltCodec2)\n    )\n  }\n  {\n    val componentTypes = cqlType.getComponentTypes\n    require(componentTypes.size() == 2, s\"Expecting TupleType with 2 components, got ${componentTypes.size()}\")\n    require(eltCodecs._1.accepts(componentTypes.get(0)), s\"Codec for component 1 does not accept component type: ${componentTypes.get(0)}\")\n    require(eltCodecs._2.accepts(componentTypes.get(1)), s\"Codec for component 2 does not accept component type: ${componentTypes.get(1)}\")\n  }\n  override protected def newInstance(): (T1, T2) = null\n  override protected def serializeField(source: (T1, T2), index: Int, protocolVersion: ProtocolVersion): ByteBuffer = index match {\n    case 0 =&gt; eltCodecs._1.serialize(source._1, protocolVersion)\n    case 1 =&gt; eltCodecs._2.serialize(source._2, protocolVersion)\n  }\n  override protected def deserializeAndSetField(input: ByteBuffer, target: (T1, T2), index: Int, protocolVersion: ProtocolVersion): (T1, T2) = index match {\n    case 0 =&gt; Tuple2(eltCodecs._1.deserialize(input, protocolVersion), null.asInstanceOf[T2])\n    case 1 =&gt; target.copy(_2 = eltCodecs._2.deserialize(input, protocolVersion))\n  }\n  override protected def formatField(source: (T1, T2), index: Int): String = index match {\n    case 0 =&gt; eltCodecs._1.format(source._1)\n    case 1 =&gt; eltCodecs._2.format(source._2)\n  }\n  override protected def parseAndSetField(input: String, target: (T1, T2), index: Int): (T1, T2) = index match {\n    case 0 =&gt; Tuple2(eltCodecs._1.parse(input), null.asInstanceOf[T2])\n    case 1 =&gt; target.copy(_2 = eltCodecs._2.parse(input))\n  }\n}\nobject Tuple2Codec {\n  def apply[T1, T2](eltCodec1: TypeCodec[T1], eltCodec2: TypeCodec[T2]): Tuple2Codec[T1, T2] =\n    new Tuple2Codec[T1, T2](eltCodec1, eltCodec2)\n}\n</pre>\n<p>A very similar codec for <a href=\"https://www.scala-lang.org/api/current/scala/Tuple3.html\"><code>Tuple3</code></a>\ncan be found <a href=\"https://github.com/datastax/java-driver-scala-extras/blob/master/src/main/scala/com/datastax/driver/extras/codecs/scala/Tuple3Codec.scala\">\nhere</a>. Extending this principle to <code>Tuple4</code>,\n<code>Tuple5</code>, etc. is straightforward and left for the\nreader as an exercise.</p>\n<h3>Going incognito with implicits</h3>\n<p>The careful reader noticed that <code>Tuple2Codec</code>'s\nconstructor takes two implicit arguments: <a href=\"http://docs.datastax.com/en/drivers/java/3.3/com/datastax/driver/core/CodecRegistry.html\">\n<code>CodecRegistry</code></a> and <a href=\"http://docs.datastax.com/en/drivers/java/3.3/com/datastax/driver/core/CodecRegistry.html\">\n<code>ProtocolVersion</code></a>. They are omnipresent in the\n<code>TypeCodec</code> API and hence, good candidates for implicit\narguments – and besides, both have nice default values. To make the\ncode above compile, simply put in your scope something along the\nlines of:</p>\n<pre class=\"brush: scala; gutter: true; title: ; notranslate\">\nobject Implicits {\n  implicit val protocolVersion = ProtocolVersion.NEWEST_SUPPORTED\n  implicit val codecRegistry = CodecRegistry.DEFAULT_INSTANCE\n}\n</pre>\n<p>Speaking of implicits, let's now see how we can simplify our\ncodecs by adding a pinch of those. Let's take a look at our first\ntrait in this tutorial:</p>\n<pre class=\"brush: scala; gutter: true; title: ; notranslate\">\ntrait VersionAgnostic[T] {  this: TypeCodec[T] =&gt;\n  def serialize(value: T)(implicit protocolVersion: ProtocolVersion, marker: ClassTag[T]): ByteBuffer = \n    this.serialize(value, protocolVersion)\n  def deserialize(bytes: ByteBuffer)(implicit protocolVersion: ProtocolVersion, marker: ClassTag[T]): T = \n    this.deserialize(bytes, protocolVersion)\n}\n</pre>\n<p>This trait basically creates two overloaded methods,\n<code>serialize</code> and <code>deserialize</code>, which will\ninfer the appropriate protocol version to use and forward the call\nto the relevant method (the marker argument is just the usual trick\nto work around erasure).</p>\n<p>We can now mix-in this trait with an existing codec, and then\navoid passing the protocol version to every call to\n<code>serialize</code> or <code>deserialize</code>:</p>\n<pre class=\"brush: scala; gutter: true; title: ; notranslate\">\nimport Implicits._\nval codec = new SeqCodec(IntCodec) with VersionAgnostic[Seq[Int]]\ncodec.serialize(List(1,2,3))\n</pre>\n<p>We can now go even further and simplify the way codecs are\ncomposed together to create complex codecs. What if, instead of\nwriting <code>SeqCodec(OptionCodec(IntCodec))</code>, we could\nsimply write <code>SeqCodec[Option[Int]]</code>? To achieve that,\nlet's enhance the companion object of <code>SeqCodec</code> with a\nmore sophisticated <code>apply</code> method:</p>\n<pre class=\"brush: scala; gutter: true; title: ; notranslate\">\nobject SeqCodec {\n  def apply[E](eltCodec: TypeCodec[E]): SeqCodec[E] = new SeqCodec[E](eltCodec)\n  import scala.reflect.runtime.universe._\n  def apply[E](implicit eltTag: TypeTag[E]): SeqCodec[E] = {\n    val eltCodec = ??? // implicit TypeTag -&gt; TypeCodec conversion\n    apply(eltCodec)\n  }\n}\n</pre>\n<p>The second <code>apply</code> method guesses the element type by\nusing implicit <code>TypeTag</code> instances (these are created by\nthe Scala compiler, so you don't need to worry about instantiating\nthem), then locates the appropriate codec for it. We can now\nwrite:</p>\n<pre class=\"brush: scala; title: ; notranslate\">\nval codec = SeqCodec[Option[Int]]\n</pre>\n<p>Elegant, huh? Of course, we need some magic to locate the right\ncodec given a <code>TypeTag</code> instance. Here we need to\nintroduce another helper object, <a href=\"https://github.com/datastax/java-driver-scala-extras/blob/master/src/main/scala/com/datastax/driver/extras/codecs/scala/TypeConversions.scala\">\n<code>TypeConversions</code></a>. Its method <code>toCodec</code>\ntakes a Scala type and, with the help of some pattern matching,\nlocates the most appropriate codec. We refer the interested reader\nto <code>TypeConversions</code> code for more details.</p>\n<p>With the help of <code>TypeConversions</code>, we can now\ncomplete our new <code>apply</code> method:</p>\n<pre class=\"brush: scala; gutter: true; title: ; notranslate\">\ndef apply[E](implicit eltTag: TypeTag[E]): SeqCodec[E] = {\n  val eltCodec = TypeConversions.toCodec[E](eltTag.tpe)\n  apply(eltCodec)\n}\n</pre>\n<p>Note: similar <code>apply</code> methods can be added to other\ncodec companion objects as well.</p>\n<p>It's now time to go really wild, bearing in mind that the\nfollowing features should only be used with caution by expert\nusers.</p>\n<p>If only we could convert Scala's <code>TypeTag</code> instances\ninto Guava's <code>TypeToken</code> ones, <em>and then make them\nimplicit like we did above</em>, we would be able to completely\nabstract away these annoying types and write very concise code,\nsuch as:</p>\n<pre class=\"brush: scala; gutter: true; title: ; notranslate\">\nval statement: BoundStatement = ???\nstatement.set(0, List(1,2,3)) // implicit TypeTag -&gt; TypeToken conversion\nval row: Row = ???\nval list: Seq[Int] = row.get(0) // implicit TypeTag -&gt; TypeToken conversion\n</pre>\n<p>Well, this can be achieved in a few different ways; we are going\nto explore here the so-called <a href=\"https://blog.scalac.io/2017/04/19/typeclasses-in-scala.html\"><em>Type\nClass pattern</em></a>.</p>\n<p>The first step is be to create implicit classes containing \"get\"\nand \"set\" methods that take <code>TypeTag</code> instances instead\nof <code>TypeToken</code> ones; we'll name them\n<code>getImplicitly</code> and <code>setImplicitly</code> to avoid\nname clashes. Let's do it for <code>Row</code> and\n<code>BoundStatement</code>:</p>\n<pre class=\"brush: scala; collapse: true; gutter: true; light: false; title: ; toolbar: true; notranslate\">\nimplicit class RowOps(val self: Row) {\n  def getImplicitly[T](i: Int)(implicit typeTag: TypeTag[T]): T = \n    self.get(i, ???) // implicit TypeTag -&gt; TypeToken conversion\n  def getImplicitly[T](name: String)(implicit typeTag: TypeTag[T]): T =\n    self.get(name, ???) // implicit TypeTag -&gt; TypeToken conversion\n}\nimplicit class BoundStatementOps(val self: BoundStatement) {\n  def setImplicitly[T](i: Int, value: T)(implicit typeTag: TypeTag[T]): BoundStatement =\n    self.set(i, value, ???) // implicit TypeTag -&gt; TypeToken conversion\n  def setImplicitly[T](name: String, value: T)(implicit typeTag: TypeTag[T]): BoundStatement = \n    self.set(name, value, ???) // implicit TypeTag -&gt; TypeToken conversion\n  }\n}\n</pre>\n<p>Remember what we stated at the beginning of this tutorial:\n\"there is no easy bridge between Java types and Scala types\"? Well,\nwe will have to lay one now to cross that river.</p>\n<p>Our helper object <code>TypeConversions</code> has another\nmethod, <code>toJavaType</code>, that does just that. Again,\ndigging into its details is out of the scope of this tutorial, but\nwith this method we can complete our implicit classes as below:</p>\n<pre class=\"brush: scala; gutter: true; title: ; notranslate\">\ndef getImplicitly[T](i: Int)(implicit typeTag: TypeTag[T]): T = \n  val javaType: java.lang.reflect.Type = TypeConversions.toJavaType(typeTag.tpe)\n  self.get(i, TypeToken.of(javaType).wrap().asInstanceOf[TypeToken[T]])\n</pre>\n<p>And we are done!</p>\n<p>Now, by simply placing the above implicit classes into scope, we\nwill be able to write code as concise as:</p>\n<pre class=\"brush: scala; gutter: true; title: ; notranslate\">\nstatement.setImplicitly(0, List(1,2,3)) // implicitly converted to statement.setImplicitly(0, List(1,2,3)) (TypeTag[Seq[Int]]), then\n                                        // implicitly converted to statement.set          (0, List(1,2,3), TypeToken[Seq[Int]])\n</pre>\n<p>When retrieving values, it's a bit more complicated because the\nScala compiler needs some help from the developer to be able to\nfill in the appropriate implicit <code>TypeTag</code> instance; we\ndo so like this:</p>\n<pre class=\"brush: scala; gutter: true; title: ; notranslate\">\nval list = row.getImplicitly[Seq[Int]](0) // implicitly converted to statement.getImplicitly(0) (TypeTag[Seq[Int]]), then\n                                          // implicitly converted to statement.get          (0,  TypeToken[Seq[Int]])\n</pre>\n<p>That's it. We hope that with this tutorial, we could demonstrate\nhow easy it is to create codecs for the Java driver that are\nfirst-class citizens in Scala. Enjoy!</p>\n                    </div>\n                                        <div class=\"post\" readability=\"248\">\n                        \n                        \n                        <p>One of the big challenges people face when starting out working\nwith Cassandra and time series data is understanding the impact of\nhow your write workload will affect your cluster. Writing too\nquickly to a single partition can create hot spots that limit your\nability to scale out. Partitions that get too large can lead to\nissues with repair, streaming, and read performance. Reading from\nthe middle of a large partition carries a lot of overhead, and\nresults in increased GC pressure. Cassandra 4.0 should <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-9754\">improve the\nperformance of large partitions</a>, but it won’t fully solve the\nother issues I’ve already mentioned. For the foreseeable future, we\nwill need to consider their performance impact and plan for them\naccordingly.</p>\n<p>In this post, I’ll discuss a common Cassandra data modeling\ntechnique called <em>bucketing</em>. Bucketing is a strategy that\nlets us control how much data is stored in each partition as well\nas spread writes out to the entire cluster. This post will discuss\ntwo forms of bucketing. These techniques can be combined when a\ndata model requires further scaling. Readers should already be\nfamiliar with the anatomy of a partition and basic CQL\ncommands.</p>\n<p>When we first learn about data modeling with Cassandra, we might\nsee something like the following:</p>\n<div class=\"highlighter-rouge\" readability=\"15\">\n<pre>CREATE TABLE raw_data (\n    sensor text,\n    ts timeuuid,\n    readint int,\n    primary key(sensor, ts)\n) WITH CLUSTERING ORDER BY (ts DESC) \n  AND compaction = {'class': 'TimeWindowCompactionStrategy', \n                    'compaction_window_size': 1, \n                    'compaction_window_unit': 'DAYS'};\n</pre></div>\n<p>This is a great first data model for storing some very simple\nsensor data. Normally the data we collect is more complex than an\ninteger, but in this post we’re going to focus on the keys. We’re\nleveraging <a href=\"http://thelastpickle.com/blog/2016/12/08/TWCS-part1.html\">TWCS</a>\nas our compaction strategy. TWCS will help us deal with the\noverhead of compacting large partitions, which should keep our CPU\nand I/O under control. Unfortunately it still has some significant\nlimitations. If we aren’t using a TTL, as we take in more data, our\npartition size will grow constantly, unbounded. As mentioned above,\nlarge partitions carry significant overhead when repairing,\nstreaming, or reading from arbitrary time slices.</p>\n<p>To break up this big partition, we’ll leverage our first form of\nbucketing. We’ll break our partitions into smaller ones based on\ntime window. The ideal size is going to keep partitions under\n100MB. For example, one partition per sensor per day would be a\ngood choice if we’re storing 50-75MB of data per day. We could just\nas easily use week (starting from some epoch), or month and year as\nlong as the partitions stay under 100MB. Whatever the choice,\nleaving a little headroom for growth is a good idea.</p>\n<p>To accomplish this, we’ll add another component to our partition\nkey. Modifying our earlier data model, we’ll add a <code class=\"highlighter-rouge\">day</code> field:</p>\n<div class=\"highlighter-rouge\" readability=\"17\">\n<pre>CREATE TABLE raw_data_by_day (\nsensor text,\nday text,\nts timeuuid,\nreading int,\nprimary key((sensor, day), ts)\n) WITH CLUSTERING ORDER BY (ts DESC) \n       AND COMPACTION = {'class': 'TimeWindowCompactionStrategy', \n                     'compaction_window_unit': 'DAYS', \n                     'compaction_window_size': 1};\n</pre></div>\n<p>Inserting into the table requires using the date as well as the\n<code class=\"highlighter-rouge\">now()</code> value (you could also\ngenerate a TimeUUID in your application code):</p>\n<div class=\"highlighter-rouge\" readability=\"13\">\n<pre>INSERT INTO raw_data_by_day (sensor, day, ts, reading) \nVALUES ('mysensor', '2017-01-01', now(), 10);\n</pre></div>\n<p>This is one way of limiting the amount of data per partition.\nFor fetching large amounts of data across multiple days, you’ll\nneed to issue one query per day. The nice part about querying like\nthis is we can spread the work over the entire cluster rather than\nasking a single node to perform a lot of work. We can also issue\nthese queries in parallel by relying on the async calls in the\ndriver. The Python driver even has a convenient helper function for\nthis sort of use case:</p>\n<div class=\"language-python highlighter-rouge\" readability=\"29\">\n<pre>from itertools import product\nfrom cassandra.concurrent import execute_concurrent_with_args\ndays = [\"2017-07-01\", \"2017-07-12\", \"2017-07-03\"]  # collecting three days worth of data\nsession  = Cluster([\"127.0.0.1\"]).connect(\"blog\")\nprepared = session.prepare(\"SELECT day, ts, reading FROM raw_data_by_day WHERE sensor = ? and day = ?\")\nargs = product([\"mysensor\"], days) \n# args: ('test', '2017-07-01'), ('test', '2017-07-12'), ('test', '2017-07-03')\n# driver handles concurrency for you\nresults = execute_concurrent_with_args(session, prepared, args)\n# Results:\n#[ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d36750&gt;),\n# ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d36a90&gt;),\n# ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d36550&gt;)]\n</pre></div>\n<p>A variation on this technique is to use a different table per\ntime window. For instance, using a table per month means you’d have\ntwelve tables per year:</p>\n<div class=\"highlighter-rouge\" readability=\"15\">\n<pre>CREATE TABLE raw_data_may_2017 (\n    sensor text,\n    ts timeuuid,\n    reading int,\n    primary key(sensor, ts)\n) WITH COMPACTION = {'class': 'TimeWindowCompactionStrategy', \n                     'compaction_window_unit': 'DAYS', \n                     'compaction_window_size': 1};\n</pre></div>\n<p>This strategy has a primary benefit of being useful for\narchiving and quickly dropping old data. For instance, at the\nbeginning of each month, we could archive last month’s data to HDFS\nor S3 in parquet format, taking advantage of cheap storage for\nanalytics purposes. When we don’t need the data in Cassandra\nanymore, we can simply drop the table. You can probably see there’s\na bit of extra maintenance around creating and removing tables, so\nthis method is really only useful if archiving is a requirement.\nThere are other methods to archive data as well, so this style of\nbucketing may be unnecessary.</p>\n<p>The above strategies focuses on keeping partitions from getting\ntoo big over a long period of time. This is fine if we have a\npredictable workload and partition sizes that have very little\nvariance. It’s possible to be ingesting so much information that we\ncan overwhelm a single node’s ability to write data out, or the\ningest rate is significantly higher for a small percentage of\nobjects. Twitter is a great example, where certain people have tens\nof millions of followers but it’s not the common case. It’s common\nto have a separate code path for these types of accounts where we\nneed massive scale</p>\n<p>The second technique uses multiple partitions at any given time\nto fan out inserts to the entire cluster. The nice part about this\nstrategy is we can use a single partition for low volume, and many\npartitions for high volume.</p>\n<p>The tradeoff we make with this design is on reads we need to use\na scatter gather, which has significantly higher overhead. This can\nmake pagination more difficult, amongst other things. We need to be\nable to track how much data we’re ingesting for each gizmo we have.\nThis is to ensure we can pick the right number of partitions to\nuse. If we use too many buckets, we end up doing a lot of really\nsmall reads across a lot of partitions. Too few buckets, we end up\nwith really large partitions that don’t compact, repair, stream\nwell, and have poor read performance.</p>\n<p>For this example, we’ll look at a theoretical model for someone\nwho’s following a lot of users on a social network like Twitter.\nMost accounts would be fine to have a single partition for incoming\nmessages, but some people / bots might follow millions of\naccounts.</p>\n<p><em>Disclaimer: I have no knowledge of how Twitter is actually\nstoring their data, it’s just an easy example to discuss.</em></p>\n<div class=\"highlighter-rouge\" readability=\"19\">\n<pre>CREATE TABLE tweet_stream (\n    account text,\n    day text,\n    bucket int,\n    ts timeuuid,\n    message text,\n    primary key((account, day, bucket), ts)\n) WITH CLUSTERING ORDER BY (ts DESC) \n         AND COMPACTION = {'class': 'TimeWindowCompactionStrategy', \n                       'compaction_window_unit': 'DAYS', \n                       'compaction_window_size': 1};\n</pre></div>\n<p>This data model extends our previous data model by adding\n<code class=\"highlighter-rouge\">bucket</code> into the partition\nkey. Each day can now have multiple buckets to fetch from. When\nit’s time to read, we need to fetch from all the partitions, and\ntake the results we need. To demonstrate, we’ll insert some data\ninto our partitions:</p>\n<div class=\"highlighter-rouge\" readability=\"43\">\n<pre>cqlsh:blog&gt; insert into tweet_stream (account, day, bucket, ts, message) VALUES ('jon_haddad', '2017-07-01', 0, now(), 'hi');\ncqlsh:blog&gt; insert into tweet_stream (account, day, bucket, ts, message) VALUES ('jon_haddad', '2017-07-01', 1, now(), 'hi2');\ncqlsh:blog&gt; insert into tweet_stream (account, day, bucket, ts, message) VALUES ('jon_haddad', '2017-07-01', 2, now(), 'hi3');\ncqlsh:blog&gt; insert into tweet_stream (account, day, bucket, ts, message) VALUES ('jon_haddad', '2017-07-01', 3, now(), 'hi4');\n</pre></div>\n<p>If we want the ten most recent messages, we can do something\nlike this:</p>\n<div class=\"language-python highlighter-rouge\" readability=\"54\">\n<pre>from itertools import chain\nfrom cassandra.util import unix_time_from_uuid1\nprepared = session.prepare(\"SELECT ts, message FROM tweet_stream WHERE account = ? and day = ? and bucket = ? LIMIT 10\")\n# let's get 10 buckets \npartitions = range(10)\n# [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nargs = product([\"jon_haddad\"], [\"2017-07-01\"], partitions)\nresult = execute_concurrent_with_args(session, prepared, args)\n# [ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1e6d0&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1d710&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1d4d0&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1d950&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1db10&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1dfd0&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1dd90&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1d290&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1e250&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1e490&gt;)]\nresults = [x.result_or_exc for x in result]\n# append all the results together\ndata = chain(*results)\n            \nsorted_results = sorted(data, key=lambda x: unix_time_from_uuid1(x.ts), reverse=True)            \n# newest stuff first\n# [Row(ts=UUID('e1c59e60-7406-11e7-9458-897782c5d96c'), message=u'hi4'),\n#  Row(ts=UUID('dd6ddd00-7406-11e7-9458-897782c5d96c'), message=u'hi3'),\n#  Row(ts=UUID('d4422560-7406-11e7-9458-897782c5d96c'), message=u'hi2'),\n#  Row(ts=UUID('d17dae30-7406-11e7-9458-897782c5d96c'), message=u'hi')]\n</pre></div>\n<p>This example is only using a LIMIT of 10 items, so we can be\nlazy programmers, merge the lists, and then sort them. If we wanted\nto grab a lot more elements we’d want to use a k-way merge\nalgorithm. We’ll come back to that in a future blog post when we\nexpand on this topic.</p>\n<p>At this point you should have a better understanding of how you\ncan distribute your data and requests around the cluster, allowing\nit to scale much further than if a single partition were used. Keep\nin mind each problem is different, and there’s no one size fits all\nsolution.</p>\n                    </div>\n                                        <div class=\"post\" readability=\"36\">\n                        \n                        \n                        <p>Instaclustr is happy to announce the immediate\navailability of <a href=\"http://www.scylladb.com/2017/07/10/scylla-release-version-1-7-2/\" target=\"_blank\" rel=\"noopener\">Scylla 1.7.2</a> through its\n<a href=\"https://www.instaclustr.com/solutions/managed-scylladb/\" target=\"_blank\" rel=\"noopener\">Managed Service</a>. Scylla 1.7.2 is\na significant step forward from the version previously available\nthrough Instaclustr with relevant enhancements\nincluding:</p>\n<ul><li>Support Counters as a native type\n(experimental)</li>\n<li>Upgrade Java tools to match Cassandra 3.0</li>\n<li>Update to CQL 3.3.1 to match Cassandra 2.2\nrelease</li>\n<li>Fetch size auto-tune</li>\n<li>Improves range scans</li>\n<li>Slow Query Tracing</li>\n<li>Thrift support</li>\n<li>Date Tiered Compaction Strategy (DTCS)\nsupport</li>\n<li>Improved Large Partitions Support</li>\n<li>CQL Tracing support</li>\n</ul><p>Instaclustr’s Scylla support is currently in preview\nstatus. However, we are very happy to work with individual\ncustomers to complete application testing and move to full\nproduction SLAs. For more information, please contact <a href=\"mailto:sales@instaclustr.com\" target=\"_blank\" rel=\"noopener\">sales@instaclustr.com</a>.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://www.instaclustr.com/instaclustr-releases-support-scylla-1-7-2/\">\nInstaclustr Releases Support for Scylla 1.7.2</a> appeared first on\n<a rel=\"nofollow\" href=\"https://www.instaclustr.com/\">Instaclustr</a>.</p>\n                    </div>\n                                        <div class=\"post\" readability=\"45\">\n                        \n                        \n                        <p>Instaclustr is pleased to announce the availability of\n<a href=\"https://spark.apache.org/releases/spark-release-2-1-1.html\" target=\"_blank\" rel=\"noopener\">Spark 2.1.1</a> on its managed\nservice. Spark 2.1 provides increased stability and minor feature\nenhancements while providing access to the key benefits of Spark\n2.0 such as:</p>\n<ul><li>Up to 10x performance improvements</li>\n<li>Structured Streaming API</li>\n<li>ANSI SQL parser for Spark SQL</li>\n<li>Streamlined APIs</li>\n</ul><p><a href=\"https://www.instaclustr.com/solutions/managed-apache-spark/\" target=\"_blank\" rel=\"noopener\">Instaclustr’s Spark Managed\nService</a> offering focuses providing managed solution for people\nwanting to run Spark over their Cassandra cluster. It\nprovides:</p>\n<ul><li>automated provisioning of Cassandra cluster with\nco-located Spark workers for maximum processing\nefficiency;</li>\n<li>automated provisioning of Spark Jobserver and Apache\nZeppelin to provide simple REST and interactive interfaces for\nworking with your Spark cluster;</li>\n<li>high availability configuration of Spark using\nZookeeper;</li>\n<li>integrated Spark management and monitoring through the\nInstaclustr Console; and</li>\n<li>the same great 24×7 monitoring and support we provide for\nour Cassandra customers.</li>\n</ul><p>Instaclustr’s focus is the provision of managed,\nopen-source components for reliability at scale and our Spark\noffering is designed to provide the best solution for those looking\nto utilize Spark as a component of their overall application\narchitecture, particular where you’re also using Cassandra to store\nyour data.</p>\n<p>For more information about Instaclustr’s Spark offering,\nor to initiate a proof of concept evaluation, please contact\n<a href=\"mailto:sales@instaclustr.com\" target=\"_blank\" rel=\"noopener\">sales@instaclustr.com</a>.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://www.instaclustr.com/instaclustr-managed-service-apache-spark-2-1-1-released/\">\nInstaclustr Managed Service for Apache Spark 2.1.1 Released</a>\nappeared first on <a rel=\"nofollow\" href=\"https://www.instaclustr.com/\">Instaclustr</a>.</p>\n                    </div>\n                                        <div class=\"post\" readability=\"175\">\n                        \n                        \n                        <h2><a href=\"https://www.datastax.com/wp-content/uploads/9999/07/fig1.png\"><img class=\"alignnone size-full wp-image-57441 dxDevBlog_img700px\" src=\"https://www.datastax.com/wp-content/uploads/9999/07/fig1.png\" alt=\"\" srcset=\"https://www.datastax.com/wp-content/uploads/9999/07/fig1.png 1600w, https://www.datastax.com/wp-content/uploads/9999/07/fig1-250x111.png 250w, https://www.datastax.com/wp-content/uploads/9999/07/fig1-768x340.png 768w, https://www.datastax.com/wp-content/uploads/9999/07/fig1-700x310.png 700w, https://www.datastax.com/wp-content/uploads/9999/07/fig1-120x53.png 120w\" sizes=\"(max-width: 1600px) 100vw, 1600px\"/></a></h2>\n<p><a href=\"https://docs.datastax.com/en/dse/5.1/dse-dev/datastax_enterprise/spark/byosIntro.html\">\nBring Your Own Spark</a> (BYOS) is a feature of DSE Analytics\ndesigned to connect from external Apache Spark™ systems to DataStax\nEnterprise with minimal configuration efforts. In this post we\nintroduce how to configure BYOS and show some common use cases.</p>\n<p>BYOS extends the <a href=\"https://github.com/datastax/spark-cassandra-connector/\">DataStax\nSpark Cassandra Connector</a> with DSE security features such as\nKerberos and SSL authentication. It also includes drivers to access\nthe DSE Cassandra File System (CFS) and DSE File System (DSEFS) in\n5.1.</p>\n<p>There are three parts of the deployment:</p>\n<ul><li>&lt;dse_home&gt;clients/dse-byos_2.10-5.0.6.jar is a fat jar.\nIt includes everything you need to connect the DSE cluster: Spark\nCassandra Connector with dependencies, DSE security connection\nimplementation, and CFS driver.</li>\n<li>'dse client-tool configuration byos-export' tool help to\nconfigure external Spark cluster to connect to the DSE</li>\n<li>'dse client-tool spark sql-schema' tool generates\nSparkSQL-compatible scripts to create external tables for all or\npart of DSE tables in SparkSQL metastore.</li>\n</ul><p>HDP 2.3+ and CDH 5.3+ are the only Hadoop distributions which\nsupport Java 8 officially and which have been tested with BYOS in\nDSE 5.0 and 5.1.</p>\n<h2>Pre-requisites:</h2>\n<p>There is installed and configured a Hadoop or standalone Spark\nsystem and you have access to at least one host on the cluster with\na preconfigured Spark client. Let’s call it spark-host. The Spark\ninstallation should be pointed to by $SPARK_HOME.</p>\n<p>There is installed and configured a DSE cluster and you have\naccess to it. Let’s call it dse-host. I will assume you have a\ncassandra_keyspace.exampletable C* table created on it.The DSE is\nlocated at $DSE_HOME.</p>\n<p>DSE supports Java 8 only. Make sure your Hadoop, Yarn and Spark\nuse Java 8. See your Hadoop distro documentation on how to upgrade\nJava version (<a href=\"http://www.cloudera.com/documentation/enterprise/5-4-x/topics/cm_ig_java_home_location.html?scroll=cmig_topic_16\">CDH</a>,\n<a href=\"http://docs.hortonworks.com/HDPDocuments/Ambari-2.1.2.1/bk_ambari_reference_guide/content/ch_changing_the_jdk_version_on_an_existing_cluster.html\">\nHDP</a>).</p>\n<h2>Prepare the configuration file</h2>\n<p>On dse-host run:</p>\n<pre class=\"dxDBcode_block\">\n$DSE_HOME/bin/dse client-tool configuration byos-export byos.conf\n</pre>\n<p>It will store DSE client connection configuration in\nSpark-compatible format into byos.conf.</p>\n<p>Note: if SSL or password authentication is enabled, additional\nparameters needed to be stored. See dse client-tool documentation\nfor details.</p>\n<p>Copy the byos.conf to spark-host.</p>\n<p>On spark-host append the ~/byos.conf file to the Spark default\nconfiguration</p>\n<pre class=\"dxDBcode_block\">\ncat byos.conf &gt;&gt; $SPARK_HOME/conf/conf/spark-defaults.conf\n</pre>\n<p>Note: If you expect conflicts with spark-defaults.conf, the\nbyos-export tool can merge properties itself; refer to the\ndocumentation for details.</p>\n<h2>Prepare C* to SparkSQL mapping (optional)</h2>\n<p>On dse-host run:</p>\n<pre class=\"dxDBcode_block\">\ndse client-tool spark sql-schema -all &gt; cassandra_maping.sql\n</pre>\n<p>That will create cassandra_maping.sql with spark-sql compatible\ncreate table statements.</p>\n<p>Copy the file to spark-host.</p>\n<h2>Run Spark</h2>\n<p>Copy $DSE_HOME/dse/clients/dse-byos-5.0.0-all.jar to the\nspark-host</p>\n<p>Run Spark with the jar.</p>\n<pre class=\"dxDBcode_block\">\n$SPARK_HOME/bin/spark-shell --jars dse-byos-5.0.0-all.jar\nscala&gt; import com.datastax.spark.connector._\nscala&gt; sc.cassandraTable(“cassandra_keyspace”, \"exampletable\" ).collect\n</pre>\n<p>Note: External Spark can not connect to DSE Spark master and\nsubmit jobs. Thus you can not point it to DSE Spark master.</p>\n<h2>SparkSQL</h2>\n<p>BYOS does not support the legacy Cassandra-to-Hive table mapping\nformat. The spark data frame external table format should be used\nfor mapping: <a href=\"https://github.com/datastax/spark-cassandra-connector/blob/master/doc/14_data_frames.md\">\nhttps://github.com/datastax/spark-cassandra-connector/blob/master/doc/14_data_frames.md</a></p>\n<p>DSE provides a tool to auto generate the mapping for external\nspark metastore: dse client-tool spark sql-schema</p>\n<p>On the dse-host run:</p>\n<pre class=\"dxDBcode_block\">\ndse client-tool spark sql-schema -all &gt; cassandra_maping.sql\n</pre>\n<p>That will create cassandra_maping.sql with spark-sql compatible\ncreate table statements</p>\n<p>Copy the file to spark-host</p>\n<p>Create C* tables mapping in spark meta-store</p>\n<pre class=\"dxDBcode_block\">\n$SPARK_HOME/bin/spark-sql--jars dse-byos-5.0.0-all.jar -f cassandra_maping.sql\n</pre>\n<p>Tables are now ready to use in both SparkSQL and Spark\nshell.</p>\n<pre class=\"dxDBcode_block\">\n$SPARK_HOME/bin/spark-sql --jars dse-byos-5.0.0-all.jar\nspark-sql&gt; select * from cassandra_keyspace.exampletable\n</pre>\n<pre class=\"dxDBcode_block\">\n$SPARK_HOME/bin/spark-shell —jars dse-byos-5.0.0-all.jar\nscala&gt;sqlConext.sql(“select * from cassandra_keyspace.exampletable\");\n</pre>\n<h2>Access external HDFS from dse spark</h2>\n<p>DSE is built with Hadoop 2.7.1 libraries. So it is able to\naccess any Hadoop 2.x HDFS file system.</p>\n<p>To get access you need just proved full path to the file in\nSpark commands:</p>\n<pre class=\"dxDBcode_block\">\nscala&gt; sc.textFile(\"hdfs://&lt;namenode_host&gt;/&lt;path to the file&gt;\")\n</pre>\n<p>To get a namenode host you can run the following command on the\nHadoop cluster:</p>\n<pre class=\"dxDBcode_block\">\nhdfs getconf -namenodes\n</pre>\n<p>If the Hadoop cluster has custom configuration or enabled\nkerberos security, the configuration should be copied into the DSE\nHadoop config directory:</p>\n<pre class=\"dxDBcode_block\">\ncp /etc/hadoop/conf/hdfs-site.xml $DSE_HOME/resources/hadoop2-client/conf/hdfs-site.xml\n</pre>\n<p>Make sure that firewall does not block the following HDFS data\nnode and name node ports:</p>\n<table class=\"dxblogtableA1_css\"><tbody readability=\"0\"><tr readability=\"1\"><td>NameNode metadata service</td>\n<td>8020/9000</td>\n</tr><tr><td>DataNode</td>\n<td>50010,50020</td>\n</tr></tbody></table>\n<h2>Security configuration</h2>\n<h3><b>SSL</b></h3>\n<p>Start with truststore generation with DSE nodes certificates. If\nclient certificate authentication is enabled\n(require_client_auth=true), client keystore will be needed.</p>\n<p>More info on certificate generation:</p>\n<p><a href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/security/secureSSLCertificates_t.html\">\nhttps://docs.datastax.com/en/cassandra/2.1/cassandra/security/secureSSLCertificates_t.html</a></p>\n<p>Copy both file to each Spark node on the same location. The\nSpark '--files' parameter can be used for the coping in Yarn\ncluster.</p>\n<p>Use byos-export parameters to add store locations, type and\npasswords into byos.conf.</p>\n<pre class=\"dxDBcode_block\">\ndse client-tool configuration byos-export --set-truststore-path .truststore --set-truststore-password \npassword --set-keystore-path .keystore --set-keystore-password password byos.conf\n</pre>\n<p>Yarn example:</p>\n<pre class=\"dxDBcode_block\">\nspark-shell --jars byos.jar --properties-file byos.conf --files .truststore,.keystore\n</pre>\n<h3><b>Kerberos</b></h3>\n<p><a href=\"https://www.datastax.com/wp-content/uploads/9999/07/fig2.png\"><img class=\"alignnone size-full wp-image-57443 dxDevBlog_img450px\" src=\"https://www.datastax.com/wp-content/uploads/9999/07/fig2.png\" alt=\"\" srcset=\"https://www.datastax.com/wp-content/uploads/9999/07/fig2.png 972w, https://www.datastax.com/wp-content/uploads/9999/07/fig2-250x234.png 250w, https://www.datastax.com/wp-content/uploads/9999/07/fig2-768x719.png 768w, https://www.datastax.com/wp-content/uploads/9999/07/fig2-700x655.png 700w, https://www.datastax.com/wp-content/uploads/9999/07/fig2-120x112.png 120w\" sizes=\"(max-width: 972px) 100vw, 972px\"/></a></p>\n<p>Make sure your Spark client host (where spark driver will be\nrunning) has kerberos configured and C* nodes DNS entries are\nconfigured properly. See more details in the <a href=\"https://docs.datastax.com/en/datastax_enterprise/4.8/datastax_enterprise/sec/secSetKerbEnv.html\">\nSpark Kerberos documentation</a>.</p>\n<p>If the Spark cluster mode deployment will be used or no Kerberos\nconfigured on the spark client host use \"Token based\nauthentication\" to access Kerberized DSE cluster.</p>\n<p>byos.conf file will contains all necessary Kerberos principal\nand service names exported from the DSE.</p>\n<p>The JAAS configuration file with the following options need to\nbe copied from DSE node or created manually on the Spark client\nnode only and stored at $HOME/.java.login.config file.</p>\n<pre class=\"dxDBcode_block\">\nDseClient {\n       com.sun.security.auth.module.Krb5LoginModule required\n       useTicketCache=true\n       renewTGT=true;\n};\n</pre>\n<p>Note: If a custom file location is used, Spark driver property\nneed to be set pointing to the location of the file.</p>\n<pre class=\"dxDBcode_block\">\n--conf 'spark.driver.extraJavaOptions=-Djava.security.auth.login.config=login_config_file'\n</pre>\n<p>BYOS authenticated by Kerberos and request C* token for\nexecutors authentication. The token authentication should be\nenabled in DSE. the spark driver will automatically cancel the\ntoken on exit</p>\n<p>Note: the CFS root should be passed to the Spark to request\ntoken with:</p>\n<pre class=\"dxDBcode_block\">\n--conf spark.yarn.access.namenodes=cfs://dse_host/\n</pre>\n<h3>Spark Thrift Server with Kerberos</h3>\n<p>It is possible to authenticate services with keytab. Hadoop/YARN\nservices already preconfigured with keytab files and kerberos useк\nif kerberos was enabled in the hadoop. So you need to grand\npermissions to these users. Here is example for hive user</p>\n<pre class=\"dxDBcode_block\">\ncqlsh&gt; create role 'hive/hdp0.dc.datastax.com@DC.DATASTAX.COM' with LOGIN = true;\n</pre>\n<p>Now you can login as a hive kerberos user, merge configs and\nstart Spark thrift server. It will be able to query DSE data:</p>\n<pre class=\"dxDBcode_block\">\n#&gt; kinit -kt /etc/security/keytabs/hive.service.keytab \\ hive/hdp0.dc.datastax.com@DC.DATASTAX.COM\n#&gt; cat /etc/spark/conf/spark-thrift-sparkconf.conf byos.conf &gt; byos-thrift.conf\n#&gt; start-thriftserver.sh --properties-file byos-thrift.conf --jars dse-byos*.jar\n</pre>\n<p>Connect to it with beeline for testing:</p>\n<pre class=\"dxDBcode_block\">\n#&gt; kinit\n#&gt; beeline -u 'jdbc:hive2://hdp0:10015/default;principal=hive/_HOST@DC.DATASTAX.COM'\n</pre>\n<h3>Token based authentication</h3>\n<p>Note: This approach is less secure than Kerberos one, use it\nonly in case kerberos is not enabled on your spark cluster.</p>\n<p>DSE clients use hadoop like token based authentication when\nKerberos is enabled in DSE server.</p>\n<p>The Spark driver authenticates to DSE server with Kerberos\ncredentials, requests a special token, send the token to the\nexecutors. Executors authenticates to DSE server with the token. So\nno kerberos libraries needed on executors node.</p>\n<p>If the Spark driver node has no Kerberos configured or spark\napplication should be run in cluster mode. The token could be\nrequested during configuration file generation with\n--generate-token parameters.</p>\n<pre class=\"dxDBcode_block\">\n$DSE_HOME/bin/dse client-tool configuration byos-export --generate-token byos.conf\n</pre>\n<p>Following property will be added to the byos.conf:</p>\n<pre class=\"dxDBcode_block\">\nspark.hadoop.cassandra.auth.token=NwAJY2Fzc2FuZHJhCWNhc3NhbmRyYQljYXNzYW5kcmGKAVPlcaJsigFUCX4mbIQ7YU_yjEJgRUwQNIzpkl7yQ4inoxtZtLDHQBpDQVNTQU5EUkFfREVMRUdBVElPTl9UT0tFTgA\n</pre>\n<p>It is important to manually cancel it after task is finished to\nprevent re usage attack.</p>\n<pre class=\"dxDBcode_block\">\ndse client-tool cassandra cancel-token NwAJY2Fzc2FuZHJhCWNhc3NhbmRyYQljYXNzYW5kcmGKAVPlcaJsigFUCX4mbIQ7YU_yjEJgRUwQNIzpkl7yQ4inoxtZtLDHQBpDQVNTQU5EUkFfREVMRUdBVElPTl9UT0tFTgA\n</pre>\n<h2>Instead of Conclusion</h2>\n<p>Open Source Spark Cassandra Connector and Bring Your Own Spark\nfeature comparison:</p>\n<table class=\"dxblogtableA1_css\"><tbody readability=\"4\"><tr><td><b>Feature</b></td>\n<td><b>OSS</b></td>\n<td><b>DSE BYOS</b></td>\n</tr><tr readability=\"1\"><td>DataStax Official Support</td>\n<td>No</td>\n<td>Yes</td>\n</tr><tr readability=\"1\"><td>Spark SQL Source Tables / Cassandra DataFrames</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr><tr readability=\"1\"><td>CassandraDD batch and streaming</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr><tr readability=\"1\"><td>C* to Spark SQL table mapping generator</td>\n<td>No</td>\n<td>Yes</td>\n</tr><tr readability=\"1\"><td>Spark Configuration Generator</td>\n<td>No</td>\n<td>Yes</td>\n</tr><tr readability=\"1\"><td>Cassandra File System Access</td>\n<td>No</td>\n<td>Yes</td>\n</tr><tr><td>SSL Encryption</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr><tr readability=\"1\"><td>User/password authentication</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr><tr readability=\"1\"><td>Kerberos authentication</td>\n<td>No</td>\n<td>Yes</td>\n</tr></tbody></table>\n                    </div>\n                                        <div class=\"post\" readability=\"228\">\n                        \n                        \n                        <p>DSE Advanced Replication feature in <a href=\"https://www.datastax.com/products/datastax-enterprise\">DataStax\nEnterprise</a> underwent a major refactoring between\nDSE 5.0 (“V1”) and DSE 5.1 (“V2”), radically overhauling its design\nand performance characteristics.</p>\n<p>DSE Advanced Replication builds on the multi-datacenter\nsupport in Apache Cassandra® to facilitate scenarios where\nselective or \"hub and spoke\" replication is required. DSE Advanced\nReplication is specifically designed to tolerate sporadic\nconnectivity that can occur in constrained environments, such as\nretail, oil-and-gas remote sites and cruise ships.</p>\n<p>This blog post provides a broad overview of the main\nperformance improvements and  drills down into how we support\n<a href=\"http://docs.datastax.com/en/cassandra/3.0/cassandra/configuration/configCDCLogging.html\">\nCDC</a> ingestion and deduplication to ensure efficient\ntransmission of mutations.</p>\n<p><strong>Note:</strong> This blog post was written targeting DSE\n5.1. Please refer to the <a title=\"DataStax Documentation\" href=\"http://docs.datastax.com/en/\">DataStax documentation</a> for your\nspecific version of DSE if different.</p>\n<p>Discussion of performance enhancements is split into three\nbroad stages:</p>\n<ol><li><b>Ingestion</b>: Capturing the Cassandra mutations for an\nAdvance Replication enabled table</li>\n<li><b>Queueing</b>: Sorting and storing the ingested mutations in\nan appropriate message queue</li>\n<li><b>Replication</b>: Replicating the ingested mutation to the\ndesired destination(s).</li>\n</ol><h2>Ingestion</h2>\n<p>In Advanced Replication v1 (included in DSE 5.0); capturing\nmutations for an Advanced Replication enabled table used Cassandra\ntriggers. Inside the trigger we unbundled the mutation and extract\nthe various partition updates and key fields for the mutation. By\nusing the trigger in the ingestion transaction, we provided\nbackpressure to ingestion and reduced throughput latency, as the\nmutations were processed in the ingestion cycle.</p>\n<p>In Advanced Replication v2 (included in DSE 5.1), we replaced\ntriggers with the Cassandra Change Data Capture (CDC) feature added\nin Cassandra version 3.8. CDC is an optional mechanism for\nextracting mutations from specific tables from the commitlog. This\nmutation extraction occurs outside the Ingestion transaction, so it\nadds negligible direct overhead to the ingestion cycle latency.</p>\n<p>Post-processing the CDC logs requires CPU and memory. This\nprocess competes with DSE for resources, so decoupling of\ningestion into DSE and ingestion into Advanced Replication allows\nus to support bursting for mutation ingestion.</p>\n<p>The trigger in v1 was previously run on a single node in\nthe source cluster. CDC is run on every node in the source cluster,\nwhich means that there are replication factor (RF) number of copies\nof each mutation. This change creates the need for deduplication\nwhich we’ll explain later on.</p>\n<h2>Queuing</h2>\n<p>In Advanced Replication v1, we stored the mutations in a\nblob of data within a vanilla DSE table, relying on DSE to manage\nthe replication of the queue and maintain the data integrity. The\nissue was that this insertion was done within the ingestion cycle\nwith a negative impact on ingestion latency, at a minimum doubling\nthe ingestion time. This could increase the latency enough to\ncreate a query timeout, causing an exception for the whole\nCassandra query.</p>\n<p>In Advanced Replication v2 we offloaded the queue outside\nof DSE and used local files. So for each mutation, we have RF\ncopies of it that mutation - due to capturing the mutations at the\nreplica level via CDC versus at the coordinator level via triggers\nin v1 – on the same nodes as the mutation is stored for Cassandra.\nThis change ensures data integrity and redundancy and\nprovides RF copies of the mutation.</p>\n<p>We have solved this CDC deduplication problem based on an\nintimate understanding of token ranges, gossip, and mutation\nstructures to ensure that, on average, each mutation is only\nreplicated once.The goal is to replicate all mutations at least\nonce, and to try to minimize replicating a given mutation multiple\ntimes. This solution will be described later.</p>\n<h2>Replication</h2>\n<p>Previously in Advanced Replication v1, replication could be\nconfigured only to a single destination. This replication stream\nwas fine for a use case which was a net of source clusters storing\ndata and forwarding to a central hub destination, essentially\n'edge-to-hub.'</p>\n<p>In Advanced Replication v2 we added support for multiple\ndestinations, where data could be replicated to multiple\ndestinations for distribution or redundancy purposes. As part of\nthis we added the ability to prioritize which destinations and\nchannels (pairs of source table to destination table) are\nreplicated first, and  configure whether channel replication\nis LIFO or FIFO to ensure newest or oldest data is replicated\nfirst.</p>\n<p>With the new implementation of the v2 mutation Queue, we have\nthe situation where we have each mutation stored in Replication\nFactor number of queues, and the mutations on each Node are\ninterleaved depending on which subset of token ranges are stored on\nthat node.</p>\n<p>There is no guarantee that the mutations are received on\neach node in the same order.</p>\n<p>With the Advanced Replication v1 trigger implementation\nthere was a single consolidated queue which made it significantly\neasier to replicate each mutation only once.</p>\n<h2>Deduplication</h2>\n<p>In order to minimize the number of times we process each\nmutation, we triage the mutations that extract from the CDC log in\nthe following way:</p>\n<ol><li>Separate the mutations into their distinct tables.</li>\n<li>Separate them into their distinct token ranges.</li>\n<li>Collect the mutations in time sliced buckets according to their\nmutation timestamp (which is the same for <b>that</b> mutation\nacross all the replica nodes.)</li>\n</ol><h2>Distinct Tables</h2>\n<p>Separating them into their distinct table represents the\ndirectory structure:</p>\n<h2><a href=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure1.png\"><img class=\"alignnone size-full wp-image-56950\" src=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure1.png\" alt=\"\" srcset=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure1.png 1232w, https://www.datastax.com/wp-content/uploads/9999/06/Figure1-250x113.png 250w, https://www.datastax.com/wp-content/uploads/9999/06/Figure1-768x347.png 768w, https://www.datastax.com/wp-content/uploads/9999/06/Figure1-700x316.png 700w, https://www.datastax.com/wp-content/uploads/9999/06/Figure1-120x54.png 120w\" sizes=\"(max-width: 1232px) 100vw, 1232px\"/></a></h2>\n<h2>token Range configuration</h2>\n<p>Assume a three node cluster with a replication factor of 3.</p>\n<p>For the sake of simplicity, this is the token-range structure on\nthe nodes:</p>\n<p><a href=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure2.png\"><img class=\"alignnone size-full wp-image-56952\" src=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure2.png\" alt=\"\" srcset=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure2.png 1229w, https://www.datastax.com/wp-content/uploads/9999/06/Figure2-250x168.png 250w, https://www.datastax.com/wp-content/uploads/9999/06/Figure2-768x517.png 768w, https://www.datastax.com/wp-content/uploads/9999/06/Figure2-700x471.png 700w, https://www.datastax.com/wp-content/uploads/9999/06/Figure2-120x81.png 120w\" sizes=\"(max-width: 1229px) 100vw, 1229px\"/></a></p>\n<p>Primary, Secondary and Tertiary are an arbitrary but consistent\nway to prioritize the token Ranges on the node – and are based on\nthe token Configuration of the keyspace – as we know that Cassandra\nhas no concept of a primary, secondary or tertiary node.</p>\n<p>However, it allows us to illustrate that we have three token\nranges that we are dealing with in this example. If we have\nVirtual-Nodes, then naturally there will be more token-ranges, and\na node can be ‘primary’ for multiple ranges.</p>\n<h2>Time slice separation</h2>\n<p>Assume the following example CDC files for a given table:</p>\n<p><a href=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure4.png\"><img class=\"alignnone size-full wp-image-56953\" src=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure4.png\" alt=\"\" srcset=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure4.png 1276w, https://www.datastax.com/wp-content/uploads/9999/06/Figure4-250x216.png 250w, https://www.datastax.com/wp-content/uploads/9999/06/Figure4-768x662.png 768w, https://www.datastax.com/wp-content/uploads/9999/06/Figure4-700x603.png 700w, https://www.datastax.com/wp-content/uploads/9999/06/Figure4-120x103.png 120w\" sizes=\"(max-width: 1276px) 100vw, 1276px\"/></a></p>\n<p>As we can see the mutation timestamps are NOT always received in\norder (look at the id numbers), but in this example we contain the\nsame set of mutations.</p>\n<p>In this case, all three nodes share the same token ranges, but\nif we had a 5 node cluster with a replication factor of 3, then the\ntoken range configuration would look like this, and the mutations\non each node would differ:</p>\n<p><a href=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure6.png\"><img class=\"alignnone size-full wp-image-56954\" src=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure6.png\" alt=\"\" srcset=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure6.png 1287w, https://www.datastax.com/wp-content/uploads/9999/06/Figure6-250x255.png 250w, https://www.datastax.com/wp-content/uploads/9999/06/Figure6-768x784.png 768w, https://www.datastax.com/wp-content/uploads/9999/06/Figure6-700x714.png 700w, https://www.datastax.com/wp-content/uploads/9999/06/Figure6-120x122.png 120w, https://www.datastax.com/wp-content/uploads/9999/06/Figure6-32x32.png 32w, https://www.datastax.com/wp-content/uploads/9999/06/Figure6-50x50.png 50w, https://www.datastax.com/wp-content/uploads/9999/06/Figure6-64x64.png 64w\" sizes=\"(max-width: 1287px) 100vw, 1287px\"/></a></p>\n<h2>Time slice buckets</h2>\n<p>As we process the mutations from the CDC file, we store them in\ntime slice buckets of one minute’s worth of data. We also keep a\nstack of 5 time slices in memory at a time, which means that we can\nhandle data up to 5 minutes out of order. Any data which is\nprocessed more than 5 minutes out of order would be put into the\nout of sequence file and treated as exceptional data which will be\nneed to be replicated from all replica nodes.</p>\n<h3>Example CDC Time Window Ingestion</h3>\n<p><a href=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure7.png\"><img class=\"alignnone size-full wp-image-56955\" src=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure7.png\" alt=\"\" srcset=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure7.png 1313w, https://www.datastax.com/wp-content/uploads/9999/06/Figure7-250x85.png 250w, https://www.datastax.com/wp-content/uploads/9999/06/Figure7-768x260.png 768w, https://www.datastax.com/wp-content/uploads/9999/06/Figure7-700x237.png 700w, https://www.datastax.com/wp-content/uploads/9999/06/Figure7-120x41.png 120w\" sizes=\"(max-width: 1313px) 100vw, 1313px\"/></a></p>\n<ul><li>In this example, assume that there are 2 time slices of 30\nseconds</li>\n<li>Deltas which are positive are ascending in time so are\nacceptable.</li>\n<li>Id’s 5, 11 and 19 jump backwards in time.</li>\n<li>As the sliding time window is 30 seconds, Id’s 5, 12 &amp; 19\nwould be processed, whilst ID 11 is a jump back of 45 seconds so\nwould not be processed into the correct Time Slice but placed in\nthe Out Of Sequence files.</li>\n</ul><h2>Comparing Time slices</h2>\n<p>So we have a time slice of mutations on different replica nodes,\nthey <b>should</b> be identical, but there is no guarantee that\nthey are in the same order. But we need to be able to compare the\ntime slices and treat them as identical regardless of order. So we\ntake the CRC of each mutation, and when we have sealed (rotated it\nout of memory because the current mutation that we are ingesting is\n5 minutes later than this time slice) the time slice , we sort\nthe CRCs and take a CRC of all of the mutation CRCs.<br/>That [TimeSlice] CRC is comparable between time slices to ensure\nthey are identical.</p>\n<p>The CRCs for each time slice are communicated between nodes in\nthe cluster via the Cassandra table.</p>\n<h2>Transmission of mutations</h2>\n<p>In the ideal situation, identical time slices and all three\nnodes are active – so each node is happily ticking away only\ntransmitting its primary token range segment files.</p>\n<p>However, we need to deal with robustness and assume that nodes\nfail, time slices do not match and we still have the requirement\nthat ALL data is replicated.</p>\n<p>We use gossip to monitor which nodes are active and not, and\nthen if a node fails – the ‘secondary’ become active for that nodes\n‘primary’ token range.</p>\n<h3>Time slice CRC processing</h3>\n<p>If a CRC matches for a time slice between 2 node – then when\nthat time slice is fully transmitted (for a given destination),\nthen the corresponding time slice (with the matching crc) can be\nmarked as sent (synchdeleted.)</p>\n<p>If the CRC mismatches, and there is no higher priority active\nnode with a matching CRC, then that time slice is to be transmitted\n– this is to ensure that no data is missed and everything is fully\ntransmitted.</p>\n<h3>Active Node Monitoring Algorithm</h3>\n<p>Assume that the token ranges are (a,b], (b,c], (c,a], and the\nentire range of tokens is [a,c], we have three nodes (n1, n2 and\nn3) and replication factor 3.</p>\n<div>\n<ul><li>On startup the token ranges for the keyspace are determined -\nwe actively listen for token range changes and adjust the schema\nappropriately.</li>\n<li>These are remapped so we have the following informations:\n<ul><li>node =&gt; [{primary ranges}, {secondary ranges}, {tertiary\nranges}]</li>\n<li><b>Note</b>: We support vnodes where there may be multiple\nprimary ranges for a node.</li>\n</ul></li>\n<li>In our example we have:\n<ul><li>n1 =&gt; [{(a,b]}, {(b,c]}, {c,a]}]</li>\n<li>n2 =&gt; [{(b,c]}, {c,a]}, {(a,b]}]</li>\n<li>n3 =&gt; [{c,a]}, {(a,b]}, {(b,c]}]</li>\n</ul></li>\n<li>When all <b>three nodes are live</b>, the active token ranges\nfor the node are as follows:\n<ul><li>n1 =&gt; [{(a,b]}, <del>{(b,c]}</del>, <del>{c,a]}</del>] =&gt;\n{(a,b]}</li>\n<li>n2 =&gt; [{(b,c]}, <del>{c,a]}, {(a,b]}</del>] =&gt;\n{(b,c]}</li>\n<li>n3 =&gt; [{c,a]}, <del>{(a,b]}, {(b,c]}</del>] =&gt;\n{(c,a]}</li>\n</ul></li>\n<li>Assume that <b>n3 has died</b>, its primary range is then\nsearched for in the secondary replicas of live nodes:\n<ul><li>n1 =&gt; [{(a,b]}, <del>{(b,c]}</del>, <del>{c,a]}</del>] =&gt;\n{(a,b], }</li>\n<li>n2 =&gt; [{(b,c]}, {c,a]}, <del>{(a,b]}</del>] =&gt; {(b,c],\n(c,a]}</li>\n<li><del><b>n3</b></del> <b>=&gt;</b>\n<b><del>[{c,a]}</del>,</b> <b><del>{(a,b]}</del>,</b>\n<b><del>{(b,c]}</del>]</b> =&gt; {}</li>\n</ul></li>\n<li>Assume that <b>n2 and n3 have died</b>, their primary range is\nthen searched for in the secondary replicas of live nodes, and if\nnot found the tertiary replicas (assuming replication factor 3) :\n<ul><li>n1 =&gt; [{(a,b]}, {(b,c]}, {c,a]}] =&gt; {(a,b], (b,c],\n(c,a]}</li>\n<li><b><del>n2</del> =&gt; <del>[{(b,c]}</del>,\n<del>{c,a]}</del>, <del>{(a,b]}]</del> =&gt; {}</b></li>\n<li><del><b>n3</b></del> <b>=&gt;</b>\n<b><del>[{c,a]}</del>,</b> <b><del>{(a,b]}</del>,</b>\n<del><b>{(b,c]}]</b></del> =&gt; <b>{}</b></li>\n</ul></li>\n</ul></div>\n<ul><li>This ensures that data is only sent once from each edge node,\nand that dead nodes do not result in orphaned data which is not\nsent.</li>\n</ul><h4>Handling the Node Failure Case</h4>\n<p>Below illustrates the three stages of a failure case.</p>\n<ol><li>Before - where everything is working as expected.</li>\n<li>Node 2 Fails - so Node 1 becomes Active for its token Slices\nand ignores what it has already been partially sent for 120-180,\nand resends from its secondary directory.</li>\n<li>Node 2 restarts - this is after Node 1 has sent 3 Slices for\nwhich Node 2 <em>was</em> primary (but Node 1 was Active because it\nwas Node 2’s secondary), it synchronously Deletes those because the\nCRCs match. It ignores what has already been partially sent for\n300-360 and resends those from its primary directory and carries\non.</li>\n</ol><p><a href=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure8.png\"><img class=\"alignnone size-full wp-image-56956\" src=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure8.png\" alt=\"\" srcset=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure8.png 1240w, https://www.datastax.com/wp-content/uploads/9999/06/Figure8-250x28.png 250w, https://www.datastax.com/wp-content/uploads/9999/06/Figure8-768x85.png 768w, https://www.datastax.com/wp-content/uploads/9999/06/Figure8-700x77.png 700w, https://www.datastax.com/wp-content/uploads/9999/06/Figure8-120x13.png 120w\" sizes=\"(max-width: 1240px) 100vw, 1240px\"/></a></p>\n<p>Before</p>\n<p><a href=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure9.png\"><img class=\"alignnone size-full wp-image-56957\" src=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure9.png\" alt=\"\" srcset=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure9.png 1236w, https://www.datastax.com/wp-content/uploads/9999/06/Figure9-250x152.png 250w, https://www.datastax.com/wp-content/uploads/9999/06/Figure9-768x467.png 768w, https://www.datastax.com/wp-content/uploads/9999/06/Figure9-700x426.png 700w, https://www.datastax.com/wp-content/uploads/9999/06/Figure9-120x73.png 120w\" sizes=\"(max-width: 1236px) 100vw, 1236px\"/></a></p>\n<p>Node 2 Dies</p>\n<p><a href=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure10.png\"><img class=\"alignnone size-full wp-image-56958\" src=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure10.png\" alt=\"\" srcset=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure10.png 1224w, https://www.datastax.com/wp-content/uploads/9999/06/Figure10-250x153.png 250w, https://www.datastax.com/wp-content/uploads/9999/06/Figure10-768x471.png 768w, https://www.datastax.com/wp-content/uploads/9999/06/Figure10-700x429.png 700w, https://www.datastax.com/wp-content/uploads/9999/06/Figure10-120x74.png 120w\" sizes=\"(max-width: 1224px) 100vw, 1224px\"/></a></p>\n<p>Node 2 Restarts</p>\n\n<p><a href=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure11.png\"><img class=\"alignnone size-full wp-image-56959\" src=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure11.png\" alt=\"\" srcset=\"https://www.datastax.com/wp-content/uploads/9999/06/Figure11.png 1222w, https://www.datastax.com/wp-content/uploads/9999/06/Figure11-250x154.png 250w, https://www.datastax.com/wp-content/uploads/9999/06/Figure11-768x474.png 768w, https://www.datastax.com/wp-content/uploads/9999/06/Figure11-700x432.png 700w, https://www.datastax.com/wp-content/uploads/9999/06/Figure11-120x74.png 120w\" sizes=\"(max-width: 1222px) 100vw, 1222px\"/></a></p>\n<p>The vastly improved and revamped DSE Advanced Replication\nv2 in DSE 5.1 is more resilient and performant with support for\nmulti-hubs and multi-clusters.</p>\n<p>For more information see our documentation <a href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/advReplication/advRepTOC.html\">\nhere</a>.</p>\n                    </div>\n                                        <div class=\"post\" readability=\"133\">\n                        \n                        \n                        <h4>by <a href=\"https://twitter.com/rusmeshenberg\">Ruslan\nMeshenberg</a></h4>\n<p><strong><em>“Aren’t you done with every interesting challenge\nalready?”</em></strong></p>\n<p>I get this question in various forms a lot. During interviews.\nAt conferences, after we present on some of our technologies and\npractices. At meetups and social events.</p>\n<p><em>You have</em> <a href=\"https://media.netflix.com/en/company-blog/completing-the-netflix-cloud-migration\">\n<em>fully migrated to the Cloud</em></a><em>, you must\nbe done…</em></p>\n<p><em>You created</em> <a href=\"https://medium.com/netflix-techblog/global-cloud-active-active-and-beyond-a0fdfa2c3a45\">\n<em>multi-regional resiliency framework</em></a><em>, you must\nbe done…</em></p>\n<p><em>You</em> <a href=\"https://media.netflix.com/en/press-releases/netflix-is-now-available-around-the-world\">\n<em>launched globally</em></a><em>, you must be done…</em></p>\n<p><em>You deploy everything through</em> <a href=\"https://medium.com/netflix-techblog/global-continuous-delivery-with-spinnaker-2a6896c23ba7\">\n<em>Spinnaker</em></a><em>, you must be done…</em></p>\n<p><em>You</em> <a href=\"http://netflix.github.io/\"><em>open\nsourced</em></a> <em>large parts of your infrastructure, you must\nbe done…</em></p>\n<p>And so on. These assumptions could not be farther from the\ntruth, though. We’re now tackling tougher and more interesting\nchallenges than in years past, but the nature of the challenges has\nchanged, and the ecosystem itself has evolved and matured.</p>\n<p><strong>Cloud ecosystem:</strong></p>\n<p>When Netflix started our Cloud Migration back in 2008, the Cloud\nwas new. The collection of Cloud-native services was fairly\nlimited, as was the knowledge about best practices and\nanti-patterns. We had to trail-blaze and figure out a few novel\npractices for ourselves. For example, practices such as <a href=\"https://medium.com/netflix-techblog/netflix-chaos-monkey-upgraded-1d679429be5d\">\nChaos Monkey</a> gave birth to new disciplines like <a href=\"https://medium.com/netflix-techblog/chaos-engineering-upgraded-878d341f15fa\">\nChaos Engineering</a>. The architectural pattern of multi-regional\nresiliency led to the implementation and contribution of Cassandra\nasynchronous data replication. The Cloud ecosystem is a lot more\nmature now. Some of our approaches resonated with other companies\nin the community and became best practices in the industry. In\nother cases, better standards, technologies and practices have\nemerged, and we switched from our in-house developed technologies\nto leverage community-supported Open Source alternatives. For\nexample, a couple of years ago we switched to use Apache Kafka for\nour data pipeline queues, and more recently to Apache Flink for our\nstream <a href=\"https://www.slideshare.net/mdaxini/flink-forward2017netflix-keystonespaas\">\nprocessing / routing component</a>. We’ve also undergone a huge\nevolution of our Runtime Platform. From replacing our old in-house\nRPC system with gRPC (to better support developers outside the Java\nrealm and to eliminate the need to hand-write client libraries) to\ncreating <a href=\"https://www.slideshare.net/TimBozarth/netflix-from-zero-to-productionready-in-minutes-qcon-2017\">\npowerful application generators</a> that allow engineers to create\nnew production-ready services in a matter of minutes.</p>\n<p>As new technologies and development practices emerge, we have to\nstay on top of the trends to ensure ongoing agility and robustness\nof our systems. Historically, a unit of deployment at Netflix was\nan AMI / Virtual Machine — and that worked well for us. A couple of\nyears ago we made a bet that Container technology will enable our\ndevelopers be more productive when applied to the end to end\nlifecycle of an application. Now we have a robust multi-tenant\n<a href=\"https://www.slideshare.net/aspyker/series-of-unfortunate-netflix-container-events-qconnyc17\">\nContainer Runtime (codename: Titus)</a> that powers many batch and\nservice-style systems, whose developers enjoy the benefits of rapid\ndevelopment velocity.</p>\n<p>With the recent emergence of FaaS / Serverless patterns and\npractices, we’re currently exploring how to expose the value to our\nengineers, while fully integrating their solutions into our\necosystem, and providing first-class support in terms of telemetry\n/ insight, secure practices, etc.</p>\n<p><strong>Scale:</strong></p>\n<p>Netflix has grown significantly in recent years, across many\ndimensions:</p>\n<p>The number of subscribers</p>\n<p>The amount of streaming our members enjoy</p>\n<p>The amount of content we bring to the service</p>\n<p>The number of engineers that develop the\nNetflix service</p>\n<p>The number of countries and languages we support</p>\n<p>The number of device types that we support</p>\n<p>These aspects of growth led to many interesting challenges,\nbeyond standard “scale” definitions. The solutions that worked for\nus just a few years ago no longer do so, or work less effectively\nthan they once did. The best practices and patterns we thought\neveryone knew are now growing and diverging depending on the use\ncases and applicability. What this means is that now we have to\ntackle many challenges that are incredibly complex in nature, while\n“replacing the engine on the plane, while in flight”. All of our\nservices must be up and running, yet we have to keep making\nprogress in making the underlying systems more available, robust,\nextensible, secure and usable.</p>\n<p><strong>The Netflix ecosystem:</strong></p>\n<p>Much like the Cloud, the Netflix microservices ecosystem has\ngrown and matured over the recent years. With hundreds of\nmicroservices running to support our global members, we have to\nre-evaluate many assumptions all the way from what databases and\ncommunication protocols to use, to how to effectively deploy and\ntest our systems to ensure greatest availability and resiliency, to\nwhat UI paradigms work best on different devices. As we evolve our\nthinking on these and many other considerations, our underlying\nsystems constantly evolve and grow to serve bigger scale, more use\ncases and help Netflix bring more joy to our users.</p>\n<p><strong>Summary:</strong></p>\n<p>As Netflix continues to evolve and grow, so do our engineering\nchallenges. The nature of such challenges changes over time — from\n“greenfield” projects, to “scaling” activities, to\n“operationalizing” endeavors — all at great scale and break-neck\nspeed. Rest assured, there are plenty of interesting and rewarding\nchallenges ahead. To learn more, follow posts on our <a href=\"http://techblog.netflix.com/\">Tech Blog</a>, check out our\n<a href=\"http://netflix.github.io/\">Open Source Site</a>, and join\nour <a href=\"https://www.meetup.com/Netflix-Open-Source-Platform/\">OSS\nMeetup group</a>.</p>\n<p>Done? We’re not done. <strong>We’re just\ngetting started!</strong></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=267f65c4d1a7\" width=\"1\" height=\"1\"/><hr/><p><a href=\"https://medium.com/netflix-techblog/neflix-platform-engineering-were-just-getting-started-267f65c4d1a7\">\nNeflix Platform Engineering — we’re just getting started</a> was\noriginally published in <a href=\"https://medium.com/netflix-techblog\">Netflix TechBlog</a> on\nMedium, where people are continuing the conversation by\nhighlighting and responding to this story.</p>\n                    </div>\n                                        <div class=\"post\" readability=\"28\">\n                        \n                        \n                        <p>This course is designed for developers, and database\nadministrators who want to a rapid, deep-dive and ‘hands on’\nexploration of core Cassandra theories and data modelling\npractices.</p>\n<p>Continue reading <a rel=\"nofollow\" href=\"https://opencredo.com/training/cassandra-fundamentals-data-modelling/\">\nCassandra Fundamentals &amp; Data Modelling</a> on\nopencredo.com.</p>\n                    </div>"}}]}},"pageContext":{"alternative_id":1663}}