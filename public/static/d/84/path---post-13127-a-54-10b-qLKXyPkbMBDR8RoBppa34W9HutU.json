{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Patterns of Successful Cassandra Data Modelling - OpenCredo","alternative_id":13127,"content":"Patterns of Successful Cassandra Data Modelling - OpenCredo&#13;\n&#13;\n\t\t<header class=\"bg-white-90 text-blue fixed w-full z-9999 transition-slow\">&#13;\n</header><section class=\"breadcrumb\"><section><div class=\"wrapper justify-start sm:flex-row pt-0 sm:pt-90 pb-60 max-w-full-minus-60 lg:max-w-895 mb-120\"><div class=\"w-full sm:w-2/3 pr-0 sm:pr-20\"><p class=\"font-title mb-10 uppercase\">&#13;\n\t\tSeptember 6, 2016&#13;\n\t | <a href=\"https://opencredo.com/blogs/category/cassandra/\" class=\"text-blue\">Cassandra</a></p><p>A growing number of clients are asking OpenCredo for help with using <a href=\"http://cassandra.apache.org/\" target=\"_blank\" rel=\"noopener\">Apache Cassandra</a> and solving specific problems they encounter. Clients have different use cases, requirements, implementation and teams but experience similar issues. We have noticed that Cassandra data modelling problems are the most consistent cause of Cassandra failing to meet their expectations. Data modelling is one of the most complex areas of using Cassandra and has many considerations.</p></div><div class=\"w-full sm:w-1/3 text-center first-on-sm mb-30\"><p class=\"font-title font-bold mt-30\">WRITTEN BY</p><img src=\"https://15rf562os5r61q4tvf2630fw-wpengine.netdna-ssl.com/wp-content/uploads/2018/11/Alla-Babkina-_-No-Job-Title.jpg\" alt=\"Alla Babkina\" class=\"circle mb-7.5 h-64 sm:h-auto w-64 sm:w-auto\" /><p class=\"font-title text-orange uppercase font-bold mb-0\"><a href=\"https://opencredo.com/authors/alla-babkina/\" title=\"Posts by Alla Babkina\">Alla Babkina</a></p></div></div></section><section class=\"bg-light-grey\"><div class=\"wrapper max-w-full-minus-60 lg:max-w-895 mb-30\"><div class=\"max-w-full\"><img src=\"https://15rf562os5r61q4tvf2630fw-wpengine.netdna-ssl.com/wp-content/uploads/2018/10/banner-about-895x400.jpg\" alt=\"Patterns of Successful Cassandra Data Modelling\" class=\"-mt-120 mb-30 md:mb-60\" /><p class=\"p1\">For a business it is essential to invest resources into data modelling from the early stages of Cassandra projects; unlike operational settings that can be tuned, a Cassandra data model is very costly to fix.</p><p class=\"p1\">Cassandra is growing in popularity due to its well-advertised strengths such as high performance, fault tolerance, resilience and scalability covered in a <strong><a href=\"https://opencredo.com/?p=28752\">previous blog post</a></strong> by Guy Richardson. How well these strengths are realised in an application depends heavily on the quality of the underlying Cassandra data model. The main rule for designing a good Cassandra data model is crafting it specifically to your business domain and application use cases.</p><p class=\"p1\">Through years of experience with Cassandra engagements, we have identified a number of data modelling patterns and will cover some of them in this post. Their successful application requires a working knowledge of how Cassandra stores data. Without understanding the underlying architecture you risk making one of the common Cassandra mistakes covered in a separate post. Evaluating a seemingly valid and straightforward data model for potential pitfalls requires a level of expertise in Cassandra internals, the storage format in particular.</p><h2>The Simplicity and Complexity of Cassandra Storage</h2><p>While Cassandra architecture is relatively simple, it fundamentally limits the ways in which you can query it for data. Cassandra is designed to be a high-performance database and discourages inefficient queries. Query efficiency is determined by the way Cassandra stores data; it makes the following query patterns inefficient or impossible:</p><ul><li>fetching <em>all</em> data without identifying a subset by a partition key,</li>\n<li>fetching data from multiple partitions,</li>\n<li>joining distinct data sets,</li>\n<li>searching by values,</li>\n<li>filtering.</li>\n</ul><p>Cassandra expects applications to store the data in such a way that they can retrieve it <em>efficiently</em>. It is therefore up to the client to know the ways it will query Cassandra and design the data model accordingly upfront.</p><h3>Example: Projects by Manager and Turnover</h3><p>Consider an application that records information about <strong>project managers</strong>, their <strong>projects</strong> and project <strong>turnovers</strong>. Even for this intentionally simple use case, there are a many ways you could store the data and a number things to consider to produce a good Cassandra data model. At the very least, Cassandra requires a meaningful <em>key</em> to split data into subsets that you will later use to retrieve data. Without much knowledge of how Cassandra works, the first response might be to store this data in a simplified table:</p><p>In this table all data about projects and their turnovers is partitioned by manager. This data model will work if you only want to retrieve all project data for a particular manager. Disappointingly, this is the only type of query this table will support out of the box. Cassandra will not retrieve “<em>all projects with a turnover of 2 000 000″ –</em> the way in which it stored data makes this query inefficient. The reason behind this becomes obvious looking at the format of Cassandra SSTables.</p><h3>Partitioned Row Store</h3><p>Cassandra is a partitioned row store and its storage structure is in effect a nested sorted map which makes it is easy to grab a subset of data by <em>key. </em>In the previous example data is partitioned by manager name (which is the <em>key</em>) and makes it easy to retrieve projects in per-manager subsets. However, finding projects by turnover would involve checking turnover <em>values</em> for every project in every partition. Cassandra rightfully considers this inefficient, as a result such queries are not supported. Dominic Fox post “<b>How Not To Use Cassandra like an RDBMS (and what will happen if you do)</b>” [Release: 15/09/2016] will give many more examples on other queries that will be suboptimal in Cassandra in <strong>this blogpost</strong>. Luckily, there are patterns for designing Cassandra data models in a way that will be able to provide answers to most reasonable questions.</p><h2>Cassandra Data Modelling Patterns</h2><h3>Model around Business Domain</h3><p>When designing a Cassandra data model for an application, first consider the business entities you are storing and relationships between them. Your ultimate goal will be to store precomputed answers to business questions that the application asks about the stored data, an understanding its structure and meaning is a precondition for modelling these answers. Knowledge of the business domain model is also key to understanding the cardinality of certain data elements and estimating the changes in future data volumes. A data model designed around business domain will also spread data more evenly and keep partition sizes predictable. Naturally, achieving this requires close collaboration between business stakeholders and development teams.</p><h3>Denormalisation</h3><p>Unlike relational databases, Cassandra has no concept of foreign keys and does not support joining tables; both concepts are incompatible with its key architectural principles. Foreign keys have a significant negative impact on write performance while joining tables on reads is one of the inefficient querying patterns that Cassandra discourages. Cassandra demands that the structure of tables support the simplest and most efficient queries. On the other hand, writes in Cassandra are cheap and fast out of the box due to its simple storage model. As a result, it is usually a good idea to avoid extra reads at the expense of extra writes. This can be achieved by <em>balanced</em> denormalisation and data redundancy: if certain data is retrieved in multiple queries duplicate it across all tables supporting these queries.</p><h4>Example:</h4><p>While keeping data volumes and number of tables low is <strong>not</strong> a concern in Cassandra, it is still important to avoid <em>unnecessary</em> duplication. Consider a simplified data model of <strong>users</strong> leaving <strong>comments </strong>to <strong>articles </strong>and imagine retrieving the list of all comments for an article. Even though a separate table stores full user information, a good data model will also duplicate the author name in the comments table: application users want to see who wrote the comment. The following table structure would be appropriate for a common use case:</p><p>Note that storing full user information for the author with each comment would be excessive and create unnecessary duplication. It is unlikely that someone will ever want to immediately see full user information next to a comment left by that user – it is easy to retrieve upon request. It is vital to understand the types of queries the application will run to strike the right balance.</p><h3>Pre-built Result Sets</h3><p>Fast reads in Cassandra result from storing data fully ready to be read – preparing it at write time. Cassandra does not support any complex processing at query time such as filtering, joins, search, pattern and aggregation. The data must be stored partitioned, sorted, denormalised, and filtered in advance. Ideally, all that is left to Cassandra is to read the results from a single location. For this reason, having roughly a table per query is often a good approach in Cassandra. On the contrary, storing non-contextual isolated denormalised models often leads to common Cassandra anti-patterns.</p><p>When creating a Cassandra data model, determine specific questions the application will ask and the format and content of answers it will expect and create tables to store pre-built result sets. There are limited options to incrementally add support for new queries to an existing model through indexing. Besides, this approach introduces additional complexity and it is possible to avoid it by modelling data correctly in advance.</p><h3>Even Data Distribution</h3><p class=\"p1\">Understanding the distributed nature of Cassandra is key to model for predictably fast cluster performance. All nodes in a Cassandra cluster are equal by design and should run on identical hardware. Accordingly, for nodes to demonstrate comparable performance, they should bear the same load. Spreading data evenly across the cluster by choosing the right partition key helps achieve this. There are several data distribution aspects to consider when designing the data model.</p><ol><li class=\"p1\"><strong>Likely partition sizes</strong>. A single partition will always be stored in its entirety on a single node, therefore, it must be small enough to fit on that node accounting for free space required for <a href=\"https://docs.datastax.com/en/cassandra/3.x/cassandra/dml/dmlHowDataMaintain.html#dmlHowDataMaintain__dml-compaction\" target=\"_blank\" rel=\"noopener\">compaction</a>. For this reason, it is important to understand compaction and choose the right compaction strategy.</li>\n<li class=\"p1\"><strong>Keeping partition data bounded</strong>. If the amount of data in a single partition is likely to become too big, adding an additional partitioning column will limit its growth potential. For example, additionally bounding a partition of events by time in addition to type would provide a reasonable guarantee of reasonable partition size. That said, it is important to carefully choose the time granularity best suited to a particular use case.</li>\n<li class=\"p1\"><strong>Partition key cardinality</strong>. Choosing a column with a reasonably large number of unique values (high cardinality) is important to keep partition sizes small. However, it is important to balance this with the aim of on partition ideally being able to satisfy each query.</li>\n<li class=\"p1\"><strong>Separating read-heavy and write-heavy data. </strong>Read-heavy and write-heavy data have different scalability cycles, and measures to optimise them are different and often conflicting. For example, different compaction strategies suit read-heavy and write-heavy workflows best. For this reason, it often helps to keep such data separate even if there is a strong semantic relationship.</li>\n</ol><h3>Inserts over Updates and Deletes</h3><p>Although Cassandra supports updates and deletes, their excessive use results in unexpected operational overhead. Using them safely in Cassandra requires detailed knowledge of their implementation and operational processes in Cassandra. While a record may appear updated or deleted on the surface, physical storage will not reflect it straight away. Cassandra reconciles updates and deletes in physical storage during compaction and only under certain conditions. Use of update and delete operations in Cassandra substantially complicate cluster operations and rely on regular well scheduled repairs, monitoring and manual compaction. In our experience, data models that avoid updating or deleting data help reduce operational risks and costs.</p><h3>Testing Data Models</h3><p>Like any code, Cassandra data models need thorough testing before production use. There are too many factors that affect the performance and operations of the cluster to predict it with reasonable certainty. It is necessary to validate a Cassandra data model by testing it against real business scenarios. In fact, Cassandra ships with a <a href=\"https://docs.datastax.com/en/cassandra_win/3.0/cassandra/tools/toolsCStress.html\" target=\"_blank\" rel=\"noopener\">cassandra-stress</a> tool which can help load test the performance of your cluster with a chosen data model. The <a href=\"https://docs.datastax.com/en/cassandra/3.x/cassandra/tools/toolsTablehisto.html\" target=\"_blank\" rel=\"noopener\">nodetool tablehistograms</a> (<a href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsCFhisto.html\" target=\"_blank\" rel=\"noopener\">nodetool cfhistograms</a> prior to Cassandra 2.2) utility provides further table statistics. There may be a temptation to avoid testing the data model altogether because gathering fully accurate metrics is impossible. While it is rarely possible to run tests against a production-sized Cassandra cluster, testing against a smaller test cluster will highlight common data model problems. In short, prefer testing the data model for a subset of scenarios over speculating about all aspects of its performance.</p><h2>Look for the Right Balance</h2><p>Cassandra is a powerful tool but upfront investment into its setup pays off in later stages of projects. Performance and smooth operations of a Cassandra cluster depend in large part on the quality of the data model and how well it suits the application. There are many factors that shape the suitable data model and its design involves many complex decisions and tradeoffs. While data modelling in Cassandra requires a high level of expertise in its architecture, there are a number of patterns that help achieve good results. Carefully balancing these decisions helps avoid mistakes and anti-patterns that lead to common Cassandra operational problems.</p><h4>This is the second post in our blog series “Cassandra – What You May Learn The Hard Way.” Get the full overview <a href=\"https://opencredo.com/?p=28779\" target=\"_blank\" rel=\"noopener\">here</a>.</h4><h4>The associated webinar, “Cassandra – The Good, the Bad, and the Ugly” was broadcast on October 6th, 2016. View the recording <strong><a href=\"https://opencredo.com/?p=28889\"> here</a>.</strong></h4><h4><a class=\"button\" href=\"#text_icl-6\">Sign up to receive updates via email</a></h4></div></div></section><section class=\"bg-white text-blue pt-60 pb-60\"><div class=\"wrapper\"><p>&#13;\n\t\t\t\t\t</p><h2 class=\"mb-30 text-16 font-bold\">SIMILAR POSTS</h2>&#13;<div class=\"flex flex-wrap w-full lg:w-5/6 justify-center\"><div class=\"owl-carousel owl-carousel-insights flex items-stretch mb-30 md:mb-30 pl-0 sm:pl-30 pr-0 sm:pr-30 lg:pl-0 lg:pr-0\"><div class=\"bg-orange h-full\"><a href=\"https://opencredo.com/blogs/riak-the-dynamo-paper-and-life-beyond-basho/\" title=\"Read More\"><img src=\"https://15rf562os5r61q4tvf2630fw-wpengine.netdna-ssl.com/wp-content/uploads/2018/10/banner-case-studies-297x236.jpg\" alt=\"Riak, the Dynamo paper and life beyond Basho\" class=\"block w-full\" /></a><div class=\"text-white p-30\"><p class=\"text-18 mb-30\"><a href=\"https://opencredo.com/blogs/riak-the-dynamo-paper-and-life-beyond-basho/\" class=\"text-white no-underline font-bold\">Riak, the Dynamo paper and life beyond Basho</a></p>Recently, the sad news has emerged that Basho, which developed the Riak distributed database, has gone into receivership. This would appear to present a problem…<p><a href=\"https://opencredo.com/blogs/riak-the-dynamo-paper-and-life-beyond-basho/\" class=\"text-white no-underline font-bold\">Read More <img src=\"https://15rf562os5r61q4tvf2630fw-wpengine.netdna-ssl.com/wp-content/themes/wp-ocd/assets/img/arrow-white.png\" alt=\"Read More\" class=\"h-16 ml-7.5\" /></a></p></div></div><div class=\"bg-orange h-full\"><a href=\"https://opencredo.com/blogs/spark-testing/\" title=\"Read More\"><img src=\"https://15rf562os5r61q4tvf2630fw-wpengine.netdna-ssl.com/wp-content/uploads/2018/10/banner-expertise-297x236.jpg\" alt=\"Testing a Spark Application\" class=\"block w-full\" /></a><div class=\"text-white p-30\"><p class=\"text-18 mb-30\"><a href=\"https://opencredo.com/blogs/spark-testing/\" class=\"text-white no-underline font-bold\">Testing a Spark Application</a></p>Data analytics isn’t a field commonly associated with testing, but there’s no reason we can’t treat it like any other application. Data analytics services are…<p><a href=\"https://opencredo.com/blogs/spark-testing/\" class=\"text-white no-underline font-bold\">Read More <img src=\"https://15rf562os5r61q4tvf2630fw-wpengine.netdna-ssl.com/wp-content/themes/wp-ocd/assets/img/arrow-white.png\" alt=\"Read More\" class=\"h-16 ml-7.5\" /></a></p></div></div><div class=\"bg-orange h-full\"><a href=\"https://opencredo.com/blogs/deploy-spark-apache-cassandra/\" title=\"Read More\"><img src=\"https://15rf562os5r61q4tvf2630fw-wpengine.netdna-ssl.com/wp-content/uploads/2018/10/banner-services-297x236.jpg\" alt=\"Deploy Spark with an Apache Cassandra cluster\" class=\"block w-full\" /></a><div class=\"text-white p-30\"><p class=\"text-18 mb-30\"><a href=\"https://opencredo.com/blogs/deploy-spark-apache-cassandra/\" class=\"text-white no-underline font-bold\">Deploy Spark with an Apache Cassandra cluster</a></p>My recent blogpost I explored a few cases where using Cassandra and Spark together can be useful. My focus was on the functional behaviour of…<p><a href=\"https://opencredo.com/blogs/deploy-spark-apache-cassandra/\" class=\"text-white no-underline font-bold\">Read More <img src=\"https://15rf562os5r61q4tvf2630fw-wpengine.netdna-ssl.com/wp-content/themes/wp-ocd/assets/img/arrow-white.png\" alt=\"Read More\" class=\"h-16 ml-7.5\" /></a></p></div></div></div><a href=\"https://opencredo.com/blogs/\" class=\"btn hover-scale-105 text-18\">Blog</a></div></div></section><noscript>\n<div id=\"catapult-cookie-bar\" class=\"\"><p>This website uses cookies to maximise your experience and help us to understand how we can improve it. By continuing to browse this website or by clicking 'Accept', you consent to the use of cookies. If you would like to manage your cookie settings, you can control this in your internet browser. </p></div>\t</noscript></section>"}}]}},"pageContext":{"alternative_id":13127}}