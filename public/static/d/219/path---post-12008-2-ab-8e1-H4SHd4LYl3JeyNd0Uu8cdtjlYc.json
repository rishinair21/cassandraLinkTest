{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"The Correct Way to Connect Spark Shell to Cassandra","alternative_id":12008,"content":"<p>tl;dr</p><ul><li>using the cassandra connector in the spark-shell is fairly straightforward</li>\n  <li>setting up the connection in a way that doens’t break the existing <code class=\"highlighter-rouge\">sc</code> is not documented anywhere</li>\n  <li><a href=\"#solution\">the correct solution</a> is to not call <code class=\"highlighter-rouge\">sc.stop</code> but provide the cassandra host on startup of the shell</li>\n</ul>\n<p>Apache Cassandra is a NoSQL distributed database that’s been gaining popularity recently. It’s also pretty high performance, scoring very high in a (not so) recent <a href=\"http://vldb.org/pvldb/vol5/p1724_tilmannrabl_vldb2012.pdf\" title=\"Solving Big Data Challenges for Enterprise Application Performance Management [Rabl et. al 2012] (PDF)\">comparison of key-value stores</a> (PDF) for different workloads. Among the contenders were HBase, Cassandra, Voldemort, Redis, VoltDB and MySQL, HBase tends to be the winner (by one to two orders of magnitude) when it comes to latency and Cassandra when it comes to throughput - depending on the number of nodes in cluster. A key-value store is nice, but it isn’t much use unless you have something doing reads and writes into it. That’s where <code class=\"highlighter-rouge\">spark</code> comes in.</p>\n<p>Every data scientist’s<sup><a href=\"https://en.wikipedia.org/wiki/Data_science\">[1]</a> <a href=\"https://www-01.ibm.com/software/data/infosphere/data-scientist/\">[2]</a><a href=\"#footnote\" title=\"I don't like the term either but that's what we seem to have settled for.\">[3]</a></sup> favourite new toy <code class=\"highlighter-rouge\">spark</code> is a distributed in-memory data processing framework. Cassandra very helpfully comes with a <code class=\"highlighter-rouge\">spark</code> connector that allows you to pull data into spark as <code class=\"highlighter-rouge\">RDD</code>s or <code class=\"highlighter-rouge\">DataFrame</code>s directly from Cassandra.</p>\n<p>Connecting to a Cassandra host from <code class=\"highlighter-rouge\">spark</code> isn’t all that complicated, just import the connector and tell <code class=\"highlighter-rouge\">SparkConf</code> where to find the Cassandra host from and you’re off to the races. The Cassandra connector <a href=\"https://github.com/datastax/spark-cassandra-connector/#documentation\">docs</a> cover the basic usage pretty well. Aside from the bazillion different versions of the connector getting everything up and running is fairly straightforward.</p>\n<p>Start the spark shell with the necessary Cassandra connector dependencies <code class=\"highlighter-rouge\">bin/spark-shell --packages datastax:spark-cassandra-connector:1.6.0-M2-s_2.10</code>.</p>\n<figure class=\"highlight\"><pre class=\"language-scala\" data-lang=\"scala\">1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n</pre><pre>import org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.sql.SQLContext\nimport com.datastax.spark.connector._\n# connect to a local cassandra instance\nval conf = new SparkConf(true)\n    .set(\"spark.cassandra.connection.host\", \"127.0.0.1\")\nval sc = new SparkContext(conf)\nval sqlContext = new SQLContext(sc)\n# read in some data as a DataFrame\nval df = sqlContext\n    .read\n    .format(\"org.apache.spark.sql.cassandra\")\n    .options(Map(\"table\" -&gt; \"fooTable\", \"keyspace\" -&gt; \"bar\")).load.cache()</pre></figure><p>Lovely, you now have a DataFrame that acts just like any other <code class=\"highlighter-rouge\">spark</code> DataFrame. So far so good. Now let’s say you wanted to test something in the <code class=\"highlighter-rouge\">spark-shell</code> and pull in data from Cassandra. No problem, just do what you did before, except that you need to stop the existing <code class=\"highlighter-rouge\">SparkContext</code> that is created automagically when the shell starts up, before you can create a new one. This isn’t really documented anywhere, except sporadically on <a href=\"https://stackoverflow.com/questions/25837436/how-to-load-spark-cassandra-connector-in-the-shell\">StackOverflow</a>. The accepted answer is actually the wrong way to do this.</p>\n<figure class=\"highlight\"><pre class=\"language-scala\" data-lang=\"scala\">1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n</pre><pre>// DO NOT DO THIS\nsc.stop // NOOOooo\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.sql.SQLContext\nimport com.datastax.spark.connector._\n// connect to a local cassandra instance\nval conf = new SparkConf(true)\n    .set(\"spark.cassandra.connection.host\", \"127.0.0.1\")\nval sc = new SparkContext(conf)\nval sqlContext = new SQLContext(sc)\n// read in some data as a DataFrame\nval df = sqlContext\n    .read\n    .format(\"org.apache.spark.sql.cassandra\")\n    .options(Map(\"table\" -&gt; \"fooTable\", \"keyspace\" -&gt; \"bar\")).load.cache()</pre></figure><p>The <code class=\"highlighter-rouge\">SparkContext</code> created above will not function like the old <code class=\"highlighter-rouge\">SparkContext</code> created when the shell started up. This doesn’t actually have anything to do the Cassandra connector perse, it’s just that the setup for the Cassandra connector brings up this issue. To see the problem consider the following simplified code without the Cassandra connector.</p>\n<figure class=\"highlight\"><pre class=\"language-scala\" data-lang=\"scala\">1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n</pre><pre>import org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nsc.stop\nval conf = sc.getConf\nval sc = new SparkContext(conf)\nval rdd = sc.parallelize(Array(0, 1), (1, 10), (2, 15), (3, 7))\nval map = Map(0-&gt;3, 1-&gt;2, 2-&gt;1, 3-&gt;0)\nval BVmap = sc.broadcast(map)\nrdd.map( r =&gt; BVmap.value(r._1))</pre></figure><p>The above doesn’t do anything particularly worthwhile, but it illustrates the problem. Because the <code class=\"highlighter-rouge\">SparkContext</code> was recreated the code will fail in the shell, due to <code class=\"highlighter-rouge\">sc</code> being not serialisable anymore.</p>\n<p>The solution is extremely simple, but suprisingly difficult to find. Instead of calling <code class=\"highlighter-rouge\">sc.stop</code> and then recreating the <code class=\"highlighter-rouge\">conf</code> with the Cassandra host details added, just add the Cassandra host details to the configuration defaults in <code class=\"highlighter-rouge\">$SPARK_HOME/conf/spark-defaults.conf</code>. Should you not have access to the default conf you can also provide the connection host in the call to <code class=\"highlighter-rouge\">spark-shell</code></p>\n<p><code class=\"highlighter-rouge\">bin/spark-shell --packages datastax:spark-cassandra-connector:1.6.0-M2-s_2.10 --conf spark.cassandra.connection.host=127.0.0.1</code></p>\n<p>This not being included in the official Cassandra connector documentation is bizarre.</p>\n<p>[3] I don’t like the term either but that’s what we seem to have settled for.</p>\n<ul><li><a href=\"http://spark.apache.org\">Spark</a></li>\n  <li><a href=\"http://cassandra.apache.org\">Cassandra</a></li>\n  <li><a href=\"https://github.com/datastax/spark-cassandra-connector\">Spark Cassandra Connector</a></li>\n</ul>"}}]}},"pageContext":{"alternative_id":12008}}