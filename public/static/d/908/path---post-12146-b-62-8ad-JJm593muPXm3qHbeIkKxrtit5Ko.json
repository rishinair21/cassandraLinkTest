{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Cassandra NoSQL Data Model Design - Instaclustr","alternative_id":12146,"content":"<h2>Abstract</h2><p>This paper describes the process that we follow at<br />Instaclustr to design a <a href=\"https://www.instaclustr.com/apache-cassandra/\">Cassandra</a> data model for our<br />customers.</p><p>While not a prescriptive, formal process it does define<br />phases and steps that our team follows when we are<br />design a new data model for our customers:</p><p><strong>Phase 1: Understand the data</strong></p><p><strong>Phase 2: Define the entities</strong></p><p><strong>Phase 3: Review &amp; tune</strong></p><p>As well as defining the process we also provide a<br />worked example based on building a database to<br />store and retrieve log messages from multiple servers.</p><h2>Overview</h2><p>We recently published a blog post on the most common data modelling mistakes that we see with<br /><a href=\"https://www.instaclustr.com/apache-cassandra/\">Cassandra</a>. This post was very popular and led me to think about what advice we could provide on<br />how to approach designing your Cassandra data model so as to come up with a quality design that<br />avoids the traps.</p><p>There are a number of good articles around that with rules and patterns to fit your data model into:</p><p><a href=\"https://www.instaclustr.com/resource/6-step-guide-to-apache-cassandra-data-modelling-white-paper/\" target=\"_blank\" rel=\"noopener\">6 Step Guide to Apache Cassandra Data Modelling </a></p><p>and</p><p><a href=\"https://support.instaclustr.com/hc/en-us/articles/207071957-Data-Modelling-Recommended-Practices\" target=\"_blank\" rel=\"noopener\">Data Modelling Recommended Practices</a></p><p>However, we haven’t found a step by step guide to analysing your data to determine how to fit in<br />these rules and patterns. This white paper is a quick attempt at filling that gap.</p><h2>Phase 1: Understand the data</h2><p>This phase has two distinct steps that are both designed to gain a good understanding of the data that<br />you are modelling and the access patterns required.</p><h3>Define the data domain</h3><p>The first step is to get a good understanding of your data domain. As someone very familiar with<br />relation data modelling, I tend to sketch (or at least think) ER diagrams to understand the entities,<br />their keys and relationships. However, if you’re familiar with another notation then it would likely<br />work just as well. The key things you need to understand at a logical level are:</p><p>• What are the entities (or objects) in your data model?<br />• What are the primary key attributes of the entities?<br />• What are the relationships between the entities (i.e. references from one to the other)?<br />• What is the relative cardinality of the relationships (i.e. if you have a one to many is it one to<br />10 or one to 10,000 on average)?</p><p>Basically, these are the same things you’d expect in from logical ER model (although we probably<br />don’t need a complete picture of all the attributes) along with a complete understanding of the<br />cardinality of relationships that you’d normally need for a relational model.<br />An understanding of the demographics of key attributes (cardinality, distribution) will also be useful<br />in finalising your Cassandra model. Also, understand which key attributes are fixed and which change<br />over the life of a record.</p><h3>Define the required access patterns</h3><p>The next step, or quite likely a step carried out in conjunction with step 1, is to understand how you<br />will need to access your data:</p><ul><li>List out the paths you will follow to access the data, such as: <ul><li>Start with a customer id, search for transactions in a date range and then look up all the<br />details about a particular transaction from the search resultsStart with a particular server and metric, retrieve x metrics values in ascending age</li> <li>Start with a particular server and metric, retrieve x metrics values in ascending age starting at a particular point in time.</li> <li>For a given sensor, retrieve all readings of multiple metrics for a given day.</li> <li>For a given sensor, retrieve the current value.</li> </ul></li> </ul><ul><li>Remember that any updates of a record are an access path that needs to be considered</li> <li>Determine which accesses are the most crucial from a performance point of view – are there some which need to be as quick as possible while performance requirements for others allow time for multiple reads or range scans?</li> <li>Remember that you need a pretty complete understanding of how you will access your data at this stage – part of the trade-off for Cassandra’s performance, reliability and scalability is a fairly restricted set of methods for accessing data in a particular table.</li> </ul><h2>Phase 2: Understand the entities</h2><p>This phase has two specific steps designed to gain an understanding of both the primary and<br />secondary entities associated with the data.</p><h3>Identify primary access entities</h3><p>Now we’re moving from analysing your data domain and application requirements to starting to<br />design your data model. You really want to be pretty solid on steps 1 and 2 before moving on to this<br />stage.</p><p>The idea here is to denormalize your data into the smallest number of tables possible based on your<br />access patterns. For each lookup by key that your access patterns require, you will need a table to<br />satisfy that lookup. I’ve coined the term primary access entity to describe the entity your using for<br />the lookup (for example, a lookup by client id is using client as the primary access entity, a lookup by<br />server and metric name is using a server-metric entity as the primary access entity).</p><p>The primary access entity defines the partition level (or grain if you’re familiar with dimensional<br />modelling) of the resulting denormalized table (i.e. there will be one partition in the table for each<br />instance of the primary access entity).</p><p>You may choose to satisfy some access patterns using secondary indexes rather than complete replicas<br />of the data with a different primary access entity. Keep in mind that columns in include in a secondary<br />index should have a significantly lower cardinality than the table being indexed and be aware of the<br />frequency of updates of the indexed value.</p><p>For the example access patterns above, we would define the following primary access entities:</p><ul><li>customer and transaction (get a list of transactions from the customer entity and then use that<br />to look up transaction details from the transaction entity)</li> <li>server-metric</li> <li>sensor</li> <li>sensor</li> </ul><h3>Allocate secondary entities</h3><p>The next step is to find a place to store the data that belongs to entities that have not been chosen as<br />primary access entities (I’ll call these entities secondary entities). You can choose to:</p><ul><li><strong> Push down</strong> by taking data from a parent secondary entity (one side) of a one to many<br />relationship and storing multiple copies of it at the primary access entity level (for example,<br />storing customer phone number in each customer order record); or</li> <li><strong>Push up</strong> by taking data from the child secondary entity (many side) of a one to many<br />relationship and storing it at the primary access entity level either by use of cluster keys or by<br />use of multi-value types (list and maps) (for example adding a list of line items to a transaction<br />level table).</li> </ul><p>For some secondary entities, there will only be one related primary access entity and so there is no<br />need to choose where and which direction to push. For other entities, you will need to choose will<br />need to choose which primary access entities to push the data into.</p><p>For optimal read performance, you should push a copy of the data to every primary access entity that<br />is used as an access path for the data in the secondary entity.</p><p>However, this comes at an insert/update performance and application complexity cost of maintaining<br />multiple copies the data. This trade-off between read performance and data maintenance cost needs<br />to be judged in the context of the specific performance requirements of your application.</p><p>The other decision to be made at this stage is between using a cluster key or a multi-value type for<br />pushing up. In general:</p><ul><li>Use a clustering key where there is only one child secondary entity to push up and particularly<br />where the child secondary entity itself has children to roll-up.</li> <li>Use multi-value types where there are multiple child entities to push up into the primary entity</li> </ul><p>Note that these rules are probably oversimplified but serve as a starting point for more detailed<br />consideration.</p><h2>Phase 3: Review &amp; Tune</h2><p>The last phase provides an opportunity to review the data model, test and to tune as necessary.</p><h3>Review partition &amp; cluster keys</h3><p>Entering this stage, you have all the data you need to store allocated to a table or tables and your<br />tables support accessing that data according to your required access patterns. The next step is to<br />check that the resulting data model makes efficient use of Cassandra and, if not, to adjust. The items<br />to check and adjust at this stage are:</p><ul><li><strong>Do your partition keys have sufficient cardinality?</strong> If not, it may be necessary to move<br />columns from the clustering key to the partition key (e.g. changing primary key (client_id,<br />timestamp) to primary key ((client_id, timestamp))) or introduce new columns which group<br />multiple cluster keys into partitions (e.g. changing primary key (client_id, timestamp) to<br />primary key ((client_id, day), timestamp).</li> <li><strong>Will the values in your partition keys be updated frequently?</strong> Updates of a primary key<br />value will result in deletion and re-insertion of the record which can result in issues with<br />tombstones. For example, trying to maintain a table with all clients of a particular status, you<br />might have primary key (status, client ID). However, this will result in a delete and re-insert<br />every time a client’s status changes. This would be a good candidate to use a set or list data<br />type rather than including client ID as the cluster key.</li> <li><strong>Is the number of records in each partition bounded?</strong> Extremely large partitions and/or very<br />unevenly sized partitions can cause issues. For example, if you have a client_updates table<br />with primary key (client_id, update_timestamp) there is potentially no limit to how many times<br />a particular client record can be update and you may have significant unevenness if you have a<br />small number of clients that have been around for 10 years and most clients only having<br />a day or two’s history. This is another example where it’s useful to introduce new columns<br />which group multiple cluster keys into partitions partitions (e.g. changing primary key (client_<br />id, update_timestamp) to primary key ((client_id, month), update_timestamp).</li> </ul><h3>Test and tune</h3><p>The final step is perhaps the most important – test your data model and tune it as required. Keep in<br />mind that issues like partitions or rows growing too large or tombstones building up in a table may<br />only become visible after days (or longer) of use under real-world load. It’s therefore important to test<br />as closely as possible to real-world load and to monitor closely for any warning signs (the nodetool<br />cfstats and cfhistograms commands are very useful for this).</p><p>At this stage you may also consider tuning some of the settings that effect the physical storage of your<br />data. For example:</p><ul><li>changing compaction strategy;</li> <li>reducing gc_grace_seconds if you are only deleting data using TTL; <strong>or</strong></li> <li>setting caching options.</li> </ul><h2>A Worked Example</h2><p>To illustrate this, I’ll walk through a basic example based on building a database to store and retrieve<br />log messages from multiple servers. Note this is quite simplified compared to most real-world<br />requirements.</p><h3>Step 1: Define the data domain</h3><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design.png\"><img class=\"aligncenter wp-image-6928 size-large\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-1024x616.png\" alt=\"Defining the data model domain Instaclustr Data Model design\" width=\"1024\" height=\"616\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-1024x616.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-300x180.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-768x462.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-966x581.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-640x385.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-80x48.png 80w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-180x108.png 180w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design.png 1576w\" /></a></p><p>The previous ER diagram illustrated the data domain. We have:</p><ul><li>Lots (millions) of log messages which have a timestamp and a body. Although message ID is<br />shown as the primary key in the ER diagram, message time plus message type is an alternate<br />primary key.</li> <li>Each log message has a message type and types are further grouped into a message category<br />(for example, a message type might be “out of memory error” and category might be “error”).<br />There a couple of hundred message types and around 20 categories.</li> <li>Each log message comes from a message source. The message source is the server that<br />generated the message. There are 1000s of servers in our system. Each message source has a<br />source type to categorise the source (e.g. red hat server, ubuntu server, windows server,<br />router, etc.). There are around 20 source types. There are ~10,000 messages per source per<br />day.</li> <li>The message body can be parsed and stored as multiple message parts (basically key, value<br />pairs). There is typically less than 20 parts per message.</li> </ul><h3>Step 2: Define the required access patterns</h3><p>We need to be able to:</p><ul><li>Retrieve all available information about the most recent 10 messages for a given source (and<br />be able to work back in time from there).</li> <li>Retrieve all available information about the most recent 10 message for a given source type.</li> </ul><h3>Step 3: Identify primary access entities</h3><p>There are two primary access entities here – source and source type. The cardinality (~20) of source<br />type makes it a good candidate for a secondary index so we will use source as the primary access<br />entity and add a secondary index for source type.</p><h3>Step 4: Allocate secondary entities</h3><p>In this example, this step is relatively simple as all data needs to roll into the log source primary access<br />entity. So we:</p><ul><li>Push down source type name</li> <li>Push down message category and message type to log message</li> <li>Push up log message as the clustering key for the new entity</li> <li>Push up message part as a map type with.</li> </ul><p>The end result is that would be a single table with a partition key of source ID and a clustering key of<br />(message time, message type).</p><h3>Step 5: Review partition and cluster keys</h3><p>Checking these partition and cluster keys against the checklist:</p><ul><li>Do your partition keys have sufficient cardinality? Yes, there are 1000s of sources.</li> <li>Will the values in your partition keys being updated frequently? No, all the data is write-once.</li> <li>Is the number of records in each partition bounded? No – messages could build up indefinitely over time.</li> </ul><p>So, we need to address the unbound partition size. A typical pattern to address that in time series<br />data such as this is to introduce a grouping of time periods into the cluster key. In this case 10,000<br />messages per day is a reasonable number to include in one partition so we’ll use day as part of our<br />partition key.</p><p>The resulting Cassandra table will look some like:</p><h2>Conclusion</h2><p>Hopefully, this process and basic example will help you start to get familiar with Cassandra data<br />modelling. We’ve only covered a basic implementation that fits well with Cassandra, however there<br />are many other examples on the web which can help you work through more complex requirements.<br />Instaclustr also provides our customers with data modelling review and assistance, so get in touch<br />with us if you need some hands-on assistance.</p>"}}]}},"pageContext":{"alternative_id":12146}}