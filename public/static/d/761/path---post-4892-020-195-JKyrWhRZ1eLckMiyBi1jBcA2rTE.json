{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"DataStax Spark Cassandra Connector","alternative_id":4892,"content":"<h2>Quick Links</h2><table><thead><tr><th>What</th>\n<th>Where</th>\n</tr></thead><tbody><tr><td>Packages</td>\n<td><a href=\"https://spark-packages.org/package/datastax/spark-cassandra-connector\">Spark Cassandra Connector Spark Packages Website</a></td>\n</tr><tr><td>Community</td>\n<td>Chat with us at <a href=\"https://github.com/datastax/spark-cassandra-connector#slack\">DataStax Academy's #spark-connector Slack channel</a></td>\n</tr><tr><td>Scala Docs</td>\n<td>Most Recent Release (2.0.5): <a href=\"https://datastax.github.io/spark-cassandra-connector/ApiDocs/2.0.4/spark-cassandra-connector/\">Spark-Cassandra-Connector</a>, <a href=\"https://datastax.github.io/spark-cassandra-connector/ApiDocs/2.0.4/spark-cassandra-connector-embedded/\">Embedded-Cassandra</a></td>\n</tr></tbody></table><h2>Features</h2><p><em>Lightning-fast cluster computing with Apache Spark™ and Apache Cassandra®.</em></p><p>This library lets you expose Cassandra tables as Spark RDDs, write Spark RDDs to Cassandra tables, and\nexecute arbitrary CQL queries in your Spark applications.</p><ul><li>Compatible with Apache Cassandra version 2.0 or higher (see table below)</li>\n<li>Compatible with Apache Spark 1.0 through 2.0 (see table below)</li>\n<li>Compatible with Scala 2.10 and 2.11</li>\n<li>Exposes Cassandra tables as Spark RDDs</li>\n<li>Maps table rows to CassandraRow objects or tuples</li>\n<li>Offers customizable object mapper for mapping rows to objects of user-defined classes</li>\n<li>Saves RDDs back to Cassandra by implicit <code>saveToCassandra</code> call</li>\n<li>Delete rows and columns from cassandra by implicit <code>deleteFromCassandra</code> call</li>\n<li>Join with a subset of Cassandra data using <code>joinWithCassandraTable</code> call</li>\n<li>Partition RDDs according to Cassandra replication using <code>repartitionByCassandraReplica</code> call</li>\n<li>Converts data types between Cassandra and Scala</li>\n<li>Supports all Cassandra data types including collections</li>\n<li>Filters rows on the server side via the CQL <code>WHERE</code> clause</li>\n<li>Allows for execution of arbitrary CQL statements</li>\n<li>Plays nice with Cassandra Virtual Nodes</li>\n<li>Works with PySpark DataFrames</li>\n</ul><h2>Version Compatibility</h2><p>The connector project has several branches, each of which map into different\nsupported versions of  Spark and Cassandra. For previous releases the branch is\nnamed \"bX.Y\" where X.Y is the major+minor version; for example the \"b1.6\" branch\ncorresponds to the 1.6 release. The \"master\" branch will normally contain\ndevelopment for the next connector release in progress.</p><table><thead><tr><th>Connector</th>\n<th>Spark</th>\n<th>Cassandra</th>\n<th>Cassandra Java Driver</th>\n<th>Minimum Java Version</th>\n<th>Supported Scala Versions</th>\n</tr></thead><tbody><tr><td>2.0</td>\n<td>2.0, 2.1, 2.2</td>\n<td>2.1.5*, 2.2, 3.0</td>\n<td>3.0</td>\n<td>8</td>\n<td>2.10, 2.11</td>\n</tr><tr><td>1.6</td>\n<td>1.6</td>\n<td>2.1.5*, 2.2, 3.0</td>\n<td>3.0</td>\n<td>7</td>\n<td>2.10, 2.11</td>\n</tr><tr><td>1.5</td>\n<td>1.5, 1.6</td>\n<td>2.1.5*, 2.2, 3.0</td>\n<td>3.0</td>\n<td>7</td>\n<td>2.10, 2.11</td>\n</tr><tr><td>1.4</td>\n<td>1.4</td>\n<td>2.1.5*</td>\n<td>2.1</td>\n<td>7</td>\n<td>2.10, 2.11</td>\n</tr><tr><td>1.3</td>\n<td>1.3</td>\n<td>2.1.5*</td>\n<td>2.1</td>\n<td>7</td>\n<td>2.10, 2.11</td>\n</tr><tr><td>1.2</td>\n<td>1.2</td>\n<td>2.1, 2.0</td>\n<td>2.1</td>\n<td>7</td>\n<td>2.10, 2.11</td>\n</tr><tr><td>1.1</td>\n<td>1.1, 1.0</td>\n<td>2.1, 2.0</td>\n<td>2.1</td>\n<td>7</td>\n<td>2.10, 2.11</td>\n</tr><tr><td>1.0</td>\n<td>1.0, 0.9</td>\n<td>2.0</td>\n<td>2.0</td>\n<td>7</td>\n<td>2.10, 2.11</td>\n</tr></tbody></table><p>*<em>Compatible with 2.1.X where X &gt;= 5</em></p><h2>Hosted API Docs</h2><p>API documentation for the Scala and Java interfaces are available online:</p><h3>2.0.5</h3><ul><li><a href=\"http://datastax.github.io/spark-cassandra-connector/ApiDocs/2.0.5/spark-cassandra-connector/\">Spark-Cassandra-Connector</a></li>\n<li><a href=\"http://datastax.github.io/spark-cassandra-connector/ApiDocs/2.0.5/spark-cassandra-connector-embedded/\">Embedded-Cassandra</a></li>\n</ul><h3>1.6.9</h3><ul><li><a href=\"http://datastax.github.io/spark-cassandra-connector/ApiDocs/1.6.9/spark-cassandra-connector/\">Spark-Cassandra-Connector</a></li>\n<li><a href=\"http://datastax.github.io/spark-cassandra-connector/ApiDocs/1.6.9/spark-cassandra-connector-embedded/\">Embedded-Cassandra</a></li>\n</ul><h3>1.5.2</h3><ul><li><a href=\"http://datastax.github.io/spark-cassandra-connector/ApiDocs/1.5.2/spark-cassandra-connector/\">Spark-Cassandra-Connector</a></li>\n<li><a href=\"http://datastax.github.io/spark-cassandra-connector/ApiDocs/1.5.2/spark-cassandra-connector-java/\">Spark-Cassandra-Connector-Java</a></li>\n<li><a href=\"http://datastax.github.io/spark-cassandra-connector/ApiDocs/1.5.0/spark-cassandra-connector-embedded/\">Embedded-Cassandra</a></li>\n</ul><h3>1.4.5</h3><ul><li><a href=\"http://datastax.github.io/spark-cassandra-connector/ApiDocs/1.4.5/spark-cassandra-connector/\">Spark-Cassandra-Connector</a></li>\n<li><a href=\"http://datastax.github.io/spark-cassandra-connector/ApiDocs/1.4.5/spark-cassandra-connector-java/\">Spark-Cassandra-Connector-Java</a></li>\n<li><a href=\"http://datastax.github.io/spark-cassandra-connector/ApiDocs/1.4.2/spark-cassandra-connector-embedded/\">Embedded-Cassandra</a></li>\n</ul><h3>1.3.1</h3><ul><li><a href=\"http://datastax.github.io/spark-cassandra-connector/ApiDocs/1.3.1/spark-cassandra-connector/\">Spark-Cassandra-Connector</a></li>\n<li><a href=\"http://datastax.github.io/spark-cassandra-connector/ApiDocs/1.3.1/spark-cassandra-connector-java/\">Spark-Cassandra-Connector-Java</a></li>\n<li><a href=\"http://datastax.github.io/spark-cassandra-connector/ApiDocs/1.3.1/spark-cassandra-connector-embedded/\">Embedded-Cassandra</a></li>\n</ul><h3>1.2.0</h3><ul><li><a href=\"http://datastax.github.io/spark-cassandra-connector/ApiDocs/1.2.0/spark-cassandra-connector/\">Spark-Cassandra-Connector</a></li>\n<li><a href=\"http://datastax.github.io/spark-cassandra-connector/ApiDocs/1.2.0/spark-cassandra-connector-java/\">Spark-Cassandra-Connector-Java</a></li>\n<li><a href=\"http://datastax.github.io/spark-cassandra-connector/ApiDocs/1.2.0/spark-cassandra-connector-embedded/\">Embedded-Cassandra</a></li>\n</ul><h2>Download</h2><p>This project is available on Spark Packages; this is the easiest way to start using the connector:\n<a href=\"https://spark-packages.org/package/datastax/spark-cassandra-connector\">https://spark-packages.org/package/datastax/spark-cassandra-connector</a></p><p>This project has also been published to the Maven Central Repository.\nFor SBT to download the connector binaries, sources and javadoc, put this in your project\nSBT config:</p><pre>libraryDependencies += \"com.datastax.spark\" %% \"spark-cassandra-connector\" % \"2.0.3\"\n</pre><ul><li>The default Scala version for Spark 2.0+ is 2.11 please choose the appropriate build. See the\n<a href=\"https://github.com/datastax/spark-cassandra-connector/blob/master/doc/FAQ.md\">FAQ</a> for more information</li>\n</ul><h2>Building</h2><p>See <a href=\"https://github.com/datastax/spark-cassandra-connector/blob/master/doc/12_building_and_artifacts.md\">Building And Artifacts</a></p><h2>Documentation</h2><h2>Online Training</h2><h3>DataStax Academy</h3><p>DataStax Academy provides free online training for Apache Cassandra and DataStax Enterprise. In <a href=\"https://academy.datastax.com/courses/ds320-analytics-with-apache-spark\">DS320: Analytics with Spark</a>, you will learn how to effectively and efficiently solve analytical problems with Apache Spark, Apache Cassandra, and DataStax Enterprise. You will learn about Spark API, Spark-Cassandra Connector, Spark SQL, Spark Streaming, and crucial performance optimization techniques.</p><h2>Community</h2><h3>Reporting Bugs</h3><p>New issues may be reported using <a href=\"https://datastax-oss.atlassian.net/browse/SPARKC/\">JIRA</a>. Please include\nall relevant details including versions of Spark, Spark Cassandra Connector, Cassandra and/or DSE. A minimal\nreproducible case with sample code is ideal.</p><h3>Mailing List</h3><p>Questions and requests for help may be submitted to the <a href=\"https://groups.google.com/a/lists.datastax.com/forum/#!forum/spark-connector-user\">user mailing list</a>.</p><h3>Slack</h3><p>The project uses Slack to facilitate conversation in our community. Find us in the <code>#spark-connector</code> channel at <a href=\"https://academy.datastax.com/slack\">DataStax Academy Slack</a>.</p><h2>Contributing</h2><p>To protect the community, all contributors are required to sign the <a href=\"http://spark-cassandra-connector-cla.datastax.com/\">DataStax Spark Cassandra Connector Contribution License Agreement</a>. The process is completely electronic and should only take a few minutes.</p><p>To develop this project, we recommend using IntelliJ IDEA. Make sure you have\ninstalled and enabled the Scala Plugin. Open the project with IntelliJ IDEA and\nit will automatically create the project structure from the provided SBT\nconfiguration.</p><p><a href=\"https://github.com/datastax/spark-cassandra-connector/blob/master/doc/developers.md\">Tips for Developing the Spark Cassandra Connector</a></p><p>Checklist for contributing changes to the project:</p><ul><li>Create a <a href=\"https://datastax-oss.atlassian.net/projects/SPARKC/issues\">SPARKC JIRA</a></li>\n<li>Make sure that all unit tests and integration tests pass</li>\n<li>Add an appropriate entry at the top of CHANGES.txt</li>\n<li>If the change has any end-user impacts, also include changes to the ./doc files as needed</li>\n<li>Prefix the pull request description with the JIRA number, for example: \"SPARKC-123: Fix the ...\"</li>\n<li>Open a pull-request on GitHub and await review</li>\n</ul><h2>Testing</h2><p>To run unit and integration tests:</p><pre>./sbt/sbt test\n./sbt/sbt it:test\n</pre><p>By default, integration tests start up a separate, single Cassandra instance and run Spark in local mode.\nIt is possible to run integration tests with your own Cassandra and/or Spark cluster.\nFirst, prepare a jar with testing code:</p><pre>./sbt/sbt test:package\n</pre><p>Then copy the generated test jar to your Spark nodes and run:</p><pre>export IT_TEST_CASSANDRA_HOST=&lt;IP of one of the Cassandra nodes&gt;\nexport IT_TEST_SPARK_MASTER=&lt;Spark Master URL&gt;\n./sbt/sbt it:test\n</pre><h2>Generating Documents</h2><p>To generate the Reference Document use</p><pre>./sbt/sbt spark-cassandra-connector-unshaded/run (outputLocation)\n</pre><p>outputLocation defaults to doc/reference.md</p><h2>License</h2><p>Copyright 2014-2017, DataStax, Inc.</p><p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p><p><a href=\"http://www.apache.org/licenses/LICENSE-2.0\">http://www.apache.org/licenses/LICENSE-2.0</a></p><p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"}}]}},"pageContext":{"alternative_id":4892}}