{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Jepsen: Cassandra","alternative_id":13092,"content":"<p><em>Previously on <a href=\"https://aphyr.com/tags/jepsen\">Jepsen</a>, we learned about <a href=\"http://aphyr.com/posts/293-call-me-maybe-kafka\">Kafka’s proposed replication design</a>.</em></p><p>Cassandra is a Dynamo system; like Riak, it divides a hash ring into a several chunks, and keeps N replicas of each chunk on different nodes. It uses tunable quorums, hinted handoff, and active anti-entropy to keep replicas up to date. Unlike the Dynamo paper and some of its peers, Cassandra eschews vector clocks in favor of a pure last-write-wins approach.</p><h2>Some Write Loses</h2><p>If you read <a href=\"http://aphyr.com/posts/285-call-me-maybe-riak\">the Riak article</a>, you might be freaking out at this point. In Riak, last-write-wins resulted in dropping 30-70% of writes, even with the strongest consistency settings (R=W=PR=PW=ALL), even with a perfect lock service ensuring writes did not occur simultaneously. To understand why, I’d like to briefly review the problem with last-write-wins in asynchronous networks.</p><div class=\"left\"><img class=\"attachment\" src=\"https://aphyr.com/data/posts/294/cassandra-lww-diagram.jpg\" alt=\"cassandra-lww-diagram.jpg\" title=\"cassandra-lww-diagram.jpg\" /></div><p>In this causality diagram, two clients (far left and far right) add the elements “a”, “b”, and “c” to a set stored in an LWW register (middle line). The left client adds a, which is read by both clients. One client adds b, constructing the set [a b]. The other adds c, constructing the set [a c]. Both write their values back. Because the register is last-write-wins, it preserves whichever arrives with the highest timestamp. In this case, it’s as if the write from the client on the left <em>never even happened</em>. However, it could just as easily have discarded the write from the right-hand client. Without a strong external coordinator, there’s just no way to tell whose data will be preserved, and whose will be thrown away.</p><p>Again: in an LWW register, the only conditions under which you can guarantee your write will not be silently ignored are when the register’s value is <em>immutable</em>. If you never change the value, it doesn’t matter which copy you preserve.</p><p>Vector clocks avoid this problem by identifying conflicting writes, and allowing you to merge them together. </p><p><img class=\"attachment\" src=\"https://aphyr.com/data/posts/294/cassandra-vclock-merge.jpg\" alt=\"cassandra-vclock-merge.jpg\" title=\"cassandra-vclock-merge.jpg\" /></p><p>Because there’s no well-defined order for potential conflicts, the merge function needs to be associative, commutative, and idempotent. If it satisfies those three properties (in essence, if you can merge any values in any order and get the same result), the system forms a <em>semilattice</em> known as a <a href=\"http://pagesperso-systeme.lip6.fr/Marc.Shapiro/papers/RR-6956.pdf\">CRDT</a>, and you recover a type of order-free consistency known as <em>lattice consistency</em>. Last-write-wins is a particular type of CRDT–albeit, not a particularly good one, because it destroys information nondeterministically.</p><p>Early in Cassandra’s history, Cassandra chose <em>not</em> to implement vector clocks for performance reasons. Vclocks (typically) require a read before each write. By using last-write-wins in all cases, and ignoring the causality graph, Cassandra can cut the number of round trips required for a write from 2 to 1, and obtain a significant speedup. The downside is that there is no safe way to modify a Cassandra cell.</p><p>Some people claim you <em>can</em> serialize updates to a cell by perfectly synchronizing your clocks, using ConsistencyLevel.QUORUM or ALL, and using an external lock service to prevent simultaneous operations. Heck, the official Cassandra documentation even claims this:</p><p><img class=\"attachment\" src=\"https://aphyr.com/data/posts/294/cassandra-cap.png\" alt=\"cassandra-cap.png\" title=\"cassandra-cap.png\" /></p><p><img class=\"attachment\" src=\"https://aphyr.com/data/posts/294/cassandra-consistency.png\" alt=\"cassandra-consistency.png\" title=\"cassandra-consistency.png\" /></p><p>As we’ll see throughout this post, the Cassandra documentation can be less than accurate. Here’s a <a href=\"https://github.com/aphyr/jepsen/blob/master/src/jepsen/cassandra.clj#L35-L80\">Jepsen test</a> which mutates the same cell repeatedly, using perfectly synchronized clocks, QUORUM consistency, and a perfect lock service:</p><code class=\"block\">$ lein run lock cassandra\n...\nWrites completed in 200.036 seconds\n2000 total\n1009 acknowledged\n724 survivors\n285 acknowledged writes lost! (╯°□°）╯︵ ┻━┻\n1 3 6 8 11 13 ... 1986 1988 1991 1993 1996 1998\n0.5045 ack rate\n0.2824579 loss rate\n0.0 unacknowledged but successful rate</code><p>Losing 28% of your supposedly committed data is not linearizable by any definition. Next question.</p><h2>CQL and CRDTs</h2><p>Without vector clocks, Cassandra can’t safely <em>change</em> a cell–but writing immutable data <em>is</em> safe. Consequently, Cassandra has evolved around those constraints, allowing you to efficiently journal thousands of cells to a single row, and to retrieve them in sorted order. Instead of modifying a cell, you write each distinct change to its own UUID-keyed cell. Then, at read time, you read all the cells back and apply a merge function to obtain a result.</p><div class=\"left\"><img class=\"attachment\" src=\"https://aphyr.com/data/posts/294/cassandra-immutable-oplog-2.jpg\" alt=\"cassandra-immutable-oplog-2.jpg\" title=\"cassandra-immutable-oplog-2.jpg\" /></div><div class=\"right\"><img class=\"attachment\" src=\"https://aphyr.com/data/posts/294/cassandra-merge.jpg\" alt=\"cassandra-merge.jpg\" title=\"cassandra-merge.jpg\" /></div><p>Cassandra’s query language, CQL, provides some collection-oriented data structures around this model: sets, lists, maps, and so forth. They’re CRDTs, though the semantics don’t align with what you’ll find in the INRIA paper–no G-sets, 2P-sets, OR-sets, etc. However, some operations <em>are</em> safe–for instance, <a href=\"https://github.com/aphyr/jepsen/blob/master/src/jepsen/cassandra.clj#L116-L149\">adding elements to a CQL set</a>:</p><code class=\"block\">0 unrecoverable timeouts\nCollecting results.\nWrites completed in 200.036 seconds\n2000 total\n2000 acknowledged\n2000 survivors\nAll 2000 writes succeeded. :-D</code><p>That’s terrific! This is the same behavior we saw with G-sets in Riak. However, not all CQL collection operations are intuitively correct. In particular, I’d be wary of the index-based operations for lists, updating elements in a map, and <em>any</em> type of deletions. Deletes are implemented by writing special tombstone cells, which declare a range of other cells to be ignored. Because Cassandra <em>doesn’t</em> use techniques like OR-sets, you can potentially delete records that haven’t been seen yet–even delete writes from the future. Cassandra users jokingly refer to this behavior as “doomstones”.</p><p>The important thing to remember is that because there are no ordering constraints on writes, one’s merge function must still be associative and commutative. Just as we saw with Riak, AP systems require you to reason about <em>order-free</em> data structures. In fact, Cassandra and Riak are (almost) formally equivalent in their consistency semantics–the primary differences are in the granularity of updates, in garbage collection/history compaction, and in performance.</p><p>Bottom line: CQL collections are a great idea, and you should use them! Read the specs carefully to figure out whether CQL operations meet your needs, and if they don’t, you can always <a href=\"http://www.slideshare.net/rbranson/cassandra-at-instagram-aug-2013\">write your own CRDTs on top of wide rows</a> yourself.</p><h2>Counters</h2><p>If you’re familiar with CRDTs, you might be wondering whether Cassandra’s counter type is a PN-counter–a commutative, monotonic data structure which can be incremented and decremented in an eventually consistent way. The answer is no: Cassandra (via Twitter, politics, etc), wound up with a less safe type of data structure. Consequently, Cassandra counters will over- or under-count by a wide range during a network partition.</p><p>If partitioned for about half of the test run, I found counters could drift by up to 50% of the expected value. Here’s a relatively well-behaved run, drifting by less than a percent.</p><code class=\"block\">10000 total\n9700 acknowledged\n9921 survivors</code><h2>Isolation</h2><p>In <a href=\"http://www.datastax.com/dev/blog/row-level-isolation\">Coming up in Cassandra 1.1: Row Level Isolation</a>, and <a href=\"http://www.datastax.com/dev/blog/atomic-batches-in-cassandra-1-2\">Atomic batches in Cassandra 1.2</a>, DataStax asserts that a write which updates multiple keys in the same row will be atomic and isolated.</p><blockquote>\n<p>Cassandra 1.1 guarantees that if you update both the login and the password in the same update (for the same row key) then no concurrent read may see only a partial update.</p>\n</blockquote><p>And from <a href=\"http://www.datastax.com/documentation/cassandra/1.2/webhelp/?pagename=docs&amp;version=1.2&amp;file=#cassandra/dml/dml_about_transactions_c.html\">the official documentation on concurrency control</a>:</p><blockquote>\n<p>Full row-level isolation is now in place so that writes to a row are isolated to the client performing the \nwrite and are not visible to any other user until they are complete.\nFrom a transactional ACID (atomic, consistent, isolated, durable) standpoint, this enhancement now gives \nCassandra transactional AID support. </p>\n</blockquote><p>We know what “atomic” means: either all of the changes in the transaction complete, or none of them do. But what does “isolated” mean? Isolated in the sense of ACID? Let’s ask Hacker News what <em>they</em> think Cassandra’s isolation provides:</p><p><img class=\"attachment\" src=\"https://aphyr.com/data/posts/294/isolation4.png\" alt=\"isolation4.png\" title=\"isolation4.png\" /><img class=\"attachment\" src=\"https://aphyr.com/data/posts/294/isolation5.png\" alt=\"isolation5.png\" title=\"isolation5.png\" /><img class=\"attachment\" src=\"https://aphyr.com/data/posts/294/isolation2.png\" alt=\"isolation2.png\" title=\"isolation2.png\" /><img class=\"attachment\" src=\"https://aphyr.com/data/posts/294/isolation1.png\" alt=\"isolation1.png\" title=\"isolation1.png\" /><img class=\"attachment\" src=\"https://aphyr.com/data/posts/294/isolation3.png\" alt=\"isolation3.png\" title=\"isolation3.png\" /></p><p>Peter Bailis pointed me at two really <a href=\"https://aphyr.com/data/posts/294/adya-phd.pdf\">excellent</a> papers on isolation and consistency, including Berenson et al’s <a href=\"https://arxiv.org/pdf/cs/0701157.pdf\">A Critique of ANSI SQL Isolation Levels</a>–I really recommend digging into them if you’re curious about this problem. Isolation comes in many flavors, or strengths, depending on what sorts of causal histories are allowed. Serializability is one of the strongest: all transactions appear to occur in a single well-defined non-interleaved order. Cursor Stability (CS) and Snapshot Isolation (SI) are somewhat weaker.</p><p>ANSI SQL defines four levels of isolation, which really have more to do with the historical behavior of various database systems than with behavior that any sane person would consider distinguishible, so I’m not going to get into the details–but suffice it to say that there are a range of <em>phenomena</em> which are prohibited by those isolation levels. In order from least to most awful:</p><ul><li>P3: Phantom </li>\n<li>P2: Fuzzy read</li>\n<li>P1: Dirty read</li>\n<li>P0: Dirty write</li>\n</ul><p>ANSI SQL’s SERIALIZABLE level prohibits P3-P0; REPEATABLE READ prohibits P2 and below, READ COMMITTED prohibits P1 and below, and READ UNCOMMITTED only prohibits P0.</p><div class=\"right\"><img class=\"attachment\" src=\"https://aphyr.com/data/posts/294/p0-example.jpg\" alt=\"p0-example.jpg\" title=\"p0-example.jpg\" /></div><div class=\"right\"><img class=\"attachment\" src=\"https://aphyr.com/data/posts/294/cassandra-comparison-diagram.jpg\" alt=\"cassandra-comparison-diagram.jpg\" title=\"cassandra-comparison-diagram.jpg\" /></div><p>P0, or “dirty write” is especially important because <em>all</em> isolation levels must prohibit it. In P0, one transaction modifies some data; then a second transaction <em>also</em> modifies that data, <em>before</em> the first transaction commits. We <em>never</em> want writes from two different transactions to be mixed together, because it might violate integrity relationships which each transaction held independently. For instance, we might write [x=1, y=1] in one transaction, and [x=2, y=2] in a different transaction, assuming that x will always be equal to y. P0 allows those transactions to result in [x=1, y=2], or [x=2, y=1].</p><p>Cassandra allows P0.</p><p>The key thing to remember here is that in Cassandara, the order of writes is completely irrelevant. Any write made to the cluster could eventually wind up winning, if it has a higher timestamp. But–what happens if Cassandra sees two copies of a cell with the same timestamp?</p><p>It picks the lexicographically bigger <em>value</em>.</p><p>That means that if the values written to two distinct cells don’t have the same sort order (which is likely), Cassandra could pick final cell values from <em>different</em> transactions. For instance, we might write [1 -1] and [2 -2]. 2 is greater than 1, so the first cell will be 2. But -1 is bigger than -2, so -1 wins in the second cell. The result? [2 -1].</p><p>“But,” you might protest, “In order for that to happen, you’d need two timestamps to collide. It’s really unlikely that two writes will get the same microsecond-resolution timestamp, right? I’ve never seen it happen in my cluster.”</p><p>Well, it depends. If we assume N writes per second by Poisson processes to the same row, the probability of any given read seeing a conflicting value grows as the writes come closer together.</p><div class=\"right\"><img class=\"attachment\" src=\"https://aphyr.com/data/posts/294/cassandra-ts-conflict-visible-chart.jpg\" alt=\"cassandra-ts-conflict-visible-chart.jpg\" title=\"cassandra-ts-conflict-visible-chart.jpg\" /></div><pre>\nrate    probability of conflict/read\n------------------------------------\n1       1.31E-7\n10      5.74E-6\n100     5.30E-5\n1000    5.09E-4\n10000   0.00504\n100000  0.0492\n1000000 0.417\n</pre><p>So if you do 100,000 writes/sec, on any given read you’ve got a 5% chance of seeing corrupt data. If you do 10 writes/sec and 1 read/sec, in each day you’ve got about a 1/3 chance of seeing corrupt data in any given day.</p><p>What if you write <em>many</em> rows over time–maybe 2 writes to each row, separated by a mean delta of 100 milliseconds? Then the theoretical probability of any given <em>row</em> being corrupt is about 5 × 10<sup>-6.</sup> That’s a pretty small probability–and remember, most applications can tolerate some small degree of corrupt data. Let’s confirm it with <a href=\"https://github.com/aphyr/jepsen/blob/master/src/jepsen/cassandra.clj#L154-L226\">an experiment</a>:</p><code class=\"block\">10000 total\n9899 acknowledged\n9942 survivors\n58 acknowledged writes lost! (╯°□°）╯︵ ┻━┻\n127 253 277 339 423 434 ... 8112 8297 8650 8973 9096 9504\n101 unacknowledged writes found! ヽ(´ー｀)ノ\n1059 1102 1139 1142 1143 1158 ... 2701 2720 2721 2800 2815 2860\n0.9899 ack rate\n0.0058591776 loss rate\n0.01020305 unacknowledged but successful rate</code><p>Note that “writes lost” here means corrupted rows: entirely missing rows are treated as successes. Roughly 1 in 200 rows were corrupt! That’s <em>way</em> worse than 10<sup>-6!</sup> What gives?</p><p>It turns out that <em>somewhere</em> in this maze of software, either Cassandra, the DataStax Java driver, or Cassaforte is taking the current time in <em>milliseconds</em> and tacking on three zeroes to the end, calling it good. The probability of millisecond conflicts is significantly higher than microsecond conflicts, which is why we saw so much corrupt data.</p><p>Long story short, Cassandra row isolation is probabilistic at best; and remember, the only reason you actually <em>want</em> isolation is because you plan on doing two operations <em>at the same time</em>. If you rely on isolation, in <em>any</em> sense of the word, in Cassandra, you need to consider your tolerance for data corruption, and <em>verify</em> that you’re actually generating timestamps with the expected distribution. A strong external coordinator which guarantees unique timestamps might be of use.</p><h2>Lightweight Transactions</h2><p>In Cassandra 2.0.0, Lightweight Transactions offer <a href=\"http://www.datastax.com/dev/blog/lightweight-transactions-in-cassandra-2-0\">linearizable consistency</a> for compare-and-set operations. The implementation is based on naive Paxos–requiring four round trips for each write–but the performance can be improved with time. The important thing is that <a href=\"http://www.datastax.com/documentation/articles/cassandra/cassandrathenandnow.html\">Cassandra is <em>first</em></a> to have a distributed linearizable data store, or something.</p><blockquote>\n<p>That said, sometimes you really do need linearizable operations. That’s why we added lightweight transactions in Cassandra 2.0 This is a sign of Cassandra maturing — Cassandra 1.0 (released October 2011) was the fulfilment of its designers original vision; Cassandra 2.0 takes it in new directions to make it even more powerful.</p>\n<p>Open source has had the reputation of producing good imitations, but not innovation. Perhaps Cassandra’s origins as a hybrid of Dynamo and Bigtable did not disprove this, but Apache Cassandra’s development of lightweight transactions and CQL are true industry firsts.</p>\n</blockquote><p>The first thing you’ll notice if you try to test the new transaction system is that the Java driver doesn’t support it. It’ll throw some weird exceptions like “unknown consistency level SERIAL”, because it doesn’t support the v2 native Cassandra protocol yet. So you’ll need to use the Python Thrift client, or, in my case, get a patched client from DataStax.</p><p>The second thing you’ll notice is deadlocks. In my Jepsen tests, the cluster would go unresponsive after the first 10 or so transactions–and it would never recover. Any further attempts to modify a cell via transaction would spin endlessly in failed transactions, until I manually truncated the system.paxos table.</p><p>You can’t make this shit up.</p><p>So you confer with DataStax for a while, and they manage to reproduce and fix the bug: <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-6029\">#6029</a> (Lightweight transactions race render primary key useless), and <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-5985\">#5985</a> (Paxos replay of in progress update is incorrect). You start building patched versions of Cassandra.</p><code class=\"block\">git checkout paxos-fixed-hopefully</code><p>Let’s give it a whirl. In <a href=\"https://github.com/aphyr/jepsen/blob/master/src/jepsen/cassandra.clj#L228-L290\">this transaction test</a>, we perform repeated compare-and-set operations against a single cell, retrying failed attempts for up to 10 seconds. The first thing you’ll notice is that those four round-trips aren’t exactly lightweight, which means that at 50 transactions/sec, the majority of transaction attempts time out:</p><p><img class=\"attachment\" src=\"https://aphyr.com/data/posts/294/cassandra-txn-latency.png\" alt=\"cassandra-txn-latency.png\" title=\"cassandra-txn-latency.png\" /></p><p>But we’re less concerned with performance or availability than <em>safety</em>. Let’s slow down the test to 5 transactions/sec to reduce contention, and check: are lightweight transactions actually linearizable?</p><code class=\"block\">2000 total\n829 acknowledged\n827 survivors\n3 acknowledged writes lost! (╯°□°）╯︵ ┻━┻\n(102 1628 1988)\n1 unacknowledged writes found! ヽ(´ー｀)ノ\n(283)\n0.4145 ack rate\n0.0036188178 loss rate\n0.0012062726 unacknowledged but successful rate</code><p>No. <b>Cassandra lightweight transactions are not even close to correct.</b> Depending on throughput, they may drop anywhere from 1-5% of acknowledged writes–and this doesn’t even require a network partition to demonstrate. It’s just a broken implementation of Paxos. In addition to the deadlock bug, these Jepsen tests revealed <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-6012\">#6012</a> (Cassandra may accept multiple proposals for a single Paxos round) and <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-6013\">#6013</a> (unnecessarily high false negative probabilities).</p><p><a href=\"http://research.google.com/archive/paxos_made_live.pdf\">Paxos is notoriously difficult to implement correctly</a>. The Chubby authors note:</p><blockquote>\n<p>Our tests start in safety mode and inject random failures into the system. After running for a predetermined period of time, we stop injecting failures and give the system time to fully recover. Then we switch\nthe test to liveness mode. The purpose for the liveness test is to verify that the system does not deadlock\nafter a sequence of failures.</p>\n</blockquote><blockquote>\n<p>This test proved useful in finding various subtle protocol errors, including errors in our group membership\nimplementation, and our modifications to deal with corrupted disks…. We found additional bugs, some of which took weeks of simulated execution time (at extremely high failure rates) to find.</p>\n</blockquote><blockquote>\n<p>Our hooks can be used to crash a\nreplica, disconnect it from other replicas for a period of time or force a replica to pretend that it is no longer\nthe master. This test found five subtle bugs in Chubby related to master failover in its first two weeks.</p>\n</blockquote><p>And in particular, I want to emphasize:</p><blockquote>\n<p>By their very nature, fault-tolerant systems try to mask problems. Thus they can mask bugs or\nconfiguration problems while insidiously lowering their own fault-tolerance.</p>\n</blockquote><p>The bugs I found were low-hanging fruit: anyone who ran a few hundred simple transactions could reproduce them, even without causing a single node or network failure. Why didn’t DataStax catch this in the release process? Why publish glowing blog posts and smug retrospectives if the most fundamental safety properties of the application haven’t been trivially verified? And if I hadn’t reported these bugs, how many users do you suppose would have been subject to silent data loss or corruption in prod?</p><p>I can’t say this strongly enough: One way or another, software is <em>always</em> tested: either by the maintainers, by users, or by applications in production. One of my goals in this series is to push database vendors to test their software <em>prior</em> to release, so that we can all enjoy safer, faster systems. If you’re writing a database, <em>please</em> try to verify its correctness experimentally. You don’t need to do a perfect job–testing is tough!–but a little effort can catch 90% of the bugs.</p><h2>Final thoughts</h2><p>DataStax and the open-source community around Cassandra have been working hard on the AP storage problem for several years, and it shows. Cassandra runs on thousand-node clusters and accepts phenomenal write volume. It’s extraordinarily suited for high-throughput capture of immutable or otherwise log-oriented data, and its AAE and tunable durability features work well. It is, in short, a capable AP datastore, and though I haven’t deployed it personally, many engineers I respect recommend it from their production experience wholeheartedly.</p><p>Jonathan Ellis, Aleksey Yeschenko‎, and Patrick McFadin were all very helpful in helping me understand Cassandra’s model, and I hope that I have depicted it accurately here. Any errors are mine alone. I’m especially thankful that they volunteered so much of their time on nights and weekends to help someone tear apart their hard work, and that they’ve fixed the bugs I’ve found so quickly. Reproducing and fixing distributed systems bugs is an especially challenging task, and it speaks to the skill of the entire Cassandra team.</p><p>DataStax has adapted some of these Jepsen tests for use in their internal testing process, and, like Basho, may use Jepsen directly to help test future releases. I’m optimistic that they’ll notify users that the transactional features are unsafe in the current release, and clarify their documentation and marketing. Again, there’s nothing technically <em>wrong</em> with many of the behaviors I’ve discussed above–they’re simply <em>subtle</em>, and deserve clear exposition so that users can interpret them correctly.</p><p>I’m looking forward to watching a good database improve.</p>"}}]}},"pageContext":{"alternative_id":13092}}