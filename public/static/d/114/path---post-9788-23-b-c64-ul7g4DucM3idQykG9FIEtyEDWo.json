{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Search and Analytics on Streaming Data With Kafka, Solr, Cassandra, Spark","alternative_id":9788,"content":"<p>In this blog post we will see how to setup a simple search and anlytics pipeline on streaming data in scala.</p><ul><li>For sample timeseries data, we will use twitter stream.</li>\n<li>For data pipelining, we will use kafka</li>\n<li>For search, we will use Solr. We will use Banana for a UI query interface for solr data.</li>\n<li>For analytics, we will store data in cassandra. We will see example of using spark for running analytics query. We will use zeppelin for a UI query interface.</li>\n</ul><p>Full code for this post is avaliable at <a href=\"https://github.com/saumitras/twitter-analysis\">https://github.com/saumitras/twitter-analysis</a></p>\n<h2>Dependencies</h2>\n<p>Create a new project and add following dependecies in build.sbt. Note that there are few conflicting dependecies in kafka so exclude them:</p>\n<figure class=\"code\"><div class=\"highlight\"><table><tr><td class=\"gutter\"><pre class=\"line-numbers\">1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n</pre></td><td class=\"code\"><pre class=\"\">libraryDependencies ++= Seq(\n  \"org.twitter4j\" % \"twitter4j-core\" % \"4.0.4\",\n  \"org.twitter4j\" % \"twitter4j-stream\" % \"4.0.4\",\n  \"com.typesafe.akka\" % \"akka-actor_2.11\" % \"2.4.17\",\n  \"org.apache.kafka\" % \"kafka_2.11\" % \"0.10.0.0\" withSources() exclude(\"org.slf4j\",\"slf4j-log4j12\") exclude(\"javax.jms\", \"jms\") exclude(\"com.sun.jdmk\", \"jmxtools\") exclude(\"com.sun.jmx\", \"jmxri\"),\n  \"org.apache.avro\" % \"avro\" % \"1.7.7\" withSources(),\n  \"org.apache.solr\" % \"solr-solrj\" % \"6.4.1\" withSources(),\n  \"com.typesafe.scala-logging\" %% \"scala-logging\" % \"3.1.0\",\n  \"ch.qos.logback\" % \"logback-classic\" % \"1.1.2\",\n  \"com.datastax.cassandra\" % \"cassandra-driver-core\"  % \"3.0.2\",\n  \"org.apache.cassandra\" % \"cassandra-clientutil\"  % \"3.0.2\",\n  \"org.apache.spark\" %% \"spark-core\" % \"2.1.0\",\n  \"org.apache.spark\" %% \"spark-sql\" % \"2.1.0\",\n  \"org.apache.spark\" %% \"spark-hive\" % \"2.1.0\",\n  \"com.datastax.spark\" %% \"spark-cassandra-connector\" % \"2.0.0\"\n)</pre></td></tr></table></div></figure><h2>Setting up twiiter stream</h2>\n<p>For streaming data from twitter you need access keys and token. You can go to <a href=\"https://apps.twitter.com\">https://apps.twitter.com</a> and creata a new app to get these. After creating an app, click on “Keys and access token” and copy following:</p>\n<ul><li>Consumer Key (API Key)</li>\n<li>Consumer Secret (API Secret)</li>\n<li>Access Token</li>\n<li>Access Token Secret</li>\n</ul><p>We will use twitter4j. Build a configuration using token and key</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\nval cb = new ConfigurationBuilder()\ncb.setDebugEnabled(true)\ncb.setOAuthConsumerKey(\"p5vABCjRWWSXNBkypnb8ZnSzk\")   //replace this with your own keys\ncb.setOAuthConsumerSecret(\"wCVFIpwWxEyOcM9lrHa9TYExbNsLGvEUgJucePPjcTx83bD1Gt\") //replace this with your own keys\ncb.setOAuthAccessToken(\"487652626-kDOFZLu8bDjFyCKUOCDa7FtHsr22WC3PMH4iuNtn\")  //replace this with your own keys\ncb.setOAuthAccessTokenSecret(\"4W3LaQTAgGoW5SsHUAgp6gK9b5AKgl8hRcFnNYgvPTylU\")  //replace this with your own keys\n</figcaption></figure><p>You can now open a stream and listen for tweets with some specific keyswords or hashtags:</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\nval stream = new TwitterStreamFactory(cb.build()).getInstance()\nval listener = new StatusListener {\n  override def onTrackLimitationNotice(i: Int): Unit = logger.warn(s\"Track limited $i tweets\")\n  override def onStallWarning(stallWarning: StallWarning): Unit = logger.error(\"Stream stalled\")\n  override def onDeletionNotice(statusDeletionNotice: StatusDeletionNotice): Unit = logger.warn(\"Status ${statusDeletionNotice.getStatusId} deleted\")\n  override def onScrubGeo(l: Long, l1: Long): Unit = logger.warn(s\"Geo info scrubbed. userId:$l, upToStatusId:$l1\")\n  override def onException(e: Exception): Unit = logger.error(\"Exception occurred. \" + e.getMessage)\n  override def onStatus(status: Status): Unit = {\n    logger.info(\"Msg: \" + status.getText)\n  }\n}\nval keywords = List(\"#scala\", \"#kafka\", \"#cassandra\", \"#solr\", \"#bigdata\", \"#apachespark\", \"#streamingdata\")\nstream.addListener(listener)\nval fq = new FilterQuery()\nfq.track(keywords.mkString(\",\"))\nstream.filter(fq)\n</figcaption></figure><p><code>StatusListener</code> provide couple of callback to handle different scenarios. <code>onStatus</code> is the one which will get the <code>tweet</code> and its metadata. <code>stream.filter(fq)</code> will start the stream.</p>\n<p>If you run this, you should start seeing the tweets:</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\nMsg: RT @botbigdata: How to build a data science team https://t.co/xJWWgueGAV #bigdata\nMsg: RT @ATEKAssetScan: Why Velocity Of Innovation Is A Data Friction Problem https://t.co/Eo1pTNCEv9 #BigData #IoT #IIoT #InternetOfThings #Art…\nMsg: Making the Most of Big Data https://t.co/X52AZ5n5nT #BigData\nMsg: RT @botbigdata: Create editable Microsoft Office charts from R https://t.co/LnSDU0iSMq #bigdata\nMsg: RT @YarmolukDan: How #Twitter Users Can Generate Better Ideas https://t.co/b0O9iEULHG #DataScience #DataScientist #BigData #IoT… \nMsg: RT @botbigdata: VIDEO: Installing TOR on an Ubuntu Virtual Machine https://t.co/Q3FPhY8CGm #bigdata</figcaption></figure><p>Lets define a type and extract out tweet metadata</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\ncase class Tweet(id:String, username:String, userId:Long, userScreenName:String,\n                   userDesc:String, userProfileImgUrl:String, favCount:Long, retweetCount:Long,\n                   lang:String, place:String, message:String, isSensitive:Boolean,\n                   isTruncated:Boolean, isFavorited:Boolean, isRetweeted:Boolean,\n                   isRetweet:Boolean, createdAt:Long)\n</figcaption></figure><figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\noverride def onStatus(status: Status): Unit = {\n  val retweetCount = if(status.getRetweetedStatus == null) 0 else status.getRetweetedStatus.getRetweetCount\n  val userDesc = if(status.getUser.getDescription == null) \"null\" else status.getUser.getDescription\n  val userProfileImgUrl = if(status.getUser.getProfileImageURL == null) \"null\" else status.getUser.getProfileImageURL\n  val lang = if(status.getLang == null) \"null\" else status.getLang\n  val place = if(status.getPlace == null) \"null\" else status.getPlace.getFullName\n  val tweet = Tweet(\n    id = status.getId.toString,\n    username = status.getUser.getName,\n    userId = status.getUser.getId,\n    userScreenName = status.getUser.getScreenName,\n    userDesc = userDesc,\n    userProfileImgUrl = userProfileImgUrl,\n    createdAt = status.getCreatedAt.getTime,\n    favCount = status.getFavoriteCount,\n    retweetCount = retweetCount,\n    lang = lang,\n    place = place,\n    message = status.getText,\n    isSensitive = status.isPossiblySensitive,\n    isTruncated = status.isTruncated,\n    isFavorited = status.isFavorited,\n    isRetweeted = status.isRetweeted,\n    isRetweet = status.isRetweet\n  )\n  logger.info(\"Msg: \" + tweet.message)\n  }\n</figcaption></figure><p>Next we will send these tweets to kafka.</p>\n<h2>Zookeeper setup</h2>\n<p>ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. All of these kinds of services are used in some form or another by distributed applications. In our example, both Kafka and Solr will need zookeeper for their state and config management, so you need to first start zookeeper.</p>\n<ul><li>Download it from <code>http://apache.org/dist/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz</code></li>\n<li>Extract it and go inside conf directory</li>\n<li>Make a copy of zoo_sample.conf as zoo.cfg</li>\n<li>Run it using <code>bin/zkServer.sh start</code></li>\n<li>Verify its started successfully by running <code>bin/zkServer.sh status</code> command.</li>\n</ul><h2>Putting data in Kafka</h2>\n<p>Here’s steps to send data to kafka.</p>\n<ul><li>Start kafka server and broker(s)</li>\n<li>Create a topic in kafka to which data will be send</li>\n<li>Define a avro schema for the tweets</li>\n<li>Create a kafka producer which will serialize tweets using avro schema and send it to kafka</li>\n</ul><p>Download kafka from here.</p>\n<p>Start server</p>\n<figure class=\"code\"><figcaption>1\nbin/kafka-server-start.sh config/server.properties\n</figcaption></figure><p>Create a topic</p>\n<figure class=\"code\"><figcaption>1\nbin/kafka-topics.sh --create --zookeeper localhost:2181/kafka --replication-factor 1 --partitions 1 --topic tweet1\n</figcaption></figure><p>You can see if topic is created successfully</p>\n<figure class=\"code\"><figcaption>1\nbin/kafka-topics.sh --list --zookeeper localhost:2181/kafka\n</figcaption></figure><h3>Avro schema</h3>\n<p>Avro is a data serialization system. It has a JSON like data model, but can be represented as either JSON or in a compact binary form. It comes with a very sophisticated schema description language that describes data. Lets define avro schema for our <code>Tweet</code> type:</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n{\n  \"type\": \"record\",\n  \"namespace\": \"tweet\",\n  \"name\": \"tweet\",\n  \"fields\":[\n    { \"name\": \"id\", \"type\":\"string\" },\n    { \"name\": \"username\", \"type\":\"string\" },\n    { \"name\": \"userId\", \"type\":\"long\" },\n    { \"name\": \"userScreenName\", \"type\":\"string\" },\n    { \"name\": \"userDesc\", \"type\":\"string\" },\n    { \"name\": \"userProfileImgUrl\", \"type\":\"string\" },\n    { \"name\": \"favCount\", \"type\":\"int\" },\n    { \"name\": \"retweetCount\", \"type\":\"int\" },\n    { \"name\": \"lang\", \"type\":\"string\" },\n    { \"name\": \"place\", \"type\":\"string\" },\n    { \"name\": \"message\", \"type\":\"string\" },\n    { \"name\": \"isSensitive\", \"type\":\"boolean\" },\n    { \"name\": \"isTruncated\", \"type\":\"boolean\" },\n    { \"name\": \"isFavorited\", \"type\":\"boolean\" },\n    { \"name\": \"isRetweeted\", \"type\":\"boolean\" },\n    { \"name\": \"isRetweet\", \"type\":\"boolean\" },\n    { \"name\": \"createdAt\", \"type\":\"long\" }\n  ]\n}\n</figcaption></figure><p>Kafka supports lot of other formats too, but avro is the preferred format for streaming data. You can read more about it here <code>https://www.confluent.io/blog/avro-kafka-data/</code></p>\n<p>Next create a producer</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\nval props = new Properties()\nprops.put(\"bootstrap.servers\", brokerList)\nprops.put(\"client.id\", \"KafkaTweetProducer\")\nprops.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\nprops.put(\"value.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\")\nval producer = new KafkaProducer[String, Array[Byte]](props)\n</figcaption></figure><p>Creata a <code>Schema</code> using the avro schema definition</p>\n<figure class=\"code\"><figcaption>1\nval schema = new Parser().parse(Source.fromURL(getClass.getResource(\"/tweet.avsc\")).mkString)\n</figcaption></figure><p>Serialize the tweet and send it to producer</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\ndef writeToKafka(tweet: Tweet) = {\n  val row = new GenericData.Record(schema)\n  row.put(\"id\", tweet.id)\n  row.put(\"username\", tweet.username)\n  row.put(\"userId\", tweet.userId)\n  row.put(\"userScreenName\", tweet.userScreenName)\n  row.put(\"userDesc\", tweet.userDesc)\n  row.put(\"userProfileImgUrl\", tweet.userProfileImgUrl)\n  row.put(\"favCount\", tweet.favCount)\n  row.put(\"retweetCount\", tweet.retweetCount)\n  row.put(\"lang\", tweet.lang)\n  row.put(\"place\", tweet.place)\n  row.put(\"message\", tweet.message)\n  row.put(\"isSensitive\", tweet.isSensitive)\n  row.put(\"isTruncated\", tweet.isTruncated)\n  row.put(\"isFavorited\", tweet.isFavorited)\n  row.put(\"isRetweeted\", tweet.isRetweeted)\n  row.put(\"isRetweet\", tweet.isRetweet)\n  row.put(\"createdAt\", tweet.createdAt)\n  val writer = new SpecificDatumWriter[GenericRecord](schema)\n  val out = new ByteArrayOutputStream()\n  val encoder = EncoderFactory.get().binaryEncoder(out, null)\n  writer.write(row, encoder)\n  encoder.flush()\n  logger.info(\"Pushing to kafka. TweetId= \" + tweet.id)\n  val data = new ProducerRecord[String, Array[Byte]](topic, out.toByteArray)\n  producer.send(data)\n}\n</figcaption></figure><p>We will create an ActorSystem and put all this inside a <code>KafkaTweetProducer</code> actor. We will then send a message to <code>KafkaTweetProducer</code> whenever a new tweet is recieved.</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nval zkHostKafka = \"localhost:2181/kafka\"\nval kafkaBrokers = \"localhost:9092\"\nval topic = \"tweet1\"\nval system = ActorSystem(\"TwitterAnalysis\")\nval kafkaProducer = system.actorOf(Props(new KafkaTweetProducer(kafkaBrokers, topic)), name = \"kafka_tweet_producer\")\nval twitterStream = new TwitterWatcher(cb, topics, kafkaProducer)\ntwitterStream.startTracking()\n</figcaption></figure><figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nclass TwitterWatcher(cb:ConfigurationBuilder, keywords:List[String], destination:ActorRef) extends Logging {\n override def onStatus(status: Status): Unit = {\n    ...\n    val tweet = Tweet(\n      ...\n    )\n    destination ! tweet\n  }\n}\n</figcaption></figure><figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\nclass KafkaTweetProducer(brokerList:String, topic:String) extends Actor with Logging {\n  override def receive: Receive = {\n    case t:Tweet =&gt;\n      writeToKafka(t)\n    ...\n  }\n}\n</figcaption></figure><p>To test whether this data is getting written in kafka properly on not, you can use the command line console consumer and watch for the topic <code>tweet1</code>:</p>\n<figure class=\"code\"><figcaption>1\nbin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic tweet1 --from-beginning\n</figcaption></figure><p>Next we will consume this data in solr and cassandra</p>\n<h2>Putting data in solr</h2>\n<p>Here’s steps for writing data to solr:</p>\n<ul><li>Define a solr schema(config-set) corresponding to tweet type</li>\n<li>Upload the schmea to zookeeper</li>\n<li>Creata a collection in solr using this config set</li>\n<li>Create a solr consumer which will read from <code>tweet1</code> topic from kafka</li>\n<li>Deserialize the data read from kafka and create solr documents from it</li>\n<li>Send documents to solr</li>\n</ul><p>Here’s what the shema definition will look like:</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n &lt;field name=\"id\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"username\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"userId\" type=\"tlong\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"userScreenName\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"userDesc\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"userProfileImgUrl\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"favCount\" type=\"tlong\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"retweetCount\" type=\"tlong\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"lang\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"place\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"message\" type=\"text_en\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"isSensitive\" type=\"boolean\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"isTruncated\" type=\"boolean\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"isFavorited\" type=\"boolean\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"isRetweeted\" type=\"boolean\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"isRetweet\" type=\"boolean\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"createdAt\" type=\"tdate\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n</figcaption></figure><p>Upload the configset to solr and create a collection:</p>\n<figure class=\"code\"><figcaption>1\n./server/scripts/cloud-scripts/zkcli.sh -cmd upconfig -zkhost localhost:2181 -confdir tweet-schema -confname tweet-schema\n</figcaption></figure><p>Create the collection</p>\n<figure class=\"code\"><figcaption>1\nhttp://localhost:8983/solr/admin/collections?action=create&amp;name=tweet&amp;collection.configName=tweet-schema&amp;numShards=1\n</figcaption></figure><p>Next create a <code>SolrWriter</code> actor which will recieve a <code>Tweet</code> message from a <code>KafkaSolrComsumer</code> (which we will define next), convert it to <code>SolrInputDocument</code> and send it to solr</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\nclass SolrWriter(zkHost: String, collection: String, commitAfterBatch: Boolean) extends Actor with Logging {\n  val client = new CloudSolrClient.Builder().withZkHost(zkHost).build()\n  client.setDefaultCollection(collection)\n  ...\n  var batch = List[SolrInputDocument]()\n  val MAX_BATCH_SIZE = 100\n  override def receive: Receive = {\n    case doc: Tweet =&gt;\n      val solrDoc = new SolrInputDocument()\n      solrDoc.setField(\"id\", doc.id)\n      solrDoc.setField(\"username\", doc.username)\n      ...\n      batch = solrDoc :: batch\n      if (batch.size &gt; MAX_BATCH_SIZE) indexBatch()\n    case FlushBuffer =&gt;\n      indexBatch()\n    case _ =&gt;\n      logger.warn(\"Unknown message\")\n  }\n  def indexBatch(): Boolean = {\n    try {\n      logger.info(\"Flushing batch\")\n      client.add(batch.asJavaCollection)\n      batch = List[SolrInputDocument]()\n      if (commitAfterBatch) client.commit()\n      true\n    } catch {\n      case ex: Exception =&gt;\n        logger.error(s\"Failed to indexing solr batch. Exception is \" + ex.getMessage)\n        ex.printStackTrace()\n        batch = List[SolrInputDocument]()\n        false\n    }\n  }\n  ...\n}\n</figcaption></figure><p>Now we need to define a kafka consumer which will read data from solr and send it to <code>SolrWriter</code></p>\n<h3>Kafka Consumer</h3>\n<p>Consumer will read data from kafka, deserialize it using avro schema, and convert it to <code>Tweet</code> type and forward the message to a destination actor. We will keep the consumer generic so that any destination actor(solr or cassandra) can be passed to it.</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\nclass KafkaTweetConsumer(zkHost:String, groupId:String, topic:String, destination:ActorRef) extends Actor with Logging {\n  ...\n  def read() = try {\n    ...\n    destination ! tweet   //destination will be either solr or cassandra\n    ...\n  }\n}\n</figcaption></figure><p>Create consumer and avro schema object</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nprivate val props = new Properties()\nprops.put(\"group.id\", groupId)\nprops.put(\"zookeeper.connect\", zkHost)\nprops.put(\"auto.offset.reset\", \"smallest\")\nprops.put(\"consumer.timeout.ms\", \"120000\")\nprops.put(\"auto.commit.interval.ms\", \"10000\")\nprivate val consumerConfig = new ConsumerConfig(props)\nprivate val consumerConnector = Consumer.create(consumerConfig)\nprivate val filterSpec = new Whitelist(topic)\nval schemaString = Source.fromURL(getClass.getResource(\"/tweet.avsc\")).mkString\nval schema = new Schema.Parser().parse(schemaString)\n</figcaption></figure><p>Convert binary data to <code>Tweet</code> type using avro</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\nprivate def getTweet(message: Array[Byte]): Tweet = {\n    val reader = new SpecificDatumReader[GenericRecord](schema)\n    val decoder = DecoderFactory.get().binaryDecoder(message, null)\n    val record = reader.read(null, decoder)\n    val tweet = Tweet(\n      id = record.get(\"id\").toString,\n      username = record.get(\"username\").toString,\n      ...\n    )\n    tweet\n  }\n</figcaption></figure><p>Start consuming from kafka and send messages to destination, Solr in this specific case.</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\nval streams = consumerConnector.createMessageStreamsByFilter(filterSpec, 1,new DefaultDecoder(), new DefaultDecoder())(0)\nlazy val iterator = streams.iterator()\nwhile (iterator.hasNext()) {\n  val tweet = getTweet(iterator.next().message())\n  //logger.info(\"Consuming tweet: \" + tweet.id)\n  destination ! tweet\n}\n</figcaption></figure><p>You shoud now start seeing data in solr:</p>\n<figure class=\"code\"><figcaption>1\nhttp://localhost:8983/solr/tweet/select?q=*:*&amp;wt=json&amp;rows=1\n</figcaption></figure><figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n{\n   \"responseHeader\":{\n      \"zkConnected\":true,\n      \"status\":0,\n      \"QTime\":1,\n      \"params\":{\n         \"q\":\"*:*\",\n         \"rows\":\"1\",\n         \"wt\":\"json\"\n      }\n   },\n   \"response\":{\n      \"numFound\":42,\n      \"start\":0,\n      \"docs\":[\n         {\n            \"id\":\"923302396612182016\",\n            \"username\":\"Tawanna Kessler\",\n            \"userId\":898322458742337536,\n            \"userScreenName\":\"tawanna_kessler\",\n            \"userDesc\":\"null\",\n            \"userProfileImgUrl\":\"http://pbs.twimg.com/profile_images/898323854417940484/lke3BSjt_normal.jpg\",\n            \"favCount\":0,\n            \"retweetCount\":183,\n            \"lang\":\"en\",\n            \"place\":\"null\",\n            \"message\":\"RT @craigbrownphd: Two upcoming webinars: Two new Microsoft webinars are taking place over the next week that may… https://t.co/SAb9CMmVXY…\",\n            \"isSensitive\":false,\n            \"isTruncated\":false,\n            \"isFavorited\":false,\n            \"isRetweeted\":false,\n            \"isRetweet\":true,\n            \"createdAt\":\"2017-10-26T03:07:00Z\",\n            \"_version_\":1582267022370144256\n         }\n      ]\n   }\n}\n</figcaption></figure><h2>Querying solr data with banana</h2>\n<p>Banana is a data visualization tool that uses solr for data analysis and display. It can be run in same container as solr. Here’s how to set it up:</p>\n<p>Here’s how to set it up for our tweet data. We will run it in same container as solr:</p>\n<p>Download banana and put it in solr’s webapp direcory</p>\n<figure class=\"code\"><figcaption>1\n2\ncd SOLR_HOME/server/solr-webapp/webapp/\ngit clone https://github.com/lucidworks/banana --depth 1\n</figcaption></figure><p>To save dashboards and setting, banana expects a collection named <code>banana-int</code>. Lets go ahead and create it. Configset for that collection can be obtained found in <code>banana/resources/banana-int-solr-5.0/</code>.</p>\n<p>Upload banana config to zookeeper</p>\n<figure class=\"code\"><figcaption>1\n$SOLR_HOME/server/scripts/cloud-scripts/zkcli.sh -cmd upconfig -zkhost localhost:2181 -confdir banana-int-solr-5.0/conf/ -confname banana\n</figcaption></figure><p>Create the collection</p>\n<figure class=\"code\"><figcaption>1\nhttp://localhost:8983/solr/admin/collections?action=create&amp;name=banana-int&amp;collection.configName=banana&amp;numShards=1\n</figcaption></figure><p>Navigate to banana UI at <code>http://localhost:8983/solr/banana/src/index.html</code> and change the collection in settings to point to <code>tweet</code> collection in</p>\n<p>Here’s what it will look like for our tweets data:</p>\n<p><img src=\"http://saumitra.me/images/posts/banana1.png\" alt=\"image\" /></p>\n<p>Next we will create a cassandra consumer.</p>\n<h2>Putting data in cassandra</h2>\n<ul><li>Download cassandra from <a href=\"http://archive.apache.org/dist/cassandra/3.0.12/apache-cassandra-3.0.12-bin.tar.gz\">http://archive.apache.org/dist/cassandra/3.0.12/apache-cassandra-3.0.12-bin.tar.gz</a> and uncompress it</li>\n<li>Run <code>bin/cassandra</code> to start it</li>\n</ul><p>We need to first create a keyspace and table for storing tweets</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\nCREATE KEYSPACE twitter WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\nCREATE TABLE twitter.tweet (\n  topic text,\n  id text,\n  username text,\n  userId text,\n  userScreenName text,\n  userDesc text,\n  userProfileImgUrl text,\n  favCount bigint,\n  retweetCount bigint,\n  lang text,\n  place text,\n  message text,\n  isSensitive boolean,\n  isTruncated boolean,\n  isFavorited boolean,\n  isRetweeted boolean,\n  isRetweet boolean,\n  createdAt timestamp,\n  creationDate timestamp,\n  PRIMARY KEY ((topic, creationDate), username, id)\n)\n</figcaption></figure><p>Then we will create a <code>CassWriter</code> actor similar to solr one which will accept a tweet message and write it to cassandra.</p>\n<p>Connect to cluster.</p>\n<figure class=\"code\"><figcaption>1\n2\nlazy val cluster = Cluster.builder().addContactPoint(seeds).build()\nlazy val session = cluster.connect(keyspace)\n</figcaption></figure><p>Since we will be using same query repeatedly to insert data with different parameters, hence we will use prepared statement to improve performance:</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\nlazy val prepStmt = session.prepare(s\"INSERT INTO $cf (\" +\n      \"topic, id, username, userId, userScreenName, userDesc, userProfileImgUrl, favCount,\" +\n      \"retweetCount, lang, place, message, isSensitive, isTruncated, isFavorited, isRetweeted,\" +\n      \"isRetweet, createdAt, creationDate\" +\n      \") values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\")\n</figcaption></figure><p>Take <code>Tweet</code>, create a <code>BoundStatement</code> by setting values for all fields and write it to cassandra</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\ndef writeToCass(t:Tweet) = {\n  try {\n    val boundStmt = prepStmt.bind()\n      .setString(\"topic\", topic)\n      .setString(\"id\",t.id)\n      .setString(\"username\", t.username)\n      .setString(\"userId\", t.userId.toString)\n      .setString(\"userScreenName\",t.userScreenName)\n      .setString(\"userDesc\",t.userDesc)\n      .setString(\"userProfileImgUrl\",t.userProfileImgUrl)\n      .setLong(\"favCount\",t.favCount)\n      .setLong(\"retweetCount\",t.retweetCount)\n      .setString(\"lang\",t.lang)\n      .setString(\"place\",t.place)\n      .setString(\"message\",t.message)\n      .setBool(\"isSensitive\",t.isSensitive)\n      .setBool(\"isTruncated\",t.isTruncated)\n      .setBool(\"isFavorited\",t.isFavorited)\n      .setBool(\"isRetweeted\",t.isRetweeted)\n      .setBool(\"isRetweet\",t.isRetweet)\n      .setTimestamp(\"createdAt\", new Date(t.createdAt))\n      .setTimestamp(\"creationDate\", new Date(t.createdAt))\n    session.execute(boundStmt)\n  } catch {\n    case ex: Exception =&gt;\n      logger.error(\"C* insert exception. Message: \" + ex.getMessage)\n  }\n}\n</figcaption></figure><p>We will create a new instance of this actor</p>\n<figure class=\"code\"><figcaption>1\nval cassWriter = system.actorOf(Props(new CassWriter(cassSeeds, cassKeyspace, cassCf, topic)), name = \"cass_writer\")\n</figcaption></figure><p>And then create a new <code>KafkaTweetConsumer</code> whose destination will be this <code>cassWriter</code> actor</p>\n<figure class=\"code\"><figcaption>1\n2\nval cassConsumer = system.actorOf(Props(\n    new KafkaTweetConsumer(zkHostKafka, \"tweet-cass-consumer\", topic, cassWriter)), name = \"cass_consumer\")\n</figcaption></figure><p>You should start seeing data in cassandra</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\ncqlsh&gt; select creationdate, userscreenname, lang, message from twitter.tweet limit 1;\n creationdate             | userscreenname | lang | message\n--------------------------+----------------+------+--------------------------------------------------------------------------------------------------------------------------------------------\n 2017-10-25 21:56:30+0000 |   alevergara78 |   en | RT @HomesAtMetacoda: data in motion &gt;&gt; Online learning: #MachineLearning’s secret for #bigdata via\\n@SASsoftware https://t.co/eGbAumJzEt…\n</figcaption></figure><p>Next we will setup spark and use it to query cassandra data.</p>\n<h2>Query cassandra data with spark</h2>\n<p>We will use datastax spark cassandra connector <a href=\"https://github.com/datastax/spark-cassandra-connector.\">https://github.com/datastax/spark-cassandra-connector.</a> Download the correct connection version jar and place it in lib directory of your project:</p>\n<p>First thing which we need is a spark context</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nval CASS_SEEDS = \"127.0.0.1\"\nval SPARK_MASTER = \"spark://sam-ub:7077\"\nval conf = new SparkConf(true)\n  .set(\"spark.cassandra.connection.host\", CASS_SEEDS)\n  .setJars(Seq(\"lib/spark-cassandra-connector-assembly-2.0.0.jar\"))\n  .setMaster(SPARK_MASTER)\n  .setAppName(\"cass_query\")\nlazy val sc = new SparkContext(conf)\n</figcaption></figure><p>Then you can query and apply different aggregrations. This query will be picked up as a spark job and exectuted on you spark cluster:</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\nval data = sc.cassandraTable(\"twitter\", \"tweets\")\n             .select(\"topic\", \"creationdate\", \"retweetcount\", \"id\", \"isretweet\")\n             .where(\"topic = 'tweets' and creationdate = '2017-10-25 20:15:05+0000'\")\n             .groupBy(_.getLong(\"retweetcount\"))\n             .map(r =&gt; (r._1, r._2.size))\n             .collect()\nlogger.info(\"Count of rows = \" + data)\n</figcaption></figure><p>If job is successfull, you will see the result:</p>\n<figure class=\"code\"><figcaption>1\nCount of rows = 38\n</figcaption></figure><h2>Visulizing cassandra data with zeppelin</h2>\n<p>Zeppelin is a web-based notebook that can be used for interactive data analytics on cassandra data using spark.</p>\n<p>Download the binary from <a href=\"https://zeppelin.apache.org/download.html\">https://zeppelin.apache.org/download.html</a> and uncompress it.\nDefault port used by it is <code>8080</code> which conflicts with spark master web ui port, so change the port in <code>conf/zeppelin-site.xml</code>.</p>\n<p>Create a new notebook and select <code>spark interpreter</code></p>\n<p>Create a view of our <code>tweet</code> table from cassandra</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n%spark.sql\ncreate temporary view mytweets\nusing org.apache.spark.sql.cassandra\noptions (keyspace \"twitter\", table \"tweet\")\n</figcaption></figure><p>We can now run aggregations or other analytics queries on this view:</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n%spark.sql\nselect lang, count(*) as occur from mytweets where lang != 'und' group by lang order by occur desc limit 10\n</figcaption></figure><p>Here’s what output of above query will look like:</p>\n<p><img src=\"http://saumitra.me/images/posts/zepp1.png\" alt=\"image\" /></p>\n<h2>Conclusion</h2>\n<p>I hope you got the idea of how to get started with creating a search and analytics pipeline.</p>"}}]}},"pageContext":{"alternative_id":9788}}