{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Let’s play with Cassandra… (Part 1/3)","alternative_id":11850,"content":"<p>I have already talked about it but <a href=\"https://blog.octo.com/en/nosqleu-and-nosql-what&#x2019;s-the-deal/\">NoSQL</a> is about diversity and includes various different tools and even kind of tools. Cassandra is one of these tools and is certainly and currently one of the most popular in the NoSQL ecosystem. <strong>Built by Facebook and currently in production at web giants like Digg, Twitter, Cassandra is a hybrid solution between <a href=\"http://www.allthingsdistributed.com/2007/10/amazons_dynamo.html\">Dynamo</a> and </strong><strong><a href=\"http://static.googleusercontent.com/external_content/untrusted_dlcp/labs.google.com/fr//papers/bigtable-osdi06.pdf\">BigTable</a></strong>. </p>\n<p>Hybrid firstly because <a href=\"http://www.cs.cornell.edu/projects/ladis2009/papers/lakshman-ladis2009.pdf\">Cassandra </a>uses a <strong>column-oriented way of modeling data</strong> (inspired by the BigTable) and permit to use <strong>Hadoop Map/Reduce jobs</strong> and secondly because it uses patterns inspired by Dynamo like <strong>Eventually Consistent, Gossip protocols, a master-master way of serving both read and write requests</strong>…</p>\n<p>Another DNA of Cassandra (and in fact a lot of NoSQL solutions) is that <strong>Cassandra has been built to be fully decentralized, designed for failure and Datacenter aware </strong>(in a sense you can configure Cassandra to ensure data replication between several Datacenter…). Hence, Cassandra is currently used between the <a href=\"http://spyced.blogspot.com/2010/04/cassandra-fact-vs-fiction.html\">Facebook US west and east coast datacenters</a> and stored (around two years ago) <a href=\"http://www.cs.cornell.edu/projects/ladis2009/papers/lakshman-ladis2009.pdf\">50+ TB of data on a 150 node cluster</a>.<br /></p>\n<h2>Data Modeling</h2>\n<p>The column oriented model is quite more complex to understand than the <a href=\"https://blog.octo.com/amazon-simpledb-le-harry-potter-de-voldemort/\">Key/Value model</a>. In a Key/value model, you have a key that uniquely identify a value and this value can be structured (based on a JSON format for instance) or completely unstructured (typically a BLOB). Therefore and from the basis, the simplest way to understand the column-oriented model is to begin with a Key/Value model and imagine that the Value is a collection of other Key/Value elements. In brief, this is a kind of structure where Hashmaps are included in another Hashmap…</p>\n<p>Completely lost? Here are a the main elements Cassandra defined (this <a href=\"http://arin.me/blog/wtf-is-a-supercolumn-cassandra-data-model\">article</a> or the <a href=\"http://wiki.apache.org/cassandra/DataModel\">Cassandra documentation</a> provide a more in depth view of the different types of structures you can have in Cassandra)<br />– <strong>Column</strong>. The basic element which is a tuple composed of a timestamp, a column name and a column value. The timestamp is set by the client and this has an architectural impact on clients’ clocks synchronization.<br />– <strong>SuperColumn</strong>. This structure is a little more complex. You can imagine it as a column in which can store a dynamic list of Columns.<br />– <strong>ColumnFamily</strong>: A set of columns. You can compare a ColumnFamily to a table in the relational world except the number and even (I am not sure this is the best idea but anyway) the names of columns can vary from a row to another. More important, the number of columns may vary during time (in the case for instance your schemas need to be upgraded etc…). Cassandra will not force any limitation in that case but your code will have to deal with these different schemas.<br />– <strong>KeySpaces</strong>: A set of ColumnFamily. Hence, the Keyspace configuration only defines (from the data modeling concern) the included ColumnFamily. The notion of “Row” does not exist by itself: this is a list of Columns or SuperColumns identified by a row key. </p>\n<h2>Data Partitioning and replication</h2>\n<p>Data Partitioning is one of tricky point. Depending on the studied tools, partitioning can be done either by the client library or by any node of the cluster and can be calculated using different algorithms (one of the most popular and reliable being the <a href=\"https://blog.octo.com/consistent-hashing-ou-l%E2%80%99art-de-distribuer-les-donnees/\">Consistent Hashing</a>.<br />Cassandra lets the nodes of the cluster (and not the client) partitioning the data based on the row key. Out of the box, Cassandra can nevertheless use <a href=\"http://ria101.wordpress.com/2010/02/22/cassandra-randompartitioner-vs-orderpreservingpartitioner/\">two different algorithms to distribute data over the nodes</a>. The first one is the RandomPartitionner and it gives you an equally and hash-based distribution. The second one is the OrderPreservingPartitioner and guarantees the Key are organized in a natural way. Thus, the latter facilitates the range queries since you need to hit fewer nodes to get all your ranges of data whereas the former has a better load balancing since the keys are more equally partitioned across the different nodes.<br />Then, each row (so all the Columns) is stored on the same physical node and columns are sorted based on their name. </p>\n<p>Moreover, and this is more linked to data replication, Cassandra natively enables replication across multiple datacenters and you can – by <a href=\"http://wiki.apache.org/cassandra/Operations\">configuration </a>– specify which nodes are in which Data Center. Hence, Cassandra would take care of replicating the data on at least one node in the distant Data Center (the partition-tolerant property of the CAP Theorem is so meaning full and I guess simpler to understand…). </p>\n<h2>Consistency management, conflict resolution and atomicity</h2>\n<p>NoSQL solutions like <a href=\"https://blog.octo.com/amazon-simpledb-le-harry-potter-de-voldemort/\">Voldemort </a>or <a href=\"http://incubator.apache.org/cassandra\">Cassandra </a>have chosen <a href=\"http://en.wikipedia.org/wiki/CAP_theorem\">Availability and Partition-tolerance over Consistency</a>. Thus, different strategy have been setting up: “Eventually Consistent”. </p>\n<p>Dealing with consistency is so a matter of dealing with data criticism and being able to define –for each data – the consistency level you need (based on the trade-offs the CAP Theorem imply). <strong>Cassandra defines different <a href=\"http://wiki.apache.org/cassandra/API\">levels of consistency</a></strong> and I will not go into further details but here are a couple of them:<br />– <strong>ONE</strong>. Cassandra ensures the data is written to at least one node’s commit log and memory table before responding to the client. During read, the data will be returned from the first node where it is found. In that case, you must accept stale state because you have no warranty the node you hit to read the data has the last version of the data.<br />– <strong>QUORUM</strong>. In that case, Cassandra will write the data on /2 + 1 nodes before responding to the client (the Replication factor is the number of nodes the data will be replicated and is defined for a Keyspace). For the read, the data will be read on /2 + 1 nodes before returning the data. In that case, you are sure to get a consistent data (because N is smaller than R+W where N is the total number of nodes where the data is replicated, R the number of nodes where this data is being read and W the number of nodes the data is being written)<br />– <strong>ALL</strong>. In that case, Cassandra will write and read the data from all the nodes.</p>\n<p>Of course, at a given time, chances are high that each node has its own version of the data. Conflict resolution is made during the read requests (called <a href=\"http://wiki.apache.org/cassandra/ReadRepair\">read-repair</a>) and the current version of Cassandra does not provide a Vector Clock conflict resolution mechanisms (should be available in the version 0.7). <strong>Conflict resolution is so based on timestamp</strong> (the one set when you insert the row or the column): the higher timestamp win and the node you are reading the data is responsible for that.<br />This is an important point because the timestamp is specified by the client, at the moment the column is inserted.<strong> Thus, all Cassandra clients’ need to be synchronized </strong>(based on an NTP for instance) in order to ensure the resolution conflict be reliable.</p>\n<p>Atomicity is also weaker than what we are used to in the relational world. <strong>Cassandra guarantees atomicity</strong> within a <a href=\"http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/Cassandra-guarantees-reads-and-writes-to-be-atomic-within-a-single-ColumnFamily-td4288141.html\">ColumnFamily</a> s<strong>o for all the columns of a row</strong>. </p>\n<h2>Elasticity</h2>\n<p>“Cassandra is liquid” would have written any marketer and to be honest, a lot of NoSQL solutions have been built upon this DNA. <strong>First of all, elasticity is at the data modeling level. Your data will live longer than your business rules and softness in the way your data schemas can evolve across the time is an interesting point</strong>. </p>\n<p><strong>But elasticity is also about infrastructure and cluster sizing</strong>. Adding a new node to <a href=\"http://wiki.apache.org/cassandra/Operations\">Cassandra </a>is simple. Just turn on the AutoBootstrap property and specify at least one Seed of the current cluster. The node will hence be detected, added to the cluster and the data will be relocated (the needed time depends on the amount of data to transfer). Decommissioning a node is almost as simple as adding a node except you need to use the <a href=\"http://wiki.apache.org/cassandra/NodeProbe\">nodetool utility</a> (which provides more options to visualize the streams between the nodes…) or a JMX command. </p>\n<h2>Cassandra monitoring</h2>\n<p>Cassandra runs on a JVM and exposes JMX properties. You can thus collecting monitoring information using jConsole or any JMX compliant tool. </p>\n<p>For instance, you can monitor<br />– Your nodes (which are part of the cluster, which are dead…)<br /><img src=\"https://blog.octo.com/wp-content/uploads/2010/06/cassandra-jmonitoring.png\" alt=\"\" title=\"cassandra-jmonitoring\" width=\"609\" height=\"422\" class=\"aligncenter size-full wp-image-11659\" srcset=\"https://blog.octo.com/wp-content/uploads/2010/06/cassandra-jmonitoring.png 609w, https://blog.octo.com/wp-content/uploads/2010/06/cassandra-jmonitoring-300x208.png 300w, https://blog.octo.com/wp-content/uploads/2010/06/cassandra-jmonitoring-160x111.png 160w\" /></p>\n<p>– the streams between your nodes (especially in the case you added or removed nodes)<br />– Or stats. For instance, per-Column Family basic stats would be: Read Count, Read Latency, Write Count and Write Latency etc…</p>\n<p>More JMX plugins are available <a href=\"http://github.com/jbellis/cassandra-munin-plugins\">here</a> (provides graphs…) and <a href=\"http://nosql.mypopescu.com/post/611576467/cassandra-web-console\">some guys are developing web console</a>.</p>\n<h2>Access protocol</h2>\n<p>Cassandra is called using <a href=\"http://incubator.apache.org/thrift\">Thrift</a> (even if the 0.6 version introduced <a href=\"http://avro.apache.org\">Avro</a> protocol). As we told previously, Cassandra is responsible for routing the request to the proper node so you can reach any node of your cluster to serve your request. Nonetheless, <strong>the default thrift client API does not provide any load-balancing or connection pool mechanisms</strong>.<br />Concerning the connection pool, the main lacks, in my opinion, are (1) the capacity to close and reopen connections in case a node has failed, (2) the capacity to load-balance requests among all the nodes of the cluster and (3) the capacity to automatically request another node when the first attempt fails.<br />A higher level of load-balancing could be setup on the service layer</p>\n<p><img src=\"https://blog.octo.com/wp-content/uploads/2010/06/load-balancing.png\" alt=\"\" title=\"load-balancing\" width=\"639\" height=\"312\" class=\"aligncenter size-full wp-image-11660\" srcset=\"https://blog.octo.com/wp-content/uploads/2010/06/load-balancing.png 639w, https://blog.octo.com/wp-content/uploads/2010/06/load-balancing-300x146.png 300w, https://blog.octo.com/wp-content/uploads/2010/06/load-balancing-160x78.png 160w\" /></p>\n<p>Certain library (for instance <a href=\"http://prettyprint.me/2010/03/03/load-balancing-and-improved-failover-in-hector/\">Hector</a> in the Java World) provides connections pooling mechanisms and even kind of round-robin load balancing… </p>\n<p>In the next part, we will play in more details with Cassandra…</p>"}}]}},"pageContext":{"alternative_id":11850}}