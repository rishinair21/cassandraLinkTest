{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Cassandra-Tools","alternative_id":4885,"content":"<p>This repository is a collection of automation scripts that allow you to launch and test different Cassandra configurations. You can have multiple profiles to test against to see the difference in performance between an 8GB heap and a 12GB heap for example, or making multiple changes to a yaml file vs a control cluster to see if performance or stability improved with your changes.</p><p>Cassandra tools pulled a lot of bootstrapping and node configuration from the DataStax AMI Builder tool and simplified it via Fabric so that you have more control over the changes you want to make to your cluster and giving you a clear view on what is going on.</p><p>The topology that would be recommended would be something as seen below. Where you have a dedicated node running opscenter that can be static and separate instances of stress and C*</p><p><a href=\"https://camo.githubusercontent.com/2216a88d0077ecbe79a1245e70d168072b3b4dfc/68747470733a2f2f646c2e64726f70626f7875736572636f6e74656e742e636f6d2f752f393530373731322f63617373616e6472612f746f706f6c6f67792e706e67\" target=\"_blank\"><img src=\"https://camo.githubusercontent.com/2216a88d0077ecbe79a1245e70d168072b3b4dfc/68747470733a2f2f646c2e64726f70626f7875736572636f6e74656e742e636f6d2f752f393530373731322f63617373616e6472612f746f706f6c6f67792e706e67\" alt=\"Topology\" data-canonical-src=\"https://dl.dropboxusercontent.com/u/9507712/cassandra/topology.png\" /></a></p><p>Bootsrapping a cluster supports:</p><ul><li>Mounting and formatting Drives using XFS</li>\n<li>RAID0 for testing the i2 series of instance types</li>\n</ul><p>The basic workflow is as follows:</p><ol><li>Launch a machine and install Opscenter</li>\n<li>Launch and bootstrap a cluster of C* nodes</li>\n<li>Launch and bootstrap several stress runner nodes</li>\n<li>Run stress yaml files against the cluster and monitor</li>\n</ol><p>All of these scripts expect you to have the requirements installed from requirements.txt To do that simply type from the root directory</p><pre>pip install -r requirements.txt\n</pre><p>Since this is for AWS it's expected that you have your keys exported in your shell for auth.</p><pre>export AWS_ACCESS_KEY_ID=YOURKEY\nexport AWS_SECRET_ACCESS_KEY=YOURSECRET\n</pre><h4><a href=\"https://github.com/CrowdStrike/cassandra-tools#ubuntu-note\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-ubuntu-note\"></a>Ubuntu Note</h4><p>you may need to install python dev tools to get pycrypto working</p><pre>sudo apt-get install python-dev\n</pre><p>Now let's get to launching. It's assumed that you have an AMI you want to use as your base. It can just be a base Ubuntu AMI from the AWS console for example. Once you have that AMI you'll need to configure a launch json file that has the ami id, security groups you want applied, tags, launch key, etc... so launcher.py knows how to build your instances.</p><p>Take a look at the configs/*.sample files for examples of where to plug your information in. This also assumes you're in a VPC. If you're not feel free to submit a pull request to support non VPC launching or just launch your nodes via the AWS console or CLI. Launcher does nothing fancy and doesn't bootstrap anything. It just provisions instances, which you can do yourself via the AWS console.</p><p>Copy a profile sample to work with</p><pre>cd launcher/configs/\ncp c4-highperf.json.sample c4-highperf.json\n</pre><p>Now edit that profile with your AMI ID to use, tags you want on the instances, security groups, subnets to launch in, etc...</p><p>You can see the AMI ID in this screenshot as ami-d05e75b8</p><p><a href=\"https://camo.githubusercontent.com/0ce34f118f4db7cce164da103724c1a1c5d49260/68747470733a2f2f646c2e64726f70626f7875736572636f6e74656e742e636f6d2f752f393530373731322f63617373616e6472612f616d692e706e67\" target=\"_blank\"><img src=\"https://camo.githubusercontent.com/0ce34f118f4db7cce164da103724c1a1c5d49260/68747470733a2f2f646c2e64726f70626f7875736572636f6e74656e742e636f6d2f752f393530373731322f63617373616e6472612f616d692e706e67\" alt=\"AMI\" data-canonical-src=\"https://dl.dropboxusercontent.com/u/9507712/cassandra/ami.png\" /></a></p><p>Assuming you're using the launcher script, let's fire up 3 nodes in us-east-1a using the c4-highperf profile that you created from a .sample file.</p><pre>cd launcher/\npython launch.py launch --nodes=3 --config=c4-highperf --az=us-east-1a\n</pre><p>You can repeat the process across AZ's as needed to get the final cluster topology squared away. At the end of that output you'll see  a list of IPs that it provisioned. Copy those down for future use.</p><h3><a href=\"https://github.com/CrowdStrike/cassandra-tools#creating-a-profile\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-creating-a-profile\"></a>Creating a Profile</h3><p>The first step in the process will be to use one of the sample profiles to base a new profile you want to test.</p><pre>cd manage/configs\ncp -rf c4-highperf-sample c4-highperf\n</pre><p>More docs to come on this but for now go through the files in that directory and change the 10.10.10.XX ips to ones in your environment. e..g in address.yaml put in your opscenter IP address</p><p>Files of interest:</p><ul><li>address.yaml the IP address to where your opscenter node is for reporting</li>\n<li>cassandra-env.sh contains the startup params, GC settings for config</li>\n<li>cassandra.yaml contains the properties used to control various C* settings</li>\n<li>collectd.conf if you want to use collectd to monitor via graphite, put your graphite ip in there</li>\n<li>hostfile.txt contains all the IP address for the nodes you want to manage</li>\n<li>metrics.yaml if you're reporting to graphite, send C* metrics over with a whitelist</li>\n</ul><p>NOTE: For EBS volumes it's expected you mount your drives in specific locations. e.g. commit drive goes to /dev/sdk(xvdf) and data drive goes to /mnt/sdf(xvdf) or as seen below</p><p><a href=\"https://camo.githubusercontent.com/98e7119cf87200e8bbbd60688a9bdaa97b96a511/68747470733a2f2f646c2e64726f70626f7875736572636f6e74656e742e636f6d2f752f393530373731322f63617373616e6472612f6562732e706e67\" target=\"_blank\"><img src=\"https://camo.githubusercontent.com/98e7119cf87200e8bbbd60688a9bdaa97b96a511/68747470733a2f2f646c2e64726f70626f7875736572636f6e74656e742e636f6d2f752f393530373731322f63617373616e6472612f6562732e706e67\" alt=\"EBS\" data-canonical-src=\"https://dl.dropboxusercontent.com/u/9507712/cassandra/ebs.png\" /></a></p><p>Once you have your cluster up and running you're now ready to bootstrap and provision it. The manager file expects your list of ips to be newline separated and in configs/yourconfig/hostfile.txt</p><p>Place all of your IP address in that file.</p><h3><a href=\"https://github.com/CrowdStrike/cassandra-tools#bootstrappingprovisioning\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-bootstrappingprovisioning\"></a>Bootstrapping/Provisioning</h3><p>To test out your running system (assuming you ran the pip install -r requirements.txt cmd above)</p><pre>fab -u ubuntu  bootstrapcass21:config=c4-highperf\n</pre><p>That command will run all the apt-get update/installs, install java, format the EBS volume using XFS, turn off swap, etc.... One thing to note is that the fab command will always prompt you as to which hostfile you want to run. There are times you just want to bootstrap a few nodes and not the whole cluster. So you can just put those ips anywhere like /tmp/newips.txt and use type in that file in the prompt instead.</p><p>Note that at time of writing it installs Cassandra Community version 2.1.9, if you're bored, submit a PR to make that configurable from the profile :)</p><p>Once that is complete you'll want to set your seed nodes for that cluster. So pick one IP or a comma separated list and run</p><pre>fab -u ubuntu  set_seeds:config=c4-highperf,seeds='10.10.100.XX'\n</pre><p>Once you have your seeds set up now we can start up Cassandra</p><pre>fab -u ubuntu  start_cass:config=c4-highperf\n</pre><p>At this point you should login to one or all of the instances and just do a headcheck in /var/log/cassandra/system.log to ensure everything started up ok. If you want to do a quick check on what nodes are running Cassandra you can use the getrunning task</p><pre>fab -u ubuntu  getrunning:config=c4-highperf\n</pre><p>For a list of all the commands available you can ask fab to list the tasks</p><pre>fab -l\n</pre><p>Other common tasks will be changing yaml files or cassandra-env settings for testing different GC combinations. For that you would make your changes, save the files and run</p><pre>fab -u ubuntu  configs:config=c4-highperf\n</pre><p>You can also use the free form cmd task. For example, want to see all the java versions across your cluster?</p><pre>fab -u ubuntu cmd:config=c4-highperf,cmd=\"java -version 2&gt;&amp;1 | grep version  | awk '{print \\$NF}'\"\n</pre><p>Or see how much \"MAX HEAP SIZE\" your nodes are configured for</p><pre>fab -u ubuntu cmd:config=c4-highperf,cmd=\"grep MAX_HEAP_SIZE /etc/cassandra/cassandra-env.sh | grep G\"\n</pre><p>The cmd task runs in parallel so going over 60+ nodes is within seconds.</p><p>This repo also has support for running your stress machines. The workflow I was using was the following</p><ol><li>Launch stress instances</li>\n<li>Set hostfile.txt in stress/hostfile.txt with the IP address of stress machines</li>\n<li>Bootstrap stress machines with stress 2.1 code and yaml files</li>\n<li>Use csshX to view all the stress machines in multiple terminals</li>\n<li>Tweak yaml files and re-push stress to test various configs</li>\n</ol><p>A note on stress is that there is a 70MB compiled stress tarball included so rather than push that up on every node I'll run</p><pre>fab -u ubuntu installstress\n</pre><p>which will install stress on a single node (if just one is in your hostfile) then create an AMI of that node to launch more stress boxes to make things a bit faster.</p><h3><a href=\"https://github.com/CrowdStrike/cassandra-tools#csshx\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-csshx\"></a>csshX</h3><p>csshX is a neat little program that will allow you run the same command across multiple terminal windows simultaneously.</p><p>You can find it here: <a href=\"https://github.com/brockgr/csshx\">https://github.com/brockgr/csshx</a>\nor install on OSX with</p><pre>brew install csshx\n</pre><p><a href=\"https://camo.githubusercontent.com/d57641e4995a334d273acd0b90c151d04d4c7501/687474703a2f2f7777772e62726f636b2d66616d696c792e6f72672f676176696e2f6d61636f73782f63737368582e706e67\" target=\"_blank\"><img src=\"https://camo.githubusercontent.com/d57641e4995a334d273acd0b90c151d04d4c7501/687474703a2f2f7777772e62726f636b2d66616d696c792e6f72672f676176696e2f6d61636f73782f63737368582e706e67\" alt=\"csshX view\" data-canonical-src=\"http://www.brock-family.org/gavin/macosx/csshX.png\" /></a></p><p>to fire up csshX just type in</p><pre>csshX --login ubuntu `cat ../stress/hostfile.txt`\n</pre><p>that will bring up the pane of stress machines, alt+tab if you don't see it right away and find the terminal windows.</p><h3><a href=\"https://github.com/CrowdStrike/cassandra-tools#bootstrapping-stress-nodes\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-bootstrapping-stress-nodes\"></a>Bootstrapping Stress Nodes</h3><p>to install stress and the base yaml files, you can run</p><pre>fab -u ubuntu installstress\n</pre><p>that will install all the base code needed to run stress. If you make changes to the yaml files or add new ones and profiles you can use what's below to just push those changes</p><pre>fab -u ubuntu  putstress\n</pre><p>Once the files are uploaded you're ready to run stress. Using csshX in the red area that controls the output to all terminals you can type in</p><pre>python runstress.py --profile=stress --seednode=10.10.10.XX --nodenum=1\n</pre><p>That will run the following cmd under the covers</p><pre>/home/ubuntu/apache-cassandra-2.1.5/tools/bin/cassandra-stress user duration=100000m cl=ONE profile=/home/ubuntu/summit_stress.yaml ops\\(insert=1\\) no-warmup  -pop seq=1..100000000 -mode native cql3 -node 10.10.10.XX -rate threads=1000  -errors ignore\n</pre><p>Type python runstress.py -h\nfor all the available options to pass like threads, seednode, etc...</p><p>Place the correct IP there and you should be running stress against your new cluster. Dig around the runstress.py file to see what other profiles you can run, or add your own.</p>"}}]}},"pageContext":{"alternative_id":4885}}