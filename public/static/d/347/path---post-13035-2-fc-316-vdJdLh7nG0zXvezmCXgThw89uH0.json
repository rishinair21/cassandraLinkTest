{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Getting Started with Cassandra and Spark | Codementor","alternative_id":13035,"content":"<p>This tutorial is going to go through the steps required to install <a href=\"http://www.planetcassandra.org/what-is-apache-cassandra/\" target=\"_blank\" rel=\"nofollow noopener\">Cassandra</a> and <a href=\"http://spark.apache.org/\" target=\"_blank\" rel=\"nofollow noopener\">Spark</a> on a Debian system and how to get them to play nice via Scala. Spark and Cassanrda exist for the sake of applications in Big Data, as such they are intended for installation on a cluster of computers possibly spread over multiple geographic locations. This tutorial, however, will deal with a single computer installation. The aim of this tutorial is to give you a starting point from which to configure your cluster for your specific application, and give you a few ways to make sure your software is running correctly.</p><h2 id=\"major-components\"> Major Components</h2><p>This tutorial touches on quite a few different technologies. Below is a little description for each of the major ones we'll be dealing with here.</p><h3 id=\"spark\"> Spark</h3><p>Spark has been described as the swiss army knife of big data, but what does that mean? Spark started off as a replacement for <a href=\"https://hadoop.apache.org/\" target=\"_blank\" rel=\"nofollow noopener\">Hadoop</a>, and Hadoop is a sort of industry standard tool for doing large scale distributed map-reduce calculations. Hadoop solved a bunch of problems - it took one kind of algorithm and turned it into a distributed production line thus creating an efficient and robust system for solving very specific kinds of problems. Hadoop was initially a tool (well, actually it was first a small yellow elephant, then it was a tool), and then the word started being used to refer to an ecosystem of compatible tools.</p><p>Hadoop is awesome. But it has its problems. Firstly, it can be a bit horrible to use. Writing map-reduce code can be tedious and has a lot of room for misguidedness - not everyone can do it. A few scripting tools such as <a href=\"https://pig.apache.org/\" target=\"_blank\" rel=\"nofollow noopener\">Apache Pig</a> have been developed in order to abstract away from this, but it's still a problem. Besides that, it only does map-reduce - there are many big data problems that simply cannot fit into that paradigm (or whatever you want to call it). Also, it's slow. Slower than it could be anyway - the technical details are beyond the scope of this tutorial though.</p><p>Spark is a relatively recent addition to the Hadoop ecosystem and works to solve a few of the problems of vanilla Hadoop. For one thing, it is easier to use, allowing users to specify map-reduce jobs by simply opening up a shell and writing code that is generally readable, maintainable and quick to write. Users can thus execute ad-hoc queries or submit larger jobs. Spark aims to make better use of system resources, especially RAM, and is touted as being 10 times faster than Hadoop for some workloads. Spark also does stuff that doesn't fall into the map-reduce way of thinking - for example, it allows for iterative processing, something vanilla Hadoop is ill-suited for. Spark also works with any Hadoop compatible storage, that makes converting from Hadoop to Spark isn't quite as hideous as it could be. On top of all this, Spark is the Apache Foundations top project. And that is friggin awesome on its own.</p><h3 id=\"cassandra\"> Cassandra</h3><p>Cassandra is a distributed database based on <a href=\"http://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf\" target=\"_blank\" rel=\"nofollow noopener\">Google BigTable</a> and <a href=\"http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf\" target=\"_blank\" rel=\"nofollow noopener\">Amazon's Dynamo</a>. Like other Big Data databases, it allows for malleable data structures. One of Cassandra's coolest features is the fact that it scales in a predictable way - every single node on a Cassandra cluster has the same role and works in the same way, there is no central gate keeper type node through which all traffic must pass or anything like that. No single node is special so no single node becomes a bottleneck for overall cluster performance, and there is no single point of failure. And that is pretty wonderful.</p><p>Besides that, Cassandra provides many of the same guarantees as other Hadoop-y databases - it works on a cluster and can span multiple data centers; it can talk to Hadoop and Spark in such a way as to maintain data-locality mechanisms (although the importance of this aspect is questionable); the format of the data it stores is malleable to an extent; it provides robust storage... all the good stuff.</p><h3 id=\"scala\"> Scala</h3><p><a href=\"http://www.scala-lang.org/\" target=\"_blank\" rel=\"nofollow noopener\">Scala</a> is a scripting language that runs on the JVM. Spark talks to many different languages, but the Spark-Cassandra connector we are going to use likes Scala best. Scala is cool for a bunch of reasons, the fact that it runs on the JVM means that Scala components can be incorporated into Java software, and Java components into Scala. Besides that it is a lot faster to write than Java, removing the need for a lot of Java's annoying boiler plate requirements.</p><h2 id=\"installation\"> Installation</h2><p>Now we've covered the basics, time to install this stuff. You'll need sudo access. The steps that follow work fine on a fresh Ubuntu 14.04 server.</p><p>Let's get cracking.</p><h3 id=\"install-prerequisites\"> Install Prerequisites</h3><p>First up we'll need Java installed. Oracle Java 7 is the most stable version to use in this setup at the time of writing. This expects a fresh system without any nasty unwanted Java bits and pieces installed. Open up a terminal and type this stuff in:</p><pre class=\"language-bash\">sudo apt-get install software-properties-common\nsudo apt-add-repository ppa:webupd8team/java\n</pre><p>You may be asked to press [ENTER] at this point.</p><pre class=\"language-bash\">sudo apt-get update\nsudo apt-get install oracle-java7-installer\n</pre><p>Agree to the license if you dare. Now just check that it was installed correctly:</p><pre class=\"language-bash\">java -version\n</pre><p>This should execute without error and output some text that looks somewhat like:</p><pre>java version \"1.7.0_80\"\nJava(TM) SE Runtime Environment (build 1.7.0_80-b15)\nJava HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)\n</pre><h3 id=\"installing-cassandra\"> Installing Cassandra</h3><p>There are other ways to do this, but this is the simplest.</p><p>First we add the DataStax community repository, and tell your system that it is trusted:</p><pre class=\"language-bash\">echo \"deb http://debian.datastax.com/community stable main\" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list\ncurl -L http://debian.datastax.com/debian/repo_key | sudo apt-key add -\n</pre><p>Next up we do our install:</p><pre class=\"language-bash\">sudo apt-get update\nsudo apt-get install dsc21=2.1.9-1 cassandra=2.1.9 cassandra-tools=2.1.9\n</pre><p>Once the installation is complete Cassandra will be running and it will be associated with some default data. If you want to change any major configuration of your cluster then it would be best to do so before continuing. Configuring a Cassandra cluster is beyond the scope of this text.</p><p>Stopping and starting a node's Cassandra service can be achieved like so:</p><pre class=\"language-bash\">sudo service cassandra stop\nsudo service cassandra start\n</pre><p>And to see if your Cassandra cluster is up and running use the following command:</p><pre class=\"language-bash\">nodetool status\n</pre><p>This should output something like:</p><pre>Datacenter: datacenter1\n=======================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address    Load       Tokens  Owns (effective)  Host ID                               Rack\nUN  127.0.0.1  104.55 KB  256     100.0%            a6f88d74-d436-4669-9065-6854543598b3  rack1\n</pre><h3 id=\"cql\"> CQL</h3><p>CQL is Cassandra's version of SQL. Once you have successfully installed Cassandra type <code>cqlsh</code> to enter the cql shell. This is not a full of CQL tutorial. You can run this stuff to verify that everything you've done so far works.</p><p>Open up a shell and try this out:</p><pre class=\"language-sql\">// Create a keyspace\nCREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1 };\n// create a table in that keyspace \nUSE \"test\";\nCREATE TABLE my_table(key text PRIMARY KEY, value int);\n// store some goodies\nINSERT INTO my_table(key, value) VALUES ('key1', 1);\nINSERT INTO my_table(key, value) VALUES ('key2', 2);\n// and retrieve them again\nSELECT * from my_table;\n</pre><p>There shouldn't be any errors.</p><h3 id=\"installing-spark\"> Installing Spark</h3><p>This one is nice and straight forward...</p><p>Download Spark and decompress it:</p><pre class=\"language-bash\">wget http://apache.is.co.za//spark/spark-1.4.1/spark-1.4.1-bin-hadoop2.6.tgz\ntar xvzf spark-1.4.1-bin-hadoop2.6.tgz\n</pre><p>You can move that to wherever you like. Now let's test it out. Open up a spark shell by doing a cd into your spark directory and then:</p><pre class=\"language-bash\">bin/spark-shell\n</pre><p>This will take a few seconds and there will be a lot of log output. You'll eventually get presented with a Scala prompt. Now let's get spark to do a calculation for us:</p><pre class=\"language-scala\">sc.parallelize( 1 to 50 ).sum()\n</pre><p>This will eventually output the result 1275.</p><h3 id=\"the-spark-cassandra-connector\"> The Spark Cassandra Connector</h3><p>Spark doesn't natively know how to talk Cassandra, but it's functionality can be extended through use of connectors. Lucky for us the nice people at DataStax have produced one and it is available for download from <a href=\"https://github.com\" target=\"_blank\" rel=\"nofollow noopener\">GitHub</a>.</p><p>You could install git and do a clone. Like so:</p><pre class=\"language-bash\">sudo apt-get install git\ngit clone https://github.com/datastax/spark-cassandra-connector.git\n</pre><p>If you have no idea what that sentence means then you can follow <a href=\"https://github.com/datastax/spark-cassandra-connector\" target=\"_blank\" rel=\"nofollow noopener\">this link</a> and download the latest zip, and unzip it. You can also spend some time learning about Git.</p><p>Once it's cloned it then we'll need to build it:</p><pre class=\"language-bash\">cd spark-cassandra-connector\ngit checkout v1.4.0\n./sbt/sbt assembly\n</pre><p>Note that we are checking out v1.4.0. At time of writing the latest version has some build issues. This one works.</p><p>Go get yourself a cup of tea. You've deserved it. Maybe two cups, this is going to take a while.</p><p>When the build is finally finished there will be two jar files in a directory named “target”, one for Scala and one for Java. We are interested in the Scala one. It's good to have the jar accessible via a path that is easy to remember. For now let's copy it to your home directory. You can put it wherever you want really.</p><pre class=\"language-bash\">cp spark-cassandra-connector/target/scala-2.10/spark-cassandra-connector-assembly-1.4.0-SNAPSHOT.jar ~\n</pre><h2 id=\"using-the-connector\"> Using The Connector</h2><p>Now we have all the bits and pieces sorted out. Start the spark shell again (from within your spark directory), but this time load up the jar:</p><pre class=\"language-bash\">bin/spark-shell --jars ~/spark-cassandra-connector-assembly-1.4.0-SNAPSHOT.jar\n</pre><p>Now enter the following at the scala prompt:</p><pre class=\"language-scala\">sc.stop\nimport com.datastax.spark.connector._, org.apache.spark.SparkContext, org.apache.spark.SparkContext._, org.apache.spark.SparkConf\nval conf = new SparkConf(true).set(\"spark.cassandra.connection.host\", \"localhost\")\nval sc = new SparkContext(conf)\n</pre><p>This takes the spark context and replaces it with one that is connected to your local cassandra.</p><p>Now let's take it for a spin. Remember we made a keyspace called <code>test</code> and a table called <code>my_table</code>? We'll be making use of those again now. Enter the following in the scala shell:</p><pre class=\"language-scala\">val test_spark_rdd = sc.cassandraTable(\"test\", \"my_table\")\ntest_spark_rdd.first\n</pre><p>Isn't that lovely.</p><h2 id=\"conclusion\"> Conclusion</h2><p>We've taken a fresh Ubuntu installation and set up Cassandra and Spark and gotten then to talk. That's quite a feat on it's own. But to make them talk in a way that is actually useful has many implications. Firstly, we've got everything set up on a single computer. It works but the real strength of these technologies comes from the fact that they are aimed at solving problems within the sphere of big data. They should be installed on a cluster, possibly a multi-data-centre cluster.</p><p>Besides installation, there is a lot to be said about how Cassandra actually works - it is a very configurable database and can be optimised for all sorts of workloads. Then there is the topic of schema design and optimisation. The little bit of CQL we covered in this tutorial barely scratches the surface of Cassandra's capabilities. If you want to use it in any serious way it'll be best to spend some time digging into how it works.</p><p>Spark also deserves more attention than this tutorial could give it. for example, did you know that you can use a Python-based spark shell (called PySpark)? Unfortunately at the time of writing Python support for the Cassandra connector was called \"experimental\". Meh. If you want to use Spark in any useful way it would at least be useful to learn about the spark context, and what can be done with an RDD.</p><p>That's all folks.</p>"}}]}},"pageContext":{"alternative_id":13035}}