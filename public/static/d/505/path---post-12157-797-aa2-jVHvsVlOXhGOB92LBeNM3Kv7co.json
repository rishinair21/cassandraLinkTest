{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Apache Cassandra Documentation : Cassandra Config","alternative_id":12157,"content":"<div class=\"section\" id=\"cluster-name\"><h2><code class=\"docutils literal\">cluster_name</code></h2><p>The name of the cluster. This is mainly used to prevent machines in\none logical cluster from joining another.</p><p><em>Default Value:</em> ‘Test Cluster’</p></div><div class=\"section\" id=\"num-tokens\"><h2><code class=\"docutils literal\">num_tokens</code></h2><p>This defines the number of tokens randomly assigned to this node on the ring\nThe more tokens, relative to other nodes, the larger the proportion of data\nthat this node will store. You probably want all nodes to have the same number\nof tokens assuming they have equal hardware capability.</p><p>If you leave this unspecified, Cassandra will use the default of 1 token for legacy compatibility,\nand will use the initial_token as described below.</p><p>Specifying initial_token will override this setting on the node’s initial start,\non subsequent starts, this setting will apply even if initial token is set.</p><p>If you already have a cluster with 1 token per node, and wish to migrate to\nmultiple tokens per node, see <a class=\"reference external\" href=\"http://wiki.apache.org/cassandra/Operations\">http://wiki.apache.org/cassandra/Operations</a></p><p><em>Default Value:</em> 256</p></div><div class=\"section\" id=\"allocate-tokens-for-keyspace\"><h2><code class=\"docutils literal\">allocate_tokens_for_keyspace</code></h2><p><em>This option is commented out by default.</em></p><p>Triggers automatic allocation of num_tokens tokens for this node. The allocation\nalgorithm attempts to choose tokens in a way that optimizes replicated load over\nthe nodes in the datacenter for the replication strategy used by the specified\nkeyspace.</p><p>The load assigned to each node will be close to proportional to its number of\nvnodes.</p><p>Only supported with the Murmur3Partitioner.</p><p><em>Default Value:</em> KEYSPACE</p></div><div class=\"section\" id=\"initial-token\"><h2><code class=\"docutils literal\">initial_token</code></h2><p><em>This option is commented out by default.</em></p><p>initial_token allows you to specify tokens manually.  While you can use it with\nvnodes (num_tokens &gt; 1, above) – in which case you should provide a\ncomma-separated list – it’s primarily used when adding nodes to legacy clusters\nthat do not have vnodes enabled.</p></div><div class=\"section\" id=\"hinted-handoff-enabled\"><h2><code class=\"docutils literal\">hinted_handoff_enabled</code></h2><p>See <a class=\"reference external\" href=\"http://wiki.apache.org/cassandra/HintedHandoff\">http://wiki.apache.org/cassandra/HintedHandoff</a>\nMay either be “true” or “false” to enable globally</p><p><em>Default Value:</em> true</p></div><div class=\"section\" id=\"hinted-handoff-disabled-datacenters\"><h2><code class=\"docutils literal\">hinted_handoff_disabled_datacenters</code></h2><p><em>This option is commented out by default.</em></p><p>When hinted_handoff_enabled is true, a black list of data centers that will not\nperform hinted handoff</p><p><em>Default Value (complex option)</em>:</p><div class=\"highlight-default\"><div class=\"highlight\"><pre>#    - DC1\n#    - DC2\n</pre></div></div></div><div class=\"section\" id=\"max-hint-window-in-ms\"><h2><code class=\"docutils literal\">max_hint_window_in_ms</code></h2><p>this defines the maximum amount of time a dead host will have hints\ngenerated.  After it has been dead this long, new hints for it will not be\ncreated until it has been seen alive and gone down again.</p><p><em>Default Value:</em> 10800000 # 3 hours</p></div><div class=\"section\" id=\"hinted-handoff-throttle-in-kb\"><h2><code class=\"docutils literal\">hinted_handoff_throttle_in_kb</code></h2><p>Maximum throttle in KBs per second, per delivery thread.  This will be\nreduced proportionally to the number of nodes in the cluster.  (If there\nare two nodes in the cluster, each delivery thread will use the maximum\nrate; if there are three, each will throttle to half of the maximum,\nsince we expect two nodes to be delivering hints simultaneously.)</p><p><em>Default Value:</em> 1024</p></div><div class=\"section\" id=\"max-hints-delivery-threads\"><h2><code class=\"docutils literal\">max_hints_delivery_threads</code></h2><p>Number of threads with which to deliver hints;\nConsider increasing this number when you have multi-dc deployments, since\ncross-dc handoff tends to be slower</p><p><em>Default Value:</em> 2</p></div><div class=\"section\" id=\"hints-directory\"><h2><code class=\"docutils literal\">hints_directory</code></h2><p><em>This option is commented out by default.</em></p><p>Directory where Cassandra should store hints.\nIf not set, the default directory is $CASSANDRA_HOME/data/hints.</p><p><em>Default Value:</em>  /var/lib/cassandra/hints</p></div><div class=\"section\" id=\"hints-flush-period-in-ms\"><h2><code class=\"docutils literal\">hints_flush_period_in_ms</code></h2><p>How often hints should be flushed from the internal buffers to disk.\nWill <em>not</em> trigger fsync.</p><p><em>Default Value:</em> 10000</p></div><div class=\"section\" id=\"max-hints-file-size-in-mb\"><h2><code class=\"docutils literal\">max_hints_file_size_in_mb</code></h2><p>Maximum size for a single hints file, in megabytes.</p><p><em>Default Value:</em> 128</p></div><div class=\"section\" id=\"hints-compression\"><h2><code class=\"docutils literal\">hints_compression</code></h2><p><em>This option is commented out by default.</em></p><p>Compression to apply to the hint files. If omitted, hints files\nwill be written uncompressed. LZ4, Snappy, and Deflate compressors\nare supported.</p><p><em>Default Value (complex option)</em>:</p><div class=\"highlight-default\"><div class=\"highlight\"><pre>#   - class_name: LZ4Compressor\n#     parameters:\n#         -\n</pre></div></div></div><div class=\"section\" id=\"batchlog-replay-throttle-in-kb\"><h2><code class=\"docutils literal\">batchlog_replay_throttle_in_kb</code></h2><p>Maximum throttle in KBs per second, total. This will be\nreduced proportionally to the number of nodes in the cluster.</p><p><em>Default Value:</em> 1024</p></div><div class=\"section\" id=\"authenticator\"><h2><code class=\"docutils literal\">authenticator</code></h2><p>Authentication backend, implementing IAuthenticator; used to identify users\nOut of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthenticator,\nPasswordAuthenticator}.</p><ul class=\"simple\"><li>AllowAllAuthenticator performs no checks - set it to disable authentication.</li>\n<li>PasswordAuthenticator relies on username/password pairs to authenticate\nusers. It keeps usernames and hashed passwords in system_auth.roles table.\nPlease increase system_auth keyspace replication factor if you use this authenticator.\nIf using PasswordAuthenticator, CassandraRoleManager must also be used (see below)</li>\n</ul><p><em>Default Value:</em> AllowAllAuthenticator</p></div><div class=\"section\" id=\"role-manager\"><h2><code class=\"docutils literal\">role_manager</code></h2><p>Part of the Authentication &amp; Authorization backend, implementing IRoleManager; used\nto maintain grants and memberships between roles.\nOut of the box, Cassandra provides org.apache.cassandra.auth.CassandraRoleManager,\nwhich stores role information in the system_auth keyspace. Most functions of the\nIRoleManager require an authenticated login, so unless the configured IAuthenticator\nactually implements authentication, most of this functionality will be unavailable.</p><ul class=\"simple\"><li>CassandraRoleManager stores role data in the system_auth keyspace. Please\nincrease system_auth keyspace replication factor if you use this role manager.</li>\n</ul><p><em>Default Value:</em> CassandraRoleManager</p></div><div class=\"section\" id=\"roles-validity-in-ms\"><h2><code class=\"docutils literal\">roles_validity_in_ms</code></h2><p>Validity period for roles cache (fetching granted roles can be an expensive\noperation depending on the role manager, CassandraRoleManager is one example)\nGranted roles are cached for authenticated sessions in AuthenticatedUser and\nafter the period specified here, become eligible for (async) reload.\nDefaults to 2000, set to 0 to disable caching entirely.\nWill be disabled automatically for AllowAllAuthenticator.</p><p><em>Default Value:</em> 2000</p></div><div class=\"section\" id=\"permissions-validity-in-ms\"><h2><code class=\"docutils literal\">permissions_validity_in_ms</code></h2><p>Validity period for permissions cache (fetching permissions can be an\nexpensive operation depending on the authorizer, CassandraAuthorizer is\none example). Defaults to 2000, set to 0 to disable.\nWill be disabled automatically for AllowAllAuthorizer.</p><p><em>Default Value:</em> 2000</p></div><div class=\"section\" id=\"credentials-validity-in-ms\"><h2><code class=\"docutils literal\">credentials_validity_in_ms</code></h2><p>Validity period for credentials cache. This cache is tightly coupled to\nthe provided PasswordAuthenticator implementation of IAuthenticator. If\nanother IAuthenticator implementation is configured, this cache will not\nbe automatically used and so the following settings will have no effect.\nPlease note, credentials are cached in their encrypted form, so while\nactivating this cache may reduce the number of queries made to the\nunderlying table, it may not  bring a significant reduction in the\nlatency of individual authentication attempts.\nDefaults to 2000, set to 0 to disable credentials caching.</p><p><em>Default Value:</em> 2000</p></div><div class=\"section\" id=\"partitioner\"><h2><code class=\"docutils literal\">partitioner</code></h2><p>The partitioner is responsible for distributing groups of rows (by\npartition key) across nodes in the cluster.  You should leave this\nalone for new clusters.  The partitioner can NOT be changed without\nreloading all data, so when upgrading you should set this to the\nsame partitioner you were already using.</p><p>Besides Murmur3Partitioner, partitioners included for backwards\ncompatibility include RandomPartitioner, ByteOrderedPartitioner, and\nOrderPreservingPartitioner.</p><p><em>Default Value:</em> org.apache.cassandra.dht.Murmur3Partitioner</p></div><div class=\"section\" id=\"data-file-directories\"><h2><code class=\"docutils literal\">data_file_directories</code></h2><p><em>This option is commented out by default.</em></p><p>Directories where Cassandra should store data on disk.  Cassandra\nwill spread data evenly across them, subject to the granularity of\nthe configured compaction strategy.\nIf not set, the default directory is $CASSANDRA_HOME/data/data.</p><p><em>Default Value (complex option)</em>:</p><div class=\"highlight-default\"><div class=\"highlight\"><pre>#     - /var/lib/cassandra/data\n</pre></div></div></div><div class=\"section\" id=\"commitlog-directory\"><h2><code class=\"docutils literal\">commitlog_directory</code></h2><p><em>This option is commented out by default.</em>\ncommit log.  when running on magnetic HDD, this should be a\nseparate spindle than the data directories.\nIf not set, the default directory is $CASSANDRA_HOME/data/commitlog.</p><p><em>Default Value:</em>  /var/lib/cassandra/commitlog</p></div><div class=\"section\" id=\"cdc-enabled\"><h2><code class=\"docutils literal\">cdc_enabled</code></h2><p>Enable / disable CDC functionality on a per-node basis. This modifies the logic used\nfor write path allocation rejection (standard: never reject. cdc: reject Mutation\ncontaining a CDC-enabled table if at space limit in cdc_raw_directory).</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"cdc-raw-directory\"><h2><code class=\"docutils literal\">cdc_raw_directory</code></h2><p><em>This option is commented out by default.</em></p><p>CommitLogSegments are moved to this directory on flush if cdc_enabled: true and the\nsegment contains mutations for a CDC-enabled table. This should be placed on a\nseparate spindle than the data directories. If not set, the default directory is\n$CASSANDRA_HOME/data/cdc_raw.</p><p><em>Default Value:</em>  /var/lib/cassandra/cdc_raw</p></div><div class=\"section\" id=\"disk-failure-policy\"><h2><code class=\"docutils literal\">disk_failure_policy</code></h2><p>Policy for data disk failures:</p><dl class=\"docutils\"><dt>die</dt>\n<dd>shut down gossip and client transports and kill the JVM for any fs errors or\nsingle-sstable errors, so the node can be replaced.</dd>\n<dt>stop_paranoid</dt>\n<dd>shut down gossip and client transports even for single-sstable errors,\nkill the JVM for errors during startup.</dd>\n<dt>stop</dt>\n<dd>shut down gossip and client transports, leaving the node effectively dead, but\ncan still be inspected via JMX, kill the JVM for errors during startup.</dd>\n<dt>best_effort</dt>\n<dd>stop using the failed disk and respond to requests based on\nremaining available sstables.  This means you WILL see obsolete\ndata at CL.ONE!</dd>\n<dt>ignore</dt>\n<dd>ignore fatal errors and let requests fail, as in pre-1.2 Cassandra</dd>\n</dl><p><em>Default Value:</em> stop</p></div><div class=\"section\" id=\"commit-failure-policy\"><h2><code class=\"docutils literal\">commit_failure_policy</code></h2><p>Policy for commit disk failures:</p><dl class=\"docutils\"><dt>die</dt>\n<dd>shut down the node and kill the JVM, so the node can be replaced.</dd>\n<dt>stop</dt>\n<dd>shut down the node, leaving the node effectively dead, but\ncan still be inspected via JMX.</dd>\n<dt>stop_commit</dt>\n<dd>shutdown the commit log, letting writes collect but\ncontinuing to service reads, as in pre-2.0.5 Cassandra</dd>\n<dt>ignore</dt>\n<dd>ignore fatal errors and let the batches fail</dd>\n</dl><p><em>Default Value:</em> stop</p></div><div class=\"section\" id=\"key-cache-size-in-mb\"><h2><code class=\"docutils literal\">key_cache_size_in_mb</code></h2><p>Maximum size of the key cache in memory.</p><p>Each key cache hit saves 1 seek and each row cache hit saves 2 seeks at the\nminimum, sometimes more. The key cache is fairly tiny for the amount of\ntime it saves, so it’s worthwhile to use it at large numbers.\nThe row cache saves even more time, but must contain the entire row,\nso it is extremely space-intensive. It’s best to only use the\nrow cache if you have hot rows or static rows.</p><p>NOTE: if you reduce the size, you may not get you hottest keys loaded on startup.</p><p>Default value is empty to make it “auto” (min(5% of Heap (in MB), 100MB)). Set to 0 to disable key cache.</p></div><div class=\"section\" id=\"key-cache-save-period\"><h2><code class=\"docutils literal\">key_cache_save_period</code></h2><p>Duration in seconds after which Cassandra should\nsave the key cache. Caches are saved to saved_caches_directory as\nspecified in this configuration file.</p><p>Saved caches greatly improve cold-start speeds, and is relatively cheap in\nterms of I/O for the key cache. Row cache saving is much more expensive and\nhas limited use.</p><p>Default is 14400 or 4 hours.</p><p><em>Default Value:</em> 14400</p></div><div class=\"section\" id=\"key-cache-keys-to-save\"><h2><code class=\"docutils literal\">key_cache_keys_to_save</code></h2><p><em>This option is commented out by default.</em></p><p>Number of keys from the key cache to save\nDisabled by default, meaning all keys are going to be saved</p><p><em>Default Value:</em> 100</p></div><div class=\"section\" id=\"row-cache-class-name\"><h2><code class=\"docutils literal\">row_cache_class_name</code></h2><p><em>This option is commented out by default.</em></p><p>Row cache implementation class name. Available implementations:</p><dl class=\"docutils\"><dt>org.apache.cassandra.cache.OHCProvider</dt>\n<dd>Fully off-heap row cache implementation (default).</dd>\n<dt>org.apache.cassandra.cache.SerializingCacheProvider</dt>\n<dd>This is the row cache implementation availabile\nin previous releases of Cassandra.</dd>\n</dl><p><em>Default Value:</em> org.apache.cassandra.cache.OHCProvider</p></div><div class=\"section\" id=\"row-cache-size-in-mb\"><h2><code class=\"docutils literal\">row_cache_size_in_mb</code></h2><p>Maximum size of the row cache in memory.\nPlease note that OHC cache implementation requires some additional off-heap memory to manage\nthe map structures and some in-flight memory during operations before/after cache entries can be\naccounted against the cache capacity. This overhead is usually small compared to the whole capacity.\nDo not specify more memory that the system can afford in the worst usual situation and leave some\nheadroom for OS block level cache. Do never allow your system to swap.</p><p>Default value is 0, to disable row caching.</p><p><em>Default Value:</em> 0</p></div><div class=\"section\" id=\"row-cache-save-period\"><h2><code class=\"docutils literal\">row_cache_save_period</code></h2><p>Duration in seconds after which Cassandra should save the row cache.\nCaches are saved to saved_caches_directory as specified in this configuration file.</p><p>Saved caches greatly improve cold-start speeds, and is relatively cheap in\nterms of I/O for the key cache. Row cache saving is much more expensive and\nhas limited use.</p><p>Default is 0 to disable saving the row cache.</p><p><em>Default Value:</em> 0</p></div><div class=\"section\" id=\"row-cache-keys-to-save\"><h2><code class=\"docutils literal\">row_cache_keys_to_save</code></h2><p><em>This option is commented out by default.</em></p><p>Number of keys from the row cache to save.\nSpecify 0 (which is the default), meaning all keys are going to be saved</p><p><em>Default Value:</em> 100</p></div><div class=\"section\" id=\"counter-cache-size-in-mb\"><h2><code class=\"docutils literal\">counter_cache_size_in_mb</code></h2><p>Maximum size of the counter cache in memory.</p><p>Counter cache helps to reduce counter locks’ contention for hot counter cells.\nIn case of RF = 1 a counter cache hit will cause Cassandra to skip the read before\nwrite entirely. With RF &gt; 1 a counter cache hit will still help to reduce the duration\nof the lock hold, helping with hot counter cell updates, but will not allow skipping\nthe read entirely. Only the local (clock, count) tuple of a counter cell is kept\nin memory, not the whole counter, so it’s relatively cheap.</p><p>NOTE: if you reduce the size, you may not get you hottest keys loaded on startup.</p><p>Default value is empty to make it “auto” (min(2.5% of Heap (in MB), 50MB)). Set to 0 to disable counter cache.\nNOTE: if you perform counter deletes and rely on low gcgs, you should disable the counter cache.</p></div><div class=\"section\" id=\"counter-cache-save-period\"><h2><code class=\"docutils literal\">counter_cache_save_period</code></h2><p>Duration in seconds after which Cassandra should\nsave the counter cache (keys only). Caches are saved to saved_caches_directory as\nspecified in this configuration file.</p><p>Default is 7200 or 2 hours.</p><p><em>Default Value:</em> 7200</p></div><div class=\"section\" id=\"counter-cache-keys-to-save\"><h2><code class=\"docutils literal\">counter_cache_keys_to_save</code></h2><p><em>This option is commented out by default.</em></p><p>Number of keys from the counter cache to save\nDisabled by default, meaning all keys are going to be saved</p><p><em>Default Value:</em> 100</p></div><div class=\"section\" id=\"saved-caches-directory\"><h2><code class=\"docutils literal\">saved_caches_directory</code></h2><p><em>This option is commented out by default.</em></p><p>saved caches\nIf not set, the default directory is $CASSANDRA_HOME/data/saved_caches.</p><p><em>Default Value:</em>  /var/lib/cassandra/saved_caches</p></div><div class=\"section\" id=\"commitlog-sync\"><h2><code class=\"docutils literal\">commitlog_sync</code></h2><p><em>This option is commented out by default.</em></p><p>commitlog_sync may be either “periodic” or “batch.”</p><p>When in batch mode, Cassandra won’t ack writes until the commit log\nhas been fsynced to disk.  It will wait\ncommitlog_sync_batch_window_in_ms milliseconds between fsyncs.\nThis window should be kept short because the writer threads will\nbe unable to do extra work while waiting.  (You may need to increase\nconcurrent_writes for the same reason.)</p><p><em>Default Value:</em> batch</p></div><div class=\"section\" id=\"commitlog-sync-batch-window-in-ms\"><h2><code class=\"docutils literal\">commitlog_sync_batch_window_in_ms</code></h2><p><em>This option is commented out by default.</em></p><p><em>Default Value:</em> 2</p></div><div class=\"section\" id=\"id1\"><h2><code class=\"docutils literal\">commitlog_sync</code></h2><p>the other option is “periodic” where writes may be acked immediately\nand the CommitLog is simply synced every commitlog_sync_period_in_ms\nmilliseconds.</p><p><em>Default Value:</em> periodic</p></div><div class=\"section\" id=\"commitlog-sync-period-in-ms\"><h2><code class=\"docutils literal\">commitlog_sync_period_in_ms</code></h2><p><em>Default Value:</em> 10000</p></div><div class=\"section\" id=\"commitlog-segment-size-in-mb\"><h2><code class=\"docutils literal\">commitlog_segment_size_in_mb</code></h2><p>The size of the individual commitlog file segments.  A commitlog\nsegment may be archived, deleted, or recycled once all the data\nin it (potentially from each columnfamily in the system) has been\nflushed to sstables.</p><p>The default size is 32, which is almost always fine, but if you are\narchiving commitlog segments (see commitlog_archiving.properties),\nthen you probably want a finer granularity of archiving; 8 or 16 MB\nis reasonable.\nMax mutation size is also configurable via max_mutation_size_in_kb setting in\ncassandra.yaml. The default is half the size commitlog_segment_size_in_mb * 1024.</p><p>NOTE: If max_mutation_size_in_kb is set explicitly then commitlog_segment_size_in_mb must\nbe set to at least twice the size of max_mutation_size_in_kb / 1024</p><p><em>Default Value:</em> 32</p></div><div class=\"section\" id=\"commitlog-compression\"><h2><code class=\"docutils literal\">commitlog_compression</code></h2><p><em>This option is commented out by default.</em></p><p>Compression to apply to the commit log. If omitted, the commit log\nwill be written uncompressed.  LZ4, Snappy, and Deflate compressors\nare supported.</p><p><em>Default Value (complex option)</em>:</p><div class=\"highlight-default\"><div class=\"highlight\"><pre>#   - class_name: LZ4Compressor\n#     parameters:\n#         -\n</pre></div></div></div><div class=\"section\" id=\"seed-provider\"><h2><code class=\"docutils literal\">seed_provider</code></h2><p>any class that implements the SeedProvider interface and has a\nconstructor that takes a Map&lt;String, String&gt; of parameters will do.</p><p><em>Default Value (complex option)</em>:</p><div class=\"highlight-default\"><div class=\"highlight\"><pre># Addresses of hosts that are deemed contact points.\n# Cassandra nodes use this list of hosts to find each other and learn\n# the topology of the ring.  You must change this if you are running\n# multiple nodes!\n- class_name: org.apache.cassandra.locator.SimpleSeedProvider\n  parameters:\n      # seeds is actually a comma-delimited list of addresses.\n      # Ex: \"&lt;ip1&gt;,&lt;ip2&gt;,&lt;ip3&gt;\"\n      - seeds: \"127.0.0.1\"\n</pre></div></div></div><div class=\"section\" id=\"concurrent-reads\"><h2><code class=\"docutils literal\">concurrent_reads</code></h2><p>For workloads with more data than can fit in memory, Cassandra’s\nbottleneck will be reads that need to fetch data from\ndisk. “concurrent_reads” should be set to (16 * number_of_drives) in\norder to allow the operations to enqueue low enough in the stack\nthat the OS and drives can reorder them. Same applies to\n“concurrent_counter_writes”, since counter writes read the current\nvalues before incrementing and writing them back.</p><p>On the other hand, since writes are almost never IO bound, the ideal\nnumber of “concurrent_writes” is dependent on the number of cores in\nyour system; (8 * number_of_cores) is a good rule of thumb.</p><p><em>Default Value:</em> 32</p></div><div class=\"section\" id=\"concurrent-writes\"><h2><code class=\"docutils literal\">concurrent_writes</code></h2><p><em>Default Value:</em> 32</p></div><div class=\"section\" id=\"concurrent-counter-writes\"><h2><code class=\"docutils literal\">concurrent_counter_writes</code></h2><p><em>Default Value:</em> 32</p></div><div class=\"section\" id=\"concurrent-materialized-view-writes\"><h2><code class=\"docutils literal\">concurrent_materialized_view_writes</code></h2><p>For materialized view writes, as there is a read involved, so this should\nbe limited by the less of concurrent reads or concurrent writes.</p><p><em>Default Value:</em> 32</p></div><div class=\"section\" id=\"file-cache-size-in-mb\"><h2><code class=\"docutils literal\">file_cache_size_in_mb</code></h2><p><em>This option is commented out by default.</em></p><p>Maximum memory to use for sstable chunk cache and buffer pooling.\n32MB of this are reserved for pooling buffers, the rest is used as an\ncache that holds uncompressed sstable chunks.\nDefaults to the smaller of 1/4 of heap or 512MB. This pool is allocated off-heap,\nso is in addition to the memory allocated for heap. The cache also has on-heap\noverhead which is roughly 128 bytes per chunk (i.e. 0.2% of the reserved size\nif the default 64k chunk size is used).\nMemory is only allocated when needed.</p><p><em>Default Value:</em> 512</p></div><div class=\"section\" id=\"buffer-pool-use-heap-if-exhausted\"><h2><code class=\"docutils literal\">buffer_pool_use_heap_if_exhausted</code></h2><p><em>This option is commented out by default.</em></p><p>Flag indicating whether to allocate on or off heap when the sstable buffer\npool is exhausted, that is when it has exceeded the maximum memory\nfile_cache_size_in_mb, beyond which it will not cache buffers but allocate on request.</p><p><em>Default Value:</em> true</p></div><div class=\"section\" id=\"disk-optimization-strategy\"><h2><code class=\"docutils literal\">disk_optimization_strategy</code></h2><p><em>This option is commented out by default.</em></p><p>The strategy for optimizing disk read\nPossible values are:\nssd (for solid state disks, the default)\nspinning (for spinning disks)</p><p><em>Default Value:</em> ssd</p></div><div class=\"section\" id=\"memtable-heap-space-in-mb\"><h2><code class=\"docutils literal\">memtable_heap_space_in_mb</code></h2><p><em>This option is commented out by default.</em></p><p>Total permitted memory to use for memtables. Cassandra will stop\naccepting writes when the limit is exceeded until a flush completes,\nand will trigger a flush based on memtable_cleanup_threshold\nIf omitted, Cassandra will set both to 1/4 the size of the heap.</p><p><em>Default Value:</em> 2048</p></div><div class=\"section\" id=\"memtable-offheap-space-in-mb\"><h2><code class=\"docutils literal\">memtable_offheap_space_in_mb</code></h2><p><em>This option is commented out by default.</em></p><p><em>Default Value:</em> 2048</p></div><div class=\"section\" id=\"memtable-cleanup-threshold\"><h2><code class=\"docutils literal\">memtable_cleanup_threshold</code></h2><p><em>This option is commented out by default.</em></p><p>memtable_cleanup_threshold is deprecated. The default calculation\nis the only reasonable choice. See the comments on  memtable_flush_writers\nfor more information.</p><p>Ratio of occupied non-flushing memtable size to total permitted size\nthat will trigger a flush of the largest memtable. Larger mct will\nmean larger flushes and hence less compaction, but also less concurrent\nflush activity which can make it difficult to keep your disks fed\nunder heavy write load.</p><p>memtable_cleanup_threshold defaults to 1 / (memtable_flush_writers + 1)</p><p><em>Default Value:</em> 0.11</p></div><div class=\"section\" id=\"memtable-allocation-type\"><h2><code class=\"docutils literal\">memtable_allocation_type</code></h2><p>Specify the way Cassandra allocates and manages memtable memory.\nOptions are:</p><dl class=\"docutils\"><dt>heap_buffers</dt>\n<dd>on heap nio buffers</dd>\n<dt>offheap_buffers</dt>\n<dd>off heap (direct) nio buffers</dd>\n<dt>offheap_objects</dt>\n<dd>off heap objects</dd>\n</dl><p><em>Default Value:</em> heap_buffers</p></div><div class=\"section\" id=\"commitlog-total-space-in-mb\"><h2><code class=\"docutils literal\">commitlog_total_space_in_mb</code></h2><p><em>This option is commented out by default.</em></p><p>Total space to use for commit logs on disk.</p><p>If space gets above this value, Cassandra will flush every dirty CF\nin the oldest segment and remove it.  So a small total commitlog space\nwill tend to cause more flush activity on less-active columnfamilies.</p><p>The default value is the smaller of 8192, and 1/4 of the total space\nof the commitlog volume.</p><p><em>Default Value:</em> 8192</p></div><div class=\"section\" id=\"memtable-flush-writers\"><h2><code class=\"docutils literal\">memtable_flush_writers</code></h2><p><em>This option is commented out by default.</em></p><p>This sets the number of memtable flush writer threads per disk\nas well as the total number of memtables that can be flushed concurrently.\nThese are generally a combination of compute and IO bound.</p><p>Memtable flushing is more CPU efficient than memtable ingest and a single thread\ncan keep up with the ingest rate of a whole server on a single fast disk\nuntil it temporarily becomes IO bound under contention typically with compaction.\nAt that point you need multiple flush threads. At some point in the future\nit may become CPU bound all the time.</p><p>You can tell if flushing is falling behind using the MemtablePool.BlockedOnAllocation\nmetric which should be 0, but will be non-zero if threads are blocked waiting on flushing\nto free memory.</p><p>memtable_flush_writers defaults to two for a single data directory.\nThis means that two  memtables can be flushed concurrently to the single data directory.\nIf you have multiple data directories the default is one memtable flushing at a time\nbut the flush will use a thread per data directory so you will get two or more writers.</p><p>Two is generally enough to flush on a fast disk [array] mounted as a single data directory.\nAdding more flush writers will result in smaller more frequent flushes that introduce more\ncompaction overhead.</p><p>There is a direct tradeoff between number of memtables that can be flushed concurrently\nand flush size and frequency. More is not better you just need enough flush writers\nto never stall waiting for flushing to free memory.</p><p><em>Default Value:</em> 2</p></div><div class=\"section\" id=\"cdc-total-space-in-mb\"><h2><code class=\"docutils literal\">cdc_total_space_in_mb</code></h2><p><em>This option is commented out by default.</em></p><p>Total space to use for change-data-capture logs on disk.</p><p>If space gets above this value, Cassandra will throw WriteTimeoutException\non Mutations including tables with CDC enabled. A CDCCompactor is responsible\nfor parsing the raw CDC logs and deleting them when parsing is completed.</p><p>The default value is the min of 4096 mb and 1/8th of the total space\nof the drive where cdc_raw_directory resides.</p><p><em>Default Value:</em> 4096</p></div><div class=\"section\" id=\"cdc-free-space-check-interval-ms\"><h2><code class=\"docutils literal\">cdc_free_space_check_interval_ms</code></h2><p><em>This option is commented out by default.</em></p><p>When we hit our cdc_raw limit and the CDCCompactor is either running behind\nor experiencing backpressure, we check at the following interval to see if any\nnew space for cdc-tracked tables has been made available. Default to 250ms</p><p><em>Default Value:</em> 250</p></div><div class=\"section\" id=\"index-summary-capacity-in-mb\"><h2><code class=\"docutils literal\">index_summary_capacity_in_mb</code></h2><p>A fixed memory pool size in MB for for SSTable index summaries. If left\nempty, this will default to 5% of the heap size. If the memory usage of\nall index summaries exceeds this limit, SSTables with low read rates will\nshrink their index summaries in order to meet this limit.  However, this\nis a best-effort process. In extreme conditions Cassandra may need to use\nmore than this amount of memory.</p></div><div class=\"section\" id=\"index-summary-resize-interval-in-minutes\"><h2><code class=\"docutils literal\">index_summary_resize_interval_in_minutes</code></h2><p>How frequently index summaries should be resampled.  This is done\nperiodically to redistribute memory from the fixed-size pool to sstables\nproportional their recent read rates.  Setting to -1 will disable this\nprocess, leaving existing index summaries at their current sampling level.</p><p><em>Default Value:</em> 60</p></div><div class=\"section\" id=\"trickle-fsync\"><h2><code class=\"docutils literal\">trickle_fsync</code></h2><p>Whether to, when doing sequential writing, fsync() at intervals in\norder to force the operating system to flush the dirty\nbuffers. Enable this to avoid sudden dirty buffer flushing from\nimpacting read latencies. Almost always a good idea on SSDs; not\nnecessarily on platters.</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"trickle-fsync-interval-in-kb\"><h2><code class=\"docutils literal\">trickle_fsync_interval_in_kb</code></h2><p><em>Default Value:</em> 10240</p></div><div class=\"section\" id=\"storage-port\"><h2><code class=\"docutils literal\">storage_port</code></h2><p>TCP port, for commands and data\nFor security reasons, you should not expose this port to the internet.  Firewall it if needed.</p><p><em>Default Value:</em> 7000</p></div><div class=\"section\" id=\"ssl-storage-port\"><h2><code class=\"docutils literal\">ssl_storage_port</code></h2><p>SSL port, for encrypted communication.  Unused unless enabled in\nencryption_options\nFor security reasons, you should not expose this port to the internet.  Firewall it if needed.</p><p><em>Default Value:</em> 7001</p></div><div class=\"section\" id=\"listen-address\"><h2><code class=\"docutils literal\">listen_address</code></h2><p>Address or interface to bind to and tell other Cassandra nodes to connect to.\nYou _must_ change this if you want multiple nodes to be able to communicate!</p><p>Set listen_address OR listen_interface, not both.</p><p>Leaving it blank leaves it up to InetAddress.getLocalHost(). This\nwill always do the Right Thing _if_ the node is properly configured\n(hostname, name resolution, etc), and the Right Thing is to use the\naddress associated with the hostname (it might not be).</p><p>Setting listen_address to 0.0.0.0 is always wrong.</p><p><em>Default Value:</em> localhost</p></div><div class=\"section\" id=\"listen-interface\"><h2><code class=\"docutils literal\">listen_interface</code></h2><p><em>This option is commented out by default.</em></p><p>Set listen_address OR listen_interface, not both. Interfaces must correspond\nto a single address, IP aliasing is not supported.</p><p><em>Default Value:</em> eth0</p></div><div class=\"section\" id=\"listen-interface-prefer-ipv6\"><h2><code class=\"docutils literal\">listen_interface_prefer_ipv6</code></h2><p><em>This option is commented out by default.</em></p><p>If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address\nyou can specify which should be chosen using listen_interface_prefer_ipv6. If false the first ipv4\naddress will be used. If true the first ipv6 address will be used. Defaults to false preferring\nipv4. If there is only one address it will be selected regardless of ipv4/ipv6.</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"broadcast-address\"><h2><code class=\"docutils literal\">broadcast_address</code></h2><p><em>This option is commented out by default.</em></p><p>Address to broadcast to other Cassandra nodes\nLeaving this blank will set it to the same value as listen_address</p><p><em>Default Value:</em> 1.2.3.4</p></div><div class=\"section\" id=\"listen-on-broadcast-address\"><h2><code class=\"docutils literal\">listen_on_broadcast_address</code></h2><p><em>This option is commented out by default.</em></p><p>When using multiple physical network interfaces, set this\nto true to listen on broadcast_address in addition to\nthe listen_address, allowing nodes to communicate in both\ninterfaces.\nIgnore this property if the network configuration automatically\nroutes  between the public and private networks such as EC2.</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"internode-authenticator\"><h2><code class=\"docutils literal\">internode_authenticator</code></h2><p><em>This option is commented out by default.</em></p><p>Internode authentication backend, implementing IInternodeAuthenticator;\nused to allow/disallow connections from peer nodes.</p><p><em>Default Value:</em> org.apache.cassandra.auth.AllowAllInternodeAuthenticator</p></div><div class=\"section\" id=\"start-native-transport\"><h2><code class=\"docutils literal\">start_native_transport</code></h2><p>Whether to start the native transport server.\nThe address on which the native transport is bound is defined by rpc_address.</p><p><em>Default Value:</em> true</p></div><div class=\"section\" id=\"native-transport-port\"><h2><code class=\"docutils literal\">native_transport_port</code></h2><p>port for the CQL native transport to listen for clients on\nFor security reasons, you should not expose this port to the internet.  Firewall it if needed.</p><p><em>Default Value:</em> 9042</p></div><div class=\"section\" id=\"native-transport-port-ssl\"><h2><code class=\"docutils literal\">native_transport_port_ssl</code></h2><p><em>This option is commented out by default.</em>\nEnabling native transport encryption in client_encryption_options allows you to either use\nencryption for the standard port or to use a dedicated, additional port along with the unencrypted\nstandard native_transport_port.\nEnabling client encryption and keeping native_transport_port_ssl disabled will use encryption\nfor native_transport_port. Setting native_transport_port_ssl to a different value\nfrom native_transport_port will use encryption for native_transport_port_ssl while\nkeeping native_transport_port unencrypted.</p><p><em>Default Value:</em> 9142</p></div><div class=\"section\" id=\"native-transport-max-threads\"><h2><code class=\"docutils literal\">native_transport_max_threads</code></h2><p><em>This option is commented out by default.</em>\nThe maximum threads for handling requests (note that idle threads are stopped\nafter 30 seconds so there is not corresponding minimum setting).</p><p><em>Default Value:</em> 128</p></div><div class=\"section\" id=\"native-transport-max-frame-size-in-mb\"><h2><code class=\"docutils literal\">native_transport_max_frame_size_in_mb</code></h2><p><em>This option is commented out by default.</em></p><p>The maximum size of allowed frame. Frame (requests) larger than this will\nbe rejected as invalid. The default is 256MB. If you’re changing this parameter,\nyou may want to adjust max_value_size_in_mb accordingly.</p><p><em>Default Value:</em> 256</p></div><div class=\"section\" id=\"native-transport-max-concurrent-connections\"><h2><code class=\"docutils literal\">native_transport_max_concurrent_connections</code></h2><p><em>This option is commented out by default.</em></p><p>The maximum number of concurrent client connections.\nThe default is -1, which means unlimited.</p><p><em>Default Value:</em> -1</p></div><div class=\"section\" id=\"native-transport-max-concurrent-connections-per-ip\"><h2><code class=\"docutils literal\">native_transport_max_concurrent_connections_per_ip</code></h2><p><em>This option is commented out by default.</em></p><p>The maximum number of concurrent client connections per source ip.\nThe default is -1, which means unlimited.</p><p><em>Default Value:</em> -1</p></div><div class=\"section\" id=\"rpc-address\"><h2><code class=\"docutils literal\">rpc_address</code></h2><p>The address or interface to bind the native transport server to.</p><p>Set rpc_address OR rpc_interface, not both.</p><p>Leaving rpc_address blank has the same effect as on listen_address\n(i.e. it will be based on the configured hostname of the node).</p><p>Note that unlike listen_address, you can specify 0.0.0.0, but you must also\nset broadcast_rpc_address to a value other than 0.0.0.0.</p><p>For security reasons, you should not expose this port to the internet.  Firewall it if needed.</p><p><em>Default Value:</em> localhost</p></div><div class=\"section\" id=\"rpc-interface\"><h2><code class=\"docutils literal\">rpc_interface</code></h2><p><em>This option is commented out by default.</em></p><p>Set rpc_address OR rpc_interface, not both. Interfaces must correspond\nto a single address, IP aliasing is not supported.</p><p><em>Default Value:</em> eth1</p></div><div class=\"section\" id=\"rpc-interface-prefer-ipv6\"><h2><code class=\"docutils literal\">rpc_interface_prefer_ipv6</code></h2><p><em>This option is commented out by default.</em></p><p>If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address\nyou can specify which should be chosen using rpc_interface_prefer_ipv6. If false the first ipv4\naddress will be used. If true the first ipv6 address will be used. Defaults to false preferring\nipv4. If there is only one address it will be selected regardless of ipv4/ipv6.</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"broadcast-rpc-address\"><h2><code class=\"docutils literal\">broadcast_rpc_address</code></h2><p><em>This option is commented out by default.</em></p><p>RPC address to broadcast to drivers and other Cassandra nodes. This cannot\nbe set to 0.0.0.0. If left blank, this will be set to the value of\nrpc_address. If rpc_address is set to 0.0.0.0, broadcast_rpc_address must\nbe set.</p><p><em>Default Value:</em> 1.2.3.4</p></div><div class=\"section\" id=\"rpc-keepalive\"><h2><code class=\"docutils literal\">rpc_keepalive</code></h2><p>enable or disable keepalive on rpc/native connections</p><p><em>Default Value:</em> true</p></div><div class=\"section\" id=\"internode-send-buff-size-in-bytes\"><h2><code class=\"docutils literal\">internode_send_buff_size_in_bytes</code></h2><p><em>This option is commented out by default.</em></p><p>Uncomment to set socket buffer size for internode communication\nNote that when setting this, the buffer size is limited by net.core.wmem_max\nand when not setting it it is defined by net.ipv4.tcp_wmem\nSee also:\n/proc/sys/net/core/wmem_max\n/proc/sys/net/core/rmem_max\n/proc/sys/net/ipv4/tcp_wmem\n/proc/sys/net/ipv4/tcp_wmem\nand ‘man tcp’</p></div><div class=\"section\" id=\"internode-recv-buff-size-in-bytes\"><h2><code class=\"docutils literal\">internode_recv_buff_size_in_bytes</code></h2><p><em>This option is commented out by default.</em></p><p>Uncomment to set socket buffer size for internode communication\nNote that when setting this, the buffer size is limited by net.core.wmem_max\nand when not setting it it is defined by net.ipv4.tcp_wmem</p></div><div class=\"section\" id=\"incremental-backups\"><h2><code class=\"docutils literal\">incremental_backups</code></h2><p>Set to true to have Cassandra create a hard link to each sstable\nflushed or streamed locally in a backups/ subdirectory of the\nkeyspace data.  Removing these links is the operator’s\nresponsibility.</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"snapshot-before-compaction\"><h2><code class=\"docutils literal\">snapshot_before_compaction</code></h2><p>Whether or not to take a snapshot before each compaction.  Be\ncareful using this option, since Cassandra won’t clean up the\nsnapshots for you.  Mostly useful if you’re paranoid when there\nis a data format change.</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"auto-snapshot\"><h2><code class=\"docutils literal\">auto_snapshot</code></h2><p>Whether or not a snapshot is taken of the data before keyspace truncation\nor dropping of column families. The STRONGLY advised default of true\nshould be used to provide data safety. If you set this flag to false, you will\nlose data on truncation or drop.</p><p><em>Default Value:</em> true</p></div><div class=\"section\" id=\"column-index-size-in-kb\"><h2><code class=\"docutils literal\">column_index_size_in_kb</code></h2><p>Granularity of the collation index of rows within a partition.\nIncrease if your rows are large, or if you have a very large\nnumber of rows per partition.  The competing goals are these:</p><ul class=\"simple\"><li>a smaller granularity means more index entries are generated\nand looking up rows withing the partition by collation column\nis faster</li>\n<li>but, Cassandra will keep the collation index in memory for hot\nrows (as part of the key cache), so a larger granularity means\nyou can cache more hot rows</li>\n</ul><p><em>Default Value:</em> 64</p></div><div class=\"section\" id=\"column-index-cache-size-in-kb\"><h2><code class=\"docutils literal\">column_index_cache_size_in_kb</code></h2><p>Per sstable indexed key cache entries (the collation index in memory\nmentioned above) exceeding this size will not be held on heap.\nThis means that only partition information is held on heap and the\nindex entries are read from disk.</p><p>Note that this size refers to the size of the\nserialized index information and not the size of the partition.</p><p><em>Default Value:</em> 2</p></div><div class=\"section\" id=\"concurrent-compactors\"><h2><code class=\"docutils literal\">concurrent_compactors</code></h2><p><em>This option is commented out by default.</em></p><p>Number of simultaneous compactions to allow, NOT including\nvalidation “compactions” for anti-entropy repair.  Simultaneous\ncompactions can help preserve read performance in a mixed read/write\nworkload, by mitigating the tendency of small sstables to accumulate\nduring a single long running compactions. The default is usually\nfine and if you experience problems with compaction running too\nslowly or too fast, you should look at\ncompaction_throughput_mb_per_sec first.</p><p>concurrent_compactors defaults to the smaller of (number of disks,\nnumber of cores), with a minimum of 2 and a maximum of 8.</p><p>If your data directories are backed by SSD, you should increase this\nto the number of cores.</p><p><em>Default Value:</em> 1</p></div><div class=\"section\" id=\"compaction-throughput-mb-per-sec\"><h2><code class=\"docutils literal\">compaction_throughput_mb_per_sec</code></h2><p>Throttles compaction to the given total throughput across the entire\nsystem. The faster you insert data, the faster you need to compact in\norder to keep the sstable count down, but in general, setting this to\n16 to 32 times the rate you are inserting data is more than sufficient.\nSetting this to 0 disables throttling. Note that this account for all types\nof compaction, including validation compaction.</p><p><em>Default Value:</em> 16</p></div><div class=\"section\" id=\"sstable-preemptive-open-interval-in-mb\"><h2><code class=\"docutils literal\">sstable_preemptive_open_interval_in_mb</code></h2><p>When compacting, the replacement sstable(s) can be opened before they\nare completely written, and used in place of the prior sstables for\nany range that has been written. This helps to smoothly transfer reads\nbetween the sstables, reducing page cache churn and keeping hot rows hot</p><p><em>Default Value:</em> 50</p></div><div class=\"section\" id=\"cas-contention-timeout-in-ms\"><h2><code class=\"docutils literal\">cas_contention_timeout_in_ms</code></h2><p>How long a coordinator should continue to retry a CAS operation\nthat contends with other proposals for the same row</p><p><em>Default Value:</em> 1000</p></div><div class=\"section\" id=\"streaming-keep-alive-period-in-secs\"><h2><code class=\"docutils literal\">streaming_keep_alive_period_in_secs</code></h2><p><em>This option is commented out by default.</em></p><p>Set keep-alive period for streaming\nThis node will send a keep-alive message periodically with this period.\nIf the node does not receive a keep-alive message from the peer for\n2 keep-alive cycles the stream session times out and fail\nDefault value is 300s (5 minutes), which means stalled stream\ntimes out in 10 minutes by default</p><p><em>Default Value:</em> 300</p></div><div class=\"section\" id=\"streaming-connections-per-host\"><h2><code class=\"docutils literal\">streaming_connections_per_host</code></h2><p><em>This option is commented out by default.</em></p><p>Limit number of connections per host for streaming\nIncrease this when you notice that joins are CPU-bound rather that network\nbound (for example a few nodes with big files).</p><p><em>Default Value:</em> 1</p></div><div class=\"section\" id=\"phi-convict-threshold\"><h2><code class=\"docutils literal\">phi_convict_threshold</code></h2><p><em>This option is commented out by default.</em></p><p>phi value that must be reached for a host to be marked down.\nmost users should never need to adjust this.</p><p><em>Default Value:</em> 8</p></div><div class=\"section\" id=\"endpoint-snitch\"><h2><code class=\"docutils literal\">endpoint_snitch</code></h2><p>endpoint_snitch – Set this to a class that implements\nIEndpointSnitch.  The snitch has two functions:</p><ul class=\"simple\"><li>it teaches Cassandra enough about your network topology to route\nrequests efficiently</li>\n<li>it allows Cassandra to spread replicas around your cluster to avoid\ncorrelated failures. It does this by grouping machines into\n“datacenters” and “racks.”  Cassandra will do its best not to have\nmore than one replica on the same “rack” (which may not actually\nbe a physical location)</li>\n</ul><p>CASSANDRA WILL NOT ALLOW YOU TO SWITCH TO AN INCOMPATIBLE SNITCH\nONCE DATA IS INSERTED INTO THE CLUSTER.  This would cause data loss.\nThis means that if you start with the default SimpleSnitch, which\nlocates every node on “rack1” in “datacenter1”, your only options\nif you need to add another datacenter are GossipingPropertyFileSnitch\n(and the older PFS).  From there, if you want to migrate to an\nincompatible snitch like Ec2Snitch you can do it by adding new nodes\nunder Ec2Snitch (which will locate them in a new “datacenter”) and\ndecommissioning the old ones.</p><p>Out of the box, Cassandra provides:</p><dl class=\"docutils\"><dt>SimpleSnitch:</dt>\n<dd>Treats Strategy order as proximity. This can improve cache\nlocality when disabling read repair.  Only appropriate for\nsingle-datacenter deployments.</dd>\n<dt>GossipingPropertyFileSnitch</dt>\n<dd>This should be your go-to snitch for production use.  The rack\nand datacenter for the local node are defined in\ncassandra-rackdc.properties and propagated to other nodes via\ngossip.  If cassandra-topology.properties exists, it is used as a\nfallback, allowing migration from the PropertyFileSnitch.</dd>\n<dt>PropertyFileSnitch:</dt>\n<dd>Proximity is determined by rack and data center, which are\nexplicitly configured in cassandra-topology.properties.</dd>\n<dt>Ec2Snitch:</dt>\n<dd>Appropriate for EC2 deployments in a single Region. Loads Region\nand Availability Zone information from the EC2 API. The Region is\ntreated as the datacenter, and the Availability Zone as the rack.\nOnly private IPs are used, so this will not work across multiple\nRegions.</dd>\n<dt>Ec2MultiRegionSnitch:</dt>\n<dd>Uses public IPs as broadcast_address to allow cross-region\nconnectivity.  (Thus, you should set seed addresses to the public\nIP as well.) You will need to open the storage_port or\nssl_storage_port on the public IP firewall.  (For intra-Region\ntraffic, Cassandra will switch to the private IP after\nestablishing a connection.)</dd>\n<dt>RackInferringSnitch:</dt>\n<dd>Proximity is determined by rack and data center, which are\nassumed to correspond to the 3rd and 2nd octet of each node’s IP\naddress, respectively.  Unless this happens to match your\ndeployment conventions, this is best used as an example of\nwriting a custom Snitch class and is provided in that spirit.</dd>\n</dl><p>You can use a custom Snitch by setting this to the full class name\nof the snitch, which will be assumed to be on your classpath.</p><p><em>Default Value:</em> SimpleSnitch</p></div><div class=\"section\" id=\"dynamic-snitch-reset-interval-in-ms\"><h2><code class=\"docutils literal\">dynamic_snitch_reset_interval_in_ms</code></h2><p>controls how often to reset all host scores, allowing a bad host to\npossibly recover</p><p><em>Default Value:</em> 600000</p></div><div class=\"section\" id=\"dynamic-snitch-badness-threshold\"><h2><code class=\"docutils literal\">dynamic_snitch_badness_threshold</code></h2><p>if set greater than zero and read_repair_chance is &lt; 1.0, this will allow\n‘pinning’ of replicas to hosts in order to increase cache capacity.\nThe badness threshold will control how much worse the pinned host has to be\nbefore the dynamic snitch will prefer other replicas over it.  This is\nexpressed as a double which represents a percentage.  Thus, a value of\n0.2 means Cassandra would continue to prefer the static snitch values\nuntil the pinned host was 20% worse than the fastest.</p><p><em>Default Value:</em> 0.1</p></div><div class=\"section\" id=\"server-encryption-options\"><h2><code class=\"docutils literal\">server_encryption_options</code></h2><p>Enable or disable inter-node encryption\nJVM defaults for supported SSL socket protocols and cipher suites can\nbe replaced using custom encryption options. This is not recommended\nunless you have policies in place that dictate certain settings, or\nneed to disable vulnerable ciphers or protocols in case the JVM cannot\nbe updated.\nFIPS compliant settings can be configured at JVM level and should not\ninvolve changing encryption settings here:\n<a class=\"reference external\" href=\"https://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/FIPS.html\">https://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/FIPS.html</a>\n<em>NOTE</em> No custom encryption options are enabled at the moment\nThe available internode options are : all, none, dc, rack</p><p>If set to dc cassandra will encrypt the traffic between the DCs\nIf set to rack cassandra will encrypt the traffic between the racks</p><p>The passwords used in these options must match the passwords used when generating\nthe keystore and truststore.  For instructions on generating these files, see:\n<a class=\"reference external\" href=\"http://download.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html#CreateKeystore\">http://download.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html#CreateKeystore</a></p><p><em>Default Value (complex option)</em>:</p><div class=\"highlight-default\"><div class=\"highlight\"><pre>internode_encryption: none\nkeystore: conf/.keystore\nkeystore_password: cassandra\ntruststore: conf/.truststore\ntruststore_password: cassandra\n# More advanced defaults below:\n# protocol: TLS\n# algorithm: SunX509\n# store_type: JKS\n# cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]\n# require_client_auth: false\n# require_endpoint_verification: false\n</pre></div></div></div><div class=\"section\" id=\"client-encryption-options\"><h2><code class=\"docutils literal\">client_encryption_options</code></h2><p>enable or disable client/server encryption.</p><p><em>Default Value (complex option)</em>:</p><div class=\"highlight-default\"><div class=\"highlight\"><pre>enabled: false\n# If enabled and optional is set to true encrypted and unencrypted connections are handled.\noptional: false\nkeystore: conf/.keystore\nkeystore_password: cassandra\n# require_client_auth: false\n# Set trustore and truststore_password if require_client_auth is true\n# truststore: conf/.truststore\n# truststore_password: cassandra\n# More advanced defaults below:\n# protocol: TLS\n# algorithm: SunX509\n# store_type: JKS\n# cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]\n</pre></div></div></div><div class=\"section\" id=\"internode-compression\"><h2><code class=\"docutils literal\">internode_compression</code></h2><p>internode_compression controls whether traffic between nodes is\ncompressed.\nCan be:</p><dl class=\"docutils\"><dt>all</dt>\n<dd>all traffic is compressed</dd>\n<dt>dc</dt>\n<dd>traffic between different datacenters is compressed</dd>\n<dt>none</dt>\n<dd>nothing is compressed.</dd>\n</dl><p><em>Default Value:</em> dc</p></div><div class=\"section\" id=\"inter-dc-tcp-nodelay\"><h2><code class=\"docutils literal\">inter_dc_tcp_nodelay</code></h2><p>Enable or disable tcp_nodelay for inter-dc communication.\nDisabling it will result in larger (but fewer) network packets being sent,\nreducing overhead from the TCP protocol itself, at the cost of increasing\nlatency if you block for cross-datacenter responses.</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"tracetype-query-ttl\"><h2><code class=\"docutils literal\">tracetype_query_ttl</code></h2><p>TTL for different trace types used during logging of the repair process.</p><p><em>Default Value:</em> 86400</p></div><div class=\"section\" id=\"tracetype-repair-ttl\"><h2><code class=\"docutils literal\">tracetype_repair_ttl</code></h2><p><em>Default Value:</em> 604800</p></div><div class=\"section\" id=\"transparent-data-encryption-options\"><h2><code class=\"docutils literal\">transparent_data_encryption_options</code></h2><p>Enables encrypting data at-rest (on disk). Different key providers can be plugged in, but the default reads from\na JCE-style keystore. A single keystore can hold multiple keys, but the one referenced by\nthe “key_alias” is the only key that will be used for encrypt opertaions; previously used keys\ncan still (and should!) be in the keystore and will be used on decrypt operations\n(to handle the case of key rotation).</p><p>It is strongly recommended to download and install Java Cryptography Extension (JCE)\nUnlimited Strength Jurisdiction Policy Files for your version of the JDK.\n(current link: <a class=\"reference external\" href=\"http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html\">http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html</a>)</p><p>Currently, only the following file types are supported for transparent data encryption, although\nmore are coming in future cassandra releases: commitlog, hints</p><p><em>Default Value (complex option)</em>:</p><div class=\"highlight-default\"><div class=\"highlight\"><pre>enabled: false\nchunk_length_kb: 64\ncipher: AES/CBC/PKCS5Padding\nkey_alias: testing:1\n# CBC IV length for AES needs to be 16 bytes (which is also the default size)\n# iv_length: 16\nkey_provider:\n  - class_name: org.apache.cassandra.security.JKSKeyProvider\n    parameters:\n      - keystore: conf/.keystore\n        keystore_password: cassandra\n        store_type: JCEKS\n        key_password: cassandra\n</pre></div></div></div><div class=\"section\" id=\"tombstone-warn-threshold\"><h2><code class=\"docutils literal\">tombstone_warn_threshold</code></h2><div class=\"section\" id=\"safety-thresholds\"><h3>SAFETY THRESHOLDS #</h3><p>When executing a scan, within or across a partition, we need to keep the\ntombstones seen in memory so we can return them to the coordinator, which\nwill use them to make sure other replicas also know about the deleted rows.\nWith workloads that generate a lot of tombstones, this can cause performance\nproblems and even exaust the server heap.\n(<a class=\"reference external\" href=\"http://www.datastax.com/dev/blog/cassandra-anti-patterns-queues-and-queue-like-datasets\">http://www.datastax.com/dev/blog/cassandra-anti-patterns-queues-and-queue-like-datasets</a>)\nAdjust the thresholds here if you understand the dangers and want to\nscan more tombstones anyway.  These thresholds may also be adjusted at runtime\nusing the StorageService mbean.</p><p><em>Default Value:</em> 1000</p></div></div><div class=\"section\" id=\"tombstone-failure-threshold\"><h2><code class=\"docutils literal\">tombstone_failure_threshold</code></h2><p><em>Default Value:</em> 100000</p></div><div class=\"section\" id=\"batch-size-warn-threshold-in-kb\"><h2><code class=\"docutils literal\">batch_size_warn_threshold_in_kb</code></h2><p>Log WARN on any multiple-partition batch size exceeding this value. 5kb per batch by default.\nCaution should be taken on increasing the size of this threshold as it can lead to node instability.</p><p><em>Default Value:</em> 5</p></div><div class=\"section\" id=\"batch-size-fail-threshold-in-kb\"><h2><code class=\"docutils literal\">batch_size_fail_threshold_in_kb</code></h2><p>Fail any multiple-partition batch exceeding this value. 50kb (10x warn threshold) by default.</p><p><em>Default Value:</em> 50</p></div><div class=\"section\" id=\"unlogged-batch-across-partitions-warn-threshold\"><h2><code class=\"docutils literal\">unlogged_batch_across_partitions_warn_threshold</code></h2><p>Log WARN on any batches not of type LOGGED than span across more partitions than this limit</p><p><em>Default Value:</em> 10</p></div><div class=\"section\" id=\"compaction-large-partition-warning-threshold-mb\"><h2><code class=\"docutils literal\">compaction_large_partition_warning_threshold_mb</code></h2><p>Log a warning when compacting partitions larger than this value</p><p><em>Default Value:</em> 100</p></div><div class=\"section\" id=\"gc-log-threshold-in-ms\"><h2><code class=\"docutils literal\">gc_log_threshold_in_ms</code></h2><p><em>This option is commented out by default.</em></p><p>GC Pauses greater than 200 ms will be logged at INFO level\nThis threshold can be adjusted to minimize logging if necessary</p><p><em>Default Value:</em> 200</p></div><div class=\"section\" id=\"gc-warn-threshold-in-ms\"><h2><code class=\"docutils literal\">gc_warn_threshold_in_ms</code></h2><p><em>This option is commented out by default.</em></p><p>GC Pauses greater than gc_warn_threshold_in_ms will be logged at WARN level\nAdjust the threshold based on your application throughput requirement. Setting to 0\nwill deactivate the feature.</p><p><em>Default Value:</em> 1000</p></div><div class=\"section\" id=\"max-value-size-in-mb\"><h2><code class=\"docutils literal\">max_value_size_in_mb</code></h2><p><em>This option is commented out by default.</em></p><p>Maximum size of any value in SSTables. Safety measure to detect SSTable corruption\nearly. Any value size larger than this threshold will result into marking an SSTable\nas corrupted.</p><p><em>Default Value:</em> 256</p></div><div class=\"section\" id=\"back-pressure-enabled\"><h2><code class=\"docutils literal\">back_pressure_enabled</code></h2><p>Back-pressure settings #\nIf enabled, the coordinator will apply the back-pressure strategy specified below to each mutation\nsent to replicas, with the aim of reducing pressure on overloaded replicas.</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"back-pressure-strategy\"><h2><code class=\"docutils literal\">back_pressure_strategy</code></h2><p>The back-pressure strategy applied.\nThe default implementation, RateBasedBackPressure, takes three arguments:\nhigh ratio, factor, and flow type, and uses the ratio between incoming mutation responses and outgoing mutation requests.\nIf below high ratio, outgoing mutations are rate limited according to the incoming rate decreased by the given factor;\nif above high ratio, the rate limiting is increased by the given factor;\nsuch factor is usually best configured between 1 and 10, use larger values for a faster recovery\nat the expense of potentially more dropped mutations;\nthe rate limiting is applied according to the flow type: if FAST, it’s rate limited at the speed of the fastest replica,\nif SLOW at the speed of the slowest one.\nNew strategies can be added. Implementors need to implement org.apache.cassandra.net.BackpressureStrategy and\nprovide a public constructor accepting a Map&lt;String, Object&gt;.</p></div><div class=\"section\" id=\"otc-coalescing-strategy\"><h2><code class=\"docutils literal\">otc_coalescing_strategy</code></h2><p><em>This option is commented out by default.</em></p><p>Coalescing Strategies #\nCoalescing multiples messages turns out to significantly boost message processing throughput (think doubling or more).\nOn bare metal, the floor for packet processing throughput is high enough that many applications won’t notice, but in\nvirtualized environments, the point at which an application can be bound by network packet processing can be\nsurprisingly low compared to the throughput of task processing that is possible inside a VM. It’s not that bare metal\ndoesn’t benefit from coalescing messages, it’s that the number of packets a bare metal network interface can process\nis sufficient for many applications such that no load starvation is experienced even without coalescing.\nThere are other benefits to coalescing network messages that are harder to isolate with a simple metric like messages\nper second. By coalescing multiple tasks together, a network thread can process multiple messages for the cost of one\ntrip to read from a socket, and all the task submission work can be done at the same time reducing context switching\nand increasing cache friendliness of network message processing.\nSee CASSANDRA-8692 for details.</p><p>Strategy to use for coalescing messages in OutboundTcpConnection.\nCan be fixed, movingaverage, timehorizon, disabled (default).\nYou can also specify a subclass of CoalescingStrategies.CoalescingStrategy by name.</p><p><em>Default Value:</em> DISABLED</p></div><div class=\"section\" id=\"otc-coalescing-window-us\"><h2><code class=\"docutils literal\">otc_coalescing_window_us</code></h2><p><em>This option is commented out by default.</em></p><p>How many microseconds to wait for coalescing. For fixed strategy this is the amount of time after the first\nmessage is received before it will be sent with any accompanying messages. For moving average this is the\nmaximum amount of time that will be waited as well as the interval at which messages must arrive on average\nfor coalescing to be enabled.</p><p><em>Default Value:</em> 200</p></div><div class=\"section\" id=\"otc-coalescing-enough-coalesced-messages\"><h2><code class=\"docutils literal\">otc_coalescing_enough_coalesced_messages</code></h2><p><em>This option is commented out by default.</em></p><p>Do not try to coalesce messages if we already got that many messages. This should be more than 2 and less than 128.</p><p><em>Default Value:</em> 8</p></div><div class=\"section\" id=\"otc-backlog-expiration-interval-ms\"><h2><code class=\"docutils literal\">otc_backlog_expiration_interval_ms</code></h2><p><em>This option is commented out by default.</em></p><p>How many milliseconds to wait between two expiration runs on the backlog (queue) of the OutboundTcpConnection.\nExpiration is done if messages are piling up in the backlog. Droppable messages are expired to free the memory\ntaken by expired messages. The interval should be between 0 and 1000, and in most installations the default value\nwill be appropriate. A smaller value could potentially expire messages slightly sooner at the expense of more CPU\ntime and queue contention while iterating the backlog of messages.\nAn interval of 0 disables any wait time, which is the behavior of former Cassandra versions.</p><p><em>Default Value:</em> 200</p></div><div class=\"section\" id=\"ideal-consistency-level\"><h2><code class=\"docutils literal\">ideal_consistency_level</code></h2><p><em>This option is commented out by default.</em></p><p>Track a metric per keyspace indicating whether replication achieved the ideal consistency\nlevel for writes without timing out. This is different from the consistency level requested by\neach write which may be lower in order to facilitate availability.</p><p><em>Default Value:</em> EACH_QUORUM</p></div>"}}]}},"pageContext":{"alternative_id":12157}}