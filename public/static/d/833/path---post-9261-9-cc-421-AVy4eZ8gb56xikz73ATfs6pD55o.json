{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Fourth Contact with a Monolith - Instaclustr","alternative_id":9261,"content":"<h6><i>“The thing’s hollow — it goes on forever — and — oh my God! — it’s full of stars!”</i></h6><h2>It’s full of Spreadsheets! (DataFrames)</h2><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/10/Fourth-Contact-with-a-monolith-Paul-Brebner-Instaclustr.gif\"><img class=\"alignnone size-full wp-image-7409\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/10/Fourth-Contact-with-a-monolith-Paul-Brebner-Instaclustr.gif\" alt=\"Fourth Contact with a monolith Paul Brebner Instaclustr\" width=\"373\" height=\"220\" /></a></p><p>Given that a dog, Laika, was the 1st astronaut to orbit the earth, it’s appropriate for a dog to travel through the wormhole.</p><p>After travelling through the wormhole, the 2001 story (book version, as is the “it’s full of stars” quote which is not in the movie) concludes.</p><p>Dave leaves the pod and explores the hotel room. He finds a telephone and telephone book, but the phone doesn’t work and the telephone book is blank. He explores more and finds a refrigerator, where there is a variety of packaged food, but it all contains the same blue substance (even, to his disappointment, in the beer cans). He eats the blue food, and drinks the tap water – which tasted terrible – because, being distilled water, it had no taste at all. He turns on a television but all the programs were two years old. Dave lies down on the bed, turns off the light and “So, for the last time, David Bowman slept.”</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/10/Fourth-Contact-with-a-monolith-Space-Odyssey.png\"><img class=\"aligncenter wp-image-7412 size-full\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/10/Fourth-Contact-with-a-monolith-Space-Odyssey.png\" alt=\"Fourth Contact with a monolith space odyssey\" width=\"599\" height=\"269\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Fourth-Contact-with-a-monolith-Space-Odyssey.png 599w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Fourth-Contact-with-a-monolith-Space-Odyssey-300x135.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Fourth-Contact-with-a-monolith-Space-Odyssey-107x48.png 107w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Fourth-Contact-with-a-monolith-Space-Odyssey-240x108.png 240w\" /></a></p><p>What’s new in this blog? DataFrames (giant Spreadsheets), ML Pipelines and Scala!</p><p>As I noted <a href=\"https://www.instaclustr.com/third-contact-monolith-part-c-pod/\">last blog, </a>the trip took a while and the TV shows are old. The ML API for Spark is now based on the DataFrame API:</p><p><a href=\"https://spark.apache.org/docs/latest/ml-guide.html\">https://spark.apache.org/docs/latest/ml-guide.html</a></p><p><b>The MLlib RDD-based API is now in maintenance mode.</b></p><p>As of Spark 2.0, the <a href=\"https://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds\">RDD</a>-based APIs in the spark.mllib package has entered maintenance mode. The primary Machine Learning API for Spark is now the <a href=\"https://spark.apache.org/docs/latest/sql-programming-guide.html\">DataFrame</a>-based API in the spark.ml package.</p><p>This architecture diagram shows the current Spark architecture. The components of particular interest are Cassandra (the data source), MLlib, DataFrames, ML Pipelines and Scala:</p><p><img class=\"alignnone\" src=\"https://dhenschen.files.wordpress.com/2015/06/spark-2015-vision.jpg\" alt=\"Apache Spark Architecture Diagram Instaclustr\" width=\"793\" height=\"559\" /></p><p><a href=\"https://www.instaclustr.com/third-contact-monolith-part-c-pod/\">Last blog</a> we explored Decision Tree Machine Learning based on RDDs, using a sample of the Instametrics monitoring data, to try and predict long JVM Garbage Collections. Now we’ll update the code to DataFrames and use all the available real monitoring data (a snapshot from the Instaclustr pre-production clusters).</p><p>What are <a href=\"https://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes\">DataFrames</a>? Big, distributed, scalable spreadsheets! They are immutable and can be transformed with a DSL, <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\">pre-defined functions, and user-defined functions.</a>  <a href=\"https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-sql-DataFrame.html\">This is a good introduction</a>. </p><h2>Scala code</h2><p>Why did I ditch Java (at least temporarily)? Have I become “transcendent” after watching 2001 too many times?</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/10/Scala-Code-Fourth-contact-with-a-monolith.png\"><img class=\"aligncenter wp-image-7414 size-full\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/10/Scala-Code-Fourth-contact-with-a-monolith.png\" alt=\"Scala Code Fourth Contact with a monolith instaclustr\" width=\"606\" height=\"351\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Scala-Code-Fourth-contact-with-a-monolith.png 606w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Scala-Code-Fourth-contact-with-a-monolith-300x174.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Scala-Code-Fourth-contact-with-a-monolith-83x48.png 83w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Scala-Code-Fourth-contact-with-a-monolith-186x108.png 186w\" /></a></p><p>Not exactly. Because Instaclustr provides a <a href=\"https://support.instaclustr.com/hc/en-us/articles/213097877-Getting-Started-with-Instaclustr-Spark-Cassandra\">fully managed cluster option which includes Cassandra, Spark and Zeppelin</a> (a Spark Scala web-based notebook) it was easier to quickly deploy, run and debug Scala code than Java. You just type Scala code into Zeppelin in a browser, run it and look at the output (and repeat). There’s no need to set up a separate client instance in AWS. Trying to learn both Scala and Spark at the same time was “fun”. Scala is syntactically similar to Java (just throw away the semi-colons as Scala is line-oriented, and you don’t need variable type declarations as types are inferred, but statically typed), so the example should still make sense even without any Scala experience (like me).</p><p>For example:</p><p>var anyGreeting = “Hello world!” // universal greeting<br />val queenGreeting = “How do you do?” // How to greet the Queen (of England)</p><p>// equivalent to<br />var anyGreeting : String = “Hello world!”<br />val queenGreeting : String = “How do you do?”</p><p>vars are mutable and can be changed, vals are immutable and can’t be changed, so:</p><p>anyGreeting = “Kia ora” // hello in Maori<br />queenGreeting = “Ow ya goin mate” // hello in Australian, won’t work as immutable<br />anyGreeting = 3.14159 // won’t work, as anyGreeting is a String</p><p>Let’s revisit the Decision Tree ML code in Scala + DataFrames.</p><p><a href=\"https://spark.apache.org/docs/latest/ml-classification-regression.html#decision-tree-classifier\">Here’s the latest documentation for the Decision Tree Classifier for DataFrames. </a></p><p>As the Instametrics monitoring data we have is in Cassandra we need to read the data into Spark from Cassandra first. To do this we use the <a href=\"https://github.com/datastax/spark-cassandra-connector/blob/master/doc/14_data_frames.md\">spark cassandra connector.</a></p><p>Instaclustr provides a <a href=\"https://support.instaclustr.com/hc/en-us/articles/213097877-Getting-Started-with-Instaclustr-Spark-Cassandra\">Spark Cassandra assembly Jar</a>.</p><p>In Zeppelin (more on Zeppelin in a future blog) this is loaded as follows:</p><p>Then lots of imports (in Zeppelin you have to run each line separately):</p><p>In Zeppelin, the spark context will already be defined for you, and there are <a href=\"https://github.com/datastax/spark-cassandra-connector/blob/master/doc/14_data_frames.md#example-using-format-helper-functions\">helper functions </a>to simplify reading in a Cassandra table. For some reason, the 1st argument to cassandraFormat is the <i>table</i> name, and the 2nd is the <i>keyspace</i>.  This returns (lazily) a DataFrame object.</p><p>val data = spark<br />.read<br />.cassandraFormat(“mllib_wide”, “instametrics”)<br />.load()</p><p>Loading lots of data from Cassandra works well, it’s fast, and DataFrames infers the schema correctly.  The format of the data DataFrame is lots of examples (Rows) with columns as follows (wide table format has each data variable/feature in a separate column):</p><p>&lt;host, bucket_time, label, metric1, metric2, metric3, …&gt;</p><p>Where did this data come from? From some complex pre-processing (next blog). The label column was also previously computed and saved (the label is the class to be learned, in this case either 1.0 for positive examples, or 0.0 for negative examples).</p><p>The next trick is to replace null values with something else (e.g. 0), as we run into problems with nulls later on (even though in theory the MLLib algorithms cope with sparse vectors).</p><p>val data2 = data.na.fill(0)</p><p>Here are the relevant documents:</p><p><a href=\"https://spark.apache.org/docs/1.3.1/api/scala/index.html#org.apache.spark.sql.DataFrame\">https://spark.apache.org/docs/1.3.1/api/scala/index.html#org.apache.spark.sql.DataFrame</a></p><p><a href=\"https://spark.apache.org/docs/1.4.0/api/java/org/apache/spark/sql/DataFrameNaFunctions.html\">https://spark.apache.org/docs/1.4.0/api/java/org/apache/spark/sql/DataFrameNaFunctions.html</a></p><p>As in the previous RDD example we need two subsets of the data, for training and testing:</p><p>val Array(trainingData, testData) = data2.randomSplit(Array(0.7, 0.3))</p><p>Next we need to produce an Array(String) of column names to be used as features. In theory all of the columns can be used as features, except the label column, but in practice some columns are not the correct type for the classifier and must be filtered out (e.g. for this example we can only have Doubles):</p><p>val featureCols = data2.columns.filter(!_.equals(“host”)).filter(!_.equals(“bucket_time”)).filter(!_.equals(“label”))</p><p>Next we need to create a single column of features. VectorAssembler is a transformer that combines a list of columns into a single vector column, which is what we want.  However, it turns out to have problems with null values (which is why we replaced them above).  </p><p>val features = new VectorAssembler()<br />.setInputCols(featureCols)<br />.setOutputCol(“features”)</p><p>Here’s the VectorAssembler documentation:</p><p><a href=\"https://spark.apache.org/docs/2.1.0/ml-features.html#vectorassembler\">https://spark.apache.org/docs/2.1.0/ml-features.html#vectorassembler</a></p><p><a href=\"https://spark.apache.org/docs/2.1.0/api/scala/index.html#org.apache.spark.ml.feature.VectorAssembler\">https://spark.apache.org/docs/2.1.0/api/scala/index.html#org.apache.spark.ml.feature.VectorAssembler</a></p><p>Next create a DecisionTreeClassifier, with the column to predict as “label”, and the features to use called “features” (from the VectorAssembler above):</p><p>val dt = new DecisionTreeClassifier()<br />.setLabelCol(“label”)<br />.setFeaturesCol(“features”)</p><p>More documentation:</p><p><a href=\"https://spark.apache.org/docs/1.5.2/ml-decision-tree.html\">https://spark.apache.org/docs/1.5.2/ml-decision-tree.html</a></p><p><a href=\"https://spark.apache.org/docs/2.0.0/api/java/org/apache/spark/ml/classification/DecisionTreeClassifier.html\">https://spark.apache.org/docs/2.0.0/api/java/org/apache/spark/ml/classification/DecisionTreeClassifier.html</a></p><h2>Pipelines, Tunnels, Wormholes</h2><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/10/Pipelines-tunnels-Wormholes-Instaclustr.png\"><img class=\"aligncenter wp-image-7422 size-full\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/10/Pipelines-tunnels-Wormholes-Instaclustr.png\" alt=\"Pipelines, tunnels, Wormholes Instaclustr\" width=\"1364\" height=\"964\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Pipelines-tunnels-Wormholes-Instaclustr.png 1364w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Pipelines-tunnels-Wormholes-Instaclustr-300x212.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Pipelines-tunnels-Wormholes-Instaclustr-768x543.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Pipelines-tunnels-Wormholes-Instaclustr-1024x724.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Pipelines-tunnels-Wormholes-Instaclustr-872x616.png 872w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Pipelines-tunnels-Wormholes-Instaclustr-640x452.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Pipelines-tunnels-Wormholes-Instaclustr-68x48.png 68w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Pipelines-tunnels-Wormholes-Instaclustr-153x108.png 153w\" /></a></p><p>Another new Spark feature is <a href=\"https://spark.apache.org/docs/2.1.1/ml-pipeline.html\">Pipelines</a> (tunnels, wormholes, etc).  A Pipeline chains multiple Transformers and Estimators together for a ML workflow. A Transformer is an algorithm (feature transformers or learned models) that can transform one DataFrame into another DataFrame (e.g. a ML model transforms a DataFrame with labels and features into a DataFrame with predictions). An Estimator is an algorithm which can be fit on a DataFrame to produce a Model (which is a Transformer). This Pipeline consists of only two stages, features (feature transformer) and dt (estimator) (More on <a href=\"https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-mllib/spark-mllib-models.html#PipelineModel\">stages</a>):</p><p>val pipeline = new Pipeline()<br />.setStages(Array(features, dt))</p><p>Now we actually train the model by calling the pipeline fit method on the trainingData:</p><p>val model = pipeline.fit(trainingData)</p><p>And then test it by applying the model to the testData. This produces a DataFrame with a “predictions” column (by default), a “label” column (actual class), and the “features” column:</p><p>val predictions = model.transform(testData)<br />predictions.select(“prediction”, “label”, “features”)</p><p>  The <a href=\"https://spark.apache.org/docs/2.0.1/api/java/org/apache/spark/ml/evaluation/MulticlassClassificationEvaluator.html\">MulticlassClassificationEvaluator</a> can compute a limited number of evaluation metrics, here’s an example for “accuracy”:</p><p>val evaluator = new MulticlassClassificationEvaluator()<br />        .setLabelCol(“label”)<br />.setPredictionCol(“prediction”)<br />.setMetricName(“accuracy”)</p><p>val accuracy = evaluator.evaluate(predictions)<br />println(“Test Error = ” + (1.0 – accuracy))</p><p>However,  as pointed out the previous blog the model accuracy isn’t a particularly useful metric depending on the actual ratio of positive/negative examples.  We can use the <i>MulticlassMetrics</i> to compute the confusion matrix, precision, and recall. And then also compute the actual rate of negative examples for comparison with the model accuracy metric. <a href=\"https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/evaluation/MulticlassMetrics.scala\"><i>MulticlassMetrics</i></a> has an auxiliary constructor for DataFrames but I still had to convert to an RDD before use. See <a href=\"https://books.google.com.au/books?id=NJwnDwAAQBAJ&amp;pg=PA78&amp;lpg=PA78&amp;dq=MulticlassMetrics+Dataframe&amp;source=bl&amp;ots=fcuHIgop3m&amp;sig=1-VuKMSYhBGyCGFxhvb2rnGeuA0&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjJgvnWlefWAhXLwLwKHWNVD6EQ6AEISzAG#v=onepage&amp;q=MulticlassMetrics%20Dataframe&amp;f=false\">this book f</a>or more information and ways of creating a confusion matrix from a DataFrame directly.</p><p>And now print the model out.</p><p>val treeModel = model.stages(1).asInstanceOf[DecisionTreeClassificationModel]<br />println(“Learned classification tree model:\\n” + treeModel.toDebugString)</p><p>The decision tree actually doesn’t make much “sense” as the named features have been replaced by feature numbers (e.g. If (feature 23 &lt;= 8720.749999999996) Predict 0.0 Else etc). Ideally there should be some way of automatically converting them back to the original names.</p><h2>Results</h2><p>Did it work? I’m sorry, I can’t tell you. In theory after going through the wormhole, information flow to the outside world is limited (the telephone didn’t work).  Ok, we’re not really in some virtual reality hotel/zoo at the other end of the universe (probably), and Dave did travel back to this universe, so I can tell you…</p><p>For this example we used real data, but obtained from our pre-production clusters. Rather than pre-process all the raw data I took a shortcut and use the rolled up data which had min, avg and max values computed for every metric over 5 minute periods. However, I forgot to check the bucket_time values, and later discovered that the bucket_time was actually hourly, so I ended up using hours as the default time period for learning. I also noticed that the JVM GC durations had not been rolled up for this snapshot of data, so I used these read and write SLA metrics to compute the label:</p><p>/cassandra/sla/latency/read_avg(avg)<br />/cassandra/sla/latency/read_max(max)<br />/cassandra/sla/latency/write_avg(avg)<br />/cassandra/sla/latency/write_max(max)</p><p>These metrics were combined with thresholds to ensure that about 5% of the examples were positive (i.e. had “long” read/write times). And these metrics were removed from the data before use to prevent cheating by the machine learning algorithm.</p><p>The wide data table read from Cassandra had 1518 examples (rows), which was split into training and test data. The training data set had 1067 examples, but only 60 positive examples (not a huge number for a Big Data problem, and in practice the number of examples needs to be &gt;&gt; the number of features, at least 2x). There were 2839 features (columns).</p><p>Learning only took a few minutes. Model accuracy was 0.96, however, as the negative example rate was 0.95 this isn’t much better than guessing. Precision was 0.60 and Recall was 0.66.</p><p>Is this result any good? A precision of 0.6 means that given a <i>prediction</i> of a SLA violation by the model, it is likely to be correct 60% of the time (but gets it wrong 40% of the time). A recall of 0.66 means that given an <i>actual</i> SLA violation, the model will correctly predict it 66% of the time (i.e. but misses it 34% of the time).  By comparison, using a sample of the data in the previous blog the recall was only 45%, so 66% is an improvement! Assuming that the cost of checking a SLA violation warning (from the model prediction) is minimal, then it’s more important to correctly predict as many actual SLA violations (assuming we can do something to prevent/mitigate them in advance, and that SLA violations are expensive if they occur), and increase the recall accuracy.   How could this be done? More and better quality data (e.g. from production clusters, as pre-production clusters and metrics are somewhat atypical, for example, many nodes may be spun up and down for short time periods, and they don’t have typical user workloads running on them), reducing the number of features (e.g. <a href=\"https://spark.apache.org/docs/2.1.0/ml-features.html#feature-selectors\">feature selection</a>, by removing redundant or highly correlated features), and different ML algorithms… </p><p>What features did the model pick to use to predict the long SLAs times from the 2839 features available? Only these 11 (which appear to be plausible):</p><ol><li>/cassandra/jvm/memory/heapMemoryUsage/max</li>\n<li>/cassandra/jvm/memory/heapMemoryUsage/used_avg</li>\n<li>/cassandra/metrics/type=ClientRequest/scope=Read/name=Latency/95thPercentile_avg</li>\n<li>/cassandra/metrics/type=ClientRequest/scope=Read/name=Latency/count_max</li>\n<li>/cassandra/metrics/type=ClientRequest/scope=Read/name=Latency/latency_per_operation_avg</li>\n<li>/cassandra/metrics/type=ClientRequest/scope=Write/name=Latency/95thPercentile_avg</li>\n<li>/cassandra/metrics/type=ClientRequest/scope=Write/name=Latency/latency_per_operation_avg</li>\n<li>/cassandra/metrics/type=ColumnFamily/keyspace=test/scope=testuncompressed/name=TotalDiskSpaceUsed_max</li>\n<li>/proc/diskstats/xvda1/sectorsWritten_max</li>\n<li>/proc/diskstats/xvdx/sectorsRead_avg</li>\n<li>/proc/stat/cpu-agg/user_max</li>\n</ol><p>Thanks to Instaclustr people for help (with Instametrics data, support for Cassandra, Spark and Zeppelin, and machine learning discussions) including Christophe, Jordan, Alwyn, Alex_I, Juan, Joe &amp; Jen.</p><h2>Next Blog</h2><p>How did we get here? Next blog we’ll look behind the scenes at the code that was used to preprocess the raw metrics from Cassandra, clean the data, convert it into the wide table format, and correctly label the examples. And, how to run Spark Scala code in Zeppelin!</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/10/How-to-run-Spark-Scala-Code-in-Zeppelin-Instaclustr.png\"><img class=\"size-full wp-image-7425 aligncenter\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/10/How-to-run-Spark-Scala-Code-in-Zeppelin-Instaclustr.png\" alt=\"How to run Spark Scala Code in Zeppelin Instaclustr\" width=\"1248\" height=\"778\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/How-to-run-Spark-Scala-Code-in-Zeppelin-Instaclustr.png 1248w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/How-to-run-Spark-Scala-Code-in-Zeppelin-Instaclustr-300x187.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/How-to-run-Spark-Scala-Code-in-Zeppelin-Instaclustr-768x479.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/How-to-run-Spark-Scala-Code-in-Zeppelin-Instaclustr-1024x638.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/How-to-run-Spark-Scala-Code-in-Zeppelin-Instaclustr-966x602.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/How-to-run-Spark-Scala-Code-in-Zeppelin-Instaclustr-640x399.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/How-to-run-Spark-Scala-Code-in-Zeppelin-Instaclustr-77x48.png 77w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/How-to-run-Spark-Scala-Code-in-Zeppelin-Instaclustr-173x108.png 173w\" /></a></p><h2>Instaclustr SPARK trial offer</h2><h5>You can try out a special offer of an Instaclustr trial cluster provisioned with <b>Cassandra, Spark and Zepplin </b>using the coupon code ST2M14:</h5><p><a href=\"https://console.instaclustr.com/user/signup?coupon-code=ST2M14\">https://console.instaclustr.com/user/signup?coupon-code=ST2M14</a></p><h2>CODE</h2>"}}]}},"pageContext":{"alternative_id":9261}}