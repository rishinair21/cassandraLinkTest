{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Understanding How YugaByte DB Runs on Kubernetes – The YugaByte Database Blog","alternative_id":12461,"content":"<p>As we reviewed in <a href=\"https://blog.yugabyte.com/docker-kubernetes-and-the-rise-of-cloud-native-databases/\">“Docker, Kubernetes and the Rise of Cloud Native Databases”</a>, Kubernetes has benefited from rapid adoption to become the de-facto choice for container orchestration. This has happened in a short span of only 4 years since Google open sourced the project in 2014. YugaByte DB’s <a href=\"https://blog.yugabyte.com/how-does-the-raft-consensus-based-replication-protocol-work-in-yugabyte-db/\">automated sharding and strongly consistent replication architecture</a> lends itself extremely well to containerized deployments powered by Kubernetes orchestration. In this post we’ll look at the various components involved in getting YugaByte DB up and running as <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\">Kubernetes StatefulSets</a>.</p><h2>YugaByte DB Architecture</h2><p>As shown in the architecture diagram below, YugaByte DB is comprised of two types of distributed services.</p><ul><li><strong>YB-Master:</strong> Responsible for keeping system metadata (such as shard-to-node mapping), coordinating system-wide operations (such as create/alter drop tables), and initiating maintenance operations (such as load-balancing). For fault-tolerance purposes, the number of YB-Masters equals the Replication Factor (RF) of the cluster. The minimum RF needed for fault-tolerance is 3.</li>\n<li><strong>YB-TServer:</strong> The data nodes responsible for hosting/serving user data in shards (also known as tablets). The number of data nodes can be increased or decreased on-demand in a cluster.</li>\n</ul><div id=\"attachment_286\" class=\"wp-caption aligncenter\"><img class=\"wp-image-286\" src=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturea-300x140.png\" alt=\"\" width=\"566\" height=\"264\" srcset=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturea-300x140.png 300w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturea-768x360.png 768w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturea.png 974w\" /><p class=\"wp-caption-text\">Architecture of a YugaByte DB Cluster</p></div><h2>Modeling YugaByte DB as a Workload on Kubernetes</h2><p><a href=\"https://blog.yugabyte.com/orchestrating-stateful-apps-with-kubernetes-statefulsets/\">Orchestrating Stateful Apps with Kubernetes</a> highlights how running stateful applications such as databases in Kubernetes require the use of the <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\">StatefulSets workload API</a>. In this context, YB-Master and YB-TServer are modeled as independent StatefulSets, as shown in the <a href=\"https://github.com/YugaByte/yugabyte-db/tree/master/cloud/kubernetes\">YugaByte DB Kubernetes YAML</a> on Github. Each of these StatefulSet pods instantiate one instance of the same <code>yugabytedb/yugabyte</code> container image but the command used to start the container is changed based on the type of server needed. The next few sections detail how exactly the YugaByte DB StatefulSets are structured in the context of running a four-node RF3 cluster on Kubernetes.</p><h2>Running YB-Master on Kubernetes</h2><p>The YB-Master deployment on Kubernetes needs one StatefulSet and two Services. One of these Services is the headless service that enables discovery of the underlying StatefulSet pods and the other is a LoadBalancer service needed to view the YB-Master Admin UI. YugaByte DB admin clients (such as the YugaByte DB EE Admin Console) connect to the any of the pods using the headless service, while admin users can connect to the LoadBalancer service.</p><div id=\"attachment_287\" class=\"wp-caption aligncenter\"><img class=\"wp-image-287\" src=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Pictureb-300x176.png\" alt=\"\" width=\"598\" height=\"351\" srcset=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Pictureb-300x176.png 300w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Pictureb-768x449.png 768w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Pictureb.png 974w\" /><p class=\"wp-caption-text\">YB-Master on Kubernetes</p></div><h3>yb-master StatefulSet</h3><p>The <a href=\"https://github.com/YugaByte/yugabyte-db/blob/master/cloud/kubernetes/yugabyte-statefulset.yaml\">YugaByte DB Kubernetes YAML</a> has a section for the yb-master StatefulSet. A few points to highlight in the specification.</p><p><strong>Replica count</strong></p><p>As shown in the figure above, to setup a 4-node YugaByte DB cluster with RF3, only 3 yb-master StatefulSet pods are needed. Hence the YAML setting for replicas to 3.</p><p><strong>Pod anti-affinity</strong></p><p>Pod anti-affinity rules are used to ensure no two yb-master pods can be scheduled onto the same Kubernetes node. This enforces maximum fault tolerance possible since a single node failure will only impact one yb-master pod and the cluster will continue functioning normally even with the remaining two yb-master pods on the two other nodes. Additionally, as noted in Kubernetes docs, the <code>preferredDuringSchedulingIgnoredDuringExecution</code> is a soft guarantee from Kubernetes that is better set to <code>requiredDuringSchedulingIgnoredDuringExecution</code> in mission-critical environments such as production.</p><p><strong>Communicating with other yb-masters</strong></p><p>Each yb-master gets to know of the other yb-masters with the –master_addresses flag populated using the fully qualified endpoint of the <code>yb-masters</code> headless service <code>yb-masters.default.svc.cluster.local:7100</code> (see next section).</p><p><strong>Ports</strong></p><p>The rpc port where other yb-masters and yb-tservers communicate is <code>7100</code> while the UI port for checking the current state of the master is <code>7000</code>.</p><p><strong>Volume mounts</strong></p><p>The <code>--fs_data_dirs</code> flag in the command points to the same disk <code>/mnt/data0</code> that is mounted to the container using the <code>datadir</code> volume mount.</p><p><strong>Update strategy</strong></p><p>The <code>RollingUpdate</code> strategy will update all the pods in the yb-master StatefulSet, in reverse ordinal order, while respecting the StatefulSet guarantees.</p><h3>yb-masters Headless service</h3><p>Kubernetes StatefulSets require the use of a headless service so that the StatefulSet pods can be discovered individually and communicated directly by other services (such as client applications). Kubernetes is not responsible for any load balancing across these pods. Such a headless service is created by simply specifying the clusterIP of the service to be <code>None</code>.</p><p>As shown above, the yb-masters headless service yaml is extremely simple. It simply opens up the UI and the rpc ports of the underlying yb-master pods.</p><h3>yb-master-ui LoadBalancer service</h3><p>The clusterwide admin UI for the yb-master can be viewed at the <code>7000</code> port of any yb-master. The <code>yb-master-ui</code> service is of the <code>LoadBalancer</code> type for this port which means that the service will load balance all the incoming requests across all the underlying pods.</p><h2>Running YB-TServer on Kubernetes</h2><p>Assuming you don’t need to view the YB-TServer’s Admin UI, the YB-TServer Kubernetes deployment needs one StatefulSet and one headless service. One important point to note is that the YB-Master service has to be up and running before the YB-TServer service.</p><div id=\"attachment_288\" class=\"wp-caption aligncenter\"><img class=\"wp-image-288\" src=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturec-300x152.png\" alt=\"\" width=\"572\" height=\"290\" srcset=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturec-300x152.png 300w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturec-768x390.png 768w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturec.png 974w\" /><p class=\"wp-caption-text\">YB-TServer and YB-Master on Kubernetes</p></div><h3>yb-tserver StatefulSet</h3><p>The YugaByte DB Kubernetes YAML has a section for the yb-master StatefulSet. A few points to highlight in the specification.</p><p><strong>Replica count</strong></p><p>As shown in the figure above, to setup a 4-node YugaByte DB cluster with RF3, 4 replicas of the yb-master StatefulSet pod are needed.</p><p><strong>Pod anti-affinity</strong></p><p>Pod anti-affinity rules are used to ensure no two yb-tserver pods can be scheduled onto the same Kubernetes node. This enforces maximum fault tolerance possible since a single node failure will only impact one yb-tserver pod and the cluster will continue functioning normally as long as quorum of yb-tserver pods are available. Note that 1 yb-tserver pod and 1 yb-master pod can be located on the same node. Again as noted in the yb-masters section, a stronger guarantee like <code>requiredDuringSchedulingIgnoredDuringExecution</code> is better for mission-critical environments.</p><p><strong>Communicating with yb-masters</strong></p><p>Each yb-tserver gets to know of the other yb-masters with the <code>--tserver_master_addrs</code> flag populated using the fully qualified endpoint of the yb-masters headless service <code>yb-masters.default.svc.cluster.local:7100</code>.</p><p><strong>Ports</strong></p><p>The rpc port where yb-masters and other yb-tservers communicate is <code>9100</code> while the UI port for checking the current state of the tserver is <code>9000</code>. Additionally, YCQL (the Cassandra compatible API) is available at port <code>9042</code> and YEDIS (the Redis compatible API) is available at port <code>6379</code>. PostgreSQL API, currently in beta, can be enabled by adding the port <code>5433</code>.</p><p><strong>Volume mounts</strong></p><p>The <code>--fs_data_dirs</code> flag points to the same disk /mnt/data0 that is mounted to the container using the datadir volume mount.</p><p><strong>Update strategy</strong></p><p>The <code>RollingUpdate</code> update strategy will update all the pods in the yb-tserver StatefulSet, in reverse ordinal order, while respecting the StatefulSet guarantees.</p><h3>yb-tservers Headless service</h3><p>As expected, the yb-tservers headless service yaml is extremely simple. It opens up the UI, the rpc ports as well as client API ports of the underlying yb-tserver pods.</p><h2>YugaByte DB on Kubernetes in Action</h2><p>In order to keep things simple to understand, we will run a 4-node YugaByte DB cluster on <a href=\"https://kubernetes.io/docs/setup/minikube/\">minikube</a>, the preferred method for running Kubernetes on your local environment.</p><h3>Prerequisites</h3><p>Follow the instructions to <a href=\"https://kubernetes.io/docs/tasks/tools/install-minikube/\">install minikube and kubectl</a> if you don’t have them setup already.</p><h3>Step 1 – Download the YugaByte DB Kubernetes YAML</h3><h3>Step 2 – Change the yb-tserver replica count from 3 to 4</h3><p>Open the the YAML in the editor of your choice and set the yb-tserver replica count to 4.</p><h3>Step 3 – Create the YugaByte DB cluster</h3><p>Now you can create the YugaByte DB cluster through the following command.</p><h3>Step 4 – Check status of the pods and services</h3><p>Since Kubernetes has to first pull the yugabytedb/yugabyte image from hub.docker.com, the cluster may take a few minutes to become live. You can check the status using the following commands.</p><p>When the cluster is ready, it will have all the 7 pods (3 for yb-master and 4 for yb-tserver) in the Running status.</p><p>You can also check the status of the 3 services we launched along with the status of the default kubernetes service itself.</p><p><code>NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE<br />kubernetes ClusterIP 10.96.0.1 443/TCP 10m<br />yb-master-ui LoadBalancer 10.102.121.64 7000:31283/TCP 8m<br />yb-masters ClusterIP None 7000/TCP,7100/TCP 8m<br />yb-tservers ClusterIP None 9000/TCP,9100/TCP,9042/TCP,6379/TCP 8m</code></p><p>Finally, you can view the nice UI dashboard provided by Kubernetes that you can launch by the following command.</p><div id=\"attachment_290\" class=\"wp-caption aligncenter\"><img class=\"wp-image-290\" src=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picture1d-300x153.png\" alt=\"\" width=\"626\" height=\"319\" srcset=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picture1d-300x153.png 300w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picture1d-768x392.png 768w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picture1d.png 974w\" /><p class=\"wp-caption-text\">Kubernetes Dashboard with YugaByte DB Installed</p></div><h3>Step 5 – View the YB-Master Admin UI</h3><p>Once the cluster is live, you can launch the YB-Master Admin UI. First use the command below to get the exact URL for the UI and then launch the URL via the browser.</p><div id=\"attachment_289\" class=\"wp-caption aligncenter\"><img class=\"wp-image-289\" src=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturee-300x162.png\" alt=\"\" width=\"603\" height=\"326\" srcset=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturee-300x162.png 300w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturee-768x414.png 768w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturee.png 974w\" /><p class=\"wp-caption-text\">YB-Master Admin UI</p></div><h3>Step 6 – Perform Day 2 Operational Tasks</h3><p>The next few steps show how to perform common day 2 operational tasks such as adding/removing nodes and performing rolling upgrades. All these operations do not impact the availability and performance of client applications thus allowing the applications to continue to operate normally.</p><p><strong>Add a Node</strong></p><p>Horizontal scaling is a breeze with YugaByte DB and with Kubernetes, the process could not be simpler. All we have to do is to let Kubernetes know how many replicas to scale to.</p><p>Now we can check the status of the scaling operation. Note that YugaByte DB automatically moves a few tablet-leaders and a few tablet-followers into the newly added node so that the cluster remains balanced across all the nodes.</p><p><strong>Remove Two Nodes</strong></p><p>Removing nodes is also very simple. Reduce the number of replicas and see the combination of Kubernetes and YugaByte DB do the rest.<br /><code>kubectl scale statefulset yb-tserver --replicas=3</code></p><p>As expected in StatefulSets, we can see that the nodes with the largest ordinal indexes (i.e. 4 and 3) are removed first.</p><p><strong>Perform Rolling Upgrade</strong></p><p>We can also perform rolling upgrades on the YugaByte DB cluster. This involves changing the YugaByte DB container image to a different version first on the yb-master StatefulSet and then on the yb-tserver StatefulSet. As expected in StatefulSets, we can see that the nodes with the largest ordinal indexes are upgraded first.</p><p>Upgrading the yb-master StatefulSet uses the command below. Assuming the new container image is not already available with Kubernetes, the image will be pulled from hub.docker.com first and this may result in the first pod upgrade taking a few minutes.</p><p>Now we can upgrade the yb-tserver StatefulSet as well. This will lead to the yb-tserver pods getting upgraded in the same way we saw for the yb-master pods.</p><h2>Summary</h2><p>Running distributed databases using a distributed orchestration technology such as Kubernetes continues to remain a non-trivial problem. YugaByte DB is a distributed database with a unique sharding and replication architecture that makes it a perfect fit for Kubernetes-based orchestration. In this post, we reviewed the underlying details of how YugaByte DB runs on Kubernetes and how this looks in action in the context of a real cluster. As part of our upcoming 1.1 release, we expect to release additional Kubernetes-related enhancements such as running the YugaByte DB Enterprise Admin Console on the same Kubernetes cluster as YugaByte DB. Subscribe to our blog at the bottom of this page and stay tuned with our progress.</p><h2>What’s Next?</h2><ul><li>Read <a href=\"https://blog.yugabyte.com/orchestrating-stateful-apps-with-kubernetes-statefulsets/\">“Orchestrating Stateful Apps with Kubernetes.”</a></li>\n<li><a href=\"https://docs.yugabyte.com/latest/comparisons/\">Compare YugaByte DB to databases like Amazon DynamoDB, Cassandra, MongoDB and Azure Cosmos DB.</a></li>\n<li><a href=\"https://docs.yugabyte.com/latest/quick-start/\">Get started with YugaByte DB on Kubernetes.</a></li>\n<li><a href=\"https://www.yugabyte.com/about/contact/\">Contact us</a> to learn more about licensing, pricing or to schedule a technical overview.</li>\n</ul>"}}]}},"pageContext":{"alternative_id":12461}}