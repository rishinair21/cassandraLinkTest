{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Getting started with the Kafka Connect Cassandra Source","alternative_id":9723,"content":"<p id=\"ebf2\" class=\"graf graf--p graf-after--h3\">This post will look at how to setup and tune the <a href=\"http://lenses.stream/connectors/source/cassandra.html\" data-href=\"http://lenses.stream/connectors/source/cassandra.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Cassandra Source connector</a> that is available from <a href=\"http://www.landoop.com/\" data-href=\"http://www.landoop.com/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Landoop</a>. The Cassandra Source connector is used to read data from a Cassandra table, writing the contents into a Kafka topic using only a configuration file. This enables data that has been saved to be easily turned into an event stream.</p><figure id=\"f902\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*Fi0Zp7l0lbj-y9QT.png\" data-width=\"720\" data-height=\"240\" data-action=\"zoom\" data-action-value=\"0*Fi0Zp7l0lbj-y9QT.png\" src=\"https://cdn-images-1.medium.com/max/1600/0*Fi0Zp7l0lbj-y9QT.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\">all logos are trademark of Apache Foundation</figcaption></div></figure><p id=\"85be\" class=\"graf graf--p graf-after--figure\">In our example we will be capturing data representing a pack (i.e. a large box) of items being shipped. Each pack is pushed to consumers in a JSON format on a Kafka topic.</p><h3 id=\"d7da\" class=\"graf graf--h3 graf-after--p\">The Cassandra data model and Cassandra Source connector</h3><p id=\"10c8\" class=\"graf graf--p graf-after--h3\">Modeling data in Cassandra must be done around the queries that are needed to access the data (see <a href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\" data-href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">this article</a> for details). Typically this means that there will be one table for each query and data (in our case about the pack) will be duplicated across numerous tables.</p><p id=\"afac\" class=\"graf graf--p graf-after--p\">Regardless of the other tables used for the product, the Cassandra Source connector needs a table that will allow us to query for data using a time range. The connector is designed around its ability to generate a CQL query based on configuration. It uses this query to retrieve data from the table that is available within a configurable time range. Once all of this data has been published, Kafka Connect will mark the upper end of the time range as an offset. The connector will then query the table for more data using the next time range starting with the date/time stored in the offset. We will look at how to configure this later. For now we want to focus on the constraints for the table. Since Cassandra doesn’t support joins, the table we are pulling data from must have all of the data that we want to put onto the Kafka topic. Data in other tables will not be available to Kafka Connect.</p><p id=\"361f\" class=\"graf graf--p graf-after--p\">In it’s simplest form a table used by the Cassandra Source connector might look like this:</p><pre id=\"e45f\" class=\"graf graf--pre graf-after--p\">CREATE TABLE IF NOT EXISTS “pack_events” (<br />event_id TEXT, <br />event_ts TIMESTAMP, <br />event_data TEXT, <br />PRIMARY KEY ((event_id),event_ts));</pre><p id=\"8ca8\" class=\"graf graf--p graf-after--pre\">The <code class=\"markup--code markup--p-code\">event_id</code> is the partition key. This is used by Cassandra to determine which nodes in the cluster will store the data. The <code class=\"markup--code markup--p-code\">event_ts</code> is part of the cluster key. It determines the order of the data within the partition (see <a href=\"https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key\" data-href=\"https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">this article for details</a>). It is also the column that is used by the Cassandra source connector to manage time ranges. In this example, the <code class=\"markup--code markup--p-code\">event_data</code> column stores the JSON representation of the pack.</p><p id=\"92bc\" class=\"graf graf--p graf-after--p\">This is not the only structure for a table that will work. The table that is queried by the Cassandra Source connector can use numerous columns to represent the partition key and the data. However, <strong class=\"markup--strong markup--p-strong\">the connector requires a single time based column</strong> (either <code class=\"markup--code markup--p-code\">TIMESTAMP</code> or <code class=\"markup--code markup--p-code\">TIMEUUID</code>) in order to work correctly.</p><p id=\"10c3\" class=\"graf graf--p graf-after--p\">This would be an equally valid table for use with the Cassandra Source connector.</p><pre id=\"3608\" class=\"graf graf--pre graf-after--p\">CREATE TABLE IF NOT EXISTS “kc_events” (<br />event_id1 TEXT, <br />event_id2 TEXT, <br />event_ts TIMEUUID, <br />event_data1 TEXT, <br />event_data2 TEXT, <br />PRIMARY KEY ((event_id1, event_id2)));</pre><p id=\"6eb6\" class=\"graf graf--p graf-after--pre\">The most efficient way to access data in this table is to query for data with the partition key. This would allow Cassandra to quickly identify the node containing the data we are interested in.</p><pre id=\"1c5f\" class=\"graf graf--pre graf-after--p\">SELECT * FROM pack_events WHERE event_id = “1234”;</pre><p id=\"9ca8\" class=\"graf graf--p graf-after--pre\">However, the Cassandra Source connector has no way of knowing the ids of the data that it will need to publish to a Kafka topic. That is why it uses a time range.</p><p id=\"89b1\" class=\"graf graf--p graf-after--p\">The reason we can’t use the <code class=\"markup--code markup--p-code\">event_ts</code> as the partition key is because Cassandra does not support these operators (&gt;, &gt;=, &lt;=, &lt;) on the partition key when querying. And without these we would not be able to query across date/time ranges (see <a href=\"https://www.datastax.com/dev/blog/a-deep-look-to-the-cql-where-clause\" data-href=\"https://www.datastax.com/dev/blog/a-deep-look-to-the-cql-where-clause\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">this article for details</a>).</p><p id=\"88ff\" class=\"graf graf--p graf-after--p\">There’s just one more thing. If we tried to run the following query it would fail.</p><pre id=\"a243\" class=\"graf graf--pre graf-after--p\">SELECT * FROM pack_events <br />WHERE event_ts &gt; ‘2018–01–22T20:28:20.869Z’ <br />AND event_ts &lt;= '2018-01-22T20:28:50.869Z';</pre><p id=\"25fc\" class=\"graf graf--p graf-after--pre\">The connector must supply the <code class=\"markup--code markup--p-code\">ALLOW FILTERING</code> option to the end of this query for it to work. This addition allows Cassandra to search all of the nodes in the cluster for the data in the specified time range (see<a href=\"https://www.datastax.com/dev/blog/allow-filtering-explained-2\" data-href=\"https://www.datastax.com/dev/blog/allow-filtering-explained-2\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\"> this article for details</a>).</p><h3 id=\"3efd\" class=\"graf graf--h3 graf-after--p\">Configuring the connector: KCQL basics</h3><p id=\"0b92\" class=\"graf graf--p graf-after--h3\">The Landoop connectors are configured using Kafka Connect Query Language (KCQL). This provides a concise and consistent way to configure the connectors (at least the ones from Landoop). The KCQL and other basic properties are provided via a JSON formatted property file.</p><p id=\"f64d\" class=\"graf graf--p graf-after--p\">For the sake of this post, let’s create a file named <code class=\"markup--code markup--p-code\">connect-cassandra-source.json</code>.</p><pre id=\"a5e0\" class=\"graf graf--pre graf-after--p\">{ <br />“name”: “packs”, <br />“config”: { <br />“tasks.max”: “1”, <br />“connector.class”: … </pre><p id=\"46c8\" class=\"graf graf--p graf-after--pre\">The <code class=\"markup--code markup--p-code\">name</code> of the connector needs to be unique across all the connectors installed into Kafka Connect.</p><p id=\"bad2\" class=\"graf graf--p graf-after--p\">The <code class=\"markup--code markup--p-code\">connector.class</code> is used to specify which connector is being used.​​</p><ul class=\"postList\"><li id=\"036c\" class=\"graf graf--li graf-after--p\"><code class=\"markup--code markup--li-code\">com.datamountaineer.streamreactor.connect.cassandra.source.CassandraSourceConnector</code></li></ul><p id=\"ebc5\" class=\"graf graf--p graf-after--li\">The next set of configuration (shown below) is used to specify the information needed to connect to the Cassandra cluster and which keyspace to use.</p><ul class=\"postList\"><li id=\"f802\" class=\"graf graf--li graf-after--p\"><code class=\"markup--code markup--li-code\">​connect.cassandra.contact.points</code></li><li id=\"a421\" class=\"graf graf--li graf-after--li\"><code class=\"markup--code markup--li-code\">connect.cassandra.port</code></li><li id=\"1a09\" class=\"graf graf--li graf-after--li\"><code class=\"markup--code markup--li-code\">connect.cassandra.username</code></li><li id=\"978e\" class=\"graf graf--li graf-after--li\"><code class=\"markup--code markup--li-code\">connect.cassandra.password</code></li><li id=\"a2b0\" class=\"graf graf--li graf-after--li\"><code class=\"markup--code markup--li-code\">connect.cassandra.consistency.level</code></li><li id=\"f3e5\" class=\"graf graf--li graf-after--li\"><code class=\"markup--code markup--li-code\">connect.cassandra.key.space</code></li></ul><pre id=\"86de\" class=\"graf graf--pre graf-after--li\">{ <br />“name”: “packs”, <br />“config”: { <br />“tasks.max”: “1”, <br />“connector.class”: “com.datamountaineer.streamreactor.connect.cassandra.source.CassandraSourceConnector”, <br />“connect.cassandra.contact.points”: “localhost”,    <br />“connect.cassandra.port”: 9042, <br />“connect.cassandra.username”: “cassandra”,   <br />“connect.cassandra.password”: “cassandra”,<br />“connect.cassandra.consistency.level”: “LOCAL_ONE”,<br />“connect.cassandra.key.space”: “blog”, “connect.cassandra.import.mode”: “incremental”, <br />“connect.cassandra.kcql”: “INSERT INTO test_topic SELECT event_data, event_ts FROM pack_events IGNORE event_ts PK event_ts WITHUNWRAP INCREMENTALMODE=TIMESTAMP”, … <br />} <br />}</pre><p id=\"682e\" class=\"graf graf--p graf-after--pre\">There are two values for the <code class=\"markup--code markup--p-code\">connect.cassandra.import.mode</code>. Those are <code class=\"markup--code markup--p-code\">bulk</code> and <code class=\"markup--code markup--p-code\">incremental</code>. The <code class=\"markup--code markup--p-code\">bulk</code> option will query everything in the table <em class=\"markup--em markup--p-em\">every time</em> that the Kafka Connect polling occurs. We will set this to <code class=\"markup--code markup--p-code\">incremental</code>.</p><p id=\"7bdb\" class=\"graf graf--p graf-after--p\">The interesting part of the configuration is the <code class=\"markup--code markup--p-code\">connect.cassandra.kcql</code> property (shown above). The KCQL statement tells the connector which table in the Cassandra cluster to use, how to use the columns on the table, and where to publish the data.</p><p id=\"0cb8\" class=\"graf graf--p graf-after--p\">The first part of the KCQL statement tells the connector the name of the Kafka topic where the data will be published. In our case that is the topic named <code class=\"markup--code markup--p-code\">test_topic</code>.</p><pre id=\"7d9c\" class=\"graf graf--pre graf-after--p\">INSERT INTO test_topic</pre><p id=\"4e45\" class=\"graf graf--p graf-after--pre\">The next part of the KCQL statement tells the connector how to deal with the table. The <code class=\"markup--code markup--p-code\">SELECT/FROM</code> specifies the table to poll with the queries. It also specifies the columns whose values should be retrieved. The column that keeps track of the date/time must be part of the <code class=\"markup--code markup--p-code\">SELECT</code>statement. However, if we don't want that data as part of what we publish to the Kafka topic we can use the <code class=\"markup--code markup--p-code\">IGNORE.</code></p><pre id=\"a537\" class=\"graf graf--pre graf-after--p\">SELECT event_data, event_ts FROM pack_events IGNORE event_ts</pre><p id=\"af45\" class=\"graf graf--p graf-after--pre\">The next part of the statement, the <code class=\"markup--code markup--p-code\">PK</code>, tells the connector which of the columns is used to manage the date/time. This is considered the primary key for the connector.</p><pre id=\"ba12\" class=\"graf graf--pre graf-after--p\">PK event_ts WITHUNWRAP INCREMENTALMODE=”TIMESTAMP”</pre><p id=\"3009\" class=\"graf graf--p graf-after--pre\">The <code class=\"markup--code markup--p-code\">INCREMENTALMODE</code> tells the connector what the data type of the <code class=\"markup--code markup--p-code\">PK</code> column is. That is going to be either <code class=\"markup--code markup--p-code\">TIMESTAMP</code> or <code class=\"markup--code markup--p-code\">TIMEUUID</code>.</p><p id=\"43cc\" class=\"graf graf--p graf-after--p\">Finally, the <code class=\"markup--code markup--p-code\">WITHUNWRAP</code> option tells the connector to publish the data to the topic as a String rather than as a JSON object.</p><p id=\"cf66\" class=\"graf graf--p graf-after--p\">For example, if we had the following value in the <code class=\"markup--code markup--p-code\">event_data</code> column:</p><pre id=\"69e1\" class=\"graf graf--pre graf-after--p\">{ “foo”:”bar” }</pre><p id=\"e638\" class=\"graf graf--p graf-after--pre\">We would want to publish this as seen above.</p><p id=\"462c\" class=\"graf graf--p graf-after--p\">Leaving the <code class=\"markup--code markup--p-code\">WITHUNWRAP</code> option off will result in the following value being published to the topic.</p><pre id=\"4821\" class=\"graf graf--pre graf-after--p\">{ <br />“schema”: {<br />“type”: “struct”, <br />“fields”: [{ <br />“type”: “string”,<br />“optional”: true,<br />“field”: “event_data” <br />}],<br />“optional”: false, <br />“name”: “blog.pack_events” <br />}, <br />“payload”: { <br />“event_data”: “{\\”foo\\”:\\”bar\\”}” <br />} <br />}</pre><p id=\"b0d5\" class=\"graf graf--p graf-after--pre\">If we leave <code class=\"markup--code markup--p-code\">WITHUNWRAP</code> off, when using the <code class=\"markup--code markup--p-code\">StringConverter</code> (more on that later) we would get the following:</p><pre id=\"b7c7\" class=\"graf graf--pre graf-after--p\">Struct:{event_data={“foo”:”bar\"}}</pre><p id=\"8d85\" class=\"graf graf--p graf-after--pre\">We will need to use the combination of <code class=\"markup--code markup--p-code\">WITHUNWRAP</code> and the<code class=\"markup--code markup--p-code\">StringConverter</code> to get the result we want.</p><h3 id=\"38f1\" class=\"graf graf--h3 graf-after--p\">Configuring the connector: Tuning Parameters</h3><p id=\"c7f8\" class=\"graf graf--p graf-after--h3\">We’ll explore these in another post. But for now let’s start looking for data in our table with a starting date/time of today. We’ll also poll every second.</p><pre id=\"a759\" class=\"graf graf--pre graf-after--p\">{ <br />“name”: “packs”, <br />“config”: { <br />“tasks.max”: “1”,<br />… <br />“connect.cassandra.initial.offset”: “2018–01–22 00:00:00.0000000Z”, <br />“connect.cassandra.import.poll.interval”: 1000 <br />} <br />}</pre><h3 id=\"070d\" class=\"graf graf--h3 graf-after--pre\">Setting up the infrastructure</h3><p id=\"3e92\" class=\"graf graf--p graf-after--h3\">We will be using the following products:</p><ul class=\"postList\"><li id=\"20b6\" class=\"graf graf--li graf-after--p\">Apache Cassandra 3.11.1</li><li id=\"a854\" class=\"graf graf--li graf-after--li\">Apache Kafka and Kafka Connect 1.0</li><li id=\"0b78\" class=\"graf graf--li graf-after--li\">Landoop Cassandra Source 1.0</li></ul><h3 id=\"cd3f\" class=\"graf graf--h3 graf-after--li\">Installing Cassandra</h3><p id=\"15df\" class=\"graf graf--p graf-after--h3\">Installation instructions for Apache Cassandra can be found on the web (<a href=\"https://cassandra.apache.org/doc/latest/getting_started/installing.html\" data-href=\"https://cassandra.apache.org/doc/latest/getting_started/installing.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">link</a>). Once installed and started the cluster can be verified using the following command:</p><pre id=\"3564\" class=\"graf graf--pre graf-after--p\">nodetool -h [IP] status</pre><p id=\"f19b\" class=\"graf graf--p graf-after--pre\">this will generate a response as follows:</p><pre id=\"6a27\" class=\"graf graf--pre graf-after--p\">Datacenter: dc1<br />===============<br />Status=Up/Down<br />|/ State=Normal/Leaving/Joining/Moving<br />--  Address   Load       Tokens       Owns (effective)  Host ID   Rack<br />UN  10.x.x.x  96.13 GiB   64           39.6%            [UUID]    r6<br />UN  10.x.x.x  148.98 GiB  64           33.6%            [UUID]    r5<br />UN  10.x.x.x  88.08 GiB   64           36.4%            [UUID]    r5<br />UN  10.x.x.x  97.96 GiB   64           30.4%            [UUID]    r6<br />UN  10.x.x.x  146.89 GiB  64           33.2%            [UUID]    r7<br />UN  10.x.x.x  205.24 GiB  64           36.8%            [UUID]    r7</pre><h3 id=\"b219\" class=\"graf graf--h3 graf-after--pre\">Installing Kafka and Kafka Connect</h3><p id=\"c5fe\" class=\"graf graf--p graf-after--h3\">Kafka Connect is shipped and installed as part of Apache Kafka. Instructions for these are also available on the web (<a href=\"https://kafka.apache.org/quickstart\" data-href=\"https://kafka.apache.org/quickstart\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">link</a>).</p><ol class=\"postList\"><li id=\"1fbd\" class=\"graf graf--li graf-after--p\">Download the tar file (<a href=\"https://kafka.apache.org/downloads\" data-href=\"https://kafka.apache.org/downloads\" class=\"markup--anchor markup--li-anchor\" rel=\"nofollow noopener\" target=\"_blank\">link</a>).</li><li id=\"85ed\" class=\"graf graf--li graf-after--li\">Install the tar file</li></ol><pre id=\"f250\" class=\"graf graf--pre graf-after--li\">tar -xzf kafka_2.11–1.0.0.tgz <br />cd kafka_2.11–1.0.0</pre><h3 id=\"0bdd\" class=\"graf graf--h3 graf-after--pre\">Starting Kafka</h3><p id=\"ff0d\" class=\"graf graf--p graf-after--h3\">This post will not attempt to explain the architecture behind a Kafka cluster. However, a typical installation will have several Kafka brokers and Apache Zookeeper.</p><figure id=\"8095\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*HoKUvzx_Q25-P5Rq.png\" data-width=\"439\" data-height=\"230\" src=\"https://cdn-images-1.medium.com/max/1600/0*HoKUvzx_Q25-P5Rq.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\">all logos are trademark of Apache Foundation</figcaption></div></figure><p id=\"846b\" class=\"graf graf--p graf-after--figure\">To run Kafka, first start Zookeeper, then start the Kafka brokers. The commands below assume a local installation with only one node.</p><pre id=\"eed8\" class=\"graf graf--pre graf-after--p\">bin/zookeeper-server-start.sh config/zookeeper.properties</pre><p id=\"77cd\" class=\"graf graf--p graf-after--pre\">and</p><pre id=\"ded3\" class=\"graf graf--pre graf-after--p\">bin/kafka-server-start.sh config/server.properties</pre><p id=\"7528\" class=\"graf graf--p graf-after--pre\">Once we have Kafka installed and running, we need to create four topics. One is used by our application to publish our pack JSON. The other three are required by Kafka Connect. We will continue to assume that most are running this initially on a laptop so we will set the replication factor to 1.</p><pre id=\"541c\" class=\"graf graf--pre graf-after--p\">bin/kafka-topics.sh — create — topic test_topic -zookeeper localhost:2181 — replication-factor 1 — partitions 3</pre><p id=\"1251\" class=\"graf graf--p graf-after--pre\">and</p><pre id=\"054b\" class=\"graf graf--pre graf-after--p\">bin/kafka-topics.sh — create — zookeeper localhost:2181 — topic connect-configs — replication-factor 1 — partitions 1 — config cleanup.policy=compact </pre><pre id=\"43f4\" class=\"graf graf--pre graf-after--pre\">bin/kafka-topics.sh — create — zookeeper localhost:2181 — topic connect-offsets — replication-factor 1 — partitions 50 — config cleanup.policy=compact </pre><pre id=\"ad4c\" class=\"graf graf--pre graf-after--pre\">bin/kafka-topics.sh — create — zookeeper localhost:2181 — topic connect-status — replication-factor 1 — partitions 10 — config cleanup.policy=compact</pre><p id=\"1461\" class=\"graf graf--p graf-after--pre\">In order to verify that the four topics have been created, run the following command:</p><pre id=\"f0fe\" class=\"graf graf--pre graf-after--p\">bin/kafka-topics.sh — list — zookeeper localhost:2181</pre><h3 id=\"e7e9\" class=\"graf graf--h3 graf-after--pre\">Installing the Cassandra Source connector</h3><p id=\"9f7c\" class=\"graf graf--p graf-after--h3\">Landoop offers numerous connectors for Kafka Connect. These are all available as open source. The first thing we need to do is download the Cassandra Source connector jar file (<a href=\"https://github.com/Landoop/stream-reactor/releases\" data-href=\"https://github.com/Landoop/stream-reactor/releases\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">link</a>).</p><ul class=\"postList\"><li id=\"9df6\" class=\"graf graf--li graf-after--p\">kafka-connect-cassandra-1.0.0–1.0.0-all.tar.gz</li></ul><p id=\"c2c7\" class=\"graf graf--p graf-after--li\">Unzip the tar file and copy the jar file to the <code class=\"markup--code markup--p-code\">libs</code> folder under the Kafka install directory.</p><h3 id=\"0a03\" class=\"graf graf--h3 graf-after--p\">Configuring Kafka Connect</h3><p id=\"f0d8\" class=\"graf graf--p graf-after--h3\">We need to tell Kafka Connect where the Kafka cluster is. In the <code class=\"markup--code markup--p-code\">config</code> folder where Kafka was installed we will find the file: <code class=\"markup--code markup--p-code\">connect-distributed.properties.</code>Look for the <code class=\"markup--code markup--p-code\">bootstrap.servers</code> key. Update that to point to the cluster.</p><pre id=\"01dd\" class=\"graf graf--pre graf-after--p\">bootstrap.servers=localhost:9092</pre><h3 id=\"66ff\" class=\"graf graf--h3 graf-after--pre\">Starting Kafka Connect</h3><p id=\"18b5\" class=\"graf graf--p graf-after--h3\">We can now start up our distributed Kafka Connect service. For more information on stand-alone vs distributed mode, see the documentation (<a href=\"https://docs.confluent.io/current/connect/userguide.html\" data-href=\"https://docs.confluent.io/current/connect/userguide.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">link</a>).</p><pre id=\"62ce\" class=\"graf graf--pre graf-after--p\">bin/connect-distributed.sh config/connect-distributed.properties</pre><p id=\"24bf\" class=\"graf graf--p graf-after--pre\">If all has gone well you should see the following on your console:</p><figure id=\"867a\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*M6D9vzHCNtCScBNN.png\" data-width=\"817\" data-height=\"260\" data-action=\"zoom\" data-action-value=\"0*M6D9vzHCNtCScBNN.png\" src=\"https://cdn-images-1.medium.com/max/1600/0*M6D9vzHCNtCScBNN.png\" alt=\"image\" /></div></div></figure><p id=\"627d\" class=\"graf graf--p graf-after--figure\">In case you are wondering , “Data Mountaineer”, was the name of the company before being renamed to Landoop.</p><h3 id=\"fb57\" class=\"graf graf--h3 graf-after--p\">Adding the Cassandra Source connector</h3><p id=\"cfca\" class=\"graf graf--p graf-after--h3\">Kafka Connect has a REST API to interact with connectors (<a href=\"https://docs.confluent.io/current/connect/restapi.html\" data-href=\"https://docs.confluent.io/current/connect/restapi.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">check this out for details</a> on the API). We need to add the Cassandra Source connector to the Kafka Connect. This is done by sending the property file (<code class=\"markup--code markup--p-code\">connect-cassandra-source.json</code>) to Kafka Connect through the REST API.</p><pre id=\"bb81\" class=\"graf graf--pre graf-after--p\">curl -X POST -H “Content-Type: application/json” -d @connect-cassandra-source.json localhost:8083/connectors</pre><p id=\"47f0\" class=\"graf graf--p graf-after--pre\">Once we have successfully loaded the connector, we can check to see the installed connectors using this API:</p><pre id=\"6cc1\" class=\"graf graf--pre graf-after--p\">curl localhost:8083/connectors</pre><p id=\"dacd\" class=\"graf graf--p graf-after--pre\">That should return a list of the connectors by their configured names.</p><pre id=\"b394\" class=\"graf graf--pre graf-after--p\">[“packs”]</pre><h3 id=\"4ce8\" class=\"graf graf--h3 graf-after--pre\">Testing the Cassandra Source connector</h3><p id=\"2758\" class=\"graf graf--p graf-after--h3\">In order to test everything out we will need to insert some data into our table.</p><pre id=\"3d44\" class=\"graf graf--pre graf-after--p\">INSERT INTO pack_events (event_id, event_ts, event_data) <br />VALUES (‘500’, ‘2018–01–22T20:28:50.869Z’, ‘{“foo”:”bar”}’);</pre><p id=\"5f22\" class=\"graf graf--p graf-after--pre\">We can check what is being written to the Kafka topic by running the following command:</p><pre id=\"7fa3\" class=\"graf graf--pre graf-after--p\">bin/kafka-console-consumer.sh — bootstrap-server localhost:9092 — topic test_topic</pre><p id=\"b4a1\" class=\"graf graf--p graf-after--pre\">At this point, we might be surprised to see something like this:</p><pre id=\"eaa5\" class=\"graf graf--pre graf-after--p\">{ <br />“schema”:{ <br />“type”:”string”, <br />“optional”:false <br />}, <br />“payload”:”{\\”foo\\”:\\”bar\\”}” <br />}</pre><p id=\"361b\" class=\"graf graf--p graf-after--pre\">That is better than what we were getting without <code class=\"markup--code markup--p-code\">WITHUNWRAP</code> but isn't exactly what we were hoping for. To get the JSON value that was written to the table column we need to update the <code class=\"markup--code markup--p-code\">connect-distributed.properties</code>file. Open this up and look for <code class=\"markup--code markup--p-code\">JsonConverter</code>. Replace those lines with the following:</p><pre id=\"e984\" class=\"graf graf--pre graf-after--p\">key.converter=org.apache.kafka.connect.storage.StringConverter value.converter=org.apache.kafka.connect.storage.StringConverter</pre><p id=\"b8d7\" class=\"graf graf--p graf-after--pre\">Restart Kafka Connect.<br /> Insert another row into the table.<br /> Now we should get what we want.</p><pre id=\"012c\" class=\"graf graf--pre graf-after--p\">{ “foo”:”bar” }</pre><p id=\"08e6\" class=\"graf graf--p graf-after--pre\">Happy coding!</p><p id=\"3cbb\" class=\"graf graf--p graf-after--p graf--trailing\">This originally appeared on TheAgileJedi blog (<a href=\"https://theagilejedi.wordpress.com/2018/01/23/using-the-kafka-connect-cassandra-source-part-1/\" data-href=\"https://theagilejedi.wordpress.com/2018/01/23/using-the-kafka-connect-cassandra-source-part-1/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">here</a>)</p>"}}]}},"pageContext":{"alternative_id":9723}}