{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"spotify/cstar","alternative_id":12264,"content":"<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p><a href=\"https://circleci.com/gh/spotify/cstar\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/261a421941d35a414a19dac873482ed1b7e8b7ca/68747470733a2f2f636972636c6563692e636f6d2f67682f73706f746966792f63737461722f747265652f6d61737465722e7376673f7374796c653d736869656c64\" alt=\"CircleCI\" data-canonical-src=\"https://circleci.com/gh/spotify/cstar/tree/master.svg?style=shield\" /></a>\n<a href=\"https://github.com/spotify/cstar/blob/master/LICENSE\"><img src=\"https://camo.githubusercontent.com/38ba649f1d42cc25c982fe6511d30c6dc8a6a2a3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f73706f746966792f63737461722e737667\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/spotify/cstar.svg\" /></a></p>\n<p><code>cstar</code> is an Apache Cassandra cluster orchestration tool for the command line.</p>\n<p><a href=\"https://asciinema.org/a/BJkHpAGCdkSXTAhYf7bPVmerz?autoplay=1\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/abfd4b51f4c1180fcf22ef4c8f36c83110766359/68747470733a2f2f61736369696e656d612e6f72672f612f424a6b4870414743646b53585441685966376250566d65727a2e706e67\" alt=\"asciicast\" data-canonical-src=\"https://asciinema.org/a/BJkHpAGCdkSXTAhYf7bPVmerz.png\" /></a></p>\n<h2><a id=\"user-content-why-not-simply-use-ansible-or-fabric\" class=\"anchor\" aria-hidden=\"true\" href=\"#why-not-simply-use-ansible-or-fabric\"></a>Why not simply use Ansible or Fabric?</h2>\n<p>Ansible does not have the primitives required to run things in a topology aware fashion. One could\nsplit the C* cluster into groups that can be safely executed in parallel and run one group at a time.\nBut unless the job takes almost exactly the same amount of time to run on every host, such a solution\nwould run with a significantly lower rate of parallelism, not to mention it would be kludgy enough to\nbe unpleasant to work with.</p>\n<p>Unfortunately, Fabric is not thread safe, so the same type of limitations apply. Fabric allows one to\nrun a job in parallel on many machines, but with similar restrictions as those of Ansible groups.\nIt’s possibly to use fabric and celery together to do what is needed, but it’s a very complicated\nsolution.</p>\n<h2><a id=\"user-content-requirements\" class=\"anchor\" aria-hidden=\"true\" href=\"#requirements\"></a>Requirements</h2>\n<p>All involved machines are assumed to be some sort of UNIX-like system like OS X or Linux. The machine\nrunning cstar must have python3, the Cassandra hosts must have a Bourne style shell.</p>\n<h2><a id=\"user-content-installing\" class=\"anchor\" aria-hidden=\"true\" href=\"#installing\"></a>Installing</h2>\n<p>You need to have Python3 and run an updated version of pip (9.0.1).</p>\n<pre># pip3 install cstar\n</pre>\n<p>It's also possible to install straight from repo. This installs the latest version that may not be pushed to pypi:</p>\n<pre># pip install git+https://github.com/spotify/cstar.git\n</pre>\n<p>Some systems (like Ubuntu 14.04) might trigger ssh2-python related errors when installing because the locally available libssh2 is too old (&lt;1.6.0).\nIn such case, please apply the following procedure :</p>\n<pre>sudo apt-get install cmake libssl-dev libffi-dev python3-pip -y\ngit clone --recurse-submodules https://github.com/ParallelSSH/ssh2-python.git\ncd ssh2-python; sudo ./ci/install-ssh2.sh\nsudo pip3 install setuptools bcrypt --upgrade\nsudo pip3 install cstar --upgrade\n# or: \n# sudo pip3 install git+https://github.com/spotify/cstar.git --upgrade\n</pre>\n<p>This will build libssh2 from source using the one that ships with ssh2-python and install some required dependencies.</p>\n<h2><a id=\"user-content-code-of-conduct\" class=\"anchor\" aria-hidden=\"true\" href=\"#code-of-conduct\"></a>Code of conduct</h2>\n<p>This project adheres to the\n<a href=\"https://github.com/spotify/code-of-conduct/blob/master/code-of-conduct.md\">Open Code of Conduct</a>.\nBy participating, you are expected to honor this code.</p>\n<h2><a id=\"user-content-cli\" class=\"anchor\" aria-hidden=\"true\" href=\"#cli\"></a>CLI</h2>\n<p>CStar is run through the cstar command, like so</p>\n<pre># cstar COMMAND [HOST-SPEC] [PARAMETERS]\n</pre>\n<p>The HOST-SPEC specifies what nodes to run the script on. There are three ways to specify a the spec:</p>\n<ol><li>The <code>--seed-host</code> switch tells cstar to connect to a specific host and fetch the full ring topology\nfrom there, and then run the script on all nodes in the cluster. <code>--seed-host</code> can be specified\nmultiple times, and multiple hosts can be specified as a comma-separated list in order to run a\nscript across multiple clusters.</li>\n<li>The <code>--host</code> switch specifies an exact list of hosts to use. <code>--host</code> can be specified multiple\ntimes, and multiple hosts can be specified as a comma-separated list.</li>\n<li>The <code>--host-file</code> switch points to a file name containing a newline separated list of hosts. This\ncan be used together with process substitution, e.g. <code>--host-file &lt;(dig -t srv ...)</code></li>\n</ol><p>The command is the name of a script located in either <code>/usr/lib/cstar/commands</code> or in\n<code>~/.cstar/commands</code>. This script will be uploaded to all nodes in the cluster and executed. File suffixes\nare stripped. The requirements of the script are described below. Cstar comes pre-packaged with one script file\ncalled <code>run</code> which takes a single parameter <code>--command</code> - see examples below.</p>\n<p>Some additional switches to control cstar:</p>\n<ul><li>One can override the parallelism specified in a script by setting the switches\n<code>--cluster-parallelism</code>, <code>--dc-parallelism</code> and <code>--strategy</code>.</li>\n</ul><p>There are two special case invocations:</p>\n<ul><li>\n<p>One can skip the script name and instead use the <code>continue</code> command to specify a previously halted job\nto resume.</p>\n</li>\n<li>\n<p>One can skip the script name and instead use the <code>cleanup-jobs</code>. See <a href=\"#Cleaning-up-old-jobs\">Cleaning up old jobs</a>.</p>\n</li>\n<li>\n<p>Two python ssh modules can be used : <code>paramiko</code> (default) and <code>ssh2-python</code>. To use the faster (but experimental) ssh2-python module add the following flag : <code>--ssh-lib=ssh2</code></p>\n</li>\n<li>\n<p>If you need to access the remote cluster with a specific username, add <code>--ssh-username=remote_username</code> to your cstar command line. A private key file can also be specified using <code>--ssh-identity-file=my_key_file.pem</code>.</p>\n</li>\n<li>\n<p>To use plain text authentication, please add <code>--ssh-password=my_password</code> to the command line.</p>\n</li>\n<li>\n<p>In order to run the command first on a single node and then stop execution to verify everything worked as expected, add the following flag to your command line : <code>--stop-after=1</code>. cstar will stop after the first node executed the command and print out the appropriate resume command to continue the execution when ready : <code>cstar continue &lt;JOB_ID&gt;</code></p>\n</li>\n</ul><p>A script file can specify additional parameters.</p>\n<h2><a id=\"user-content-command-syntax\" class=\"anchor\" aria-hidden=\"true\" href=\"#command-syntax\"></a>Command syntax</h2>\n<p>In order to run a command, it is first uploaded to the relevant host, and then executed from there.</p>\n<p>Commands can be written in any scripting language in which the hash symbol starts a line comment, e.g.\nshell-script, python, perl or ruby.</p>\n<p>The first line must be a valid shebang. After that, commented lines containing key value pairs may\nbe used to override how the script is parallelised as well as providing additional parameters for\nthe script, e.g. <code># C* dc-parallel: true</code></p>\n<p>The possible keys are:</p>\n<p><code>cluster-parallelism</code>, can the script be run on multiple clusters in parallel. Default value is <code>true</code>.</p>\n<p><code>dc-parallelism</code>, can the script be run on multiple data centers in the same cluster in parallel. Default value is <code>false</code>.</p>\n<p><code>strategy</code>, how many nodes within one data center can the script be run on. Default is <code>topology</code>.\nCan be one of:</p>\n<ul><li><code>one</code>, only one node per data center</li>\n<li><code>topology</code>, inspect topology and run on as many nodes as the topology allows</li>\n<li><code>all</code>, can be run on all nodes at once</li>\n</ul><p><code>description</code>, specifies a description for the script used in the help message.</p>\n<p><code>argument</code>, specifies an additional input parameter for the script, as well as a help text and an\noptional default value.</p>\n<h2><a id=\"user-content-job-output\" class=\"anchor\" aria-hidden=\"true\" href=\"#job-output\"></a>Job output</h2>\n<p>Cstar automatically saves the job status to file during operation.</p>\n<p>Standard output, standard error and exit status of each command run against a Cassandra host is\nsaved locally on machine where cstar is running. They are available under the users home directory in\n<code>.cstar/jobs/JOB_ID/HOSTNAME</code></p>\n<h2><a id=\"user-content-how-jobs-are-run\" class=\"anchor\" aria-hidden=\"true\" href=\"#how-jobs-are-run\"></a>How jobs are run</h2>\n<p>When a new cstar job is created, it is assigned an id. (It's a UUID)</p>\n<p>Cstar stores intermediate job output in the directory\n<code>~/.cstar/remote_jobs/&lt;JOB_ID&gt;</code>. This directory contains files with the stdout, stderr and PID of the\nscript, and once it finishes, it will also contain a file with the exit status of the script.</p>\n<p>Once the job finishes, these files will be moved over to the original host and put in the directory <code>~/.cstar/jobs/&lt;JOB_ID&gt;/&lt;REMOTE_HOST_NAME&gt;</code>.</p>\n<p>Cstar jobs are run nohuped, this means that even if the ssh connection is severed, the job will proceed.\nIn order to kill a cstar script invocation on a specific host, you will need ssh to the host and kill\nthe proccess.</p>\n<p>If a job is halted half-way, either by pressing <code>^C</code> or by using the <code>--stop-after</code> parameter, it can be\nrestarted using <code>cstar continue &lt;JOB_ID&gt;</code>. If the script was finished or already running when cstar\nshut down, it will not be rerun.</p>\n<h2><a id=\"user-content-cleaning-up-old-jobs\" class=\"anchor\" aria-hidden=\"true\" href=\"#cleaning-up-old-jobs\"></a>Cleaning up old jobs</h2>\n<p>Even on successful completion, the output of a cstar job is not deleted. This means it's easy to check\nwhat the output of a script was after it completed. The downside of this is that you can get a lot of\ndata lying around in <code>~/.cstar/jobs</code>. In order to clean things up, you can use\n<code>cstar cleanup-jobs</code>. By default it will remove all jobs older than one week. You can override the\nmaximum age of a job before it's deleted by using the <code>--max-job-age</code> parameter.</p>\n<h2><a id=\"user-content-examples\" class=\"anchor\" aria-hidden=\"true\" href=\"#examples\"></a>Examples</h2>\n<pre># cstar run --command='service cassandra restart' --seed-host some-host\n</pre>\n<p>Explanation: Run the local cli command <code>service cassandra restart</code> on a cluster. If necessary, add <code>sudo</code> to the\ncommand.</p>\n<pre># cstar puppet-upgrade-cassandra --seed-host some-host --puppet-branch=cass-2.2-upgrade\n</pre>\n<p>Explanation: Run the command puppet-upgrade-cassandra on a cluster. The puppet-upgrade-cassandra\ncommand expects a parameter, the puppet branch to run in order to perform the Cassandra upgrade. See the\npuppet-upgrade-cassandra example <a href=\"#Example-script-file\">below</a>.</p>\n<pre># cstar puppet-upgrade-cassandra --help\n</pre>\n<p>Explanation: Show help for the puppet-upgrade-cassandra command. This includes documentation for any\nadditional command-specific switches for the puppet-upgrade-cassandra command.</p>\n<pre># cstar continue 90642c11-4714-44c4-a13a-94b86f09e3bb\n</pre>\n<p>Explanation: Resume previously created job with job id 90642c11-4714-44c4-a13a-94b86f09e3bb.\nThe job id is the first line written on any executed job.</p>\n<h2><a id=\"user-content-example-script-file\" class=\"anchor\" aria-hidden=\"true\" href=\"#example-script-file\"></a>Example script file</h2>\n<p>This is an example script file that would saved to <code>~/.cstar/commands/puppet-upgrade-cassandra.sh</code>. It upgrades a\nCassandra cluster by running puppet on a different branch, then restarting the node, then upgrading the sstables.</p>\n<pre># !/usr/bin/env bash\n# C* cluster-parallel: true                                                                                                                                                                                    \n# C* dc-parallel: true                                                                                                                                                                                         \n# C* strategy: topology                                                                                                                                                                                        \n# C* description: Upgrade one or more clusters by switching to a different puppet branch                                                                                                                       \n# C* argument: {\"option\":\"--snapshot-name\", \"name\":\"SNAPSHOT_NAME\", \"description\":\"Name of pre-upgrade snapshot\", \"default\":\"preupgrade\"}                                                                      \n# C* argument: {\"option\":\"--puppet-branch\", \"name\":\"PUPPET_BRANCH\", \"description\":\"Name of puppet branch to switch to\", \"required\":true}                                                                       \nnodetool snapshot -t $SNAPSHOT_NAME\nsudo puppet --branch $PUPPET_BRANCH\nsudo service cassandra restart\nnodetool upgradesstables\n</pre>\n</article>"}}]}},"pageContext":{"alternative_id":12264}}