{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Introduction to Spark & Cassandra â€” Rustyrazorblade","alternative_id":5172,"content":"<!--- title: DESTROY YOUR NEW YEARS RESOLUTIONS WITH SPARK AND CASSANDRA ---><p>I've been messing with <a href=\"https://spark.apache.org/\">Apache Spark</a> quite a bit lately.  If you aren't familiar, Spark is a general purpose engine for large scale data processing.  Initially it comes across as simply a replacement for Hadoop, but that would be selling it short.  Big time.  In addition to bulk processing (goodbye MapReduce!), Spark includes:</p><ul><li>SQL engine</li>\n<li>Stream processing via Kafka, Flume, ZeroMQ</li>\n<li>Machine Learning</li>\n<li>Graph Processing</li>\n</ul><p>Sounds awesome, right?   That's because it is, babaganoush.  The next question is where do we store our data?  Spark works with a number of projects, but my database of choice these days is <a href=\"http://cassandra.apache.org/\">Apache Cassandra</a>.  Easy scale out and always up.  It's approximately this epic:</p>\n<p><img alt=\"cat-riding-a-fire-breathing-unicorn\" src=\"http://rustyrazorblade.com/images/cat-riding-a-fire-breathing-unicorn.jpg\"/></p>\n<p>If you haven't used <a href=\"http://cassandra.apache.org/\">Apache Cassandra</a> yet, that's ok.  I've included some links at the end of the post to help you get started.</p>\n<p>We're going to set up a really simple Spark job that does a data migration for Cassandra.  We'll be copying 1 table to another, but changing the table structure.  Currently Spark / Cassandra jobs require the open source connector from DataStax.</p>\n<p>I'm going to be using <a href=\"https://www.jetbrains.com/idea/#community_edition\">IntelliJ</a>, (community version is free) and hard coding a few things that'll make it easy to test Spark jobs from IntelliJ itself.  In a future post I'll go over how to actually deploy spark into production.</p>\n<p>For convenience / reference, I've pushed the working code as a standalone project up to <a href=\"https://github.com/rustyrazorblade/spark-data-migration\">github</a>.</p>\n<p>You'll need to have a <a href=\"http://planetcassandra.org/try-cassandra/\">Cassandra server up and running locally</a> to execute the Spark job.</p>\n<p>Let's suppose we have a system with a users table.  In that table, we store a user's name, and their favorite food.  We want to be able to create a table that maps food to users.  For performance reasons, we want this in a single table.  At a CQL prompt, create the tables and add sample data:</p>\n<div class=\"highlight\"><pre>CREATE KEYSPACE tutorial WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};\nuse tutorial;\nCREATE TABLE tutorial.user (\nname text primary key,\nfavorite_food text\n);\ncreate table tutorial.food_to_user_index ( food text, user text, primary key (food, user));\ninsert into user (name, favorite_food) values ('Jon', 'bacon');\ninsert into user (name, favorite_food) values ('Luke', 'steak');\ninsert into user (name, favorite_food) values ('Al', 'salmon');\ninsert into user (name, favorite_food) values ('Chris', 'chicken');\ninsert into user (name, favorite_food) values ('Rebecca', 'bacon');\ninsert into user (name, favorite_food) values ('Patrick', 'brains');\ninsert into user (name, favorite_food) values ('Duy Hai', 'brains');\n</pre></div>\n<p>With your sample data all set up, go ahead and create your project.  Choose Scala -&gt; SBT.</p>\n<p><img alt=\"create_project\" src=\"http://rustyrazorblade.com/images/spark_intro_create_project.png\"/></p>\n<p>Make sure you pick Scala 2.10.4 as your Scala version.</p>\n<p><img alt=\"create_project2\" src=\"http://rustyrazorblade.com/images/spark_intro_create_project2.png\"/></p>\n<p>We're going to need to edit our SBT file to include the Spark and Cassandra dependencies.  Open up <code>build.sbt</code> and add the libraryDependencies of Spark and the Spark connector.  It should look like the following:</p>\n<div class=\"highlight\"><pre>name := \"intro_to_spark\"\nversion := \"1.0\"\nscalaVersion := \"2.10.4\"\nlibraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"1.2.0\"\nlibraryDependencies += \"com.datastax.spark\" %% \"spark-cassandra-connector\" %  \"1.1.0\" withSources() withJavadoc()\n</pre></div>\n<p>Right click and create a new Scala file under <code>src/main/scala</code>, and pick Object.  I called mine DataMigration.  Go ahead and open it up for editing.  For this example it'll contain all the code we need.</p>\n<p>The first thing that we're going to do is to import the SparkContext and SparkConf.  These classes are used to represent the Spark Cluster and set up configuration, respectively.</p>\n<div class=\"highlight\"><pre>import org.apache.spark.{SparkContext,SparkConf}\n</pre></div>\n<p>To talk to Apache Cassandra, we need to import the connector.  The connector adds functionality to the spark context - specifically the ability to create RDDs (Spark's term for a distributed dataset) from Cassandra tables.</p>\n<div class=\"highlight\"><pre>import com.datastax.spark.connector._\n</pre></div>\n<p>Next we're going to create our <code>object</code>.  An object in Scala is a singleton.  It's very useful for Spark jobs.  Our object has a <code>main</code> method:</p>\n<div class=\"highlight\"><pre>object DataMigration {\n  def main(args: Array[String]): Unit = {\n</pre></div>\n<p>Alright, time for some meat.  We're going to set up the SparkConf (the configuration) now.  We want to tell it where to find Cassandra.</p>\n<div class=\"highlight\"><pre>val conf = new SparkConf(true)\n  .set(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n</pre></div>\n<p>We create our Spark Context here by providing the Spark configuration.  The first parameter, <code>local</code>, is hard coded here to make development easier.  We're going to name the job \"test\" and provide the Spark Configuration we created earlier.</p>\n<div class=\"highlight\"><pre>val sc = new SparkContext(\"local\", \"test\", conf)\n</pre></div>\n<p>We need to define a case class to represent a Cassandra row.  This comes in handy later.</p>\n<div class=\"highlight\"><pre>case class FoodToUserIndex(food: String, user: String)\n</pre></div>\n<p>Next, we get a reference to our user table (cleverly named user_table) through the context.  The <code>.cassandraTable</code> method takes a keyspace and a table name.  We get back an RDD, which we can perform operations on, like mapping, and filtering.</p>\n<div class=\"highlight\"><pre>val user_table = sc.cassandraTable(\"tutorial\", \"user\")\n</pre></div>\n<p>This line's a little tricky.  What we're doing is saying, hey, <code>user_table</code>!  Let's perform an operation on every row you've got!  The map takes a function; the function will be applied to every row in the table.  Each row in the table will be \"r\" in the below example.  For each row, we're going to create a new FoodToUserIndex case class.  The parameters of the case class are the <code>favorite_food</code> and <code>name</code> from the user row.  We're going to store this in the <code>food_index</code> immutable variable.</p>\n<div class=\"highlight\"><pre>val food_index = user_table.map(r =&gt; new FoodToUserIndex(r.getString(\"favorite_food\"), r.getString(\"name\")))\n</pre></div>\n<p>Functional programming can be a bit tricky.  Don't give up!  Break it down to small pieces, and then put it all together.</p>\n<p>Here's where the data gets written to the new table.  We can take an RDD, and save it to Cassandra with... <code>saveToCassandra(keyspace, table_name)</code>.  That's actually the end of the code.</p>\n<div class=\"highlight\"><pre>    food_index.saveToCassandra(\"tutorial\", \"food_to_user_index\")\n  }\n}\n</pre></div>\n<p>OK, time to run the Spark job.  If you didn't create your <code>tutorial</code> keyspace, tables, and sample data already, you'll need to do it now or the job will fail miserably.  To run the job right in IntelliJ, right click, and \"run\".</p>\n<p><img alt=\"intro_to_spark_run\" src=\"http://rustyrazorblade.com/images/intro_to_spark_run.png\"/></p>\n<p>Assuming the job completes successfully, you should now be able to select data out of your table in Cassandra:</p>\n<div class=\"highlight\"><pre>cqlsh:tutorial&gt; select * from food_to_user_index ;\n food    | user\n---------+---------\n  salmon |      Al\n   steak |    Luke\n chicken |   Chris\n  brains | Duy Hai\n  brains | Patrick\n   bacon |     Jon\n   bacon | Rebecca\n(7 rows)\ncqlsh:tutorial&gt; select * from food_to_user_index where food = 'bacon';\n food  | user\n-------+---------\n bacon |     Jon\n bacon | Rebecca\n(2 rows)\n</pre></div>\n<p>Guess what?  You just ran your first Spark job talking to Cassandra.  The good news is, you can do a ton more than just simple <code>map()</code> calls.  I'll be posting followups on using Spark streaming and machine learning in the near future.</p>\n<p>The code in all it's glory can be found in my <a href=\"https://github.com/rustyrazorblade/spark-data-migration/blob/master/src/main/scala/DataMigration.scala\">GitHub</a>.  Please pull and try it for yourself.  If you don't want to use my IDE of choice, the build tools on the command line are perfectly fine.  Simply do the following:</p>\n<div class=\"highlight\"><pre>sbt compile\nsbt run\n</pre></div>\n<p>Well, that about does it.  I hope you've found this useful.  If you're interested in further learning materials:</p>\n<ul><li><a href=\"https://www.youtube.com/watch?v=W45Ysb9b6oE\">Cassandra Crash Course</a></li>\n<li><a href=\"https://spark.apache.org/docs/latest/\">Apache Spark docs</a></li>\n<li><a href=\"http://planetcassandra.org/try-cassandra/\">10 minute walkthough</a></li>\n</ul>"}}]}},"pageContext":{"alternative_id":5172}}