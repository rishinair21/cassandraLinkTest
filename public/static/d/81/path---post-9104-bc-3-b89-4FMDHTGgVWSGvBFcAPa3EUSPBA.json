{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Real-Time Replication from MySQL to Cassandra","alternative_id":9104,"content":"<p>Earlier this month I blogged about our new Hadoop applier, I published the docs for that this week (<a href=\"http://docs.continuent.com/tungsten-replicator-3.0/deployment-hadoop.html\">http://docs.continuent.com/tungsten-replicator-3.0/deployment-hadoop.html</a>) as part of the Tungsten Replicator 3.0 documentation (<a href=\"http://docs.continuent.com/tungsten-replicator-3.0/index.html\">http://docs.continuent.com/tungsten-replicator-3.0/index.html</a>). It contains some additional interesting nuggets that will appear in future blog posts.</p>\n<p>The main part of that functionality that performs the actual applier for Hadoop is based around a JavaScript applier engine – there will eventually be docs for that as part of the Batch Applier content (<a href=\"http://docs.continuent.com/tungsten-replicator-3.0/deployment-batchloading.html\">http://docs.continuent.com/tungsten-replicator-3.0/deployment-batchloading.html</a>). The core of this system is that it    takes the information from the data stream of the THL and the CSV file that was written by the batch applier system, and runs the commands necessary to load it into Hadoop and perform any necessary merges.</p>\n<p>I wanted to see how easy it would be to use the same system to use that same flexible system and bend it to another database system, in my case, I chose Cassandra.</p>\n<p>For the record, it took me a couple of hours to have this working, and I’m guessing another hour will file down some of the rough edges.</p>\n<p>Cassandra is interesting as a database because it mixes a big distributed key/value store with a close-enough to SQL like interface in the form of CQL. And that means we can make use of the CQL to help us perform the merging into the final tables in a manner not dissimilar to the method we use for loading into Vertica.</p>\n<p>Back to the Javascript batch loader, the applier provides five different implementable functions (all are technically optional) that you can use at different stages of the applier process. These are:</p>\n<ul><li>prepare() – called once when the applier goes online and can be used to create temporary directories or spaces</li>\n<li>begin() – called at the start of each transaction</li>\n<li>apply() – called at the end of the transaction once the data file has been written, but before the commit</li>\n<li>commit() – called after each transaction commit has taken place; this where we can consolidate info.</li>\n<li>release() – called when the applier goes offline</li>\n</ul><p>We can actually align these functions with a typical transaction – prepare() happens before the statements even start, begin() is the same as BEGIN, apply() happens immediately before COMMIT and commit() happens just after. release() can be used to do any clean up afterwards.</p>\n<p>So let’s put this into practice and use it for Cassandra.</p>\n<p>The basic process for loading is as follows:</p>\n<ol><li>Write a CSV file to load into Cassandra</li>\n<li>Load the CSV file into a staging table within Cassandra; this is easy through CQL using the ‘COPY tablename FROM filename’ CQL statement.</li>\n<li>Merge the staging table data with a live table to create a carbon copy of our MySQL table content.</li>\n</ol><p>For the loading portion, what we’ll do is load the CSV into a staging table, and then we’ll merge the staging table and live table data together during the commit stage of our batch applier. We’ll return to this in more detail.</p>\n<p>For the merging, we’ll take the information from the staging table, which includes the sequence number and operation type, and then write the ‘latest’ version of that row and put it into the live table. That gives us a structure like this:</p>\n<p><a href=\"https://mcslp.files.wordpress.com/2014/02/cassandra-loader.png\"><img data-attachment-id=\"10003\" data-permalink=\"https://mcbguru.blog/2014/02/27/real-time-replication-from-mysql-to-cassandra/cassandra-loader/\" data-orig-file=\"https://mcslp.files.wordpress.com/2014/02/cassandra-loader.png\" data-orig-size=\"2441,1812\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}\" data-image-title=\"Cassandra Loader\" data-image-description=\"\" data-medium-file=\"https://mcslp.files.wordpress.com/2014/02/cassandra-loader.png?w=300&amp;h=222\" data-large-file=\"https://mcslp.files.wordpress.com/2014/02/cassandra-loader.png?w=810\" class=\"alignnone size-medium wp-image-10003\" alt=\"Cassandra Loader\" src=\"https://mcslp.files.wordpress.com/2014/02/cassandra-loader.png?w=300&amp;h=222\" width=\"300\" height=\"222\" srcset=\"https://mcslp.files.wordpress.com/2014/02/cassandra-loader.png?w=300&amp;h=222 300w, https://mcslp.files.wordpress.com/2014/02/cassandra-loader.png?w=598&amp;h=444 598w, https://mcslp.files.wordpress.com/2014/02/cassandra-loader.png?w=150&amp;h=111 150w\" /></a></p>\n<p>Tungsten Replicator is going to manage this entire process for us – all we need to do ins install the replicators, plug in these custom bits, and let it run.</p>\n<p>As with the Hadoop applier, what we’re going to do is use the batch applier to generate only insert and delete rows; UPDATE statements will be converted into a delete of the original version and insert of the new version. So:</p>\n<pre><code>INSERT INTO sample VALUES (1,’Message’)</code></pre>\n<p>Is an insert…</p>\n<pre><code>DELETE sample WHERE id  = 1</code></pre>\n<p>Is a delete, and:</p>\n<pre><code>UPDATE sample SET message = ’Now you see me’ WHERE id = 1</code></pre>\n<p>is actually:</p>\n<pre><code>DELETE sample WHERE id  = 1&#13;\n INSERT INTO sample VALUES (1,’Now you see me’)</code></pre>\n<p>This gets round the problem of doing updates (which in big data stores are expensive, particularly Hadoop which doesn’t support updating existing data), into a more efficient delete and insert.</p>\n<p>In the CSV data itself, this is represented by prefix every row with three fields:</p>\n<pre><code>optype, sequence number, unique id</code></pre>\n<p>Optype is ‘D’ for a delete and ‘I’ for an insert and is used to identify what needs to be done. The sequence number is the unique transaction ID from the replicator THL. This number increases by one for every transaction, and this means we can always identify the ‘latest’ version of a row, which is important to us when processing the transaction into Cassandra. the unique ID is the primary key (or compound key) from the source data. We need this to ensure we update the right row. To replicate data in this way, we must have a primary key on the data. If you don’t have primary keys, you are probably in a world of hurt anyway, so it shouldn’t be a stretch.</p>\n<p>One difficulty here is that we need to cope with an idiosyncracy of Cassandra, which is that by default, Cassandra orders fields in the ‘tables’ (really collections of key/values) so that integers and numbers appear first in the table, and text appears last. This is an optimisation that Cassandra makes that complicates things for us, but only in a very small way. For the moment, we’ll handle it by assuming that we are loading only one table with a known format into Cassandra. We could handle multiple tables by using a simple IF statement in the JS and using different formats for that, or we could actually extract the info from the incoming data; I’m going to skip that because it keeps us away from the cool element of actually getting the data in.</p>\n<p>Within Cassandra then we have two tables, the table we are loading data into, and the staging table that we load the CSV data into. For our sample, the live schema is ‘sample’, the live table is ‘sample’ and the staging table is ‘staging_sample’.</p>\n<p>The definitions for these in Cassandra are for the sample live table:</p>\n<pre><code> CREATE TABLE sample (&#13;\n id int,&#13;\n message text,&#13;\n PRIMARY KEY (id)&#13;\n ) WITH&#13;\n bloom_filter_fp_chance=0.010000 AND&#13;\n caching='KEYS_ONLY' AND&#13;\n comment='' AND&#13;\n dclocal_read_repair_chance=0.000000 AND&#13;\n gc_grace_seconds=864000 AND&#13;\n index_interval=128 AND&#13;\n read_repair_chance=0.100000 AND&#13;\n replicate_on_write='true' AND&#13;\n populate_io_cache_on_flush='false' AND&#13;\n default_time_to_live=0 AND&#13;\n speculative_retry='99.0PERCENTILE' AND&#13;\n memtable_flush_period_in_ms=0 AND&#13;\n compaction={'class': 'SizeTieredCompactionStrategy'} AND&#13;\n compression={'sstable_compression': 'LZ4Compressor'};</code></pre>\n<p>And for the staging_sample table:</p>\n<pre><code>CREATE TABLE staging_sample (&#13;\n optype text,&#13;\n seqno int,&#13;\n fragno int,&#13;\n id int,&#13;\n message text,&#13;\n PRIMARY KEY (optype, seqno, fragno, id)&#13;\n ) WITH&#13;\n bloom_filter_fp_chance=0.010000 AND&#13;\n caching='KEYS_ONLY' AND&#13;\n comment='' AND&#13;\n dclocal_read_repair_chance=0.000000 AND&#13;\n gc_grace_seconds=864000 AND&#13;\n index_interval=128 AND&#13;\n read_repair_chance=0.100000 AND&#13;\n replicate_on_write='true' AND&#13;\n populate_io_cache_on_flush='false' AND&#13;\n default_time_to_live=0 AND&#13;\n speculative_retry='99.0PERCENTILE' AND&#13;\n memtable_flush_period_in_ms=0 AND&#13;\n compaction={'class': 'SizeTieredCompactionStrategy'} AND&#13;\n compression={'sstable_compression': 'LZ4Compressor'};</code></pre>\n<p>I’ve put both tables into a ‘sample’ collection.</p>\n<p>Remember that that idiosyncrasy I mentioned? Here it is, a bare table loading from CSV will actually order the data as:</p>\n<pre><code>seqno,uniqno,id,optype,message</code></pre>\n<p>This is Cassandra’s way of optimising integers over text to speed up lookups, but for us is a minor niggle. Right now, I’m going to handle it by assuming we are replicating only one schema/table and we we not what the structure of that looks like. Longer term, I want to pull it out of the metadata, but that’s a refinement.</p>\n<p>So let’s start by having a look at the basic JS loader script, it’s really the component that is going to handle the core element of the work, managing the CSV files that come in from the batch engine and applying them into Cassandra. Remember, there are five functions that we can define, but for the purposes of this demonstration we’re going to use only two of them, apply(), which will load the CSV file into Cassandra, and the commit() function, which will perform the steps to merge the stage data.</p>\n<p>The apply() function does two things, it identifies the table and schema, and then runs the command to load this data into Cassandra through the cqlsh command-line tool. We actually can’t run CQL directly from this command line, but I wrote a quick shell script that pipes CQL from the command-line into a running cqlsh.</p>\n<p>The commit() function on the other hand is simpler, although it does a much more complicated job using another external script, this time written in Ruby.</p>\n<p>So this gives us a cassandra.js script for the batch applier that looks like this:</p>\n<pre>function apply(csvinfo)&#13;\n{&#13;\n   sqlParams = csvinfo.getSqlParameters();&#13;\n   csv_file = sqlParams.get(\"%%CSV_FILE%%\");&#13;\n   schema = csvinfo.schema;&#13;\n   table = csvinfo.table;&#13;\n  runtime.exec(\"/opt/continuent/share/applycqlsh.sh \" + schema + ' \"copy staging_' + table + \" (optype,seqno,uniqno,id,message) from '\" + csv_file + \"';\\\"\");&#13;\n}&#13;\n&#13;\nfunction commit()&#13;\n{&#13;\n  runtime.exec(\"/opt/continuent/share/merge.rb \" + schema);&#13;\n}</pre>\n<p>So, the apply() function is called for each event as written into the THL from the MySQL binary log, and the content of the CSV file generated at that point contains the contents of the THL event; if it’s one row, it’s a one-row CSV file; if it’s a statement or transaction that created 2000 rows, it’s a 2000 row CSV file.</p>\n<p>The csvinfo object that is provided contains information about the batch file that is written, including, as you can see here, the schema and table names, and the sequence number. Note that we could, at this point, pull out table info, but we’re going to concentrate on pulling a single table here just for demo purposes.</p>\n<p>The CQL for loading the CSV data is:</p>\n<pre><code>COPY staging_tablename (optype,seqno,uniqno,id,message) from ‘FILENAME’;</code></pre>\n<p>This says, copy the the specific columns in this order from the file into the specified table.  As I mentioned, currently this is hard coded into the applier JS, but would be easy to handle for more complex schemas and structures.</p>\n<p>The commit() function is even simpler, because it just calls a script that will do the merging for us – we’ll get to that in a minute.</p>\n<p>So here’s the script that applies an arbitrary CQL statement into Cassandra:</p>\n<pre> #!/bin/bash&#13;\nSCHEMA=$1;shift&#13;\necho \"$*\" |cqlsh -k $SCHEMA tr-cassandra2</pre>\n<p>Really simple, but gets round a simple issue.</p>\n<p>The script that does the merge work is more complex; in other environments we might be able to do this all within SQL, but CQL is fairly limited with no sub-queries. So we do it long-hand using Ruby. The basic sequence is quite simple, and is in two phases:</p>\n<ol><li>Delete every row mentioned in the staging table with an optype of D with a matching unique key</li>\n<li>Insert the *last* version of an insert for each unique ID – the last version will be the latest one in the output. We can pick this out by just iterating over every insert and picking the one with the highest Sequence number as generated by the THL transaction ID.</li>\n<li>Delete the content from the staging table because we’ve finished with it. That empties the staging table ready for the next set of transactions.</li>\n</ol><p>That file looks like this:</p>\n<pre>#!/usr/bin/ruby&#13;\n&#13;\nrequire 'cql'&#13;\n&#13;\nclient = Cql::Client.connect(hosts: ['192.168.1.51'])&#13;\nclient.use('sample')&#13;\n&#13;\nrows = client.execute(\"SELECT id FROM staging_sample where optype = 'D'\")&#13;\n&#13;\ndeleteids = Array.new()&#13;\n&#13;\nrows.each do |row|&#13;\nputs \"Found ID #{row['id']} has to be deleted\"&#13;\ndeleteids.push(row['id'])&#13;\nend&#13;\n&#13;\ndeleteidlist = deleteids.join(\",\")&#13;\n&#13;\nclient.execute(\"delete from sample where id in (#{deleteidlist})\");&#13;\nputs(\"delete from sample where id in (#{deleteidlist})\");&#13;\nrows = client.execute(\"SELECT * FROM staging_sample where optype = 'I'\");&#13;\n&#13;\nupdateids = Hash.new()&#13;\nupdatedata = Hash.new()&#13;\n&#13;\nrows.each do |row|&#13;\nid = row['id']&#13;\nputs \"Found ID #{id} seq #{row['seqno']} has to be inserted\"&#13;\nif updateids[id]&#13;\nif updateids[id] &lt; row['seqno']&#13;\nupdateids[id] = row['seqno']&#13;\nrow.delete('seqno')&#13;\nrow.delete('fragno')&#13;\nrow.delete('optype')&#13;\nupdatedata[id] = row&#13;\nend&#13;\nelse&#13;\nupdateids[id] = row['seqno']&#13;\nrow.delete('seqno')&#13;\nrow.delete('fragno')&#13;\nrow.delete('optype')&#13;\nupdatedata[id] = row&#13;\nend&#13;\nend&#13;\n&#13;\nupdatedata.each do |rowid,rowdata|&#13;\nputs \"Should update #{rowdata['id']} with #{rowdata['message']}\"&#13;\ncollist = rowdata.keys.join(',')&#13;\ncolcount = rowdata.keys.length&#13;\nsubstbase = Array.new()&#13;\n#  (1..colcount).each {substbase.push('?')}&#13;\nrowdata.values.each do |value|&#13;\nif value.is_a?(String)&#13;\nsubstbase.push(\"'\" + value.to_s + \"'\")&#13;\nelse&#13;\nsubstbase.push(value)&#13;\nend&#13;\nend&#13;\n&#13;\nsubstlist = substbase.join(',')&#13;\n&#13;\nputs('Column list: ',collist)&#13;\nputs('Subst list: ',substlist)&#13;\ncqlinsert = \"insert into sample (\"+collist+\") values (\"+substlist+\")\"&#13;\nputs(\"Statement: \" + cqlinsert)&#13;\nclient.execute(cqlinsert)&#13;\nend&#13;\n&#13;\nclient.execute(\"delete from staging_sample where optype in ('D','I')\")</pre>\n<p>Again, currently, this is hard coded, but I could easily of got the schema/table name from the JS batch applier – the actual code is table agnostic and will work with any table.</p>\n<p>So, I’ve setup two replicators – one uses the cassandra.js rather than hadoop.js but works the same way, and copied the applycqlsh.sh and merge.rb into /opt/continuent/share.</p>\n<p>And we’re ready to run. Let’s try it:</p>\n<pre>mysql&gt; insert into sample values (0,'First Message’);&#13;\nQuery OK, 1 row affected (0.01 sec)</pre>\n<p>We’ve inserted one row. Let’s go check Cassandra:</p>\n<pre>cqlsh:sample&gt; select * from sample;&#13;\n&#13;\nid  | message&#13;\n-----+---------------&#13;\n489 | First Message</pre>\n<p>Woohoo – data from MySQL straight into Cassandra.</p>\n<p>Now let’s try updating it:</p>\n<pre>mysql&gt; update sample set message = 'Updated Message' where id = 489;&#13;\nQuery OK, 1 row affected (0.01 sec)&#13;\nRows matched: 1  Changed: 1  Warnings: 0</pre>\n<p>And in Cassandra:</p>\n<pre>cqlsh:sample&gt; select * from sample;&#13;\n&#13;\nid  | message&#13;\n-----+-----------------&#13;\n489 | Updated Message</pre>\n<p>Bigger woohoo. Not only am I loading data directly into Cassandra, but I can update it as well. Now I can have a stream of update and information within MySQL replicated over to Cassandra for whatever analysis or information that I need without any issues.</p>\n<p>Cool huh? I certainly think so (OK, but I’m biased).</p>\n<p>Now I haven’t tested it, but this should just as easily work from Oracle; I’ll be testing that and let you know.</p>\n<p>Any other database destinations people would like to see for replicating into? If so, let me know and I’ll see what I can do.</p>"}}]}},"pageContext":{"alternative_id":9104}}