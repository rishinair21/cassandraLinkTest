{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Deploy a highly-available Cassandra cluster in AWS using Kubernetes","alternative_id":12460,"content":"<div class=\"section-inner sectionLayout--insetColumn\"><figure id=\"9017\" class=\"graf graf--figure graf--leading\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*kdSlB9fHygMjSY-4ZC568A.png\" data-width=\"603\" data-height=\"304\" src=\"https://cdn-images-1.medium.com/max/1600/1*kdSlB9fHygMjSY-4ZC568A.png\" alt=\"image\" /></div></div></figure><h1 id=\"8079\" class=\"graf graf--h3 graf-after--figure graf--title\">Deploy a highly-available Cassandra cluster in AWS using Kubernetes</h1><p id=\"48e3\" class=\"graf graf--p graf-after--h3\">At Merapar, we use Cassandra as the back-bone for some of our highly-available cloud solutions. Cassandra is a highly available, highly performant, truly horizontal scalable NoSQL database. However, deploying Cassandra in the cloud in a highly-available manner is a non-trivial task and needs proper configuration. Fortunately, due to the emergence of new technologies, deploying a Cassandra cluster in AWS is more easy nowadays. This blog will show you how to deploy a Cassandra cluster in AWS, using kops (Kubernetes Operations). It will also show that the setup is highly-available by testing some failure scenarios.</p><p id=\"e322\" class=\"graf graf--p graf-after--p\">This blog assumes basic knowledge of AWS, Kubernetes and Cassandra.</p><h3 id=\"8bab\" class=\"graf graf--h3 graf-after--p\">Deployment</h3><p id=\"1d48\" class=\"graf graf--p graf-after--h3\">The following picture shows what the final Cassandra deployment in AWS looks like:</p></div><div class=\"section-inner sectionLayout--outsetColumn\"><figure id=\"4ae7\" class=\"graf graf--figure graf--layoutOutsetCenter graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*mp72MLMoTnlsnR0VBLomCw.png\" data-width=\"1879\" data-height=\"698\" data-action=\"zoom\" data-action-value=\"1*mp72MLMoTnlsnR0VBLomCw.png\" src=\"https://cdn-images-1.medium.com/max/2000/1*mp72MLMoTnlsnR0VBLomCw.png\" alt=\"image\" /></div></div></figure></div><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"b45b\" class=\"graf graf--p graf-after--figure\">Let’s describe the deployment in detail:</p><ul class=\"postList\"><li id=\"1ae9\" class=\"graf graf--li graf-after--p\">The deployment uses one geographical region: eu-west-1 (Ireland)</li><li id=\"f37e\" class=\"graf graf--li graf-after--li\">Within this region, three availability zones are used. An availability zone is an isolated area within a region, i.e. a failure in one availability zone does not impact operations in another availability zone. Hence, high-availability is achieved by spreading processing and storage across availability zones.</li><li id=\"8ccd\" class=\"graf graf--li graf-after--li\">There are three Kubernetes masters, one in each availability zone. A Kubernetes master schedules the pods (containers), that is, it determines which Kubernetes node runs which pod. By having a master in each availability-zone, the Kubernetes masters can continue to schedule pods, even if one availability-zone is down. The master machines are deployed in an auto-scaling group, so if an EC2 instance in this group terminates, a new EC2 instance (and Kubernetes master) is started automatically.</li><li id=\"11ba\" class=\"graf graf--li graf-after--li\">There are six Kubernetes nodes, two in each availability zone. A Kubernetes node runs your application pods. In this setup, each Kubernetes node will run one Cassandra pod. If one pod fails, a new one will be scheduled by the Kubernetes master. The Kubernetes nodes are also deployed in an auto-scaling group.</li><li id=\"e961\" class=\"graf graf--li graf-after--li\">A Kubernetes stateful set with persistent volumes is used to deploy Cassandra: each Cassandra pod has a known identity (e.g. cassandra-0) and a known volume (e.g. cassandra-storage-cassandra-0). The pod identity and volume identity are tightly coupled. This enables a Cassandra pod to restart on another node and transfer its state. When a Cassandra pod starts, it attaches the same EBS volume as previously, and therefore, has the same state as before. EBS volumes are automatically created the first time a Cassandra pod starts.</li></ul><p id=\"bbe6\" class=\"graf graf--p graf-after--li\">Now let’s discuss how Cassandra has to be configured in order to replicate the data in multiple availability-zones. This is achieved by setting the following properties:</p><ul class=\"postList\"><li id=\"49bd\" class=\"graf graf--li graf-after--p\">Snitch.<br />The snitch determines to which data-center and rack a node belongs. Cassandra uses the terms “data-center” and “rack” to identify the network topology. The EC2Snitch is used. When a Cassandra node starts, the EC2Snitch retrieves the region and availability-zone information from the EC2 meta-data endpoint: the data-center is set to the region and the rack is set to the availability-zone.</li><li id=\"6f31\" class=\"graf graf--li graf-after--li\">Replication factor.<br />This determines the number of data copies. A replication-factor of three is used, i.e. three replicas are stored on different nodes.</li><li id=\"a846\" class=\"graf graf--li graf-after--li\">Replication strategy<br />This property determines which nodes store the replicas. The NetworkTopologyStrategy is used. With this strategy, a replica is stored in each availability-zone. How this is done precisely, is discussed next.</li></ul><p id=\"3fed\" class=\"graf graf--p graf-after--li\">The following picture shows how data is stored. Each Cassandra node creates multiple tokens (32 in our case). Each token is a random number between -2⁶³ to 2⁶³ -1. The Cassandra token ring is a virtual ring in token order of all tokens (192 in our case: 32 tokens * 6 nodes). When a row must be stored, its key is hashed and the result determines where on the ring the record is stored. Imagine the key of a record is hashed to a number between 1829762156858167353 and 1843966738638345890. In the picture below you can see that this corresponds to the top token. This token belongs to cassandra-2. This means cassandra-2 in zone 1b stores the record. Subsequently, the ring is followed clockwise until two additional nodes are found in other availability-zones. In this example, the other replicas are stored on cassandra-3 and cassandra-1.</p><figure id=\"0982\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*p5_BEyYVQ8LK7S6X6gdDqg.png\" data-width=\"1112\" data-height=\"458\" data-action=\"zoom\" data-action-value=\"1*p5_BEyYVQ8LK7S6X6gdDqg.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*p5_BEyYVQ8LK7S6X6gdDqg.png\" alt=\"image\" /></div></div></figure><h3 id=\"cba7\" class=\"graf graf--h3 graf-after--figure\">Step-by-step setup</h3><p id=\"c8bf\" class=\"graf graf--p graf-after--h3\">Next we will install the described deployment from scratch. In order to follow these steps, you only need an AWS account and a Linux machine to run the commands from. We use the following versions for the various components:</p><ul class=\"postList\"><li id=\"424d\" class=\"graf graf--li graf-after--p\">Ubuntu 16.04</li><li id=\"2ef5\" class=\"graf graf--li graf-after--li\">Kops 1.8.1</li><li id=\"3304\" class=\"graf graf--li graf-after--li\">Kubernetes 1.7.16</li><li id=\"8e88\" class=\"graf graf--li graf-after--li\">Cassandra 2.2.9</li></ul><p id=\"3c6f\" class=\"graf graf--p graf-after--li\">We use Cassandra 2.2.9 in production with only a few changes from the default configuration. For details see: <a href=\"https://github.com/merapar/cassandra-docker/tree/master/docker\" data-href=\"https://github.com/merapar/cassandra-docker/tree/master/docker\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://github.com/merapar/cassandra-docker/tree/master/docker</a>. Note that this setup also works perfectly well with newer versions of Cassandra.</p><h4 id=\"01dc\" class=\"graf graf--h4 graf-after--p\">Kubernetes setup</h4><p id=\"bad1\" class=\"graf graf--p graf-after--h4\">First, you need the kops command, which is used to setup the infrastructure in AWS:</p><pre id=\"efd7\" class=\"graf graf--pre graf-after--p\">curl -LO <a href=\"https://github.com/kubernetes/kops/releases/download/1.8.1/kops-linux-amd64\" data-href=\"https://github.com/kubernetes/kops/releases/download/1.8.1/kops-linux-amd64\" class=\"markup--anchor markup--pre-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://github.com/kubernetes/kops/releases/download/1.8.1/kops-linux-amd64</a> <br />sudo mv kops-linux-amd64 /usr/local/bin/kops &amp;&amp; sudo chmod a+x /usr/local/bin/kops</pre><p id=\"f49d\" class=\"graf graf--p graf-after--pre\">You need the kubectl command to interact with the Kubernetes cluster in AWS:</p><pre id=\"6f62\" class=\"graf graf--pre graf-after--p\">curl -LO <a href=\"https://storage.googleapis.com/kubernetes-release/release/v1.7.16/bin/linux/amd64/kubectl\" data-href=\"https://storage.googleapis.com/kubernetes-release/release/v1.7.16/bin/linux/amd64/kubectl\" class=\"markup--anchor markup--pre-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://storage.googleapis.com/kubernetes-release/release/v1.7.16/bin/linux/amd64/kubectl</a><br />sudo mv kubectl /usr/local/bin/kubectl &amp;&amp; sudo chmod a+x /usr/local/bin/kubectl</pre><p id=\"a9d4\" class=\"graf graf--p graf-after--pre\">Kops uses the awscli command to interact with AWS. On Ubuntu, you can install this tool via:</p><pre id=\"20f2\" class=\"graf graf--pre graf-after--p\">apt-get install awscli</pre><p id=\"4265\" class=\"graf graf--p graf-after--pre\">Other means to install awscli can be found here: <a href=\"https://docs.aws.amazon.com/cli/latest/userguide/installing.html\" data-href=\"https://docs.aws.amazon.com/cli/latest/userguide/installing.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://docs.aws.amazon.com/cli/latest/userguide/installing.html</a></p><p id=\"7635\" class=\"graf graf--p graf-after--p\">Now you have to make sure there is an IAM user which kops can use to install the required components in AWS. The IAM user needs programmatic access (i.e. use an access-key and secret-access-key to login). In addition, the IAM user requires the following permissions: AmazonEC2FullAccess AmazonRoute53FullAccess AmazonS3FullAccess IAMFullAccess AmazonVPCFullAccess. More info can be found here: <a href=\"https://github.com/kubernetes/kops/blob/master/docs/aws.md\" data-href=\"https://github.com/kubernetes/kops/blob/master/docs/aws.md\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://github.com/kubernetes/kops/blob/master/docs/aws.md</a></p><p id=\"94fa\" class=\"graf graf--p graf-after--p\">Now let’s configure AWS:</p><pre id=\"3a53\" class=\"graf graf--pre graf-after--p\">aws configure</pre><p id=\"9692\" class=\"graf graf--p graf-after--pre\">Enter the credentials of the IAM user created in the previous step. We use eu-west-1 as the region. Accept the default output format. Next you have to create an S3 bucket. Kops stores the configuration of the deployment in this bucket</p><pre id=\"cadd\" class=\"graf graf--pre graf-after--p\">aws s3api create-bucket --bucket kops-cassandra-blog --region eu-west-1</pre><p id=\"8122\" class=\"graf graf--p graf-after--pre\">Now generate a public/private key-pair:</p><pre id=\"90ce\" class=\"graf graf--pre graf-after--p\">ssh-keygen -f kops-cassandra-blog</pre><p id=\"50ef\" class=\"graf graf--p graf-after--pre\">This key-pair is used to access the EC2 machines. The following command creates the cluster definition:</p><pre id=\"fef7\" class=\"graf graf--pre graf-after--p\">kops create cluster \\<br />--cloud=aws \\<br />--name=kops-cassandra-blog.k8s.local \\<br />--zones=eu-west-1a,eu-west-1b,eu-west-1c \\<br />--master-size=\"t2.small\" \\<br />--master-zones=eu-west-1a,eu-west-1b,eu-west-1c \\<br />--node-size=\"t2.small\" \\<br />--ssh-public-key=\"kops-cassandra-blog.pub\" \\<br />--state=s3://kops-cassandra-blog \\<br />--node-count=6</pre><p id=\"0054\" class=\"graf graf--p graf-after--pre\">Now apply the cluster definition, i.e. create the actual resources in AWS:</p><pre id=\"5302\" class=\"graf graf--pre graf-after--p\">kops update cluster --name=kops-cassandra-blog.k8s.local --state=s3://kops-cassandra-blog --yes</pre><p id=\"cdb3\" class=\"graf graf--p graf-after--pre\">After a few minutes, we will have a high-available Kubernetes cluster in AWS. Kops automatically configures kubectl. Use the following command to check the Kubernetes master nodes (the -L argument shows labels, while the -l argument filters on labels):</p><pre id=\"aa24\" class=\"graf graf--pre graf-after--p\">kubectl get no -L failure-domain.beta.kubernetes.io/zone -l kubernetes.io/role=master</pre><p id=\"b7b1\" class=\"graf graf--p graf-after--pre\">The following output is visible:</p><pre id=\"9841\" class=\"graf graf--pre graf-after--p\">NAME               STATUS  AGE  VERSION  ZONE<br />ip-172-20-112-210  Ready   1m   v1.8.7   eu-west-1c<br />ip-172-20-58-140   Ready   1m   v1.8.7   eu-west-1a<br />ip-172-20-85-234   Ready   1m   v1.8.7   eu-west-1b</pre><p id=\"e324\" class=\"graf graf--p graf-after--pre\">As can be seen, the three Kubernetes masters each reside in a separate availability zone. Now run the same command for the Kubernetes nodes:</p><pre id=\"f986\" class=\"graf graf--pre graf-after--p\">kubectl get no -L failure-domain.beta.kubernetes.io/zone -l kubernetes.io/role=node</pre><p id=\"d73a\" class=\"graf graf--p graf-after--pre\">As can be seen in the output, each availability zone has two Kubernetes nodes:</p><pre id=\"6bd9\" class=\"graf graf--pre graf-after--p\">NAME               STATUS    AGE  VERSION  ZONE<br />ip-172-20-114-66   Ready     1m   v1.8.7   eu-west-1c<br />ip-172-20-116-132  Ready     1m   v1.8.7   eu-west-1c<br />ip-172-20-35-200   Ready     1m   v1.8.7   eu-west-1a<br />ip-172-20-42-220   Ready     1m   v1.8.7   eu-west-1a<br />ip-172-20-94-29    Ready     1m   v1.8.7   eu-west-1b<br />ip-172-20-94-34    Ready     1m   v1.8.7   eu-west-1b</pre><p id=\"c29f\" class=\"graf graf--p graf-after--pre\">You can destroy the environment at all times by running the following command:</p><pre id=\"6aad\" class=\"graf graf--pre graf-after--p\">kops delete cluster --name=kops-cassandra-blog.k8s.local --state=s3://kops-cassandra-blog --yes</pre><h4 id=\"7f99\" class=\"graf graf--h4 graf-after--pre\">Cassandra setup</h4><p id=\"d23d\" class=\"graf graf--p graf-after--h4\">Create a file cassandra.yml, containing the following definitions:</p><pre id=\"1206\" class=\"graf graf--pre graf-after--p\">apiVersion: v1<br />kind: Service<br />metadata:<br />  name: cassandra<br />spec:<br />  clusterIP: None<br />  ports:<br />    - name: cql<br />      port: 9042<br />  selector:<br />    app: cassandra<br />---<br />apiVersion: \"apps/v1beta1\"<br />kind: StatefulSet<br />metadata:<br />  name: cassandra<br />spec:<br />  serviceName: cassandra<br />  replicas: 6<br />  template:<br />    metadata:<br />      labels:<br />        app: cassandra<br />    spec:<br />      affinity:<br />        podAntiAffinity:<br />          requiredDuringSchedulingIgnoredDuringExecution:<br />          - topologyKey: kubernetes.io/hostname<br />            labelSelector:<br />              matchLabels:<br />                app: cassandra<br />      containers:<br />        - env:<br />            - name: MAX_HEAP_SIZE<br />              value: 512M<br />            - name: HEAP_NEWSIZE<br />              value: 512M<br />            - name: POD_IP<br />              valueFrom:<br />                fieldRef:<br />                  fieldPath: status.podIP<br />          image: merapar/cassandra:2.3<br />          name: cassandra<br />          volumeMounts:<br />            - mountPath: /cassandra-storage<br />              name: cassandra-storage<br />  volumeClaimTemplates:<br />  - metadata:<br />      name: cassandra-storage<br />    spec:<br />      accessModes:<br />      - ReadWriteOnce<br />      resources:<br />        requests:<br />          storage: 10Gi</pre><p id=\"c3fd\" class=\"graf graf--p graf-after--pre\">And run</p><pre id=\"d24e\" class=\"graf graf--pre graf-after--p\">kubectl create -f cassandra.yml</pre><p id=\"72a5\" class=\"graf graf--p graf-after--pre\">The following components are installed within the timespan of a few minutes:</p><ul class=\"postList\"><li id=\"40bd\" class=\"graf graf--li graf-after--p\">Service cassandra. This service is used by clients within the Kubernetes cluster to connect to Cassandra. It does not have a cluster-IP. This is on purpose because Cassandra node-discovery and load-balancing is handled by the Cassandra client itself (not via Kubernetes). The client library connects to one contact point only: the cassandra DNS name. This is translated by the DNS pod to the IP address of one of the Cassandra pods. That pod will tell the IP addresses of the other Cassandra pods.</li><li id=\"7b4d\" class=\"graf graf--li graf-after--li\">StatefulSet cassandra. The stateful set makes sure that there are six Cassandra pods running at all times with a fixed identity: cassandra-0 up to and including cassandra-5.</li></ul><p id=\"9d64\" class=\"graf graf--p graf-after--li\">In order to connect to the Cassandra cluster, we use the cqlsh command which is available on each node:</p><pre id=\"0531\" class=\"graf graf--pre graf-after--p\">kubectl exec -ti cassandra-0 cqlsh cassandra-0</pre><p id=\"1ce1\" class=\"graf graf--p graf-after--pre\">This opens a CQL prompt and lets you interact with the cluster using CQL. The command “cqlsh cassandra-0” actually connects to the server listed in the first argument (cassandra-0). So in this case, it connects to itself.</p><p id=\"bae3\" class=\"graf graf--p graf-after--p\">Now we are going to create a key-space, a table and 100 records. First set the consistency level:</p><pre id=\"1b02\" class=\"graf graf--pre graf-after--p\">CONSISTENCY QUORUM;</pre><p id=\"8c24\" class=\"graf graf--p graf-after--pre\">Quorum means that a majority of the replica’s (2 in our case) must be read or written in order for the read or write command to succeed. Now create the key-space:</p><pre id=\"c6b4\" class=\"graf graf--pre graf-after--p\">CREATE KEYSPACE test WITH REPLICATION = { 'class' : 'NetworkTopologyStrategy', 'eu-west' : 3 };</pre><p id=\"7daa\" class=\"graf graf--p graf-after--pre\">Switch to the test key-space:</p><pre id=\"84e8\" class=\"graf graf--pre graf-after--p\">USE test;</pre><p id=\"8856\" class=\"graf graf--p graf-after--pre\">Create a table</p><pre id=\"d230\" class=\"graf graf--pre graf-after--p\">CREATE TABLE persons (id uuid, name text, PRIMARY KEY (id));</pre><p id=\"f17f\" class=\"graf graf--p graf-after--pre\">Now run the following command 100 times to insert 100 records. We need enough records so that each node contains replicas.</p><pre id=\"97e0\" class=\"graf graf--pre graf-after--p\">INSERT INTO persons (id,name) VALUES (uuid(),'name');</pre><p id=\"3d53\" class=\"graf graf--p graf-after--pre\">You can also run a script from the cassandra-0 machine:</p><pre id=\"9e3d\" class=\"graf graf--pre graf-after--p\">kubectl exec -ti cassandra-0 bash</pre><p id=\"a11c\" class=\"graf graf--p graf-after--pre\">With the following loop:</p><pre id=\"b373\" class=\"graf graf--pre graf-after--p\">for i in {1..100}<br /> do<br />   echo \"adding customer $i\"<br />   cqlsh cassandra-0 -e \"USE test; CONSISTENCY QUORUM; INSERT INTO persons (id,name) VALUES (uuid(),'name');\"<br /> done</pre><p id=\"0d92\" class=\"graf graf--p graf-after--pre\">Now run the following command and make sure 100 records are returned:</p><pre id=\"4041\" class=\"graf graf--pre graf-after--p\">SELECT * FROM persons;</pre><h3 id=\"e3e9\" class=\"graf graf--h3 graf-after--pre\">Testing the high-availability</h3><p id=\"a154\" class=\"graf graf--p graf-after--h3\">Now that we have a Cassandra cluster in AWS with some data inside, we can test the high-availability. Note that this setup is only resilient against failures in one availability-zone. In order to be resilient against multiple concurrent availability-zone failures, one should consider using a disaster recovery site in another region. While running the failure scenarios in the following sections, the select query presented earlier on should always return 100 records, i.e. Cassandra should be high-available (all data should be available) at all times.</p><h4 id=\"6473\" class=\"graf graf--h4 graf-after--p\">EC2 instance failure</h4><p id=\"2b6f\" class=\"graf graf--p graf-after--h4\">Note that we will only test Kubernetes-node failure (not Kubernetes-master failure). Let’s terminate an EC2 instance via the AWS console. We should not terminate an EC2 instance running the cassandra-0 pod. Otherwise our CQL prompt terminates. Via the following commands:</p><pre id=\"a6e3\" class=\"graf graf--pre graf-after--p\">kubectl get no -L failure-domain.beta.kubernetes.io/zone<br />kubectl get po -o wide</pre><p id=\"fc8f\" class=\"graf graf--p graf-after--pre\">We can construct the following table:</p><pre id=\"eac3\" class=\"graf graf--pre graf-after--p\">Cassandra-node  EC2 instance       Availability-zone<br />----------------------------------------------------<br />cassandra-0     ip-172-20-94-34    eu-west-1b<br />cassandra-1     ip-172-20-116-132  eu-west-1c<br />cassandra-2     ip-172-20-42-220   eu-west-1a<br />cassandra-3     ip-172-20-94-29    eu-west-1b<br />cassandra-4     ip-172-20-114-66   eu-west-1c<br />cassandra-5     ip-172-20-35-200   eu-west-1a</pre><p id=\"8dca\" class=\"graf graf--p graf-after--pre\">For this test, we will terminate instance ip-172–20–116–132 which will terminate cassandra-1. AWS will try to launch a new EC2 instance in the availability zone with the fewest instances. In our case, the auto-scaling group called “nodes” contains one instance for the eu-west-1c zone, while it contains two for the other zones. Therefore, the new instance is launched in eu-west-1c. Note that this is best effort; if a whole availability-zone is down for an extended period of time, manual intervention is required to recover. This will be discussed later-on. When requesting the pods, the following output is visible:</p><pre id=\"70ec\" class=\"graf graf--pre graf-after--p\">NAME          READY     STATUS    RESTARTS   AGE<br />cassandra-0   1/1       Running   0          1h<br />cassandra-1   0/1       Pending   0          8s<br />cassandra-2   1/1       Running   1          1h<br />cassandra-3   1/1       Running   4          1h<br />cassandra-4   1/1       Running   0          1h<br />cassandra-5   1/1       Running   0          1h</pre><p id=\"52c3\" class=\"graf graf--p graf-after--pre\">While the EC2 instance is starting, the status of the pod is pending. The read-query, as expected, still returns 100 rows. This is because we use quorum reads: we need two of the three replicas and since all zones contain one replica, the read succeeds. Pod cassandra-1 will be rescheduled to the new EC2 instance. The following policies apply during rescheduling:</p><ul class=\"postList\"><li id=\"877e\" class=\"graf graf--li graf-after--p\">When a Kubernetes node is started, it automatically gets a label with availability-zone information. When Kubernetes schedules pods of a stateful set, it tries to spread them across the availability-zones. Persistent volumes (the EBS volume) of a pod are also located in a particular availability-zone. When a persistent volume is created, it too gets a label with availability-zone information. Now, when a pod is scheduled and claims a volume, Kubernetes makes sure the pod is scheduled to a node in the same availability zone as the volume. In our case, cassandra-1 must be rescheduled. This pod claims (wants to link) volume cassandra-storage-cassandra-1. Since this volume is located in zone eu-west-1c, cassandra-1 will get scheduled on a node running in eu-west-1c</li><li id=\"8aea\" class=\"graf graf--li graf-after--li\">We use anti-pod-affinity to make sure a Kubernetes node runs a maximum of one Cassandra pod. Although it is perfectly viable to run without this policy, it has two benefits: While the new EC2 instance is starting, the Cassandra pod is not started on the remaining nodes. Therefore, no manual rescheduling is required afterwards ( in order to balance the pods). The second benefit is that the full resources of the node are available for the Cassandra node. Note that the same can be achieved using other means (e.g. using resource quotas)</li></ul><h4 id=\"a30a\" class=\"graf graf--h4 graf-after--li\">Availability zone failure</h4><p id=\"892a\" class=\"graf graf--p graf-after--h4\">In order to test this scenario, we will terminate Cassandra nodes running in zone eu-west-1a: cassandra-2 (EC2 instance ip-172–20–42–220) and cassandra-5 (EC2 instance ip-172–20–35–200). The “get pods” command now shows only five nodes, of which one is in the pending state:</p><pre id=\"d71d\" class=\"graf graf--pre graf-after--p\">NAME          READY     STATUS    RESTARTS   AGE<br />cassandra-0   1/1       Running   0          2h<br />cassandra-1   1/1       Running   0          18m<br />cassandra-2   0/1       Pending   0          55s<br />cassandra-3   1/1       Running   4          2h<br />cassandra-4   1/1       Running   0          2h</pre><p id=\"386d\" class=\"graf graf--p graf-after--pre\">Since Cassandra replicates data across all zones, all data is still available. This can be confirmed by running the read query which still return 100 records.</p><p id=\"dcc0\" class=\"graf graf--p graf-after--p\">The recovery process for this scenario is basically the same as the single EC2 instance failure scenario describe in the previous scenario. Although very rare, an availability-zone can fail in such a way that instances cannot be restarted in the failing zone, but are started in another zone instead. More information can be found here: <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html\" data-href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html</a></p><p id=\"ab2d\" class=\"graf graf--p graf-after--p\">The previous scenario described how pods with volume claims are scheduled. Because there will be a mismatch between the new EC2 instance zone-info and the volume zone-info, the pods cannot reschedule. Manual intervention is required. When the failing availability-zone is up again, terminate all machines which could previously not start in the failing availability-zone and all should recover automatically.</p><h3 id=\"d549\" class=\"graf graf--h3 graf-after--p\">Final remarks</h3><p id=\"5e58\" class=\"graf graf--p graf-after--h3 graf--trailing\">In this post, we have shown how to deploy a high-available Cassandra cluster in AWS. We have also shown that the deployment automatically recovers from node failures in the same availability zone. In a next post, we will discuss scaling the cluster horizontally. Another topic, not discussed here, is performance, which might also be the subject of a future post.</p></div>"}}]}},"pageContext":{"alternative_id":12460}}