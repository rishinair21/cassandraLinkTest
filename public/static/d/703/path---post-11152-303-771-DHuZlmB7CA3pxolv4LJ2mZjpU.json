{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Installing the Cassandra / Spark OSS Stack","alternative_id":11152,"content":"<h2>Init</h2><p>As mentioned in my <a href=\"http://tobert.github.io/post/2014-07-14-portacluster-system-imaging.html\">portacluster system imaging</a> post,\nI am performing this install on 1 admin node (node0) and 6 worker nodes (node[1-6]) running 64-bit Arch Linux.\nMost of what I describe in this post should work on other Linux variants with minor adjustments.</p><h2>Overview</h2><p>When assembling an analytics stack, there are usually myriad choices to make. For this build, I decided to\nbuild the smallest stack possible that lets me run Spark queries on Cassandra data. As configured it is\nnot highly available since the Spark master is standalone. (note: Datastax Enterprise Spark's master has\nHA based on Cassandra). It's a decent tradeoff for portacluster, since\nI can run the master on the admin node which doesn't get rebooted/reimaged constantly. I'm also going to\nskip HDFS or some kind of HDFS replacement for now. Options I plan to look at later are GlusterFS's HDFS\nadapter and <a href=\"http://pithos.io\">Pithos</a> as an S3 adapter. In the end, the stack is simply Cassandra and\nSpark with the <a href=\"https://github.com/datastax/spark-cassandra-connector\">spark-cassandra-connector</a>.</p><h2>Responsible Configuration</h2><p>For this post I've used my <a href=\"https://github.com/tobert/perl-ssh-tools\">perl-ssh-tools</a> suite. The intent\nis to show what needs to be done and one way to do it. For production deployments, I recommend using\nyour favorite configuration management tool.</p><p>perl-ssh-tools uses a configuration similar to dsh, which uses simple files with one\nhost per line. I use two lists below. Most commands run on the fleet of workers. Because\ncl-run.pl provides more than ssh commands, it's also used to run commands on node0 using\nits --incl flag e.g. <code>cl-run.pl --list all --incl node0</code>.</p><pre>cat .dsh/machines.workers\nnode1\nnode2\nnode3\nnode4\nnode5\nnode6\n</pre><p><code>machines.all</code> is the same with node0 added.</p><h2>Install Cassandra</h2><p>My first pass at this section involved setting up a package repo, but since I don't have time to package\nSpark properly right now, I'm going to use the tarball distros of Cassandra and Spark to keep it simple.\n<a href=\"https://github.com/joschi\">joschi</a> maintains a package on the <a href=\"https://aur.archlinux.org/packages/cassandra/\">AUR</a>\nbut I have chosen not to use it for this install.\nI'm also using the Arch packages of OpenJDK, which isn't supported by Datastax, but works fine for hacking.\nThe JDK is pre-installed on my Arch image, it's as simple as <code>sudo pacman -S extra/jdk7-openjdk</code>.</p><p>First, I downloaded the Cassandra tarball from <a href=\"http://cassandra.apache.org/download/\">apache.org</a> to\nnode0 in /srv/public/tgz. Then on the worker nodes, it gets downloaded and expanded in /opt.</p><pre>pkg=\"apache-cassandra-2.0.9-bin.tar.gz\"\nsudo curl -o /srv/public/tgz/$pkg \\\n  http://mirrors.gigenet.com/apache/cassandra/2.0.9/apache-cassandra-2.0.9-bin.tar.gz\ncl-run.pl --list workers -c \"curl http://node0/tgz/$pkg |sudo tar -C /opt -xzf -\"\ncl-run.pl --list workers -c \"sudo ln -s /opt/apache-cassandra-2.0.9 /opt/cassandra\"\n</pre><p>To make it easier to do upgrades without regenerating the configuration, I\nrelocate the conf dir to /etc/cassandra to match what packages do. This assumes there\nis no existing /etc/cassandra.</p><pre>cl-run.pl --list workers -c \"sudo mv /opt/cassandra/conf /etc/cassandra\"\ncl-run.pl --list workers -c \"sudo ln -s /etc/cassandra /opt/cassandra/conf\"\n</pre><p>I will start Cassandra with a systemd unit, so I push that out as well. This unit\nfile runs Cassandra out of the tarball as the cassandra user with the stdout/stderr going\nto the systemd journal (view with <code>journalctl -f</code>). I also included some\nulimit settings and bump the OOM score downwards to make it less likely that the kernel\nwill kill Cassandra when out of memory. Since we're going to be running two large JVM apps\non each worker node, this unit also enables cgroups so Cassandra can be given priority\nover Spark. Finally, since the target machines have 16GB of RAM, the heap needs to be\nset to 8GB (cassandra-env.sh calculates 3995M which is way too low).</p><pre>cat &gt; cassandra.service &lt;&lt;EOF\n[Unit]\nDescription=Cassandra Tarball\nAfter=network.target\n[Service]\nUser=cassandra\nGroup=cassandra\nRuntimeDirectory=cassandra\nPIDFile=/run/cassandra/cassandra.pid\nExecStart=/opt/cassandra/bin/cassandra -f -p /run/cassandra/cassandra.pid\nStandardOutput=journal\nStandardError=journal\nOOMScoreAdjust=-500\nLimitNOFILE=infinity\nLimitMEMLOCK=infinity\nLimitNPROC=infinity\nLimitAS=infinity\nEnvironment=MAX_HEAP_SIZE=8G HEAP_NEWSIZE=1G CASSANDRA_HEAPDUMP_DIR=/srv/cassandra/log\nCPUAccounting=true\nCPUShares=1000\n[Install]\nWantedBy=multi-user.target\nEOF\ncl-sendfile.pl --list workers -x -l cassandra.service -r /etc/systemd/system/multi-user.target.wants/cassandra.service\ncl-run.pl --list workers -c \"sudo systemctl daemon-reload\"\n</pre><p>Since all Cassandra data is being redirected to /srv/cassandra and it's going to run as the\ncassandra user, those need to be created.</p><pre>cat &gt; cassandra-user.sh &lt;&lt;EOF\nmkdir -p /srv/cassandra/{log,data,commitlogs,saved_caches}\n(grep -q '^cassandra:' /etc/group)  || groupadd -g 1234 cassandra\n(grep -q '^cassandra:' /etc/passwd) || useradd -u 1234 -c \"Apache Cassandra\" -g cassandra -s /bin/bash -d /srv/cassandra cassandra\nchown -R cassandra:cassandra /srv/cassandra\nEOF\ncl-run.pl --list workers -x -s cassandra-user.sh\n</pre><h2>Configure Cassandra</h2><p>Before starting Cassandra I want to make a few changes to the standard configurations. I'm not a big\nfan of LSB so I redirect all of the /var files to /srv/cassandra so they're all in one place. There's\nonly one SSD in the target systems so the commit log goes on the same drive.</p><p>I configured portacluster nodes to have a bridge in front of the default interface, making br0 the default interface.</p><pre>cat cassandra-config.sh\nip=$(ip addr show br0 |perl -ne 'if ($_ =~ /inet (\\d+\\.\\d+\\.\\d+\\.\\d+)/) { print $1 }')\nperl -i.bak -pe \"\n  s/^(cluster_name:).*/\\$1 'Portable Cluster'/;\n  s/^(listen|rpc)_address:.*/\\${1}_address: $ip/;\n  s|/var/lib|/srv|;\n  s/(\\s+-\\s+seeds:).*/\\$1 '192.168.10.11,192.168.10.12,192.168.10.13,192.168.10.14,192.168.10.15,192.168.10.16'/\n\" /opt/cassandra/conf/cassandra.yaml\n# EOF\ncl-run.pl --list workers -x -s cassandra-config.sh\n</pre><p>The default log4-server.propterties has log4j printing to stdout. This is not desirable in a background\nservice configuration, so I remove it. The logs are also now written to /srv/cassandra/log.</p><pre>cat &gt; log4j-server.properties &lt;&lt;EOF\nlog4j.rootLogger=INFO,R\nlog4j.appender.R=org.apache.log4j.RollingFileAppender\nlog4j.appender.R.maxFileSize=20MB\nlog4j.appender.R.maxBackupIndex=20\nlog4j.appender.R.layout=org.apache.log4j.PatternLayout\nlog4j.appender.R.layout.ConversionPattern=%5p [%t] %d{ISO8601} %F (line %L) %m%n\nlog4j.appender.R.File=/srv/cassandra/log/system.log\nlog4j.logger.org.apache.thrift.server.TNonblockingServer=ERROR\nEOF\ncl-sendfile.pl --list workers -x -l log4j-server.properties -r /opt/cassandra/conf/log4j-server.properties\n</pre><p>And with that, Cassandra is ready to start.</p><pre>cl-run.pl --list workers -c \"sudo systemctl start cassandra.service\"\nssh node3 tail -f /srv/cassandra/log/system.log\n</pre><h2>Installing Spark</h2><p>The process for Spark is quite similar, except that unlike Cassandra, it has a master.</p><p>Since I'm not using any Hadoop components, any of the builds should be fine so I used the\nhadoop2 build.</p><pre>pkg=\"spark-1.0.1-bin-hadoop2.tgz\"\nsudo curl -o /srv/public/tgz/$pkg http://d3kbcqa49mib13.cloudfront.net/spark-1.0.1-bin-hadoop2.tgz\ncl-run.pl --list all -c \"curl http://node0/tgz/$pkg |sudo tar -C /opt -xzf -\"\ncl-run.pl --list all -c \"sudo ln -s /opt/spark-1.0.1-bin-hadoop2 /opt/spark\"\ncl-run.pl --list all -c \"sudo mv /opt/spark/conf /etc/spark\"\ncl-run.pl --list all -c \"sudo ln -s /etc/spark /opt/spark/conf\"\n</pre><p>Create /srv/spark and the spark user.</p><pre>cat &gt; spark-user.sh &lt;&lt;EOF\nmkdir -p /srv/spark/{logs,work,tmp,pids}\n(grep -q '^spark:' /etc/group)  || groupadd -g 4321 spark\n(grep -q '^spark:' /etc/passwd) || useradd -u 4321 -c \"Apache Spark\" -g spark -s /bin/bash -d /srv/spark spark\nchown -R spark:spark /srv/spark\n# make spark tmp world writable and sticky\nchmod 4755 /srv/spark/tmp\nEOF\ncl-run.pl --list all -x -s spark-user.sh\n</pre><h2>Configuring Spark</h2><p>Many of Spark's settings are controlled by environment variables. Since I want all volatile data\nin /srv, many of these need to be changed. Spark will pick up spark-env.sh automatically.</p><p>The Intel NUC systems I'm running this stack on have 4 cores and 16G of RAM, so I'll give\nSpark 2 cores and 4G of memory for now.</p><p>One line worth calling out is the <code>SPARK_WORKER_PORT=9000</code>. It can be any port. If you don't set\nit, every time a work is restarted the master will have a stale entry for a while. It's not\na big deal but I like it better this way.</p><pre>cat &gt; spark-env.sh &lt;&lt;EOF\nexport SPARK_WORKER_CORES=\"2\"\nexport SPARK_WORKER_MEMORY=\"4g\"\nexport SPARK_DRIVER_MEMORY=\"2g\"\nexport SPARK_REPL_MEM=\"4g\"\nexport SPARK_WORKER_PORT=9000\nexport SPARK_CONF_DIR=\"/etc/spark\"\nexport SPARK_TMP_DIR=\"/srv/spark/tmp\"\nexport SPARK_PID_DIR=\"/srv/spark/pids\"\nexport SPARK_LOG_DIR=\"/srv/spark/logs\"\nexport SPARK_WORKER_DIR=\"/srv/spark/work\"\nexport SPARK_LOCAL_DIRS=\"/srv/spark/tmp\"\nexport SPARK_COMMON_OPTS=\"$SPARK_COMMON_OPTS -Dspark.kryoserializer.buffer.mb=32 \"\nLOG4J=\"-Dlog4j.configuration=file://$SPARK_CONF_DIR/log4j.properties\"\nexport SPARK_MASTER_OPTS=\" $LOG4J -Dspark.log.file=/srv/spark/logs/master.log \"\nexport SPARK_WORKER_OPTS=\" $LOG4J -Dspark.log.file=/srv/spark/logs/worker.log \"\nexport SPARK_EXECUTOR_OPTS=\" $LOG4J -Djava.io.tmpdir=/srv/spark/tmp/executor \"\nexport SPARK_REPL_OPTS=\" -Djava.io.tmpdir=/srv/spark/tmp/repl/\\$USER \"\nexport SPARK_APP_OPTS=\" -Djava.io.tmpdir=/srv/spark/tmp/app/\\$USER \"\nexport PYSPARK_PYTHON=\"/bin/python2\"\nEOF\n</pre><p>spark-submit and other tools may use spark-defaults.conf to find the master and other configuration items.</p><pre>cat &gt; spark-defaults.conf &lt;&lt;EOF\nspark.master            spark://node0.pc.datastax.com:7077\nspark.executor.memory   512m\nspark.eventLog.enabled  true\nspark.serializer        org.apache.spark.serializer.KryoSerializer\nEOF\n</pre><p>The systemd units are a little less complex than Cassandra's. The spark-master.service unit\nshould only exist on node0, while every other node runs spark-worker. Spark workers are given\na weight of 100 compared to Cassandra's weight of 1000 so that Cassandra is given priority over\nSpark without starving it entirely.</p><pre>cat &gt; spark-worker.service &lt;&lt;EOF\n[Unit]\nDescription=Spark Worker\nAfter=network.target\n[Service]\nType=forking\nUser=spark\nGroup=spark\nExecStart=/opt/spark/sbin/start-slave.sh 1 spark://node0.pc.datastax.com:7077\nStandardOutput=journal\nStandardError=journal\nLimitNOFILE=infinity\nLimitMEMLOCK=infinity\nLimitNPROC=infinity\nLimitAS=infinity\nCPUAccounting=true\nCPUShares=100\n[Install]\nWantedBy=multi-user.target\nEOF\n</pre><p>The master unit is similar and only gets installed on node0. Since it is not competing\nfor resources, there's no need to turn on cgroups for now.</p><pre>cat &gt; spark-master.service &lt;&lt;EOF\n[Unit]\nDescription=Spark Master\nAfter=network.target\n[Service]\nType=forking\nUser=spark\nGroup=spark\nExecStart=/opt/spark/sbin/start-master.sh 1\nStandardOutput=journal\nStandardError=journal\nLimitNOFILE=infinity\nLimitMEMLOCK=infinity\nLimitNPROC=infinity\nLimitAS=infinity\n[Install]\nWantedBy=multi-user.target\nEOF\n</pre><p>Now deploy all of these configs. Relocate the spark config into /etc/spark and copy\na couple templates, then write all the files there. spark-env.sh goes on all nodes.\nThe unit files are described above. Finally,\na command is run to instruct systemd to read the new unit files.</p><pre>cl-run.pl --list all -c \"sudo cp /opt/spark/conf/log4j.properties.template /opt/spark/conf/log4j.properties\"\ncl-run.pl --list all -c \"sudo cp /opt/spark/conf/fairscheduler.xml.template /opt/spark/conf/fairscheduler.xml\"\ncl-sendfile.pl --list all -x -l spark-env.sh -r /etc/spark/spark-env.sh\ncl-sendfile.pl --list all -x -l spark-defaults.conf -r /etc/spark/spark-defaults.conf\ncl-sendfile.pl --list workers -x -l spark-worker.service -r /etc/systemd/system/multi-user.target.wants/spark-worker.service\ncl-sendfile.pl --list all --incl node0 -x -l spark-master.service -r /etc/systemd/system/multi-user.target.wants/spark-master.service\ncl-run.pl --list all -c \"sudo systemctl daemon-reload\"\n</pre><p>With all of that done, it's time to turn on Spark to see if it works.</p><pre>cl-run.pl --list all --incl node0 -c \"sudo systemctl start spark-master.service\"\ncl-run.pl --list workers -c \"sudo systemctl start spark-worker.service\"\n</pre><p>Now I can browse to the Spark master webui.</p><p><img src=\"http://tobert.github.io/images/spark-master-screenshot-2014-07-15.jpg\" alt=\"screenshot\" /></p><h2>Installing spark-cassandra-connector</h2><p>The connector is now published in Maven and can be installed easiest using ivy on the\ncommand line. Ivy can pull all dependencies as well as the connector jar, saving a lot of\nfiddling around. In addition, while ivy can download the connector directly, it will\nend up pulling down all of Cassandra and Spark. The script fragment below pulls down only what\nis necessary to run the connector against a pre-built Spark.</p><p>This is only really needed for the spark-shell so it can access Cassandra. Most projects\nshould include the necessary jars in a fat jar rather than pushing these packages\nto every node.</p><p>I run these commands on node0 since that's where I usually work with spark-shell. To run it on\nanother machine, Spark will have to be present and match the version of the cluster, then this\nsame process will get everything needed to use the connector.</p><pre>cat &gt; download-connector.sh &lt;&lt;EOF\nmkdir /opt/connector\ncd /opt/connector\nrm *.jar\ncurl -o ivy-2.3.0.jar \\\n  'http://search.maven.org/remotecontent?filepath=org/apache/ivy/ivy/2.3.0/ivy-2.3.0.jar'\ncurl -o spark-cassandra-connector_2.10-1.0.0-beta1.jar \\\n  'http://search.maven.org/remotecontent?filepath=com/datastax/spark/spark-cassandra-connector_2.10/1.0.0-beta1/spark-cassandra-connector_2.10-1.0.0-beta1.jar'\nivy () { java -jar ivy-2.3.0.jar -dependency \\$* -retrieve \"[artifact]-[revision](-[classifier]).[ext]\"; }\nivy org.apache.cassandra cassandra-thrift 2.0.9\nivy com.datastax.cassandra cassandra-driver-core 2.0.3\nivy joda-time joda-time 2.3\nivy org.joda joda-convert 1.6\nrm -f *-{sources,javadoc}.jar\nEOF\nsudo bash download-connector.sh\n</pre><h2>Using spark-cassandra-connector With spark-shell</h2><p>All that's left to get started with the connector now is to get spark-shell to pick it up. The easiest\nway I've found is to set the classpath with --driver-class-path then restart the context in the REPL\nwith the necessary classes imported to make sc.cassandraTable() visible.</p><p>The newly loaded methods will not show up in tab completion. I don't know why.</p><pre>/opt/spark/bin/spark-shell --driver-class-path $(echo /opt/connector/*.jar |sed 's/ /:/g')\n</pre><p>It will print a bunch of log information then present scala&gt; prompt.</p><pre>scala&gt; sc.stop\n</pre><p>Now that the context is stopped, it's time to import the connector.</p><pre>scala&gt; import com.datastax.spark.connector._\nscala&gt; val conf = new SparkConf()\nscala&gt; conf.set(\"cassandra.connection.host\", \"node1.pc.datastax.com\")\nscala&gt; val sc = new SparkContext(\"local[2]\", \"Cassandra Connector Test\", conf)\nscala&gt; val table = sc.cassandraTable(\"keyspace\", \"table\")\nscala&gt; table.count\n</pre><p>To make sure everything is working, I ran some code I'm working on for my 2048 game analytics\nproject. Each context gets an application webui that displays job status.</p><p><img src=\"http://tobert.github.io/images/spark-stages-screenshot-2014-07-15.jpg\" alt=\"screenshot\" /></p><h2>Conclusion</h2><p>It was a lot of work getting here, but what we have at the end is a Spark shell that can\naccess tables in Cassandra as RDDs with types pre-mapped and ready to go.</p><p>There are some things that can be improved upon. I will likely package all of this into\na Docker image at some point. For now, I need it up and running for some demos that will\nbe running on portacluster at <a href=\"http://www.oscon.com/oscon2014\">OSCON 2014</a>.</p>"}}]}},"pageContext":{"alternative_id":11152}}