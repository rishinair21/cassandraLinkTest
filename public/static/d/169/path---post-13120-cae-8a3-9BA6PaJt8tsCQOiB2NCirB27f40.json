{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Aggregate operations on Azure Cosmos DB Cassandra API tables from Spark","alternative_id":13120,"content":"<ul class=\"metadata page-metadata\" data-bi-name=\"page info\" lang=\"bs-latn-ba\" dir=\"ltr\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<li class=\"contributors-holder\">&#13;\n\t\t\t\t\t\t\t\tSaradnici&#13;\n\t\t\t\t\t\t\t\t</li>\n\n\t\t\t\t\t\t</ul><p>This article describes basic aggregation operations against Azure Cosmos DB Cassandra API tables from Spark. </p>\n<div class=\"NOTE\"><p>Note</p><p>Server-side filtering, and server-side aggregation is currently not supported in Azure Cosmos DB Cassandra API.</p></div>\n<h2 id=\"cassandra-api-configuration\">Cassandra API configuration</h2>\n<pre class=\"lang-scala\">import org.apache.spark.sql.cassandra._\n//Spark connector\nimport com.datastax.spark.connector._\nimport com.datastax.spark.connector.cql.CassandraConnector\n//CosmosDB library for multiple retry\nimport com.microsoft.azure.cosmosdb.cassandra\n//Connection-related\nspark.conf.set(\"spark.cassandra.connection.host\",\"YOUR_ACCOUNT_NAME.cassandra.cosmosdb.azure.com\")\nspark.conf.set(\"spark.cassandra.connection.port\",\"10350\")\nspark.conf.set(\"spark.cassandra.connection.ssl.enabled\",\"true\")\nspark.conf.set(\"spark.cassandra.auth.username\",\"YOUR_ACCOUNT_NAME\")\nspark.conf.set(\"spark.cassandra.auth.password\",\"YOUR_ACCOUNT_KEY\")\nspark.conf.set(\"spark.cassandra.connection.factory\", \"com.microsoft.azure.cosmosdb.cassandra.CosmosDbConnectionFactory\")\n//Throughput-related...adjust as needed\nspark.conf.set(\"spark.cassandra.output.batch.size.rows\", \"1\")\nspark.conf.set(\"spark.cassandra.connection.connections_per_executor_max\", \"10\")\nspark.conf.set(\"spark.cassandra.output.concurrent.writes\", \"1000\")\nspark.conf.set(\"spark.cassandra.concurrent.reads\", \"512\")\nspark.conf.set(\"spark.cassandra.output.batch.grouping.buffer.size\", \"1000\")\nspark.conf.set(\"spark.cassandra.connection.keep_alive_ms\", \"600000000\")\n</pre><h2 id=\"sample-data-generator\">Sample data generator</h2>\n<pre class=\"lang-scala\">// Generate a simple dataset containing five values\nval booksDF = Seq(\n   (\"b00001\", \"Arthur Conan Doyle\", \"A study in scarlet\", 1887,11.33),\n   (\"b00023\", \"Arthur Conan Doyle\", \"A sign of four\", 1890,22.45),\n   (\"b01001\", \"Arthur Conan Doyle\", \"The adventures of Sherlock Holmes\", 1892,19.83),\n   (\"b00501\", \"Arthur Conan Doyle\", \"The memoirs of Sherlock Holmes\", 1893,14.22),\n   (\"b00300\", \"Arthur Conan Doyle\", \"The hounds of Baskerville\", 1901,12.25)\n).toDF(\"book_id\", \"book_author\", \"book_name\", \"book_pub_year\",\"book_price\")\nbooksDF.write\n  .mode(\"append\")\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -&gt; \"books\", \"keyspace\" -&gt; \"books_ks\", \"output.consistency.level\" -&gt; \"ALL\", \"ttl\" -&gt; \"10000000\"))\n  .save()\n</pre><h2 id=\"count-operation\">Count operation</h2>\n<h3 id=\"rdd-api\">RDD API</h3>\n<pre class=\"lang-scala\">sc.cassandraTable(\"books_ks\", \"books\").count\n</pre><p><strong>Output:</strong></p>\n<pre>res48: Long = 5\n</pre><h3 id=\"dataframe-api\">Dataframe API</h3>\n<p>Count against dataframes is currently not supported.  The sample below shows how to execute a dataframe count after persisting the dataframe to memory as a workaround.</p>\n<p>Choose a <a href=\"https://spark.apache.org/docs/2.2.0/rdd-programming-guide.html#which-storage-level-to-choose\" data-linktype=\"external\">storage option</a> from the following available options, to avoid running into \"out of memory\" issues:</p>\n<ul><li><p>MEMORY_ONLY: This is the default storage option. Stores RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and they are recomputed on the fly each time they're needed.</p>\n</li>\n<li><p>MEMORY_AND_DISK: Stores RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that don't fit on disk, and whenever required, read them from the location they are stored.</p>\n</li>\n<li><p>MEMORY_ONLY_SER (Java/Scala): Stores RDD as serialized Java objects- one-byte array per partition. This option is space-efficient when compared to deserialized objects, especially when using a fast serializer, but more CPU-intensive to read.</p>\n</li>\n<li><p>MEMORY_AND_DISK_SER (Java/Scala): This storage option is like MEMORY_ONLY_SER, the only difference is that it spills partitions that don't fit in the disk memory instead of recomputing them when they're needed.</p>\n</li>\n<li><p>DISK_ONLY: Stores the RDD partitions on the disk only.</p>\n</li>\n<li><p>MEMORY_ONLY_2, MEMORY_AND_DISK_2â€¦: Same as the levels above, but replicates each partition on two cluster nodes.</p>\n</li>\n<li><p>OFF_HEAP (experimental): Similar to MEMORY_ONLY_SER, but it stores the data in off-heap memory, and it requires off-heap memory to be enabled ahead of time. </p>\n</li>\n</ul><pre class=\"lang-scala\">//Workaround\nimport org.apache.spark.storage.StorageLevel\n//Read from source\nval readBooksDF = spark\n  .read\n  .cassandraFormat(\"books\", \"books_ks\", \"\")\n  .load()\n//Explain plan\nreadBooksDF.explain\n//Materialize the dataframe\nreadBooksDF.persist(StorageLevel.MEMORY_ONLY)\n//Subsequent execution against this DF hits the cache \nreadBooksDF.count\n//Persist as temporary view\nreadBooksDF.createOrReplaceTempView(\"books_vw\")\n</pre><h3 id=\"sql\">SQL</h3>\n<pre class=\"lang-sql\">select * from books_vw;\nselect count(*) from books_vw where book_pub_year &gt; 1900;\nselect count(book_id) from books_vw;\nselect book_author, count(*) as count from books_vw group by book_author;\nselect count(*) from books_vw;\n</pre><h2 id=\"average-operation\">Average operation</h2>\n<h3 id=\"rdd-api-1\">RDD API</h3>\n<pre class=\"lang-scala\">sc.cassandraTable(\"books_ks\", \"books\").select(\"book_price\").as((c: Double) =&gt; c).mean\n</pre><p><strong>Output:</strong></p>\n<pre>res24: Double = 16.016000175476073\n</pre><h3 id=\"dataframe-api-1\">Dataframe API</h3>\n<pre class=\"lang-scala\">spark\n  .read\n  .cassandraFormat(\"books\", \"books_ks\", \"\")\n  .load()\n  .select(\"book_price\")\n  .agg(avg(\"book_price\"))\n  .show\n</pre><p><strong>Output:</strong></p>\n<pre>+------------------+\n|   avg(book_price)|\n+------------------+\n|16.016000175476073|\n+------------------+\n</pre><h3 id=\"sql-1\">SQL</h3>\n<pre class=\"lang-sql\">select avg(book_price) from books_vw;\n</pre><p><strong>Output:</strong></p>\n<pre>16.016000175476073\n</pre><h2 id=\"min-operation\">Min operation</h2>\n<h3 id=\"rdd-api-2\">RDD API</h3>\n<pre class=\"lang-scala\">sc.cassandraTable(\"books_ks\", \"books\").select(\"book_price\").as((c: Float) =&gt; c).min\n</pre><p><strong>Output:</strong></p>\n<pre>res31: Float = 11.33\n</pre><h3 id=\"dataframe-api-2\">Dataframe API</h3>\n<pre class=\"lang-scala\">spark\n  .read\n  .cassandraFormat(\"books\", \"books_ks\", \"\")\n  .load()\n  .select(\"book_id\",\"book_price\")\n  .agg(min(\"book_price\"))\n  .show\n</pre><p><strong>Output:</strong></p>\n<pre>+---------------+\n|min(book_price)|\n+---------------+\n|          11.33|\n+---------------+\n</pre><h3 id=\"sql-2\">SQL</h3>\n<pre class=\"lang-sql\">select min(book_price) from books_vw;\n</pre><p><strong>Output:</strong></p>\n<pre>11.33\n</pre><h2 id=\"max-operation\">Max operation</h2>\n<h3 id=\"rdd-api-3\">RDD API</h3>\n<pre class=\"lang-scala\">sc.cassandraTable(\"books_ks\", \"books\").select(\"book_price\").as((c: Float) =&gt; c).max\n</pre><h3 id=\"dataframe-api-3\">Dataframe API</h3>\n<pre class=\"lang-scala\">spark\n  .read\n  .cassandraFormat(\"books\", \"books_ks\", \"\")\n  .load()\n  .select(\"book_price\")\n  .agg(max(\"book_price\"))\n  .show\n</pre><p><strong>Output:</strong></p>\n<pre>+---------------+\n|max(book_price)|\n+---------------+\n|          22.45|\n+---------------+\n</pre><h3 id=\"sql-3\">SQL</h3>\n<pre class=\"lang-sql\">select max(book_price) from books_vw;\n</pre><p><strong>Output:</strong>\n<code>22.45</code></p>\n<h2 id=\"sum-operation\">Sum operation</h2>\n<h3 id=\"rdd-api-4\">RDD API</h3>\n<pre class=\"lang-scala\">sc.cassandraTable(\"books_ks\", \"books\").select(\"book_price\").as((c: Float) =&gt; c).sum\n</pre><p><strong>Output:</strong></p>\n<pre>res46: Double = 80.08000087738037\n</pre><h3 id=\"dataframe-api-4\">Dataframe API</h3>\n<pre class=\"lang-scala\">spark\n  .read\n  .cassandraFormat(\"books\", \"books_ks\", \"\")\n  .load()\n  .select(\"book_price\")\n  .agg(sum(\"book_price\"))\n  .show\n</pre><p><strong>Output:</strong></p>\n<pre>+-----------------+\n|  sum(book_price)|\n+-----------------+\n|80.08000087738037|\n+-----------------+\n</pre><h3 id=\"sql-4\">SQL</h3>\n<pre class=\"lang-sql\">select sum(book_price) from books_vw;\n</pre><p><strong>Output:</strong></p>\n<pre>80.08000087738037\n</pre><h2 id=\"top-or-comparable-operation\">Top or comparable operation</h2>\n<h3 id=\"rdd-api-5\">RDD API</h3>\n<pre class=\"lang-scala\">val readCalcTopRDD = sc.cassandraTable(\"books_ks\", \"books\").select(\"book_name\",\"book_price\").sortBy(_.getFloat(1), false)\nreadCalcTopRDD.zipWithIndex.filter(_._2 &lt; 3).collect.foreach(println)\n//delivers the first top n items without collecting the rdd to the driver.\n</pre><p><strong>Output:</strong></p>\n<pre>(CassandraRow{book_name: A sign of four, book_price: 22.45},0)\n(CassandraRow{book_name: The adventures of Sherlock Holmes, book_price: 19.83},1)\n(CassandraRow{book_name: The memoirs of Sherlock Holmes, book_price: 14.22},2)\nreadCalcTopRDD: org.apache.spark.rdd.RDD[com.datastax.spark.connector.CassandraRow] = MapPartitionsRDD[430] at sortBy at command-2371828989676374:1\n</pre><h3 id=\"dataframe-api-5\">Dataframe API</h3>\n<pre class=\"lang-scala\">import org.apache.spark.sql.functions._\nval readBooksDF = spark.read.format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -&gt; \"books\", \"keyspace\" -&gt; \"books_ks\"))\n  .load\n  .select(\"book_name\",\"book_price\")\n  .orderBy(desc(\"book_price\"))\n  .limit(3)\n//Explain plan\nreadBooksDF.explain\n//Top\nreadBooksDF.show\n</pre><p><strong>Output:</strong></p>\n<pre>== Physical Plan ==\nTakeOrderedAndProject(limit=3, orderBy=[book_price#1840 DESC NULLS LAST], output=[book_name#1839,book_price#1840])\n+- *(1) Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@29cd5f58 [book_name#1839,book_price#1840] PushedFilters: [], ReadSchema: struct&lt;book_name:string,book_price:float&gt;\n+--------------------+----------+\n|           book_name|book_price|\n+--------------------+----------+\n|      A sign of four|     22.45|\n|The adventures of...|     19.83|\n|The memoirs of Sh...|     14.22|\n+--------------------+----------+\nimport org.apache.spark.sql.functions._\nreadBooksDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [book_name: string, book_price: float]\n</pre><h3 id=\"sql-5\">SQL</h3>\n<pre class=\"lang-sql\">select book_name,book_price from books_vw order by book_price desc limit 3;\n</pre><h2 id=\"next-steps\">Next steps</h2>\n<p>To perform table copy operations, see:</p>\n<ul><li><a href=\"https://docs.microsoft.com/bs-latn-ba/azure/cosmos-db/cassandra-spark-table-copy-ops\" data-linktype=\"relative-path\">Table copy operations</a></li>\n</ul>"}}]}},"pageContext":{"alternative_id":13120}}