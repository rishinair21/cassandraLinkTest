{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"From Cassandra to S3, with Spark","alternative_id":9068,"content":"<p>Apache Cassandra, a scalable and high-availability platform, is a good choice for high volume event management applications, such as large deployments of sensors. Applications include telematics data for large fleets, smart meter telemetry in electric, gas or water utility systems, and wide area weather station reporting. By analyzing this raw event data, system level intelligence can be extracted to discover trends and clustering along dimensions such as space, time and environmental parameters. Apache Spark enables this analysis, connecting directly to Cassandra and performing fault-tolerant processing with an architecture that scales out with Cassandra clustering.</p><p>This focus of this blog is showing how to connect Spark to Cassandra, analyze event data from Cassandra, and store the results of the analysis into S3, making it available for reporting or further analysis. The example uses 911 call event data collected over a number of weeks. For an example of Spark processing this same data from a Kafka based event stream, see <a href=\"https://objectpartners.com/2016/10/13/analyzing-kafka-data-streams-with-spark/\">this earlier blog</a>, based on the same 911 call event data set.</p><p><strong>Demonstration components</strong><br />To provide a demonstration that can be run locally, this blog describes a runnable Spark application <a href=\"https://github.com/waldmark/spark-cassandra-batch-s3-examples\">available from Github</a>,  along with Docker images that provide a stand alone (single node) Cassandra cluster and a local S3 object store. The Docker images are described at the end of this blog.</p><p><strong>Initializing Cassandra with demo data</strong><br />To provide a repeatable demo with minimal pre-conditions, the Cassandra schema is created and populated with demo data on application startup.</p><div><pre>    private void createSchema() {&#13;\n        try {&#13;\n            session.execute(dropKeyspaceCommand);&#13;\n        } catch (Exception e) {&#13;\n            System.out.println(e.getMessage());&#13;\n            System.exit(-1);&#13;\n        }&#13;\n&#13;\n        session.execute(createKeyspaceCommand);&#13;\n        session.execute(createRT911TableCommand);&#13;\n    }&#13;\n</pre></div><p> <br />The createSchema will drop the demo Cassandra keyspace if it exists, and then creates the keyspace and a table in the keyspace. These commands are CQL statements (Cassandra’s form of SQL) loaded into the application from an application.yml file:</p><div><pre>---&#13;\n# cassandra&#13;\ncassandra:&#13;\n  defaultQueryConsistency: ONE&#13;\n  defaultUpdateConsistency: ONE&#13;\n  updateTimeoutMillis: 5000&#13;\n  compression: LZ4&#13;\n  nodeAddress: localhost&#13;\n  host: 127.0.0.1&#13;\n&#13;\n  keyspaces:&#13;\n    - name: rt911&#13;\n      createCommand: \"CREATE KEYSPACE testkeyspace WITH replication = {'class':'SimpleStrategy', 'replication_factor':1};\"&#13;\n      dropCommand: \"DROP KEYSPACE IF EXISTS testkeyspace;\"&#13;\n      truncateCommand: \"TRUNCATE testkeyspace.rt911\"&#13;\n      tables:&#13;\n        - name: calls&#13;\n          createCommand:&#13;\n              CREATE TABLE testkeyspace.rt911 (&#13;\n                address varchar,&#13;\n                calltype varchar,&#13;\n                calltime varchar,&#13;\n                latitude varchar,&#13;\n                longitude varchar,&#13;\n                location varchar,&#13;\n                id varchar PRIMARY KEY);&#13;\n          insertPreparedStatementCommand:&#13;\n              INSERT INTO testkeyspace.rt911 (address, calltype, calltime, latitude, longitude, location, id)&#13;\n                VALUES ( ?, ?, ?, ?, ?, ?, ? );&#13;\n</pre></div><p>Once the schema and table are created, the application loads the 911 call data from a gzip csv file, populating Cassandra with the 911 call data. The application uses the Cassandra session to execute this CQL prepared statement:</p><div><pre>            insertPreparedStatementCommand:&#13;\n              INSERT INTO testkeyspace.rt911 (address, calltype, calltime, latitude, longitude, location, id)&#13;\n                VALUES ( ?, ?, ?, ?, ?, ?, ? );&#13;\n</pre></div><p> <br />When finished, the table ‘rt911’ in keyspace ‘testkeyspace’ contains call event data.</p><p><strong>Connecting Spark with Cassandra</strong><br />DataStax provides a ready to use <a href=\"https://github.com/datastax/spark-cassandra-connector\">Spark to Cassandra connector</a>. This connector exposes Cassandra data in terms of Spark structures, such as RDDs, supports writing Spark data to Cassandra, and allows CQL queries to be made from a Spark application. A quick start guide for the connector is also <a href=\"https://github.com/datastax/spark-cassandra-connector/blob/master/doc/0_quick_start.md\">available</a>. The Spark Cassandra Connector’s Java API utility class static method javaFunctions is used to read data from Cassandra. The data is read and transformed into Java objects that represent 911 calls, all in a single statemement:</p><div><pre>        JavaRDD callData = javaFunctions(sc)&#13;\n                .cassandraTable(\"testkeyspace\", \"rt911\")&#13;\n                .map(new Map911Call());&#13;\n</pre></div><p><strong>Analyzing the event data with Spark</strong><br />The 911 call data is filtered by event type based on simple classification using a contains match on the event type (in this case, event types that contain ‘Fire’).</p><div><pre>callData = callData.filter( c -&gt; (c.getCallType().matches(\"(?i:.*\\\\bFire\\\\b.*)\")));&#13;\n</pre></div><p> <br />The data is then mapped to key/value pairs, keyed by week of year:<br /></p><div><pre>        MapByCallDate mapByCallDate = new MapByCallDate();&#13;\n        return callData.mapToPair(mapByCallDate);&#13;\n</pre></div><p> <br />which uses this class to do the mapping:<br /></p><div><pre>public class MapByCallDate implements PairFunction {&#13;\n&#13;\n    @Override&#13;\n    public Tuple2 call(RealTime911 realTime911) throws Exception {&#13;\n        // create time bucket to group by dates (no time) - use MM/dd/yyyy&#13;\n        String timeBucket = realTime911.getDateTime().substring(0,10);&#13;\n        return new Tuple2(timeBucket, realTime911);&#13;\n    }&#13;\n}&#13;\n</pre></div><p> <br />The event object data is then grouped by date (MM/dd/yyyy –  e.g. 08/03/2015):<br /></p><div><pre>        JavaPairRDD&lt;String, Iterable&gt; groupedCalls = callsByCallDate.groupByKey();&#13;\n</pre></div><p> <br />The pair data is then transformed from Java objects into JSON documents, one for each date group, and written to a Map with the date as key and the JSON as the value:<br /></p><div><pre>        Map&lt;String, Iterable&gt; groupedCallMap = groupedCalls.collectAsMap();&#13;\n        Set keys = groupedCallMap.keySet();&#13;\n&#13;\n        ObjectMapper mapper = new ObjectMapper();&#13;\n&#13;\n        Map s3BucketData = new HashMap();&#13;\n        for(String key: keys) {&#13;\n            List jsonArrayElements = new ArrayList();&#13;\n            Iterable iterable = groupedCallMap.get(key);&#13;\n            Iterator iterator = iterable.iterator();&#13;\n            while(iterator.hasNext()) {&#13;\n                RealTime911 rt911 = iterator.next();&#13;\n                LOG.debug(rt911.getDateTime() + \" \" + rt911.getCallType());&#13;\n                try {&#13;\n                    String jsonRT911 = mapper.writeValueAsString(rt911);&#13;\n                    jsonArrayElements.add(jsonRT911);&#13;\n                } catch (JsonProcessingException e) {&#13;\n                    LOG.error(e.getMessage());&#13;\n                }&#13;\n            }&#13;\n&#13;\n            StringJoiner joiner = new StringJoiner(\",\");&#13;\n            jsonArrayElements.forEach(joiner::add);&#13;\n            s3BucketData.put(key, \"[\" + joiner.toString() + \"]\");&#13;\n        }&#13;\n</pre></div><p> <br />To store the data into S3, the AWS Java SDK is used to create an S3 client to the Scality S3 server.</p><div><pre>@Component&#13;\npublic class S3Client {&#13;\n&#13;\n    private AmazonS3Client s3 = null;&#13;\n&#13;\n    public S3Client() {&#13;\n        s3 = getClient();&#13;\n    }&#13;\n&#13;\n    private AmazonS3Client getClient() {&#13;\n        if(null == s3) {&#13;\n            System.setProperty(SDKGlobalConfiguration.DISABLE_CERT_CHECKING_SYSTEM_PROPERTY, \"true\");&#13;\n            BasicAWSCredentials credentials = new BasicAWSCredentials(\"accessKey1\", \"verySecretKey1\");&#13;\n            s3 = new AmazonS3Client(credentials);&#13;\n            S3ClientOptions options = S3ClientOptions.builder().setPathStyleAccess(true).build();&#13;\n            s3.setS3ClientOptions(options);&#13;\n            s3.setEndpoint(\"http://127.0.0.1:8000/\");&#13;\n        }&#13;\n        return s3;&#13;\n    }&#13;\n&#13;\n... the complete class can viewed in the GitHub project.&#13;\n }&#13;\n</pre></div><p> <br />Note that this client is written specifically for the Scality S3 server, which uses “accessKey1” as the access key and “verySecretKey1” as the secret key. These are the default AWSCredentials for a Scality S3 server.</p><p>This code saves the JSON documents to S3. First it removes the bucket if it already exists, then it creates the S3 bucket and writes key/value pairs to the bucket, storing a JSON document per date for which Cassandra has data. The bucket removal is to provide a repeatable demo, one that does not depend on, or conflict with previous runs.</p><div><pre>         try {&#13;\n            // remove the S3 bucket, this removes all objects in the bucket first&#13;\n            s3Client.removeBucket(bucketName);&#13;\n            LOG.info(\"S3 bucket \" + bucketName + \" deleted\");&#13;\n        } catch (Exception e) {&#13;\n            // bucket not deleted, may not have been there&#13;\n        }&#13;\n&#13;\n        try {&#13;\n            // create the bucket to start fresh&#13;\n            s3Client.createBucket(bucketName);&#13;\n            LOG.info(\"S3 bucket \" + bucketName + \" created\");&#13;\n&#13;\n            Set bucketKeys = s3BucketData.keySet();&#13;\n            // save to S3&#13;\n            for(String key: bucketKeys) {&#13;\n                s3Client.storeString(bucketName, key, s3BucketData.get(key));&#13;\n            }&#13;\n            LOG.info(\"finished saving JSON to S3 completed\");&#13;\n&#13;\n            LOG.info(\"displaying all JSON objects and their keys saved to \" + bucketName + \"\\n\");&#13;\n            for(String key: bucketKeys) {&#13;\n                String storedObject = s3Client.readS3Object(bucketName, key);&#13;\n                LOG.info(\"key: \" + key + \" value: \" + storedObject);&#13;\n            }&#13;\n        } catch (Exception e) {&#13;\n            LOG.error(e.getMessage());&#13;\n        } finally {&#13;\n            // clean up&#13;\n            s3Client.removeBucket(bucketName);&#13;\n        }&#13;\n</pre></div><p> <br />The end to end process of extraction, computing new information from this data, and storage into S3 for reporting, business intelligence analysis and other repurposing is a pattern applicable to a wide variety of needs.</p><p><strong>Installing Scality S3 server Docker image</strong><br />The S3 object store is based on the Scality S3 Docker image, which provides an S3 instance without requiring Amazon Web Service (AWS) credentials, and is a convenient way to develop against S3 without using AWS credits.</p><p>Installing Scality’s S3 server as a Docker image and running are described at <a href=\"https://hub.docker.com/r/scality/s3server\">https://hub.docker.com/r/scality/s3server</a></p><p><strong>Installing Cassandra Docker image</strong><br />The instructions on installing a docker image of Cassandra are described at <a href=\"https://hub.docker.com/_/cassandra/\">https://hub.docker.com/_/cassandra/</a></p><p>The instructions include installing the server image, as well as instructions for starting a cluster locally, and running Cassandra’s Query Language Shell (e.g. cqlsh) as a client to the Cassandra server.</p>"}}]}},"pageContext":{"alternative_id":9068}}