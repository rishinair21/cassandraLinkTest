{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"How to log in Apache Spark, a functional approach","alternative_id":12357,"content":"<p id=\"305f\" class=\"graf graf--p graf-after--h3\">Logging in Apache Spark comes very easy since Spark offers access to a <em class=\"markup--em markup--p-em\">log </em>object out of the box. Only some configuration setups need to be done. In a <a href=\"https://medium.com/@anicolaspp/how-to-log-in-apache-spark-f4204fad78a#.7asl0yplw\" data-href=\"https://medium.com/@anicolaspp/how-to-log-in-apache-spark-f4204fad78a#.7asl0yplw\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\"><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">previous post</em></strong></a><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\"> </em></strong>we have looked at how to do this while showing some problems that may arise. However, the solution presented might cause some problems at the moment we want to collect the logs since they are distributed across the entire cluster. Even if we utilize <strong class=\"markup--strong markup--p-strong\">Yarn</strong> log aggregation capabilities, there will be some contentions that might affect performance or even worse, in some cases we could end with log interleaves corrupting the nature of logs itself, they time ordered properties they should present.</p><p id=\"2119\" class=\"graf graf--p graf-after--p\">In order to solve these problems, a different approach needs to be taken, a functional one.</p><h4 id=\"6b67\" class=\"graf graf--h4 graf-after--p\">The Monad Writer</h4><p id=\"a193\" class=\"graf graf--p graf-after--h4\">I do not intend to go over the details about monads or in this particular case, the Monad Writer, if you are interested in learning more, take a look at this link (<a href=\"http://adit.io/posts/2013-04-17-functors,_applicatives,_and_monads_in_pictures.html\" data-href=\"http://adit.io/posts/2013-04-17-functors,_applicatives,_and_monads_in_pictures.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\"><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">functor, applicative, and monad</em></strong></a>) which is very informative about this topic.</p><p id=\"294b\" class=\"graf graf--p graf-after--p\">Just to put things in context, let’s say that the monad writer (<strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">writer) </em></strong>is a container that holds the current value of a computation in addition to history (log) of the value (set of transformation on the value).</p><p id=\"d207\" class=\"graf graf--p graf-after--p\">Because the <strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">writer </em></strong>monadic properties, it allows us to do functional transformations and we will soon see how everything sticks together.</p><h4 id=\"c555\" class=\"graf graf--h4 graf-after--p\">A Simplistic Log</h4><p id=\"a2c4\" class=\"graf graf--p graf-after--h4\">The following code demonstrates a simplistic log.</p><figure id=\"a739\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"7b79\" class=\"graf graf--p graf-after--figure\">The only thing to note is that logging is actually happening on the Spark driver, so we don’t have synchronization or contention problems. Everything starts to get complicated once we start distributing our computations.</p><p id=\"b811\" class=\"graf graf--p graf-after--p\">The following code won’t work (read <a href=\"https://medium.com/@anicolaspp/how-to-log-in-apache-spark-f4204fad78a#.7asl0yplw\" data-href=\"https://medium.com/@anicolaspp/how-to-log-in-apache-spark-f4204fad78a#.7asl0yplw\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\"><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">previous post</em></strong></a> to know why)</p><figure id=\"b690\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"dcc8\" class=\"graf graf--p graf-after--figure\">A solution to this was also presented in the <a href=\"https://medium.com/@anicolaspp/how-to-log-in-apache-spark-f4204fad78a#.7asl0yplw\" data-href=\"https://medium.com/@anicolaspp/how-to-log-in-apache-spark-f4204fad78a#.7asl0yplw\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\"><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">previous post</em></strong></a>, but it requires extra work to manage the logs.</p><p id=\"7b80\" class=\"graf graf--p graf-after--p\">Once we start logging on each node of the cluster, we need to go to each node, and collect each log file in order to make sense of whatever is in the logs. Hopefully, you are using some kind of tool to help you with this task, such as Splunk, Datalog, etc… yet you still need to know a lot of stuffs to get those logs into your system.</p><h4 id=\"b17f\" class=\"graf graf--h4 graf-after--p\">Our Data Set</h4><p id=\"b5b9\" class=\"graf graf--p graf-after--h4\">Our data set is a collection of the class Person that is going to be transformed while keeping an unified log of the operations on our data set.</p><p id=\"beb9\" class=\"graf graf--p graf-after--p\">Let’s suppose we want our data set to get loaded, then filter each people whose age is less than 20 years, and finally extract its name. It is a very silly example, but it will demonstrate how the logs are produced. You could replace these computations, but the ideas of building an unified log will remain.</p><h4 id=\"2b96\" class=\"graf graf--h4 graf-after--p\">Getting the Writer</h4><p id=\"4202\" class=\"graf graf--p graf-after--h4\">We are going to use <a href=\"http://typelevel.org/projects/\" data-href=\"http://typelevel.org/projects/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\"><em class=\"markup--em markup--p-em\">Typelevel / Cats</em></a><em class=\"markup--em markup--p-em\"> </em>library to import the monad writer, to do this we add the following line to our <strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">build.sbt</em></strong> file.</p><figure id=\"cb47\" class=\"graf graf--figure graf--iframe graf-after--p\"><h4 id=\"0424\" class=\"graf graf--h4 graf-after--figure\">Playing with our data</h4><p id=\"f54c\" class=\"graf graf--p graf-after--h4\">Now, let’s define the transformations we are going to use.</p><p id=\"472e\" class=\"graf graf--p graf-after--p\">First, let’s load the data.</p><figure id=\"d889\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"7ae9\" class=\"graf graf--p graf-after--figure\">In here the <strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">~&gt; </em></strong>operation is defined via implicit conversions as follows.</p><figure id=\"4697\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"ddf8\" class=\"graf graf--p graf-after--figure\">If you look closely, our loading operation is not returning an RDD, in fact, it returns the monad writer that keeps track of the logs.</p><p id=\"25dc\" class=\"graf graf--p graf-after--p\">Let’s define the filter that we want to apply over the collection of users.</p><figure id=\"6021\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"baac\" class=\"graf graf--p graf-after--figure\">Again, we are applying the same function (<strong class=\"markup--strong markup--p-strong\">~&gt;</strong>) to keep track of this transformation.</p><p id=\"b90b\" class=\"graf graf--p graf-after--p\">Lastly, we define the mapping which follows the same pattern we just saw.</p><figure id=\"cc92\" class=\"graf graf--figure graf--iframe graf-after--p\"><h4 id=\"a1d0\" class=\"graf graf--h4 graf-after--figure\">Putting it together</h4><p id=\"db37\" class=\"graf graf--p graf-after--h4\">So far we have only defined our transformations, but we need to stick them together. Scala <strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">for</em></strong> is a very convenient way to work with monadic structures. Let’s see how.</p><figure id=\"d74c\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"ab50\" class=\"graf graf--p graf-after--figure\">Please note that <strong class=\"markup--strong markup--p-strong\">result</strong> is of type: <strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">Writer[List[String], RDD[String]]</em></strong>.</p><p id=\"34a3\" class=\"graf graf--p graf-after--p\">Calling <strong class=\"markup--strong markup--p-strong\">result.run </strong>will give us the <strong class=\"markup--strong markup--p-strong\">log: List[String] </strong>and the final computation expressed by <strong class=\"markup--strong markup--p-strong\">rdd</strong>: <strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">RDD[String]</em></strong>.</p><p id=\"7ed8\" class=\"graf graf--p graf-after--p\">At this point we could use <em class=\"markup--em markup--p-em\">Spark logger </em>to write down the log generated by the chain of transformations. Note that this operation will be executed on Spark master which implies that one log file will be created with all the log information. Also, we are removing potential contention problems during the log writes. In addition, we are not locking the log file, which avoid performance issues by creating and writing to the file in a serial way.</p><h4 id=\"034d\" class=\"graf graf--h4 graf-after--p\">Conclusions</h4><p id=\"c5cb\" class=\"graf graf--p graf-after--h4\">We have improved how we log on Apache Spark by using the <em class=\"markup--em markup--p-em\">Monad Writer</em>. This functional approach allows us to distribute the creation of logs along with our computations, something Spark knows well how to do. However, instead of writing the logs on each worker node, we are collecting them back to the master to write them down. This mechanism has certain advantages over our previous implementation. We now control exactly how and when our logs are going to be written down, we boost performance by removing IO operations on the worker nodes, we also removed synchronization issues by writing the logs in a serial way, and we avoid the hazard of fishing logs across our entire cluster.</p><blockquote id=\"4cff\" class=\"graf graf--blockquote graf-after--p\"><a href=\"http://bit.ly/Hackernoon\" data-href=\"http://bit.ly/Hackernoon\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"noopener nofollow noopener\" target=\"_blank\">Hacker Noon</a><div> is how hackers start their afternoons. We’re a part of the </div><a href=\"http://bit.ly/atAMIatAMI\" data-href=\"http://bit.ly/atAMIatAMI\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener nofollow noopener\" target=\"_blank\">@AMI</a><div>family. We are now </div><a href=\"http://bit.ly/hackernoonsubmission\" data-href=\"http://bit.ly/hackernoonsubmission\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener nofollow noopener\" target=\"_blank\">accepting submissions</a><div> and happy to </div><a href=\"mailto:partners@amipublications.com\" data-href=\"mailto:partners@amipublications.com\" class=\"markup--anchor markup--blockquote-anchor\" target=\"_blank\">discuss advertising &amp;sponsorship</a><div> opportunities.</div></blockquote><blockquote id=\"dca4\" class=\"graf graf--blockquote graf-after--blockquote\"><div>To learn more, <a href=\"https://goo.gl/4ofytp\" data-href=\"https://goo.gl/4ofytp\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">read our about page</a>, <a href=\"http://bit.ly/HackernoonFB\" data-href=\"http://bit.ly/HackernoonFB\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">like/message us on Facebook</a>, or simply, <a href=\"https://goo.gl/k7XYbx\" data-href=\"https://goo.gl/k7XYbx\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">tweet/DM @HackerNoon.</a></div></blockquote><blockquote id=\"708a\" class=\"graf graf--blockquote graf-after--blockquote\"><div>If you enjoyed this story, we recommend reading our <a href=\"http://bit.ly/hackernoonlatestt\" data-href=\"http://bit.ly/hackernoonlatestt\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener nofollow noopener\" target=\"_blank\">latest tech stories</a> and <a href=\"https://hackernoon.com/trending\" data-href=\"https://hackernoon.com/trending\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener nofollow\" target=\"_blank\">trending tech stories</a>. Until next time, don’t take the realities of the world for granted!</div></blockquote><figure id=\"0f71\" class=\"graf graf--figure graf-after--blockquote graf--trailing\"><div class=\"aspectRatioPlaceholder is-locked\"><a href=\"https://bit.ly/2O1yNyY\" data-href=\"https://bit.ly/2O1yNyY\" class=\"graf-imageAnchor\" data-action=\"image-link\" data-action-observe-only=\"true\"><img class=\"graf-image\" data-image-id=\"1*QCV7h713dLgy5COZTyBLdQ@2x.png\" data-width=\"1400\" data-height=\"701\" src=\"https://cdn-images-1.medium.com/max/1600/1*QCV7h713dLgy5COZTyBLdQ@2x.png\" alt=\"image\" /></a></div></figure></figure></figure></figure></figure></figure></figure></figure></figure>"}}]}},"pageContext":{"alternative_id":12357}}