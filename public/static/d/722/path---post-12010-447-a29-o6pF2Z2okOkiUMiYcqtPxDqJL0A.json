{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Spark SQL Against Cassandra Example - DZone Database","alternative_id":12010,"content":"<div class=\"content-html\" itemprop=\"text\"><p>Spark SQL is awesome.  It allows you to query any Resilient Distributed Dataset (RDD) using SQL (including data stored in Cassandra!).</p><p>First thing to do is to create a SQLContext from your SparkContext.  I'm using Java so... <br />(sorry -- I'm still not hip enough for Scala) </p>\n<div>\n <pre lang=\"text/x-java\">JavaSparkContext context =new JavaSparkContext(conf);\nJavaSQLContext sqlContext =new JavaSQLContext(context);</pre>\n</div>\n<p> Now you have a SQLContext, but you have no data.  Go ahead and create an RDD, just like you would in regular Spark:</p>\n<div>\n <pre lang=\"text/x-java\">JavaPairRDD&lt;Integer, Product&gt; productsRDD = \n  javaFunctions(context).cassandraTable(\"test_keyspace\", \"products\",\n    productReader).keyBy(new Function&lt;Product, Integer&gt;() {\n  @Override\n  public Integer call(Product product) throws Exception {\n    return product.getId();\n  }\n});</pre>\n <br /></div>\n<p> (The example above comes from the <a href=\"https://github.com/boneill42/spark-on-cassandra-quickstart\" rel=\"nofollow\">spark-on-cassandra-quickstart project</a>, as described in <a href=\"http://brianoneill.blogspot.com/2015/04/holy-momentum-batman-spark-and.html\" rel=\"nofollow\">my previous post</a>.)</p>\n<p> Now that we have a plain vanilla RDD,  we need to spice it up with a schema, and let the sqlContext know about it.  We can do that with the following lines:</p>\n<div>\n <pre lang=\"text/x-java\">JavaSchemaRDD schemaRDD =   sqlContext.applySchema(productsRDD.values(), Product.class);        \nsqlContext.registerRDDAsTable(schemaRDD, \"products\");   </pre>\n <br /></div>\n<p> Shazam.  Now your sqlContext is ready for querying.  Notice that it inferred the schema from the Java bean. (Product.class).  (Next blog post, I'll show how to do this dynamically)</p>\n<p> You can prime the pump with a:</p>\n<div>\n <pre lang=\"text/x-java\">System.out.println(\"Total Records = [\" + productsRDD.count() + \"]\");</pre>\n <br /></div>\n<p> The count operation forces Spark to load the data into memory, which makes queries like the following lightning fast:</p>\n<div>\n <pre lang=\"text/x-java\">JavaSchemaRDD result = sqlContext.sql(\"SELECT id from products WHERE price &lt; 0.50\");\nfor (Row row : result.collect()){\n  System.out.println(row);\n}</pre>\n</div>\n<p> That's it.  You're off to the SQL races.</p>\n<div>\n <div>\n  <p> <em>P.S.  If you try querying the sqlContext without applying a schema and/or without registering the RDD as a table, you may see something similar to this:</em></p>\n  <pre lang=\"text/x-java\">Exception in thread \"main\" org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Unresolved attributes: 'id, tree:\n'Project ['id]\n 'Filter ('price &lt; 0.5)\n  NoRelation$\n</pre>\n </div>\n</div></div><div class=\"content-html\" itemprop=\"text\"><a>\n                        <img class=\"pub-image\" width=\"420\" itemprop=\"image\" src=\"src\" alt=\"image\" /></a></div>"}}]}},"pageContext":{"alternative_id":12010}}