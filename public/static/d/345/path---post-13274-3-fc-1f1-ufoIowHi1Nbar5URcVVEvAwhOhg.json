{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Thousand Instances of Cassandra using Kubernetes Pet Set","alternative_id":13274,"content":"<p><em>Editor’s note: this post is part of a <a href=\"https://kubernetes.io/blog/2016/07/five-days-of-kubernetes-1.3\" target=\"_blank\">series of in-depth articles</a> on what’s new in Kubernetes 1.3</em></p><p>For the <a href=\"https://kubernetes.io/blog/2016/07/kubernetes-1.3-bridging-cloud-native-and-enterprise-workloads\" target=\"_blank\">Kubernetes 1.3 launch</a>, we wanted to put the new Pet Set through its paces. By testing a thousand instances of <a href=\"https://cassandra.apache.org/\" target=\"_blank\">Cassandra</a>, we could make sure that Kubernetes 1.3 was production ready. Read on for how we adapted Cassandra to Kubernetes, and had our largest deployment ever.</p><p>It’s fairly straightforward to use containers with basic stateful applications today. Using a persistent volume, you can mount a disk in a pod, and ensure that your data lasts beyond the life of your pod. However, with deployments of distributed stateful applications, things can become more tricky. With Kubernetes 1.3, the new <a href=\"http://kubernetes.io/docs/user-guide/petset/\" target=\"_blank\">Pet Set</a> component makes everything much easier. To test this new feature out at scale, we decided to host the Greek Pet Monster Races! We raced Centaurs and other Ancient Greek Monsters over hundreds of thousands of races across multiple availability zones.</p><p><a href=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/42/Cassandra1.jpeg/283px-Cassandra1.jpeg\" target=\"_blank\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/42/Cassandra1.jpeg/283px-Cassandra1.jpeg\" alt=\"File:Cassandra1.jpeg\" /></a><br />As many of you know Kubernetes is from the Ancient Greek: κυβερνήτης. This means helmsman, pilot, steersman, or ship master. So in order to keep track of race results, we needed a data store, and we choose Cassandra. Κασσάνδρα, Cassandra who was the daughter of King of Priam and Queen Hecuba of Troy. With multiple references to the ancient Greek language, we thought it would be appropriate to race ancient Greek monsters.</p><p>From there the story kinda goes sideways because Cassandra was actually the Pets as well. Read on and we will explain.</p><p>One of the new exciting features in Kubernetes 1.3 is Pet Set. In order to organize the deployment of containers inside of Kubernetes, different deployment mechanisms are available. Examples of these components include Resource Controllers and Daemon Set. Pet Sets is a new feature that delivers the capability to deploy containers, as Pets, inside of Kubernetes. Pet Sets provide a guarantee of identity for various aspects of the pet / pod deployment: DNS name, consistent storage, and ordered pod indexing. Previously, using components like Deployments and Replication Controllers, would only deploy an application with a weak uncoupled identity. A weak identity is great for managing applications such as microservices, where service discovery is important, the application is stateless, and the naming of individual pods does not matter. Many software applications do require strong identity, including many different types of distributed stateful systems. Cassandra is a great example of a distributed application that requires consistent network identity, and stable storage.</p><p>Pet Sets provides the following capabilities:</p><ul><li>A stable hostname, available to others in DNS. Number is based off of the Pet Set name and starts at zero. For example cassandra-0.</li><li>An ordinal index of Pets. 0, 1, 2, 3, etc.</li><li>Stable storage linked to the ordinal and hostname of the Pet.</li><li>Peer discovery is available via DNS. With Cassandra the names of the peers are known before the Pets are created.</li><li>Startup and Teardown ordering. Which numbered Pet is going to be created next is known, and which Pet will be destroyed upon reducing the Pet Set size. This feature is useful for such admin tasks as draining data from a Pet, when reducing the size of a cluster.</li></ul><p>If your application has one or more of these requirements, then it may be a candidate for Pet Set.<br />A relevant analogy is that a Pet Set is composed of Pet dogs. If you have a white, brown or black dog and the brown dog runs away, you can replace it with another brown dog no one would notice. If over time you can keep replacing your dogs with only white dogs then someone would notice. Pet Set allows your application to maintain the unique identity or hair color of your Pets.</p><p>Example workloads for Pet Set:</p><ul><li>Clustered software like Cassandra, Zookeeper, etcd, or Elastic require stable membership.</li><li>Databases like MySQL or PostgreSQL that require a single instance attached to a persistent volume at any time.</li></ul><p>Only use Pet Set if your application requires some or all of these properties. Managing pods as stateless replicas is vastly easier.</p><p>So back to our races!</p><p>As we have mentioned, Cassandra was a perfect candidate to deploy via a Pet Set. A Pet Set is much like a <a href=\"http://kubernetes.io/docs/user-guide/replication-controller/\" target=\"_blank\">Replica Controller</a> with a few new bells and whistles. Here’s an example YAML manifest:</p><pre>apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: cassandra\n  name: cassandra\nspec:\n  clusterIP: None\n  ports:\n    - port: 9042\n  selector:\n    app: cassandra-data\n----\n# new API name\napiVersion: \"apps/v1alpha1\"\nkind: PetSet\nmetadata:\n  name: cassandra\nspec:\n  serviceName: cassandra\n  # replicas are the same as used by Replication Controllers\n  # except pets are deployed in order 0, 1, 2, 3, etc\n  replicas: 5\n  template:\n    metadata:\n      annotations:\n        pod.alpha.kubernetes.io/initialized: \"true\"\n      labels:\n        app: cassandra-data\n    spec:\n      # just as other component in Kubernetes one\n      # or more containers are deployed\n      containers:\n      - name: cassandra\n        image: \"cassandra-debian:v1.1\"\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 7000\n          name: intra-node\n        - containerPort: 7199\n          name: jmx\n        - containerPort: 9042\n          name: cql\n        resources:\n          limits:\n            cpu: \"4\"\n            memory: 11Gi\n          requests:\n           cpu: \"4\"\n           memory: 11Gi\n        securityContext:\n          privileged: true\n        env:\n          - name: MAX\\_HEAP\\_SIZE\n            value: 8192M\n          - name: HEAP\\_NEWSIZE\n            value: 2048M\n          # this is relying on guaranteed network identity of Pet Sets, we\n          # will know the name of the Pets / Pod before they are created\n          - name: CASSANDRA\\_SEEDS\n            value: \"cassandra-0.cassandra.default.svc.cluster.local,cassandra-1.cassandra.default.svc.cluster.local\"\n          - name: CASSANDRA\\_CLUSTER\\_NAME\n            value: \"OneKDemo\"\n          - name: CASSANDRA\\_DC\n            value: \"DC1-Data\"\n          - name: CASSANDRA\\_RACK\n            value: \"OneKDemo-Rack1-Data\"\n          - name: CASSANDRA\\_AUTO\\_BOOTSTRAP\n            value: \"false\"\n          # this variable is used by the read-probe looking\n          # for the IP Address in a `nodetool status` command\n          - name: POD\\_IP\n            valueFrom:\n              fieldRef:\n                fieldPath: status.podIP\n        readinessProbe:\n          exec:\n            command:\n            - /bin/bash\n            - -c\n            - /ready-probe.sh\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n        # These volume mounts are persistent. They are like inline claims,\n        # but not exactly because the names need to match exactly one of\n        # the pet volumes.\n        volumeMounts:\n        - name: cassandra-data\n          mountPath: /cassandra\\_data\n  # These are converted to volume claims by the controller\n  # and mounted at the paths mentioned above.  Storage can be automatically\n  # created for the Pets depending on the cloud environment.\n  volumeClaimTemplates:\n  - metadata:\n      name: cassandra-data\n      annotations:\n        volume.alpha.kubernetes.io/storage-class: anything\n    spec:\n      accessModes: [\"ReadWriteOnce\"]\n      resources:\n        requests:\n          storage: 380Gi\n</pre><p>You may notice that these containers are on the rather large size, and it is not unusual to run Cassandra in production with 8 CPU and 16GB of ram. There are two key new features that you will notice above; dynamic volume provisioning, and of course Pet Set. The above manifest will create 5 Cassandra Pets / Pods starting with the number 0: cassandra-data-0, cassandra-data-1, etc.</p><p>In order to generate data for the races, we used another Kubernetes feature called Jobs. Simple python code was written to generate the random speed of the monster for every second of the race. Then that data, position information, winners, other data points, and metrics were stored in Cassandra. To visualize the data, we used JHipster to generate a AngularJS UI with Java services, and then used D3 for graphing.</p><p>An example of one of the Jobs:</p><pre>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: pet-race-giants\n  labels:\n    name: pet-races\nspec:\n  parallelism: 2\n  completions: 4\n  template:\n    metadata:\n      name: pet-race-giants\n      labels:\n        name: pet-races\n    spec:\n      containers:\n      - name: pet-race-giants\n        image: py3numpy-job:v1.0\n        command: [\"pet-race-job\", --length=100\", \"--pet=Giants\", \"--scale=3\"]\n        resources:\n          limits:\n            cpu: \"2\"\n          requests:\n            cpu: \"2\"\n      restartPolicy: Never\n</pre><p><a href=\"https://upload.wikimedia.org/wikipedia/commons/0/0e/Polyphemus.gif\" target=\"_blank\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/0/0e/Polyphemus.gif\" alt=\"File:Polyphemus.gif\" /></a>Since we are talking about Monsters, we had to go big. We deployed 1,009 minion nodes to <a href=\"https://cloud.google.com/compute/\" target=\"_blank\">Google Compute Engine</a> (GCE), spread across 4 zones, running a custom version of the Kubernetes 1.3 beta. We ran this demo on beta code since the demo was being set up before the 1.3 release date. For the minion nodes, GCE virtual machine n1-standard-8 machine size was chosen, which is vm with 8 virtual CPUs and 30GB of memory. It would allow for a single instance of Cassandra to run on one node, which is recommended for disk I/O.</p><p>Then the pets were deployed! One thousand of them, in two different Cassandra Data Centers. Cassandra distributed architecture is specifically tailored for multiple-data center deployment. Often multiple Cassandra data centers are deployed inside the same physical or virtual data center, in order to separate workloads. Data is replicated across all data centers, but workloads can be different between data centers and thus application tuning can be different. Data centers named ‘DC1-Analytics’ and ‘DC1-Data’ where deployed with 500 pets each. The race data was created by the python Batch Jobs connected to DC1-Data, and the JHipster UI was connected DC1-Analytics.</p><p>Here are the final numbers:</p><ul><li>8,072 Cores. The master used 24, minion nodes used the rest</li><li>1,009 IP addresses</li><li>1,009 routes setup by Kubernetes on Google Cloud Platform</li><li>100,510 GB persistent disk used by the Minions and the Master</li><li>380,020 GB SSD disk persistent disk. 20 GB for the master and 340 GB per Cassandra Pet.</li><li>1,000 deployed instances of Cassandra\nYes we deployed 1,000 pets, but one really did not want to join the party! Technically with the Cassandra setup, we could have lost 333 nodes without service or data loss.<br /></li></ul><h3 id=\"limitations-with-pet-sets-in-1-3-release\">Limitations with Pet Sets in 1.3 Release</h3><ul><li>Pet Set is an alpha resource not available in any Kubernetes release prior to 1.3.</li><li>The storage for a given pet must either be provisioned by a dynamic storage provisioner based on the requested storage class, or pre-provisioned by an admin.</li><li>Deleting the Pet Set will not delete any pets or Pet storage. You will need to delete your Pets and possibly its storage by hand.</li><li>All Pet Sets currently require a “governing service”, or a Service responsible for the network identity of the pets. The user is responsible for this Service.</li><li>Updating an existing Pet Set is currently a manual process. You either need to deploy a new Pet Set with the new image version or orphan Pets one by one and update their image, which will join them back to the cluster.</li></ul><h4 id=\"resources-and-references\">Resources and References</h4><ul><li>The source code for the demo is available on <a href=\"https://github.com/k8s-for-greeks/gpmr\" target=\"_blank\">GitHub</a>: (Pet Set examples will be merged into the Kubernetes Cassandra Examples).</li><li>More information about <a href=\"http://kubernetes.io/docs/user-guide/jobs/\" target=\"_blank\">Jobs</a></li><li><a href=\"https://github.com/kubernetes/kubernetes.github.io/blob/release-1.3/docs/user-guide/petset.md\" target=\"_blank\">Documentation for Pet Set</a></li><li>Image credits: Cassandra <a href=\"https://commons.wikimedia.org/wiki/File:Cassandra1.jpeg\" target=\"_blank\">image</a> and Cyclops <a href=\"https://commons.wikimedia.org/wiki/File:Polyphemus.gif\" target=\"_blank\">image</a></li></ul><p><em>– Chris Love, Senior DevOps Open Source Consultant for <a href=\"https://www.datapipe.com/\" target=\"_blank\">Datapipe</a>. <a href=\"https://twitter.com/chrislovecnm/\" target=\"_blank\">Twitter @chrislovecnm</a></em></p><div class=\"PageNavigation\"><h4><a class=\"button\" href=\"https://kubernetes.io/blog/2016/07/autoscaling-in-kubernetes/\">« Prev</a></h4><h4><a class=\"button\" href=\"https://kubernetes.io/blog/2016/07/stateful-applications-in-containers-kubernetes/\">Next &gt;&gt;</a></h4></div>"}}]}},"pageContext":{"alternative_id":13274}}