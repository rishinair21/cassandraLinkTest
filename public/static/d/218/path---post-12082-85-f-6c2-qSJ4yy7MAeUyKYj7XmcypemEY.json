{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Scaling Time Series Data Storage — Part I","alternative_id":12082,"content":"<div class=\"section-divider\"><hr class=\"section-divider\" /></div><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><h1 id=\"ffa0\" class=\"graf graf--h3 graf--leading graf--title\">Scaling Time Series Data Storage — Part I</h1><h2 id=\"32f2\" class=\"graf graf--h4 graf-after--h3 graf--subtitle\">by <a href=\"https://www.linkedin.com/in/ketan-duvedi-19395019/\" data-href=\"https://www.linkedin.com/in/ketan-duvedi-19395019/\" class=\"markup--anchor markup--h4-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Ketan Duvedi</a>, <a href=\"https://www.linkedin.com/in/jinhua-li-00830744/\" data-href=\"https://www.linkedin.com/in/jinhua-li-00830744/\" class=\"markup--anchor markup--h4-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Jinhua Li</a>, <a href=\"https://www.linkedin.com/in/dhruv-garg-1362862/\" data-href=\"https://www.linkedin.com/in/dhruv-garg-1362862/\" class=\"markup--anchor markup--h4-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Dhruv Garg</a>, <a href=\"https://www.linkedin.com/in/philfish/\" data-href=\"https://www.linkedin.com/in/philfish/\" class=\"markup--anchor markup--h4-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Philip Fisher-Ogden</a></h2><h3 id=\"4d6b\" class=\"graf graf--h3 graf-after--h4\">Introduction</h3><p id=\"f574\" class=\"graf graf--p graf-after--h3\">The growth of internet connected devices has led to a vast amount of easily accessible time series data. Increasingly, companies are interested in mining this data to derive useful insights and make data-informed decisions. Recent technology advancements have improved the efficiency of collecting, storing and analyzing time series data, spurring an increased appetite to consume this data. However this explosion of time series data can overwhelm most initial time series data architectures.</p><p id=\"0dbf\" class=\"graf graf--p graf-after--p\">Netflix, being a data-informed company, is no stranger to these challenges and over the years has enhanced its solutions to manage the growth. In this 2-part blog post series, we will share how Netflix has evolved a time series data storage architecture through multiple increases in scale.</p><h3 id=\"69a3\" class=\"graf graf--h3 graf-after--p\">Time Series Data — Member Viewing History</h3><p id=\"11bc\" class=\"graf graf--p graf-after--h3\">Netflix members watch over 140 million hours of content per day. Each member provides several data points while viewing a title and they are stored as viewing records. Netflix analyzes the viewing data and provides real time accurate bookmarks and personalized recommendations as described in these posts:</p><ul class=\"postList\"><li id=\"0451\" class=\"graf graf--li graf-after--p\"><a href=\"https://medium.com/netflix-techblog/netflixs-viewing-data-how-we-know-where-you-are-in-house-of-cards-608dd61077da\" data-href=\"https://medium.com/netflix-techblog/netflixs-viewing-data-how-we-know-where-you-are-in-house-of-cards-608dd61077da\" class=\"markup--anchor markup--li-anchor\" target=\"_blank\">How we know where you are in a show?</a></li><li id=\"a5e4\" class=\"graf graf--li graf-after--li\"><a href=\"https://medium.com/netflix-techblog/to-be-continued-helping-you-find-shows-to-continue-watching-on-7c0d8ee4dab6\" data-href=\"https://medium.com/netflix-techblog/to-be-continued-helping-you-find-shows-to-continue-watching-on-7c0d8ee4dab6\" class=\"markup--anchor markup--li-anchor\" target=\"_blank\">Helping you find shows to continue watching on Netflix</a></li></ul><p id=\"fcbe\" class=\"graf graf--p graf-after--li\">Viewing history data increases along the following 3 dimensions:</p><ol class=\"postList\"><li id=\"5ebd\" class=\"graf graf--li graf-after--p\">As time progresses, more viewing data is stored for each member.</li><li id=\"ec3d\" class=\"graf graf--li graf-after--li\">As member count grows, viewing data is stored for more members.</li><li id=\"350f\" class=\"graf graf--li graf-after--li\">As member monthly viewing hours increase, more viewing data is stored for each member.</li></ol><p id=\"6aa2\" class=\"graf graf--p graf-after--li\">As Netflix streaming has grown to 100M+ global members in its first 10 years there has been a massive increase in viewing history data. In this blog post we will focus on how we approached the big challenge of scaling storage of viewing history data.</p><h3 id=\"ed8f\" class=\"graf graf--h3 graf-after--p\">Start Simple</h3><p id=\"c308\" class=\"graf graf--p graf-after--h3\">The first cloud-native version of the viewing history storage architecture used Cassandra for the following reasons:</p><ul class=\"postList\"><li id=\"6268\" class=\"graf graf--li graf-after--p\">Cassandra has good support for <a href=\"https://academy.datastax.com/resources/getting-started-time-series-data-modeling\" data-href=\"https://academy.datastax.com/resources/getting-started-time-series-data-modeling\" class=\"markup--anchor markup--li-anchor\" rel=\"nofollow noopener\" target=\"_blank\">modelling time series data</a> wherein each row can have dynamic number of columns.</li><li id=\"4d7f\" class=\"graf graf--li graf-after--li\">The viewing history data write to read ratio is about 9:1. Since Cassandra is <a href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/dml/dml_manage_ondisk_c.html\" data-href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/dml/dml_manage_ondisk_c.html\" class=\"markup--anchor markup--li-anchor\" rel=\"nofollow noopener\" target=\"_blank\">highly efficient with writes</a>, this write heavy workload is a good fit for Cassandra.</li><li id=\"b341\" class=\"graf graf--li graf-after--li\">Considering the <a href=\"https://en.wikipedia.org/wiki/CAP_theorem\" data-href=\"https://en.wikipedia.org/wiki/CAP_theorem\" class=\"markup--anchor markup--li-anchor\" rel=\"nofollow noopener\" target=\"_blank\">CAP theorem</a>, the team favors eventual consistency over loss of availability. Cassandra supports this tradeoff via <a href=\"http://docs.datastax.com/en/archived/cassandra/2.0/cassandra/dml/dml_config_consistency_c.html\" data-href=\"http://docs.datastax.com/en/archived/cassandra/2.0/cassandra/dml/dml_config_consistency_c.html\" class=\"markup--anchor markup--li-anchor\" rel=\"nofollow noopener\" target=\"_blank\">tunable consistency</a>.</li></ul><p id=\"9944\" class=\"graf graf--p graf-after--li\">In the initial approach, each member’s viewing history was stored in Cassandra in a single row with row key:CustomerId. This horizontal partitioning enabled effective scaling with member growth and made the common use case of reading a member’s entire viewing history very simple and efficient. However as member count increased and, more importantly, each member streamed more and more titles, the row sizes as well as the overall data size increased. Over time, this resulted in high storage and operation cost as well as slower performance for members with large viewing history.</p><p id=\"bae4\" class=\"graf graf--p graf-after--p\">The following figure illustrates the read and write flows of the initial data model:</p><figure id=\"7224\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*QHOtYeP8F9J46FRq.\" data-width=\"720\" data-height=\"399\" data-action=\"zoom\" data-action-value=\"0*QHOtYeP8F9J46FRq.\" src=\"https://cdn-images-1.medium.com/max/1600/0*QHOtYeP8F9J46FRq.\" alt=\"image\" /></div><figcaption class=\"imageCaption\">Figure 1: Single Table Data Model</figcaption></div></figure><h3 id=\"2cea\" class=\"graf graf--h3 graf-after--figure\">Write Flow</h3><p id=\"9835\" class=\"graf graf--p graf-after--h3\">One viewing record was inserted as a new column when a member started playing a title. That viewing record was updated after member paused or stopped the title. This single column write was fast and efficient.</p><h3 id=\"2494\" class=\"graf graf--h3 graf-after--p\">Read Flows</h3><p id=\"dd03\" class=\"graf graf--p graf-after--h3\"><em class=\"markup--em markup--p-em\">Whole row read to retrieve all viewing records for one member:</em> The read was efficient when the number of records per member was small. As a member watched more titles, the number of viewing records increased. Reading rows with a large number of columns put additional stress on Cassandra that negatively impacted read latencies.</p><p id=\"443e\" class=\"graf graf--p graf-after--p\"><em class=\"markup--em markup--p-em\">Time range query to read a time slice of a member’s data:</em> This resulted in the same inconsistent performance as above depending on the number of viewing records within the specified time range.</p><p id=\"0800\" class=\"graf graf--p graf-after--p\"><em class=\"markup--em markup--p-em\">Whole row read via pagination for large viewing history:</em> This was better for Cassandra as it wasn’t waiting for all the data to be ready before sending it back. This also avoided client timeouts. However it increased overall latency to read the whole row as the number of viewing records increased.</p><h3 id=\"febd\" class=\"graf graf--h3 graf-after--p\">Slowdown Reasons</h3><p id=\"65d1\" class=\"graf graf--p graf-after--h3\">Let’s look at some of the Cassandra internals to understand why our initial simple design slowed down. As the data grew, the number of <a href=\"https://wiki.apache.org/cassandra/MemtableSSTable\" data-href=\"https://wiki.apache.org/cassandra/MemtableSSTable\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">SSTables</a> increased accordingly. Since only recent data was in memory, in many cases <a href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbInternals/dbIntAboutReads.html\" data-href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbInternals/dbIntAboutReads.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">both the memtables and SSTables had to be read</a> to retrieve viewing history. This had a negative impact on read latency. Similarly <a href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbInternals/dbIntHowDataMaintain.html\" data-href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbInternals/dbIntHowDataMaintain.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Compaction</a> took more IOs and time as the data size increased. <a href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbArch/archRepairNodesReadRepair.html\" data-href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbArch/archRepairNodesReadRepair.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Read repair</a> and <a href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbArch/archAntiEntropyRepair.html\" data-href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbArch/archAntiEntropyRepair.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Full column repair</a> became slower as rows got wider.</p><h3 id=\"988b\" class=\"graf graf--h3 graf-after--p\">Caching Layer</h3><p id=\"ae36\" class=\"graf graf--p graf-after--h3\">Cassandra performed very well writing viewing history data but there was a need to improve the read latencies. To optimize read latencies, at the expense of increased work during the write path, we added an in-memory sharded caching layer (<a href=\"https://medium.com/netflix-techblog/announcing-evcache-distributed-in-memory-datastore-for-cloud-c26a698c27f7\" data-href=\"https://medium.com/netflix-techblog/announcing-evcache-distributed-in-memory-datastore-for-cloud-c26a698c27f7\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">EVCache</a>) in front of Cassandra storage. The cache was a simple key value store with the key being CustomerId and value being the compressed binary representation of viewing history data. Each write to Cassandra incurred an additional cache lookup and on cache hit the new data was merged with the existing value. Viewing history reads were serviced by the cache first. On a cache miss, the entry was read from Cassandra, compressed and then inserted in the cache.</p><p id=\"8385\" class=\"graf graf--p graf-after--p\">With the addition of the caching layer, this single Cassandra table storage approach worked very well for many years. Partitioning based on CustomerId scaled well in the Cassandra cluster. By 2012, the Viewing History Cassandra cluster was one of the biggest dedicated Cassandra clusters at Netflix. To scale further, the team needed to double the cluster size. This meant venturing into uncharted territory for Netflix’s usage of Cassandra. In the meanwhile, Netflix business was continuing to grow rapidly, including an increasing international member base and forthcoming ventures into original content.</p><h3 id=\"3495\" class=\"graf graf--h3 graf-after--p\">Redesign: Live and Compressed Storage Approach</h3><p id=\"fd31\" class=\"graf graf--p graf-after--h3\">It became clear that a different approach was needed to scale for growth anticipated over the next 5 years. The team analyzed the data characteristics and usage patterns, and redesigned viewing history storage with two main goals in mind:</p><ol class=\"postList\"><li id=\"ab0b\" class=\"graf graf--li graf-after--p\">Smaller Storage Footprint.</li><li id=\"9eea\" class=\"graf graf--li graf-after--li\">Consistent Read/Write Performance as viewing per member grows.</li></ol><p id=\"2106\" class=\"graf graf--p graf-after--li\">For each member, viewing history data is divided into two sets:</p><ul class=\"postList\"><li id=\"5139\" class=\"graf graf--li graf-after--p\"><em class=\"markup--em markup--li-em\">Live or Recent Viewing History (LiveVH):</em> Small number of recent viewing records with frequent updates. The data is stored in uncompressed form as in the simple design detailed above.</li><li id=\"2972\" class=\"graf graf--li graf-after--li\"><em class=\"markup--em markup--li-em\">Compressed or Archival Viewing History (CompressedVH):</em> Large number of older viewing records with rare updates. The data is compressed to reduce storage footprint. Compressed viewing history is stored in a single column per row key.</li></ul><p id=\"5e85\" class=\"graf graf--p graf-after--li\">LiveVH and CompressedVH are stored in different tables and are tuned differently to achieve better performance. Since LiveVH has frequent updates and small number of viewing records, compactions are run frequently and <a href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbInternals/dbIntAboutDeletes.html\" data-href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbInternals/dbIntAboutDeletes.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">gc_grace_seconds</a> is small to reduce number of SSTables and data size. <a href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbArch/archRepairNodesReadRepair.html\" data-href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbArch/archRepairNodesReadRepair.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Read repair</a> and<a href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbArch/archAntiEntropyRepair.html\" data-href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbArch/archAntiEntropyRepair.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\"> full column family repair</a> are run frequently to improve data consistency. Since updates to CompressedVH are rare, manual and infrequent full compactions are sufficient to reduce number of SSTables. Data is checked for consistency during the rare updates. This obviates the need for read repair as well as full column family repair.</p><h3 id=\"3c67\" class=\"graf graf--h3 graf-after--p\">Write Flow</h3><p id=\"dae1\" class=\"graf graf--p graf-after--h3\">New viewing records are written to LiveVH using the same approach as described earlier.</p><h3 id=\"06e7\" class=\"graf graf--h3 graf-after--p\">Read Flows</h3><p id=\"d975\" class=\"graf graf--p graf-after--h3\">To get the benefit of the new design, the viewing history API was updated with an option to read recent or full data:</p><ul class=\"postList\"><li id=\"d3b2\" class=\"graf graf--li graf-after--p\"><em class=\"markup--em markup--li-em\">Recent Viewing History:</em> For most cases this results in reading from LiveVH only, which limits the data size resulting in much lower latencies.</li><li id=\"49a7\" class=\"graf graf--li graf-after--li\"><em class=\"markup--em markup--li-em\">Full Viewing History:</em> Implemented as parallel reads of LiveVH and CompressedVH. Due to data compression and CompressedVH having fewer columns, less data is read thereby significantly speeding up reads.</li></ul><h3 id=\"24b1\" class=\"graf graf--h3 graf-after--li\">CompressedVH Update Flow</h3><p id=\"79c5\" class=\"graf graf--p graf-after--h3\">While reading viewing history records from LiveVH, if the number of records is over a configurable threshold then the recent viewing records are rolled up, compressed and stored in CompressedVH via a background task. Rolled up data is stored in a new row with row key:CustomerId. The new rollup is versioned and after being written is read to check for consistency. Only after verifying the consistency of the new version, the old version of rolled up data is deleted. For simplicity there is no locking during rollup and Cassandra takes care of resolving very rare duplicate writes (i.e., the last writer wins).</p><figure id=\"fdb3\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*VWfKAENoS8-OcyCL.\" data-width=\"720\" data-height=\"407\" data-is-featured=\"true\" data-action=\"zoom\" data-action-value=\"0*VWfKAENoS8-OcyCL.\" src=\"https://cdn-images-1.medium.com/max/1600/0*VWfKAENoS8-OcyCL.\" alt=\"image\" /></div><figcaption class=\"imageCaption\">Figure 2: Live and Compressed Data Model</figcaption></div></figure><p id=\"a2fb\" class=\"graf graf--p graf-after--figure\">As shown in figure 2, the rolled up row in CompressedVH also stores metadata information like the latest version, object size and chunking information (more on that later). The version column stores a reference to the latest version of rolled up data so that reads for a CustomerId always return only the latest rolled up data. The rolled up data is stored in a single column to reduce compaction pressure. To minimize the frequency of rollups for members with frequent viewing pattern, just the last couple of days worth of viewing history records are kept in LiveVH after rollup and the rest are merged with the records in CompressedVH during rollup.</p><h3 id=\"0727\" class=\"graf graf--h3 graf-after--p\">Auto Scaling via Chunking</h3><p id=\"c814\" class=\"graf graf--p graf-after--h3\">For the majority of members, storing their entire viewing history in a single row of compressed data resulted in good performance during the read flows. For a small percentage of members with very large viewing history, reading CompressedVH from a single row started to slow down due to similar reasons as described in the first architecture. There was a need to have an upper bound on the read and write latencies for this rare case without negatively impacting the read and write latencies for the common case.</p><p id=\"b2e0\" class=\"graf graf--p graf-after--p\">To solve for this, we split the rolled up compressed data into multiple chunks if the data size is greater than a configurable threshold. These chunks are stored on different Cassandra nodes. Parallel reads and writes of these chunks results in having an upper bound on the read and write latencies even for very large viewing data.</p><figure id=\"dcc2\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*rQ0DZVO0oR_MHVNg.\" data-width=\"720\" data-height=\"647\" data-action=\"zoom\" data-action-value=\"0*rQ0DZVO0oR_MHVNg.\" src=\"https://cdn-images-1.medium.com/max/1600/0*rQ0DZVO0oR_MHVNg.\" alt=\"image\" /></div><figcaption class=\"imageCaption\">Figure 3: Auto Scale via Chunking</figcaption></div></figure><h4 id=\"7720\" class=\"graf graf--h4 graf-after--figure\">Write Flow</h4><p id=\"be96\" class=\"graf graf--p graf-after--h4\">As figure 3 indicates, rolled up compressed data is split into multiple chunks based on a configurable chunk size. All chunks are written in parallel to different rows with row key:CustomerId$Version$ChunkNumber. Metadata is written to its own row with row key:CustomerId after successful write of the chunked data. This bounds the write latency to two writes for rollups of very large viewing data. In this case the metadata row has an empty data column to enable fast read of metadata.</p><p id=\"bd49\" class=\"graf graf--p graf-after--p\">To make the common case (compressed viewing data is smaller than the configurable threshold) fast, metadata is combined with the viewing data in the same row to eliminate metadata lookup overhead as shown in figure 2.</p><h4 id=\"80df\" class=\"graf graf--h4 graf-after--p\">Read Flow</h4><p id=\"ba04\" class=\"graf graf--p graf-after--h4\">The metadata row is first read using CustomerId as the key. For the common case, the chunk count is 1 and the metadata row also has the most recent version of rolled up compressed viewing data. For the rare case, there are multiple chunks of compressed viewing data. Using the metadata information like version and chunk count, different row keys for the chunks are generated and all chunks are read in parallel. This bounds the read latency to two reads.</p><h3 id=\"3434\" class=\"graf graf--h3 graf-after--p\">Caching Layer Changes</h3><p id=\"e1c0\" class=\"graf graf--p graf-after--h3\">The in-memory caching layer was enhanced to support chunking for large entries. For members with large viewing history, it was not possible to fit the entire compressed viewing history in a single EVCache entry. So similar to the CompressedVH model, each large viewing history cache entry is broken into multiple chunks and the metadata is stored along with the first chunk.</p><h3 id=\"1e53\" class=\"graf graf--h3 graf-after--p\">Results</h3><p id=\"c4d8\" class=\"graf graf--p graf-after--h3\">By leveraging parallelism, compression, and an improved data model, the team was able to meet all of the goals:</p><ol class=\"postList\"><li id=\"b384\" class=\"graf graf--li graf-after--p\">Smaller Storage Footprint via compression.</li><li id=\"5920\" class=\"graf graf--li graf-after--li\">Consistent Read/Write Performance via chunking and parallel reads/writes. Latency bound to one read and one write for common cases and latency bound to two reads and two writes for rare cases.</li></ol><figure id=\"c6e1\" class=\"graf graf--figure graf-after--li\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*t2YDONTlaJIDxDnf.\" data-width=\"720\" data-height=\"400\" data-action=\"zoom\" data-action-value=\"0*t2YDONTlaJIDxDnf.\" src=\"https://cdn-images-1.medium.com/max/1600/0*t2YDONTlaJIDxDnf.\" alt=\"image\" /></div><figcaption class=\"imageCaption\">Figure 4: Results</figcaption></div></figure><p id=\"3d8e\" class=\"graf graf--p graf-after--figure\">The team achieved ~6X reduction in data size, ~13X reduction in system time spent on Cassandra maintenance, ~5X reduction in average read latency and ~1.5X reduction in average write latency. More importantly, it gave the team a scalable architecture and headroom to accommodate rapid growth of Netflix viewing data.</p><p id=\"97a7\" class=\"graf graf--p graf-after--p graf--trailing\">In the next part of this blog post series, we will explore the latest scalability challenges motivating the next iteration of viewing history storage architecture. If you are interested in solving similar problems, <a href=\"https://jobs.netflix.com/jobs/866030\" data-href=\"https://jobs.netflix.com/jobs/866030\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">join us</a>.</p></div></div>"}}]}},"pageContext":{"alternative_id":12082}}