{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Landoop/stream-reactor","alternative_id":9599,"content":"<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\"><p><a href=\"https://datamountaineer.ci.landoop.com/job/stream-reactor/\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/66bb79f11d5cecbbc57c12f7b8bf4cfb61be30e7/68747470733a2f2f646174616d6f756e7461696e6565722e63692e6c616e646f6f702e636f6d2f6275696c645374617475732f69636f6e3f6a6f623d73747265616d2d72656163746f72267374796c653d666c6174262e706e67\" alt=\"Build Status\" data-canonical-src=\"https://datamountaineer.ci.landoop.com/buildStatus/icon?job=stream-reactor&amp;style=flat&amp;.png\" /></a>\n<a href=\"http://lenses.stream/connectors/index.html\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/0b233e4a210326868c5be7197bd9ccb416aacff0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d2d6f72616e67652e7376673f\" data-canonical-src=\"https://img.shields.io/badge/docs--orange.svg?\" alt=\"image\" /></a>\n<a href=\"http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22com.datamountaineer%22\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/c0fb8d333898ab15428ba7e8b2420b7427908d7e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c617465737425323072656c656173652d312e302e302d626c75652e7376673f6c6162656c3d6c617465737425323072656c65617365\" data-canonical-src=\"https://img.shields.io/badge/latest%20release-1.0.0-blue.svg?label=latest%20release\" alt=\"image\" /></a></p>\n<p>Join us on slack <a href=\"https://launchpass.com/landoop-community\" rel=\"nofollow\"><img src=\"https://github.com/Landoop/stream-reactor/raw/master/images/slack.jpeg\" alt=\"Alt text\" /></a></p>\n<p>Lenses offers SQL (for data browsing and Kafka Streams), Kafka Connect connector management, cluster monitoring and more.</p>\n<p>You can find more on <a href=\"http://www.landoop.com/kafka-lenses/\" rel=\"nofollow\">landoop.com!</a></p>\n<p><a target=\"_blank\" href=\"https://github.com/Landoop/stream-reactor/blob/master/images/streamreactor-logo.png\"><img src=\"https://github.com/Landoop/stream-reactor/raw/master/images/streamreactor-logo.png\" alt=\"Alt text\" /></a></p>\n<p>A collection of components to build a real time ingestion pipeline.</p>\n<h3><a id=\"user-content-connectors\" class=\"anchor\" aria-hidden=\"true\" href=\"#connectors\"></a>Connectors</h3>\n<p><strong>Please take a moment and read the documentation and make sure the software prerequisites are met!!</strong></p>\n<table><thead><tr><th>Connector</th>\n<th>Type</th>\n<th>Description</th>\n<th>Docs</th>\n</tr></thead><tbody><tr><td>AzureDocumentDb</td>\n<td>Sink</td>\n<td>Kafka connect Azure DocumentDb sink to subscribe to write to the cloud Azure Document Db.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/azuredocdb.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>BlockChain</td>\n<td>Source</td>\n<td>Kafka connect Blockchain source to subscribe to Blockchain streams and write to Kafka.</td>\n<td><a href=\"https://lenses.stream/connectors/source/blockchain.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Bloomberg</td>\n<td>Source</td>\n<td>Kafka connect source to subscribe to Bloomberg streams and write to Kafka.</td>\n<td><a href=\"https://lenses.stream/connectors/source/bloomberg.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Cassandra</td>\n<td>Source</td>\n<td>Kafka connect Cassandra source to read Cassandra and write to Kafka.</td>\n<td><a href=\"https://lenses.stream/connectors/source/cassandra.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>*Cassandra</td>\n<td>Sink</td>\n<td>Certified DSE Kafka connect Cassandra sink task to write Kafka topic payloads to Cassandra.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/cassandra.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Coap</td>\n<td>Source</td>\n<td>Kafka connect Coap source to read from IoT Coap endpoints using Californium.</td>\n<td><a href=\"https://lenses.stream/connectors/source/coap.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Coap</td>\n<td>Sink</td>\n<td>Kafka connect Coap sink to write kafka topic payload to IoT Coap endpoints using Californium.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/coap.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Druid</td>\n<td>Sink</td>\n<td>Kafka connect Druid sink to write Kafka topic payloads to Druid.</td>\n<td>\n</td></tr><tr><td>Elastic</td>\n<td>Sink</td>\n<td>Kafka connect Elastic Search sink to write Kafka topic payloads to Elastic Search 2.x</td>\n<td><a href=\"https://lenses.stream/connectors/sink/elastic.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Elastic 5</td>\n<td>Sink</td>\n<td>Kafka connect Elastic Search sink to write payloads to Elastic Search 5.x w. tcp or http</td>\n<td><a href=\"https://lenses.stream/connectors/sink/elastic5.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Elastic 6</td>\n<td>Sink</td>\n<td>Kafka connect Elastic Search sink to write payloads to Elastic Search 6.x w. tcp or http</td>\n<td><a href=\"https://lenses.stream/connectors/sink/elastic6.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>FTP/HTTP</td>\n<td>Source</td>\n<td>Kafka connect FTP and HTTP source to write file data into Kafka topics.</td>\n<td><a href=\"https://lenses.stream/connectors/source/ftp.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>HBase</td>\n<td>Sink</td>\n<td>Kafka connect HBase sink to write Kafka topic payloads to HBase.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/hbase.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Hazelcast</td>\n<td>Sink</td>\n<td>Kafka connect Hazelcast sink to write Kafka topic payloads to Hazelcast.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/hazelcast.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Kudu</td>\n<td>Sink</td>\n<td>Kafka connect Kudu sink to write Kafka topic payloads to Kudu.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/kudu.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>InfluxDb</td>\n<td>Sink</td>\n<td>Kafka connect InfluxDb sink to write Kafka topic payloads to InfluxDb.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/influx.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>JMS</td>\n<td>Source</td>\n<td>Kafka connect JMS source to write from JMS to Kafka topics.</td>\n<td><a href=\"https://lenses.stream/connectors/source/jms.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>JMS</td>\n<td>Sink</td>\n<td>Kafka connect JMS sink to write Kafka topic payloads to JMS.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/jms.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>MongoDB</td>\n<td>Sink</td>\n<td>Kafka connect MongoDB sink to write Kafka topic payloads to MongoDB.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/mongo.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>MQTT</td>\n<td>Source</td>\n<td>Kafka connect MQTT source to write data from MQTT to Kafka.</td>\n<td><a href=\"https://lenses.stream/connectors/source/mqtt.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>MQTT</td>\n<td>Sink</td>\n<td>Kafka connect MQTT sink to write data from Kafka to MQTT.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/mqtt.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Pulsar</td>\n<td>Source</td>\n<td>Kafka connect Pulsar source to write data from Pulsar to Kafka.</td>\n<td><a href=\"https://lenses.stream/connectors/source/pulsar.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Pulsar</td>\n<td>Sink</td>\n<td>Kafka connect Pulsar sink to write data from Kafka to Pulsar.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/pulsar.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Redis</td>\n<td>Sink</td>\n<td>Kafka connect Redis sink to write Kafka topic payloads to Redis.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/redis.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>ReThinkDB</td>\n<td>Source</td>\n<td>Kafka connect RethinkDb source subscribe to ReThinkDB changefeeds and write to Kafka.</td>\n<td><a href=\"https://lenses.stream/connectors/source/rethink.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>ReThinkDB</td>\n<td>Sink</td>\n<td>Kafka connect RethinkDb sink to write Kafka topic payloads to RethinkDb.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/rethink.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>VoltDB</td>\n<td>Sink</td>\n<td>Kafka connect Voltdb sink to write Kafka topic payloads to Voltdb.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/voltdb.html\" rel=\"nofollow\">Docs</a></td>\n</tr></tbody></table><h2><a id=\"user-content-release-notes\" class=\"anchor\" aria-hidden=\"true\" href=\"#release-notes\"></a>Release Notes</h2>\n<p><strong>1.1.0</strong> Pending</p>\n<ul><li>Added SSL, subscription, partitioning, batching and key selection to Pulsar source and sink</li>\n<li>Elastic6 connector @caiooliveiraeti !</li>\n<li>HTTP Basic Auth for Elasticsearch http client thanks @justinsoong !</li>\n<li>Add polling timeout on the JMS source connector to avoid high CPU in the source connector poll thanks #373 @matthedude</li>\n<li>Fixes on the elastic primary key separator thanks @caiooliveiraeti!</li>\n<li>Fix on the MQTT class loader</li>\n<li>Fix on the JMS class loader</li>\n<li>Fix on JMS to close down connections cleanly #363 thanks @matthedude!</li>\n<li>Fix on MQTT to correctly handle authentication</li>\n<li>Moved MongoDB batch size to KCQL. <code>connect.mongodb.batch.size</code> is deprecated</li>\n<li>Added <code>connect.mapping.collection.to.json</code> to treat maps, list, sets as json when inserting into Cassandra</li>\n<li>Added support for Elastic Pipelines thanks @caiooliveiraeti!</li>\n<li>Moved ReThinkDB batch size to KCQL <code>connect.rethink.batch.size</code> is deprecated</li>\n<li>MQTT source allows full control of matching the topic <code>INSERT INTO targetTopic SELECT * FROM mqttTopic ... WITHREGEX=`$THE_REGEX`</code></li>\n</ul><p><strong>1.0.0</strong></p>\n<ul><li>Kafka 1.0.o Support</li>\n</ul><p><strong>0.4.0</strong></p>\n<ul><li>Add FTPS support to FTP connector, new configuration option <code>ftp.protocol</code> introduced, either ftp (default) or ftps.</li>\n<li>Fix for MQTT source High CPU Thanks @masahirom!</li>\n<li>Improve logging on Kudu</li>\n<li>DELETE functionality add to the Cassandra sink, deletion now possible for null payloads, thanks @sandonjacobs !</li>\n<li>Fix in kafka-connect-common to handle primary keys with doc strings thanks, @medvekoma !</li>\n<li>Fix writing multiple topics to the same table in Cassandra #284</li>\n<li>Upgrade to Cassandra driver 3.3.0 and refactor Cassandra tests</li>\n<li>Fix on JMS source transacted queues #285 thanks @matthedude !</li>\n<li>Fix on Cassandra source, configurable timespan queries. You can now control the timespan the Connector will query for</li>\n<li>Allow setting initial query timestamp on Cassandra source</li>\n<li>Allow multiple primary keys on the redis sink</li>\n</ul><p><strong>0.3.0</strong></p>\n<ul><li>Upgrade CoAP to 2.0.0-M4</li>\n<li>Upgrade to Confluent 3.3 and Kafka 0.11.0.0.</li>\n<li>Added MQTT Sink.</li>\n<li>Add MQTT wildcard support.</li>\n<li>Upgrade CoAP to 2.0.0-M4.</li>\n<li>Added WITHCONVERTERS and WITHTYPE to JMS and MQTT connectors in KCQL to simplify configuration.</li>\n<li>Added FLUSH MODE to Kudu. Thanks! @patsak</li>\n</ul><p><strong>0.2.6</strong></p>\n<h3><a id=\"user-content-features\" class=\"anchor\" aria-hidden=\"true\" href=\"#features\"></a>Features</h3>\n<ul><li>Added MQTT Sink</li>\n<li>Upgrade to Confluent 3.2.2</li>\n<li>Upgrade to KCQL 2x</li>\n<li>Add CQL generator to Cassandra source</li>\n<li>Add KCQL INCREMENTALMODE support to the Cassandra source, bulk mode and the timestamp column type is now take from KCQL</li>\n<li>Support for setting key and truststore type on Cassandra connectors</li>\n<li>Added token based paging support for Cassandra source</li>\n<li>Added default bytes converter to JMS Source</li>\n<li>Added default connection factory to JMS Source</li>\n<li>Added support for SharedDurableConsumers to JMS Connectors</li>\n<li>Upgraded JMS Connector to JMS 2.0</li>\n<li>Moved to Elastic4s 2.4</li>\n<li>Added Elastic5s with TCP, TCP+XPACK and HTTP client support</li>\n<li>Upgrade Azure Documentdb to 1.11.0</li>\n<li>Added optional progress counter to all connectors, it can be enabled with <code>connect.progress.enabled</code> which will periodically report log messages processed</li>\n<li>Added authentication and TLS to ReThink Connectors</li>\n<li>Added TLS support for ReThinkDB, add batch size option to source for draining the internal queues.</li>\n<li>Upgrade Kudu Client to 1.4.0</li>\n<li>Support for dates in Elastic Indexes and custom document types</li>\n<li>Upgrade Connect CLI to 1.0.2 (Renamed to connect-cli)</li>\n</ul><h3><a id=\"user-content-bug-fixes\" class=\"anchor\" aria-hidden=\"true\" href=\"#bug-fixes\"></a>Bug Fixes</h3>\n<ul><li>Fixes for high CPU on CoAP source</li>\n<li>Fixes for high CPU on Cassandra source</li>\n<li>Fixed Avro double fields mapping to Kudu columns</li>\n<li>Fixes on JMS properties converter, Invalid schema when extracting properties</li>\n</ul><h3><a id=\"user-content-misc\" class=\"anchor\" aria-hidden=\"true\" href=\"#misc\"></a>Misc</h3>\n<ul><li>Refactored Cassandra Tests to use only one embedded instance</li>\n<li>Removed unused batch size and bucket size options from Kudu, they are taken from KCQL</li>\n<li>Removed unused batch size option from DocumentDb</li>\n<li>Rename Azure DocumentDb <code>connect.documentdb.db</code> to <code>connect.documentdb.db</code></li>\n<li>Rename Azure DocumentDb <code>connect.documentdb.database.create</code> to <code>connect.documentdb.db.create</code></li>\n<li>Rename Cassandra Source <code>connect.cassandra.source.kcql</code> to <code>connect.cassandra.kcql</code></li>\n<li>Rename Cassandra Source <code>connect.cassandra.source.timestamp.type</code> to <code>connect.cassandra.timestamp.type</code></li>\n<li>Rename Cassandra Source <code>connect.cassandra.source.import.poll.interval</code> to <code>connect.cassandra.import.poll.interval</code></li>\n<li>Rename Cassandra Source <code>connect.cassandra.source.error.policy</code> to <code>connect.cassandra.error.policy</code></li>\n<li>Rename Cassandra Source <code>connect.cassandra.source.max.retries</code> to <code>connect.cassandra.max.retries</code></li>\n<li>Rename Cassandra Sink <code>connect.cassandra.source.retry.interval</code> to <code>connect.cassandra.retry.interval</code></li>\n<li>Rename Cassandra Sink <code>connect.cassandra.sink.kcql</code> to <code>connect.cassandra.kcql</code></li>\n<li>Rename Cassandra Sink <code>connect.cassandra.sink.error.policy</code> to <code>connect.cassandra.error.policy</code></li>\n<li>Rename Cassandra Sink <code>connect.cassandra.sink.max.retries</code> to <code>connect.cassandra.max.retries</code></li>\n<li>Rename Cassandra Sink Sink <code>connect.cassandra.sink.retry.interval</code> to <code>connect.cassandra.retry.interval</code></li>\n<li>Rename Coap Source <code>connect.coap.bind.port</code> to <code>connect.coap.port</code></li>\n<li>Rename Coap Sink <code>connect.coap.bind.port</code> to <code>connect.coap.port</code></li>\n<li>Rename Coap Source <code>connect.coap.bind.host</code> to <code>connect.coap.host</code></li>\n<li>Rename Coap Sink <code>connect.coap.bind.host</code> to <code>connect.coap.host</code></li>\n<li>Rename MongoDb <code>connect.mongo.database</code> to <code>connect.mongo.db</code></li>\n<li>Rename MongoDb <code>connect.mongo.sink.batch.size</code> to <code>connect.mongo.batch.size</code></li>\n<li>Rename Druid <code>connect.druid.sink.kcql</code> to <code>connect.druid.kcql</code></li>\n<li>Rename Druid <code>connect.druid.sink.conf.file</code> to <code>connect.druid.kcql</code></li>\n<li>Rename Druid <code>connect.druid.sink.write.timeout</code> to <code>connect.druid.write.timeout</code></li>\n<li>Rename Elastic <code>connect.elastic.sink.kcql</code> to <code>connect.elastic.kcql</code></li>\n<li>Rename HBase <code>connect.hbase.sink.column.family</code> to <code>connect.hbase.column.family</code></li>\n<li>Rename HBase <code>connect.hbase.sink.kcql</code> to <code>connect.hbase.kcql</code></li>\n<li>Rename HBase <code>connect.hbase.sink.error.policy</code> to <code>connect.hbase.error.policy</code></li>\n<li>Rename HBase <code>connect.hbase.sink.max.retries</code> to <code>connect.hbase.max.retries</code></li>\n<li>Rename HBase <code>connect.hbase.sink.retry.interval</code> to <code>connect.hbase.retry.interval</code></li>\n<li>Rename Influx <code>connect.influx.sink.kcql</code> to <code>connect.influx.kcql</code></li>\n<li>Rename Influx <code>connect.influx.connection.user</code> to <code>connect.influx.username</code></li>\n<li>Rename Influx <code>connect.influx.connection.password</code> to <code>connect.influx.password</code></li>\n<li>Rename Influx <code>connect.influx.connection.database</code> to <code>connect.influx.db</code></li>\n<li>Rename Influx <code>connect.influx.connection.url</code> to <code>connect.influx.url</code></li>\n<li>Rename Kudu <code>connect.kudu.sink.kcql</code> to <code>connect.kudu.kcql</code></li>\n<li>Rename Kudu <code>connect.kudu.sink.error.policy</code> to <code>connect.kudu.error.policy</code></li>\n<li>Rename Kudu <code>connect.kudu.sink.retry.interval</code> to <code>connect.kudu.retry.interval</code></li>\n<li>Rename Kudu <code>connect.kudu.sink.max.retries</code> to <code>connect.kudu.max.reties</code></li>\n<li>Rename Kudu <code>connect.kudu.sink.schema.registry.url</code> to <code>connect.kudu.schema.registry.url</code></li>\n<li>Rename Redis <code>connect.redis.connection.password</code> to <code>connect.redis.password</code></li>\n<li>Rename Redis <code>connect.redis.sink.kcql</code> to <code>connect.redis.kcql</code></li>\n<li>Rename Redis <code>connect.redis.connection.host</code> to <code>connect.redis.host</code></li>\n<li>Rename Redis <code>connect.redis.connection.port</code> to <code>connect.redis.port</code></li>\n<li>Rename ReThink <code>connect.rethink.source.host</code> to <code>connect.rethink.host</code></li>\n<li>Rename ReThink <code>connect.rethink.source.port</code> to <code>connect.rethink.port</code></li>\n<li>Rename ReThink <code>connect.rethink.source.db</code> to <code>connect.rethink.db</code></li>\n<li>Rename ReThink <code>connect.rethink.source.kcql</code> to <code>connect.rethink.kcql</code></li>\n<li>Rename ReThink Sink <code>connect.rethink.sink.host</code> to <code>connect.rethink.host</code></li>\n<li>Rename ReThink Sink <code>connect.rethink.sink.port</code> to <code>connect.rethink.port</code></li>\n<li>Rename ReThink Sink <code>connect.rethink.sink.db</code> to <code>connect.rethink.db</code></li>\n<li>Rename ReThink Sink <code>connect.rethink.sink.kcql</code> to <code>connect.rethink.kcql</code></li>\n<li>Rename JMS <code>connect.jms.user</code> to <code>connect.jms.username</code></li>\n<li>Rename JMS <code>connect.jms.source.converters</code> to <code>connect.jms.converters</code></li>\n<li>Remove JMS <code>connect.jms.converters</code> and replace my kcql <code>withConverters</code></li>\n<li>Remove JMS <code>connect.jms.queues</code> and replace my kcql <code>withType QUEUE</code></li>\n<li>Remove JMS <code>connect.jms.topics</code> and replace my kcql <code>withType TOPIC</code></li>\n<li>Rename Mqtt <code>connect.mqtt.source.kcql</code> to <code>connect.mqtt.kcql</code></li>\n<li>Rename Mqtt <code>connect.mqtt.user</code> to <code>connect.mqtt.username</code></li>\n<li>Rename Mqtt <code>connect.mqtt.hosts</code> to <code>connect.mqtt.connection.hosts</code></li>\n<li>Remove Mqtt <code>connect.mqtt.converters</code> and replace my kcql <code>withConverters</code></li>\n<li>Remove Mqtt <code>connect.mqtt.queues</code> and replace my kcql <code>withType=QUEUE</code></li>\n<li>Remove Mqtt <code>connect.mqtt.topics</code> and replace my kcql <code>withType=TOPIC</code></li>\n<li>Rename Hazelcast <code>connect.hazelcast.sink.kcql</code> to <code>connect.hazelcast.kcql</code></li>\n<li>Rename Hazelcast <code>connect.hazelcast.sink.group.name</code> to <code>connect.hazelcast.group.name</code></li>\n<li>Rename Hazelcast <code>connect.hazelcast.sink.group.password</code> to <code>connect.hazelcast.group.password</code></li>\n<li>Rename Hazelcast <code>connect.hazelcast.sink.cluster.members</code> tp <code>connect.hazelcast.cluster.members</code></li>\n<li>Rename Hazelcast <code>connect.hazelcast.sink.batch.size</code> to <code>connect.hazelcast.batch.size</code></li>\n<li>Rename Hazelcast <code>connect.hazelcast.sink.error.policy</code> to <code>connect.hazelcast.error.policy</code></li>\n<li>Rename Hazelcast <code>connect.hazelcast.sink.max.retries</code> to <code>connect.hazelcast.max.retries</code></li>\n<li>Rename Hazelcast <code>connect.hazelcast.sink.retry.interval</code> to <code>connect.hazelcast.retry.interval</code></li>\n<li>Rename VoltDB <code>connect.volt.sink.kcql</code> to <code>connect.volt.kcql</code></li>\n<li>Rename VoltDB <code>connect.volt.sink.connection.servers</code> to <code>connect.volt.servers</code></li>\n<li>Rename VoltDB <code>connect.volt.sink.connection.user</code> to <code>connect.volt.username</code></li>\n<li>Rename VoltDB <code>connect.volt.sink.connection.password</code> to <code>connect.volt.password</code></li>\n<li>Rename VoltDB <code>connect.volt.sink.error.policy</code> to <code>connect.volt.error.policy</code></li>\n<li>Rename VoltDB <code>connect.volt.sink.max.retries</code> to <code>connect.volt.max.retries</code></li>\n<li>Rename VoltDB <code>connect.volt.sink.retry.interval</code> to <code>connect.volt.retry.interval</code></li>\n</ul><p><strong>0.2.5 (8 Apr 2017)</strong></p>\n<ul><li>Added Azure DocumentDB Sink Connector</li>\n<li>Added JMS Source Connector.</li>\n<li>Added UPSERT to Elastic Search</li>\n<li>Support Confluent 3.2 and Kafka 0.10.2.</li>\n<li>Cassandra improvements <code>withunwrap</code></li>\n<li>Upgrade to Kudu 1.0 and CLI 1.0</li>\n<li>Add ingest_time to CoAP Source</li>\n<li>InfluxDB bug fixes for tags and field selection.</li>\n<li>Added Schemaless Json and Json with schema support to JMS Sink.</li>\n<li>Support for Cassandra data type of <code>timestamp</code> in the Cassandra Source for timestamp tracking.</li>\n</ul><p><strong>0.2.4</strong> (26 Jan 2017)</p>\n<ul><li>Added FTP and HTTP Source.</li>\n<li>Added InfluxDB tag support. KCQL: INSERT INTO targetdimension <code>SELECT * FROM influx-topic WITHTIMESTAMP sys_time() WITHTAG(field1, CONSTANT_KEY1=CONSTANT_VALUE1, field2,CONSTANT_KEY2=CONSTANT_VALUE1)</code></li>\n<li>Added InfluxDb consistency level. Default is <code>ALL</code>. Use <code>connect.influx.consistency.level</code> to set it to ONE/QUORUM/ALL/ANY</li>\n<li>InfluxDb <code>connect.influx.sink.route.query</code> was renamed to <code>connect.influx.sink.kcql</code></li>\n<li>Added support for multiple contact points in Cassandra</li>\n</ul><p><strong>0.2.3</strong> (5 Jan 2017)</p>\n<ul><li>Added CoAP Source and Sink.</li>\n<li>Added MongoDB Sink.</li>\n<li>Added MQTT Source.</li>\n<li>Hazelcast support for ring buffers.</li>\n<li>Redis support for Sorted Sets.</li>\n<li>Added start scripts.</li>\n<li>Added Kafka Connect and Schema Registry CLI.</li>\n<li>Kafka Connect CLI now supports pause/restart/resume; checking connectors on the classpath and validating configuration of connectors.</li>\n<li>Support for <code>Struct</code>, <code>Schema.STRING</code> and <code>Json</code> with schema in the Cassandra, ReThinkDB, InfluxDB and MongoDB sinks.</li>\n<li>Rename <code>export.query.route</code> to <code>sink.kcql</code>.</li>\n<li>Rename <code>import.query.route</code> to <code>source.kcql</code>.</li>\n<li>Upgrade to KCQL 0.9.5 - Add support for <code>STOREAS</code> so specify target sink types, e.g. Redis Sorted Sets, Hazelcast map, queues, ringbuffers.</li>\n</ul><h3><a id=\"user-content-building\" class=\"anchor\" aria-hidden=\"true\" href=\"#building\"></a>Building</h3>\n<p><em><strong>Requires gradle 3.0 to build.</strong></em></p>\n<p>To build</p>\n<div class=\"highlight highlight-source-shell\"><pre>gradle compile</pre></div>\n<p>To test</p>\n<div class=\"highlight highlight-source-shell\"><pre>gradle test</pre></div>\n<p>To create a fat jar</p>\n<div class=\"highlight highlight-source-shell\"><pre>gradle shadowJar</pre></div>\n<p>You can also use the gradle wrapper</p>\n<pre>./gradlew shadowJar\n</pre>\n<p>To view dependency trees</p>\n<pre>gradle dependencies # or\ngradle :kafka-connect-cassandra:dependencies\n</pre>\n<p>To build a particular project</p>\n<pre>gradle :kafka-connect-elastic5:build\n</pre>\n<p>To create a jar of a particular project:</p>\n<pre>gradle :kafka-connect-elastic5:shadowJar\n</pre>\n<h2><a id=\"user-content-contributing\" class=\"anchor\" aria-hidden=\"true\" href=\"#contributing\"></a>Contributing</h2>\n<p>We'd love to accept your contributions! Please use GitHub pull requests: fork the repo, develop and test your code,\n<a href=\"http://karma-runner.github.io/1.0/dev/git-commit-msg.html\" rel=\"nofollow\">semantically commit</a> and submit a pull request. Thanks!</p>\n</article>"}}]}},"pageContext":{"alternative_id":9599}}