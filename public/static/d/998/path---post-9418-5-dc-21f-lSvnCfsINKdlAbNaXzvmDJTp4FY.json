{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Report Number Three (Cassandra, Spark and Solr)","alternative_id":9418,"content":"<p>Changing from Hadoop to Spark, refining mandatory calculation, adding field statistics, storing records \nin Cassandra, indexing with Solr and calculating uniqueness.</p><h2 id=\"changing-to-spark\">Changing to Spark</h2><p>Last year I did some Coursera courses on Big Data and Data Science (I recommend you <a href=\"https://www.coursera.org/learn/data-manipulation\" target=\"_blank\">Bill Howe’s course</a> \nfrom the University of Washington if you like to understand theoretical background behind relational databases and\ndata science, and I don’t recommend <a href=\"https://www.coursera.org/specializations/big-data\" target=\"_blank\">these courses</a>\nprovided by the University of California San Diego) where I have learnt\nabout <a href=\"http://spark.apache.org/\" target=\"_blank\">Apache Spark</a>. Spark’s big promise is that it is quicker \nthan <a href=\"http://hadoop.apache.org/\" target=\"_blank\">Hadoop</a>’s MapReduce and more memory effective. For me it\nwas even more important, that I really don’t use the “reduce” part of MapReduce, and Spark is fine with that.\nThe change was not hard at all, since the business logic is separated in different classes to which Hadoop was \njust a client (the only existing client actually, but I planned to add other interfaces). For the same functionality Spark\nran 4 times faster than Hadoop (1.5 hours versus 6 hours). It was a real gain, it means that I can change the codes and\nrun it again several times a day if I want to implement something new.</p><h2 id=\"refining-mandatory-calculation\">Refining mandatory calculation</h2><p>Thanks to the feedbacks from the Europeana Data Quality Comittee we improved the mandatory dimension of the \ncompleteness measure. This tell us how a record fit to the basic Europeanaa Data Model (EDM) schema requirements. Previously\nI calculated each field individually, but that was bad: some fields are alternative to each other, so a record is valid if\neither dc:title, dcterms:alternative or dc:description is present. Now the mandatory score gives what is expected, and it \nis a real discriminator of bad records.</p><p>Now the program assigns 1 if a field is existing and 0 if not for 30+ fields. The R script creates record sets summary\nof it, and on the interface I introduces the d3.js data visualization library to display those values. I also introduces \nsome filters in the UI: the user can hide those collections which doesn’t have a particular field, those in which\nevery records have it, and those in which some records have it. The user can investigate each fields.</p><p>The usage of the fields accross all the records:</p><p><img src=\"http://pkiraly.github.io/assets/field-frequency.png\" class=\"real\" title=\"Field frequency\" alt=\"Field frequency\" /></p><p>The usage of dcterms:alternative in those data providers, which uses it in some of the records, but not in all:</p><p><img src=\"http://pkiraly.github.io/assets/field-alternative-per-data-providers.png\" class=\"real\" title=\"dcterms:alternative frequency\" alt=\"dcterms:alternative frequency\" /></p><p>An <a href=\"http://144.76.218.178/europeana-qa/field.php?field=proxy_dcterms_alternative&amp;type=data-providers&amp;exclusions%5B%5D=0&amp;exclusions%5B%5D=1\" target=\"_blank\">example</a>\nabout the dcterms:alternative across data providers.</p><h2 id=\"storing-records-in-apache-cassandra\">Storing records in Apache Cassandra</h2><p>So far it was a problem, that in the record view of the UI I was nat able to extract an individual record from the \nhuge JSON files I stored the data. After sime investigation I choosed \n<a href=\"http://cassandra.apache.org/\" target=\"_blank\">Apache Cassandra</a>, which has an interface for \nboth Java and PHP. I imported every records to it, but now I still use thhe JSON files stored in HDFS (Hadoop Distributed \nFile System) in the Spark analysis. It is on my TODO list to compare the performance if using files and interating over \nevery Cassandra records – my hipothesis is that file based iteration is quicker. Now only the ID and the JSON content is\nstored in Cassandra, I am thinking about to store the measurements as well: it would be good if I would like to search for \neach record having a score between say 0.2 and 0.4.</p><h2 id=\"uniqueness-calculation\">Uniqueness calculation</h2><p>I have started investigating the uniqueness or entropy of some selected fields (dc:title, dcterms:alternative \nand dc:description). The basic idea is that if the terms in these fields are frequent accross the records, then \nit is less unique, so less important. If a term is frequent in the same field it is more important than terms \nappear only once. This is called information entropy or in the search engive world TF-IDF formula (term frequency, \ninverse document frequency) – see <a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\" target=\"_blank\">tf-idf</a> Wikipedia article.</p><p>The <a href=\"http://lucene.apache.org/solr/\" target=\"_blank\">Apache Solr</a> search engine’s relevancy ranking \nmostly based on this formula (however there are lots of tuning \npossibilities, such as give fields weights etc.), but Solr doesn’t provide an interface by default for \nextracting the terms TF-IDF score. There is a Term Vector Component however which provides this interface \ngiven that you apply some additional indexing rules. It is not available in the ordinary Europeana Solr \nsetup so I have created a new Solr instance with this special settings, and created a brand new index with \nlimited fieldset. (If you want to check how to setup Solr and what interface you can use, check this \n<a href=\"https://cwiki.apache.org/confluence/display/solr/The+Term+Vector+Component\" target=\"_blank\">wiki page</a>.</p><p>When the index were created (it took five days, but it is improvable) the scores of a field (in this case the dc:title “Fleming/Mair wedding, Slaithwaite, Huddersfield” – from <a href=\"http://www.europeana.eu/portal/record/2022320/3F61C612ED9C42CCB85E533B4736795E8BDC7E77.html\" target=\"_blank\">this record</a>) can be read from Solr API in the following form:</p><div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>\"dc_title_txt\":{\n  \"fleming\":{\n    \"tf\":1,\n    \"df\":1073,\n    \"tf-idf\":9.319664492078285E-4\n  },\n  \"huddersfield\":{\n    \"tf\":1,\n    \"df\":12073,\n    \"tf-idf\":8.282945415389712E-5\n  },\n  \"mair\":{\n    \"tf\":1,\n    \"df\":178,\n    \"tf-idf\":0.0056179775280898875\n  },\n  \"slaithwaite\":{\n    \"tf\":1,\n    \"df\":477,\n    \"tf-idf\":0.0020964360587002098\n  },\n  \"wedding\":{\n    \"tf\":1,\n    \"df\":10226,\n    \"tf-idf\":9.778994719342852E-5\n  }\n}\n</pre></div></div><p>Note: I did not applied truncation and other fancy Solr analyzes on the fields, it is only the lower case transformation\napplied. The API returns the stored terms in alphabetical order. I removed some properties Solr reports but \nirrelevant from our current perspective.</p><p>I have extracted these info for the above mentiond three fields, and created two scores: a cumulative \nscore which summarizes the all terms in the field, and an average, which is the average of \nthe terms’ tf-idf score. Those records which don’t have the field get 0 for both.</p><p><img src=\"http://pkiraly.github.io/assets/uniquness.png\" class=\"real\" title=\"Uniqueness\" alt=\"Uniquenes\" /></p><p>The graphs visualize the average uniqueness of terms in the field. You can see that - as expected - there are lots of records where this value is quite low - it means that the words of the title are usually common words. There are however some titles which have unique words. If the value is higher than 1, it means that a unique word appears multiple times in this field (unique means that it appears in only one record). In this particularly record set there is no such an example, but there are others, such as “Doog, Doog, Doog, Doog” (an indian one) or “Csalo! Csalo!” (a Hungarian one). In this particular dataset the most unique title is “Iganteçtaco pronoua, eta hilen pronoua.” in which “eta” is a common term, “hilen” and “Iganteçtaco” is unique, and “pronoua” is repeated unique.</p><p>In order to make comparision of the scores and the record, a two new features were added to the record view.</p><p>The first one is the term frequency viewer. Here you can see the terms stored, the term frequency (how many times the term appears in the current field instance), the document frequency (how many document has this term in this field) and the tf-idf scores Solr calculated.</p><p><img src=\"http://pkiraly.github.io/assets/term-frequencies.png\" class=\"real\" title=\"Term frequencies\" alt=\"Term frequencies\" /></p><p>The second one is a “naked” record view: it displays the non technical fields of the <code class=\"highlighter-rouge\">ore:Aggregation</code> and <code class=\"highlighter-rouge\">ore:Proxy</code> of the record. Those fields which are not analyzed in the current session (such as tableOfContents) are displayed in grey.</p><p><img src=\"http://pkiraly.github.io/assets/record-view.png\" class=\"real\" title=\"Record view\" alt=\"Record view\" /></p><p>You can access thhe UI in the usual web interface:</p><p><a href=\"http://144.76.218.178/europeana-qa/\" target=\"_blank\">http://144.76.218.178/europeana-qa/</a></p><p>Select one of the last six dimension to get the results.</p><h2 id=\"events-presentation-article\">Events, presentation, article</h2><p>The big news is that the Europeana <a href=\"http://pro.europeana.eu/page/data-quality-committee\" target=\"_blank\">Data Quality Committee</a> as a Europeana Network and EuropeanaTech Working Group is formed in March. It is a great honor, that I was involved. We have a quite active message board, a bi-weekly teleconference and a bi-yearly face-to-face meeting.</p><p><img src=\"http://pkiraly.github.io/assets/gwdg-nachrichten.png\" class=\"real\" title=\"GWDG Nachrichten\" alt=\"GWDG Nachrichten\" /></p><p>I wrote an <a href=\"https://www.gwdg.de/documents/20182/27257/GN_3-2016_www.pdf\" target=\"_blank\">article for GWDG Nachrichten</a> about the metadata quality issues in Europeana covering the roles of the Data Quality Committee, and Mr Yahyapour, the director of GWDG wrote a recommendation in the editorial column. The GWDG Nachrichten is circulated in the Göttingen Campus and in Max Planck Institutes.</p><p><img src=\"http://pkiraly.github.io/assets/networkshop.png\" class=\"real\" title=\"networkshop\" alt=\"networkshop\" /></p><p>I presented the research in Networkshop 2016 conference at the end of March in my home town. It was exceptional for me that I talked at the Auditorium Maximum of the University of Debrecen where I saw soo many unforgottable concerts, movies and speachhes as a teenager. Unfortunatelly I was the very last speaker on that day, and there were no time left for discussions. Here you can see <a href=\"http://www.slideshare.net/pkiraly/a-jk-s-a-rosszak-metaadatok-minsgellenrzse\" target=\"_blank\">the slides</a> (note: they are in Hungarian).</p>"}}]}},"pageContext":{"alternative_id":9418}}