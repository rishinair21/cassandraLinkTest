{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"PatrickCallaghan/datastax-eventsourcing","alternative_id":9419,"content":"<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>This demo shows how Cassandra and DSE can be using to store and replay events.</p>\n<p>To use Spark you will need to provide your own Cassandra and Spark deployments. In this demo we will use DSE as they are already integrated.</p>\n<p>First we start DSE in SearchAnalyics mode to allow us to use both Spark and DSE Search -\n<a href=\"http://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/startStop/startDseStandalone.html?hl=starting\" rel=\"nofollow\">http://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/startStop/startDseStandalone.html?hl=starting</a></p>\n<p>The implementation uses bucketing to group all data into particular time buckets for replay. The time bucket used in this example is 1 minute but any time bucket can be used. Also depending how many days, months, years of events that need to be kept, it may be beneficial to spread the events over different tiers of tables.</p>\n<p>To create the schema, run the following</p>\n<pre>mvn clean compile exec:java -Dexec.mainClass=\"com.datastax.demo.SchemaSetup\" -DcontactPoints=localhost\n</pre>\n<p>To create the solr core to make our table searchable, run the following</p>\n<pre>dsetool create_core datastax.eventsource generateResources=true\n</pre>\n<p>To create events, run the following (Default of 10 million events)</p>\n<pre>mvn clean compile exec:java -Dexec.mainClass=\"com.datastax.events.Main\"  -DcontactPoints=localhost -DnoOfEvents=10000000\n</pre>\n<p>To replay a sample event set, run</p>\n<pre>mvn clean compile exec:java -Dexec.mainClass=\"com.datastax.events.ReadEvents\"  -DcontactPoints=localhost -Dfrom=yyyyMMdd-hhmmss -Dto=yyyyMMdd-hhmmss\n</pre>\n<p>eg</p>\n<pre>mvn clean compile exec:java -Dexec.mainClass=\"com.datastax.events.ReadEvents\"  -DcontactPoints=localhost -Dfrom=20160805-000000 -Dto=20160805-010000\n</pre>\n<p>This replays 2 scenarios</p>\n<pre>1. Replay all events for a specified time range\n2. Replay all events for a specified time range and a specific event type.\t\t\n</pre>\n<p>To run the webservice</p>\n<pre>mvn jetty:run -Djetty.port=8081 \n</pre>\n<p>To run a rest query, go the brower and enter a url in the format <a href=\"http://localhost:8080/datastax-eventsourcing/rest/getevents/from/to\" rel=\"nofollow\">http://localhost:8080/datastax-eventsourcing/rest/getevents/from/to</a>,\nwhere the date format is 'yyyyMMdd-hhmmss' e.g. For all events from midnight to 1:00 am on the 1st of August 2016 run -</p>\n<pre>http://localhost:8081/datastax-eventsourcing/rest/getevents/20160801-000000/20160801-010000/\n</pre>\n<p>We can also use cql to query using the Solr query from DSE Search</p>\n<p>Get all LOGIN Events from 9th Aug 2016 at 12:30 to 11th Aug 2016 at 12:30</p>\n<pre>select * from datastax.eventsource where solr_query = '{\"q\":\"eventtype:LOGIN\", \"fq\": \"time:[2016-08-09T12:30:00.000Z TO 2016-08-11T12:30:00.000Z]\", \"sort\":\"time desc\"}' limit 10000;\n</pre>\n<p>To use Spark, using DSE we can just 'dse spark' to use the repl.</p>\n<p>First we will create an Event object which will hold our events objects</p>\n<pre>case class Event (date: String, bucket: Int, id: java.util.UUID, data: String, eventtype: String, \naggregatetype: String, time: java.util.Date, loglevel: String, host: String); \nval events =  sc.cassandraTable[Event](\"datastax\", \"eventsource\").cache; \nevents.count\nval max = events.map(_.time).max\nval min = events.map(_.time).min\n</pre>\n<p>We can query our data and return events before or after a certain time.</p>\n<pre>val yesterday = new java.util.Date(java.util.Calendar.getInstance().getTime().getTime()-200000000);\nyesterday\nval before = events.filter(_.time.before(yesterday)); \nbefore.take(10).foreach(print) \nbefore.count\n \nval after = events.filter(_.time.after(yesterday)); \nafter.take(10).foreach(print) \nafter.count\n</pre>\n<p>Or we can use filtering to just get the events between two dates.</p>\n<pre>val start = new java.util.Date(java.util.Calendar.getInstance().getTime().getTime()-200000000);\nval end = new java.util.Date(java.util.Calendar.getInstance().getTime().getTime()-190000000);\nval filtered = events.filter(_.time.after(start)).filter(_.time.before(end)).cache;\nfiltered.count\n</pre>\n<p>Lets get all number of events per host and a list of all distinct hosts.</p>\n<pre>var hostCounts =  events.map(f =&gt; (f.host, 1)).reduceByKey(_ + _)\nhostCounts.collect().foreach(println)\nvar hosts =  hostCounts.map(f =&gt; (f._1))\nhosts.collect().foreach(println)\n</pre>\n<p>To use spark sql - try the following with a valid date</p>\n<pre>val results = sqlContext.sql(\"SELECT * from datastax.eventsource where date = '20161019'\")\n \nresults.take(5).foreach(println)\nval results = sqlContext.sql(\"SELECT * from datastax.eventsource where time &gt; '2016-10-22 16:18:07' \");\nresults.take(5).foreach(println)\nval results = sqlContext.sql(\"SELECT * from datastax.eventsource where time &gt; '2016-10-22 16:18:07' and time &lt; '2016-10-23 16:18:07'\");\nresults.count\n</pre>\n<p>To remove the tables and the schema, run the following.</p>\n<pre>mvn clean compile exec:java -Dexec.mainClass=\"com.datastax.demo.SchemaTeardown\"\n</pre>\n</article>"}}]}},"pageContext":{"alternative_id":9419}}