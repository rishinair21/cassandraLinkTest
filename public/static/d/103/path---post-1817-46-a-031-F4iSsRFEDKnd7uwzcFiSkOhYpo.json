{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"gianlucaborello/cassandradump","alternative_id":1817,"content":"<a href=\"https://travis-ci.org/gianlucaborello/cassandradump\"><img alt=\"https://travis-ci.org/gianlucaborello/cassandradump.svg?branch=master\" src=\"https://camo.githubusercontent.com/a9f74060d2029018273eedf4b8e6642ffe14ab40/68747470733a2f2f7472617669732d63692e6f72672f6769616e6c756361626f72656c6c6f2f63617373616e64726164756d702e7376673f6272616e63683d6d6173746572\" data-canonical-src=\"https://travis-ci.org/gianlucaborello/cassandradump.svg?branch=master\" /></a><h2><a id=\"user-content-description\" class=\"anchor\" href=\"https://github.com/gianlucaborello/cassandradump#description\" aria-hidden=\"true\"></a>Description</h2><p>A data exporting tool for Cassandra inspired from mysqldump, with some\nadditional slice and dice capabilities.</p><p>Disclaimer: most of the times, you really shouldn't be using this. It's\nfragile, non-scalable, inefficient and verbose. Cassandra already offers\nexcellent exporting/importing tools:</p><ul><li>Snapshots</li>\n<li>CQL's COPY FROM/TO</li>\n<li>sstable2json</li>\n</ul><p>However, especially during development, I frequently need to:</p><ul><li>Quickly take a snapshot of an entire keyspace, and import it just as\nquickly without copying too many files around or losing too much time</li>\n<li>Ability to take a very small subset of a massive production database\n(according to some CQL-like filtering) and import it quickly on my\ndevelopment environment</li>\n</ul><p>If these use cases sound familiar, this tool might be useful for you.</p><p>It's still missing many major Cassandra features that I don't use daily,\nso feel free to open an issue pointing them out (or send a pull request)\nif you need something.</p><h2><a id=\"user-content-usage\" class=\"anchor\" href=\"https://github.com/gianlucaborello/cassandradump#usage\" aria-hidden=\"true\"></a>Usage</h2><p>The help should already contain some useful information:</p><pre>usage: cassandradump.py [-h] [--cf CF] [--export-file EXPORT_FILE]\n                        [--filter FILTER] [--host HOST] [--port PORT]\n                        [--import-file IMPORT_FILE] [--keyspace KEYSPACE]\n                        [--exclude-cf EXCLUDE_CF] [--no-create] [--no-insert]\n                        [--password PASSWORD]\n                        [--protocol-version PROTOCOL_VERSION] [--quiet]\n                        [--sync] [--username USERNAME] [--ssl]\n                        [--certfile CERTFILE]\nA data exporting tool for Cassandra inspired from mysqldump, with some added\nslice and dice capabilities.\noptional arguments:\n  -h, --help            show this help message and exit\n  --cf CF               export a column family. The name must include the\n                        keyspace, e.g. \"system.schema_columns\". Can be\n                        specified multiple times\n  --export-file EXPORT_FILE\n                        export data to the specified file\n  --filter FILTER       export a slice of a column family according to a CQL\n                        filter. This takes essentially a typical SELECT query\n                        stripped of the initial \"SELECT ... FROM\" part (e.g.\n                        \"system.schema_columns where keyspace_name\n                        ='OpsCenter'\", and exports only that data. Can be\n                        specified multiple times\n  --host HOST           the address of a Cassandra node in the cluster\n                        (localhost if omitted)\n  --port PORT           the port of a Cassandra node in the cluster\n                        (9042 if omitted)\n  --import-file IMPORT_FILE\n                        import data from the specified file\n  --keyspace KEYSPACE   export a keyspace along with all its column families.\n                        Can be specified multiple times\n  --exclude-cf EXCLUDE_CF\n                        when using --keyspace, specify column family to\n                        exclude. Can be specified multiple times\n  --no-create           don't generate create (and drop) statements\n  --no-insert           don't generate insert statements\n  --password PASSWORD   set password for authentication (only if\n                        protocol-version is set)\n  --protocol-version PROTOCOL_VERSION\n                        set auth_provider version (required for\n                        authentication)\n  --quiet               quiet progress logging\n  --sync                import data in synchronous mode (default asynchronous)\n  --username USERNAME   set username for auth (only if protocol-version is\n                        set)\n  --ssl                 enable ssl connection to Cassandra cluster.  Must also\n                        set --certfile.\n  --certfile CERTFILE   ca cert file for SSL.  Assumes --ssl.\n</pre><p>In its simplest invocation, it exports data and schemas for all\nkeyspaces:</p><pre>$ python cassandradump.py --export-file dump.cql\nExporting all keyspaces\nExporting schema for keyspace OpsCenter\nExporting schema for column family OpsCenter.events_timeline\nExporting data for column family OpsCenter.events_timeline\nExporting schema for column family OpsCenter.settings\nExporting data for column family OpsCenter.settings\nExporting schema for column family OpsCenter.rollups60\nExporting data for column family OpsCenter.rollups60\n...\n</pre><pre>$ cat dump.cql\nDROP KEYSPACE IF EXISTS \"OpsCenter\";\nCREATE KEYSPACE \"OpsCenter\" WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}  AND durable_writes = true;\nDROP TABLE IF EXISTS \"OpsCenter\".\"events_timeline\";\nCREATE TABLE \"OpsCenter\".events_timeline (key text, column1 bigint, value blob, PRIMARY KEY (key, column1)) WITH COMPACT STORAGE AND CLUSTERING ORDER BY (column1 ASC) AND caching = '{\"keys\":\"ALL\", \"rows_per_partition\":\"NONE\"}' AND comment = '{\"info\": \"OpsCenter management data.\", \"version\": [5, 1, 0]}' AND compaction = {'min_threshold': '4', 'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '8'} AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'} AND dclocal_read_repair_chance = 0.0 AND default_time_to_live = 0 AND gc_grace_seconds = 864000 AND max_index_interval = 2048 AND memtable_flush_period_in_ms = 0 AND min_index_interval = 128 AND read_repair_chance = 0.25 AND speculative_retry = 'NONE';\nINSERT INTO \"OpsCenter\".\"events_timeline\" (key, column1, value) VALUES ('201501', 1419841027332869, 0x)\nINSERT INTO \"OpsCenter\".\"events_timeline\" (key, column1, value) VALUES ('201501', 1419841027352525, 0x)\nINSERT INTO \"OpsCenter\".\"events_timeline\" (key, column1, value) VALUES ('201501', 1419928979070954, 0x)\n...\n</pre><p>The created dump file can be directly used with <code>cqlsh -f</code>, or there's\nalso a <code>--import-file</code> that uses asynchronous import so it goes\ndefinitely fast.</p><p>Using <code>--keyspace</code>, it's possible to filter for a specific set of\nkeyspaces</p><pre>$ python cassandradump.py --keyspace system --export-file dump.cql\nExporting schema for keyspace system\nExporting schema for column family system.peers\nExporting data for column family system.peers\nExporting schema for column family system.range_xfers\nExporting data for column family system.range_xfers\nExporting schema for column family system.schema_columns\nExporting data for column family system.schema_columns\n...\n</pre><pre>$ cat dump.cql\nDROP KEYSPACE IF EXISTS \"system\";\nCREATE KEYSPACE system WITH replication = {'class': 'LocalStrategy'}  AND durable_writes = true;\nDROP TABLE IF EXISTS \"system\".\"peers\";\nCREATE TABLE system.peers (peer inet PRIMARY KEY, data_center text, host_id uuid, preferred_ip inet, rack text, release_version text, rpc_address inet, schema_version uuid, tokens set&lt;text&gt;) WITH bloom_filter_fp_chance = 0.01 AND caching = '{\"keys\":\"ALL\", \"rows_per_partition\":\"NONE\"}' AND comment = 'known peers in the cluster' AND compaction = {'min_threshold': '4', 'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32'} AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'} AND dclocal_read_repair_chance = 0.0 AND default_time_to_live = 0 AND gc_grace_seconds = 0 AND max_index_interval = 2048 AND memtable_flush_period_in_ms = 3600000 AND min_index_interval = 128 AND read_repair_chance = 0.0 AND speculative_retry = '99.0PERCENTILE';\n...\n</pre><p>Using <code>--cf</code>, it's possible to filter for a specific set of column\nfamilies:</p><pre>$ python cassandradump.py --cf OpsCenter.rollups7200 --no-create --export-file dump.cql\nExporting data for column family OpsCenter.rollups7200\n</pre><pre>$ cat dump.cql\nINSERT INTO \"OpsCenter\".\"rollups7200\" (key, column1, value) VALUES ('127.0.0.1-foo', 718946047, 0x000000000000000000000000)\nINSERT INTO \"OpsCenter\".\"rollups7200\" (key, column1, value) VALUES ('127.0.0.1-foo', 718953247, 0x000000000000000000000000)\nINSERT INTO \"OpsCenter\".\"rollups7200\" (key, column1, value) VALUES ('127.0.0.1-foo', 718960447, 0x000000000000000000000000)\nINSERT INTO \"OpsCenter\".\"rollups7200\" (key, column1, value) VALUES ('127.0.0.1-foo', 718967647, 0x000000000000000000000000)\nINSERT INTO \"OpsCenter\".\"rollups7200\" (key, column1, value) VALUES ('127.0.0.1-foo', 719032447, 0x40073fc200000000437bc000)\n...\n</pre><p>Using <code>--no-insert</code> and <code>--no-create</code> it's possible to tweak what\nCQL statements are actually included in the dump.</p><p>Most of the times, the column families in a production scenario are\nhuge, and you might just want a little slice of it. With <code>--filter</code>,\nit's possible to specify a set of CQL filters, and just the data that\nsatisfies those filters will be included in the dump:</p><pre>$ python cassandradump.py --filter \"system.schema_columns WHERE keyspace_name='OpsCenter'\" --export-file dump.cql\nExporting data for filter \"system.schema_columns where keyspace_name ='OpsCenter'\"\n</pre><pre>$ cat dump.cql\nINSERT INTO \"system\".\"schema_columns\" (keyspace_name, columnfamily_name, column_name, component_index, index_name, index_options, index_type, type, validator) VALUES ('OpsCenter', 'backup_reports', 'backup_id', 1, NULL, 'null', NULL, 'clustering_key', 'org.apache.cassandra.db.marshal.UTF8Type')\nINSERT INTO \"system\".\"schema_columns\" (keyspace_name, columnfamily_name, column_name, component_index, index_name, index_options, index_type, type, validator) VALUES ('OpsCenter', 'backup_reports', 'deleted_at', 4, NULL, 'null', NULL, 'regular', 'org.apache.cassandra.db.marshal.TimestampType')\n</pre>"}}]}},"pageContext":{"alternative_id":1817}}