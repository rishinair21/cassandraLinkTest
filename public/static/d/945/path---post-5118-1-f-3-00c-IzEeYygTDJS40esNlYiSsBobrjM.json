{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Multiple Data Center Deployment","alternative_id":5118,"content":"<div id=\"mw-content-text\" lang=\"en\" dir=\"ltr\" class=\"mw-content-ltr\" readability=\"159\"><div id=\"toc\" class=\"toc\"><p><h2>Contents</h2></p>\n<ul><li class=\"toclevel-1 tocsection-1\"><a href=\"https://docs.genesys.com/Documentation/HTCC/latest/Dep/multiDCDepExample#Multiple_Data_Center_Deployment\">1 Multiple Data Center Deployment</a>\n<ul><li class=\"toclevel-2 tocsection-2\"><a href=\"https://docs.genesys.com/Documentation/HTCC/latest/Dep/multiDCDepExample#Overview\">1.1 Overview</a></li>\n<li class=\"toclevel-2 tocsection-3\"><a href=\"https://docs.genesys.com/Documentation/HTCC/latest/Dep/multiDCDepExample#Architecture\">1.2 Architecture</a></li>\n<li class=\"toclevel-2 tocsection-4\"><a href=\"https://docs.genesys.com/Documentation/HTCC/latest/Dep/multiDCDepExample#Incoming_traffic_distribution\">1.3 Incoming traffic distribution</a></li>\n<li class=\"toclevel-2 tocsection-5\"><a href=\"https://docs.genesys.com/Documentation/HTCC/latest/Dep/multiDCDepExample#Configuration\">1.4 Configuration</a></li>\n<li class=\"toclevel-2 tocsection-10\"><a href=\"https://docs.genesys.com/Documentation/HTCC/latest/Dep/multiDCDepExample#GWS_Cluster_Management\">1.5 GWS Cluster Management</a></li>\n</ul></li>\n</ul></div>\n<p>Starting in release 8.5.2, GWS supports a deployment with multiple (two or more) data centers. This section describes this type of deployment.\n</p>\n<h2>Overview</h2>\n<p>A multiple data center deployment implies a logical partitioning of all GWS nodes into segregated groups that are using dedicated service resources, such as T-Server, StatServers, and so on.\n</p><p>The topology of a GWS Cluster can be considered as a standard directory tree where a leaf node is a GWS data center. The following diagram shows a GWS Cluster with 2 geographical regions (US and EU), and 3 GWS data centers (East and West in the US region, and EU as its own data center).\n</p>\n<div class=\"center\"><div class=\"floatnone\"><a href=\"https://docs.genesys.com/File:Gws_multidatacenters-topo_851.png\" class=\"image\"><img alt=\"Gws multidatacenters-topo 851.png\" src=\"https://docs.genesys.com/images/3/34/Gws_multidatacenters-topo_851.png\" width=\"200\" height=\"174\"/></a></div></div>\n<p>For data handling and distribution between GWS data centers, the following third-party applications are used:\n</p>\n<ul><li> Cassandra—a NoSQL database cluster with multiple data centers with data replication between each other.</li>\n<li> Elasticsearch—a search engine which provides fast and efficient solution for pattern searching across Cassandra data. Genesys recommends that each GWS data center have an independent, standalone Elasticsearch cluster.</li></ul><h2>Architecture</h2>\n<p>A typical GWS data center in a multiple data center deployment consists of the following components:\n</p>\n<ul><li> 2 GWS API nodes</li>\n<li> 2 GWS Stat nodes</li>\n<li> 3 Cassandra nodes</li>\n<li> 3 Elasticsearch nodes</li>\n<li> 1 GWS Sync node (only for Primary region)</li></ul><p>The following diagram illustrates the architecture of this sample multiple data center deployment.\n</p>\n<div class=\"center\"><div class=\"thumb tnone\"><div class=\"thumbinner\"><a href=\"https://docs.genesys.com/File:Gws-multidc-arch-852.png\" class=\"image\"><img alt=\"Gws-multidc-arch-852.png\" src=\"https://docs.genesys.com/images/a/a3/Gws-multidc-arch-852.png\" width=\"300\" height=\"170\" class=\"thumbimage\"/></a>  </div></div></div>\n<p>Note the following restrictions of this architecture:\n</p>\n<ul><li> Only 1 Sync node is deployed within a GWS Cluster</li>\n<li> Each data center must have a dedicated list of Genesys servers, such as Configuration Servers, Stat Servers, and T-Servers.</li>\n<li> The Cassandra Keyspace definition must comply with the number of GWS data centers.</li>\n<li> Each GWS data center must have its own standalone and dedicated Elasticsearch Cluster.</li>\n<li> The GWS node identity must be unique across the entire Cluster.</li></ul><h2>Incoming traffic distribution</h2>\n<p>GWS does not support traffic distribution between GWS nodes natively. To enable this, any third-party reverse proxy can be used that can provide a session stickiness based on association with sessions on the backend server; this rule is commonly referred to as <i>Application-Controlled Session Stickiness</i>. In other words, when a GWS node creates a new session and returns <tt>Set-Cookie</tt> in response, the load-balancer should issue its own stickiness cookie. GWS uses a <tt>JSESSIONID</tt> cookie by default, but this can be reconfigured by using the following option in the <b>application.yaml</b> file:\n</p>\n<div dir=\"ltr\" readability=\"5\"><div class=\"source-text\" readability=\"31\"><pre>jetty:\n  cookies:\n    name: &lt;HTTP Session Cookie Name&gt;</pre></div></div>\n<h2>Configuration</h2>\n<p>This section describes the additional configuration required to set up a multiple data center deployment.\n</p>\n<h3>Cassandra</h3>\n<p>Configure Cassandra in the same way as for a single data center deployment (described earlier in this document), making sure that the following conditions are met:\n</p>\n<ul><li> All Cassandra nodes must have the same cluster name in <b>application.yaml</b>.</li>\n<li> The same data center name must be assigned to all Cassandra nodes across the GWS data center (specified in <b>cassandra-network.properties</b> or  <b>cassandra-rackdc.properties</b>, depending on the Cassandra deployment).</li>\n<li> The Keyspace definition must be created based on <b>ks-schema-prod_HA.cql</b> from the Installation Package, changing only the following:\n<ul><li> The name and <tt>ReplicationFactor</tt> of each.</li>\n<li> The number of data centers between which the replication is enabled.</li>\n</ul><p>For example:\n</p>\n<div dir=\"ltr\" readability=\"7\"><div class=\"source-text\" readability=\"35\"><pre>CREATE KEYSPACE sipfs WITH replication = {'class': 'NetworkTopologyStrategy', 'USWest': '3', 'USEast': '3', 'EU': '3'} AND durable_writes = true;</pre></div></div></li>\n</ul><h3>Genesys Web Services and Applications</h3>\n<p>The position of each node inside the GWS Cluster is specified by the mandatory property <b>nodePath</b> provided in <b>application.yaml</b>. The value of this property is in the standard file path format, and uses the forward slash (<tt>/</tt>) symbol as a delimiter. This property has the following syntax:\n</p>\n<div dir=\"ltr\" readability=\"5\"><div class=\"source-text\" readability=\"31\"><pre>nodePath: &lt;path-to-node-in-cluster&gt;/&lt;node-identity&gt;</pre></div></div>\n<p>Where:\n</p>\n<ul><li> <tt>&lt;path-to-node-in-cluster&gt;</tt> is the path inside the cluster with all logical sub-groups.</li>\n<li> &lt;node-identity&gt; is the unique identity of the node. Genesys recommends that you use the name of the host on which this data center is running for this parameter.</li></ul><p>For example:\n</p>\n<div dir=\"ltr\" readability=\"5\"><div class=\"source-text\" readability=\"31\"><pre>nodePath: /US/West/api-node-1</pre></div></div>\n<p>In addition to the configuration options set in the standard deployment procedure, set the following configuration options in <b>application.yaml</b> for all GWS nodes to enable the multiple data center functionality:\n</p>\n<div dir=\"ltr\" readability=\"6\"><div class=\"source-text\" readability=\"33\"><pre>cassandraCluster:\n  write_consistency_level: CL_LOCAL_QUORUM\n  read_consistency_level: CL_LOCAL_QUORUM\n \nserverSettings:\n nodePath: &lt;path-to-node-in-cluster&gt;/&lt;node-identity&gt;\n \nstatistics:\n locationAwareMonitoringDistribution: true\n enableMultipleDataCenterMonitoring: true</pre></div></div>\n<div class=\"new-note Important\"><p>Important</p>\n<ul><li> If the replication factor is changed in the keyspace definition (that is, if additional Cassandra nodes are added) then <tt>replication_factor</tt> in <tt>cassandraCluster</tt> of <b>application.yaml</b> should be adjusted to agree with the keyspace definition.</li></ul>\n</div>\n<p>In addition, set the following options on all Stat nodes:\n</p>\n<div dir=\"ltr\" readability=\"6\"><div class=\"source-text\" readability=\"32\"><pre>serverSettings:\n   elasticSearchSettings:\n      enableScheduledIndexVerification: true\n      enableIndexVerificationAtStartUp: true</pre></div></div>\n<h3>GWS Sync Node</h3>\n<p>A Synchronization Node is a special node, and as indicated elsewhere in this Guide, in Deployment Guide, this node imports existing data from Configuration Server and keeps track of all changes. \n</p>\n<div class=\"new-note Warning\" readability=\"10\"><p>Warning</p>Only one Sync Node can be running at any point of time even if you have two sync nodes in your architecture. Genesys strongly recommends that you deploy Sync node in the same data center where the primary Configuration Server is located.</div>\n<p>If any disaster causes this node to terminate or become unavailable because of network issues or the whole data center goes down, provisioning of any object in Configuration Server will not be reflected in the GWS cluster until the Sync node is recovered. Other functionality related to Agent activity is not affected in this case.\n</p>\n<h3>Configuration Server</h3>\n<p>The GWS Cluster Application object (typically named <b>CloudCluster</b>) in the Configuration Database must be configured with a specified location for each connection to Genesys servers, like Configuration Server, Stat Server, T-Server, and so on. This setting defines which server instance is used by the GWS node based on its position in the GWS Cluster. The visibility resource rule is based on comparing the <b>nodePath</b> attribute and the specified specification in connections. \n</p><p>Set these locations as Application Parameters of each connection, as follows:\n</p>\n<div dir=\"ltr\" readability=\"5\"><div class=\"source-text\" readability=\"31\"><pre>locations=&lt;path-to-node-in-cluster&gt;</pre></div></div>\n<p>where <tt>&lt;path-to-node-in-cluster&gt;</tt> is the same path to the data center specified by the <b>nodePath</b> property in <b>application.yaml</b>.\n</p><p>For example:\n</p>\n<div dir=\"ltr\"><div class=\"source-text\"><pre>locations=/US/West</pre></div></div>\n<h2>GWS Cluster Management</h2>\n<h3>Add a New Data Center</h3>\n<p>Before deploying new GWS nodes, you must extend the Cassandra cluster by adding new nodes into the data ring and updating the keyspace definition with a replication strategy for this new data center. Using the CQLSH utility, run the following command to update the existing Cassandra keyspace:\n</p>\n<div dir=\"ltr\" readability=\"8\"><div class=\"source-text\" readability=\"36\"><pre>ALTER KEYSPACE sipfs WITH REPLICATION = {'class': 'NetworkTopologyStrategy', 'USWest': '3', 'USEast': '3', 'EU': '3', 'CA': '3'};</pre></div></div>\n<p>After you have deployed the new Cassandra data center, you can use the normal procedure to deploy additional GWS nodes.\n</p>\n<h3>Remove an Existing Data Center</h3>\n<p>Before removing a Cassandra data center, you must stop all GWS nodes in this data center to avoid writing data into Cassandra.\n</p>\n<ol><li> Stop all GWS nodes, and remove them if necessary.</li> \n<li> Update the keyspace definition by removing the appropriate data center from the replication strategy. Use the same CQL command as you used for adding a new data center.</li>\n<li> Run the following command on each Cassandra node in the data center being removed:\n<div dir=\"ltr\" readability=\"5\"><div class=\"source-text\" readability=\"31\"><pre>nodetool decommission</pre></div></div></li>\n<li> Stop all Cassandra nodes, and remove them if necessary.</li>\n</ol></div><p> This page was last modified on 30 November 2017, at 09:41.</p>"}}]}},"pageContext":{"alternative_id":5118}}