{
  "page": 2,
  "limit": 40,
  "pages": 12,
  "total": 474,
  "_links": {
    "self": {
      "href": "http://leaves.anant.us:82/api/entries?sort=created&order=desc&tags=cassandra&since=0&page=2&perPage=40"
    },
    "first": {
      "href": "http://leaves.anant.us:82/api/entries?sort=created&order=desc&tags=cassandra&since=0&page=1&perPage=40"
    },
    "last": {
      "href": "http://leaves.anant.us:82/api/entries?sort=created&order=desc&tags=cassandra&since=0&page=12&perPage=40"
    },
    "next": {
      "href": "http://leaves.anant.us:82/api/entries?sort=created&order=desc&tags=cassandra&since=0&page=3&perPage=40"
    },
    "previous": {
      "href": "http://leaves.anant.us:82/api/entries?sort=created&order=desc&tags=cassandra&since=0&page=1&perPage=40"
    }
  },
  "_embedded": {
    "items": [
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12977,
        "uid": null,
        "title": "Cassandra Datacenter & Racks",
        "url": "https://compositecode.blog/2018/10/29/cassandra-datacenter-racks/",
        "content": "<p>This last post in this series is <a href=\"https://compositecode.blog/2018/10/08/distributed-database-things-to-know-consistent-hashing/\" target=\"_blank\" rel=\"noopener\">Distributed Database Things to Know: Consistent Hashing</a>.</p>\n<p>Let’s talk about the analogy of <em>Apache Cassandra Datacenter &amp; Racks</em> to actual datacenter and racks. I kind of enjoy the use of the terms datacenter and racks to describe architectural elements of Cassandra. However, as time moves on the relationship between these terms and why they’re called datacenter and racks can be obfuscated.</p>\n<p>Take for instance, a datacenter could just be a cloud provider, an actual physical datacenter location, a zone in Azure, or region in some other provider. What an actual Datacenter in Cassandra parlance actually is can vary, but the origins of why it’s called a Datacenter remains the same. The elements of racks also can vary, but also remain the same.</p>\n<h2>Origins: Racks &amp; Datacenters?</h2>\n<p>Let’s cover the actual things in this industry we call <em>datacenter</em> and <em>racks</em> first, unrelated to Apache Cassandra terms.</p>\n<p><strong>Racks</strong>: The easiest way to describe a physical rack is to show pictures of datacenter racks via the ole’ Google images.</p>\n<p><a href=\"https://www.google.com/search?rlz=1C5CHFA_enUS793US793&amp;biw=1280&amp;bih=1090&amp;tbm=isch&amp;sa=1&amp;ei=mjvWW9X-KsW10PEP76i52AI&amp;q=data+center+racks&amp;oq=data+center+racks&amp;gs_l=img.3..0l2j0i7i30j0i5i30l2j0i8i30l2j0i24l3.4078.5164..5371...0.0..0.45.507.12......1....1..gws-wiz-img.......0i8i7i30j0i7i5i30.b-QP90OSuUs\" target=\"_blank\" rel=\"noopener\"><img data-attachment-id=\"14484\" data-permalink=\"https://compositecode.blog/2018/10/29/cassandra-datacenter-racks/racks/\" data-orig-file=\"https://compositecode.files.wordpress.com/2018/08/racks.png?w=625\" data-orig-size=\"1277,581\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"racks\" data-image-description=\"\" data-medium-file=\"https://compositecode.files.wordpress.com/2018/08/racks.png?w=625?w=300\" data-large-file=\"https://compositecode.files.wordpress.com/2018/08/racks.png?w=625?w=625\" class=\"alignnone size-full wp-image-14484\" src=\"https://compositecode.files.wordpress.com/2018/08/racks.png?w=625\" alt=\"racks.png\" srcset=\"https://compositecode.files.wordpress.com/2018/08/racks.png?w=625 625w, https://compositecode.files.wordpress.com/2018/08/racks.png?w=1248 1248w, https://compositecode.files.wordpress.com/2018/08/racks.png?w=150 150w, https://compositecode.files.wordpress.com/2018/08/racks.png?w=300 300w, https://compositecode.files.wordpress.com/2018/08/racks.png?w=768 768w, https://compositecode.files.wordpress.com/2018/08/racks.png?w=1024 1024w\" /></a></p>\n<p>A rack is something that is located in a data-center, or even just someone’s garage in some odd scenarios. Ya know, if somebody wants serious hardware to work with. The rack then has a number of servers, often various kinds, within that rack itself. As you can see from the images above there’s a wide range of these racks.</p>\n<p><strong>Datacenter</strong>: Again the easiest way to describe a datacenter is to just look at a bunch of pictures of datacenter, albeit you see lots of racks again. But really, that’s what a datacenter is, is a building that has lots and lots of racks.</p>\n<p><a href=\"https://www.google.com/search?q=data+center&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=0ahUKEwj0wYHomqreAhXNGTQIHdEDBXMQ_AUIDygC&amp;biw=1280&amp;bih=1090\" target=\"_blank\" rel=\"noopener\"><img data-attachment-id=\"14485\" data-permalink=\"https://compositecode.blog/2018/10/29/cassandra-datacenter-racks/data-center/\" data-orig-file=\"https://compositecode.files.wordpress.com/2018/08/data-center.png?w=625\" data-orig-size=\"1279,765\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"data-center\" data-image-description=\"\" data-medium-file=\"https://compositecode.files.wordpress.com/2018/08/data-center.png?w=625?w=300\" data-large-file=\"https://compositecode.files.wordpress.com/2018/08/data-center.png?w=625?w=625\" class=\"alignnone size-full wp-image-14485\" src=\"https://compositecode.files.wordpress.com/2018/08/data-center.png?w=625\" alt=\"data-center.png\" srcset=\"https://compositecode.files.wordpress.com/2018/08/data-center.png?w=625 625w, https://compositecode.files.wordpress.com/2018/08/data-center.png?w=1250 1250w, https://compositecode.files.wordpress.com/2018/08/data-center.png?w=150 150w, https://compositecode.files.wordpress.com/2018/08/data-center.png?w=300 300w, https://compositecode.files.wordpress.com/2018/08/data-center.png?w=768 768w, https://compositecode.files.wordpress.com/2018/08/data-center.png?w=1024 1024w\" /></a></p>\n<p>However in Apache Cassandra (and respectively DataStax Enterprise products) a datacenter and rack do not directly correlate to a physical rack or datacenter. The idea is more of an abstraction than hard mapping to the physical realm. In turn it is better to think of datacenter and racks as a way to structure and organize your DataStax Enterprise or Apache Cassandra architecture. From a tree perspective of organizing your cluster, think of things in this hierarchy.</p>\n<ul><li>Cluster\n<ul><li>Datacenter(s)\n<ul><li>Rack(s)\n<ul><li>Server(s)\n<ul><li>Node (vnode)</li>\n</ul></li>\n</ul></li>\n</ul></li>\n</ul></li>\n</ul><h2>Apache Cassandra Datacenter</h2>\n<p>An Apache Cassandra Datacenter is a group of nodes, related and configured within a cluster for replication purposes. Setting up a specific set of related nodes into a datacenter helps to reduce latency, prevent transactions from impact by other workloads, and related effects. The replication factor can also be setup to write to multiple datacenter, providing additional flexibility in architectural design and organization. One specific element of datacenter to note is that they must contain only one node type:</p>\n<ul><li>Transactional: A standard Cassandra node.</li>\n<li><a href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/graph/graphTOC.html\" target=\"_blank\" rel=\"noopener\">DataStax Enterprise Graph</a>: The Graph database offering from Datastax.</li>\n<li><a href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/analytics/analyticsOverview.html\" target=\"_blank\" rel=\"noopener\">DataStax Enterprise Analytics</a>: An integration with <a href=\"https://spark.apache.org/\" target=\"_blank\" rel=\"noopener\">Apache Spark</a>.</li>\n<li><a href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/search/searchTOC.html\" target=\"_blank\" rel=\"noopener\">DataStax Enterprise Search</a>: Integration with <a href=\"http://lucene.apache.org/solr/\" target=\"_blank\" rel=\"noopener\">Apache Solr</a>.</li>\n<li><a href=\"https://docs.datastax.com/en/dse/6.0/dse-dev/datastax_enterprise/analytics/dseSearchAnalyticsOverview.html\" target=\"_blank\" rel=\"noopener\">DataStax Enterprise Search Analytics</a>: Search queries within analytics jobs.</li>\n</ul><p>Depending on the replication factor, data can be written to multiple datacenters. Datacenters must never span physical locations.Each datacenter usually contains only one node type. The node types are:</p>\n<ul class=\"ul\"><li class=\"li\">Transactional: Previously referred to as a Cassandra node.</li>\n<li class=\"li\"><a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/graph/graphTOC.html\" target=\"_blank\" rel=\"noopener\">DSE Graph</a>: A graph database for managing, analyzing, and searching highly-connected data.</li>\n<li class=\"li\"><a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/analytics/analyticsOverview.html\" target=\"_blank\" rel=\"noopener\">DSE Analytics</a>: Integration with Apache Spark.</li>\n<li class=\"li\"><a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/search/searchAbout.html\" target=\"_blank\" rel=\"noopener\">DSE Search</a>: Integration with Apache Solr. Previously referred to as a Solr node.</li>\n<li class=\"li\"><a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/analytics/dseSearchAnalyticsOverview.html\" target=\"_blank\" rel=\"noopener\">DSE SearchAnalytics</a>: DSE Search queries within DSE Analytics jobs.</li>\n</ul><h2>Apache Cassandra Racks</h2>\n<p>An Apache Cassandra Rack is a grouped set of servers. The architecture of Cassandra uses racks so that no replica is stored redundantly inside a singular rack, ensuring that replicas are spread around through different racks in case one rack goes down. Within a datacenter there could be multiple racks with multiple servers, as the hierarchy shown above would dictate.</p>\n<p>To determine where data goes within a rack or sets of racks Apache Cassandra uses what is referred to as a snitch. A snitch determines which racks and datacenter a particular node belongs to, and by respect of that, determines where the replicas of data will end up. This replication strategy which is informed by the snitch can take the form of numerous kinds of snitches, some examples include;</p>\n<ul><li>SimpleSnitch – this snitch treats order as proximity. This is primarily only used when in a single-datacenter deployment.</li>\n<li>Dynamic Snitching – the dynamic snitch monitors read latencies to avoid reading from hosts that have slowed down.</li>\n<li>RackInferringSnitch – Proximity is determined by rack and datacenter, assumed corresponding to 3rd and 2nd octet of each node’s IP address. This particular snitch is often used as an example for writing a custom snitch class since it isn’t particularly useful unless it happens to match one’s deployment conventions.</li>\n</ul><p>In the future I’ll outline a few more snitches, how some of them work with more specific detail, and I’ll get into a whole selection of other topics. Be sure to subscribe to the blog, the ole’ RSS feed works great too, and follow <a href=\"https://twitter.com/CompositeCode\" target=\"_blank\" rel=\"noopener\">@CompositeCode</a> for blog updates. For discourse and hot takes follow me <a href=\"https://twitter.com/Adron\" target=\"_blank\" rel=\"noopener\">@Adron</a>.</p>\n<blockquote>\n<h2>Distributed Database Things to Know Series</h2>\n<ol><li><a href=\"https://compositecode.blog/2018/10/08/distributed-database-things-to-know-consistent-hashing/\" target=\"_blank\" rel=\"noopener\">Consistent Hashing</a></li>\n<li>Apache Cassandra Datacenter &amp; Racks (this post)</li>\n</ol></blockquote>\n\n\t\t<div id=\"geo-post-14483\" class=\"geo geo-post\">\n\t\t\t47.666712\n\t\t\t-122.383132\n\t\t</div>",
        "created_at": "2019-01-23T21:47:37+0000",
        "updated_at": "2019-01-23T21:47:43+0000",
        "published_at": "2018-10-29T14:13:20+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 4,
        "domain_name": "compositecode.blog",
        "preview_picture": "https://compositecode.files.wordpress.com/2018/10/fixed1.png?w=1200",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12977"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12976,
        "uid": null,
        "title": "Cassandra Datacenter & Racks",
        "url": "https://compositecode.blog/category/datastax-enterprise/",
        "content": "<div class=\"entry-content\">\n\t\t\t\t<p>This last post in this series is <a href=\"https://compositecode.blog/2018/10/08/distributed-database-things-to-know-consistent-hashing/\" target=\"_blank\" rel=\"noopener\">Distributed Database Things to Know: Consistent Hashing</a>.</p>\n<p>Let’s talk about the analogy of <em>Apache Cassandra Datacenter &amp; Racks</em> to actual datacenter and racks. I kind of enjoy the use of the terms datacenter and racks to describe architectural elements of Cassandra. However, as time moves on the relationship between these terms and why they’re called datacenter and racks can be obfuscated.</p>\n<p>Take for instance, a datacenter could just be a cloud provider, an actual physical datacenter location, a zone in Azure, or region in some other provider. What an actual Datacenter in Cassandra parlance actually is can vary, but the origins of why it’s called a Datacenter remains the same. The elements of racks also can vary, but also remain the same.</p>\n<h2>Origins: Racks &amp; Datacenters?</h2>\n<p>Let’s cover the actual things in this industry we call <em>datacenter</em> and <em>racks</em> first, unrelated to Apache Cassandra terms.</p>\n<p><strong>Racks</strong>: The easiest way to describe a physical rack is to show pictures of datacenter racks via the ole’ Google images.</p>\n<p><a href=\"https://www.google.com/search?rlz=1C5CHFA_enUS793US793&amp;biw=1280&amp;bih=1090&amp;tbm=isch&amp;sa=1&amp;ei=mjvWW9X-KsW10PEP76i52AI&amp;q=data+center+racks&amp;oq=data+center+racks&amp;gs_l=img.3..0l2j0i7i30j0i5i30l2j0i8i30l2j0i24l3.4078.5164..5371...0.0..0.45.507.12......1....1..gws-wiz-img.......0i8i7i30j0i7i5i30.b-QP90OSuUs\" target=\"_blank\" rel=\"noopener\"><img data-attachment-id=\"14484\" data-permalink=\"https://compositecode.blog/2018/10/29/cassandra-datacenter-racks/racks/\" data-orig-file=\"https://compositecode.files.wordpress.com/2018/08/racks.png?w=625\" data-orig-size=\"1277,581\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"racks\" data-image-description=\"\" data-medium-file=\"https://compositecode.files.wordpress.com/2018/08/racks.png?w=625?w=300\" data-large-file=\"https://compositecode.files.wordpress.com/2018/08/racks.png?w=625?w=625\" class=\"alignnone size-full wp-image-14484\" src=\"https://compositecode.files.wordpress.com/2018/08/racks.png?w=625\" alt=\"racks.png\" srcset=\"https://compositecode.files.wordpress.com/2018/08/racks.png?w=625 625w, https://compositecode.files.wordpress.com/2018/08/racks.png?w=1248 1248w, https://compositecode.files.wordpress.com/2018/08/racks.png?w=150 150w, https://compositecode.files.wordpress.com/2018/08/racks.png?w=300 300w, https://compositecode.files.wordpress.com/2018/08/racks.png?w=768 768w, https://compositecode.files.wordpress.com/2018/08/racks.png?w=1024 1024w\" /></a></p>\n<p>A rack is something that is located in a data-center, or even just someone’s garage in some odd scenarios. Ya know, if somebody wants serious hardware to work with. The rack then has a number of servers, often various kinds, within that rack itself. As you can see from the images above there’s a wide range of these racks.</p>\n<p><strong>Datacenter</strong>: Again the easiest way to describe a datacenter is to just look at a bunch of pictures of datacenter, albeit you see lots of racks again. But really, that’s what a datacenter is, is a building that has lots and lots of racks.</p>\n<p><a href=\"https://www.google.com/search?q=data+center&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=0ahUKEwj0wYHomqreAhXNGTQIHdEDBXMQ_AUIDygC&amp;biw=1280&amp;bih=1090\" target=\"_blank\" rel=\"noopener\"><img data-attachment-id=\"14485\" data-permalink=\"https://compositecode.blog/2018/10/29/cassandra-datacenter-racks/data-center/\" data-orig-file=\"https://compositecode.files.wordpress.com/2018/08/data-center.png?w=625\" data-orig-size=\"1279,765\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"data-center\" data-image-description=\"\" data-medium-file=\"https://compositecode.files.wordpress.com/2018/08/data-center.png?w=625?w=300\" data-large-file=\"https://compositecode.files.wordpress.com/2018/08/data-center.png?w=625?w=625\" class=\"alignnone size-full wp-image-14485\" src=\"https://compositecode.files.wordpress.com/2018/08/data-center.png?w=625\" alt=\"data-center.png\" srcset=\"https://compositecode.files.wordpress.com/2018/08/data-center.png?w=625 625w, https://compositecode.files.wordpress.com/2018/08/data-center.png?w=1250 1250w, https://compositecode.files.wordpress.com/2018/08/data-center.png?w=150 150w, https://compositecode.files.wordpress.com/2018/08/data-center.png?w=300 300w, https://compositecode.files.wordpress.com/2018/08/data-center.png?w=768 768w, https://compositecode.files.wordpress.com/2018/08/data-center.png?w=1024 1024w\" /></a></p>\n<p>However in Apache Cassandra (and respectively DataStax Enterprise products) a datacenter and rack do not directly correlate to a physical rack or datacenter. The idea is more of an abstraction than hard mapping to the physical realm. In turn it is better to think of datacenter and racks as a way to structure and organize your DataStax Enterprise or Apache Cassandra architecture. From a tree perspective of organizing your cluster, think of things in this hierarchy.</p>\n<ul><li>Cluster\n<ul><li>Datacenter(s)\n<ul><li>Rack(s)\n<ul><li>Server(s)\n<ul><li>Node (vnode)</li>\n</ul></li>\n</ul></li>\n</ul></li>\n</ul></li>\n</ul><h2>Apache Cassandra Datacenter</h2>\n<p>An Apache Cassandra Datacenter is a group of nodes, related and configured within a cluster for replication purposes. Setting up a specific set of related nodes into a datacenter helps to reduce latency, prevent transactions from impact by other workloads, and related effects. The replication factor can also be setup to write to multiple datacenter, providing additional flexibility in architectural design and organization. One specific element of datacenter to note is that they must contain only one node type:</p>\n<ul><li>Transactional: A standard Cassandra node.</li>\n<li><a href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/graph/graphTOC.html\" target=\"_blank\" rel=\"noopener\">DataStax Enterprise Graph</a>: The Graph database offering from Datastax.</li>\n<li><a href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/analytics/analyticsOverview.html\" target=\"_blank\" rel=\"noopener\">DataStax Enterprise Analytics</a>: An integration with <a href=\"https://spark.apache.org/\" target=\"_blank\" rel=\"noopener\">Apache Spark</a>.</li>\n<li><a href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/search/searchTOC.html\" target=\"_blank\" rel=\"noopener\">DataStax Enterprise Search</a>: Integration with <a href=\"http://lucene.apache.org/solr/\" target=\"_blank\" rel=\"noopener\">Apache Solr</a>.</li>\n<li><a href=\"https://docs.datastax.com/en/dse/6.0/dse-dev/datastax_enterprise/analytics/dseSearchAnalyticsOverview.html\" target=\"_blank\" rel=\"noopener\">DataStax Enterprise Search Analytics</a>: Search queries within analytics jobs.</li>\n</ul><p>Depending on the replication factor, data can be written to multiple datacenters. Datacenters must never span physical locations.Each datacenter usually contains only one node type. The node types are:</p>\n<ul class=\"ul\"><li class=\"li\">Transactional: Previously referred to as a Cassandra node.</li>\n<li class=\"li\"><a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/graph/graphTOC.html\" target=\"_blank\" rel=\"noopener\">DSE Graph</a>: A graph database for managing, analyzing, and searching highly-connected data.</li>\n<li class=\"li\"><a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/analytics/analyticsOverview.html\" target=\"_blank\" rel=\"noopener\">DSE Analytics</a>: Integration with Apache Spark.</li>\n<li class=\"li\"><a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/search/searchAbout.html\" target=\"_blank\" rel=\"noopener\">DSE Search</a>: Integration with Apache Solr. Previously referred to as a Solr node.</li>\n<li class=\"li\"><a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/analytics/dseSearchAnalyticsOverview.html\" target=\"_blank\" rel=\"noopener\">DSE SearchAnalytics</a>: DSE Search queries within DSE Analytics jobs.</li>\n</ul><h2>Apache Cassandra Racks</h2>\n<p>An Apache Cassandra Rack is a grouped set of servers. The architecture of Cassandra uses racks so that no replica is stored redundantly inside a singular rack, ensuring that replicas are spread around through different racks in case one rack goes down. Within a datacenter there could be multiple racks with multiple servers, as the hierarchy shown above would dictate.</p>\n<p>To determine where data goes within a rack or sets of racks Apache Cassandra uses what is referred to as a snitch. A snitch determines which racks and datacenter a particular node belongs to, and by respect of that, determines where the replicas of data will end up. This replication strategy which is informed by the snitch can take the form of numerous kinds of snitches, some examples include;</p>\n<ul><li>SimpleSnitch – this snitch treats order as proximity. This is primarily only used when in a single-datacenter deployment.</li>\n<li>Dynamic Snitching – the dynamic snitch monitors read latencies to avoid reading from hosts that have slowed down.</li>\n<li>RackInferringSnitch – Proximity is determined by rack and datacenter, assumed corresponding to 3rd and 2nd octet of each node’s IP address. This particular snitch is often used as an example for writing a custom snitch class since it isn’t particularly useful unless it happens to match one’s deployment conventions.</li>\n</ul><p>In the future I’ll outline a few more snitches, how some of them work with more specific detail, and I’ll get into a whole selection of other topics. Be sure to subscribe to the blog, the ole’ RSS feed works great too, and follow <a href=\"https://twitter.com/CompositeCode\" target=\"_blank\" rel=\"noopener\">@CompositeCode</a> for blog updates. For discourse and hot takes follow me <a href=\"https://twitter.com/Adron\" target=\"_blank\" rel=\"noopener\">@Adron</a>.</p>\n<blockquote>\n<h2>Distributed Database Things to Know Series</h2>\n<ol><li><a href=\"https://compositecode.blog/2018/10/08/distributed-database-things-to-know-consistent-hashing/\" target=\"_blank\" rel=\"noopener\">Consistent Hashing</a></li>\n<li>Apache Cassandra Datacenter &amp; Racks (this post)</li>\n</ol></blockquote>\n\n\t\t<div id=\"geo-post-14483\" class=\"geo geo-post\">\n\t\t\t47.666712\n\t\t\t-122.383132\n\t\t</div>\t\t\t</div><div class=\"entry-content\">\n\t\t\t<p>Project Repo: <a href=\"https://github.com/Adron/InteroperabilityBlackBox\" target=\"_blank\" rel=\"noopener\">Interoperability Black Box</a></p>\n<p>First steps. Let’s get .NET installed and setup. I’m running Ubuntu 18.04 for this setup and start of project. To install .NET on Ubuntu one needs to go through a multi-command process of keys and some other stuff, fortunately Microsoft’s teams have made this almost easy by providing the commands for the various Linux distributions here. The commands I ran are as follows to get all this initial setup done.</p>\n<pre class=\"brush: bash; title: ; notranslate\" title=\"\">\nwget -qO- https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor &gt; microsoft.asc.gpg\nsudo mv microsoft.asc.gpg /etc/apt/trusted.gpg.d/\nwget -q https://packages.microsoft.com/config/ubuntu/18.04/prod.list\nsudo mv prod.list /etc/apt/sources.list.d/microsoft-prod.list\nsudo chown root:root /etc/apt/trusted.gpg.d/microsoft.asc.gpg\nsudo chown root:root /etc/apt/sources.list.d/microsoft-prod.list\n</pre>\n<p>After all this I could then install the .NET SDK. It’s been so long since I actually installed .NET on anything that I wasn’t sure if I just needed the runtime, the SDK, or what I’d actually need. I just assumed it would be safe to install the SDK and then install the runtime too.</p>\n<pre class=\"brush: bash; title: ; notranslate\" title=\"\">\nsudo apt-get install apt-transport-https\nsudo apt-get update\nsudo apt-get install dotnet-sdk-2.1\n</pre>\n<p>Then the runtime.</p>\n<pre class=\"brush: bash; title: ; notranslate\" title=\"\">\nsudo apt-get install aspnetcore-runtime-2.1\n</pre>\n<p><img data-attachment-id=\"14246\" data-permalink=\"https://compositecode.blog/2018/08/05/dse6-net-v/logo/\" data-orig-file=\"https://compositecode.files.wordpress.com/2018/06/logo.png?w=102&amp;h=102\" data-orig-size=\"2000,2000\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"logo\" data-image-description=\"\" data-medium-file=\"https://compositecode.files.wordpress.com/2018/06/logo.png?w=102&amp;h=102?w=300\" data-large-file=\"https://compositecode.files.wordpress.com/2018/06/logo.png?w=102&amp;h=102?w=625\" class=\"wp-image-14246 alignright\" src=\"https://compositecode.files.wordpress.com/2018/06/logo.png?w=102&amp;h=102\" alt=\"logo\" width=\"102\" height=\"102\" srcset=\"https://compositecode.files.wordpress.com/2018/06/logo.png?w=102&amp;h=102 102w, https://compositecode.files.wordpress.com/2018/06/logo.png?w=204&amp;h=204 204w, https://compositecode.files.wordpress.com/2018/06/logo.png?w=150&amp;h=150 150w\" />Alright. Now with this installed, I wanted to also see if <a href=\"http://www.jetbrains.com/rider/\" target=\"_blank\" rel=\"noopener\">Jetbrains Rider</a> would detect – or at least what would I have to do – to have the IDE detect that .NET is now installed. So I opened up the IDE to see what the results would be. Over the left hand side of the new solution dialog, if anything isn’t installed Rider usually will display a message that X whatever needs installed. But it looked like everything is showing up as installed, “<em>yay for things working (at this point)!</em>”</p>\n<p><img data-attachment-id=\"14248\" data-permalink=\"https://compositecode.blog/2018/08/05/dse6-net-v/rider-01/\" data-orig-file=\"https://compositecode.files.wordpress.com/2018/06/rider-01.png?w=625\" data-orig-size=\"799,536\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"rider-01\" data-image-description=\"\" data-medium-file=\"https://compositecode.files.wordpress.com/2018/06/rider-01.png?w=625?w=300\" data-large-file=\"https://compositecode.files.wordpress.com/2018/06/rider-01.png?w=625?w=625\" class=\"alignnone size-full wp-image-14248\" src=\"https://compositecode.files.wordpress.com/2018/06/rider-01.png?w=625\" alt=\"rider-01\" srcset=\"https://compositecode.files.wordpress.com/2018/06/rider-01.png?w=625 625w, https://compositecode.files.wordpress.com/2018/06/rider-01.png?w=150 150w, https://compositecode.files.wordpress.com/2018/06/rider-01.png?w=300 300w, https://compositecode.files.wordpress.com/2018/06/rider-01.png?w=768 768w, https://compositecode.files.wordpress.com/2018/06/rider-01.png 799w\" /></p>\n<p>Next up is to get a solution started with the pertinent projects for what I want to build.</p>\n<p><img data-attachment-id=\"14249\" data-permalink=\"https://compositecode.blog/2018/08/05/dse6-net-v/dse2-2/\" data-orig-file=\"https://compositecode.files.wordpress.com/2018/06/dse2.png?w=625\" data-orig-size=\"800,139\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"dse2\" data-image-description=\"\" data-medium-file=\"https://compositecode.files.wordpress.com/2018/06/dse2.png?w=625?w=300\" data-large-file=\"https://compositecode.files.wordpress.com/2018/06/dse2.png?w=625?w=625\" class=\"alignnone size-full wp-image-14249\" src=\"https://compositecode.files.wordpress.com/2018/06/dse2.png?w=625\" alt=\"dse2\" srcset=\"https://compositecode.files.wordpress.com/2018/06/dse2.png?w=625 625w, https://compositecode.files.wordpress.com/2018/06/dse2.png?w=150 150w, https://compositecode.files.wordpress.com/2018/06/dse2.png?w=300 300w, https://compositecode.files.wordpress.com/2018/06/dse2.png?w=768 768w, https://compositecode.files.wordpress.com/2018/06/dse2.png 800w\" /></p>\n<p><img data-attachment-id=\"14250\" data-permalink=\"https://compositecode.blog/2018/08/05/dse6-net-v/kazam_screenshot_00001/\" data-orig-file=\"https://compositecode.files.wordpress.com/2018/06/kazam_screenshot_00001.png?w=625\" data-orig-size=\"1264,364\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"Kazam_screenshot_00001\" data-image-description=\"\" data-medium-file=\"https://compositecode.files.wordpress.com/2018/06/kazam_screenshot_00001.png?w=625?w=300\" data-large-file=\"https://compositecode.files.wordpress.com/2018/06/kazam_screenshot_00001.png?w=625?w=625\" class=\"alignnone size-full wp-image-14250\" src=\"https://compositecode.files.wordpress.com/2018/06/kazam_screenshot_00001.png?w=625\" alt=\"Kazam_screenshot_00001\" srcset=\"https://compositecode.files.wordpress.com/2018/06/kazam_screenshot_00001.png?w=625 625w, https://compositecode.files.wordpress.com/2018/06/kazam_screenshot_00001.png?w=1250 1250w, https://compositecode.files.wordpress.com/2018/06/kazam_screenshot_00001.png?w=150 150w, https://compositecode.files.wordpress.com/2018/06/kazam_screenshot_00001.png?w=300 300w, https://compositecode.files.wordpress.com/2018/06/kazam_screenshot_00001.png?w=768 768w, https://compositecode.files.wordpress.com/2018/06/kazam_screenshot_00001.png?w=1024 1024w\" /></p>\n<p>For the next stage I created three projects.</p>\n<ol><li>InteroperationalBlackBox – A basic class library that will be used by a console application or whatever other application or service that may need access to the specific business logic or what not.</li>\n<li>InteroperationalBlackBox.Tests – An <a href=\"https://xunit.github.io/\" target=\"_blank\" rel=\"noopener\">xunit</a> testing project for testing anything that might need some good ole’ testing.</li>\n<li>InteroperationalBlackBox.Cli – A console application (CLI) that I’ll use to interact with the class library and add capabilities going forward.</li>\n</ol><p>Alright, now that all the basic projects are setup in the solution, I’ll go out and see about the <a href=\"https://docs.datastax.com/en/developer/csharp-driver-dse/2.3/\" target=\"_blank\" rel=\"noopener\">.NET DataStax Enterprise driver</a>. Inside Jetbrains Rider I can right click on a particular project that I want to add or manage dependencies for. I did that and then put “<em>dse</em>” in the search box. The dialog pops up from the bottom of the IDE and you can add it by clicking on the bottom right plus sign in the description box to the right. Once you click the plus sign, once installed, it becomes a little red x.</p>\n<p><img data-attachment-id=\"14252\" data-permalink=\"https://compositecode.blog/2018/08/05/dse6-net-v/dse-adding-package/\" data-orig-file=\"https://compositecode.files.wordpress.com/2018/06/dse-adding-package.png?w=625\" data-orig-size=\"1398,594\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"dse-adding-package\" data-image-description=\"\" data-medium-file=\"https://compositecode.files.wordpress.com/2018/06/dse-adding-package.png?w=625?w=300\" data-large-file=\"https://compositecode.files.wordpress.com/2018/06/dse-adding-package.png?w=625?w=625\" class=\"alignnone size-full wp-image-14252\" src=\"https://compositecode.files.wordpress.com/2018/06/dse-adding-package.png?w=625\" alt=\"dse-adding-package\" srcset=\"https://compositecode.files.wordpress.com/2018/06/dse-adding-package.png?w=625 625w, https://compositecode.files.wordpress.com/2018/06/dse-adding-package.png?w=1250 1250w, https://compositecode.files.wordpress.com/2018/06/dse-adding-package.png?w=150 150w, https://compositecode.files.wordpress.com/2018/06/dse-adding-package.png?w=300 300w, https://compositecode.files.wordpress.com/2018/06/dse-adding-package.png?w=768 768w, https://compositecode.files.wordpress.com/2018/06/dse-adding-package.png?w=1024 1024w\" /></p>\n<p>Alright. Now it’s <em>almost</em> time to get some code working. We need ourselves a database first however. I’m going to setup a cluster in <a href=\"https://cloud.google.com/\" target=\"_blank\" rel=\"noopener\">Google Cloud Platform</a> (GCP), but feel free to use whatever cluster you’ve got. These instructions will basically be reusable across wherever you’ve got your cluster setup. I wrote up a walk through and instructions for the GCP Marketplace a few weeks ago. I used the same offering to get this example cluster up and running to use. So, now back to getting the first snippets of code working.</p>\n<p>Let’s write a test first.</p>\n<pre class=\"brush: csharp; title: ; notranslate\" title=\"\">\n[Fact]\npublic void ConfirmDatabase_Connects_False()\n{\n    var box = new BlackBox();\n    Assert.Equal(false, box.ConfirmConnection());\n}\n</pre>\n<p>In this test, I named the class called BlackBox and am planning to have a parameterless constructor. But as things go tests are very fluid, or ought to be, and I may change it in the next iteration. I’m thinking, at least to get started, that I’ll have a method to test and confirm a connection for the CLI. I’ve named it ConfirmConnection for that purpose. Initially I’m going to test for false, but that’s primarily just to get started. Now, time to implement.</p>\n<pre class=\"brush: csharp; title: ; notranslate\" title=\"\">\nnamespace InteroperabilityBlackBox\nusing System;\nusing Dse;\nusing Dse.Auth;\n\nnamespace InteroperabilityBlackBox\n{\n    public class BlackBox\n    {\n        public BlackBox()\n        {}\n\n        public bool ConfirmConnection()\n        {\n            return false;\n        }\n    }\n}\n</pre>\n<p>That gives a passing test and I move forward. For more of the run through of moving from this first step to the finished code session check out this</p>\n<p><iframe class=\"youtube-player\" width=\"625\" height=\"352\" src=\"https://www.youtube.com/embed/2a6_oDV5Dqs?version=3&amp;rel=1&amp;fs=1&amp;autohide=2&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;wmode=transparent\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe></p>\n<p>By the end of the coding session I had a few tests.</p>\n<pre class=\"brush: csharp; title: ; notranslate\" title=\"\">\nusing Xunit;\n\nnamespace InteroperabilityBlackBox.Tests\n{\n    public class MakingSureItWorksIntegrationTests\n    {\n        [Fact]\n        public void ConfirmDatabase_Connects_False()\n        {\n            var box = new BlackBox();\n            Assert.Equal(false, box.ConfirmConnection());\n        }\n\n        [Fact]\n        public void ConfirmDatabase_PassedValuesConnects_True()\n        {\n            var box = new BlackBox(\"cassandra\", \"\", \"\");\n            Assert.Equal(false, box.ConfirmConnection());\n        }\n\n        [Fact]\n        public void ConfirmDatabase_PassedValuesConnects_False()\n        {\n            var box = new BlackBox(\"cassandra\", \"notThePassword\", \"\");\n            Assert.Equal(false, box.ConfirmConnection());\n        }\n    }\n}\n</pre>\n<p>The respective code for connecting to the database cluster, per the walk through I wrote about here, at session end looked like this.</p>\n<pre class=\"brush: csharp; title: ; notranslate\" title=\"\">\nusing System;\nusing Dse;\nusing Dse.Auth;\n\nnamespace InteroperabilityBlackBox\n{\n    public class BlackBox : IBoxConnection\n    {\n        public BlackBox(string username, string password, string contactPoint)\n        {\n            UserName = username;\n            Password = password;\n            ContactPoint = contactPoint;\n        }\n\n        public BlackBox()\n        {\n            UserName = \"ConfigValueFromSecretsVault\";\n            Password = \"ConfigValueFromSecretsVault\";\n            ContactPoint = \"ConfigValue\";\n        }\n\n        public string ContactPoint { get; set; }\n        public string UserName { get; set; }\n        public string Password { get; set; }\n\n        public bool ConfirmConnection()\n        {\n            IDseCluster cluster = DseCluster.Builder()\n                .AddContactPoint(ContactPoint)\n                .WithAuthProvider(new DsePlainTextAuthProvider(UserName, Password))\n                .Build();\n\n            try\n            {\n                cluster.Connect();\n                return true;\n            }\n            catch (Exception e)\n            {\n                Console.WriteLine(e);\n                return false;\n            }\n\n        }\n    }\n}\n</pre>\n<p>With my interface providing the contract to meet.</p>\n<pre class=\"brush: csharp; title: ; notranslate\" title=\"\">\nnamespace InteroperabilityBlackBox\n{\n    public interface IBoxConnection\n    {\n        string ContactPoint { get; set; }\n        string UserName { get; set; }\n        string Password { get; set; }\n        bool ConfirmConnection();\n    }\n}\n</pre>\n<h2>Conclusions &amp; Next Steps</h2>\n<p>After I wrapped up the session two things stood out that needed fixed for the next session. I’ll be sure to add these as objectives for the next coding session at 3pm PST on Thursday.</p>\n<ol><li>The tests really needed to more resiliently confirm the integrations that I was working to prove out. My plan at this point is to add some Docker images that would provide the development integration tests a point to work against. This would alleviate the need for something outside of the actual project in the repository to exist. Removing that fragility.</li>\n<li>The application, in its “Black Box”, should do something. For the next session we’ll write up some feature requests we’d want, or maybe someone has some suggestions of functionality they’d like to see implemented in a CLI using .NET Core working against a DataStax Enterprise Cassandra Database Cluster? Feel free to leave a comment or three about a feature, I’ll work on adding it during the next session.</li>\n</ol><ul><li>Project Repo: <a href=\"https://github.com/Adron/InteroperabilityBlackBox\" target=\"_blank\" rel=\"noopener\">https://github.com/Adron/InteroperabilityBlackBox</a></li>\n<li>File an Feature Request: <a href=\"https://github.com/Adron/InteroperabilityBlackBox/issues/new?template=feature_request.md\" target=\"_blank\" rel=\"noopener\">https://github.com/Adron/InteroperabilityBlackBox/issues/new?template=feature_request.md</a></li>\n</ul>\t\t\t\t\t</div><div class=\"entry-content\">\n\t\t\t<p>SITREP = Situation Report. It’s military speak. 💂🏻‍♂️</p>\n<p><a href=\"http://cassandra.apache.org/\" target=\"_blank\" rel=\"noopener\">Apache Cassandra</a> is one of the most popular databases in use today. It has many <em>characteristics</em> and distinctive <em>architectural</em> details. In this post I’ll provide a description and some details for a number of these <em>features</em> and <em>characteristics</em>, divided as such. Then, after that (<em>i.e. toward the end, so skip there if you just want to the differences</em>) I’m doing to summarize key differences with the latest release of the <a href=\"https://www.datastax.com/products/datastax-enterprise\" target=\"_blank\" rel=\"noopener\">DataStax Enterprise 6</a> version of the database.</p>\n<h2>Cassandra Characteristics</h2>\n<p>Cassandra is a linearly scalable, highly available, fault tolerant, distributed database. That is, just to name a few of the most important characteristics. The Cassandra database is also cross-platform (runs on any operating systems), multi-cloud (runs on and across multiple clouds), and can survive regional data center outages or even in multi-cloud scenarios entire cloud provider outages!</p>\n<p>Columnar Store, Column Based, or Column Family? What? Ok, so you might have read a number of things about what Cassandra actually is. Let’s break this down. First off, a columnar or column store or column oriented database guarantees data location for a single column in a node on disk. The column may span a bunch of or all of the rows that depend on where or how you specify partitions. However, this isn’t what the Cassandra Database uses. Cassandra is a column-family database.</p>\n<p>A column-family storage architecture makes sure the data is stored based on locality of the data at the partition level, not the column level. Cassandra partitions group rows and columns split by a partition key, then clustered together by a specified clustering column or columns. To query Cassandra, because of this, you must know the partition key in order to avoid full data scans!</p>\n<p>Cassandra has these partitions that guarantee to be on the same node and sort strings table (referred to most commonly as an SSTable *) in the same location within that file. Even though, depending on the compaction strategy, this can change things and the partition can be split across multiple files on a disk. So really, data locality isn’t guaranteed.</p>\n<p>Column-family stores are great for high throughput writes and the ability to linearly scale horizontally (<em>ya know, getting lots and lots of nodes in the cloud!</em>). Reads using the partition key are extremely fast since this key points to exactly where the data resides. However, this often – at least last I know of – leads to a full scan of the data for any type of ad-hoc query.</p>\n<p>A sort of historically trivial but important point is the column-family term comes from the storage engine originally used based on a key value store. The value was a set of column value tuples, which where often referenced as <em>family</em>, and later this <em>family</em> was abstracted into <em>partitions</em>, and then the storage engine was matched to that abstraction. Whew, ok, so that’s a lot of knowledge being coagulated into a solid eh!  [scuse’ my odd artful language use if you visualized that!]</p>\n<p>With all of this described, a that little history sprinkled in, when reading the description of Cassandra in the <a href=\"https://github.com/apache/cassandra\" target=\"_blank\" rel=\"noopener\">README.asc</a> file of the actual <a href=\"https://github.com/apache/cassandra\" target=\"_blank\" rel=\"noopener\">Cassandra Github Repo</a> things make just a little more sense. In the file it starts off with a description,</p>\n<blockquote><p>Apache Cassandra is a highly-scalable partitioned row store. Rows are organized into tables with a required primary key.</p>\n<p>Partitioning means that Cassandra can distribute your data across multiple machines in an application-transparent matter. Cassandra will automatically repartition as machines are added and removed from the cluster.</p>\n<p>Row store means that like relational databases, Cassandra organizes data by rows and columns. The Cassandra Query Language (CQL) is a close relative of SQL.</p></blockquote>\n<p>Now that I’ve covered the 101 level of what Cassandra is I’ll give a look at DataStax and their respective offering.</p>\n<h2>DataStax</h2>\n<p>DataStax Enterprise at first glance might be a bit confusing since immediate questions pop up like, “Doesn’t DataStax make Cassandra?”, “Isn’t DataStax just selling support for Cassandra?”, or “Eh, wha, who is DataStax and what does this have to do with Cassandra?”. Well, I’m gonna tell ya all about where we are today regarding all of these things fit.</p>\n<h3>Performance</h3>\n<p>DataStax provides a whole selection of amenities around a database, which is derived from the Cassandra Distributed Database System. The core product and these amenities are built into what we refer to as the “<em><a href=\"https://www.datastax.com/products/datastax-enterprise-6\" target=\"_blank\" rel=\"noopener\">DataStax Enterprise 6</a></em>“. Some of specific differences are that the database engine itself has been modified out of band and now delivers 2x the performance of the standard Cassandra implemented database engine. I was somewhat dubious when I joined but after the third party benchmarks where completed that showed the difference I grew more confident. My confidence in this speed increase grew as I’ve gotten to work with the latest version I can tell in more than a few situations that it’s faster.</p>\n<h3>Read Repair &amp; NodeSync</h3>\n<p>If you already use Cassandra, read repair works a certain way and that still works just fine in DataStax Enterprise 6. But one also has the option of using NodeSync which can help eliminate scripting, manual intervention, and other repair operations.</p>\n<h3>Spark SQL Connectivity</h3>\n<p>There’s also an always on SQL Engine for automated uptime for apps using DataStax Enterprise Analytics. This provides a better level of analytics requests and end -user analytics. Sort of on this related note, DataStax Studio also has notebook support for Spark SQL now. Writing one’s Spark SQL gets a little easier with this option.</p>\n<h3>Multi-Cloud / Hybrid-Cloud</h3>\n<p>Another huge advantage of DataStax Enterprise is going multi-cloud or hybrid-cloud with DataStax Enterprise Cassandra. Between the Lifecycle Manager (LCM), OpsCenter, and related tooling getting up and running with a cluster across a varying range of data-centers wherever they may be is quick and easy.</p>\n<h2>Summary</h2>\n<p>I’ll be providing deeper dives into the particular technology, the specific differences, and more in the future. For now I’ll wrap up this post as I’ve got a few others coming distinctively related to distributed database systems themselves ranging from specific principles (like CAP Theorem) to operational (how to and best ways to manage) and development (patterns and practices of developing against) related topics.</p>\n<p>Overall the solutions that DataStax offers are solid advantages if you’re stepping into any large scale data (big data or whatever one would call their plethora of data) needs. Over the coming months I’ve got a lot of material – from architectural research and guidance to tactical coding implementation work – that I’ll be blogging about and providing. I’m really looking forward to exploring these capabilities, being the developer advocate to DataStax for the community of users, and learning a thing or three million.</p>\n\t\t<div id=\"geo-post-14307\" class=\"geo geo-post\">\n\t\t\t47.671392\n\t\t\t-122.376081\n\t\t</div>\t\t\t\t\t</div>",
        "created_at": "2019-01-23T21:47:04+0000",
        "updated_at": "2019-01-23T21:47:15+0000",
        "published_at": null,
        "published_by": [
          "Adron"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 15,
        "domain_name": "compositecode.blog",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12976"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          },
          {
            "id": 1347,
            "label": "architecture",
            "slug": "architecture"
          }
        ],
        "is_public": false,
        "id": 12974,
        "uid": null,
        "title": "Dell Reference Configuration for DataStax Enterprise powered by Apache Cassandra - PDF",
        "url": "https://docplayer.net/8944426-Dell-reference-configuration-for-datastax-enterprise-powered-by-apache-cassandra.html",
        "content": "<p><a href=\"#\" class=\"btn purple\">\n                                        Similar documents\n                                        <i class=\"fa fa-puzzle-piece top-news-icon\">\n                                    </i></a></p><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/2777703-Dell-reference-configuration-for-hortonworks-data-platform.html\">Dell Reference Configuration for Hortonworks Data Platform</a>\n    </h3><p>\n        <img alt=\"Dell Reference Configuration for Hortonworks Data Platform\" title=\"Dell Reference Configuration for Hortonworks Data Platform\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/24/2777703.jpg\" />\n        Dell Reference Configuration for Hortonworks Data Platform A Quick Reference Configuration Guide Armando Acosta Hadoop Product Manager Dell Revolutionary Cloud and Big Data Group Kris Applegate Solution    </p><a href=\"https://docplayer.net/2777703-Dell-reference-configuration-for-hortonworks-data-platform.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/18739515-Dell-compellent-storage-center-san-vmware-view-1-000-desktop-reference-architecture-dell-compellent-product-specialist-team.html\">Dell Compellent Storage Center SAN &amp; VMware View 1,000 Desktop Reference Architecture. Dell Compellent Product Specialist Team</a>\n    </h3><p>\n        <img alt=\"Dell Compellent Storage Center SAN &amp; VMware View 1,000 Desktop Reference Architecture. Dell Compellent Product Specialist Team\" title=\"Dell Compellent Storage Center SAN &amp; VMware View 1,000 Desktop Reference Architecture. Dell Compellent Product Specialist Team\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/39/18739515.jpg\" />\n        Dell Compellent Storage Center SAN &amp; VMware View 1,000 Desktop Reference Architecture Dell Compellent Product Specialist Team THIS WHITE PAPER IS FOR INFORMATIONAL PURPOSES ONLY, AND MAY CONTAIN TYPOGRAPHICAL    </p><a href=\"https://docplayer.net/18739515-Dell-compellent-storage-center-san-vmware-view-1-000-desktop-reference-architecture-dell-compellent-product-specialist-team.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/17772005-Dell-cloudera-syncsort-data-warehouse-optimization-etl-offload.html\">Dell Cloudera Syncsort Data Warehouse Optimization ETL Offload</a>\n    </h3><p>\n        <img alt=\"Dell Cloudera Syncsort Data Warehouse Optimization ETL Offload\" title=\"Dell Cloudera Syncsort Data Warehouse Optimization ETL Offload\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/37/17772005.jpg\" />\n        Dell Cloudera Syncsort Data Warehouse Optimization ETL Offload Drive operational efficiency and lower data transformation costs with a Reference Architecture for an end-to-end optimization and offload    </p><a href=\"https://docplayer.net/17772005-Dell-cloudera-syncsort-data-warehouse-optimization-etl-offload.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/3728210-Elasticsearch-on-cisco-unified-computing-system-optimizing-your-ucs-infrastructure-for-elasticsearch-s-analytics-software-stack.html\">Elasticsearch on Cisco Unified Computing System: Optimizing your UCS infrastructure for Elasticsearch s analytics software stack</a>\n    </h3><p>\n        <img alt=\"Elasticsearch on Cisco Unified Computing System: Optimizing your UCS infrastructure for Elasticsearch s analytics software stack\" title=\"Elasticsearch on Cisco Unified Computing System: Optimizing your UCS infrastructure for Elasticsearch s analytics software stack\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/24/3728210.jpg\" />\n        Elasticsearch on Cisco Unified Computing System: Optimizing your UCS infrastructure for Elasticsearch s analytics software stack HIGHLIGHTS Real-Time Results Elasticsearch on Cisco UCS enables a deeper    </p><a href=\"https://docplayer.net/3728210-Elasticsearch-on-cisco-unified-computing-system-optimizing-your-ucs-infrastructure-for-elasticsearch-s-analytics-software-stack.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/13344386-Dell-s-oracle-database-advisor.html\">DELL s Oracle Database Advisor</a>\n    </h3><p>\n        <img alt=\"DELL s Oracle Database Advisor\" title=\"DELL s Oracle Database Advisor\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/29/13344386.jpg\" />\n        DELL s Oracle Database Advisor Underlying Methodology A Dell Technical White Paper Database Solutions Engineering By Roger Lopez Phani MV Dell Product Group January 2010 THIS WHITE PAPER IS FOR INFORMATIONAL    </p><a href=\"https://docplayer.net/13344386-Dell-s-oracle-database-advisor.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/3176305-Interactive-data-analytics-drive-insights.html\">Interactive data analytics drive insights</a>\n    </h3><p>\n        <img alt=\"Interactive data analytics drive insights\" title=\"Interactive data analytics drive insights\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/24/3176305.jpg\" />\n        Big data Interactive data analytics drive insights Daniel Davis/Invodo/S&amp;P. Screen images courtesy of Landmark Software and Services By Armando Acosta and Joey Jablonski The Apache Hadoop Big data has    </p><a href=\"https://docplayer.net/3176305-Interactive-data-analytics-drive-insights.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/7181383-Datastax-enterprise-reference-architecture.html\">DataStax Enterprise Reference Architecture</a>\n    </h3><p>\n        <img alt=\"DataStax Enterprise Reference Architecture\" title=\"DataStax Enterprise Reference Architecture\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/26/7181383.jpg\" />\n        DataStax Enterprise Reference Architecture DataStax Enterprise Reference Architecture 7.8.15 1 Table of Contents ABSTRACT... 3 INTRODUCTION... 3 DATASTAX ENTERPRISE... 3 ARCHITECTURE... 3 OPSCENTER: EASY-    </p><a href=\"https://docplayer.net/7181383-Datastax-enterprise-reference-architecture.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/23510394-Optimizing-dell-poweredge-configurations-for-hadoop.html\">Optimizing Dell PowerEdge Configurations for Hadoop</a>\n    </h3><p>\n        <img alt=\"Optimizing Dell PowerEdge Configurations for Hadoop\" title=\"Optimizing Dell PowerEdge Configurations for Hadoop\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/46/23510394.jpg\" />\n        Optimizing Dell PowerEdge Configurations for Hadoop Understanding how to get the most out of Hadoop running on Dell hardware A Dell technical white paper July 2013 Michael Pittaro Principal Architect,    </p><a href=\"https://docplayer.net/23510394-Optimizing-dell-poweredge-configurations-for-hadoop.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/7314295-Benchmarking-cassandra-on-violin.html\">Benchmarking Cassandra on Violin</a>\n    </h3><p>\n        <img alt=\"Benchmarking Cassandra on Violin\" title=\"Benchmarking Cassandra on Violin\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/26/7314295.jpg\" />\n        Technical White Paper Report Technical Report Benchmarking Cassandra on Violin Accelerating Cassandra Performance and Reducing Read Latency With Violin Memory Flash-based Storage Arrays Version 1.0 Abstract    </p><a href=\"https://docplayer.net/7314295-Benchmarking-cassandra-on-violin.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/13034384-Converged-storage-architecture-for-oracle-rac-based-on-nvme-ssds-and-standard-x86-servers.html\">Converged storage architecture for Oracle RAC based on NVMe SSDs and standard x86 servers</a>\n    </h3><p>\n        <img alt=\"Converged storage architecture for Oracle RAC based on NVMe SSDs and standard x86 servers\" title=\"Converged storage architecture for Oracle RAC based on NVMe SSDs and standard x86 servers\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/28/13034384.jpg\" />\n        Converged storage architecture for Oracle RAC based on NVMe SSDs and standard x86 servers White Paper rev. 2015-11-27 2015 FlashGrid Inc. 1 www.flashgrid.io Abstract Oracle Real Application Clusters (RAC)    </p><a href=\"https://docplayer.net/13034384-Converged-storage-architecture-for-oracle-rac-based-on-nvme-ssds-and-standard-x86-servers.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/10280288-Accelerating-enterprise-applications-and-reducing-tco-with-sandisk-zetascale-software.html\">Accelerating Enterprise Applications and Reducing TCO with SanDisk ZetaScale Software</a>\n    </h3><p>\n        <img alt=\"Accelerating Enterprise Applications and Reducing TCO with SanDisk ZetaScale Software\" title=\"Accelerating Enterprise Applications and Reducing TCO with SanDisk ZetaScale Software\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/27/10280288.jpg\" />\n        WHITEPAPER Accelerating Enterprise Applications and Reducing TCO with SanDisk ZetaScale Software SanDisk ZetaScale software unlocks the full benefits of flash for In-Memory Compute and NoSQL applications    </p><a href=\"https://docplayer.net/10280288-Accelerating-enterprise-applications-and-reducing-tco-with-sandisk-zetascale-software.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/5628355-Dell-in-memory-appliance-for-cloudera-enterprise.html\">Dell In-Memory Appliance for Cloudera Enterprise</a>\n    </h3><p>\n        <img alt=\"Dell In-Memory Appliance for Cloudera Enterprise\" title=\"Dell In-Memory Appliance for Cloudera Enterprise\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/25/5628355.jpg\" />\n        Dell In-Memory Appliance for Cloudera Enterprise Hadoop Overview, Customer Evolution and Dell In-Memory Product Details Author: Armando Acosta Hadoop Product Manager/Subject Matter Expert Armando_Acosta@Dell.com/    </p><a href=\"https://docplayer.net/5628355-Dell-in-memory-appliance-for-cloudera-enterprise.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/10092772-Jvm-performance-study-comparing-oracle-hotspot-and-azul-zing-using-apache-cassandra.html\">JVM Performance Study Comparing Oracle HotSpot and Azul Zing Using Apache Cassandra</a>\n    </h3><p>\n        <img alt=\"JVM Performance Study Comparing Oracle HotSpot and Azul Zing Using Apache Cassandra\" title=\"JVM Performance Study Comparing Oracle HotSpot and Azul Zing Using Apache Cassandra\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/27/10092772.jpg\" />\n        JVM Performance Study Comparing Oracle HotSpot and Azul Zing Using Apache Cassandra January 2014 Legal Notices Apache Cassandra, Spark and Solr and their respective logos are trademarks or registered trademarks    </p><a href=\"https://docplayer.net/10092772-Jvm-performance-study-comparing-oracle-hotspot-and-azul-zing-using-apache-cassandra.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/9385553-Intel-cloud-builder-guide-cloud-design-and-deployment-on-intel-platforms.html\">Intel Cloud Builder Guide: Cloud Design and Deployment on Intel Platforms</a>\n    </h3><p>\n        <img alt=\"Intel Cloud Builder Guide: Cloud Design and Deployment on Intel Platforms\" title=\"Intel Cloud Builder Guide: Cloud Design and Deployment on Intel Platforms\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/26/9385553.jpg\" />\n        EXECUTIVE SUMMARY Intel Cloud Builder Guide Intel Xeon Processor-based Servers Red Hat* Cloud Foundations Intel Cloud Builder Guide: Cloud Design and Deployment on Intel Platforms Red Hat* Cloud Foundations    </p><a href=\"https://docplayer.net/9385553-Intel-cloud-builder-guide-cloud-design-and-deployment-on-intel-platforms.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/18641320-The-methodology-behind-the-dell-sql-server-advisor-tool.html\">The Methodology Behind the Dell SQL Server Advisor Tool</a>\n    </h3><p>\n        <img alt=\"The Methodology Behind the Dell SQL Server Advisor Tool\" title=\"The Methodology Behind the Dell SQL Server Advisor Tool\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/39/18641320.jpg\" />\n        The Methodology Behind the Dell SQL Server Advisor Tool Database Solutions Engineering By Phani MV Dell Product Group October 2009 Executive Summary The Dell SQL Server Advisor is intended to perform capacity    </p><a href=\"https://docplayer.net/18641320-The-methodology-behind-the-dell-sql-server-advisor-tool.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/2777816-Intel-distribution-for-apache-hadoop-on-dell-poweredge-servers.html\">Intel Distribution for Apache Hadoop on Dell PowerEdge Servers</a>\n    </h3><p>\n        <img alt=\"Intel Distribution for Apache Hadoop on Dell PowerEdge Servers\" title=\"Intel Distribution for Apache Hadoop on Dell PowerEdge Servers\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/24/2777816.jpg\" />\n        Intel Distribution for Apache Hadoop on Dell PowerEdge Servers A Dell Technical White Paper Armando Acosta Hadoop Product Manager Dell Revolutionary Cloud and Big Data Group Kris Applegate Solution Architect    </p><a href=\"https://docplayer.net/2777816-Intel-distribution-for-apache-hadoop-on-dell-poweredge-servers.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/4718636-Optimizing-sql-server-storage-performance-with-the-poweredge-r720.html\">Optimizing SQL Server Storage Performance with the PowerEdge R720</a>\n    </h3><p>\n        <img alt=\"Optimizing SQL Server Storage Performance with the PowerEdge R720\" title=\"Optimizing SQL Server Storage Performance with the PowerEdge R720\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/25/4718636.jpg\" />\n        Optimizing SQL Server Storage Performance with the PowerEdge R720 Choosing the best storage solution for optimal database performance Luis Acosta Solutions Performance Analysis Group Joe Noyola Advanced    </p><a href=\"https://docplayer.net/4718636-Optimizing-sql-server-storage-performance-with-the-poweredge-r720.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/12382730-Dell-virtualization-solution-for-microsoft-sql-server-2012-using-poweredge-r820.html\">Dell Virtualization Solution for Microsoft SQL Server 2012 using PowerEdge R820</a>\n    </h3><p>\n        <img alt=\"Dell Virtualization Solution for Microsoft SQL Server 2012 using PowerEdge R820\" title=\"Dell Virtualization Solution for Microsoft SQL Server 2012 using PowerEdge R820\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/27/12382730.jpg\" />\n        Dell Virtualization Solution for Microsoft SQL Server 2012 using PowerEdge R820 This white paper discusses the SQL server workload consolidation capabilities of Dell PowerEdge R820 using Virtualization.    </p><a href=\"https://docplayer.net/12382730-Dell-virtualization-solution-for-microsoft-sql-server-2012-using-poweredge-r820.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/5880848-Nosql-performance-test-in-memory-performance-comparison-of-sequoiadb-cassandra-and-mongodb.html\">NoSQL Performance Test In-Memory Performance Comparison of SequoiaDB, Cassandra, and MongoDB</a>\n    </h3><p>\n        <img alt=\"NoSQL Performance Test In-Memory Performance Comparison of SequoiaDB, Cassandra, and MongoDB\" title=\"NoSQL Performance Test In-Memory Performance Comparison of SequoiaDB, Cassandra, and MongoDB\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/25/5880848.jpg\" />\n        bankmark UG (haftungsbeschränkt) Bahnhofstraße 1 9432 Passau Germany www.bankmark.de info@bankmark.de T +49 851 25 49 49 F +49 851 25 49 499 NoSQL Performance Test In-Memory Performance Comparison of SequoiaDB,    </p><a href=\"https://docplayer.net/5880848-Nosql-performance-test-in-memory-performance-comparison-of-sequoiadb-cassandra-and-mongodb.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/21043253-Best-practices-for-deploying-ssds-in-a-microsoft-sql-server-2008-oltp-environment-with-dell-equallogic-ps-series-arrays.html\">Best Practices for Deploying SSDs in a Microsoft SQL Server 2008 OLTP Environment with Dell EqualLogic PS-Series Arrays</a>\n    </h3><p>\n        <img alt=\"Best Practices for Deploying SSDs in a Microsoft SQL Server 2008 OLTP Environment with Dell EqualLogic PS-Series Arrays\" title=\"Best Practices for Deploying SSDs in a Microsoft SQL Server 2008 OLTP Environment with Dell EqualLogic PS-Series Arrays\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/40/21043253.jpg\" />\n        Best Practices for Deploying SSDs in a Microsoft SQL Server 2008 OLTP Environment with Dell EqualLogic PS-Series Arrays Database Solutions Engineering By Murali Krishnan.K Dell Product Group October 2009    </p><a href=\"https://docplayer.net/21043253-Best-practices-for-deploying-ssds-in-a-microsoft-sql-server-2008-oltp-environment-with-dell-equallogic-ps-series-arrays.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/6524773-High-performance-sql-server-with-storage-center-6-4-all-flash-array.html\">High Performance SQL Server with Storage Center 6.4 All Flash Array</a>\n    </h3><p>\n        <img alt=\"High Performance SQL Server with Storage Center 6.4 All Flash Array\" title=\"High Performance SQL Server with Storage Center 6.4 All Flash Array\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/25/6524773.jpg\" />\n        High Performance SQL Server with Storage Center 6.4 All Flash Array Dell Storage November 2013 A Dell Compellent Technical White Paper Revisions Date November 2013 Description Initial release THIS WHITE    </p><a href=\"https://docplayer.net/6524773-High-performance-sql-server-with-storage-center-6-4-all-flash-array.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/4718908-Removing-performance-bottlenecks-in-databases-with-red-hat-enterprise-linux-and-violin-memory-flash-storage-arrays-red-hat-performance-engineering.html\">Removing Performance Bottlenecks in Databases with Red Hat Enterprise Linux and Violin Memory Flash Storage Arrays. Red Hat Performance Engineering</a>\n    </h3><p>\n        <img alt=\"Removing Performance Bottlenecks in Databases with Red Hat Enterprise Linux and Violin Memory Flash Storage Arrays. Red Hat Performance Engineering\" title=\"Removing Performance Bottlenecks in Databases with Red Hat Enterprise Linux and Violin Memory Flash Storage Arrays. Red Hat Performance Engineering\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/25/4718908.jpg\" />\n        Removing Performance Bottlenecks in Databases with Red Hat Enterprise Linux and Violin Memory Flash Storage Arrays Red Hat Performance Engineering Version 1.0 August 2013 1801 Varsity Drive Raleigh NC    </p><a href=\"https://docplayer.net/4718908-Removing-performance-bottlenecks-in-databases-with-red-hat-enterprise-linux-and-violin-memory-flash-storage-arrays-red-hat-performance-engineering.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/11893324-Dell-in-memory-appliance-for-cloudera-enterprise.html\">Dell* In-Memory Appliance for Cloudera* Enterprise</a>\n    </h3><p>\n        <img alt=\"Dell* In-Memory Appliance for Cloudera* Enterprise\" title=\"Dell* In-Memory Appliance for Cloudera* Enterprise\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/27/11893324.jpg\" />\n        Built with Intel Dell* In-Memory Appliance for Cloudera* Enterprise Find out what faster big data analytics can do for your business The need for speed in all things related to big data is an enormous    </p><a href=\"https://docplayer.net/11893324-Dell-in-memory-appliance-for-cloudera-enterprise.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/15947470-Dell-desktop-virtualization-solutions-simplified-all-in-one-vdi-appliance-creates-a-new-level-of-simplicity-for-desktop-virtualization.html\">Dell Desktop Virtualization Solutions Simplified. All-in-one VDI appliance creates a new level of simplicity for desktop virtualization</a>\n    </h3><p>\n        <img alt=\"Dell Desktop Virtualization Solutions Simplified. All-in-one VDI appliance creates a new level of simplicity for desktop virtualization\" title=\"Dell Desktop Virtualization Solutions Simplified. All-in-one VDI appliance creates a new level of simplicity for desktop virtualization\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/33/15947470.jpg\" />\n        Dell Desktop Virtualization Solutions Simplified All-in-one VDI appliance creates a new level of simplicity for desktop virtualization Executive summary Desktop virtualization is a proven method for delivering    </p><a href=\"https://docplayer.net/15947470-Dell-desktop-virtualization-solutions-simplified-all-in-one-vdi-appliance-creates-a-new-level-of-simplicity-for-desktop-virtualization.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/206229-High-performance-tier-implementation-guideline.html\">High Performance Tier Implementation Guideline</a>\n    </h3><p>\n        <img alt=\"High Performance Tier Implementation Guideline\" title=\"High Performance Tier Implementation Guideline\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/19/206229.jpg\" />\n        High Performance Tier Implementation Guideline A Dell Technical White Paper PowerVault MD32 and MD32i Storage Arrays THIS WHITE PAPER IS FOR INFORMATIONAL PURPOSES ONLY, AND MAY CONTAIN TYPOGRAPHICAL ERRORS    </p><a href=\"https://docplayer.net/206229-High-performance-tier-implementation-guideline.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/12425214-Amazon-ec2-product-details-page-1-of-5.html\">Amazon EC2 Product Details Page 1 of 5</a>\n    </h3><p>\n        <img alt=\"Amazon EC2 Product Details Page 1 of 5\" title=\"Amazon EC2 Product Details Page 1 of 5\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/27/12425214.jpg\" />\n        Amazon EC2 Product Details Page 1 of 5 Amazon EC2 Functionality Amazon EC2 presents a true virtual computing environment, allowing you to use web service interfaces to launch instances with a variety of    </p><a href=\"https://docplayer.net/12425214-Amazon-ec2-product-details-page-1-of-5.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/2357503-Platfora-big-data-analytics.html\">Platfora Big Data Analytics</a>\n    </h3><p>\n        <img alt=\"Platfora Big Data Analytics\" title=\"Platfora Big Data Analytics\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/24/2357503.jpg\" />\n        Platfora Big Data Analytics ISV Partner Solution Case Study and Cisco Unified Computing System Platfora, the leading enterprise big data analytics platform built natively on Hadoop and Spark, delivers    </p><a href=\"https://docplayer.net/2357503-Platfora-big-data-analytics.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/13460372-Introduction-to-apache-cassandra.html\">Introduction to Apache Cassandra</a>\n    </h3><p>\n        <img alt=\"Introduction to Apache Cassandra\" title=\"Introduction to Apache Cassandra\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/29/13460372.jpg\" />\n        Introduction to Apache Cassandra White Paper BY DATASTAX CORPORATION JULY 2013 1 Table of Contents Abstract 3 Introduction 3 Built by Necessity 3 The Architecture of Cassandra 4 Distributing and Replicating    </p><a href=\"https://docplayer.net/13460372-Introduction-to-apache-cassandra.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/2089080-Well-packaged-sets-of-preinstalled-integrated-and-optimized-software-on-select-hardware-in-the-form-of-engineered-systems-and-appliances.html\">Well packaged sets of preinstalled, integrated, and optimized software on select hardware in the form of engineered systems and appliances</a>\n    </h3><p>\n        <img alt=\"Well packaged sets of preinstalled, integrated, and optimized software on select hardware in the form of engineered systems and appliances\" title=\"Well packaged sets of preinstalled, integrated, and optimized software on select hardware in the form of engineered systems and appliances\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/26/2089080.jpg\" />\n        INSIGHT Oracle's All- Out Assault on the Big Data Market: Offering Hadoop, R, Cubes, and Scalable IMDB in Familiar Packages Carl W. Olofson IDC OPINION Global Headquarters: 5 Speen Street Framingham, MA    </p><a href=\"https://docplayer.net/2089080-Well-packaged-sets-of-preinstalled-integrated-and-optimized-software-on-select-hardware-in-the-form-of-engineered-systems-and-appliances.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/12425328-Datastax-enterprise-powered-by-apache-cassandra-tm.html\">DataStax Enterprise, powered by Apache Cassandra (TM)</a>\n    </h3><p>\n        <img alt=\"DataStax Enterprise, powered by Apache Cassandra (TM)\" title=\"DataStax Enterprise, powered by Apache Cassandra (TM)\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/27/12425328.jpg\" />\n        PerfAccel (TM) Performance Benchmark on Amazon: DataStax Enterprise, powered by Apache Cassandra (TM) Disclaimer: All of the documentation provided in this document, is copyright Datagres Technologies    </p><a href=\"https://docplayer.net/12425328-Datastax-enterprise-powered-by-apache-cassandra-tm.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/12980340-Virtualized-hadoop-a-dell-hadoop-whitepaper-by-joey-jablonski-a-dell-hadoop-whitepaper.html\">Virtualized Hadoop. A Dell Hadoop Whitepaper. By Joey Jablonski. A Dell Hadoop Whitepaper</a>\n    </h3><p>\n        <img alt=\"Virtualized Hadoop. A Dell Hadoop Whitepaper. By Joey Jablonski. A Dell Hadoop Whitepaper\" title=\"Virtualized Hadoop. A Dell Hadoop Whitepaper. By Joey Jablonski. A Dell Hadoop Whitepaper\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/28/12980340.jpg\" />\n        Virtualized Hadoop A Dell Hadoop Whitepaper By Joey Jablonski A Dell Hadoop Whitepaper Introduction to Virtualized Hadoop Hadoop has become a standard within many organizations and data centers for its    </p><a href=\"https://docplayer.net/12980340-Virtualized-hadoop-a-dell-hadoop-whitepaper-by-joey-jablonski-a-dell-hadoop-whitepaper.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/1201380-Open-modern-data-architecture-for-financial-services-risk-management.html\">OPEN MODERN DATA ARCHITECTURE FOR FINANCIAL SERVICES RISK MANAGEMENT</a>\n    </h3><p>\n        <img alt=\"OPEN MODERN DATA ARCHITECTURE FOR FINANCIAL SERVICES RISK MANAGEMENT\" title=\"OPEN MODERN DATA ARCHITECTURE FOR FINANCIAL SERVICES RISK MANAGEMENT\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/21/1201380.jpg\" />\n        WHITEPAPER OPEN MODERN DATA ARCHITECTURE FOR FINANCIAL SERVICES RISK MANAGEMENT A top-tier global bank s end-of-day risk analysis jobs didn t complete in time for the next start of trading day. To solve    </p><a href=\"https://docplayer.net/1201380-Open-modern-data-architecture-for-financial-services-risk-management.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/2357561-Hadooptm-analytics-ddn.html\">HadoopTM Analytics DDN</a>\n    </h3><p>\n        <img alt=\"HadoopTM Analytics DDN\" title=\"HadoopTM Analytics DDN\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/24/2357561.jpg\" />\n        DDN Solution Brief Accelerate&gt; HadoopTM Analytics with the SFA Big Data Platform Organizations that need to extract value from all data can leverage the award winning SFA platform to really accelerate    </p><a href=\"https://docplayer.net/2357561-Hadooptm-analytics-ddn.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/9385181-For-servers-2-2-feature-matrix.html\">FOR SERVERS 2.2: FEATURE matrix</a>\n    </h3><p>\n        <img alt=\"FOR SERVERS 2.2: FEATURE matrix\" title=\"FOR SERVERS 2.2: FEATURE matrix\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/26/9385181.jpg\" />\n        RED hat ENTERPRISE VIRTUALIZATION FOR SERVERS 2.2: FEATURE matrix Red hat enterprise virtualization for servers Server virtualization offers tremendous benefits for enterprise IT organizations server consolidation,    </p><a href=\"https://docplayer.net/9385181-For-servers-2-2-feature-matrix.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/1468007-Self-service-provisioning-and-the-private-cloud.html\">Self-Service Provisioning and the Private Cloud</a>\n    </h3><p>\n        <img alt=\"Self-Service Provisioning and the Private Cloud\" title=\"Self-Service Provisioning and the Private Cloud\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/22/1468007.jpg\" />\n        Self-Service Provisioning and the Private Cloud Using Microsoft Server Virtualization and Dell Compellent Storage Virtualization to Improve Delivery of Infrastructure as a Service Solution Overview Published:    </p><a href=\"https://docplayer.net/1468007-Self-service-provisioning-and-the-private-cloud.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/4436354-An-oracle-white-paper-june-2012-high-performance-connectors-for-load-and-access-of-data-from-hadoop-to-oracle-database.html\">An Oracle White Paper June 2012. High Performance Connectors for Load and Access of Data from Hadoop to Oracle Database</a>\n    </h3><p>\n        <img alt=\"An Oracle White Paper June 2012. High Performance Connectors for Load and Access of Data from Hadoop to Oracle Database\" title=\"An Oracle White Paper June 2012. High Performance Connectors for Load and Access of Data from Hadoop to Oracle Database\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/24/4436354.jpg\" />\n        An Oracle White Paper June 2012 High Performance Connectors for Load and Access of Data from Hadoop to Oracle Database Executive Overview... 1 Introduction... 1 Oracle Loader for Hadoop... 2 Oracle Direct    </p><a href=\"https://docplayer.net/4436354-An-oracle-white-paper-june-2012-high-performance-connectors-for-load-and-access-of-data-from-hadoop-to-oracle-database.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/18036610-Big-data-value-use-cases-and-architectures-petar-torre-lead-architect-service-provider-group-dubrovnik-croatia-south-east-europe-20-22-may-2013.html\">Big Data. Value, use cases and architectures. Petar Torre Lead Architect Service Provider Group. Dubrovnik, Croatia, South East Europe 20-22 May, 2013</a>\n    </h3><p>\n        <img alt=\"Big Data. Value, use cases and architectures. Petar Torre Lead Architect Service Provider Group. Dubrovnik, Croatia, South East Europe 20-22 May, 2013\" title=\"Big Data. Value, use cases and architectures. Petar Torre Lead Architect Service Provider Group. Dubrovnik, Croatia, South East Europe 20-22 May, 2013\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/38/18036610.jpg\" />\n        Dubrovnik, Croatia, South East Europe 20-22 May, 2013 Big Data Value, use cases and architectures Petar Torre Lead Architect Service Provider Group 2011 2013 Cisco and/or its affiliates. All rights reserved.    </p><a href=\"https://docplayer.net/18036610-Big-data-value-use-cases-and-architectures-petar-torre-lead-architect-service-provider-group-dubrovnik-croatia-south-east-europe-20-22-may-2013.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/8943636-Architecting-for-the-next-generation-of-big-data-hortonworks-hdp-2-0-on-red-hat-enterprise-linux-6-with-openjdk-7.html\">Architecting for the next generation of Big Data Hortonworks HDP 2.0 on Red Hat Enterprise Linux 6 with OpenJDK 7</a>\n    </h3><p>\n        <img alt=\"Architecting for the next generation of Big Data Hortonworks HDP 2.0 on Red Hat Enterprise Linux 6 with OpenJDK 7\" title=\"Architecting for the next generation of Big Data Hortonworks HDP 2.0 on Red Hat Enterprise Linux 6 with OpenJDK 7\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/26/8943636.jpg\" />\n        Architecting for the next generation of Big Data Hortonworks HDP 2.0 on Red Hat Enterprise Linux 6 with OpenJDK 7 Yan Fisher Senior Principal Product Marketing Manager, Red Hat Rohit Bakhshi Product Manager,    </p><a href=\"https://docplayer.net/8943636-Architecting-for-the-next-generation-of-big-data-hortonworks-hdp-2-0-on-red-hat-enterprise-linux-6-with-openjdk-7.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/693948-Cloudera-enterprise-reference-architecture-for-google-cloud-platform-deployments.html\">Cloudera Enterprise Reference Architecture for Google Cloud Platform Deployments</a>\n    </h3><p>\n        <img alt=\"Cloudera Enterprise Reference Architecture for Google Cloud Platform Deployments\" title=\"Cloudera Enterprise Reference Architecture for Google Cloud Platform Deployments\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/18/693948.jpg\" />\n        Cloudera Enterprise Reference Architecture for Google Cloud Platform Deployments Important Notice 2010-2015 Cloudera, Inc. All rights reserved. Cloudera, the Cloudera logo, Cloudera Impala, Impala, and    </p><a href=\"https://docplayer.net/693948-Cloudera-enterprise-reference-architecture-for-google-cloud-platform-deployments.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/7209780-Red-hat-enterprise-virtualization-for-servers-pricing-licensing-guide.html\">RED HAT ENTERPRISE VIRTUALIZATION FOR SERVERS: PRICING &amp; LICENSING GUIDE</a>\n    </h3><p>\n        <img alt=\"RED HAT ENTERPRISE VIRTUALIZATION FOR SERVERS: PRICING &amp; LICENSING GUIDE\" title=\"RED HAT ENTERPRISE VIRTUALIZATION FOR SERVERS: PRICING &amp; LICENSING GUIDE\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/26/7209780.jpg\" />\n        RED HAT ENTERPRISE VIRTUALIZATION FOR SERVERS: PRICING &amp; LICENSING GUIDE Red Hat Enterprise Virtualization for Servers: Pricing Guide 1 TABLE OF CONTENTS Introduction to Red Hat Enterprise Virtualization    </p><a href=\"https://docplayer.net/7209780-Red-hat-enterprise-virtualization-for-servers-pricing-licensing-guide.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/23519835-Design-and-performance-report-for-hadoop-on-intel-xeon-based-hp-proliant-dl380-servers.html\">Design and performance report for Hadoop on Intel Xeon-based HP ProLiant DL380 servers</a>\n    </h3><p>\n        <img alt=\"Design and performance report for Hadoop on Intel Xeon-based HP ProLiant DL380 servers\" title=\"Design and performance report for Hadoop on Intel Xeon-based HP ProLiant DL380 servers\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/46/23519835.jpg\" />\n        Technical white paper Design and performance report for Hadoop on Intel Xeon-based HP ProLiant DL380 servers Table of contents Executive summary... 2 Introduction... 2 Test topology... 3 System design    </p><a href=\"https://docplayer.net/23519835-Design-and-performance-report-for-hadoop-on-intel-xeon-based-hp-proliant-dl380-servers.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/821604-Oracle-s-big-data-solutions-roger-wullschleger-insert-picture-here.html\">Oracle s Big Data solutions. Roger Wullschleger. &lt;Insert Picture Here&gt;</a>\n    </h3><p>\n        <img alt=\"Oracle s Big Data solutions. Roger Wullschleger. &lt;Insert Picture Here&gt;\" title=\"Oracle s Big Data solutions. Roger Wullschleger. &lt;Insert Picture Here&gt;\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/18/821604.jpg\" />\n         s Big Data solutions Roger Wullschleger DBTA Workshop on Big Data, Cloud Data Management and NoSQL 10. October 2012, Stade de Suisse, Berne 1 The following is intended to outline    </p><a href=\"https://docplayer.net/821604-Oracle-s-big-data-solutions-roger-wullschleger-insert-picture-here.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/8731510-Red-hat-enterprise-virtualization-for-servers-competitive-features.html\">RED HAT ENTERPRISE VIRTUALIZATION FOR SERVERS: COMPETITIVE FEATURES</a>\n    </h3><p>\n        <img alt=\"RED HAT ENTERPRISE VIRTUALIZATION FOR SERVERS: COMPETITIVE FEATURES\" title=\"RED HAT ENTERPRISE VIRTUALIZATION FOR SERVERS: COMPETITIVE FEATURES\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/26/8731510.jpg\" />\n        RED HAT ENTERPRISE VIRTUALIZATION FOR SERVERS: COMPETITIVE FEATURES RED HAT ENTERPRISE VIRTUALIZATION FOR SERVERS Server virtualization offers tremendous benefits for enterprise IT organizations server    </p><a href=\"https://docplayer.net/8731510-Red-hat-enterprise-virtualization-for-servers-competitive-features.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/8330899-Dell-powervault-md32xx-deployment-guide-for-vmware-esx4-1-server.html\">Dell PowerVault MD32xx Deployment Guide for VMware ESX4.1 Server</a>\n    </h3><p>\n        <img alt=\"Dell PowerVault MD32xx Deployment Guide for VMware ESX4.1 Server\" title=\"Dell PowerVault MD32xx Deployment Guide for VMware ESX4.1 Server\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/26/8330899.jpg\" />\n        Dell PowerVault MD32xx Deployment Guide for VMware ESX4.1 Server A Dell Technical White Paper PowerVault MD32xx Storage Array www.dell.com/md32xx THIS WHITE PAPER IS FOR INFORMATIONAL PURPOSES ONLY, AND    </p><a href=\"https://docplayer.net/8330899-Dell-powervault-md32xx-deployment-guide-for-vmware-esx4-1-server.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/4786415-Parallels-cloud-storage.html\">Parallels Cloud Storage</a>\n    </h3><p>\n        <img alt=\"Parallels Cloud Storage\" title=\"Parallels Cloud Storage\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/25/4786415.jpg\" />\n        Parallels Cloud Storage White Paper Best Practices for Configuring a Parallels Cloud Storage Cluster www.parallels.com Table of Contents Introduction... 3 How Parallels Cloud Storage Works... 3 Deploying    </p><a href=\"https://docplayer.net/4786415-Parallels-cloud-storage.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/4719306-Sql-server-virtualization.html\">SQL Server Virtualization</a>\n    </h3><p>\n        <img alt=\"SQL Server Virtualization\" title=\"SQL Server Virtualization\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/25/4719306.jpg\" />\n        The Essential Guide to SQL Server Virtualization S p o n s o r e d b y Virtualization in the Enterprise Today most organizations understand the importance of implementing virtualization. Virtualization    </p><a href=\"https://docplayer.net/4719306-Sql-server-virtualization.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/17287106-Unisys-clearpath-forward-fabric-based-platform-to-power-the-weather-enterprise.html\">Unisys ClearPath Forward Fabric Based Platform to Power the Weather Enterprise</a>\n    </h3><p>\n        <img alt=\"Unisys ClearPath Forward Fabric Based Platform to Power the Weather Enterprise\" title=\"Unisys ClearPath Forward Fabric Based Platform to Power the Weather Enterprise\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/35/17287106.jpg\" />\n        Unisys ClearPath Forward Fabric Based Platform to Power the Weather Enterprise Introducing Unisys All in One software based weather platform designed to reduce server space, streamline operations, consolidate    </p><a href=\"https://docplayer.net/17287106-Unisys-clearpath-forward-fabric-based-platform-to-power-the-weather-enterprise.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/14489788-Brocade-and-emc-solution-for-microsoft-hyper-v-and-sharepoint-clusters.html\">Brocade and EMC Solution for Microsoft Hyper-V and SharePoint Clusters</a>\n    </h3><p>\n        <img alt=\"Brocade and EMC Solution for Microsoft Hyper-V and SharePoint Clusters\" title=\"Brocade and EMC Solution for Microsoft Hyper-V and SharePoint Clusters\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/30/14489788.jpg\" />\n        Brocade and EMC Solution for Microsoft Hyper-V and SharePoint Clusters Highlights a Brocade-EMC solution with EMC CLARiiON, EMC Atmos, Brocade Fibre Channel (FC) switches, Brocade FC HBAs, and Brocade    </p><a href=\"https://docplayer.net/14489788-Brocade-and-emc-solution-for-microsoft-hyper-v-and-sharepoint-clusters.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/2307271-Massively-scaled-infrastructure-for-verizon-cloud-compute-and-storage.html\">MASSIVELY SCALED INFRASTRUCTURE FOR VERIZON CLOUD COMPUTE AND STORAGE</a>\n    </h3><p>\n        <img alt=\"MASSIVELY SCALED INFRASTRUCTURE FOR VERIZON CLOUD COMPUTE AND STORAGE\" title=\"MASSIVELY SCALED INFRASTRUCTURE FOR VERIZON CLOUD COMPUTE AND STORAGE\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/24/2307271.jpg\" />\n        MASSIVELY SCALED INFRASTRUCTURE FOR VERIZON CLOUD COMPUTE AND STORAGE Challenge Create the world s highest performance enterprise class public cloud Provide granular, customized configurations defined    </p><a href=\"https://docplayer.net/2307271-Massively-scaled-infrastructure-for-verizon-cloud-compute-and-storage.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/20413688-Reference-architecture-for-dell-vis-self-service-creator-and-vmware-vsphere-4.html\">Reference Architecture for Dell VIS Self-Service Creator and VMware vsphere 4</a>\n    </h3><p>\n        <img alt=\"Reference Architecture for Dell VIS Self-Service Creator and VMware vsphere 4\" title=\"Reference Architecture for Dell VIS Self-Service Creator and VMware vsphere 4\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/39/20413688.jpg\" />\n        Reference Architecture for Dell VIS Self-Service Creator and VMware vsphere 4 Solutions for Large Environments Virtualization Solutions Engineering Ryan Weldon and Tom Harrington THIS WHITE PAPER IS FOR    </p><a href=\"https://docplayer.net/20413688-Reference-architecture-for-dell-vis-self-service-creator-and-vmware-vsphere-4.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/10025813-Red-hat-network-satellite-management-and-automation-of-your-red-hat-enterprise-linux-environment.html\">Red Hat Network Satellite Management and automation of your Red Hat Enterprise Linux environment</a>\n    </h3><p>\n        <img alt=\"Red Hat Network Satellite Management and automation of your Red Hat Enterprise Linux environment\" title=\"Red Hat Network Satellite Management and automation of your Red Hat Enterprise Linux environment\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/27/10025813.jpg\" />\n        Red Hat Network Satellite Management and automation of your Red Hat Enterprise Linux environment WHAT IS IT? Red Hat Network (RHN) Satellite server is an easy-to-use, advanced systems management platform    </p><a href=\"https://docplayer.net/10025813-Red-hat-network-satellite-management-and-automation-of-your-red-hat-enterprise-linux-environment.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/2778756-How-transactional-analytics-is-changing-the-future-of-business-a-look-at-the-options-use-cases-and-anti-patterns.html\">How Transactional Analytics is Changing the Future of Business A look at the options, use cases, and anti-patterns</a>\n    </h3><p>\n        <img alt=\"How Transactional Analytics is Changing the Future of Business A look at the options, use cases, and anti-patterns\" title=\"How Transactional Analytics is Changing the Future of Business A look at the options, use cases, and anti-patterns\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/24/2778756.jpg\" />\n        How Transactional Analytics is Changing the Future of Business A look at the options, use cases, and anti-patterns Table of Contents Abstract... 3 Introduction... 3 Definition... 3 The Expanding Digitization    </p><a href=\"https://docplayer.net/2778756-How-transactional-analytics-is-changing-the-future-of-business-a-look-at-the-options-use-cases-and-anti-patterns.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/2896678-8gb-fibre-channel-adapter-of-choice-in-microsoft-hyper-v-environments.html\">8Gb Fibre Channel Adapter of Choice in Microsoft Hyper-V Environments</a>\n    </h3><p>\n        <img alt=\"8Gb Fibre Channel Adapter of Choice in Microsoft Hyper-V Environments\" title=\"8Gb Fibre Channel Adapter of Choice in Microsoft Hyper-V Environments\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/24/2896678.jpg\" />\n        8Gb Fibre Channel Adapter of Choice in Microsoft Hyper-V Environments QLogic 8Gb Adapter Outperforms Emulex QLogic Offers Best Performance and Scalability in Hyper-V Environments Key Findings The QLogic    </p><a href=\"https://docplayer.net/2896678-8gb-fibre-channel-adapter-of-choice-in-microsoft-hyper-v-environments.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/23988962-Improved-virtualization-performance-with-9th-generation-servers.html\">Improved Virtualization Performance with 9th Generation Servers</a>\n    </h3><p>\n        <img alt=\"Improved Virtualization Performance with 9th Generation Servers\" title=\"Improved Virtualization Performance with 9th Generation Servers\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/48/23988962.jpg\" />\n        Improved Virtualization Performance with 9th Generation Servers David J. Morse Dell, Inc. August 2006 Contents Introduction... 4 VMware ESX Server 3.0... 4 SPECjbb2005... 4 BEA JRockit... 4 Hardware/Software    </p><a href=\"https://docplayer.net/23988962-Improved-virtualization-performance-with-9th-generation-servers.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/17368540-Dell-microsoft-business-intelligence-and-data-warehousing-reference-configuration-performance-results-phase-iii.html\">Dell Microsoft Business Intelligence and Data Warehousing Reference Configuration Performance Results Phase III</a>\n    </h3><p>\n        <img alt=\"Dell Microsoft Business Intelligence and Data Warehousing Reference Configuration Performance Results Phase III\" title=\"Dell Microsoft Business Intelligence and Data Warehousing Reference Configuration Performance Results Phase III\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/36/17368540.jpg\" />\n        White Paper Dell Microsoft Business Intelligence and Data Warehousing Reference Configuration Performance Results Phase III Performance of Microsoft SQL Server 2008 BI and D/W Solutions on Dell PowerEdge    </p><a href=\"https://docplayer.net/17368540-Dell-microsoft-business-intelligence-and-data-warehousing-reference-configuration-performance-results-phase-iii.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/10470845-Real-time-big-data-analytics-sap-hana-with-the-intel-distribution-for-apache-hadoop-software.html\">Real-Time Big Data Analytics SAP HANA with the Intel Distribution for Apache Hadoop software</a>\n    </h3><p>\n        <img alt=\"Real-Time Big Data Analytics SAP HANA with the Intel Distribution for Apache Hadoop software\" title=\"Real-Time Big Data Analytics SAP HANA with the Intel Distribution for Apache Hadoop software\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/27/10470845.jpg\" />\n        Real-Time Big Data Analytics with the Intel Distribution for Apache Hadoop software Executive Summary is already helping businesses extract value out of Big Data by enabling real-time analysis of diverse    </p><a href=\"https://docplayer.net/10470845-Real-time-big-data-analytics-sap-hana-with-the-intel-distribution-for-apache-hadoop-software.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/12656133-Intel-cloud-builder-guide-to-cloud-design-and-deployment-on-intel-platforms.html\">Intel Cloud Builder Guide to Cloud Design and Deployment on Intel Platforms</a>\n    </h3><p>\n        <img alt=\"Intel Cloud Builder Guide to Cloud Design and Deployment on Intel Platforms\" title=\"Intel Cloud Builder Guide to Cloud Design and Deployment on Intel Platforms\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/28/12656133.jpg\" />\n        Intel Cloud Builder Guide to Cloud Design and Deployment on Intel Platforms Ubuntu* Enterprise Cloud Executive Summary Intel Cloud Builder Guide Intel Xeon Processor Ubuntu* Enteprise Cloud Canonical*    </p><a href=\"https://docplayer.net/12656133-Intel-cloud-builder-guide-to-cloud-design-and-deployment-on-intel-platforms.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/10109135-Diablo-technologies-memory-channel-storage-and-vmware-virtual-san-vdi-acceleration.html\">DIABLO TECHNOLOGIES MEMORY CHANNEL STORAGE AND VMWARE VIRTUAL SAN : VDI ACCELERATION</a>\n    </h3><p>\n        <img alt=\"DIABLO TECHNOLOGIES MEMORY CHANNEL STORAGE AND VMWARE VIRTUAL SAN : VDI ACCELERATION\" title=\"DIABLO TECHNOLOGIES MEMORY CHANNEL STORAGE AND VMWARE VIRTUAL SAN : VDI ACCELERATION\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/27/10109135.jpg\" />\n        DIABLO TECHNOLOGIES MEMORY CHANNEL STORAGE AND VMWARE VIRTUAL SAN : VDI ACCELERATION A DIABLO WHITE PAPER AUGUST 2014 Ricky Trigalo Director of Business Development Virtualization, Diablo Technologies    </p><a href=\"https://docplayer.net/10109135-Diablo-technologies-memory-channel-storage-and-vmware-virtual-san-vdi-acceleration.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/23223745-Big-data-big-deal-for-public-sector-organizations.html\">Big Data Big Deal for Public Sector Organizations</a>\n    </h3><p>\n        <img alt=\"Big Data Big Deal for Public Sector Organizations\" title=\"Big Data Big Deal for Public Sector Organizations\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/44/23223745.jpg\" />\n        Big Data Big Deal for Public Sector Organizations Hoàng Xuân Hiếu Director, FAB &amp; Government Business Indochina &amp; Myanmar 1 Copyright 2013, Oracle and/or its affiliates. All rights reserved. The following    </p><a href=\"https://docplayer.net/23223745-Big-data-big-deal-for-public-sector-organizations.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/10313372-Configuring-dell-openmanage-it-assistant-8-0-to-monitor-snmp-traps-generated-by-vmware-esx-server.html\">Configuring Dell OpenManage IT Assistant 8.0 to Monitor SNMP Traps Generated by VMware ESX Server</a>\n    </h3><p>\n        <img alt=\"Configuring Dell OpenManage IT Assistant 8.0 to Monitor SNMP Traps Generated by VMware ESX Server\" title=\"Configuring Dell OpenManage IT Assistant 8.0 to Monitor SNMP Traps Generated by VMware ESX Server\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/27/10313372.jpg\" />\n        Configuring Dell OpenManage IT Assistant 8.0 to Monitor SNMP Traps Generated by VMware ESX Server Amresh Singh Dell Virtualization Solutions Engineering January 2007 Dell Inc. 1 www.dell.com/vmware Contents    </p><a href=\"https://docplayer.net/10313372-Configuring-dell-openmanage-it-assistant-8-0-to-monitor-snmp-traps-generated-by-vmware-esx-server.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/10760212-Cisco-for-sap-hana-scale-out-solution-on-cisco-ucs-with-netapp-storage.html\">Cisco for SAP HANA Scale-Out Solution on Cisco UCS with NetApp Storage</a>\n    </h3><p>\n        <img alt=\"Cisco for SAP HANA Scale-Out Solution on Cisco UCS with NetApp Storage\" title=\"Cisco for SAP HANA Scale-Out Solution on Cisco UCS with NetApp Storage\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/27/10760212.jpg\" />\n        Cisco for SAP HANA Scale-Out Solution Solution Brief December 2014 With Intelligent Intel Xeon Processors Highlights Scale SAP HANA on Demand Scale-out capabilities, combined with high-performance NetApp    </p><a href=\"https://docplayer.net/10760212-Cisco-for-sap-hana-scale-out-solution-on-cisco-ucs-with-netapp-storage.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/17473427-Dell-virtual-desktop-infrastructure-study-end-to-end-computing-dell-enterprise-solutions-engineering.html\">DELL. Virtual Desktop Infrastructure Study END-TO-END COMPUTING. Dell Enterprise Solutions Engineering</a>\n    </h3><p>\n        <img alt=\"DELL. Virtual Desktop Infrastructure Study END-TO-END COMPUTING. Dell Enterprise Solutions Engineering\" title=\"DELL. Virtual Desktop Infrastructure Study END-TO-END COMPUTING. Dell Enterprise Solutions Engineering\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/36/17473427.jpg\" />\n        DELL Virtual Desktop Infrastructure Study END-TO-END COMPUTING Dell Enterprise Solutions Engineering 1 THIS WHITE PAPER IS FOR INFORMATIONAL PURPOSES ONLY, AND MAY CONTAIN TYPOGRAPHICAL ERRORS AND TECHNICAL    </p><a href=\"https://docplayer.net/17473427-Dell-virtual-desktop-infrastructure-study-end-to-end-computing-dell-enterprise-solutions-engineering.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/10066224-Using-red-hat-network-satellite-server-to-manage-dell-poweredge-servers.html\">Using Red Hat Network Satellite Server to Manage Dell PowerEdge Servers</a>\n    </h3><p>\n        <img alt=\"Using Red Hat Network Satellite Server to Manage Dell PowerEdge Servers\" title=\"Using Red Hat Network Satellite Server to Manage Dell PowerEdge Servers\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/27/10066224.jpg\" />\n        Using Red Hat Network Satellite Server to Manage Dell PowerEdge Servers Enterprise Product Group (EPG) Dell White Paper By Todd Muirhead and Peter Lillian July 2004 Contents Executive Summary... 3 Introduction...    </p><a href=\"https://docplayer.net/10066224-Using-red-hat-network-satellite-server-to-manage-dell-poweredge-servers.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/17103227-Fast-low-overhead-encryption-for-apache-hadoop.html\">Fast, Low-Overhead Encryption for Apache Hadoop*</a>\n    </h3><p>\n        <img alt=\"Fast, Low-Overhead Encryption for Apache Hadoop*\" title=\"Fast, Low-Overhead Encryption for Apache Hadoop*\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/34/17103227.jpg\" />\n        Fast, Low-Overhead Encryption for Apache Hadoop* Solution Brief Intel Xeon Processors Intel Advanced Encryption Standard New Instructions (Intel AES-NI) The Intel Distribution for Apache Hadoop* software    </p><a href=\"https://docplayer.net/17103227-Fast-low-overhead-encryption-for-apache-hadoop.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/10867809-Emc-unified-storage-for-microsoft-sql-server-2008.html\">EMC Unified Storage for Microsoft SQL Server 2008</a>\n    </h3><p>\n        <img alt=\"EMC Unified Storage for Microsoft SQL Server 2008\" title=\"EMC Unified Storage for Microsoft SQL Server 2008\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/27/10867809.jpg\" />\n        EMC Unified Storage for Microsoft SQL Server 2008 Enabled by EMC CLARiiON and EMC FAST Cache Reference Copyright 2010 EMC Corporation. All rights reserved. Published October, 2010 EMC believes the information    </p><a href=\"https://docplayer.net/10867809-Emc-unified-storage-for-microsoft-sql-server-2008.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/17928915-Dell-poweredge-blades-outperform-cisco-ucs-in-east-west-network-performance.html\">Dell PowerEdge Blades Outperform Cisco UCS in East-West Network Performance</a>\n    </h3><p>\n        <img alt=\"Dell PowerEdge Blades Outperform Cisco UCS in East-West Network Performance\" title=\"Dell PowerEdge Blades Outperform Cisco UCS in East-West Network Performance\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/38/17928915.jpg\" />\n        Dell PowerEdge Blades Outperform Cisco UCS in East-West Network Performance This white paper compares the performance of blade-to-blade network traffic between two enterprise blade solutions: the Dell    </p><a href=\"https://docplayer.net/17928915-Dell-poweredge-blades-outperform-cisco-ucs-in-east-west-network-performance.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/6387819-Microsoft-exchange-2010-on-dell-systems-simple-distributed-configurations.html\">Microsoft Exchange 2010 on Dell Systems. Simple Distributed Configurations</a>\n    </h3><p>\n        <img alt=\"Microsoft Exchange 2010 on Dell Systems. Simple Distributed Configurations\" title=\"Microsoft Exchange 2010 on Dell Systems. Simple Distributed Configurations\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/25/6387819.jpg\" />\n        Microsoft Exchange 2010 on Dell Systems Simple Distributed Configurations Global Solutions Engineering Dell Product Group Microsoft Exchange 2010 on Dell Systems Simple Distributed Configurations This    </p><a href=\"https://docplayer.net/6387819-Microsoft-exchange-2010-on-dell-systems-simple-distributed-configurations.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/7946515-Intel-ethernet-switch-load-balancing-system-design-using-advanced-features-in-intel-ethernet-switch-family.html\">Intel Ethernet Switch Load Balancing System Design Using Advanced Features in Intel Ethernet Switch Family</a>\n    </h3><p>\n        <img alt=\"Intel Ethernet Switch Load Balancing System Design Using Advanced Features in Intel Ethernet Switch Family\" title=\"Intel Ethernet Switch Load Balancing System Design Using Advanced Features in Intel Ethernet Switch Family\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/26/7946515.jpg\" />\n        Intel Ethernet Switch Load Balancing System Design Using Advanced Features in Intel Ethernet Switch Family White Paper June, 2008 Legal INFORMATION IN THIS DOCUMENT IS PROVIDED IN CONNECTION WITH INTEL    </p><a href=\"https://docplayer.net/7946515-Intel-ethernet-switch-load-balancing-system-design-using-advanced-features-in-intel-ethernet-switch-family.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/18527978-Vmware-esx-2-5-server-software-backup-and-restore-guide-on-dell-poweredge-servers-and-powervault-storage.html\">VMware ESX 2.5 Server Software Backup and Restore Guide on Dell PowerEdge Servers and PowerVault Storage</a>\n    </h3><p>\n        <img alt=\"VMware ESX 2.5 Server Software Backup and Restore Guide on Dell PowerEdge Servers and PowerVault Storage\" title=\"VMware ESX 2.5 Server Software Backup and Restore Guide on Dell PowerEdge Servers and PowerVault Storage\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/39/18527978.jpg\" />\n        VMware ESX 2.5 Server Software Backup and Restore Guide on Dell PowerEdge Servers and PowerVault Storage This document provides best practices for backup and recovery of Virtual Machines running on VMware    </p><a href=\"https://docplayer.net/18527978-Vmware-esx-2-5-server-software-backup-and-restore-guide-on-dell-poweredge-servers-and-powervault-storage.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/11467199-Powerful-duo-mapr-big-data-analytics-with-cisco-aci-network-switches.html\">Powerful Duo: MapR Big Data Analytics with Cisco ACI Network Switches</a>\n    </h3><p>\n        <img alt=\"Powerful Duo: MapR Big Data Analytics with Cisco ACI Network Switches\" title=\"Powerful Duo: MapR Big Data Analytics with Cisco ACI Network Switches\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/27/11467199.jpg\" />\n        Powerful Duo: MapR Big Data Analytics with Cisco ACI Network Switches Introduction For companies that want to quickly gain insights into or opportunities from big data - the dramatic volume growth in corporate    </p><a href=\"https://docplayer.net/11467199-Powerful-duo-mapr-big-data-analytics-with-cisco-aci-network-switches.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/3188954-Improving-it-operational-efficiency-with-a-vmware-vsphere-private-cloud-on-lenovo-servers-and-lenovo-storage-san-s3200.html\">Improving IT Operational Efficiency with a VMware vsphere Private Cloud on Lenovo Servers and Lenovo Storage SAN S3200</a>\n    </h3><p>\n        <img alt=\"Improving IT Operational Efficiency with a VMware vsphere Private Cloud on Lenovo Servers and Lenovo Storage SAN S3200\" title=\"Improving IT Operational Efficiency with a VMware vsphere Private Cloud on Lenovo Servers and Lenovo Storage SAN S3200\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/24/3188954.jpg\" />\n        Improving IT Operational Efficiency with a VMware vsphere Private Cloud on Lenovo Servers and Lenovo Storage SAN S3200 Most organizations routinely utilize a server virtualization infrastructure to benefit    </p><a href=\"https://docplayer.net/3188954-Improving-it-operational-efficiency-with-a-vmware-vsphere-private-cloud-on-lenovo-servers-and-lenovo-storage-san-s3200.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/8107260-Cost-effective-business-intelligence-with-red-hat-and-open-source.html\">Cost-Effective Business Intelligence with Red Hat and Open Source</a>\n    </h3><p>\n        <img alt=\"Cost-Effective Business Intelligence with Red Hat and Open Source\" title=\"Cost-Effective Business Intelligence with Red Hat and Open Source\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/26/8107260.jpg\" />\n        Cost-Effective Business Intelligence with Red Hat and Open Source Sherman Wood Director, Business Intelligence, Jaspersoft September 3, 2009 1 Agenda Introductions Quick survey What is BI?: reporting,    </p><a href=\"https://docplayer.net/8107260-Cost-effective-business-intelligence-with-red-hat-and-open-source.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/8232124-Hp-reference-configuration-for-entry-level-sas-grid-manager-solutions.html\">HP reference configuration for entry-level SAS Grid Manager solutions</a>\n    </h3><p>\n        <img alt=\"HP reference configuration for entry-level SAS Grid Manager solutions\" title=\"HP reference configuration for entry-level SAS Grid Manager solutions\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/26/8232124.jpg\" />\n        HP reference configuration for entry-level SAS Grid Manager solutions Up to 864 simultaneous SAS jobs and more than 3 GB/s I/O throughput Technical white paper Table of contents Executive summary... 2    </p><a href=\"https://docplayer.net/8232124-Hp-reference-configuration-for-entry-level-sas-grid-manager-solutions.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/13460875-Hdb-high-availability-with-l-tango-meeting-l-20-may-2015-l-reynald-bourtembourg.html\">HDB++: HIGH AVAILABILITY WITH. l TANGO Meeting l 20 May 2015 l Reynald Bourtembourg</a>\n    </h3><p>\n        <img alt=\"HDB++: HIGH AVAILABILITY WITH. l TANGO Meeting l 20 May 2015 l Reynald Bourtembourg\" title=\"HDB++: HIGH AVAILABILITY WITH. l TANGO Meeting l 20 May 2015 l Reynald Bourtembourg\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/29/13460875.jpg\" />\n        HDB++: HIGH AVAILABILITY WITH Page 1 OVERVIEW What is Cassandra (C*)? Who is using C*? CQL C* architecture Request Coordination Consistency Monitoring tool HDB++ Page 2 OVERVIEW What is Cassandra (C*)?    </p><a href=\"https://docplayer.net/13460875-Hdb-high-availability-with-l-tango-meeting-l-20-may-2015-l-reynald-bourtembourg.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/2093594-Can-flash-help-you-ride-the-big-data-wave-steve-fingerhut-vice-president-marketing-enterprise-storage-solutions-corporation.html\">Can Flash help you ride the Big Data Wave? Steve Fingerhut Vice President, Marketing Enterprise Storage Solutions Corporation</a>\n    </h3><p>\n        <img alt=\"Can Flash help you ride the Big Data Wave? Steve Fingerhut Vice President, Marketing Enterprise Storage Solutions Corporation\" title=\"Can Flash help you ride the Big Data Wave? Steve Fingerhut Vice President, Marketing Enterprise Storage Solutions Corporation\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/26/2093594.jpg\" />\n        Can Flash help you ride the Big Data Wave? Steve Fingerhut Vice President, Marketing Enterprise Storage Solutions Corporation Forward-Looking Statements During our meeting today we may make forward-looking    </p><a href=\"https://docplayer.net/2093594-Can-flash-help-you-ride-the-big-data-wave-steve-fingerhut-vice-president-marketing-enterprise-storage-solutions-corporation.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/822724-Hur-hanterar-vi-utmaningar-inom-omradet-big-data-jan-ostling-enterprise-technologies-intel-corporation-ner.html\">Hur hanterar vi utmaningar inom området - Big Data. Jan Östling Enterprise Technologies Intel Corporation, NER</a>\n    </h3><p>\n        <img alt=\"Hur hanterar vi utmaningar inom området - Big Data. Jan Östling Enterprise Technologies Intel Corporation, NER\" title=\"Hur hanterar vi utmaningar inom området - Big Data. Jan Östling Enterprise Technologies Intel Corporation, NER\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/18/822724.jpg\" />\n        Hur hanterar vi utmaningar inom området - Big Data Jan Östling Enterprise Technologies Intel Corporation, NER Legal Disclaimers All products, computer systems, dates, and figures specified are preliminary    </p><a href=\"https://docplayer.net/822724-Hur-hanterar-vi-utmaningar-inom-omradet-big-data-jan-ostling-enterprise-technologies-intel-corporation-ner.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/13499113-Oracle-big-data-sql-technical-update.html\">Oracle Big Data SQL Technical Update</a>\n    </h3><p>\n        <img alt=\"Oracle Big Data SQL Technical Update\" title=\"Oracle Big Data SQL Technical Update\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/29/13499113.jpg\" />\n        Oracle Big Data SQL Technical Update Jean-Pierre Dijcks Oracle Redwood City, CA, USA Keywords: Big Data, Hadoop, NoSQL Databases, Relational Databases, SQL, Security, Performance Introduction This technical    </p><a href=\"https://docplayer.net/13499113-Oracle-big-data-sql-technical-update.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/14263009-Microsoft-sharepoint-server-2010.html\">Microsoft SharePoint Server 2010</a>\n    </h3><p>\n        <img alt=\"Microsoft SharePoint Server 2010\" title=\"Microsoft SharePoint Server 2010\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/30/14263009.jpg\" />\n        Microsoft SharePoint Server 2010 Small Farm Performance Study Dell SharePoint Solutions Ravikanth Chaganti and Quocdat Nguyen November 2010 THIS WHITE PAPER IS FOR INFORMATIONAL PURPOSES ONLY, AND MAY    </p><a href=\"https://docplayer.net/14263009-Microsoft-sharepoint-server-2010.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/10066332-Red-hat-satellite-management-and-automation-of-your-red-hat-enterprise-linux-environment.html\">Red Hat Satellite Management and automation of your Red Hat Enterprise Linux environment</a>\n    </h3><p>\n        <img alt=\"Red Hat Satellite Management and automation of your Red Hat Enterprise Linux environment\" title=\"Red Hat Satellite Management and automation of your Red Hat Enterprise Linux environment\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/27/10066332.jpg\" />\n        Red Hat Satellite Management and automation of your Red Hat Enterprise Linux environment WHAT IS IT? Red Hat Satellite server is an easy-to-use, advanced systems management platform for your Linux infrastructure.    </p><a href=\"https://docplayer.net/10066332-Red-hat-satellite-management-and-automation-of-your-red-hat-enterprise-linux-environment.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/5724407-Intel-raid-ssd-cache-controller-rcs25zb040.html\">Intel RAID SSD Cache Controller RCS25ZB040</a>\n    </h3><p>\n        <img alt=\"Intel RAID SSD Cache Controller RCS25ZB040\" title=\"Intel RAID SSD Cache Controller RCS25ZB040\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/25/5724407.jpg\" />\n        SOLUTION Brief Intel RAID SSD Cache Controller RCS25ZB040 When Faster Matters Cost-Effective Intelligent RAID with Embedded High Performance Flash Intel RAID SSD Cache Controller RCS25ZB040 When Faster    </p><a href=\"https://docplayer.net/5724407-Intel-raid-ssd-cache-controller-rcs25zb040.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/2722060-Makemytrip-customer-success-story.html\">MakeMyTrip CUSTOMER SUCCESS STORY</a>\n    </h3><p>\n        <img alt=\"MakeMyTrip CUSTOMER SUCCESS STORY\" title=\"MakeMyTrip CUSTOMER SUCCESS STORY\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/24/2722060.jpg\" />\n        MakeMyTrip CUSTOMER SUCCESS STORY MakeMyTrip is the leading travel site in India that is running two ClustrixDB clusters as multi-master in two regions. It removed single point of failure. MakeMyTrip frequently    </p><a href=\"https://docplayer.net/2722060-Makemytrip-customer-success-story.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/11293596-Ms-exchange-server-acceleration.html\">MS Exchange Server Acceleration</a>\n    </h3><p>\n        <img alt=\"MS Exchange Server Acceleration\" title=\"MS Exchange Server Acceleration\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/27/11293596.jpg\" />\n        White Paper MS Exchange Server Acceleration Using virtualization to dramatically maximize user experience for Microsoft Exchange Server Allon Cohen, PhD Scott Harlin OCZ Storage Solutions, Inc. A Toshiba    </p><a href=\"https://docplayer.net/11293596-Ms-exchange-server-acceleration.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/16259198-Built-for-business-ready-for-the-future.html\">Built for Business. Ready for the Future.</a>\n    </h3><p>\n        <img alt=\"Built for Business. Ready for the Future.\" title=\"Built for Business. Ready for the Future.\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/33/16259198.jpg\" />\n        Built for Business. Ready for the Future. Addressing End User and IT Needs Introducing 4 th Generation Intel Core Products Addressing Datacenter Needs Introducing Intel in Dell PowerEdge VRTX Usage Model    </p><a href=\"https://docplayer.net/16259198-Built-for-business-ready-for-the-future.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/2348822-Load-balancing-for-microsoft-office-communication-server-2007-release-2.html\">Load Balancing for Microsoft Office Communication Server 2007 Release 2</a>\n    </h3><p>\n        <img alt=\"Load Balancing for Microsoft Office Communication Server 2007 Release 2\" title=\"Load Balancing for Microsoft Office Communication Server 2007 Release 2\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/24/2348822.jpg\" />\n        Load Balancing for Microsoft Office Communication Server 2007 Release 2 A Dell and F5 Networks Technical White Paper End-to-End Solutions Team Dell Product Group Enterprise Dell/F5 Partner Team F5 Networks    </p><a href=\"https://docplayer.net/2348822-Load-balancing-for-microsoft-office-communication-server-2007-release-2.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/21693677-Dual-core-processors-on-dell-supported-operating-systems.html\">Dual-Core Processors on Dell-Supported Operating Systems</a>\n    </h3><p>\n        <img alt=\"Dual-Core Processors on Dell-Supported Operating Systems\" title=\"Dual-Core Processors on Dell-Supported Operating Systems\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/40/21693677.jpg\" />\n        Dual-Core Processors on Dell-Supported Operating Systems With the advent of Intel s dual-core, in addition to existing Hyper-Threading capability, there is confusion regarding the number of that software    </p><a href=\"https://docplayer.net/21693677-Dual-core-processors-on-dell-supported-operating-systems.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/759353-The-open-cloud-near-term-infrastructure-trends-in-cloud-computing.html\">The Open Cloud Near-Term Infrastructure Trends in Cloud Computing</a>\n    </h3><p>\n        <img alt=\"The Open Cloud Near-Term Infrastructure Trends in Cloud Computing\" title=\"The Open Cloud Near-Term Infrastructure Trends in Cloud Computing\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/18/759353.jpg\" />\n        The Open Cloud Near-Term Infrastructure Trends in Cloud Computing Markus Leberecht BELNET Networking Conference 25-Oct-2012 1 Growth &amp; IT Challenges Drive Need for Cloud Computing IT Pros Growth IT Challenges    </p><a href=\"https://docplayer.net/759353-The-open-cloud-near-term-infrastructure-trends-in-cloud-computing.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/14682808-Accelerating-database-applications-on-linux-servers.html\">Accelerating Database Applications on Linux Servers</a>\n    </h3><p>\n        <img alt=\"Accelerating Database Applications on Linux Servers\" title=\"Accelerating Database Applications on Linux Servers\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/30/14682808.jpg\" />\n        White Paper Accelerating Database Applications on Linux Servers Introducing OCZ s LXL Software - Delivering a Data-Path Optimized Solution for Flash Acceleration Allon Cohen, PhD Yaron Klein Eli Ben Namer    </p><a href=\"https://docplayer.net/14682808-Accelerating-database-applications-on-linux-servers.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/7432074-Performance-characterization-report-for-microsoft-hyper-v-r2-on-hp-storageworks-p4500-san-storage.html\">Performance characterization report for Microsoft Hyper-V R2 on HP StorageWorks P4500 SAN storage</a>\n    </h3><p>\n        <img alt=\"Performance characterization report for Microsoft Hyper-V R2 on HP StorageWorks P4500 SAN storage\" title=\"Performance characterization report for Microsoft Hyper-V R2 on HP StorageWorks P4500 SAN storage\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/26/7432074.jpg\" />\n        Performance characterization report for Microsoft Hyper-V R2 on HP StorageWorks P4500 SAN storage Technical white paper Table of contents Executive summary... 2 Introduction... 2 Test methodology... 3    </p><a href=\"https://docplayer.net/7432074-Performance-characterization-report-for-microsoft-hyper-v-r2-on-hp-storageworks-p4500-san-storage.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/2777967-Comparing-the-hadoop-distributed-file-system-hdfs-with-the-cassandra-file-system-cfs.html\">Comparing the Hadoop Distributed File System (HDFS) with the Cassandra File System (CFS)</a>\n    </h3><p>\n        <img alt=\"Comparing the Hadoop Distributed File System (HDFS) with the Cassandra File System (CFS)\" title=\"Comparing the Hadoop Distributed File System (HDFS) with the Cassandra File System (CFS)\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/24/2777967.jpg\" />\n        Comparing the Hadoop Distributed File System (HDFS) with the Cassandra File System (CFS) White Paper BY DATASTAX CORPORATION August 2013 1 Table of Contents Abstract 3 Introduction 3 Overview of HDFS 4    </p><a href=\"https://docplayer.net/2777967-Comparing-the-hadoop-distributed-file-system-hdfs-with-the-cassandra-file-system-cfs.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/4436765-Mapr-enterprise-edition-enterprise-database-edition.html\">MapR Enterprise Edition &amp; Enterprise Database Edition</a>\n    </h3><p>\n        <img alt=\"MapR Enterprise Edition &amp; Enterprise Database Edition\" title=\"MapR Enterprise Edition &amp; Enterprise Database Edition\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/26/4436765.jpg\" />\n        MapR Enterprise Edition &amp; Enterprise Database Edition Reference Architecture A PSSC Labs Reference Architecture Guide June 2015 Introduction PSSC Labs continues to bring innovative compute server and cluster    </p><a href=\"https://docplayer.net/4436765-Mapr-enterprise-edition-enterprise-database-edition.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/9257451-Impact-of-big-data-growth-on-transparent-computing.html\">Impact of Big Data growth On Transparent Computing</a>\n    </h3><p>\n        <img alt=\"Impact of Big Data growth On Transparent Computing\" title=\"Impact of Big Data growth On Transparent Computing\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/26/9257451.jpg\" />\n        Impact of Big Data growth On Transparent Computing Michael A. Greene Intel Vice President, Software and Services Group, General Manager, System Technologies and Optimization 1 Transparent Computing (TC)    </p><a href=\"https://docplayer.net/9257451-Impact-of-big-data-growth-on-transparent-computing.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/6164563-Datastax-enterprise-reference-architecture-white-paper.html\">DataStax Enterprise Reference Architecture. White Paper</a>\n    </h3><p>\n        <img alt=\"DataStax Enterprise Reference Architecture. White Paper\" title=\"DataStax Enterprise Reference Architecture. White Paper\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/25/6164563.jpg\" />\n        DataStax Enterprise Reference Architecture White Paper BY DATASTAX CORPORATION January 2014 Table of Contents Abstract...3 Introduction...3 DataStax Enterprise Architecture...3 Management Interface...    </p><a href=\"https://docplayer.net/6164563-Datastax-enterprise-reference-architecture-white-paper.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/14533693-Maxdeploy-ready-hyper-converged-virtualization-solution-with-sandisk-fusion-iomemory-products.html\">MaxDeploy Ready. Hyper- Converged Virtualization Solution. With SanDisk Fusion iomemory products</a>\n    </h3><p>\n        <img alt=\"MaxDeploy Ready. Hyper- Converged Virtualization Solution. With SanDisk Fusion iomemory products\" title=\"MaxDeploy Ready. Hyper- Converged Virtualization Solution. With SanDisk Fusion iomemory products\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/30/14533693.jpg\" />\n        MaxDeploy Ready Hyper- Converged Virtualization Solution With SanDisk Fusion iomemory products MaxDeploy Ready products are configured and tested for support with Maxta software- defined storage and with    </p><a href=\"https://docplayer.net/14533693-Maxdeploy-ready-hyper-converged-virtualization-solution-with-sandisk-fusion-iomemory-products.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/7970503-N8103-149-150-151-160-raid-controller-n8103-156-megaraid-cachecade-feature-overview.html\">N8103-149/150/151/160 RAID Controller. N8103-156 MegaRAID CacheCade. Feature Overview</a>\n    </h3><p>\n        <img alt=\"N8103-149/150/151/160 RAID Controller. N8103-156 MegaRAID CacheCade. Feature Overview\" title=\"N8103-149/150/151/160 RAID Controller. N8103-156 MegaRAID CacheCade. Feature Overview\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/26/7970503.jpg\" />\n        N8103-149/150/151/160 RAID Controller N8103-156 MegaRAID CacheCade Feature Overview April 2012 Rev.1.0 NEC Corporation Contents 1 Introduction... 3 2 Types of RAID Controllers... 3 3 New Features of RAID    </p><a href=\"https://docplayer.net/7970503-N8103-149-150-151-160-raid-controller-n8103-156-megaraid-cachecade-feature-overview.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/6548218-White-paper-recording-server-virtualization.html\">White Paper. Recording Server Virtualization</a>\n    </h3><p>\n        <img alt=\"White Paper. Recording Server Virtualization\" title=\"White Paper. Recording Server Virtualization\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/25/6548218.jpg\" />\n        White Paper Recording Server Virtualization Prepared by: Mike Sherwood, Senior Solutions Engineer Milestone Systems 23 March 2011 Table of Contents Introduction... 3 Target audience and white paper purpose...    </p><a href=\"https://docplayer.net/6548218-White-paper-recording-server-virtualization.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/8668438-Dell-s-sap-hana-appliance.html\">Dell s SAP HANA Appliance</a>\n    </h3><p>\n        <img alt=\"Dell s SAP HANA Appliance\" title=\"Dell s SAP HANA Appliance\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/26/8668438.jpg\" />\n        Dell s SAP HANA Appliance SAP HANA is the next generation of SAP in-memory computing technology. Dell and SAP have partnered to deliver an SAP HANA appliance that provides multipurpose, data source-agnostic,    </p><a href=\"https://docplayer.net/8668438-Dell-s-sap-hana-appliance.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/3165445-The-future-of-data-management.html\">The Future of Data Management</a>\n    </h3><p>\n        <img alt=\"The Future of Data Management\" title=\"The Future of Data Management\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/24/3165445.jpg\" />\n        The Future of Data Management with Hadoop and the Enterprise Data Hub Amr Awadallah (@awadallah) Cofounder and CTO Cloudera Snapshot Founded 2008, by former employees of Employees Today ~ 800 World Class    </p><a href=\"https://docplayer.net/3165445-The-future-of-data-management.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div><div class=\"news-blocks\"><h3>\n        <a href=\"https://docplayer.net/19848674-Stackiq-enterprise-data-reference-architecture.html\">StackIQ Enterprise Data Reference Architecture</a>\n    </h3><p>\n        <img alt=\"StackIQ Enterprise Data Reference Architecture\" title=\"StackIQ Enterprise Data Reference Architecture\" class=\"news-block-img pull-right\" src=\"https://docplayer.net/thumbs/39/19848674.jpg\" />\n        white paper StackIQ Enterprise Data Reference Architecture StackIQ and Hortonworks worked together to Bring You World-class Reference Configurations for Apache Hadoop Clusters. Abstract Contents The Need    </p><a href=\"https://docplayer.net/19848674-Stackiq-enterprise-data-reference-architecture.html\" class=\"news-block-btn\">\n        More information <i class=\"m-icon-swapright m-icon-black\">\n    </i></a></div>",
        "created_at": "2019-01-23T21:39:15+0000",
        "updated_at": "2019-01-23T21:39:28+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 20,
        "domain_name": "docplayer.net",
        "preview_picture": "https://docplayer.net/thumbs/26/8944426.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12974"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 10,
            "label": "api",
            "slug": "api"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 217,
            "label": "tool",
            "slug": "tool"
          },
          {
            "id": 235,
            "label": "rest",
            "slug": "rest"
          }
        ],
        "is_public": false,
        "id": 12931,
        "uid": null,
        "title": "aksakalli/sandraREST",
        "url": "https://github.com/aksakalli/sandraREST",
        "content": "<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/aksakalli/sandraREST/blob/master/docs/img/screenshot.png\"><img src=\"https://github.com/aksakalli/sandraREST/raw/master/docs/img/screenshot.png\" alt=\"screen shot\" /></a></p><p>sandraREST is a Cassandra Manager REST API and Web UI.</p><h2>Features</h2><ul><li>Present database</li>\n<li>Run a CQL query</li>\n<li>Drop/Update/Create Keyspace</li>\n<li>Drop Column Family</li>\n<li>Drop/Update/Create Column</li>\n<li>List data of Column Family</li>\n<li>Download query result as CSV</li>\n</ul><h2>Getting Started</h2><p>As the project required use of multiple external dependencies\nfor both client and server side, bower and npm are used to\nmanage those respectively.</p><p>Cassandra client configuration is stored under root folder in\n<code>cassandra_config.json</code>. It is possible to configure contact\npoints and many other client options.</p><p>running the source code:</p><div class=\"highlight highlight-source-shell\"><pre>$ npm install\n$ bower install\n$ npm start</pre></div><h2>Architecture</h2><p>Main advantage of a REST API is that such organization of\na client­server communication allows single server to support\nnumerous client applications through the unified interfaces.</p><p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/aksakalli/sandraREST/blob/master/docs/img/overview.png\"><img src=\"https://github.com/aksakalli/sandraREST/raw/master/docs/img/overview.png\" alt=\"interface guide\" /></a></p><p>In the particular case, server with REST API was developed on\nthe node.js. Client side is single page web­application based\non angular.js framework that served on 'public' folder as static\nand only relies on REST API for access to the database data.</p><p><a href=\"https://github.com/aksakalli/sandraREST/blob/master/docs/rest_endpoints.md\">REST Endpoints</a></p><h2>License</h2><p>Released under <a href=\"https://github.com/aksakalli/sandraREST/blob/master/LICENSE\">the MIT license</a>.</p>",
        "created_at": "2019-01-15T15:31:49+0000",
        "updated_at": "2019-01-15T15:37:14+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/1939193?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12931"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 902,
            "label": "kubernetes",
            "slug": "kubernetes"
          }
        ],
        "is_public": false,
        "id": 12923,
        "uid": null,
        "title": "Kubernetes : Stateful Apps in Production and Distributed Database Orchestration",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=wUl81sWt928",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/wUl81sWt928?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2019-01-15T15:23:16+0000",
        "updated_at": "2019-01-15T15:23:16+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/wUl81sWt928/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12923"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 902,
            "label": "kubernetes",
            "slug": "kubernetes"
          },
          {
            "id": 1298,
            "label": "google.cloud",
            "slug": "google-cloud"
          }
        ],
        "is_public": false,
        "id": 12922,
        "uid": null,
        "title": "GoogleCloudPlatform/gke-stateful-applications-demo",
        "url": "https://github.com/GoogleCloudPlatform/gke-stateful-applications-demo",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<h2><a id=\"user-content-table-of-contents\" class=\"anchor\" aria-hidden=\"true\" href=\"#table-of-contents\"></a>Table of Contents</h2>\n<ul><li><a href=\"#google-kubernetes-engine-stateful-application-demo\">Google Kubernetes Engine Stateful Application Demo</a></li>\n<li><a href=\"#introduction\">Introduction</a></li>\n<li><a href=\"#architecture\">Architecture</a>\n<ul><li><a href=\"#architecture-diagram\">Architecture Diagram</a></li>\n<li><a href=\"#kubernetes-manifests-and-controllers\">Kubernetes Manifests and Controllers</a>\n<ul><li><a href=\"#statefulsets\">StatefulSets</a></li>\n<li><a href=\"#persistent-storage-with-persistent-volumes\">Persistent Storage with Persistent Volumes</a></li>\n<li><a href=\"#pod-disruptions-budgets\">Pod Disruptions Budgets</a></li>\n<li><a href=\"#pod-scheduling\">Pod Scheduling</a></li>\n<li><a href=\"#dns-and-headless-services\">DNS and Headless Services</a></li>\n</ul></li>\n<li><a href=\"#cassandra-container\">Cassandra Container</a></li>\n</ul></li>\n<li><a href=\"#prerequisites\">Prerequisites</a>\n<ul><li><a href=\"#run-demo-in-a-google-cloud-shell\">Run Demo in a Google Cloud Shell</a></li>\n<li><a href=\"#supported-operating-systems\">Supported Operating Systems</a></li>\n<li><a href=\"#tools\">Tools</a></li>\n<li><a href=\"#versions\">Versions</a></li>\n</ul></li>\n<li><a href=\"#deployment-steps\">Deployment Steps</a></li>\n<li><a href=\"#validation\">Validation</a>\n<ul><li><a href=\"#using-cassandara\">Using Cassandara</a></li>\n<li><a href=\"#launch-a-cassandra-container\">Launch a Cassandra container</a></li>\n<li><a href=\"#connect-to-the-ring-with-cqlsh\">Connect to the ring with cqlsh</a>\n<ul><li><a href=\"#cqlsh-command\">cqlsh command</a></li>\n</ul></li>\n<li><a href=\"#create-a-keyspace\">Create a keyspace</a></li>\n<li><a href=\"#describe-the-keyspace\">Describe the keyspace</a></li>\n<li><a href=\"#use-the-new-keyspace\">Use the new keyspace</a></li>\n<li><a href=\"#create-a-table\">Create a table</a></li>\n<li><a href=\"#see-the-newly-created-table\">See the newly created table</a></li>\n<li><a href=\"#insert-data-into-the-table\">Insert data into the table</a></li>\n<li><a href=\"#select-the-data-from-the-table\">Select the data from the table</a></li>\n<li><a href=\"#exit\">Exit</a>\n<ul><li><a href=\"#delete-the-deployment\">Delete the deployment</a></li>\n</ul></li>\n</ul></li>\n<li><a href=\"#tear-down\">Tear Down</a></li>\n<li><a href=\"#troubleshooting\">Troubleshooting</a></li>\n<li><a href=\"#relevant-materials\">Relevant Materials</a></li>\n</ul><h2><a id=\"user-content-introduction\" class=\"anchor\" aria-hidden=\"true\" href=\"#introduction\"></a>Introduction</h2>\n<p>This proof of concept deploys a\n<a href=\"https://cloud.google.com/kubernetes-engine/\" rel=\"nofollow\">Kubernetes Engine</a>\nCluster and then installs an <a href=\"http://cassandra.apache.org/\" rel=\"nofollow\">Apache Cassandra</a>\ndatabase running on that cluster.</p>\n<p>Various scripts are contained within this project that provide push button\ncreation, validation, and deletion of the Cassandra(C*) database and Kubernetes\nEngine cluster.</p>\n<p>Apache Cassandra was chosen for various reasons.  These reasons include that\nout of the box Cassandra functions well as a cloud native database.  Moreover,\nthis POC continues the work started with the Kubernetes example, and the blog\npost: <a href=\"https://kubernetes.io/blog/2016/07/thousand-instances-of-cassandra-using-kubernetes-pet-set\" rel=\"nofollow\">Thousand Instances of Cassandra using Kubernetes Pet Set</a>.</p>\n<p>When running a database using Kubernetes Engine, as an operator you need to be experienced with Kubernetes Engine\nand the datasource that you are running.  If you are able to use a hosted\nsolution then it is recommended that you use the hosted solution. On the other\nhand, if you are not able to run a datasource in a hosted solution then hosting\nit is Kubernetes Engine is doable.  The challenge is not Kubernetes or Kubernetes Engine, but the challenge\nis migrating and operating a database in a container based system that can do\nthings like moving pods/containers between nodes.</p>\n<h2><a id=\"user-content-architecture\" class=\"anchor\" aria-hidden=\"true\" href=\"#architecture\"></a>Architecture</h2>\n<p>The intricacy of the running stateful datastores via Kubernetes Engine or K8s lies with\nthe various Kubernetes manifests and pod configurations used. A container that\nhas been customized to run as a pod is used as well. This demo includes multiple\nconfigured manifests and a specialized container for Cassandra.</p>\n<p>Many people, including Kelsey Hightower, have talked about how running stateful\ndatestores inside of K8s is not trivial, and frankly quite complicated.  If you\ncan run a database on a managed service, do it.  Let someone else wake up at 2\nam to fix your database.</p>\n<p>There are two possibilities when you run a stateful datastore on K8s.</p>\n<ol><li>You are very experienced K8s user and know exactly what is around the corner.</li>\n<li>You have no idea what is around the corner, and you are going to learn very\nfast.</li>\n</ol><h3><a id=\"user-content-architecture-diagram\" class=\"anchor\" aria-hidden=\"true\" href=\"#architecture-diagram\"></a>Architecture Diagram</h3>\n<p>The following diagram represents a Cassandra cluster deployed on Kubernetes Engine.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/GoogleCloudPlatform/gke-stateful-applications-demo/blob/master/images/gke-cassandra.png\"><img src=\"https://github.com/GoogleCloudPlatform/gke-stateful-applications-demo/raw/master/images/gke-cassandra.png\" alt=\"Cassandra Running on Kubernetes Engine\" /></a></p>\n<h3><a id=\"user-content-kubernetes-manifests-and-controllers\" class=\"anchor\" aria-hidden=\"true\" href=\"#kubernetes-manifests-and-controllers\"></a>Kubernetes Manifests and Controllers</h3>\n<p>Various manifests and controllers are utilized to install Cassandra.  The\nfollowing sections outline the different types used.</p>\n<h4><a id=\"user-content-statefulsets\" class=\"anchor\" aria-hidden=\"true\" href=\"#statefulsets\"></a>StatefulSets</h4>\n<p>StatefulSets is the controller type that is used to install a Cassandra Ring on\nKubernetes Engine.  This controller manages the installation and scaling of a set of Pods and\nprovides various features:</p>\n<ol><li>Stable, unique network identifiers.</li>\n<li>Stable, persistent storage.</li>\n<li>Ordered, graceful deployment and scaling.</li>\n<li>Ordered, graceful deletion and termination.</li>\n<li>Ordered, automated rolling updates.</li>\n<li>Automated creation of storage volumes.</li>\n<li>Stable guaranteed storage.</li>\n</ol><p>Like a Deployment, a StatefulSet manages Pods that are based on an identical\ncontainer spec. Unlike a Deployment, a StatefulSet maintains a sticky identity\nfor each of their Pods. These pods are created from the same spec, but are not\ninterchangeable: each has a persistent identifier that it maintains across any\nrescheduling.</p>\n<p>Cassandra utilizes all of these features in order to run on Kubernetes Engine.</p>\n<p>Find more information <a href=\"https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/\" rel=\"nofollow\">here</a>.</p>\n<h4><a id=\"user-content-persistent-storage-with-persistent-volumes\" class=\"anchor\" aria-hidden=\"true\" href=\"#persistent-storage-with-persistent-volumes\"></a>Persistent Storage with Persistent Volumes</h4>\n<p>With a stateful datasource storage is required.</p>\n<p>A PersistentVolume (PV) is a piece of storage in the cluster that has been\nprovisioned by an administrator, or automatically by a Stateful Set. It is a\nresource in the cluster just like a node is a cluster resource. PVs are volume\nplugins like Volumes, but have a lifecycle independent of any individual pod\nthat uses the PV.</p>\n<p>A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar\nto a pod. Pods consume node resources and PVCs consume PV resources. Pods can\nrequest specific levels of resources (CPU and Memory). Claims can request\nspecific size and access modes (e.g., can be mounted once read/write or many\ntimes read-only).</p>\n<p>Find more information <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/\" rel=\"nofollow\">here</a>.</p>\n<h4><a id=\"user-content-pod-disruptions-budgets\" class=\"anchor\" aria-hidden=\"true\" href=\"#pod-disruptions-budgets\"></a>Pod Disruptions Budgets</h4>\n<p>An Application Owner can create a PodDisruptionBudget object (PDB) for each\napplication. A PDB limits the number pods of a replicated application that are\ndown simultaneously from voluntary disruptions. For example, with Cassandra\nwe need to ensure that the number of replicas running is never brought below\nthe number needed for a quorum.</p>\n<p>Find more information visit <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/\" rel=\"nofollow\">here</a>.</p>\n<h4><a id=\"user-content-pod-scheduling\" class=\"anchor\" aria-hidden=\"true\" href=\"#pod-scheduling\"></a>Pod Scheduling</h4>\n<p>This proof of concept utilizes advanced scheduling for pod placement.  Both\nscheduling anti-affinity rules and taints are used to instruct the K8s scheduler\nwhere to launch C* pods.</p>\n<p>Anti-Affinity rules instruct the scheduler to use preferred or required pod\nplacement. Preferred rules provide a weight value that impacts the scheduling\nalgorithm to not schedule C* pods on the same node.  While required rules force\nthe scheduler to not schedule C* pods on the same node.  Not having multiple\nCassandra pods on the same node increases fault tolerance, as when you lose a\nnode you only lose one pod.  But then you need to have an excess of nodes because\na pod will not reschedule.  Preferred does not provide the same level of high\navailability, but will allow pods to reschedule on existing nodes if headroom\nexists.</p>\n<p>Find more information visit <a href=\"https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity-beta-feature\" rel=\"nofollow\">here</a>.</p>\n<p>Taints prevent pods to be scheduled on a node, unless the pod's manifest\ncontains the required toleration. The typical use case for this is to target\na specific node pool for say Cassandra pods.  For instance, often larger machine\ntypes are required for C*, and adding taints to those nodes ensure that\nCassandra pods will only be scheduled on the nodes.</p>\n<p>Find more information visit <a href=\"https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\" rel=\"nofollow\">here</a>.</p>\n<h4><a id=\"user-content-dns-and-headless-services\" class=\"anchor\" aria-hidden=\"true\" href=\"#dns-and-headless-services\"></a>DNS and Headless Services</h4>\n<p>The Cassandra ring that is installed as part of this demo is named \"cassandra\".\nThe name of the service \"cassandra\" creates the DNS name, and the tld is\n\"default.svc.cluster.local\", where \"default\" is the name of the namespace.  With\nan application you would connect to at least three of the nodes to provide an HA\nconnection with a client.  The name of the nodes, for example would be:</p>\n<pre>cassandra-0.cassandra.default.svc.cluster.local\ncassandra-1.cassandra.default.svc.cluster.local\ncassandra-1.cassandra.default.svc.cluster.local\n</pre>\n<p>For more information visit <a href=\"https://kubernetes.io/docs/concepts/services-networking/service/#headless-services\" rel=\"nofollow\">here</a>.</p>\n<h3><a id=\"user-content-cassandra-container\" class=\"anchor\" aria-hidden=\"true\" href=\"#cassandra-container\"></a>Cassandra Container</h3>\n<p>Within this demo we have included a container that is specifically built for\nrunning on Kubernetes.  Features:</p>\n<ol><li>OSS maintained base container that is used by K8s OSS project</li>\n<li>Cassandra is installed from a tarball</li>\n<li>Container size has been reduced</li>\n<li>A readiness and liveness probes are built-in</li>\n<li>Prometheus is installed, but the port is not exposed in the manifest by\ndefault</li>\n<li>dumb-init is used to handle signal processing</li>\n<li>A logging encoder is provided for JSON logging</li>\n<li>Datacenter configuration</li>\n<li>JMX configuration</li>\n<li>Multiple ENV variables are exposed for manifest based configuration</li>\n</ol><p>For more information about the ENV variables that have been exposed see\n<a href=\"https://github.com/GoogleCloudPlatform/gke-stateful-applications-demo/blob/master/contain/README.md\">container/README.md</a>,\n<a href=\"https://github.com/GoogleCloudPlatform/gke-stateful-applications-demo/blob/master/container/files/run.sh\">container/files/run.sh</a> and also the\n<a href=\"https://github.com/GoogleCloudPlatform/gke-stateful-applications-demo/blob/master/manifests/cassandra-statefulset.yaml\">manifest</a>.</p>\n<p>This container is hosted by Google Professional Services.</p>\n<h2><a id=\"user-content-prerequisites\" class=\"anchor\" aria-hidden=\"true\" href=\"#prerequisites\"></a>Prerequisites</h2>\n<p>A Google Cloud account and project is required for this demo. The project must\nhave the proper quota to run a Kubernetes Engine cluster with at least\n3 n1-standard-4 and 3 n1-standard-1 nodes.  Additionally, the project must have\nat least 30 Compute Engine API CPUs and 12 Compute Engine API In-use IP Addresses.</p>\n<p>Access to an existing Google Cloud project with the Kubernetes Engine service enabled\nIf you do not have a Google Cloud account please signup for a free trial\n<a href=\"https://cloud.google.com\" rel=\"nofollow\">here</a>.</p>\n<p>How to check your account's quota is documented here: <a href=\"https://cloud.google.com/compute/quotas\" rel=\"nofollow\">quotas</a>.</p>\n<h3><a id=\"user-content-run-demo-in-a-google-cloud-shell\" class=\"anchor\" aria-hidden=\"true\" href=\"#run-demo-in-a-google-cloud-shell\"></a>Run Demo in a Google Cloud Shell</h3>\n<p>Click the button below to run the demo in a <a href=\"https://cloud.google.com/shell/docs/\" rel=\"nofollow\">Google Cloud Shell</a>.</p>\n<p><a href=\"https://console.cloud.google.com/cloudshell/open?git_repo=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fgke-stateful-applications-demo&amp;page=editor&amp;tutorial=README.md\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/acf32864fdae185325f992b48fb2132badef1e9a/687474703a2f2f677374617469632e636f6d2f636c6f75647373682f696d616765732f6f70656e2d62746e2e737667\" alt=\"Open in Cloud Shell\" data-canonical-src=\"http://gstatic.com/cloudssh/images/open-btn.svg\" /></a></p>\n<p>All the tools for the demo are installed. When using Cloud Shell execute the following\ncommand in order to setup gcloud cli. When executing this command please setup your region\nand zone.</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>gcloud init</pre></div>\n<h3><a id=\"user-content-supported-operating-systems\" class=\"anchor\" aria-hidden=\"true\" href=\"#supported-operating-systems\"></a>Supported Operating Systems</h3>\n<p>This project will run on macOS, or in a <a href=\"https://cloud.google.com/shell/docs/\" rel=\"nofollow\">Google Cloud Shell</a>.</p>\n<h3><a id=\"user-content-tools\" class=\"anchor\" aria-hidden=\"true\" href=\"#tools\"></a>Tools</h3>\n<p>When not using Cloud Shell, the following tools are required.</p>\n<ol><li><a href=\"https://cloud.google.com/sdk/docs/downloads-versioned-archives\" rel=\"nofollow\">Google Cloud SDK version &gt;= 204.0.0</a></li>\n<li><a href=\"https://cloud.google.com/sdk/gcloud/\" rel=\"nofollow\">gcloud cli</a></li>\n<li><a href=\"https://kubernetes.io/docs/tasks/tools/install-kubectl/\" rel=\"nofollow\">kubectl matching the latest GKE version</a></li>\n<li>docker (used to build container, you can use hosted container)</li>\n</ol><p>More recent versions of all the tools may function, please feel free to file an\nissue if you encounter problems with newer versions.</p>\n<h4><a id=\"user-content-install-cloud-sdk\" class=\"anchor\" aria-hidden=\"true\" href=\"#install-cloud-sdk\"></a>Install Cloud SDK</h4>\n<p>The Google Cloud SDK is used to interact with your GCP resources.\n<a href=\"https://cloud.google.com/sdk/downloads\" rel=\"nofollow\">Installation instructions</a> for multiple platforms are available online.</p>\n<h4><a id=\"user-content-install-kubectl-cli\" class=\"anchor\" aria-hidden=\"true\" href=\"#install-kubectl-cli\"></a>Install kubectl CLI</h4>\n<p>The kubectl CLI is used to interteract with both Kubernetes Engine and kubernetes in general.\n<a href=\"https://cloud.google.com/kubernetes-engine/docs/quickstart\" rel=\"nofollow\">Installation instructions</a>\nfor multiple platforms are available online.</p>\n<h3><a id=\"user-content-configure-authentication\" class=\"anchor\" aria-hidden=\"true\" href=\"#configure-authentication\"></a>Configure Authentication</h3>\n<p>The script will execute against your GCP environment and it will use your personal account to build out these resources.  To setup the default account the script will use, run the following command to select the appropriate account:</p>\n<p><code>gcloud auth login</code></p>\n<h2><a id=\"user-content-deployment-steps\" class=\"anchor\" aria-hidden=\"true\" href=\"#deployment-steps\"></a>Deployment Steps</h2>\n<p>To deploy the demo execute the following commands.</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>git clone https://github.com/GoogleCloudPlatform/gke-stateful-applications-demo\ncd gke-stateful-applications-demo\n./create.sh -c my-cluster-1</pre></div>\n<p>Replace the text 'my-cluster-1' the name of the cluster that you would like to\ncreate.</p>\n<p>The create script will output the following message when complete.</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>NAME                 MACHINE_TYPE   DISK_SIZE_GB  NODE_VERSION\nnodepool-cassdemo-2  n1-standard-1  100           1.10.2-gke.3</pre></div>\n<p>The script will:</p>\n<ol><li>create a new Kubernetes Engine cluster in your default ZONE, VPC and network.</li>\n<li>install multiple Kubernetes manifests that can be found in the\n<a href=\"https://github.com/GoogleCloudPlatform/gke-stateful-applications-demo/blob/master/manifests\">manifests</a> directory.</li>\n</ol><p>Because we are creating a cluster of 6 nodes, the cluster may go into a\n'RECONCILING' status as the control plane's instance size may be increased.</p>\n<p>Use the following command to view the current status of the cluster.</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>gcloud container clusters list</pre></div>\n<p>An example of the output while the cluster is reconciling.</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>NAME          LOCATION       MASTER_VERSION  MASTER_IP      MACHINE_TYPE   NODE_VERSION  NUM_NODES  STATUS\nmy-cluster-1  us-central1-a  1.10.2-gke.3    35.184.70.165  n1-standard-4  1.10.2-gke.3  6          RECONCILING</pre></div>\n<p>The status will change to RUNNING once the masters have been updated.</p>\n<h2><a id=\"user-content-validation\" class=\"anchor\" aria-hidden=\"true\" href=\"#validation\"></a>Validation</h2>\n<p>The following script will validate that the demo is deployed correctly:</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>./validate.sh -c my-cluster-1</pre></div>\n<p>Replace the text 'my-cluster-1' the name of the cluster that you would like to\nvalidate.</p>\n<p>The validation script uses <code>kubectl rollout status</code> to test if the rollout is\ncomplete. If the cluster is in 'RECONCILING' state this script will fail as\nwell.</p>\n<p>If the script fails it will output:</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>Validation Failed: Statefulset has not been deployed</pre></div>\n<p>If the script passes if will output:</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>Validation Passed: the Statefulset has been deployed</pre></div>\n<h3><a id=\"user-content-using-cassandara\" class=\"anchor\" aria-hidden=\"true\" href=\"#using-cassandara\"></a>Using Cassandara</h3>\n<p>These commands exercise the Cassandra cluster.</p>\n<h3><a id=\"user-content-launch-a-cassandra-container\" class=\"anchor\" aria-hidden=\"true\" href=\"#launch-a-cassandra-container\"></a>Launch a Cassandra container</h3>\n<p>These <code>kubectl</code> commands launch a K8s deployment, wait for the deployment,\nand exec into the shell.</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>kubectl run cass-dev --image gcr.io/pso-examples/cassandra:3.11.3-cqlsh-v22 --command -- /bin/bash -c \"tail -f /dev/null\"\nkubectl rollout status deployment cass-dev\nkubectl exec $(kubectl get po --no-headers | grep cass-dev | awk '{print $1}') -it -- /bin/bash</pre></div>\n<p>This will launch a bash prompt for the next steps.</p>\n<h3><a id=\"user-content-connect-to-the-ring-with-cqlsh\" class=\"anchor\" aria-hidden=\"true\" href=\"#connect-to-the-ring-with-cqlsh\"></a>Connect to the ring with cqlsh</h3>\n<h4><a id=\"user-content-cqlsh-command\" class=\"anchor\" aria-hidden=\"true\" href=\"#cqlsh-command\"></a>cqlsh command</h4>\n<div class=\"highlight highlight-text-shell-session\"><pre>/usr/local/apache-cassandra/bin/cqlsh cassandra-0.cassandra.default.svc.cluster.local</pre></div>\n<p>You will now be using the interactive <code>cqlsh</code> shell.</p>\n<p>The output of the command:</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>Connected to K8Demo at cassandra-0.cassandra.default.svc.cluster.local:9042.\n[cqlsh 5.0.1 | Cassandra 3.11.3 | CQL spec 3.4.4 | Native protocol v4]\nUse HELP for help.\ncqlsh&gt;</pre></div>\n<h3><a id=\"user-content-create-a-keyspace\" class=\"anchor\" aria-hidden=\"true\" href=\"#create-a-keyspace\"></a>Create a keyspace</h3>\n<div class=\"highlight highlight-text-shell-session\"><pre>CREATE KEYSPACE greek_monsters WITH REPLICATION = { 'class' : 'SimpleStrategy' , 'replication_factor' : 3 };</pre></div>\n<p>The user prompt is shown if the command is successful.</p>\n<h3><a id=\"user-content-describe-the-keyspace\" class=\"anchor\" aria-hidden=\"true\" href=\"#describe-the-keyspace\"></a>Describe the keyspace</h3>\n<div class=\"highlight highlight-text-shell-session\"><pre>DESC greek_monsters;</pre></div>\n<p>Example output:</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>CREATE KEYSPACE greek_monsters WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '3'}  AND durable_writes = true;\ncqlsh&gt;</pre></div>\n<h3><a id=\"user-content-use-the-new-keyspace\" class=\"anchor\" aria-hidden=\"true\" href=\"#use-the-new-keyspace\"></a>Use the new keyspace</h3>\n<div class=\"highlight highlight-text-shell-session\"><pre>USE greek_monsters;</pre></div>\n<p>The prompt will now include the keyspace that is selected.</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>cqlsh:greek_monsters&gt;</pre></div>\n<h3><a id=\"user-content-create-a-table\" class=\"anchor\" aria-hidden=\"true\" href=\"#create-a-table\"></a>Create a table</h3>\n<div class=\"highlight highlight-text-shell-session\"><pre>CREATE TABLE monsters (pet_id timeuuid, name text, description text, PRIMARY KEY (pet_id));</pre></div>\n<p>The user prompt is shown if the command is successful.</p>\n<h3><a id=\"user-content-see-the-newly-created-table\" class=\"anchor\" aria-hidden=\"true\" href=\"#see-the-newly-created-table\"></a>See the newly created table</h3>\n<div class=\"highlight highlight-text-shell-session\"><pre>DESCRIBE TABLES;</pre></div>\n<p>This command will output:</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>monsters\ncqlsh:greek_monsters&gt;</pre></div>\n<h3><a id=\"user-content-insert-data-into-the-table\" class=\"anchor\" aria-hidden=\"true\" href=\"#insert-data-into-the-table\"></a>Insert data into the table</h3>\n<div class=\"highlight highlight-text-shell-session\"><pre>INSERT INTO monsters (pet_id,name,description) VALUES (now(), 'Cerberus (Hellhound)','The three-headed giant hound, that guarded the gates of the Underworld.');\nINSERT INTO monsters (pet_id,name,description) VALUES (now(), 'Orthrus','A two-headed dog, brother of Cerberus, slain by Heracles.');\nINSERT INTO monsters (pet_id,name,description) VALUES (now(), 'Argos','Odysseus faithful dog, known for his speed, strength and his superior tracking skills.');\nINSERT INTO monsters (pet_id,name,description) VALUES (now(), 'Golden Dog','A dog which guarded the infant god Zeus.');\nINSERT INTO monsters (pet_id,name,description) VALUES (now(), 'Guard Dog of Hephaestus Temple','The temple of Hephaestus at Mount Etna was guarded by a pack of sacred dogs.');\nINSERT INTO monsters (pet_id,name,description) VALUES (now(), 'Laelaps',' female dog destined always to catch its prey.');\nINSERT INTO monsters (pet_id,name,description) VALUES (now(), 'Maera','The hound of Erigone, daughter of Icarius of Athens.');</pre></div>\n<p>The user prompt is shown if the command is successful.</p>\n<h3><a id=\"user-content-select-the-data-from-the-table\" class=\"anchor\" aria-hidden=\"true\" href=\"#select-the-data-from-the-table\"></a>Select the data from the table</h3>\n<div class=\"highlight highlight-text-shell-session\"><pre>SELECT * from monsters ;</pre></div>\n<p>Example output:</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>pet_id                               | description                                                                            | name\n--------------------------------------+----------------------------------------------------------------------------------------+--------------------------------\nca3d9f20-6a89-11e8-a0dd-114bb9e30b07 | Odysseus faithful dog, known for his speed, strength and his superior tracking skills. |                          Argos\nca3c3f90-6a89-11e8-a0dd-114bb9e30b07 |                              A two-headed dog, brother of Cerberus, slain by Heracles. |                        Orthrus\nca42f650-6a89-11e8-a0dd-114bb9e30b07 |                                   The hound of Erigone, daughter of Icarius of Athens. |                          Maera\nca3f4cd0-6a89-11e8-a0dd-114bb9e30b07 |                                               A dog which guarded the infant god Zeus. |                     Golden Dog\nca41bdd0-6a89-11e8-a0dd-114bb9e30b07 |                                          female dog destined always to catch its prey. |                        Laelaps\nca40ac60-6a89-11e8-a0dd-114bb9e30b07 |           The temple of Hephaestus at Mount Etna was guarded by a pack of sacred dogs. | Guard Dog of Hephaestus Temple\nca3a6ad0-6a89-11e8-a0dd-114bb9e30b07 |                The three-headed giant hound, that guarded the gates of the Underworld. |           Cerberus (Hellhound)</pre></div>\n<h3><a id=\"user-content-exit\" class=\"anchor\" aria-hidden=\"true\" href=\"#exit\"></a>Exit</h3>\n<p>Exit the pod with the following commands:</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>exit\nexit</pre></div>\n<p>This will return you to your command line.</p>\n<h4><a id=\"user-content-delete-the-deployment\" class=\"anchor\" aria-hidden=\"true\" href=\"#delete-the-deployment\"></a>Delete the deployment</h4>\n<p>Execute the following command to remove the deployment.</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>kubectl delete deployment cass-dev</pre></div>\n<p>The following message is displayed:</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>deployment \"cass-dev\" deleted</pre></div>\n<h2><a id=\"user-content-tear-down\" class=\"anchor\" aria-hidden=\"true\" href=\"#tear-down\"></a>Tear Down</h2>\n<p>The following script will tear down the Cassandra cluster and remove the Kubernetes Engine\ncluster.</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>./delete.sh -c my-cluster-1</pre></div>\n<p>This output will change depending on the cluster name.  In this example the name\n\"my-cluster-1\" was used.</p>\n<p>Replace the text 'my-cluster-1' the name of the cluster that you would like to\nvalidate.  The last lines of the output will be:</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>persistentvolumeclaim \"cassandra-data-cassandra-0\" deleted\npersistentvolumeclaim \"cassandra-data-cassandra-1\" deleted\npersistentvolumeclaim \"cassandra-data-cassandra-2\" deleted\nDeleting cluster</pre></div>\n<p>The tear down script removes all of the components in the manifests directory,\nand it also destroys the cluster.  This script also waits 60 seconds before\nit removes the PVC storage components.</p>\n<h2><a id=\"user-content-troubleshooting\" class=\"anchor\" aria-hidden=\"true\" href=\"#troubleshooting\"></a>Troubleshooting</h2>\n<ol><li>Since we are creating two nodepools in this demo, the cluster may upgrade\nthe control plane and it may go to a \"RECONCILING\" state.  Give the cluster\nsome time and it will scale.  The validate script will fail during this time.</li>\n<li>Run \"gcloud container clusters list\" command to check the cluster status.\nCluster</li>\n<li>If you get errors about quotas, please increase your quota in the project.\nSee <a href=\"https://cloud.google.com/compute/quotas\" rel=\"nofollow\">here</a> for more details.</li>\n<li>A great diagnostic command to run is simply <code>kubectl get pods</code>.  It will show\nthe running pods.</li>\n</ol><p>Initially, the cluster may show as \"RUNNING\" but then go into a \"RECONCILING\" state\nThe symptom will be timeouts when running <code>kubectl get pods</code>\nUse the \"gcloud container clusters list\" command to check the latest state, and wait until it changes back to \"RUNNING\"</p>\n<h2><a id=\"user-content-relevant-materials\" class=\"anchor\" aria-hidden=\"true\" href=\"#relevant-materials\"></a>Relevant Materials</h2>\n<ol><li><a href=\"https://kubernetes.io/blog/2016/07/thousand-instances-of-cassandra-using-kubernetes-pet-set\" rel=\"nofollow\">Thousand Instances of Cassandra</a></li>\n<li><a href=\"https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/\" rel=\"nofollow\">Stateful Sets</a></li>\n<li><a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/\" rel=\"nofollow\">Persistent Volumes</a></li>\n<li><a href=\"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/\" rel=\"nofollow\">Pod Disruption Budgets</a></li>\n<li><a href=\"https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity-beta-feature\" rel=\"nofollow\">Pod Scheduling</a></li>\n<li><a href=\"https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\" rel=\"nofollow\">Taints and Tolerations</a></li>\n<li><a href=\"https://kubernetes.io/docs/concepts/services-networking/service/#headless-services\" rel=\"nofollow\">Headless Services</a></li>\n<li><a href=\"https://cloud.google.com/compute/quotas\" rel=\"nofollow\">Google Cloud Quotas</a></li>\n<li><a href=\"https://cloud.google.com\" rel=\"nofollow\">Signup for Google Cloud</a></li>\n<li><a href=\"https://cloud.google.com/shell/docs/\" rel=\"nofollow\">Google Cloud Shell</a></li>\n</ol><p><strong>This is not an officially supported Google product</strong></p>\n</article>",
        "created_at": "2019-01-15T15:23:15+0000",
        "updated_at": "2019-01-15T15:23:15+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 13,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/2810941?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12922"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 902,
            "label": "kubernetes",
            "slug": "kubernetes"
          }
        ],
        "is_public": false,
        "id": 12921,
        "uid": null,
        "title": "deploy-a-scalable-apache-cassandra-database-on-kubernetes",
        "url": "https://developer.ibm.com/patterns/deploy-a-scalable-apache-cassandra-database-on-kubernetes/",
        "content": null,
        "created_at": "2019-01-15T15:23:14+0000",
        "updated_at": "2019-01-15T15:23:14+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": null,
        "language": null,
        "reading_time": 0,
        "domain_name": "developer.ibm.com",
        "preview_picture": null,
        "http_status": null,
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12921"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 996,
            "label": "monitoring",
            "slug": "monitoring"
          },
          {
            "id": 1025,
            "label": "grafana",
            "slug": "grafana"
          },
          {
            "id": 1108,
            "label": "graphite",
            "slug": "graphite"
          }
        ],
        "is_public": false,
        "id": 12908,
        "uid": null,
        "title": "monitoring-apache-cassandra-metrics-graphite-grafana",
        "url": "https://blog.pythian.com/monitoring-apache-cassandra-metrics-graphite-grafana/",
        "content": null,
        "created_at": "2019-01-10T17:50:00+0000",
        "updated_at": "2019-01-10T17:50:20+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": null,
        "language": null,
        "reading_time": 0,
        "domain_name": "blog.pythian.com",
        "preview_picture": null,
        "http_status": null,
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12908"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 10,
            "label": "api",
            "slug": "api"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 235,
            "label": "rest",
            "slug": "rest"
          }
        ],
        "is_public": false,
        "id": 12900,
        "uid": null,
        "title": "brianmhess/ambien",
        "url": "https://github.com/brianmhess/ambien",
        "content": "<p>It gives you rest...</p><h2>Building</h2><p><code>mvn clean package</code></p><h2>Running</h2><p><code>java -jar target/ambien-0.1-SNAPSHOT-jar-with-dependencies.jar &lt;options&gt;</code></p><h2>Usage:</h2><pre>version: 0.0.1\nUsage: ambien -host &lt;hostname&gt; -k &lt;keyspaceName&gt; -t &lt;tableName&gt; -o &lt;outputDir&gt; [options]\nOPTIONS:\n  -host &lt;hostname&gt;               Contact point for DSE [required]\n  -k &lt;keyspacename&gt;              Keyspace to use [required]\n  -t &lt;tablename&gt;                 Table to use [required]\n  -o &lt;outputDir&gt;                 Directory to write to (must be empty) [required]\n  -configFile &lt;filename&gt;         File with configuration options [none]\n  -port &lt;portNumber&gt;             CQL Port Number [9042]\n  -user &lt;username&gt;               Cassandra username [none]\n  -pw &lt;password&gt;                 Password for user [none]\n  -ssl-truststore-path &lt;path&gt;    Path to SSL truststore [none]\n  -ssl-truststore-pw &lt;pwd&gt;       Password for SSL truststore [none]\n  -ssl-keystore-path &lt;path&gt;      Path to SSL keystore [none]\n  -ssl-keystore-pw &lt;pwd&gt;         Password for SSL keystore [none]\n</pre><p>This will produce a directory of source code in the supplied directory.\nAfter running Ambien, change directory to the output directory and run:</p><p><code>mvn clean package</code></p><p>And then start the service with:</p><p><code>java -jar target/ambien-0.0.1-SNAPSHOT.jar</code></p><h2>Current API calls</h2><p>There is an index.html page which lists all the generated REST endpoints:</p><pre>http://hostname:8222/\n</pre><p>Print Hello World:</p><pre>http://hostname:8222/api/hello\n</pre><p>Select all rows (GET):</p><pre>http://hostname:8222/api/all\n</pre><p>Select some rows (GET and POST):</p><pre>http://hostname:8222/api/some/?some={some}\n</pre><p>Select by partition keys (GET and POST):</p><pre>http://hostname:8222/api/&lt;partitionKey1&gt;_&lt;partitionKey2&gt;_..._&lt;partitionKeyN&gt;/?partitionKey1={partitionKey1}&amp;partitionKey2={partitionKey2}&amp;...&amp;partitionKeyN={partitionKeyN}\n</pre><p>Select by partition keys and clustering key(s) (GET and POST):</p><pre>http://hostname:8222/api/&lt;pkey1&gt;_&lt;ccol1&gt;?pkey={pkey1}&amp;ccol1={ccol1}\nhttp://hostname:8222/api/&lt;pkey1&gt;_&lt;ccol1&gt;_&lt;ccol2&gt;?pkey1={pkey1}&amp;ccol1={ccol1}&amp;ccol2={ccol2}\n...\n</pre><p>Select by partition key and inequality on clustering key(s) (GET and POST):</p><pre>http://hostname:8222/api/&lt;pkey1&gt;_&lt;ccol1&gt;_lt?pkey1={pkey1}&amp;ccol1={ccol1}\nhttp://hostname:8222/api/&lt;pkey1&gt;_&lt;ccol1&gt;_gt?pkey1={pkey1}&amp;ccol1={ccol1}\nhttp://hostname:8222/api/&lt;pkey1&gt;_&lt;ccol1&gt;_lte?pkey1={pkey1}&amp;ccol1={ccol1}\nhttp://hostname:8222/api/&lt;pkey1&gt;_&lt;ccol1&gt;_gte?pkey1={pkey1}&amp;ccol1={ccol1}\nhttp://hostname:8222/api/&lt;pkey1&gt;_&lt;ccol1&gt;_&lt;ccol2&gt;_lt?pkey1={pkey1}&amp;ccol1={ccol1}&amp;ccol2={ccol2}\nhttp://hostname:8222/api/&lt;pkey1&gt;_&lt;ccol1&gt;_&lt;ccol2&gt;_gt?pkey1={pkey1}&amp;ccol1={ccol1}&amp;ccol2={ccol2}\nhttp://hostname:8222/api/&lt;pkey1&gt;_&lt;ccol1&gt;_&lt;ccol2&gt;_lte?pkey1={pkey1}&amp;ccol1={ccol1}&amp;ccol2={ccol2}\nhttp://hostname:8222/api/&lt;pkey1&gt;_&lt;ccol1&gt;_&lt;ccol2&gt;_gte?pkey1={pkey1}&amp;ccol1={ccol1}&amp;ccol2={ccol2}\n...\n</pre><h2>Spring Actuator</h2><p>You can also access various metrics from the Actuator endpoints:</p><pre>http://hostname:8222/actuator\n</pre>",
        "created_at": "2019-01-09T17:40:18+0000",
        "updated_at": "2019-01-15T15:23:17+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "github.com",
        "preview_picture": "https://avatars3.githubusercontent.com/u/7047840?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12900"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 200,
            "label": "python",
            "slug": "python"
          }
        ],
        "is_public": false,
        "id": 12898,
        "uid": null,
        "title": "r4fek/django-cassandra-engine",
        "url": "https://github.com/r4fek/django-cassandra-engine",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>All tools you need to start your journey with Apache Cassandra and Django Framework!</p>\n<p><a href=\"https://pypi.python.org/pypi/django-cassandra-engine/\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/8495a4ee11dba35b37e1fc2083c6d2e4177ca38c/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f646a616e676f2d63617373616e6472612d656e67696e652e737667\" alt=\"Latest version\" title=\"Latest version\" data-canonical-src=\"https://img.shields.io/pypi/v/django-cassandra-engine.svg\" /></a>\n<a href=\"https://travis-ci.org/r4fek/django-cassandra-engine\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/8e8d5e70778cc5806fd34aed537293e72f8eee8c/68747470733a2f2f6170692e7472617669732d63692e6f72672f723466656b2f646a616e676f2d63617373616e6472612d656e67696e652e7376673f6272616e63683d6d6173746572\" alt=\"CI\" title=\"CI\" data-canonical-src=\"https://api.travis-ci.org/r4fek/django-cassandra-engine.svg?branch=master\" /></a>\n<a href=\"https://www.paypal.me/rrafek/5\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/d5d24e33e2f4b6fe53987419a21b203c03789a8f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f6e6174652d50617950616c2d677265656e2e737667\" alt=\"Donate\" data-canonical-src=\"https://img.shields.io/badge/Donate-PayPal-green.svg\" /></a></p>\n<h2><a id=\"user-content-features\" class=\"anchor\" aria-hidden=\"true\" href=\"#features\"></a>Features</h2>\n<ul><li>integration with latest <code>python-driver</code> and optionally <code>dse-driver</code> from DataStax</li>\n<li>working <code>flush</code>, <code>migrate</code>, <code>sync_cassandra</code>, <code>inspectdb</code> and\n<code>dbshell</code> commands</li>\n<li>support for creating/destroying test database</li>\n<li>accepts all <code>Cqlengine</code> and <code>cassandra.cluster.Cluster</code> connection options</li>\n<li>automatic connection/disconnection handling</li>\n<li>works well along with relational databases (as secondary DB)</li>\n<li>storing sessions in Cassandra</li>\n<li>working django forms</li>\n<li>usable admin panel with Cassandra models</li>\n</ul><h2><a id=\"user-content-plans-todo\" class=\"anchor\" aria-hidden=\"true\" href=\"#plans-todo\"></a>Plans (TODO)</h2>\n<ul><li>User model stored in Cassandra (auth module)</li>\n</ul><h2><a id=\"user-content-installation\" class=\"anchor\" aria-hidden=\"true\" href=\"#installation\"></a>Installation</h2>\n<p>Recommended installation:</p>\n<pre>pip install django-cassandra-engine\n</pre>\n<h2><a id=\"user-content-basic-usage\" class=\"anchor\" aria-hidden=\"true\" href=\"#basic-usage\"></a>Basic Usage</h2>\n<ol><li>\n<p>Add <code>django_cassandra_engine</code> to <code>INSTALLED_APPS</code> in your <code>settings.py</code> file:</p>\n<pre> INSTALLED_APPS = ('django_cassandra_engine',) + INSTALLED_APPS\n</pre>\n</li>\n<li>\n<p>Change <code>DATABASES</code> setting:</p>\n<pre> DATABASES = {\n     'default': {\n         'ENGINE': 'django_cassandra_engine',\n         'NAME': 'db',\n         'TEST_NAME': 'test_db',\n         'HOST': 'db1.example.com,db2.example.com',\n         'OPTIONS': {\n             'replication': {\n                 'strategy_class': 'SimpleStrategy',\n                 'replication_factor': 1\n             }\n         }\n     }\n }\n</pre>\n</li>\n<li>\n<p>Define some model:</p>\n<pre> # myapp/models.py\n import uuid\n from cassandra.cqlengine import columns\n from django_cassandra_engine.models import DjangoCassandraModel\n class ExampleModel(DjangoCassandraModel):\n     example_id    = columns.UUID(primary_key=True, default=uuid.uuid4)\n     example_type  = columns.Integer(index=True)\n     created_at    = columns.DateTime()\n     description   = columns.Text(required=False)\n</pre>\n</li>\n<li>\n<p>Run <code>./manage.py sync_cassandra</code></p>\n</li>\n<li>\n<p>Done!</p>\n</li>\n</ol><h2><a id=\"user-content-documentation\" class=\"anchor\" aria-hidden=\"true\" href=\"#documentation\"></a>Documentation</h2>\n<p>The documentation can be found online <a href=\"http://r4fek.github.io/django-cassandra-engine/\" rel=\"nofollow\">here</a>.</p>\n<h2><a id=\"user-content-donation\" class=\"anchor\" aria-hidden=\"true\" href=\"#donation\"></a>Donation</h2>\n<p>If this project help you reduce time to develop, you can give me a cup of coffee :)</p>\n<p><a href=\"https://www.paypal.me/rrafek/5\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/d5d24e33e2f4b6fe53987419a21b203c03789a8f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f6e6174652d50617950616c2d677265656e2e737667\" alt=\"Donate\" data-canonical-src=\"https://img.shields.io/badge/Donate-PayPal-green.svg\" /></a></p>\n<h2><a id=\"user-content-license\" class=\"anchor\" aria-hidden=\"true\" href=\"#license\"></a>License</h2>\n<p>Copyright (c) 2014-2018, <a href=\"https://rafal-furmanski.com\" rel=\"nofollow\">Rafał Furmański</a>.</p>\n<p>All rights reserved. Licensed under BSD 2-Clause License.</p>\n</article>",
        "created_at": "2019-01-08T23:58:56+0000",
        "updated_at": "2019-01-08T23:59:06+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://avatars3.githubusercontent.com/u/1292373?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12898"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 35,
            "label": "docker",
            "slug": "docker"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12897,
        "uid": null,
        "title": "pokle/cassandra",
        "url": "https://github.com/pokle/cassandra",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>This is a collection of images and scripts to help you run Cassandra in Docker containers.\nThese images are great to provision ephemeral Cassandra topologies for testing and development purpose.</p>\n<ul><li>Currently supported:\n<ul><li>A single Cassandra node</li>\n<li>A client container to run tools such as cqlsh, nodetool, etc.</li>\n<li>A multi-node cluster - running on a single Docker host</li>\n<li>Monitored cluster using OpsCenter</li>\n</ul></li>\n</ul><p>If you'd like to help, please get in touch with me, and/or send me pull requests.</p>\n<h2><a id=\"user-content-prerequisites\" class=\"anchor\" aria-hidden=\"true\" href=\"#prerequisites\"></a>Prerequisites</h2>\n<ul><li>\n<p>A recent version of Docker - See <a href=\"https://www.docker.com\" rel=\"nofollow\">https://www.docker.com</a></p>\n</li>\n<li>\n<p>Verify that the docker command works. Try running 'docker ps' for example.</p>\n</li>\n<li>\n<p>Build the cassandra and opscenter images (optional)</p>\n<pre>  ./cassandra/build.sh\n  ./opscenter/build.sh\n</pre>\n</li>\n</ul><p>The last step is optional because Docker will automatically pull the images from <a href=\"https://index.docker.io\" rel=\"nofollow\">index.docker.io</a> if you don't already have them. The build process needs an Internet connection, but it is executed only once and then cached on Docker. If you modify the scripts, this is also how you can re-build the images with your changes.</p>\n<h2><a id=\"user-content-single-cassandra-node\" class=\"anchor\" aria-hidden=\"true\" href=\"#single-cassandra-node\"></a>Single Cassandra node</h2>\n<p>Here's how to start a Cassandra cluster with a single node, and run some CQL on it. These instructions use the docker command directly to demonstrate what's happening behind the scenes.</p>\n<ol><li>\n<p>Launch a container running Cassandra called cassone:</p>\n<pre> docker run --detach --name cassone poklet/cassandra\n</pre>\n</li>\n<li>\n<p>Connect to it using cqlsh</p>\n<pre> docker run -it --rm --net container:cassone poklet/cassandra cqlsh\n</pre>\n<p>You should see something like:</p>\n<pre> [cqlsh 5.0.1 | Cassandra 2.2.0 | CQL spec 3.3.0 | Native protocol v4]\n Use HELP for help.\n cqlsh&gt; quit\n</pre>\n<p>If not, then try it again in a few seconds - cassandra might still be starting up.</p>\n</li>\n<li>\n<p>Lets try some CQL</p>\n<p>Paste the following into your cqlsh prompt to create a test keyspace, and a test table:</p>\n<pre> CREATE KEYSPACE test_keyspace WITH REPLICATION = \n {'class': 'SimpleStrategy', 'replication_factor': 1};\n \n USE test_keyspace;\n \n CREATE TABLE test_table (\n   id text,\n   test_value text,\n   PRIMARY KEY (id)\n );\n \n INSERT INTO test_table (id, test_value) VALUES ('1', 'one');\n INSERT INTO test_table (id, test_value) VALUES ('2', 'two');\n INSERT INTO test_table (id, test_value) VALUES ('3', 'three');\n \n SELECT * FROM test_table;\n</pre>\n<p>If that worked, you should see:</p>\n<pre>  id | test_value\n ----+------------\n   3 |      three\n   2 |        two\n   1 |        one\n \n (3 rows)\n</pre>\n</li>\n</ol><h2><a id=\"user-content-3-node-cassandra-cluster\" class=\"anchor\" aria-hidden=\"true\" href=\"#3-node-cassandra-cluster\"></a>3-node Cassandra cluster</h2>\n<ol><li>\n<p>Launch three containers (one seed plus two more)</p>\n<pre> docker run -d --name cass1 poklet/cassandra start\n docker run -d --name cass2 --link cass1:seed poklet/cassandra start seed\n docker run -d --name cass3 --link cass1:seed poklet/cassandra start seed\n</pre>\n<p>Note: The poklet/cassandra docker image contains a shell script called <code>start</code> that takes an optional seed host. We use <code>--link cass1:seed</code> to name the cass1 host as our seed host.</p>\n</li>\n<li>\n<p>Run <code>nodetool status</code> on cass1 to check the cluster status:</p>\n<pre> docker run -it --rm --net container:cass1 poklet/cassandra nodetool status\n</pre>\n</li>\n<li>\n<p>Create some data on the first container:</p>\n<p>Launch <code>cqlsh</code>:</p>\n<pre> docker run -it --rm --net container:cass1 poklet/cassandra cqlsh\n</pre>\n<p>Paste this in:</p>\n<pre> create keyspace demo with replication = {'class':'SimpleStrategy', 'replication_factor':2};\n use demo;\n create table names ( id int primary key, name text );\n insert into names (id,name) values (1, 'gibberish');\n quit;\n</pre>\n</li>\n<li>\n<p>Connect to the second container, and check if it can see your data:</p>\n<p>Start up <code>cqlsh</code> (on cass2 this time):</p>\n<pre> docker run -it --rm --net container:cass2 poklet/cassandra cqlsh\n</pre>\n<p>Paste in:</p>\n<pre> select * from demo.names;\n</pre>\n<p>You should see:</p>\n<pre> cqlsh&gt; select * from demo.names;\n  id | name\n ----+-----------\n   1 | gibberish\n (1 rows)\n</pre>\n</li>\n</ol><h2><a id=\"user-content-10-node-cassandra-cluster-scripted\" class=\"anchor\" aria-hidden=\"true\" href=\"#10-node-cassandra-cluster-scripted\"></a>10-node Cassandra cluster (scripted!)</h2>\n<ol><li>\n<p>Right, lets dive right in with some shell scripts in the scripts directory to help us:</p>\n<pre> ./scripts/run.sh 10\n</pre>\n</li>\n<li>\n<p>That will start 10 nodes. Lets see what they're called:</p>\n<pre> ./scripts/ips.sh\n 172.17.0.10 cass6\n 172.17.0.12 cass4\n 172.17.0.11 cass5\n 172.17.0.6 cass10\n 172.17.0.7 cass9\n 172.17.0.9 cass7\n 172.17.0.8 cass8\n 172.17.0.4 cass2\n 172.17.0.3 cass3\n 172.17.0.2 cass1\n</pre>\n</li>\n<li>\n<p>Same, but with the nodetool:</p>\n<pre> ./scripts/nodetool.sh cass1 status\n Datacenter: datacenter1\n =======================\n Status=Up/Down\n |/ State=Normal/Leaving/Joining/Moving\n --  Address      Load       Tokens  Owns (effective)  Host ID                               Rack\n UN  172.17.0.11  74.19 KB   256     21.4%             dfd44ca5-bf73-4487-bcb2-db882d0a9231  rack1\n UN  172.17.0.10  74.21 KB   256     19.6%             f479a4e6-55ac-4533-8ce5-d137a93f2cc4  rack1\n UN  172.17.0.9   74.34 KB   256     20.4%             0bb389a0-f111-459c-9620-0faccc75cbc0  rack1\n UN  172.17.0.8   74.19 KB   256     20.1%             2eb4a4dd-2bbc-46a3-9f64-4e761509307d  rack1\n UN  172.17.0.12  74.14 KB   256     20.2%             a2547289-0c6a-458f-b982-823711c5293e  rack1\n UN  172.17.0.3   74.19 KB   256     20.3%             3667cc1a-1f63-4cd1-bebc-841f428a0f4d  rack1\n UN  172.17.0.2   74.24 KB   256     20.3%             2b48c8ac-ad68-48a0-9c41-c8f2fb7f38e6  rack1\n UN  172.17.0.7   67.7 KB    256     19.2%             e361f6d8-28ef-4cf8-baa1-88c2d1fec094  rack1\n UN  172.17.0.6   74.15 KB   256     19.6%             230f13b1-a27b-44e8-9b51-5ebdb1c4cb13  rack1\n UN  172.17.0.4   74.18 KB   256     18.8%             6c90cbaa-e5b3-41de-a160-3ecaf59b8856  rack1\n</pre>\n</li>\n<li>\n<p>When you're tired of your cluster, nuke it with:</p>\n<pre> ./scripts/nuke.sh 10\n</pre>\n</li>\n</ol><h2><a id=\"user-content-set-snitch-and-node-location\" class=\"anchor\" aria-hidden=\"true\" href=\"#set-snitch-and-node-location\"></a>Set snitch and node location</h2>\n<p>The snitch type and node location information can be configured with environment variables.\nThe datacenter and rack configuration is only valid if using the GossipingPropertyFileSnitch type snitch.\nFor example:</p>\n<pre>    docker run -d --name cass1 -e SNITCH=GossipingPropertyFileSnitch -e DC=SFO -e RACK=RAC3 poklet/cassandra\n</pre>\n<p>This will set the snitch type and set the datacenter to <strong>SFO</strong> and the rack to <strong>RAC3</strong></p>\n<h2><a id=\"user-content-auto-detect-seeds\" class=\"anchor\" aria-hidden=\"true\" href=\"#auto-detect-seeds\"></a>Auto-detect seeds</h2>\n<p>Any containers linked in the run command will also be added to the seed list.  The 3-node cluster example above may also be written as:</p>\n<pre>    docker run -d --name cass1 poklet/cassandra\n    docker run -d --name cass2 --link cass1:cass1 poklet/cassandra\n    docker run -d --name cass3 --link cass1:cass1 poklet/cassandra\n    # and so on...\n</pre>\n<h2><a id=\"user-content-specifying-clustering-parameters\" class=\"anchor\" aria-hidden=\"true\" href=\"#specifying-clustering-parameters\"></a>Specifying clustering parameters</h2>\n<p>When starting a container, you can pass the SEEDS, LISTEN_ADDRESS environment variables to override the defaults:</p>\n<pre>docker run -e SEEDS=a,b,c... -e LISTEN_ADDRESS=10.2.1.4 poklet/cassandra\n</pre>\n<p>Note that listen_address will also be used for broadcast_address</p>\n<h2><a id=\"user-content-cassandra-cluster--opscenter-monitoring\" class=\"anchor\" aria-hidden=\"true\" href=\"#cassandra-cluster--opscenter-monitoring\"></a>Cassandra cluster + OpsCenter monitoring</h2>\n<ol><li>\n<p>Start a Cassandra cluster with 3 nodes:</p>\n<pre> ./scripts/run.sh 3\n</pre>\n</li>\n<li>\n<p>Start the OpsCenter container:</p>\n<pre> docker run -d --name opscenter poklet/opscenter\n</pre>\n<p>You can also add the <code>-p 8888:8888</code> option to bind container's 8888 port to host's 8888 port</p>\n</li>\n<li>\n<p>Connect and configure OpsCenter:</p>\n<ul><li>Open a browser and connect to <a href=\"http://replace.me:8888\" rel=\"nofollow\">http://replace.me:8888</a> - replace the host by the result returned by <code>./scripts/ipof.sh opscenter</code>.</li>\n<li>Click on the \"Use Existing Cluster\" button and put at least the IP of one node in the cluster in the host text box. The result of <code>./scripts/ipof.sh cass1</code> is a good candidate. Click \"Save Cluster\" button. OpsCenter start gathering data from the cluster but you do not get full-set metrics yet.</li>\n<li>You should see a \"0 of 3 agents connected\" message on the top of the GUI. Click the \"Fix\" link aside.</li>\n<li>In the popup, click \"Enter Credentials\" link and fill form with username <code>opscenter</code> and password <code>opscenter</code>. Click \"Done\".</li>\n<li>Click \"Install on all nodes\" and then \"Accept Fingerprints\". OpsCenter installs agent on cluster'snodes remotly.</li>\n<li>Once done, you should see the \"All agents connected\" message.</li>\n</ul></li>\n</ol></article>",
        "created_at": "2019-01-08T23:55:37+0000",
        "updated_at": "2019-01-08T23:55:43+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 6,
        "domain_name": "github.com",
        "preview_picture": "https://avatars3.githubusercontent.com/u/99655?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12897"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 10,
            "label": "api",
            "slug": "api"
          },
          {
            "id": 44,
            "label": "json",
            "slug": "json"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12896,
        "uid": null,
        "title": "joaquincasares/CassandraJsonMapper",
        "url": "https://github.com/joaquincasares/CassandraJsonMapper",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>CassandraJsonMapper is an Apache Cassandra Python driver developed for direct\nfunctionality to store JSON-style objects mapped into Cassandra composite columns.</p>\n<p>This is done via the main methods: <code>save(key)</code>, <code>get(key)</code>, <code>delete(key)</code>.</p>\n<h2><a id=\"user-content-history\" class=\"anchor\" aria-hidden=\"true\" href=\"#history\"></a>History</h2>\n<p>I needed something small, light-weight, and fast enough on reads and writes for\nshort bursts of data. I was handling JSON returns from REST API calls that\nI would later do light-weight analytics over. This fit my use case efficiently,\nso I decided to share it.</p>\n<h2><a id=\"user-content-setup\" class=\"anchor\" aria-hidden=\"true\" href=\"#setup\"></a>Setup</h2>\n<pre>pip install CassandraJsonMapper\n</pre>\n<h2><a id=\"user-content-initialize-database\" class=\"anchor\" aria-hidden=\"true\" href=\"#initialize-database\"></a>Initialize Database</h2>\n<p>Since CassandraJsonMapper uses composite columns heavily to do it's nesting,\nthe schema for the column family that CassandraJsonMapper will use must look\nsimilar to what is provided.</p>\n<p><strong>NOTE:</strong>\nKeep in mind that the number of composite columns created must be at\nleast as deep as the deepest JSON document that will be saved.</p>\n<div class=\"highlight highlight-source-sql\"><pre>create keyspace json\n  with placement_strategy = 'SimpleStrategy'\n  and strategy_options = {replication_factor : 1};\nuse json;\ncreate column family json\n  with column_type = 'Standard'\n  and comparator = 'CompositeType(\n    org.apache.cassandra.db.marshal.BytesType,org.apache.cassandra.db.marshal.BytesType,\n    org.apache.cassandra.db.marshal.BytesType,org.apache.cassandra.db.marshal.BytesType,\n    org.apache.cassandra.db.marshal.BytesType,org.apache.cassandra.db.marshal.BytesType,\n    org.apache.cassandra.db.marshal.BytesType,org.apache.cassandra.db.marshal.BytesType,\n    org.apache.cassandra.db.marshal.BytesType,org.apache.cassandra.db.marshal.BytesType,\n    org.apache.cassandra.db.marshal.BytesType,org.apache.cassandra.db.marshal.BytesType,\n    org.apache.cassandra.db.marshal.BytesType,org.apache.cassandra.db.marshal.BytesType,\n    org.apache.cassandra.db.marshal.BytesType,org.apache.cassandra.db.marshal.BytesType,\n    org.apache.cassandra.db.marshal.BytesType,org.apache.cassandra.db.marshal.BytesType,\n    org.apache.cassandra.db.marshal.BytesType,org.apache.cassandra.db.marshal.BytesType,\n    org.apache.cassandra.db.marshal.BytesType,org.apache.cassandra.db.marshal.BytesType)'\n  and default_validation_class = 'BytesType'\n  and key_validation_class = 'BytesType';</pre></div>\n<h2><a id=\"user-content-example-code\" class=\"anchor\" aria-hidden=\"true\" href=\"#example-code\"></a>Example Code</h2>\n<div class=\"highlight highlight-source-python\"><pre>import CassandraJsonMapper\ndb = CassandraJsonMapper.db(keyspace='json', column_family='json')\ndocument = {\n    'key2': {\n        'test': 'okay'\n    }\n}\ndb.save(document)\nassert db.get('key2') == document['key2']\ndb.delete('key2')\nassert db.get('key2') == {}</pre></div>\n<h2><a id=\"user-content-multi-threaded-example\" class=\"anchor\" aria-hidden=\"true\" href=\"#multi-threaded-example\"></a>Multi-threaded Example</h2>\n<div class=\"highlight highlight-source-python\"><pre>db.mt_save(document)\ndb.mt_finish()\nassert db.get('key2') == document['key2']</pre></div>\n<h2><a id=\"user-content-limitations\" class=\"anchor\" aria-hidden=\"true\" href=\"#limitations\"></a>Limitations</h2>\n<p>In hopes of keeping this client simple, the following is not supported:</p>\n<ul><li>Cannot append to lists.</li>\n<li>Cannot delete portions of a document.</li>\n<li>Reads may not be returned in full, but paging is supported.</li>\n</ul><h2><a id=\"user-content-future-implementations\" class=\"anchor\" aria-hidden=\"true\" href=\"#future-implementations\"></a>Future Implementations</h2>\n<ul><li>Deleting portions of documents.</li>\n<li>Better list support.</li>\n</ul><h2><a id=\"user-content-methods\" class=\"anchor\" aria-hidden=\"true\" href=\"#methods\"></a>Methods</h2>\n<p>This client has a few main methods:</p>\n<div class=\"highlight highlight-source-python\"><pre>def __init__(self, keyspace, column_family, server_list=['localhost'],\n             write_consistency=ConsistencyLevel.ONE,\n             read_consistency=ConsistencyLevel.ONE,\n             request_size=100, batch_size=6000, thread_count=20)\n    \"\"\"Create the Cassandra connection pool and CF connection.\"\"\"\ndef save(self, dictionary_payload, write_consistency=None, batch_size=None)\n    \"\"\"Convert and save dictionary into Cassandra.\"\"\"\ndef get(self, key, read_consistency=None,\n        return_last_row=False, column_start=None, request_size=None)\n    \"\"\"Read and convert Cassandra response into dictionary.\"\"\"\ndef delete(self, key, write_consistency=None)\n    \"\"\"Delete dictionary from Cassandra.\"\"\"\ndef mt_save(self, dictionary_payload)\n    \"\"\"Save dictionary asynchronously.\"\"\"\ndef mt_finish(self)\n    \"\"\"Wait until all pending inserts are performed.\"\"\"</pre></div>\n</article>",
        "created_at": "2019-01-08T23:05:05+0000",
        "updated_at": "2019-01-08T23:05:12+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/545666?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12896"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 10,
            "label": "api",
            "slug": "api"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 235,
            "label": "rest",
            "slug": "rest"
          }
        ],
        "is_public": false,
        "id": 12895,
        "uid": null,
        "title": "rohitsakala/CassandraRestfulAPI",
        "url": "https://github.com/rohitsakala/CassandraRestfulAPI",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>=====================</p>\n<h2><a id=\"user-content-description\" class=\"anchor\" aria-hidden=\"true\" href=\"#description\"></a>Description</h2>\n<p><strong>CassandraRestfulAPI</strong> project exposes the cassandra data tables and nodes with the help of Restful API's. The project follows the standard Restful API rules. This project is developed as Major project of the Cloud Computing course by Team 15. The project is developed using Python Driver provided by Datastax using Flask framework.</p>\n<h2><a id=\"user-content-installation\" class=\"anchor\" aria-hidden=\"true\" href=\"#installation\"></a>Installation</h2>\n<h3><a id=\"user-content-flask\" class=\"anchor\" aria-hidden=\"true\" href=\"#flask\"></a>Flask</h3>\n<p><code>$ sudo pip install Flask</code></p>\n<h3><a id=\"user-content-cassandra\" class=\"anchor\" aria-hidden=\"true\" href=\"#cassandra\"></a>Cassandra</h3>\n<p>Follow these steps to install <a href=\"http://docs.datastax.com/en/cassandra/2.1/cassandra/install/installTarball_t.html\" rel=\"nofollow\">Cassandra</a></p>\n<h3><a id=\"user-content-python-cassandra-driver\" class=\"anchor\" aria-hidden=\"true\" href=\"#python-cassandra-driver\"></a>Python Cassandra Driver</h3>\n<p>Follow these steps to install python cassandra driver <a href=\"https://datastax.github.io/python-driver/installation.html\" rel=\"nofollow\">Python Driver Cassandra</a></p>\n<h3><a id=\"user-content-other-dependencies\" class=\"anchor\" aria-hidden=\"true\" href=\"#other-dependencies\"></a>Other Dependencies</h3>\n<p><code> sudo pip install flask-mongoengine </code>\n<code> sudo pip install -U flask_paginate </code></p>\n<h2><a id=\"user-content-run-the-code\" class=\"anchor\" aria-hidden=\"true\" href=\"#run-the-code\"></a>Run the code</h2>\n<p>First is to start cassandra. Second is to run flask i.e <code>python main.py</code>. Go to https:127:0.0.1:5000/ and use the following API's.</p>\n<h2><a id=\"user-content-apis\" class=\"anchor\" aria-hidden=\"true\" href=\"#apis\"></a>API's</h2>\n<h2><a id=\"user-content-----nodes----\" class=\"anchor\" aria-hidden=\"true\" href=\"#----nodes----\"></a>--- Nodes ---</h2>\n<h3><a id=\"user-content-list-all-the-nodes\" class=\"anchor\" aria-hidden=\"true\" href=\"#list-all-the-nodes\"></a>List all the nodes</h3>\n<p><code> [GET] <a href=\"http://127.0.0.1:5000/nodes/\" rel=\"nofollow\">http://127.0.0.1:5000/nodes/</a> </code></p>\n<h3><a id=\"user-content-get-info-of-a-node\" class=\"anchor\" aria-hidden=\"true\" href=\"#get-info-of-a-node\"></a>Get info of a node</h3>\n<p><code> [GET] <a href=\"http://127.0.0.1:5000/nodes/nodeid\" rel=\"nofollow\">http://127.0.0.1:5000/nodes/nodeid</a> </code></p>\n<h3><a id=\"user-content-creates-a-new-node\" class=\"anchor\" aria-hidden=\"true\" href=\"#creates-a-new-node\"></a>Creates a new node</h3>\n<p><code> [POST] <a href=\"http://127.0.0.1:5000/nodes/\" rel=\"nofollow\">http://127.0.0.1:5000/nodes/</a> </code></p>\n<h5><a id=\"user-content-body\" class=\"anchor\" aria-hidden=\"true\" href=\"#body\"></a>Body</h5>\n<pre>* { 'username' : username , 'password' : password, 'ip' : ip }\n</pre>\n<h3><a id=\"user-content-delete-a-node\" class=\"anchor\" aria-hidden=\"true\" href=\"#delete-a-node\"></a>Delete a node</h3>\n<p><code> [DELETE] <a href=\"http://127.0.0.1:5000/nodes/nodeid\" rel=\"nofollow\">http://127.0.0.1:5000/nodes/nodeid</a> </code></p>\n<h5><a id=\"user-content-body-1\" class=\"anchor\" aria-hidden=\"true\" href=\"#body-1\"></a>Body</h5>\n<pre>* { 'username' : username , 'password' : password }\n</pre>\n<h2><a id=\"user-content-----keyspaces----\" class=\"anchor\" aria-hidden=\"true\" href=\"#----keyspaces----\"></a>--- Keyspaces ---</h2>\n<h3><a id=\"user-content-list-all-the-keyspaces\" class=\"anchor\" aria-hidden=\"true\" href=\"#list-all-the-keyspaces\"></a>List all the Keyspaces</h3>\n<p><code> [GET] <a href=\"http://127.0.0.1:5000/keyspaces/\" rel=\"nofollow\">http://127.0.0.1:5000/keyspaces/</a> </code></p>\n<h3><a id=\"user-content-get-info-about-a-keyspace\" class=\"anchor\" aria-hidden=\"true\" href=\"#get-info-about-a-keyspace\"></a>Get Info about a keyspace</h3>\n<p><code> [GET] <a href=\"http://127.0.0.1:5000/keyspaces/keyspaceid\" rel=\"nofollow\">http://127.0.0.1:5000/keyspaces/keyspaceid</a> </code></p>\n<h3><a id=\"user-content-creates-a-new-keyspace\" class=\"anchor\" aria-hidden=\"true\" href=\"#creates-a-new-keyspace\"></a>Creates a new keyspace</h3>\n<p><code> [POST] <a href=\"http://127.0.0.1:5000/keyspaces/\" rel=\"nofollow\">http://127.0.0.1:5000/keyspaces/</a> </code></p>\n<h5><a id=\"user-content-body-2\" class=\"anchor\" aria-hidden=\"true\" href=\"#body-2\"></a>Body</h5>\n<pre>* { 'name' : name , 'replicationFactor' : number }\n</pre>\n<h3><a id=\"user-content-updates-the-keyspace\" class=\"anchor\" aria-hidden=\"true\" href=\"#updates-the-keyspace\"></a>Updates the keyspace</h3>\n<p><code> [PUT] <a href=\"http://127.0.0.1:5000/keyspaces/keyspaceid\" rel=\"nofollow\">http://127.0.0.1:5000/keyspaces/keyspaceid</a> </code></p>\n<h5><a id=\"user-content-body-3\" class=\"anchor\" aria-hidden=\"true\" href=\"#body-3\"></a>Body</h5>\n<pre>* { 'replicationFactor' : number }\n</pre>\n<h3><a id=\"user-content-deletes-the-keyspace\" class=\"anchor\" aria-hidden=\"true\" href=\"#deletes-the-keyspace\"></a>Deletes the keyspace</h3>\n<p><code> [DELETE] <a href=\"http://127.0.0.1:5000/keyspaces/keyspaceid\" rel=\"nofollow\">http://127.0.0.1:5000/keyspaces/keyspaceid</a> </code></p>\n<h2><a id=\"user-content------column-family----\" class=\"anchor\" aria-hidden=\"true\" href=\"#-----column-family----\"></a>---  Column Family ---</h2>\n<h3><a id=\"user-content-list-all-the-column-families-of-a-keyspace\" class=\"anchor\" aria-hidden=\"true\" href=\"#list-all-the-column-families-of-a-keyspace\"></a>List all the column families of a keyspace</h3>\n<p><code> [GET] <a href=\"http://127.0.0.1:5000/keyspace/keyspaceid/columnfamilys/\" rel=\"nofollow\">http://127.0.0.1:5000/keyspace/keyspaceid/columnfamilys/</a> </code></p>\n<h3><a id=\"user-content-get-info-of-a-column-family\" class=\"anchor\" aria-hidden=\"true\" href=\"#get-info-of-a-column-family\"></a>Get info of a column family</h3>\n<p><code> [GET] <a href=\"http://127.0.0.1:5000/keyspace/keyspaceid/columnfamilys/columnfamilyid\" rel=\"nofollow\">http://127.0.0.1:5000/keyspace/keyspaceid/columnfamilys/columnfamilyid</a> </code></p>\n<h3><a id=\"user-content-creates-a-new-column-family\" class=\"anchor\" aria-hidden=\"true\" href=\"#creates-a-new-column-family\"></a>Creates a new column family</h3>\n<p><code> [POST] <a href=\"http://127.0.0.1:5000/keyspace/keyspaceid/columnfamilys/\" rel=\"nofollow\">http://127.0.0.1:5000/keyspace/keyspaceid/columnfamilys/</a> </code></p>\n<h5><a id=\"user-content-body-4\" class=\"anchor\" aria-hidden=\"true\" href=\"#body-4\"></a>Body</h5>\n<pre>* { 'name' : name }\n</pre>\n<h3><a id=\"user-content-delete-a-column-family\" class=\"anchor\" aria-hidden=\"true\" href=\"#delete-a-column-family\"></a>Delete a column family</h3>\n<p><code> [DELETE] <a href=\"http://127.0.0.1:5000/keyspace/keyspaceid/columnfamilys/columnfamilyid\" rel=\"nofollow\">http://127.0.0.1:5000/keyspace/keyspaceid/columnfamilys/columnfamilyid</a> </code></p>\n<h2><a id=\"user-content-----row-entries----\" class=\"anchor\" aria-hidden=\"true\" href=\"#----row-entries----\"></a>--- Row Entries ---</h2>\n<h3><a id=\"user-content-list-all-the-rows\" class=\"anchor\" aria-hidden=\"true\" href=\"#list-all-the-rows\"></a>List all the rows</h3>\n<p><code> [GET] <a href=\"http://127.0.0.1:5000/keyspace/keyspaceid/columnfamilys/columnfamilyid/entrys/\" rel=\"nofollow\">http://127.0.0.1:5000/keyspace/keyspaceid/columnfamilys/columnfamilyid/entrys/</a> </code></p>\n<h3><a id=\"user-content-get-info-about-a-row\" class=\"anchor\" aria-hidden=\"true\" href=\"#get-info-about-a-row\"></a>Get Info about a row</h3>\n<p><code> [GET] <a href=\"http://127.0.0.1:5000/keyspace/keyspaceid/columnfamilys/columnfamilyid/entrys/entryname\" rel=\"nofollow\">http://127.0.0.1:5000/keyspace/keyspaceid/columnfamilys/columnfamilyid/entrys/entryname</a> </code></p>\n<h3><a id=\"user-content-creates-a-new-row\" class=\"anchor\" aria-hidden=\"true\" href=\"#creates-a-new-row\"></a>Creates a new row</h3>\n<p><code> [POST] <a href=\"http://127.0.0.1:5000/keyspace/keyspaceid/columnfamilys/columnfamilyid/entrys/\" rel=\"nofollow\">http://127.0.0.1:5000/keyspace/keyspaceid/columnfamilys/columnfamilyid/entrys/</a> </code></p>\n<h5><a id=\"user-content-body-5\" class=\"anchor\" aria-hidden=\"true\" href=\"#body-5\"></a>Body</h5>\n<pre>* { 'field1' : field name , 'field1_type' : field data type .... }\n</pre>\n<h3><a id=\"user-content-deletes-the-row\" class=\"anchor\" aria-hidden=\"true\" href=\"#deletes-the-row\"></a>Deletes the row</h3>\n<p><code> [DELETE] <a href=\"http://127.0.0.1:5000/keyspace/keyspaceid/columnfamilys/columnfamilyid/entrys/entryname\" rel=\"nofollow\">http://127.0.0.1:5000/keyspace/keyspaceid/columnfamilys/columnfamilyid/entrys/entryname</a> </code></p>\n<p>######Note:- 10.1.36.68 is the seed\n######Note:- In delete node code, path is the directory where cassandra is installed</p>\n<p>##More Info</p>\n<p>Mail :- <a href=\"mailto:rohitsakala@gmail.com\">rohitsakala@gmail.com</a></p>\n</article>",
        "created_at": "2019-01-08T22:57:33+0000",
        "updated_at": "2019-01-08T22:57:42+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "github.com",
        "preview_picture": "https://avatars3.githubusercontent.com/u/8402361?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12895"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          },
          {
            "id": 1385,
            "label": "dse",
            "slug": "dse"
          }
        ],
        "is_public": false,
        "id": 12767,
        "uid": null,
        "title": "Introducing DataStax Bulk Loader",
        "url": "https://www.datastax.com/2018/05/introducing-datastax-bulk-loader",
        "content": "Introducing DataStax Bulk Loader |  DataStax\n\n<noscript>\n\n\n\n\n<div class=\"DS17\"><div class=\"connect-us\"><a href=\"https://www.datastax.com/contactus\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Mail.svg\" alt=\"email icon\" />email</a><a href=\"https://www.datastax.com/company#offices\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Phone.svg\" alt=\"phone icon\" />call</a></div></div> \n\t\t \n\t\n\n    \n      <div class=\"DS17\"><div class=\"use-case\"><div class=\"wrapper\"><div class=\"two-col text-light-blue\"><h6>Customer Experience</h6><ul><li><a href=\"https://www.datastax.com/use-cases/customer-360\">Customer 360</a></li>\n          <li><a href=\"https://www.datastax.com/personalization\">Personalization &amp; Recommendations</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/loyalty-programs\">Loyalty Programs</a></li>\n          <li><a href=\"https://www.datastax.com/fraud-detection\">Consumer Fraud Detection</a></li>\n        </ul></div><div class=\"two-col text-light-green\"><h6><a href=\"#\">Enterprise Optimization</a></h6><ul><li><a href=\"https://www.datastax.com/use-cases/ecommerce\">eCommerce</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/identity-management\">Identity Management</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/security\">Security and Compliance</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/supply-chain\">Supply Chain</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/inventory-management\">Inventory Management</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/asset-monitoring\">Asset Monitoring</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/logistics\">Logistics</a></li>\n        </ul></div></div></div></div>\n  \n    \n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\n</noscript>",
        "created_at": "2018-11-16T17:02:35+0000",
        "updated_at": "2018-11-16T17:02:43+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 0,
        "domain_name": "www.datastax.com",
        "preview_picture": "https://www.datastax.com/wp-content/uploads/2018/05/Bulk_Loader_FB_Banner.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12767"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 12,
            "label": "video",
            "slug": "video"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 568,
            "label": "azure",
            "slug": "azure"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          },
          {
            "id": 994,
            "label": "dynamo",
            "slug": "dynamo"
          },
          {
            "id": 1298,
            "label": "google.cloud",
            "slug": "google-cloud"
          },
          {
            "id": 1385,
            "label": "dse",
            "slug": "dse"
          }
        ],
        "is_public": false,
        "id": 12757,
        "uid": null,
        "title": "Apache Cassandra vs Other Cloud Databases | DataStax",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=G8xHwnCevYo",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/G8xHwnCevYo?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-11-14T21:22:27+0000",
        "updated_at": "2018-11-14T21:22:27+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/G8xHwnCevYo/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12757"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 12,
            "label": "video",
            "slug": "video"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12756,
        "uid": null,
        "title": "Apache Cassandra Best Practices - Live Webinar",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=dhu9qzetnY8",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/dhu9qzetnY8?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-11-14T21:22:26+0000",
        "updated_at": "2018-11-14T21:22:26+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/dhu9qzetnY8/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12756"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 995,
            "label": "testing",
            "slug": "testing"
          },
          {
            "id": 1232,
            "label": "stress",
            "slug": "stress"
          },
          {
            "id": 1430,
            "label": "gatling",
            "slug": "gatling"
          }
        ],
        "is_public": false,
        "id": 12754,
        "uid": null,
        "title": "yabinmeng/cassgatling",
        "url": "https://github.com/yabinmeng/cassgatling",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>This is a sample code of utilizing <a href=\"https://gatling.io/\" rel=\"nofollow\">Gatling</a> testing framework for stress testing with DSE</p>\n<h3><a id=\"user-content-pre-requisites\" class=\"anchor\" aria-hidden=\"true\" href=\"#pre-requisites\"></a>Pre-requisites</h3>\n<p>Note: At the moment, the latest version of Gatling framework is 2.3.1. However, the CQL plugin 0.0.7 is not compatible with Gatling 2.3.1 yet.</p>\n<ol><li>Gatling high charts bundle version 2.2.5:</li>\n</ol><ul><li><a href=\"https://repo1.maven.org/maven2/io/gatling/highcharts/gatling-charts-highcharts-bundle/2.2.5/\" rel=\"nofollow\">https://repo1.maven.org/maven2/io/gatling/highcharts/gatling-charts-highcharts-bundle/2.2.5/</a></li>\n</ul><ol start=\"2\"><li>Gatling CQL plugin v0.0.7 (latest version as of writing)</li>\n</ol><p><strong>[NOTES]</strong> - as of April 10, 2018, Gatling CQL plugin has released version 0.0.8 which works with the latest Gatling framework 2.3.1. Please feel free to download these latest versions for your test. Please also be aware that Gatling 2.3.1 requires scala 2.1.2 and make sure your scala version is upgraded first.</p>\n<h3><a id=\"user-content-notes-about-the-example-simulation-scenario\" class=\"anchor\" aria-hidden=\"true\" href=\"#notes-about-the-example-simulation-scenario\"></a>Notes about the Example Simulation Scenario</h3>\n<p>The simulation scenario (MyTestSimu.scala) as included in this example simulates a mixed read/write workload. The core steps of the scenario are summarized below and you can follow the same steps when creating your own scenario:</p>\n<ol><li>Set up connection to Cassandra/DSE cluster with proper properties, such as \"contact points\", \"load balancing policy\", etc.</li>\n<li>Create the application keyspace and table schema</li>\n<li>Define the (random) value generator for table columns based on their types</li>\n<li>Define the Read/Write statements to be used in the simulation, including the Consistency Level associated with the statements</li>\n<li>Set up the user simulation behavior for Read and Write. The key parts include:</li>\n</ol><ul><li>How many concurrent users (can be constant or some variance) to be simulated per second.</li>\n<li>How long does the simulation executes</li>\n</ul><h3><a id=\"user-content-procedure\" class=\"anchor\" aria-hidden=\"true\" href=\"#procedure\"></a>Procedure</h3>\n<ol><li>Set up Gatling framework and the CQL plug-in (just unzip, as per description found <a href=\"https://github.com/gatling-cql/GatlingCql\">here</a>)</li>\n<li>Download the MyTestSimu.scala file and put it under folder &lt;GATLING_HOME&gt;/user-files/simulations/</li>\n<li>Execute the Gatling simulation (stress-testing) scenario by running command: &lt;GATLING_HOME&gt;/bin/gatling.sh. Follow the instructions on the command-line output.</li>\n</ol><p>NOTE: The simulation scenario (MyTestSimu.scala) is tested against a DSE cluster (version 5.1.6) with UserName/Password authentication. Please adjust accordingly for your case.</p>\n<hr /><p>An example is as below. Simulation number 1 is the Cassandra stres-testing scenario as defined by this example.</p>\n<pre>$ bin/gatling.sh\nGATLING_HOME is set to /home/automaton/gatling-charts-highcharts-bundle-2.2.5\nChoose a simulation number:\n     [0] cassandra.CassandraSimulation\n     [1] cassandra.MyTestSimu\n     [2] computerdatabase.BasicSimulation\n     [3] computerdatabase.advanced.AdvancedSimulationStep01\n     [4] computerdatabase.advanced.AdvancedSimulationStep02\n     [5] computerdatabase.advanced.AdvancedSimulationStep03\n     [6] computerdatabase.advanced.AdvancedSimulationStep04\n     [7] computerdatabase.advanced.AdvancedSimulationStep05\n1\nSelect simulation id (default is 'mytestsimu'). Accepted characters are a-z, A-Z, 0-9, - and _\nSelect run description (optional)\nSimulation cassandra.MyTestSimu started...\n================================================================================\n2018-04-04 15:33:43                                           5s elapsed\n---- Requests ------------------------------------------------------------------\n&gt; Global                                                   (OK=232    KO=0     )\n&gt; upsertStmt                                               (OK=93     KO=0     )\n&gt; readStmt                                                 (OK=139    KO=0     )\n---- Read Workload Scenario ----------------------------------------------------\n[#                                                                         ]  1%\n          waiting: 8861   / active: 0      / done:139\n---- Write Workload Scenario ---------------------------------------------------\n[                                                                          ]  0%\n          waiting: 20907  / active: 0      / done:93\n================================================================================\n... ...\n================================================================================\n2018-04-04 15:43:39                                         600s elapsed\n---- Requests ------------------------------------------------------------------\n&gt; Global                                                   (OK=30000  KO=0     )\n&gt; upsertStmt                                               (OK=21000  KO=0     )\n&gt; readStmt                                                 (OK=9000   KO=0     )\n---- Read Workload Scenario ----------------------------------------------------\n[##########################################################################]100%\n          waiting: 0      / active: 0      / done:9000\n---- Write Workload Scenario ---------------------------------------------------\n[##########################################################################]100%\n          waiting: 0      / active: 0      / done:21000\n================================================================================\nSimulation cassandra.MyTestSimu completed in 600 seconds\nParsing log file(s)...\nParsing log file(s) done\nGenerating reports...\n================================================================================\n---- Global Information --------------------------------------------------------\n&gt; request count                                      30000 (OK=30000  KO=0     )\n&gt; min response time                                      0 (OK=0      KO=-     )\n&gt; max response time                                    224 (OK=224    KO=-     )\n&gt; mean response time                                     2 (OK=2      KO=-     )\n&gt; std deviation                                          5 (OK=5      KO=-     )\n&gt; response time 50th percentile                          1 (OK=1      KO=-     )\n&gt; response time 75th percentile                          2 (OK=2      KO=-     )\n&gt; response time 95th percentile                          3 (OK=3      KO=-     )\n&gt; response time 99th percentile                          7 (OK=7      KO=-     )\n&gt; mean requests/sec                                     50 (OK=50     KO=-     )\n---- Response Time Distribution ------------------------------------------------\n&gt; t &lt; 800 ms                                         30000 (100%)\n&gt; 800 ms &lt; t &lt; 1200 ms                                   0 (  0%)\n&gt; t &gt; 1200 ms                                            0 (  0%)\n&gt; failed                                                 0 (  0%)\n================================================================================\nReports generated in 4s.\nPlease open the following file: /home/automaton/gatling-charts-highcharts-bundle-2.2.5/results/mytestsimu-1522856018600/index.html\n</pre>\n</article>",
        "created_at": "2018-11-14T17:34:32+0000",
        "updated_at": "2018-11-14T17:34:41+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/16789452?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12754"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 217,
            "label": "tool",
            "slug": "tool"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          }
        ],
        "is_public": false,
        "id": 12753,
        "uid": null,
        "title": "yabinmeng/dseutilities",
        "url": "https://github.com/yabinmeng/dseutilities",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\"><h2><a id=\"user-content-description\" class=\"anchor\" aria-hidden=\"true\" href=\"#description\"></a>Description</h2>\n<p>This repository contains a list of Misc. utilities, sample codes, and blog posts that I wrote in order to ease the learning and working with DataStax Enterprise (DSE).</p>\n<p><strong>Disclaimer</strong>: Most, if not all, of the utilities/sample codes/blog posts are based on DSE 5.1.x version (the latest major DSE release as of writing). For older DSE versions such as 5.0.x or 4.8.x, please use at your own discretion and feel free to test out by yourself.</p>\n<hr /><h2><a id=\"user-content-1-utilities\" class=\"anchor\" aria-hidden=\"true\" href=\"#1-utilities\"></a>1. Utilities</h2>\n<h4><a id=\"user-content-11-wrapper-utility-to-encrypt-dse-configuration\" class=\"anchor\" aria-hidden=\"true\" href=\"#11-wrapper-utility-to-encrypt-dse-configuration\"></a>1.1. <a href=\"https://github.com/yabinmeng/dseutilities/tree/master/dseconfenc\">Wrapper Utility to Encrypt DSE Configuration</a></h4>\n<h4><a id=\"user-content-12-wrapper-utility-to-encrypt-opscenter-configuration\" class=\"anchor\" aria-hidden=\"true\" href=\"#12-wrapper-utility-to-encrypt-opscenter-configuration\"></a>1.2. <a href=\"https://github.com/yabinmeng/dseutilities/tree/master/opsconfenc\">Wrapper Utility to Encrypt OpsCenter Configuration</a></h4>\n<h4><a id=\"user-content-13-cassandra-tombstone-counter\" class=\"anchor\" aria-hidden=\"true\" href=\"#13-cassandra-tombstone-counter\"></a>1.3. <a href=\"https://github.com/yabinmeng/tombstone_counter\">Cassandra Tombstone Counter</a></h4>\n<h4><a id=\"user-content-14-ansible-playbook-for-multi-dc-dse-cluster-provisioning\" class=\"anchor\" aria-hidden=\"true\" href=\"#14-ansible-playbook-for-multi-dc-dse-cluster-provisioning\"></a>1.4. <a href=\"https://github.com/yabinmeng/dseansible\">Ansible Playbook for multi-DC DSE Cluster Provisioning</a></h4>\n<h4><a id=\"user-content-15-terraform-and-ansible-automation-to-provision-multi-dc-dse-cluster-on-aws\" class=\"anchor\" aria-hidden=\"true\" href=\"#15-terraform-and-ansible-automation-to-provision-multi-dc-dse-cluster-on-aws\"></a>1.5. <a href=\"https://github.com/yabinmeng/terradse\">Terraform and Ansible Automation to Provision multi-DC DSE Cluster on AWS</a></h4>\n<h4><a id=\"user-content-16-restore-dse-backup-data-from-aws-s3\" class=\"anchor\" aria-hidden=\"true\" href=\"#16-restore-dse-backup-data-from-aws-s3\"></a>1.6. <a href=\"https://github.com/yabinmeng/opscs3restore\">Restore DSE backup data from AWS S3</a></h4>\n<h4><a id=\"user-content-17-oss-gatling-stress-testing-scenario-for-dse\" class=\"anchor\" aria-hidden=\"true\" href=\"#17-oss-gatling-stress-testing-scenario-for-dse\"></a>1.7. <a href=\"https://github.com/yabinmeng/cassgatling\">OSS Gatling Stress Testing Scenario for DSE</a></h4>\n<hr /><h2><a id=\"user-content-2-blog-posts\" class=\"anchor\" aria-hidden=\"true\" href=\"#2-blog-posts\"></a>2. Blog Posts</h2>\n<hr /><h2><a id=\"user-content-3-sample-codes\" class=\"anchor\" aria-hidden=\"true\" href=\"#3-sample-codes\"></a>3. Sample Codes</h2>\n<h4><a id=\"user-content-31-writting-udt-into-cassandra\" class=\"anchor\" aria-hidden=\"true\" href=\"#31-writting-udt-into-cassandra\"></a>3.1. <a href=\"https://github.com/yabinmeng/dseudt\">Writting UDT into Cassandra</a></h4>\n</article>",
        "created_at": "2018-11-14T17:34:02+0000",
        "updated_at": "2018-11-14T17:34:15+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/16789452?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12753"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1233,
            "label": "data.modeling",
            "slug": "data-modeling"
          }
        ],
        "is_public": false,
        "id": 12752,
        "uid": null,
        "title": "yabinmeng/dseutilities",
        "url": "https://github.com/yabinmeng/dseutilities/blob/master/documents/Yabin.meng-CassandraTombstoneInDepth.pdf",
        "content": "<div role=\"main\" class=\"application-main\" data-organization-hovercards-enabled=\"\"><div itemscope=\"itemscope\" itemtype=\"http://schema.org/SoftwareSourceCode\" class=\"\"><div id=\"js-repo-pjax-container\" data-pjax-container=\"\"><div class=\"container new-discussion-timeline experiment-repo-nav\"><div class=\"repository-content\"><a class=\"d-none js-permalink-shortcut\" data-hotkey=\"y\" href=\"https://github.com/yabinmeng/dseutilities/blob/32dd15bbd4090e34bd61c85675fef4045df6d1a3/documents/Yabin.meng-CassandraTombstoneInDepth.pdf\">Permalink</a><div class=\"commit-tease\"><a class=\"commit-tease-sha\" href=\"https://github.com/yabinmeng/dseutilities/commit/f9be5c04edf81c7cd80623ab7d4e075e17b32550\" data-pjax=\"\">\n          f9be5c0\n        </a>Jul 8, 2018<p><a class=\"user-mention\" rel=\"author\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16789452\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yabinmeng\">yabinmeng</a>\n          <a data-pjax=\"true\" title=\"Initial documents upload\" class=\"message\" href=\"https://github.com/yabinmeng/dseutilities/commit/f9be5c04edf81c7cd80623ab7d4e075e17b32550\">Initial documents upload</a></p><div class=\"commit-tease-contributors\"><details class=\"details-reset details-overlay details-overlay-dark lh-default text-gray-dark float-left mr-2\" id=\"blob_contributors_box\"><summary class=\"btn-link\" aria-haspopup=\"dialog\"><strong>1</strong> contributor\n  </summary><details class=\"Box Box--overlay d-flex flex-column anim-fade-in fast\" aria-label=\"Users who have contributed to this file\">\n    \n        <ul class=\"list-style-none overflow-auto\"><li class=\"Box-row\">\n              <a class=\"link-gray-dark no-underline\" href=\"https://github.com/yabinmeng\">\n                <img class=\"avatar mr-2\" alt=\"\" src=\"https://avatars2.githubusercontent.com/u/16789452?s=40&amp;v=4\" width=\"20\" height=\"20\" />\n                yabinmeng\n</a>            </li>\n        </ul></details></details></div></div><details class=\"details-reset details-overlay details-overlay-dark\"><summary data-hotkey=\"l\" aria-label=\"Jump to line\">    </summary></details></div></div></div></div></div><div id=\"ajax-error-message\" class=\"ajax-error-message flash flash-error\">You can’t perform that action at this time.</div>\n    \n  \n  \n  <details class=\"details-reset details-overlay details-overlay-dark lh-default text-gray-dark\" open=\"open\"><summary aria-haspopup=\"dialog\" aria-label=\"Close dialog\">\n      \n    </summary></details>\n<p>Press h to open a hovercard with more details.</p>",
        "created_at": "2018-11-14T17:33:42+0000",
        "updated_at": "2018-11-14T17:33:49+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/16789452?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12752"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          }
        ],
        "is_public": false,
        "id": 12741,
        "uid": null,
        "title": "DataStax Distribution of Apache Cassandra",
        "url": "https://www.datastax.com/products/datastax-distribution-of-apache-cassandra",
        "content": "DataStax Distribution of Apache Cassandra | DataStax \n\n<noscript>\n\n\n\n\n<div class=\"DS17\"><div class=\"connect-us\"><a href=\"https://www.datastax.com/contactus\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Mail.svg\" alt=\"email icon\" />email</a><a href=\"https://www.datastax.com/company#offices\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Phone.svg\" alt=\"phone icon\" />call</a></div></div><section class=\"intro-with-content\"><div class=\"container\"><div class=\"wrapper text-center\"><div class=\"content wow fadeInUp\"><h3>DataStax and Apache Cassandra</h3><p>DataStax and Apache Cassandra are tied at the hip - literally and figuratively. Cassandra is the backbone of DataStax technology and the reason we love data management. From its inception, Apache Cassandra has been the premier distributed database on the market, and here at DataStax we remain committed to continuing that legacy and expanding it with world-class enterprise features delivered via DataStax Enterprise 6 - the best distribution of Apache Cassandra.</p></div><div class=\"col-wrapper\"><div class=\"three-col wow fadeInUp\"><h6>#1 OSS Committer</h6><p>DataStax provides over 80% of the commits to Apache Cassandra Open Source and we are one of the driving forces behind Apache Cassandra 4.0.</p></div><div class=\"three-col wow fadeInUp\"><h6>Apache Cassandra Experts</h6><p>DataStax Enterprise is developed and updated from the open source Cassandra project, and the DataStax team has been an integral part of the Cassandra project since its inception (check out our history <a href=\"https://www.datastax.com/why-datastax/apache-cassandra\" target=\"_self\">here</a>).&#13;\n</p></div><div class=\"three-col wow fadeInUp\"><h6>Open Source Leadership</h6><p>DataStax provides open source leadership in other database-related projects (like Apache TinkerPop™) as part of our commitment to open source.</p></div></div></div></div></section><section class=\"cards wow fadeInUp dse-cards no-padding-bottom   bg-gray\" data-wow-delay=\"0.5s\"><div class=\"container\"><p>&#13;\n        </p><h3>Why DataStax Distribution of Apache Cassandra?</h3>&#13;<div class=\"column-wrapper text-center\"><div class=\"three-col wow fadeInUp col card-blue\"><p>&#13;\n              </p><h4>Production Certified Apache Cassandra</h4>&#13;<div class=\"icon-box\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Graph_Powered_Insights.svg\" alt=\"circular icon with customer surrounded by dots\" /></div><p>100% OSS compatible but also production certified with hot fixes, bug escalation, and nightly regression testing included as standard.</p></div><div class=\"three-col wow fadeInUp col card-blue\"><p>&#13;\n              </p><h4>Backed by the Apache Cassandra Experts</h4>&#13;<div class=\"icon-box\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Graph_Your_way.svg\" alt=\"circular icon showing datastax graph personalization\" /></div><p>Full access to the best-in-class Apache Cassandra software, support, and services from the experts.</p></div><div class=\"three-col wow fadeInUp col card-blue\"><p>&#13;\n              </p><h4>Streamline Operations and Support</h4>&#13;<div class=\"icon-box\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Graph_Available_Always.svg\" alt=\"circular icon showing datastax graphs are available around the clock\" /></div><p>Remove your patchwork quilt of in house, regional SI’s and third party vendors providing support and services with a single, expert team.</p></div></div></div></section><section class=\"database-table  bg-gray\" id=\"databaseTable\"><div class=\"container\"><div class=\"full-width-table\"><table><thead><tr><td><h5>Open Source Compatible</h5></td>\n          <td><div class=\"img-box\"><img src=\"https://www.datastax.com/templates/dist/images/check_link.png\" alt=\"check-arrow\" /></div></td>\n          <td><div class=\"img-box\"><img src=\"https://www.datastax.com/templates/dist/images/check_link.png\" alt=\"check-arrow\" /></div></td>\n          </tr><tr><td><h5>Production Certified</h5></td>\n          <td>&#13;\n          </td><td><div class=\"img-box\"><img src=\"https://www.datastax.com/templates/dist/images/check_link.png\" alt=\"check-arrow\" /></div></td>\n          </tr><tr><td><h5>Drivers</h5></td>\n          <td>&#13;\n          </td><td><div class=\"img-box\"><img src=\"https://www.datastax.com/templates/dist/images/check_link.png\" alt=\"check-arrow\" /></div></td>\n          </tr><tr></tr><tr><td><h5>DataStax Bulk Loader</h5></td>\n          <td>&#13;\n          </td><td><div class=\"img-box\"><img src=\"https://www.datastax.com/templates/dist/images/check_link.png\" alt=\"check-arrow\" /></div></td>\n          </tr><tr></tr><tr><td><h5>Technical Support</h5></td>\n          <td>&#13;\n          </td><td><div class=\"img-box\"><img src=\"https://www.datastax.com/templates/dist/images/check_link.png\" alt=\"check-arrow\" /></div></td>\n          </tr><tr></tr><tr><td><h5>Professional Services</h5></td>\n          <td>&#13;\n          </td><td><div class=\"img-box\"><img src=\"https://www.datastax.com/templates/dist/images/check_link.png\" alt=\"check-arrow\" /></div></td>\n          </tr><tr></tr><tr><td><h5>Hot Fixes &amp; Bug Escalation</h5></td>\n          <td>&#13;\n          </td><td><div class=\"img-box\"><img src=\"https://www.datastax.com/templates/dist/images/check_link.png\" alt=\"check-arrow\" /></div></td>\n          </tr><tr></tr></thead></table></div></div></section><section class=\"content-with-media-box\"><section class=\"latest-block  bg-gray\"><section class=\"cta round-left-bottom no-padding\"><div class=\"DS17\"><div class=\"use-case\"><div class=\"wrapper\"><div class=\"two-col text-light-blue\"><h6>Customer Experience</h6><ul><li><a href=\"https://www.datastax.com/use-cases/customer-360\">Customer 360</a></li>\n          <li><a href=\"https://www.datastax.com/personalization\">Personalization &amp; Recommendations</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/loyalty-programs\">Loyalty Programs</a></li>\n          <li><a href=\"https://www.datastax.com/fraud-detection\">Consumer Fraud Detection</a></li>\n        </ul></div><div class=\"two-col text-light-green\"><h6><a href=\"#\">Enterprise Optimization</a></h6><ul><li><a href=\"https://www.datastax.com/use-cases/ecommerce\">eCommerce</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/identity-management\">Identity Management</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/security\">Security and Compliance</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/supply-chain\">Supply Chain</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/inventory-management\">Inventory Management</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/asset-monitoring\">Asset Monitoring</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/logistics\">Logistics</a></li>\n        </ul></div></div></div></div>\n\t\n\t\n\t\n\t\n</section></section></section></noscript>",
        "created_at": "2018-11-13T21:40:01+0000",
        "updated_at": "2018-11-13T21:40:11+0000",
        "published_at": "2018-11-02T20:17:48+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 1,
        "domain_name": "www.datastax.com",
        "preview_picture": "https://www.datastax.com/wp-content/uploads/resources/social/Generic_DataStax_FB.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12741"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          },
          {
            "id": 956,
            "label": "streaming",
            "slug": "streaming"
          },
          {
            "id": 1385,
            "label": "dse",
            "slug": "dse"
          }
        ],
        "is_public": false,
        "id": 12729,
        "uid": null,
        "title": "phact/SimpleSparkStreaming",
        "url": "https://github.com/phact/SimpleSparkStreaming",
        "content": "<p>This is a guide for how to use the Streaming Analytics Proofshop asset brought to you by the Vanguard team. The Proofshop consists of demonstrating real time data processing and analytics with the DSE paltform at high throughput, low latency, and scale.</p><h3>Motivation</h3><p>DSE is often seen as a serving layer for the output of analytical applications on large datasets. Often the analytics themselves are performed in slow batch data layers and persisted to DSE only when they are ready to be exposed to a front end or downstream system. In many cases, as we look to satisfy end user requirements, it makes more sense to perform some of these operational analytics in real time using DSE's streaming analytics capabilities.</p><h3>What is included?</h3><p>This field asset includes sample usage of DSE Streaming Analytics in the following contexts:</p><ul><li>Data generation using EBDSE served via TCP</li>\n<li>Spark streaming application that persists raw events into DSE and rollups into summary tables</li>\n<li>Tuning capabilities for throughput and latencies</li>\n<li>Checkpointing into DSEFS</li>\n</ul><h3>Business Take Aways</h3><p>In the right now economy businesses need analytics that are up to date. Monthly, weekly, and hourly reports are often too old to be actionable. DSE Streaming analytics allows businesses to operationalize structured analytics to obtain real time insights to power their decision making.</p><p>Out of the Five Dimensions, this asset focuses on Relevancy and Responsiveness without ignoring the remaining dimensions (Availability, Accessibility, Engagement).</p><p>If discussing this asset with a business stakeholder it may be relevant to walk them through <a href=\"https://docs.google.com/presentation/d/1z_wGENm2RNX1oqwUkSzDg3P03xPG2P9lKEjj8UiZOQM/edit?usp=sharing\" rel=\"nofollow\">The DataStax Story</a></p><h3>Technical Take Aways</h3><p>DSE Streaming Analytics is DataStax's version of Apache Spark (TM) which ships with DSE and is modified to match the design principles that our engineering team has always focused on when building C* and DSE (CARDS). DSE Spark is optimized for high availability and operational simplicity by removing its dependency on zookeeper and using LWTs for leader elections. Furthermore, DSE Spark is optimized for performance against the DSE backend with features that include Contiunous Paging and Direct Joins.</p><p>Building Streaming Analytics applications on DSE requires:</p><ul><li>Having a streaming source - in this case we use EBDSE's tcpserver functionality and data generation capabilities as our streaming source</li>\n<li>A Spark Streaming Application - sources for the app included as part of the asset</li>\n</ul><p>For more general information on how to use Spark Streaming check out the <a href=\"https://spark.apache.org/docs/latest/streaming-programming-guide.html\" rel=\"nofollow\">Programming Guide</a> in the Spark Docs.</p><p>These docs will dive deeper on the following functionality:</p><ul><li>Cumulative Streaming Calculations</li>\n<li>Monitoring and Tuning Streaming Jobs</li>\n</ul>",
        "created_at": "2018-11-13T20:26:31+0000",
        "updated_at": "2018-11-13T20:27:26+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "github.com",
        "preview_picture": "https://avatars3.githubusercontent.com/u/1313220?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12729"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 568,
            "label": "azure",
            "slug": "azure"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          },
          {
            "id": 995,
            "label": "testing",
            "slug": "testing"
          },
          {
            "id": 1232,
            "label": "stress",
            "slug": "stress"
          },
          {
            "id": 1298,
            "label": "google.cloud",
            "slug": "google-cloud"
          },
          {
            "id": 1385,
            "label": "dse",
            "slug": "dse"
          }
        ],
        "is_public": false,
        "id": 12722,
        "uid": null,
        "title": "justinbreese/dse-cassandra-stress",
        "url": "https://github.com/justinbreese/dse-cassandra-stress",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<ul><li>I wanted to run some <code>cassandra-stress</code> testing on the three main cloud providers (Google, Amazon, and Microsoft) on similarly sized DataStax Enterprise clusters and see what the results would be</li>\n<li>Why did I want to do this? When running on a cluster of similarly-sized VMs, networking starts to come into play for replication, storage, and stronger consistency levels. This type of an exercise exposes the network so you can understand what you may have to contend with for a specific cloud provider versus another</li>\n<li>This is also a tutorial on some of the fun switches that you can use to tune your <code>cassandra-stress</code> test</li>\n<li>This is not a tutorial on how to deploy the VMs, if you want that information, please go over to another repo of mine as I repurposed it. It is 99% similar; I just didn't enable SSL and installed <code>dse-full</code>: <a href=\"https://github.com/justinbreese/dse-multi-cloud-demo\">https://github.com/justinbreese/dse-multi-cloud-demo</a></li>\n</ul><h2><a id=\"user-content-methodology\" class=\"anchor\" aria-hidden=\"true\" href=\"#methodology\"></a>Methodology</h2>\n<ul><li>Setup 4 VMs on each of the cloud providers; VM size was as close as possible with similar storage provisioned: i3.4xl, Standard_DS14_v2, and n1-highmem-16</li>\n<li>3 of the VMs will be running DataStax Enterprise as their own cluster; again each cloud is their own cluster</li>\n<li>The 4th VM runs DataStax OpsCenter and acts as a <code>cassandra-stress</code> client\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/justinbreese/dse-cassandra-stress/blob/master/architecture.png?raw=true\"><img src=\"https://github.com/justinbreese/dse-cassandra-stress/raw/master/architecture.png?raw=true\" alt=\"Screenshot\" /></a></li>\n<li>Next, I ran this test on each of the clients 35 times: <code>cassandra-stress user profile=stress.yaml n=1000000 ops\\(insert=3,likelyquery1=1\\) cl=QUORUM -mode native cql3 user=cassandra password=datastax -node 104.196.140.126 -rate threads\\&gt;=121</code>\n<ul><li>I then captured the results into an Excel file</li>\n</ul></li>\n<li>Then I decided to really turn it up and do 1B operations; and it led to this test that was run once on each of the clients: <code>nohup cassandra-stress user profile=stress.yaml n=1000000000 ops\\(insert=3,likelyquery1=1\\) cl=QUORUM -mode native cql3 user=cassandra password=datastax -node 40.118.149.27 -rate threads=300 -log file=output.txt -errors ignore</code>\n<ul><li>Again, I captured the results in an Excel file</li>\n</ul></li>\n</ul><h2><a id=\"user-content-breakdown-the-switches\" class=\"anchor\" aria-hidden=\"true\" href=\"#breakdown-the-switches\"></a>Breakdown the switches!</h2>\n<p>I told you that this would be educational!</p>\n<h3><a id=\"user-content-first-test\" class=\"anchor\" aria-hidden=\"true\" href=\"#first-test\"></a>First test</h3>\n<p><code>cassandra-stress user profile=stress.yaml n=1000000 ops\\(insert=3,likelyquery1=1\\) cl=QUORUM -mode native cql3 user=cassandra password=datastax -node 104.196.140.126 -rate threads\\&gt;=121</code></p>\n<ul><li><code>profile=stress.yaml</code> --&gt; take a look at the one that I created in the root of this repo. I went to a colleague's website and was able to build it in a few minutes (source below). Do this to create the actual data model that you want to use in production!</li>\n<li><code>n=1000000</code> --&gt; the amount of times that you want to run operations</li>\n<li><code>ops\\(insert=3,likelyquery1=1\\)</code> --&gt; the ratio of writes:reads that you want done for your test. This is a 3:1 ratio. You can be specific about the reads that you want done. In <code>stress.yaml</code>, towards the bottom, there is a query called <code>likelyquery</code> that I want to run for my reads. You can customize this to read whatever query you prefer; super useful!</li>\n<li><code>cl=QUORUM</code> --&gt; consistency level. I see way too many stress tests done just at a consistency level of <code>CL=ONE</code>. The reality is that most don't run with a <code>CL=ONE</code> when they're in production; so why would they test against it now. Run your test against the consistency level that you want to run in production!</li>\n<li><code>-mode native cql3 user=cassandra password=datastax</code> --&gt; you need the client to be able to access DataStax Enterprise, so ensure it has the correct username and password to do so</li>\n<li><code>-node 104.196.140.126</code> --&gt; as I am running on a client, I need to be able to connect to the cluster. This IP address was of one of the seed nodes for the cluster that I was targeting. Once I point it towards one node, the client will become aware of all of the other nodes via gossip.</li>\n<li><code>-rate threads\\&gt;=121</code> --&gt; this runs the test at thread levels that are greater than or equal to 121. You can run at less than that, but I wanted to run at realistic levels.\n<ul><li>A great first test is to run at <code>-rate auto</code> which will will start at a single digit thread count and work its way up as high as it can go. This is great for understanding what is going on so you can understand the relationship for latency, operations/second, threadcount, etc.</li>\n</ul></li>\n</ul><h2><a id=\"user-content-second-test\" class=\"anchor\" aria-hidden=\"true\" href=\"#second-test\"></a>Second test</h2>\n<p>I am not going to go over the flags/arguments that I went over in the previous example. Instead, I'll go over the new ones or changes.</p>\n<p><code>nohup cassandra-stress user profile=stress.yaml n=1000000000 ops\\(insert=3,likelyquery1=1\\) cl=QUORUM -mode native cql3 user=cassandra password=datastax -node 40.118.149.27 -rate threads=300 -log file=output.txt -errors ignore</code></p>\n<ul><li><code>nohup</code> --&gt; I nohup'd because I wanted this to run as its own process. The advantage here is that it will run after I exit the session, computer goes to sleep, etc.</li>\n<li><code>n=1000000000</code> --&gt; increased the ops to 1B</li>\n<li><code>-rate threads=300</code> --&gt; set the thread count to an even 300. This is generally a point at which we start to see saturation, contention, etc.; so I wanted to stay in that range for an extended period of time</li>\n<li><code>-log file=output.txt</code> --&gt; as this would be a long running test, I wanted to be able to save the output all into a file that way I could <code>SCP</code> it back to my laptop for analysis in Excel. Otherwise it will just print to the screen; and after 1B transactions, that gets messy!</li>\n<li><code>-errors ignore</code> --&gt; after you run 1B transactions, you will get errors eventually. Cassandra handles them just fine, but you don't want it to stop the test. Do this for your tests! Otherwise, you could have your test stop half-way through; ask me how I know...</li>\n</ul><h2><a id=\"user-content-results\" class=\"anchor\" aria-hidden=\"true\" href=\"#results\"></a>Results</h2>\n<p>to be continued... maybe, if I can.</p>\n<ul><li>If you want to learn more about <code>cassandra-stress</code>, then look no further than the excellent work of my colleague: <a href=\"https://www.datastax.com/dev/blog/data-modeler\" rel=\"nofollow\">https://www.datastax.com/dev/blog/data-modeler</a></li>\n<li>The actual tool that builds out the <code>stress.yaml</code>: <a href=\"https://www.sestevez.com/sestevez/CassandraDataModeler/\" rel=\"nofollow\">https://www.sestevez.com/sestevez/CassandraDataModeler/</a></li>\n</ul>\n<p>Thanks for taking a look at this repo and let me know if you have any questions.</p>\n</article>",
        "created_at": "2018-11-13T20:12:27+0000",
        "updated_at": "2018-11-13T20:12:50+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 5,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/16208950?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12722"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          },
          {
            "id": 1385,
            "label": "dse",
            "slug": "dse"
          }
        ],
        "is_public": false,
        "id": 12719,
        "uid": null,
        "title": "phact/dse-cluster-migration",
        "url": "https://github.com/phact/dse-cluster-migration",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>NOTE: If you are looking to perform a migration across two clusters, make sure\nyou deploy two clusters!</p>\n<p>This is a Demo for Migrating DSE and cassandra clusters (or even tables within the same cluster) using DSE Analytics / Spark.</p>\n<h3><a id=\"user-content-motivation\" class=\"anchor\" aria-hidden=\"true\" href=\"#motivation\"></a>Motivation</h3>\n<p>Moving data across clusters for one time migrations or bulk migrations are relatively common. DSE Analytics Makes this process almost trivial for users that are well versed in Spark. In a <a href=\"http://www.sestevez.com/cluster-migration-keeping-simple-things-simple/\" rel=\"nofollow\">previous blog post</a> I attempted to make this process simpler for users with minimal spark experience. This asset aims to make this process easy even for users that with no spark experience at all.</p>\n<h3><a id=\"user-content-what-is-included\" class=\"anchor\" aria-hidden=\"true\" href=\"#what-is-included\"></a>What is included?</h3>\n<p>This field asset (demo) includes the following:</p>\n<ul><li>dse-cluster-migration Spark Job</li>\n<li>Performs migration using massively parallel Spark compute</li>\n<li>Code is visible in the Che web IDE which ships with this asset</li>\n<li>MigUI (Migration UI) web appication</li>\n<li>React Redux frontend</li>\n<li>Dropwizard backend</li>\n<li>Uses webhdfs to programatically upload the spark job jar to DSEFS</li>\n<li>Uses the DSE only CQL Spark interface (currently an internal undocumented API) to submit the spark job to DSE</li>\n</ul><h3><a id=\"user-content-business-take-aways\" class=\"anchor\" aria-hidden=\"true\" href=\"#business-take-aways\"></a>Business Take Aways</h3>\n<p>DSE is the best distribution of Apache Cassandra and the easiest to use. By taking advantage of the migration capabilities in DSE analytics, projects can get off the ground faster and complex business requirements have a shorter time to Value.</p>\n<h3><a id=\"user-content-technical-take-aways\" class=\"anchor\" aria-hidden=\"true\" href=\"#technical-take-aways\"></a>Technical Take Aways</h3>\n<p>In some DSE to DSE or c* to DSE scenarios, there are a few cases in which a cluster migration is easier to perform than an upgrade. When the source cluster needs to remain place (i.e. data migrations accross environments, DEV &lt;-&gt; SIT &lt;-&gt; UAT &lt;-&gt; PROD) DSE Analytics can be the right solution.</p>\n<p>Look at this asset if you are interested in:\nUsing Spark to migrate data from one cluster to another\nUsing Spark to move a table to another table in the same cluster\nProgramatically writing to DSEFS from Java\nProgramatically using CQL to kick off DSE Analytics jobs (NOTE this is unsupported and undocumented at this time)</p>\n</article>",
        "created_at": "2018-11-13T18:41:41+0000",
        "updated_at": "2018-11-13T18:41:50+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://avatars3.githubusercontent.com/u/1313220?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12719"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 217,
            "label": "tool",
            "slug": "tool"
          },
          {
            "id": 1233,
            "label": "data.modeling",
            "slug": "data-modeling"
          }
        ],
        "is_public": false,
        "id": 12716,
        "uid": null,
        "title": "CQL Data Modeler",
        "url": "https://www.sestevez.com/sestevez/CassandraDataModeler/",
        "content": "<p>This tool is meant to improve your experience with Apache Cassandra.\n                Provide some information about your table definition, and the size, population, and cluster distributions of each of your fields and the tool will give you a script you can use to benchmark your data model and a storage engine visualization.\n                Please input your table definition for one table, keeping the Create statement, field definitions, and Primary Key definition on separate lines.</p><textarea id=\"tableDef\" placeholder=\"Create Table Statement\" rows=\"10\" cols=\"50\">\nCREATE TABLE reviews_by_day (\n    userid text,\n    day int,\n    productid text,\n    reviewid uuid,\n    profilename text,\n    helpfulness text,\n    score text,\n    summary text,\n    review text,\n    time timestamp,\n    PRIMARY KEY (userid, productid, time, reviewid))\n                </textarea><a href=\"https://www.sestevez.com/sestevez/CassandraDataModeler/\" class=\"ui-btn\" id=\"submit-button\">Verify Table</a>",
        "created_at": "2018-11-13T18:31:33+0000",
        "updated_at": "2018-11-13T18:31:44+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.sestevez.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12716"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 12,
            "label": "video",
            "slug": "video"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12713,
        "uid": null,
        "title": "Apache Cassandra Best Practices - Live Webinar",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=dhu9qzetnY8",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/dhu9qzetnY8?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-11-11T19:58:06+0000",
        "updated_at": "2018-11-11T19:58:06+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/dhu9qzetnY8/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12713"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 12,
            "label": "video",
            "slug": "video"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 568,
            "label": "azure",
            "slug": "azure"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          },
          {
            "id": 994,
            "label": "dynamo",
            "slug": "dynamo"
          },
          {
            "id": 1298,
            "label": "google.cloud",
            "slug": "google-cloud"
          },
          {
            "id": 1385,
            "label": "dse",
            "slug": "dse"
          }
        ],
        "is_public": false,
        "id": 12712,
        "uid": null,
        "title": "Apache Cassandra vs Other Cloud Databases | DataStax",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=G8xHwnCevYo",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/G8xHwnCevYo?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-11-11T19:58:04+0000",
        "updated_at": "2018-11-11T19:58:04+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/G8xHwnCevYo/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12712"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 12,
            "label": "video",
            "slug": "video"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12709,
        "uid": null,
        "title": "Cassandra with Tim Berglund",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=ifJK1jMEXh0",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/ifJK1jMEXh0?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-11-09T20:24:22+0000",
        "updated_at": "2018-11-09T20:24:27+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/ifJK1jMEXh0/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12709"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 12,
            "label": "video",
            "slug": "video"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 568,
            "label": "azure",
            "slug": "azure"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          },
          {
            "id": 994,
            "label": "dynamo",
            "slug": "dynamo"
          },
          {
            "id": 1298,
            "label": "google.cloud",
            "slug": "google-cloud"
          },
          {
            "id": 1420,
            "label": "spanner",
            "slug": "spanner"
          },
          {
            "id": 1421,
            "label": "strata",
            "slug": "strata"
          }
        ],
        "is_public": false,
        "id": 12708,
        "uid": null,
        "title": "Apache Cassandra vs Other Cloud Databases | DataStax",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=G8xHwnCevYo",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/G8xHwnCevYo?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-11-09T20:21:28+0000",
        "updated_at": "2018-11-09T20:21:56+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/G8xHwnCevYo/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12708"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 568,
            "label": "azure",
            "slug": "azure"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          },
          {
            "id": 994,
            "label": "dynamo",
            "slug": "dynamo"
          },
          {
            "id": 1017,
            "label": "cosmos",
            "slug": "cosmos"
          },
          {
            "id": 1420,
            "label": "spanner",
            "slug": "spanner"
          }
        ],
        "is_public": false,
        "id": 12707,
        "uid": null,
        "title": "Cassandra%20versus%20cloud%20databases%20Presentation.pdf",
        "url": "https://cdn.oreillystatic.com/en/assets/1/event/278/Cassandra%20versus%20cloud%20databases%20Presentation.pdf",
        "content": null,
        "created_at": "2018-11-09T20:19:13+0000",
        "updated_at": "2018-11-09T20:19:27+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": null,
        "language": null,
        "reading_time": 0,
        "domain_name": "cdn.oreillystatic.com",
        "preview_picture": null,
        "http_status": null,
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12707"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 11,
            "label": "database",
            "slug": "database"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 894,
            "label": "distributed",
            "slug": "distributed"
          }
        ],
        "is_public": false,
        "id": 12706,
        "uid": null,
        "title": "scalar-labs/scalardb",
        "url": "https://github.com/scalar-labs/scalardb",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\"><h2><a id=\"user-content-scalar-db\" class=\"anchor\" aria-hidden=\"true\" href=\"#scalar-db\"></a>Scalar DB</h2>\n<p><a href=\"https://circleci.com/gh/scalar-labs/scalardb/tree/master\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/684963021e0f453f74ed8e49e66799d1e938248b/68747470733a2f2f636972636c6563692e636f6d2f67682f7363616c61722d6c6162732f7363616c617264622f747265652f6d61737465722e7376673f7374796c653d73766726636972636c652d746f6b656e3d36373266373063653766326334663864396537316637633964623861653832346532636661656361\" alt=\"CircleCI\" data-canonical-src=\"https://circleci.com/gh/scalar-labs/scalardb/tree/master.svg?style=svg&amp;circle-token=672f70ce7f2c4f8d9e71f7c9db8ae824e2cfaeca\" /></a></p>\n<p>A library that provides a distributed storage abstraction and client-coordinated distributed transaction manager on the storage, and makes non-ACID distributed database/storage ACID-compliant.</p>\n<h2><a id=\"user-content-docs\" class=\"anchor\" aria-hidden=\"true\" href=\"#docs\"></a>Docs</h2>\n<ul><li><a href=\"https://github.com/scalar-labs/scalardb/blob/master/docs/getting-started.md\">Getting started</a></li>\n<li><a href=\"https://github.com/scalar-labs/scalardb/blob/master/docs/design.md\">Design document</a></li>\n<li><a href=\"https://scalar-labs.github.io/scalardb/javadoc/\" rel=\"nofollow\">Javadoc</a></li>\n<li>Sample applications by 3rd party developers (coming soon)</li>\n</ul><h2><a id=\"user-content-contributing\" class=\"anchor\" aria-hidden=\"true\" href=\"#contributing\"></a>Contributing</h2>\n<p>This library is mainly maintained by the Scalar Engineering Team, but of course we appreciate any help.</p>\n<ul><li>For asking questions, finding answers and helping other users, please use the public mailing list <a href=\"mailto:scalardb-user@googlegroups.com\">scalardb-user@googlegroups.com</a> or go to <a href=\"https://groups.google.com/forum/#!forum/scalardb-user\" rel=\"nofollow\">https://groups.google.com/forum/#!forum/scalardb-user</a>.</li>\n<li>For filing bugs, suggesting improvements, or requesting new features, help us out by opening an issue.</li>\n</ul><h2><a id=\"user-content-license\" class=\"anchor\" aria-hidden=\"true\" href=\"#license\"></a>License</h2>\n<p>Scalar DB is dual-licensed under both the AGPL (found in the LICENSE file in the root directory) and commercial license. You may select, at your option, one of the above-listed licenses. Regarding the commercial license, <a href=\"https://scalar-labs.com/contact_us/\" rel=\"nofollow\">contact us</a> for more information.</p>\n</article>",
        "created_at": "2018-11-09T14:49:44+0000",
        "updated_at": "2018-11-09T14:49:55+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/40349669?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12706"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12528,
        "uid": null,
        "title": "apache-cassandra",
        "url": "https://discovery.hgdata.com/product/apache-cassandra",
        "content": null,
        "created_at": "2018-10-30T18:46:18+0000",
        "updated_at": "2018-10-30T20:19:24+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": null,
        "language": null,
        "reading_time": 0,
        "domain_name": "discovery.hgdata.com",
        "preview_picture": null,
        "http_status": null,
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12528"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1347,
            "label": "architecture",
            "slug": "architecture"
          }
        ],
        "is_public": false,
        "id": 12466,
        "uid": null,
        "title": "Cassandra Architecture and Write Path Anatomy | Jorge Acetozi - Books, Courses and Notes",
        "url": "https://www.jorgeacetozi.com/single-post/cassandra-architecture-and-write-path-anatomy?_escaped_fragment_=",
        "content": "<img itemprop=\"image\" width=\"570.35\" title=\"Cassandra\" src=\"https://static.wixstatic.com/media/fcd55e_3923bbb56b00476692f2bb04af444cee~mv2.png/v1/fill/w_570,h_382/fcd55e_3923bbb56b00476692f2bb04af444cee~mv2.png\" height=\"382.2666666666667\" alt=\"Cassandra\" /><p>Cassandra is a NoSQL database that belongs to the Column Family NoSQL database category. It's an\n                        \n                            <a target=\"_blank\" href=\"http://cassandra.apache.org/\" shape=\"rect\">\n                                Apache project\n                            </a>\n                        \n                        and it has an Enterprise version maintained by\n                        \n                            <a target=\"_blank\" href=\"https://www.datastax.com/\" shape=\"rect\">\n                                DataStax\n                            </a>\n                        \n                        . Cassandra is written in Java and it's mainly used for time-series data such as metrics, IoT (Internet of Things), logs, chat messages, and so on. It is able to handle a massive amount of writes and reads and scale to thousands of nodes. Let's list out here some important Cassandra characteristics. Basically, Cassandra...</p><ul class=\"font_8\"><li>\n                        <p>mix ideas from Google's Big Table and Amazon's Dynamo.</p>\n                    </li>\n                    <li>\n                        <p>is based on\n                                \n                                    <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Peer-to-peer\" shape=\"rect\">\n                                        peer-to-peer\n                                    </a>\n                                \n                                architecture. Every node is equal (can perform both reads and writes), therefore there is no master or slave node, that is, there is no master single point of failure.</p>\n                    </li>\n                    <li>\n                        <p>does automatic partitioning and replication.</p>\n                    </li>\n                    <li>\n                        <p>has tunable write and read consistency for both read and write operations.</p>\n                    </li>\n                    <li>\n                        <p>is able to horizontally scale keeping\n                                \n                                    <a target=\"_blank\" href=\"https://medium.com/netflix-techblog/benchmarking-cassandra-scalability-on-aws-over-a-million-writes-per-second-39f45f066c9e\" shape=\"rect\">\n                                        linear scalability\n                                    </a>\n                                \n                                for both reads and writes.</p>\n                    </li>\n                    <li>\n                        <p>handles inter-node communication through the\n                                \n                                    <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Gossip_protocol\" shape=\"rect\">\n                                        Gossip protocol\n                                    </a>\n                                \n                                .</p>\n                    </li>\n                    <li>\n                        <p>handles client communication through the CQL (Cassandra Query Language), which is very similar to SQL.</p>\n                    </li>\n                </ul><p>Coordinator</p><p>When a request is sent to\n                        any\n                        Cassandra node, this node acts as a proxy between the application (actually, the Cassandra driver) and the nodes involved in the request flow. This proxy node is called as the\n                        coordinator\n                        . The coordinator is responsible for managing the entire request path and to respond back to the client.</p><div><img width=\"738.65\" title=\"Coordinator\" src=\"https://static.wixstatic.com/media/fcd55e_d58f6f89b5bd47478c819e946e7928b3~mv2.png/v1/fill/w_739,h_416/fcd55e_d58f6f89b5bd47478c819e946e7928b3~mv2.png\" height=\"415.7375\" alt=\"Coordinator\" /></div><p>Figure 1 - Coordinator</p><p>Besides, sometimes when the coordinator forwards a write request to the replica nodes, they may happen to be unavailable at this very moment. In this case, the coordinator plays an important role implementing a mechanism called Hinted Handoff, which will be described in details later.</p><p>Partitioner</p><p>Basically, for each node in the Cassandra cluster (Cassandra ring) is assigned a range of tokens as shown in Figure 2 for a 6-node cluster (with imaginary tokens, of course).</p><div><img width=\"589.05\" title=\"Token Ranges\" src=\"https://static.wixstatic.com/media/fcd55e_c3532c858e644de0a3cc9dbb718ed5a6~mv2.png/v1/fill/w_589,h_460/fcd55e_c3532c858e644de0a3cc9dbb718ed5a6~mv2.png\" height=\"459.8723684210526\" alt=\"\" /></div><p>Figure 2 - Token Ranges</p><p>Cassandra distributes data across the cluster using a\n                        \n                            <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Consistent_hashing\" shape=\"rect\">\n                                Consistent Hashing\n                            </a>\n                        \n                        algorithm and, starting from version 1.2, it also implements the concept of\n                        virtual nodes\n                        (vnodes), where each node owns a large number of small token ranges in order to improve token reorganization and avoid hotspots in the cluster, that is, some nodes storing much more data than the others. Virtual nodes also allows to add and remove nodes in the cluster more easily and manages the token assignment automatically for you so that you can enjoy a nice coffee when adding or removing a node instead of calculating and assigning new token ranges for each node (which is a very error prone operation, by the way). </p><p><br style=\"clear: none;\" />\n                    \n                        Well, that said, the partitioner is the component responsible for determining how to distribute the data across the nodes in the cluster given the\n                        \n                            <a target=\"_blank\" href=\"https://dzone.com/articles/cassandra-data-modeling-primary-clustering-partiti\" shape=\"rect\">\n                                partition key\n                            </a>\n                        \n                         of a row. Basically, it is a hash function for computing a token given the partition key.</p><p>Once the partitioner applies the hash function to the partition key and gets the token, it knows exactly which node is going to handle the request.</p><p>Let's consider a simple example: suppose a request is issued to node6 (that is, node6 is the coordinator for this request) with a row containing the partition key \"jorge_acetozi\". Suppose the partitioner applies the hash function to the partition key \"jorge_acetozi\" and gets the token -17. As figure 3 shows, node2 token ranges includes -17, so this node will be the one handling the request.</p><div><img width=\"897.6\" title=\"Partitioner\" src=\"https://static.wixstatic.com/media/fcd55e_11ca42aad78744a48759dd9908cdbef5~mv2.png/v1/fill/w_898,h_485/fcd55e_11ca42aad78744a48759dd9908cdbef5~mv2.png\" height=\"485.1891891891892\" alt=\"Partitioner\" /></div><p>Figure 3 - Partitioner</p><p>Cassandra offers three types of partitioners:\n                        \n                            <a target=\"_blank\" href=\"https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/dht/Murmur3Partitioner.java\" shape=\"rect\">\n                                Murmur3Partitioner\n                            </a>\n                        \n                        (which is the default),\n                        \n                            <a target=\"_blank\" href=\"https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/dht/RandomPartitioner.java\" shape=\"rect\">\n                                RandomPartitioner\n                            </a>\n                        \n                        , and\n                        \n                            <a target=\"_blank\" href=\"https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/dht/ByteOrderedPartitioner.java\" shape=\"rect\">\n                                ByteOrderedPartitioner\n                            </a>\n                        \n                        .</p><p>Replication</p><p>Life would be much easier if...</p><ul class=\"font_8\"><li>\n                        <p>Nodes never fail</p>\n                    </li>\n                    <li>\n                        <p>Networks had no latency</p>\n                    </li>\n                    <li>\n                        <p>People did not stumble on cables</p>\n                    </li>\n                    <li>\n                        <p>Amazon did not restart your instances</p>\n                    </li>\n                    <li>\n                        <p>Full GC meant \"Full Guitar Concert\"</p>\n                    </li>\n                </ul><p>And so on. Unfortunately, these things happen all the time and you already chose a software engineer career (your mother used to advise you to study hard and to become a doctor, but you chose to keep playing Counter-Strike instead. Now you are a software engineer, know what AK-47 means and have to care about stuff like that).</p><p>Fortunately, Cassandra offers automatic data replication and keeps your data redundant throughout different nodes in the cluster. This means that (in certain levels) you can even resist to node failure scenarios and your data would still be safe and available. But everything comes at a price, and the price of replication is consistency.</p><p>Replication Strategy</p><p>Basically, the coordinator uses the Replication Strategy to find out which nodes will be the replica nodes for a given request.</p><p>There are two replication strategies available:</p><p><a target=\"_blank\" href=\"https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/locator/SimpleStrategy.java\" shape=\"rect\">\n                                SimpleStrategy\n                            </a>\n                        \n                        : used for a single data center deployment (not recommended for production environment). It doesn't consider the network topology. Basically, it just takes the partitioner's decision (that is, the node that will handle the request first based on the token range) and places the remaining replicas clockwise in relation to this node. For example, in Figure 3, if the table replication factor was 3, which nodes would have been chosen by the SimpleStrategy to act as replicas (besides node2, which was already chosen by the partitioner)? That's correct, node3 and node4! What if the replication factor was 4? Well, then node5 would also be included.</p><p><a target=\"_blank\" href=\"https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/locator/NetworkTopologyStrategy.java\" shape=\"rect\">\n                                NetworkTopologyStrategy\n                            </a>\n                        \n                        : used for multiple data centers deployment (recommended for production environment). It also takes the partitioner's decision and places the remaining replicas clockwise, but it also takes into consideration the rack and data centers configuration.</p><p>Replication Factor</p><p>When you create a table (Column Family) in Cassandra, you specify the replication factor. The replication factor is the number of replicas that Cassandra will hold for this table in different nodes. If you specify REPLICATION_FACTOR=3, then your data will be replicated to 3 different nodes throughout the cluster. That provides fault tolerance and resilience because even if some nodes fail your data would still be safe and available.</p><p>Write Consistency Level</p><p>Do you still remember that when the client sends a request to a Cassandra node, this node is called a coordinator and acts as a proxy between the client and the replica nodes?</p><p>Well, when you write to a table in Cassandra (inserting data, for example), you can specify the write consistency level. The write consistency level is the number of replica nodes that have to acknowledge the coordinator that its local insert was successful (success here means that the data was appended to the commit log and written to the memtable). As soon as the coordinator gets WRITE_CONSISTENCY_LEVEL success acknowledgments from the replica nodes, it returns success back to the client and doesn't wait for the remaining replicas to acknowledge success.</p><p>For example, if an application issue an insert request with WRITE_CONSISTENCY_LEVEL=TWO to a table that is configured with REPLICATION_FACTOR=3, the coordinator will only return success to the application when two of the three replicas acknowledge success. Of course, this doesn't mean that the third replica will not write the data too; it will, but at this point, the coordinator would already have sent success back to the client.</p><p>There are many different types of write consistency levels you can specify in your write request. From the less consistent to full consistency: ANY, ONE, TWO, THREE, QUORUM, LOCAL_QUORUM, EACH_QUORUM, ALL. </p><p>Write Flow Example</p><p>For simplicity, suppose a write request is issued to a 6-node Cassandra cluster with the following characteristics:</p><p>WRITE_CONSISTENCY_LEVEL=TWO\n                            <br style=\"clear: none;\" />\n                            TABLE_REPLICATION_FACTOR=3\n                            <br style=\"clear: none;\" />\n                            REPLICATION_STRATEGY=SimpleStrategy\n                        \n                    \n                    <br style=\"clear: none;\" />\n                     </p><p>First, the client sends the write request to the Cassandra cluster using the driver. We haven't discussed the role of the driver in this post (maybe in another post), but it plays a very important role as well. The driver is responsible for a lot of features such as asynchronous IO, parallel execution, request pipelining, connection pooling, auto node discovery, automatic reconnection, token awareness, and so on. For example, by using a driver that implements a token-aware policy, the driver reduces network hops by sending requests directly to the node that owns the data instead of sending it to a \"random\" coordinator.</p><p>As soon as the coordinator gets the write request, it applies the partitioner hash function to the partition key\n                        and\n                        uses the configured Replication Strategy in order to determine the TABLE_REPLICATION_FACTOR replica nodes that will actually write the data (in this sentence, replace TABLE_REPLICATION_FACTOR with the number 3). Figure 4 shows the replica nodes (in green) that will handle the write request.</p><div><img width=\"897.6\" title=\"Replica Nodes\" src=\"https://static.wixstatic.com/media/fcd55e_e2d1c28b9b494dea9c6372622f09dd19~mv2.png/v1/fill/w_898,h_485/fcd55e_e2d1c28b9b494dea9c6372622f09dd19~mv2.png\" height=\"485.1891891891892\" alt=\"Replica Nodes\" /></div><p>Figure 4 - Replica Nodes</p><p>Now, before the coordinator forwards the write request to all the 3 replica nodes, it will ask to the\n                        \n                            <a target=\"_blank\" href=\"https://github.com/jorgeacetozi/cassandra/blob/trunk/src/java/org/apache/cassandra/gms/FailureDetector.java\" shape=\"rect\">\n                                Failure Detector component\n                            </a>\n                        \n                        how many of these replica nodes are actually available and compare it to the WRITE_CONSISTENCY_LEVEL provided in the request. If the number of replica nodes available is less than the WRITE_CONSISTENCY_LEVEL provided, the Failure Detector will immediately throw an Exception.</p><p>For our example, suppose the 3 replica nodes are available (that is, the Failure Detector will allow the request to continue) such as shown in Figure 5. Now, the coordinator will asynchronously forward the write request to\n                        all\n                        the replica nodes (in these case, the 3 replica nodes that were figured in the first step). As soon as WRITE_CONSISTENCY_LEVEL replica nodes acknowledge success (node2 and node4), the coordinator returns success back to the driver.</p><div><img width=\"897.6\" title=\"Write Success\" src=\"https://static.wixstatic.com/media/fcd55e_94c355b7090f4a85b15f5ecc50c499f2~mv2.png/v1/fill/w_898,h_485/fcd55e_94c355b7090f4a85b15f5ecc50c499f2~mv2.png\" height=\"485.1891891891892\" alt=\"Write Success\" /></div><p>Figure 5 - Write Success</p><p>If the WRITE_CONSISTENCY_LEVEL for this request was THREE (or ALL), the coordinator would have to wait until node3 acknowledges success too, and of course that this write request would be slower.</p><p>So, basically...</p><ul class=\"font_8\"><li>\n                        <p>Do you need fault tolerance and high availability? Use replication.</p>\n                    </li>\n                    <li>\n                        <p>Just keep in mind that using replication means you will pay with consistency (for most of the cases, this is not a problem. Availability is often more important than consistency).</p>\n                    </li>\n                    <li>\n                        <p>If consistency is not an issue for your domain, perfect. If it is, just increase the consistency level, but then you will pay with higher latency.</p>\n                    </li>\n                    <li>\n                        <p>If you want fault tolerance and high availability, strong consistency and low latency, then you should be the client, not the software engineer (Lol).</p>\n                    </li>\n                </ul><p>Hinted Handoff</p><p>Suppose in the last example that only 2 of 3 replica nodes were available. In this case, the Failure Detector would still allow the request to continue as the number of available replica nodes is not less than the WRITE_CONSISTENCY_LEVEL provided. In this case, the coordinator would behave exactly as described before but there would be one additional step. The coordinator would write locally the hint (the write request blob along with some metadata) in the disk (hints\n                         \n                        directory) and would keep the hint there for 3 hours (by default) waiting for the replica node to become available again. If the replica node recovers within this period, the coordinator will send the hint to the replica node so that it can update itself and become consistent with the other replicas. If the replica node is offline for more than 3 hours, then a read repair is needed. This process is referred as Hinted Handoff.\n                    \n                    <br style=\"clear: none;\" />\n                    \n                        \n                            Write Internals</p><p>In short, when a write request reaches a node, mainly two things happen:</p><ol class=\"font_8\"><li>\n                        <p>The write request is appended to the commit log in the disk. This ensures data durability (the write request data would permanently survive even in a node failure scenario)</p>\n                    </li>\n                    <li>\n                        <p>The write request is sent to the memtable (a structure stored in the memory). When the memtable is full, the data is flushed to a SSTable on disk using sequential I/O and the data in the commit log is purged.</p>\n                    </li>\n                </ol><div><img width=\"570.35\" title=\"Cassandra Node Internals\" src=\"https://static.wixstatic.com/media/fcd55e_7db3565284b1438b90ab7f3459df375b~mv2_d_1717_1371_s_2.png/v1/fill/w_570,h_456/fcd55e_7db3565284b1438b90ab7f3459df375b~mv2_d_1717_1371_s_2.png\" height=\"456.4833333333334\" alt=\"\" /></div><p>Figure 6 - Cassandra Node Internals</p><p>I really hope this article has been useful to you. If you enjoyed reading it, let me know if you would like to read another article diving into the Read Request Path. The Read Request Path is a little bit more complicated as it involves Snitches, Bloom Filter, Indexes, and so on, but it's pretty interesting as well.</p><p>If you are a software developer interested in how to use Cassandra in a realistic scenario coding a real-time chat application from the scratch, please take a look at my book: \n                        \n                            <a target=\"_blank\" href=\"https://www.jorgeacetozi.com/books\" shape=\"rect\">\n                                Pro Java Clustering and Scalability: Building Real-Time Apps with Spring, Cassandra, Redis, WebSocket and RabbitMQ\n                            </a></p><p>Thank you very much.</p>",
        "created_at": "2018-10-24T18:11:57+0000",
        "updated_at": "2018-10-24T18:12:10+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 10,
        "domain_name": "www.jorgeacetozi.com",
        "preview_picture": "https://static.wixstatic.com/media/fcd55e_3923bbb56b00476692f2bb04af444cee%7Emv2.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12466"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 901,
            "label": "security",
            "slug": "security"
          },
          {
            "id": 1303,
            "label": "ssl",
            "slug": "ssl"
          }
        ],
        "is_public": false,
        "id": 12465,
        "uid": null,
        "title": "cloudurable/cassandra-image",
        "url": "https://github.com/cloudurable/cassandra-image/wiki/Cassandra-Tutorial-2:-Setting-up-client-and-cluster-SSL-transport-in-Cassandra",
        "content": "<div class=\"markdown-body\">\n          <p>Cassandra allows you to secure the <code>client transport</code> (CQL) as well as the cluster transport (storage transport).</p>\n<p>SSL/TLS have some overhead. This is especially true in the JVM world which is not as performant for handling SSL/TLS unless you are using Netty/OpenSSl integration.</p>\n<p>If possible, use no encryption for the <code>cluster transport</code> (<code>storage transport</code>), and deploy your Cassandra nodes in a private subnet, and limit access to this subnet to the <code>client transport</code>. Also if possible avoid using TLS/SSL on the client transport and do client operations from your app tier, which is located in a non-public subnet.</p>\n<p>However, that is not always possible. You may work in an industry that requires the use of encrypted transports like the U.S. Health Insurance Portability and Accountability Act (HIPAA), Germany’s Federal Data Protection Act,\nThe Payment Card Industry Data Security Standard (PCI DSS), or U.S. Sarbanes-Oxley Act of 2002. Or you might work for a bank or other financial institution. Or it just might be a corporate policy to encrypt such transports.</p>\n<p>Another area of concern is for compliance is authorization, and encrypted data at rest. Cassandra’s has essential security features: authentication, role-based authorization, transport encryption (JMX, client transport, cluster transport), as well as data at rest encryption (encrypting SSTables).</p>\n<p>This article will focus just on setting up encryption for the Cassandra <code>client transport</code> (CQL) and the <code>cluster transport</code>. Later articles will cover various aspects of compliance and encryption.</p>\n<h4>\n<a id=\"user-content-encrypting-the-transports\" class=\"anchor\" href=\"#encrypting-the-transports\" aria-hidden=\"true\"></a>Encrypting the transports</h4>\n<p>Data that travels over the <code>client transport</code> across a network could be accessed by someone you don't want accessing said data with tools like <a href=\"https://www.wireshark.org/\" rel=\"nofollow\">wire shark</a>. If data includes private information, SSN number, credentials (password, username), credit card numbers or account numbers, then we want to make that data unintelligible (encrypted) to any and all 3rd parties. This is especially important if we don't control the network. You can also use TLS to make sure the data has not been tampered with whilst traveling the network. The Secure Sockets Layer (SSL) and Transport Layer Security (TLS) protocols are designed to provide these features (SSL is the old name for what became TLS but many people still refer to TLS as SSL).</p>\n<p>Cassandra is written in Java. Java defines the JSSE framework which in turn uses the <a href=\"http://docs.oracle.com/javase/8/docs/technotes/guides/security/crypto/CryptoSpec.html#Design\" rel=\"nofollow\">Java Cryptography Architecture (JCA)</a>. JSSE uses cryptographic service providers from JCA.</p>\n<p>If any of the above is new to you, please take a few minutes to read through the [TLS/SSL Java guide] (<a href=\"http://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/JSSERefGuide.html\" rel=\"nofollow\">http://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/JSSERefGuide.html</a>).</p>\n<h2>\n<a id=\"user-content-setting-up-client-config\" class=\"anchor\" href=\"#setting-up-client-config\" aria-hidden=\"true\"></a>Setting up client config</h2>\n<p>The client transport encryption protects data as it moves from clients to server nodes in the cluster.</p>\n<p>The <code>client_encryption_options</code> are stored in the cassandra.yaml.\nHere is an example config.</p>\n<h4>\n<a id=\"user-content-sample-config\" class=\"anchor\" href=\"#sample-config\" aria-hidden=\"true\"></a>Sample config</h4>\n<pre>\n# enable or disable client/server encryption.\nclient_encryption_options:\n    enabled: false\n    # If enabled and optional is set to true encrypted and unencrypted connections are handled.\n    optional: false\n    keystore: conf/.keystore\n    keystore_password: cassandra\n    require_client_auth: false\n    # Set trustore and truststore_password if require_client_auth is true\n    # truststore: conf/.truststore\n    # truststore_password: cassandra\n    protocol: TLS\n    algorithm: SunX509\n    store_type: JKS\n    cipher_suites: [TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDH_anon_WITH_AES_256_CBC_SHA]\n</pre>\n<h2>\n<a id=\"user-content-setup-keys\" class=\"anchor\" href=\"#setup-keys\" aria-hidden=\"true\"></a>Setup keys</h2>\n<p>Before we go into the details of setting up the cassandra.yaml file, let's create some trust stores, key stores, and export some keys. The following script generates cluster and client keys.</p>\n<h4>\n<a id=\"user-content-setupkeys-cassandra-securitysh\" class=\"anchor\" href=\"#setupkeys-cassandra-securitysh\" aria-hidden=\"true\"></a>setupkeys-cassandra-security.sh</h4>\n<div class=\"highlight highlight-source-shell\"><pre>#!/bin/bash\nKEY_STORE_PATH=\"$PWD/resources/opt/cassandra/conf/certs\"\nmkdir -p \"$KEY_STORE_PATH\"\nKEY_STORE=\"$KEY_STORE_PATH/cassandra.keystore\"\nPKS_KEY_STORE=\"$KEY_STORE_PATH/cassandra.pks12.keystore\"\nTRUST_STORE=\"$KEY_STORE_PATH/cassandra.truststore\"\nPASSWORD=cassandra\nCLUSTER_NAME=test\nCLUSTER_PUBLIC_CERT=\"$KEY_STORE_PATH/CLUSTER_${CLUSTER_NAME}_PUBLIC.cer\"\nCLIENT_PUBLIC_CERT=\"$KEY_STORE_PATH/CLIENT_${CLUSTER_NAME}_PUBLIC.cer\"\n### Cluster key setup.\n# Create the cluster key for cluster communication.\nkeytool -genkey -keyalg RSA -alias \"${CLUSTER_NAME}_CLUSTER\" -keystore \"$KEY_STORE\" -storepass \"$PASSWORD\" -keypass \"$PASSWORD\" \\\n-dname \"CN=CloudDurable Image $CLUSTER_NAME cluster, OU=Cloudurable, O=Cloudurable, L=San Francisco, ST=CA, C=USA, DC=cloudurable, DC=com\" \\\n-validity 36500\n# Create the public key for the cluster which is used to identify nodes.\nkeytool -export -alias \"${CLUSTER_NAME}_CLUSTER\" -file \"$CLUSTER_PUBLIC_CERT\" -keystore \"$KEY_STORE\" \\\n-storepass \"$PASSWORD\" -keypass \"$PASSWORD\" -noprompt\n# Import the identity of the cluster public cluster key into the trust store so that nodes can identify each other.\nkeytool -import -v -trustcacerts -alias \"${CLUSTER_NAME}_CLUSTER\" -file \"$CLUSTER_PUBLIC_CERT\" -keystore \"$TRUST_STORE\" \\\n-storepass \"$PASSWORD\" -keypass \"$PASSWORD\" -noprompt\n### Client key setup.\n# Create the client key for CQL.\nkeytool -genkey -keyalg RSA -alias \"${CLUSTER_NAME}_CLIENT\" -keystore \"$KEY_STORE\" -storepass \"$PASSWORD\" -keypass \"$PASSWORD\" \\\n-dname \"CN=CloudDurable Image $CLUSTER_NAME client, OU=Cloudurable, O=Cloudurable, L=San Francisco, ST=CA, C=USA, DC=cloudurable, DC=com\" \\\n-validity 36500\n# Create the public key for the client to identify itself.\nkeytool -export -alias \"${CLUSTER_NAME}_CLIENT\" -file \"$CLIENT_PUBLIC_CERT\" -keystore \"$KEY_STORE\" \\\n-storepass \"$PASSWORD\" -keypass \"$PASSWORD\" -noprompt\n# Import the identity of the client pub  key into the trust store so nodes can identify this client.\nkeytool -importcert -v -trustcacerts -alias \"${CLUSTER_NAME}_CLIENT\" -file \"$CLIENT_PUBLIC_CERT\" -keystore \"$TRUST_STORE\" \\\n-storepass \"$PASSWORD\" -keypass \"$PASSWORD\" -noprompt\nkeytool -importkeystore -srckeystore \"$KEY_STORE\" -destkeystore \"$PKS_KEY_STORE\" -deststoretype PKCS12 \\\n-srcstorepass \"$PASSWORD\" -deststorepass \"$PASSWORD\"\nopenssl pkcs12 -in \"$PKS_KEY_STORE\" -nokeys -out \"${CLUSTER_NAME}_CLIENT.cer.pem\" -passin pass:cassandra\nopenssl pkcs12 -in \"$PKS_KEY_STORE\" -nodes -nocerts -out \"${CLUSTER_NAME}_CLIENT.key.pem\" -passin pass:cassandra\n</pre></div>\n<p>The <a href=\"https://docs.oracle.com/javase/8/docs/technotes/tools/unix/keytool.html\" rel=\"nofollow\">keytool</a> utility ships with Java SDK. We use this keytool command to create the cluster key. Let's break down the script that generates the keys and certificates.</p>\n<h4>\n<a id=\"user-content-create-the-cluster-key\" class=\"anchor\" href=\"#create-the-cluster-key\" aria-hidden=\"true\"></a>Create the cluster key</h4>\n<pre>keytool -genkey -keyalg RSA -alias \"${CLUSTER_NAME}_CLUSTER\" -keystore \"$KEY_STORE\" -storepass \"$PASSWORD\" -keypass \"$PASSWORD\" \\\n-dname \"CN=CloudDurable Image $CLUSTER_NAME cluster, OU=Cloudurable, O=Cloudurable, L=San Francisco, ST=CA, C=USA, DC=cloudurable, DC=com\" \\\n-validity 36500\n</pre>\n<p>Once we create the cluster key, we will want to export a public key from it.</p>\n<h4>\n<a id=\"user-content-export-a-public-key-for-the-cluster-key\" class=\"anchor\" href=\"#export-a-public-key-for-the-cluster-key\" aria-hidden=\"true\"></a>Export a public key for the cluster key.</h4>\n<pre># Create the public key for the client to identify itself.\nkeytool -export -alias \"${CLUSTER_NAME}_CLIENT\" -file \"$CLIENT_PUBLIC_CERT\" -keystore \"$KEY_STORE\" \\\n-storepass \"$PASSWORD\" -keypass \"$PASSWORD\" -noprompt\n</pre>\n<p>Then we will import the public key into the trust store so that nodes can identify each other.</p>\n<h4>\n<a id=\"user-content-import-public-key-for-the-cluster-key-into-the-trust-store-so-nodes-can-identify-each-other\" class=\"anchor\" href=\"#import-public-key-for-the-cluster-key-into-the-trust-store-so-nodes-can-identify-each-other\" aria-hidden=\"true\"></a>Import public key for the cluster key into the trust store so nodes can identify each other</h4>\n<pre># Import the identity of the cluster public cluster key into the trust store so that nodes can identify each other.\nkeytool -import -v -trustcacerts -alias \"${CLUSTER_NAME}_CLUSTER\" -file \"$CLUSTER_PUBLIC_CERT\" -keystore \"$TRUST_STORE\" \\\n-storepass \"$PASSWORD\" -keypass \"$PASSWORD\" -noprompt\n</pre>\n<p>We perform the same three tasks for the client keys. Then lastly we create pem files for the client keys by exporting our Java JKS keystore as a PKCS12 trust store.</p>\n<h4>\n<a id=\"user-content-creating-client-pem-files\" class=\"anchor\" href=\"#creating-client-pem-files\" aria-hidden=\"true\"></a>Creating client pem files</h4>\n<pre>keytool -importkeystore -srcalias \"${CLUSTER_NAME}_CLIENT\" -srckeystore \"$KEY_STORE\" -destkeystore \"$PKS_KEY_STORE\" -deststoretype PKCS12 \\\n-srcstorepass \"$PASSWORD\" -deststorepass \"$PASSWORD\"\nopenssl pkcs12 -in \"$PKS_KEY_STORE\" -nokeys -out \"$KEY_STORE_PATH/${CLUSTER_NAME}_CLIENT.cer.pem\" -passin pass:cassandra\nopenssl pkcs12 -in \"$PKS_KEY_STORE\" -nodes -nocerts -out \"$KEY_STORE_PATH/${CLUSTER_NAME}_CLIENT.key.pem\" -passin pass:cassandra\n</pre>\n<p>Here are the files that get generated.</p>\n<h4>\n<a id=\"user-content-cert-files-stores-private-keys\" class=\"anchor\" href=\"#cert-files-stores-private-keys\" aria-hidden=\"true\"></a>Cert files, stores, private keys</h4>\n<pre>$ pwd\n~/github/cassandra-image\n$ ls resources/opt/cassandra/conf/certs/\nCLIENT_test_PUBLIC.cer\t\tcassandra.pks12.keystore\ttest_CLIENT.key.pem\nCLUSTER_test_PUBLIC.cer\t\tcassandra.truststore\ncassandra.keystore\t\ttest_CLIENT.cer.pem\n</pre>\n<ul><li>\n<code>CLIENT_test_PUBLIC.cer</code>     public client key for the test cluster.</li>\n<li>\n<code>cassandra.pks12.keystore</code>   PKS12 keystore for client used to generate pem</li>\n<li>\n<code>test_CLIENT.key.pem</code>        private client key in pem format used by csqlsh</li>\n<li>\n<code>CLUSTER_test_PUBLIC.cer</code>    public cluster key for the test cluster</li>\n<li>\n<code>cassandra.truststore</code>       Trust store used by cassandra</li>\n<li>\n<code>cassandra.keystore</code>         Key store used by cassandra</li>\n<li>\n<code>test_CLIENT.cer.pem</code>        public client key in pem format used by csqlsh</li>\n</ul><p>For the <a href=\"https://github.com/cloudurable/cassandra-image\">cassandra_image</a> project, these files are copied to <code>/opt/cassandra/conf/cert</code>. To learn more about our Vagrant project see <a href=\"http://cloudurable.com/blog/cassandra-image-vagrant-cluster-example/index.html\" rel=\"nofollow\">Setting up a Cassandra cluster with cassandra image and cassandra cloud project with Vagrant</a>.</p>\n<h2>\n<a id=\"user-content-use-the-keys-that-we-setup\" class=\"anchor\" href=\"#use-the-keys-that-we-setup\" aria-hidden=\"true\"></a>Use the keys that we setup.</h2>\n<p>As part of the provision script for <a href=\"https://github.com/cloudurable/cassandra-image\">cassandra_image</a>(see <a href=\"http://cloudurable.com/blog/cassandra-image-vagrant-cluster-example/index.html\" rel=\"nofollow\">Setting up a Cassandra cluster with cassandra image and cassandra cloud project with Vagrant</a>). We added the following:</p>\n<h4>\n<a id=\"user-content-cassandra-imagescripts040-install-certssh\" class=\"anchor\" href=\"#cassandra-imagescripts040-install-certssh\" aria-hidden=\"true\"></a>cassandra-image/scripts/040-install-certs.sh</h4>\n<pre>#!/bin/bash\nset -e\nDESTINATION_DIRECTORY=/opt/cassandra/conf/certs\nSOURCE_DIRECTORY=\"~/resources$DESTINATION_DIRECTORY\"\nif [ -d \"$SOURCE_DIRECTORY\" ]; then\n    mkdir -p \"$DESTINATION_DIRECTORY\"\n    cp -r \"$SOURCE_DIRECTORY\" \"$DESTINATION_DIRECTORY\"\nfi\nif [ ! -d \"$SOURCE_DIRECTORY\" ]; then\n    echo \"UNABLE TO INSTALL CERTS AS THEY WERE NOT FOUND\"\nfi\n</pre>\n<p>This will copy the certs to the right location if you generated a folder in resources (cassandra_image/resources/opt/cassandra/conf/cert), which the last <a href=\"https://github.com/cloudurable/cassandra-image/wiki/setupkeys-cassandra-security.sh\">script that we covered does</a>.</p>\n<h2>\n<a id=\"user-content-configure-cassandra-to-use-the-keys\" class=\"anchor\" href=\"#configure-cassandra-to-use-the-keys\" aria-hidden=\"true\"></a>Configure Cassandra to use the keys.</h2>\n<h4>\n<a id=\"user-content-optcassandraconf\" class=\"anchor\" href=\"#optcassandraconf\" aria-hidden=\"true\"></a>/opt/cassandra/conf</h4>\n<pre>\nserver_encryption_options:\n    internode_encryption: all\n    keystore: /opt/cassandra/conf/certs/cassandra.keystore\n    keystore_password: cassandra\n    truststore: /opt/cassandra/conf/certs/cassandra.truststore\n    truststore_password: cassandra\n    # More advanced defaults below:\n    protocol: TLS\nclient_encryption_options:\n    enabled: true\n    # If enabled and optional is set to true encrypted and unencrypted connections are handled.\n    optional: false\n    keystore: /opt/cassandra/conf/certs/cassandra.keystore\n    keystore_password: cassandra\n    truststore: /opt/cassandra/conf/certs/cassandra.truststore\n    truststore_password: cassandra\n    require_client_auth: true\n    protocol: TLS\n</pre>\n<p>Now let's test it.</p>\n<h4>\n<a id=\"user-content-testing-that-our-cassandra-nodes-can-talk-to-each-other\" class=\"anchor\" href=\"#testing-that-our-cassandra-nodes-can-talk-to-each-other\" aria-hidden=\"true\"></a>Testing that our Cassandra nodes can talk to each other</h4>\n<pre>$ vagrant up\n# Get a coffee and otherwise relax for a minute.\n# Now log into one of the nodes.\n$ vagrant ssh node0\n# Now check to see if the cluster is formed. \n[vagrant@localhost ~]$ /opt/cassandra/bin/nodetool describecluster\nCluster Information:\n\tName: test\n\tSnitch: org.apache.cassandra.locator.DynamicEndpointSnitch\n\tPartitioner: org.apache.cassandra.dht.Murmur3Partitioner\n\tSchema versions:\n\t\t86afa796-d883-3932-aa73-6b017cef0d19: [192.168.50.4, 192.168.50.5, 192.168.50.6]\n</pre>\n<p>We can see that the servers in the cluster can connect to each other (see <a href=\"http://cloudurable.com/blog/cassandra-image-vagrant-cluster-example/index.html\" rel=\"nofollow\">Setting up a Cassandra cluster with cassandra image and cassandra cloud project with Vagrant</a>).</p>\n<h2>\n<a id=\"user-content-setup-csql-client\" class=\"anchor\" href=\"#setup-csql-client\" aria-hidden=\"true\"></a>Setup csql client</h2>\n<p>We are doing this in OSX. In this example, we have the virtual machines running <em><strong>CentOS 7</strong></em> with <em><strong>Vagrant</strong></em> on <em><strong>VirtualBox</strong></em>. We can connect to those instances with <em><strong>Cassandra</strong></em> <code>cqlsh</code>.</p>\n<p>Let's copy cert files so we can access them from the client (MacBook pro / OSX).</p>\n<h4>\n<a id=\"user-content-copy-cert-files-created-earlier\" class=\"anchor\" href=\"#copy-cert-files-created-earlier\" aria-hidden=\"true\"></a>Copy cert files created earlier.</h4>\n<pre>$ cd ~/github/cassandra-image/resources/opt/cassandra/conf/certs\n$ mkdir /opt/cassandra/conf/certs\n$ cp * /opt/cassandra/conf/certs\n</pre>\n<p>Now we will create a <a href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_reference/cqlshUsingCqlshrc.html\" rel=\"nofollow\">cqlshrc</a> which is a file that dictates how we connect to Cassandra.</p>\n<h4>\n<a id=\"user-content-create-the-cqlshrc-in--cassandra\" class=\"anchor\" href=\"#create-the-cqlshrc-in--cassandra\" aria-hidden=\"true\"></a>Create the cqlshrc in  ~/.cassandra</h4>\n<pre>$ mkdir ~/.cassandra\n$ cd ~/.cassandra\n$ touch cqlshrc\n# edit this file\n</pre>\n<h4>\n<a id=\"user-content-cassandracqlshrc-contents\" class=\"anchor\" href=\"#cassandracqlshrc-contents\" aria-hidden=\"true\"></a>~/.cassandra/cqlshrc contents</h4>\n<pre>\n[connection]\nhostname = 192.168.50.4\nport = 9042\nfactory = cqlshlib.ssl.ssl_transport_factory\n[ssl]\ncertfile =  /opt/cassandra/conf/certs/test_CLIENT.cer.pem\nvalidate = false\n# Next 2 lines must be provided when require_client_auth = true in the cassandra.yaml file\nuserkey = /opt/cassandra/conf/certs/test_CLIENT.key.pem\nusercert = /opt/cassandra/conf/certs/test_CLIENT.cer.pem\n</pre>\n<p>Note we specify the nodes and we are using the pem file as our credentials via SSL to prove who we are instead of a username/password. (We could use both.) We need the <code>userkey</code> and <code>usercert</code> in the <code>cqlshrc</code> because we set <code>require_client_auth = true</code> in the <code>cassandra.yaml</code> file for the cluster nodes.</p>\n<p>Now let's test that the client connection works with SSL via <em><strong>cqlsh</strong></em>.</p>\n<h4>\n<a id=\"user-content-testing-client-connection-using-cqlsh\" class=\"anchor\" href=\"#testing-client-connection-using-cqlsh\" aria-hidden=\"true\"></a>Testing client connection using cqlsh</h4>\n<pre>$ /opt/cassandra/bin/cqlsh --ssl \nConnected to test at 192.168.50.4:9042.\n[cqlsh 5.0.1 | Cassandra 3.9 | CQL spec 3.4.2 | Native protocol v4]\nUse HELP for help.\n</pre>\n<p>Notice we can connect to Cassandra using SSL.  Notice that we are connected to the <code>test</code> cluster, which\nis the cluster we setup in <a href=\"http://cloudurable.com/blog/cassandra-image-vagrant-cluster-example/index.html\" rel=\"nofollow\">Setting up a Cassandra cluster with cassandra image and cassandra cloud project with Vagrant</a>.</p>\n<h2>\n<a id=\"user-content-review\" class=\"anchor\" href=\"#review\" aria-hidden=\"true\"></a>Review</h2>\n<p>We setup keys for client and clustering. We deployed keys to three Linux boxes using Vagrant provisioning.\nWe then setup <code>cqlsh</code> to use SSL. We then logged into one of the nodes and checked that the network was setup with the <code>nodetool describecluster</code>. Then we locally setup <code>csqlsh</code> to connect to the cluster using SSL.</p>\n<h2>\n<a id=\"user-content-more-to-come\" class=\"anchor\" href=\"#more-to-come\" aria-hidden=\"true\"></a>More to come.</h2>\n<p>Check back with us at the <a href=\"http://cloudurable.com/blog/index.html\" rel=\"nofollow\">Cloudurable blog</a> to find out more about <code>cassandra-image</code> and <code>cassandra-cloud</code>.</p>\n<h2>\n<a id=\"user-content-about-cloudurable\" class=\"anchor\" href=\"#about-cloudurable\" aria-hidden=\"true\"></a>About Cloudurable</h2>\n<p><a href=\"http://cloudurable.com/\" rel=\"nofollow\">Cloudurable</a> provides AMIs, cloudformation templates and monitoring tools\nto support <a href=\"http://cloudurable.com/services/index.html\" rel=\"nofollow\">Cassandra in production running in EC2</a>.\nWe also teach advanced <a href=\"http://cloudurable.com/cassandra-course/index.html\" rel=\"nofollow\">Cassandra courses which teaches how one could develop, support and deploy Cassandra to production in AWS EC2</a>.</p>\n<h4>\n<a id=\"user-content-references\" class=\"anchor\" href=\"#references\" aria-hidden=\"true\"></a>References</h4>\n<ul><li>Carpenter, Jeff; Hewitt, Eben (2016-06-29). <em><strong>Cassandra: The Definitive Guide: Distributed Data at Web Scale.</strong></em> O'Reilly Media.</li>\n<li><a href=\"https://developer.jboss.org/wiki/KeystoreFormatsJKSAndPEMCheatsheet\" rel=\"nofollow\">https://developer.jboss.org/wiki/KeystoreFormatsJKSAndPEMCheatsheet</a></li>\n<li><a href=\"http://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/JSSERefGuide.html\" rel=\"nofollow\">Java SSL support</a></li>\n<li><a href=\"https://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/FIPS.html\" rel=\"nofollow\">https://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/FIPS.html</a></li>\n<li><a href=\"http://docs.datastax.com/en/archived/cassandra/2.0/cassandra/security/secureSSLNodeToNode_t.html\" rel=\"nofollow\">http://docs.datastax.com/en/archived/cassandra/2.0/cassandra/security/secureSSLNodeToNode_t.html</a></li>\n<li><a href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/security/secureSSLCertificates_t.html\" rel=\"nofollow\">https://docs.datastax.com/en/cassandra/2.1/cassandra/security/secureSSLCertificates_t.html</a></li>\n<li><a href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/security/secureSSLClientToNode_t.html\" rel=\"nofollow\">https://docs.datastax.com/en/cassandra/2.1/cassandra/security/secureSSLClientToNode_t.html</a></li>\n<li><a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/configuration/secureCqlshSSL.html\" rel=\"nofollow\">Using cqlsh with SSL</a></li>\n<li><a href=\"http://cloudurable.com/blog/cassandra-image-vagrant-cluster-example/index.html\" rel=\"nofollow\">Setting up a Cassandra cluster with cassandra image and cassandra cloud project with Vagrant</a></li>\n<li>\n<a href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_reference/cqlshUsingCqlshrc.html\" rel=\"nofollow\">Configuring cqlsh from a file</a>.</li>\n</ul></div><div id=\"wiki-footer\" class=\"mt-5 mb-0 wiki-footer gollum-markdown-content\">\n          <div class=\"Box Box--condensed bg-gray box-shadow\">\n            <div class=\"Box-body  markdown-body\">\n              <h2>\n<a id=\"user-content-about-us\" class=\"anchor\" href=\"#about-us\" aria-hidden=\"true\"></a>About us</h2>\n<p><a href=\"http://cloudurable.com/\" rel=\"nofollow\">Cloudurable™: streamline DevOps for Cassandra running on AWS</a> provides AMIs, CloudWatch Monitoring, CloudFormation templates and monitoring tools\nto support <a href=\"http://cloudurable.com/services/index.html\" rel=\"nofollow\">Cassandra in production running in EC2</a>.\nWe also teach advanced <a href=\"http://cloudurable.com/services/index.html\" rel=\"nofollow\">Cassandra courses which teaches how one could develop, support and deploy Cassandra to production in AWS EC2 for Developers and DevOps</a>.</p>\n<h4>\n<a id=\"user-content-more-info\" class=\"anchor\" href=\"#more-info\" aria-hidden=\"true\"></a>More info</h4>\n<p>Please take some time to read the <a href=\"http://cloudurable.com/advantages/index.html\" rel=\"nofollow\">Advantage of using Cloudurable™</a>.</p>\n<p>Cloudurable provides:</p>\n<ul><li>\n<a href=\"http://cloudurable.com/subscription_support_benefits_cassandra/index.html\" rel=\"nofollow\">Subscription Amazon Cassandra support to streamline DevOps</a> (<a href=\"http://cloudurable.com/subscription_support/index.html\" rel=\"nofollow\">Support subscription pricing for Cassandra and Kafka in AWS</a>)</li>\n<li><a href=\"http://cloudurable.com/cassandra-course/index.html\" rel=\"nofollow\">Cassandra Course</a></li>\n<li><a href=\"http://cloudurable.com/service-quick-start-mentoring-cassandra-or-kafka-aws-ec2/index.html\" rel=\"nofollow\">Cassandra Consulting: Quick Start</a></li>\n<li><a href=\"http://cloudurable.com/service-architecture-analysis-cassandra-or-kafka-aws-ec2/index.html\" rel=\"nofollow\">Cassandra Consulting: Architecture Analysis</a></li>\n</ul><p><a href=\"http://cloudurable.com/kafka-training/index.html\" rel=\"nofollow\">Kafka training</a>, <a href=\"http://cloudurable.com/kafka-aws-consulting/index.html\" rel=\"nofollow\">Kafka consulting</a>,\n<a href=\"http://cloudurable.com/cassandra-course/index.html\" rel=\"nofollow\">Cassandra training</a>, <a href=\"http://cloudurable.com/kafka-aws-consulting/index.html\" rel=\"nofollow\">Cassandra consulting</a>,\n<a href=\"http://cloudurable.com/spark-aws-emr-training/index.html\" rel=\"nofollow\">Spark training</a>, <a href=\"http://cloudurable.com/spark-aws-emr-consulting/index.html\" rel=\"nofollow\">Spark consulting</a></p>\n            </div>\n          </div>\n        </div>",
        "created_at": "2018-10-24T18:10:13+0000",
        "updated_at": "2018-10-24T18:10:22+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 11,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/24500753?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12465"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          }
        ],
        "is_public": false,
        "id": 12464,
        "uid": null,
        "title": "Setting Up a Cassandra Cluster in AWS - DZone Cloud",
        "url": "https://dzone.com/articles/setting-up-a-cassandra-cluster-in-aws",
        "content": "<div class=\"content-html\" itemprop=\"text\"><article class=\"post-section post-2624 post type-post status-publish format-standard hentry category-developer-tips\" id=\"post-2624\"><div class=\"sw-wrapper clearfix\"> \n  <div class=\"post-detail\"> \n   <p>Apache Cassandra is a NoSQL database that allows for easy horizontal scaling, <a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archDataDistributeHashing.html\" rel=\"nofollow\">using the consistent hashing mechanism</a>. <a href=\"https://techblog.bozho.net/why-i-decided-not-to-use-cassandra/\" target=\"_blank\" rel=\"nofollow\">Seven years ago, I tried it and decided not use it for a side-project of mine</a> because it was too new. But things are different now — Cassandra is well-established, there’s a company behind it (DataStax), and there are a lot more tools, documentation, and community support. So once again, I decided to try Cassandra.</p> \n   <p>This time, I need it to run in a cluster on AWS, so I went on to set up such a cluster. Googling how to do it gives several interesting results, like <a href=\"http://highscalability.com/blog/2016/8/1/how-to-setup-a-highly-available-multi-az-cassandra-cluster-o.html\" target=\"_blank\" rel=\"nofollow\">this</a>, <a href=\"https://github.com/cloudurable/cassandra-image/wiki/Cassandra-Tutorial-6:-Setting-up-Cassandra-Cluster-in-EC2-Part-2-Multi-AZs-with-Ec2Snitch\" target=\"_blank\" rel=\"nofollow\">this</a>, and <a href=\"https://wiki.apache.org/cassandra/CloudConfig\" target=\"_blank\" rel=\"nofollow\">this</a>, but they are incomplete, outdated, or have too many irrelevant details. So they are only of moderate help.</p> \n   <p>My goal is to use CloudFormation (or Terraform potentially) to launch a stack that has a Cassandra auto-scaling group (in a single region) that can grow as easily as increasing the number of nodes in the group.</p> \n   <p>Also, in order to have the web application connect to Cassandra without hardcoding the node IPs, I wanted to have a load balancer in front of all Cassandra nodes to do the round-robin for me. The alternative to that would be to have a <a href=\"http://docs.datastax.com/en/drivers/java/2.2/com/datastax/driver/core/policies/DCAwareRoundRobinPolicy.html\" target=\"_blank\" rel=\"nofollow\">client-side round-robin</a>, but that would mean some extra complexity on the client, which seems avoidable with a load balancer in front of the Cassandra auto-scaling group.</p> \n   <p>The relevant bits from my <a href=\"https://gist.github.com/Glamdring/5139bc49037f3b5c1f8cd008ba046df0\" target=\"_blank\" rel=\"nofollow\">CloudFormation JSON can be seen here</a>. Here's what it does:</p> \n   <ul><li>Sets up three private subnets (1 per availability zone in the eu-west region)</li> \n    <li>Creates a security group that allows incoming and outgoing ports that allow Cassandra to accept connections (9042) and for the nodes to gossip (7000/7001). Note that the ports are only accessible from within the VPC — no external connection is allowed. SSH goes only through a <a href=\"https://en.wikipedia.org/wiki/Bastion_host\" target=\"_blank\" rel=\"nofollow\">bastion host</a>.</li> \n    <li>Defines a TCP load balancer for port 9042 where all clients will connect. The load balancer requires a so-called “Target group,” which is defined as well.</li> \n    <li>Configures an auto-scaling group with a pre-configured number of nodes. The autoscaling group has a reference to the “target group”so that the load balancer always sees all nodes in the auto-scaling group.</li> \n    <li>Each node in the auto-scaling group is identical based on a launch configuration. The launch configuration runs a few scripts on initialization. These scripts will be run for every node – either initially, when case a node dies and another one is spawned in its place, or when the cluster has to grow. The scripts are fetched from S3, where you can publish them (and version them) either manually or with an automated process.</li> \n    <li>Note: This does not configure specific EBS volumes and, in reality, you may need to configure and attach them if the instance storage is insufficient. Don’t worry about nodes dying, though, as data is safely replicated.</li> \n   </ul><p>That was the easy part – a bunch of AWS resources and port configurations. The Cassandra-specific setup is a bit harder, as it requires understanding of how Cassandra functions.</p> \n   <p>The two scripts are <a href=\"https://gist.github.com/Glamdring/ee5f24dbef3795a860ac91c9c14255a2\" target=\"_blank\" rel=\"nofollow\">setup-cassandra.sh</a> and <a href=\"https://gist.github.com/Glamdring/d317ff191a223d2bcf7c92fa5fdf3476\" target=\"_blank\" rel=\"nofollow\">update-cassandra-cluster-config.py</a>, so bash and Python: bash for setting-up the machine, and Python for Cassandra-specific stuff. Instead of the bash script, one could use a pre-built AMI (image), e.g. with Packer, but since only two pieces of software are installed, I thought it was a bit of an overhead to support AMIs.</p> \n   <p><a href=\"https://gist.github.com/Glamdring/ee5f24dbef3795a860ac91c9c14255a2\" target=\"_blank\" rel=\"nofollow\">The bash script can be seen here</a>, and simply installs Java 8 and the latest Cassandra, runs the Python script, runs the Cassandra services, and creates (if needed) a keyspace with proper replication configuration. A few notes here – the cassandra.yaml.template could be supplied via the CloudFormation script instead of having it fetched via bash (and having to pass the bucket name); you could also have it fetched in the Python script itself – it’s a matter of preference.</p> \n   <p>Cassandra is not configured for use with SSL, which is generally a bad idea, but the <a href=\"https://github.com/cloudurable/cassandra-image/wiki/Cassandra-Tutorial-2:-Setting-up-client-and-cluster-SSL-transport-in-Cassandra\" target=\"_blank\" rel=\"nofollow\">SSL configuration</a> is out of the scope of this basic setup. Finally, the script waits for the Cassandra process to run (using a while/sleep loop) and then creates the keyspace if needed. The keyspace (=database) has to be created with a NetworkTopologyStrategy, and the number of replicas for the particular datacenter (=AWS region) has to be configured. The value is 3, for the 3 availability zones where we’ll have nodes. That means there’s a copy in each AZ (which is seen like a “rack”, although it’s exactly that).</p> \n   <p><a href=\"https://gist.github.com/Glamdring/d317ff191a223d2bcf7c92fa5fdf3476\" target=\"_blank\" rel=\"nofollow\">The Python script </a>does some very important configurations – without them, the cluster won’t work. (I don’t work with Python normally, so feel free to criticize my Python code). The script does the following:</p> \n   <ul><li>Gets the current auto-scaling group details (using AWS EC2 APIs).</li> \n    <li>Sorts the instances by time.</li> \n    <li>Fetches the first instance in the group in order to assign it as seed node.</li> \n    <li>Sets the seed node in the configuration file (by replacing a placeholder).</li> \n    <li>Sets the listen_address (and therefore rpc_address) to the private IP of the node in order to allow Cassandra to listen for incoming connections.</li> \n   </ul><p>Designating the seed node is important, as all cluster nodes have to join the cluster by specifying at least one seed. You can get the first two nodes instead of just one, but it shouldn’t matter. Note that the seed node is not always fixed – it’s just the oldest node in the cluster. If at some point the oldest node is terminated, each new node will use the second oldest as seed.</p> \n   <p>What I haven’t shown is the cassandra.yaml.template file. It is basically a copy of the cassandra.yaml file from a standard Cassandra installation, with a few changes:</p> \n   <ul><li><code>cluster_name</code> is modified to match your application name. This is just for human-readable purposes, so it doesn’t matter what you set it to.</li> \n    <li><code>allocate_tokens_for_keyspace: your_keyspace</code> is uncommented and the keyspace is set to match your main keyspace. This enables the <a href=\"https://www.datastax.com/dev/blog/token-allocation-algorithm\" target=\"_blank\" rel=\"nofollow\">new token distribution algorithm in Cassandra 3.0</a>. It allows for evenly distributing the data across nodes.</li> \n    <li><code>endpoint_snitch: Ec2Snitch</code> is set instead of the SimpleSnitch to make use of AWS metadata APIs. Note that this setup is in a single region. For multi-region, there’s another snitch and some additional complications of exposing ports and changing the broadcast address.</li> \n    <li>As mentioned above, ${private_ip} and ${seeds} placeholders are placed in the appropriate places (listen_address and rpc_address for the IP) in order to allow substitution.</li> \n   </ul><p>The lets you run a Cassandra cluster as part of your AWS stack, which is auto-scalable and doesn’t require any manual intervention – neither on setup nor on scaling up. Well, allegedly – there may be issues that have to be resolved once you hit the use cases of reality. And for clients to connect to the cluster, simply use the load balancer DNS name (you can print it in a config file on each application node).</p> \n  </div> \n </div> \n</article></div><div class=\"content-html\" itemprop=\"text\"><a>\n                        <img class=\"pub-image\" width=\"420\" itemprop=\"image\" src=\"src\" alt=\"image\" /></a></div>",
        "created_at": "2018-10-24T18:08:00+0000",
        "updated_at": "2018-10-24T18:08:09+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 5,
        "domain_name": "dzone.com",
        "preview_picture": "https://dz2cdn3.dzone.com/storage/article-thumb/6858798-thumb.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12464"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 35,
            "label": "docker",
            "slug": "docker"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12462,
        "uid": null,
        "title": "Docker Meet Cassandra. Cassandra Meet Docker.",
        "url": "http://thelastpickle.com/blog/2018/01/23/docker-meet-cassandra.html",
        "content": "<p>After having spent quite a bit of time learning Docker and after hearing strong community interest for the technology even though few have played with it, I figured it’d be be best to share what I’ve learned. Hopefully the knowledge transfer helps newcomers get up and running with Cassandra in a concise, yet deeply informed manner.</p><p>A few years ago I finally started playing with Docker by way of Vagrant. That entire experience was weird. Don’t do it.</p>\n<p>Later Docker Compose was released and all the roadblocks I previously encountered immediately melted away and the power of Docker was made very aware to me. Since then I’ve been like my cat, but instead of “Tuna Tuna Tuna Tuna” it’s more like: “Docker Docker Docker Docker.”</p>\n<p>But the more I spoke about Docker and asked around about Docker, the sadder I became since:</p>\n<ul><li>Few really used Docker.</li>\n  <li>Fewer had even heard of Docker Compose.</li>\n  <li>Everyone was worried about how Docker performance would be in production.</li>\n  <li>Some were waiting for the Mesos and Kubernetes war to play out.\n    <ul><li>Kubernetes won by the way. Read any news around Docker-Kubernetes and AWS-Kubernetes to make your own judgements.</li>\n    </ul></li>\n</ul><p>Within The Last Pickle, I advocate for Docker as best I can. Development project? “Why not use Docker?” Quick test? “<em>cough</em> Docker <em>cough</em>.” Learn everything you can about Grafana, Graphite, and monitoring dashboards you ask? “Okay, Docker it is!”</p>\n<p>About a year later, we’re here and guess what? Now you get to be here with me as well! :tada:</p>\n<h2 id=\"docker-cassandra-bootstrap\">Docker Cassandra Bootstrap</h2>\n<p>In October, <a href=\"https://www.linkedin.com/in/nickmbailey/\">Nick Bailey</a> invited me to present at the local <a href=\"https://www.meetup.com/Austin-Cassandra-Users/\">Austin Cassandra Users Meetup</a> and I figured this was the time to consolidate my recent knowledge and learnings into a simplified project. I figured if I had already spent time on such an intricate project I could save others time and give them a clean environment they could play with, develop on, then move into production.</p>\n<p>That’s how the <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap\">docker-cassandra-bootstrap</a> project was born.</p>\n<p>I will stay away from how to run the Docker Cassandra Bootstrap project within this blog post since the instructions are already within the Github project’s <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap\">README.md</a>. Instead, I’ll focus on the individual components, what’s hidden in which nooks, and which stubs are commented out in which crannies for future use and development.</p>\n<h2 id=\"docker-composeyml\">docker-compose.yml</h2>\n<p>The <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/docker-compose.yml\">docker-compose.yml</a> is the core building block for any Docker Compose project.</p>\n<h3 id=\"building\">Building</h3>\n<p>What Docker provided was the <code class=\"highlighter-rouge\">Dockerfile</code> which allowed <em>image</em> definitions to run <em>containers</em>. (Note the differentiation that <em>containers</em> are <em>images</em> that are running.) Building an image using Docker was pretty straight forward:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>docker build .\n</pre></div></div>\n<p>However, building many images would require tagging and additional <code class=\"highlighter-rouge\">docker</code> parameters. And that can get confusing really quickly and definitely isn’t user-friendly.</p>\n<p>Instead, Docker Compose lets you build entire ecosystems of <em>services</em> with a simple command:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>docker-compose build\n</pre></div></div>\n<p>Now with Docker Compose, you don’t have to keep track of image tagging, <code class=\"highlighter-rouge\">Dockerfile</code> location, or anything else that I gratefully have too little experience with. Instead, <em>images</em> are defined by the <code class=\"highlighter-rouge\">image</code> (from Docker Hub) and <code class=\"highlighter-rouge\">build</code> (from a local <code class=\"highlighter-rouge\">Dockerfile</code>) parameters within a Docker Compose <em>service</em>.</p>\n<h3 id=\"environmental-variables\">Environmental Variables</h3>\n<p>Along with the simplification of image definitions, Docker Compose introduces the <code class=\"highlighter-rouge\">env_file</code> parameter which is a not-quite-bash environmental definition file. It’s slightly different, bash commands will not resolve, you can’t use an envar within another’s definition, and don’t use quotes since those will be considered part of the envar’s value. While these <code class=\"highlighter-rouge\">env_files</code> come with their own limitations, <code class=\"highlighter-rouge\">env_files</code> means I no longer have to have ugly, long, complicated lines like:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>docker -e KEY=VAL -e KEY2=VAL2 -e WHAT=AMI -e DOING=NOW ...\n</pre></div></div>\n<p>Through the use of <a href=\"https://docs.docker.com/compose/extends/\">multiple <code class=\"highlighter-rouge\">docker-compose.yml</code> files</a>, one can create:</p>\n<ul><li>An <code class=\"highlighter-rouge\">env_file</code> called <code class=\"highlighter-rouge\">settings.env</code> which has all generalized settings.</li>\n  <li>An <code class=\"highlighter-rouge\">env_file</code> called <code class=\"highlighter-rouge\">dev.env</code> which has dev-specific settings.</li>\n  <li>An <code class=\"highlighter-rouge\">env_file</code> called <code class=\"highlighter-rouge\">prod.env</code> which has production-specific settings.</li>\n  <li>An <code class=\"highlighter-rouge\">env_file</code> called <code class=\"highlighter-rouge\">nz.env</code> which will define variables to flip all gifs by 180-degrees to counteract the fact that New Zealanders are upside down.</li>\n</ul><p>At the end of the day, the filenames, segregation, and environmental variable values are for you to use and define within your own ecosystem.</p>\n<p>But that’s getting ahead of ourselves. Just know that you can place whatever environmental variables you want in these files as you create production-specific <code class=\"highlighter-rouge\">env_files</code> which may not get used today, but will be utilized when you move into production.</p>\n<h3 id=\"volumes\">Volumes</h3>\n<p>Within Docker everything within a container that is not stored in <code class=\"highlighter-rouge\">volumes</code> is temporary. This means that if we launch a new container using any given static Docker image, we can manipulate multiple aspects of system configuration, data, file placement, etc, without a concern on changing our stable static environment. If something ever breaks, we simply kill the container, launch a new container based off the same image, and we’re back to our familiar static and stable state. However, if we ever want to persist data, storing this valuable data within Docker <code class=\"highlighter-rouge\">volumes</code> ensures that the data is accessible across container restarts.</p>\n<p>The above statement probably makes up about 90% of my love for Docker (image layer caching probably makes up a majority of the remaining 10%).</p>\n<p>What my declaration of love means is that while a container is running: it is your pet. It does what you ask of it (unless it’s a cat) and it will live with you by your side crunching on the code you fed it. But once your pet has finished processing the provided code, it will vanish into the ether and you can replace it like you would cattle.</p>\n<p>This methodology is a perfect fit for short-lived microservice workers. However, if you want to persist data, or provide configuration files to the worker microservices, you’ll want to use <code class=\"highlighter-rouge\">volumes</code>.</p>\n<p>While you can use <a href=\"https://docs.docker.com/compose/compose-file/#volume-configuration-reference\"><em>named volumes</em></a> to allow Docker to handle the data location for you, not using named volumes will put you in full control of where the data directory will resolve to, can provide performance benefits, and will remove one layer of indirection in case anything should go wrong.</p>\n<p>When people ask about the performance of Docker in production, <code class=\"highlighter-rouge\">volumes</code> are the key component. If your service relies on disk access, use <code class=\"highlighter-rouge\">volumes</code> to ensure a higher level of performance. For all other cases, if there even is a CPU-based performance hit, it should be mild and you can always scale horizontally. The ultimate performance benefit of using Docker is that applications are containerized and are extremely effective at horizontal scaling. Also, consider the time and effort costs to test, deploy, and rollback any production changes when using containers. Although this isn’t essentially performance related, it does increase codebase velocity which may be as valuable as raw performance metrics.</p>\n<h3 id=\"entrypoints\">Entrypoints</h3>\n<p>Back to looking at the <code class=\"highlighter-rouge\">docker-compose.yml</code>, Docker <code class=\"highlighter-rouge\">entrypoints</code> define which program will be executed when a machine begins running. You can think of SSH’s entrypoint as being <code class=\"highlighter-rouge\">bash</code>, or <code class=\"highlighter-rouge\">zsh</code>.</p>\n<p>Under the <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/docker-compose.yml\"><code class=\"highlighter-rouge\">cqlsh</code> service</a>, the default <code class=\"highlighter-rouge\">entrypoint</code> is overwritten by <code class=\"highlighter-rouge\">cqlsh cassandra</code>, where <code class=\"highlighter-rouge\">cassandra</code> is the name of the Cassandra Docker Compose service. This means that we want to use the <code class=\"highlighter-rouge\">cassandra:3.11</code> image, but not use the bash script that sets up the <code class=\"highlighter-rouge\">cassandra.yaml</code> and other Cassandra settings. Instead, the service will utilize <code class=\"highlighter-rouge\">cassandra:3.11</code>’s image and start the container with <code class=\"highlighter-rouge\">cqlsh cassandra</code>. This allows the following shorthand command to be run, all within a Docker container, without any local dependencies other than Docker and Docker Compose:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>docker-compose run cqlsh\n</pre></div></div>\n<p>The above command starts up the <code class=\"highlighter-rouge\">cqlsh</code> service, calls the <code class=\"highlighter-rouge\">cqlsh</code> binary, and provides the <code class=\"highlighter-rouge\">cassandra</code> hostname as the contact point.</p>\n<p>The <code class=\"highlighter-rouge\">nodetool</code> service is good example of creating a shorthand command for an otherwise complicated process. Instead of having:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>docker-compose run --entrypoint bash cassandra\n$ nodetool -h cassandra -u cassandraUser -pw cassandraPass status\n</pre></div></div>\n<p>We can simply run:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>docker-compose run nodetool status\n</pre></div></div>\n<p>Any parameters following the service name are appended to the defined <code class=\"highlighter-rouge\">entrypoint</code>’s command and replaces the service’s <code class=\"highlighter-rouge\">command</code> parameter. For the <code class=\"highlighter-rouge\">nodetool</code> service the default <code class=\"highlighter-rouge\">command</code> is <code class=\"highlighter-rouge\">help</code>, but in the above line, the <code class=\"highlighter-rouge\">command</code> that is appended to the <code class=\"highlighter-rouge\">entrypoint</code> is <code class=\"highlighter-rouge\">status</code>.</p>\n<h3 id=\"links\">Links</h3>\n<p>If we have a service that will rely on communication with another service, the way that the <code class=\"highlighter-rouge\">cassandra-reaper</code> service must be in contact with the <code class=\"highlighter-rouge\">cassandra</code> service that Reaper will be monitoring, we can use the <code class=\"highlighter-rouge\">links</code> service parameter to define both the hostname and servicename within this link.</p>\n<p>I like to be both simple and explicit, which is why I use the same name for the hostname and service name like:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>links:\n  - cassandra:cassandra\n</pre></div></div>\n<p>The above line will allow the <code class=\"highlighter-rouge\">cassandra-reaper</code> service’s container to contact the <code class=\"highlighter-rouge\">cassandra</code> service by way of the <code class=\"highlighter-rouge\">cassandra</code> hostname.</p>\n<h3 id=\"ports\">Ports</h3>\n<p>Ports are a way to expose a service’s port to another service, or bind a service’s port to a local port.</p>\n<p>Because the <code class=\"highlighter-rouge\">cassandra-reaper</code> service is meant to be used via its web UI, we bind the service’s <code class=\"highlighter-rouge\">8080</code> and <code class=\"highlighter-rouge\">8081</code> ports from within the service onto the local machine’s <code class=\"highlighter-rouge\">8080</code> and <code class=\"highlighter-rouge\">8081</code> ports using the following lines:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>ports:\n  - \"8080:8080\"\n  - \"8081:8081\"\n</pre></div></div>\n<p>This means if we visit http://localhost:8080/webui/ from our local machine, we’ll be processing code from within a container to service that web request.</p>\n<h3 id=\"restart\">Restart</h3>\n<p>The <code class=\"highlighter-rouge\">restart</code> command is more of a Docker Compose-specific scheduler that dictates what a service should do if the container is abruptly terminated.</p>\n<p>In the case of the <code class=\"highlighter-rouge\">grafana</code> and <code class=\"highlighter-rouge\">logspout</code> services, if Grafana or Logspout ever die, the containers will exit and the <code class=\"highlighter-rouge\">grafana</code> or <code class=\"highlighter-rouge\">logspout</code> services will automatically restart and come back online.</p>\n<p>While this parameter may be ideal for some microservices, it may not be ideal for services that power data stores.</p>\n<h2 id=\"the-cassandra-service\">The <code class=\"highlighter-rouge\">cassandra</code> service</h2>\n<p>The <code class=\"highlighter-rouge\">docker-compose.yml</code> defines the <code class=\"highlighter-rouge\">cassandra</code> service as having a few mounted configuration files.</p>\n<p>The two configuration files that are enabled by default include configurations for:</p>\n<ul><li>collectd.</li>\n  <li>Prometheus JMX exporter.</li>\n</ul><p>The two configuration files that are disabled by default are for:</p>\n<ul><li>The Graphite metrics reporter.</li>\n  <li>The Filebeat log reporter for ELK.</li>\n</ul><h3 id=\"collectd\">collectd</h3>\n<p>The <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/config/collectd.cassandra.conf\">cassandra/config/collectd.cassandra.conf</a> configuration file loads a few plugins that TLP has found to be useful for enterprise metric dashboards.</p>\n<p><a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/Dockerfile#L62-L74\">A few packages</a> need to be installed for the collectd to be fully installed with the referenced plugins. The service must also be started from within the <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/docker-entrypoint.sh#L11\">cassandra/docker-entrypoint.sh</a> or just by using <code class=\"highlighter-rouge\">service collectd start</code> on hardware.</p>\n<p>The metrics that are collected include information on:</p>\n<ul><li>Disk.</li>\n  <li>CPU load.</li>\n  <li>Network traffic.</li>\n  <li>Memory usage.</li>\n  <li>System logs.</li>\n</ul><p>collectd is then configured to write to a Prometheus backend by default. Commented code is included for writing to a Graphite backend.</p>\n<p>For further information on each of the plugins, visit the <a href=\"https://collectd.org/wiki/index.php/Table_of_Plugins\">collectd wiki</a>.</p>\n<h3 id=\"prometheus-jmx-exporter\">Prometheus JMX Exporter</h3>\n<p>The <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/config/prometheus.yml\">cassandra/config/prometheus.yml</a> configuration file defines the JMX endpoints that will be collected by the JMX exporter and exposed via a REST API for Prometheus ingestion.</p>\n<p>The following jar needs to be used and referenced within <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/Dockerfile#L78-L85\">cassandra-env.sh</a>:</p>\n<ul><li><a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/lib/jmx_prometheus_javaagent-0.9.jar\">cassandra/lib/jmx_prometheus_javaagent-0.9.jar</a></li>\n</ul><p>Only the most important enterprise-centric list of metrics are being collected for Prometheus ingestion.</p>\n<p>The resulting <code class=\"highlighter-rouge\">name</code> formats did not follow the standard Prometheus naming scheme, but instead something between that of the Graphite dot-naming scheme with the use of Prometheus-styled labels when relevant. Do note that Prometheus will automatically convert all dot-separated metrics to underscore-separated metrics since Prometheus does not understand the concept of hierarchical metric keys.</p>\n<h3 id=\"graphite-metrics-reporter\">Graphite Metrics Reporter</h3>\n<p>The <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/config/graphite.cassandra.yaml\">cassandra/config/graphite.cassandra.yaml</a> configuration file is included and commented out by default.</p>\n<p>For reporting to work correctly, this file requires the following jars to be placed into <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/Dockerfile#L10-L35\">Cassandra’s lib directory and have Java 8 installed</a>:</p>\n<ul><li><a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/lib/metrics-core-3.1.2.jar\">cassandra/lib/metrics-core-3.1.2.jar</a></li>\n  <li><a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/lib/metrics-graphite-3.1.2.jar\">cassandra/lib/metrics-graphite-3.1.2.jar</a></li>\n  <li><a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/lib/reporter-config-base-3.0.3.jar\">cassandra/lib/reporter-config-base-3.0.3.jar</a></li>\n  <li><a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/lib/reporter-config3-3.0.3.jar\">cassandra/lib/reporter-config3-3.0.3.jar</a></li>\n</ul><p>Once properly enabled, the <code class=\"highlighter-rouge\">cassandra</code> service can contact a Graphite host with enterprise-centric metrics. However, we did not include the <code class=\"highlighter-rouge\">graphite</code> service within this project because our experience with Prometheus has been smoother than dealing with Graphite and because Prometheus seems to scale better than Graphite in production environments. As of the most recent Grafana releases, Prometheus has also received plenty of Prometheus-centric features and support as well.</p>\n<h3 id=\"filebeat-log-reporter\">Filebeat Log Reporter</h3>\n<p>The <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/config/filebeat.yml\">cassandra/config/filebeat.yml</a> configuration file is set up to contact a pre-existing ELK stack.</p>\n<p>The Filebeat package is required and can be started within a Docker container by referencing the Filebeat service from within <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/Dockerfile#L37-L52\">cassandra-env.sh</a>.</p>\n<p>The Filebeat configuration file is by no means complete (and assume incorrect!), but it does provide a good starting point for learning how to ingest log files into Logstash using Filebeat. Please consider this configuration file as <em>fully experimental</em>.</p>\n<h3 id=\"jmx-authentication\">JMX Authentication</h3>\n<p>Because Reaper for Apache Cassandra will be contacting the JMX port of this <code class=\"highlighter-rouge\">cassandra</code> service we will also need to add <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/Dockerfile#L87-L95\">authentication files for JMX in two locations</a> and set the <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/cassandra.env#L15-L17\"><code class=\"highlighter-rouge\">LOCAL_JMX</code> variable to <code class=\"highlighter-rouge\">no</code></a> to expose the JMX port externally while requiring authentication.</p>\n<h2 id=\"the-cqlsh-service\">The <code class=\"highlighter-rouge\">cqlsh</code> Service</h2>\n<p>The <code class=\"highlighter-rouge\">cqlsh</code> service is a helper service that simply uses the <code class=\"highlighter-rouge\">cassandra:3.11</code> image hosted on Docker Hub to provide the cqlsh binary while mounting the local <code class=\"highlighter-rouge\">./cassandra/schema.cql</code> file into the container for simple schema creation and data querying.</p>\n<p>The defined setup allows us to run the following command with ease:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>docker-compose run cqlsh -f /schema.cql\n</pre></div></div>\n<p>The <code class=\"highlighter-rouge\">nodetool</code> service is another helper service that is provided to automatically fill in the host, username, and password parameters.</p>\n<p>By default, it includes the <code class=\"highlighter-rouge\">help</code> <code class=\"highlighter-rouge\">command</code> to provide a list of options for <code class=\"highlighter-rouge\">nodetool</code>. Running the following command will contact the <code class=\"highlighter-rouge\">cassandra</code> node, automatically authenticate, and show a list of options:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>docker-compose run nodetool\n</pre></div></div>\n<p>By including an additional parameter within the command line we will overwrite the <code class=\"highlighter-rouge\">help</code> <code class=\"highlighter-rouge\">command</code> and run that requested command instead, like <code class=\"highlighter-rouge\">status</code>:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>docker-compose run nodetool status\n</pre></div></div>\n<h2 id=\"the-cassandra-reaper-service\">The <code class=\"highlighter-rouge\">cassandra-reaper</code> Service</h2>\n<p>The <code class=\"highlighter-rouge\">cassandra-reaper</code> service does not use a locally built and customized image, but instead uses an image hosted on <a href=\"https://hub.docker.com/r/thelastpickle/cassandra-reaper/\">Docker Hub</a>. The image that is specifically used is <code class=\"highlighter-rouge\">ab0fff2</code>. However, you can choose to use the <code class=\"highlighter-rouge\">latest</code> release or <code class=\"highlighter-rouge\">master</code> image if you want the bleeding edge version.</p>\n<p>The configuration is all handled via environmental variables for easy Docker consumption within <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra-reaper/cassandra-reaper.env\">cassandra-reaper/cassandra-reaper.env</a>. For a list of all configuration options, see the <a href=\"http://cassandra-reaper.io/docs/configuration/docker_vars/\">Reaper documentation</a>.</p>\n<p>The ports that are exposed onto <code class=\"highlighter-rouge\">localhost</code> are <code class=\"highlighter-rouge\">8080</code> and <code class=\"highlighter-rouge\">8181</code> for the web UI and administration UI, respectively.</p>\n<h2 id=\"the-grafana-service\">The <code class=\"highlighter-rouge\">grafana</code> Service</h2>\n<p>The <code class=\"highlighter-rouge\">grafana</code> service uses the <a href=\"https://hub.docker.com/r/grafana/grafana/\">grafana/grafana</a> image from Docker Hub and is configured using the <code class=\"highlighter-rouge\">grafana/grafana.env</code>. For a list of all configuration options via environmental variables, visit the <a href=\"http://docs.grafana.org/installation/configuration/#using-environment-variables\">Grafana documentation</a>.</p>\n<p>The two scripts included in <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/tree/master/grafana/bin\">grafana/bin/</a> are referenced in the <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap#meetup-workflow\">README.md</a> and are used to create data sources that rely on the Prometheus data store and uploads all the <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/tree/master/grafana/dashboards\">grafana/dashboard</a> JSON files into Grafana.</p>\n<p>The JSON files were meticulously created for two of our enterprise customer’s production deployments (shared here with permission). They include 7 dashboards that highlight specific Cassandra metrics in a drill-down fashion:</p>\n<ul><li>Overview</li>\n  <li>Read Path</li>\n  <li>Write Path</li>\n  <li>Client Connections</li>\n  <li>Alerts</li>\n  <li>Reaper</li>\n  <li>Big Picture</li>\n</ul><p>At TLP we typically always start off with the Overview dashboard. Depending on our metrics we’ll switch to look at the <a href=\"http://thelastpickle.com/blog/2017/12/05/datadog-tlp-dashboards.html\">Read Path, Write Path, or Client Connection dashboards</a> for further investigation. Do keep in mind that while these dashboards are a work in progress, they do include proper x/y-axis labeling, units, and tooltips/descriptions. The tooltips/descriptions should provide info in the following order, when relevant:</p>\n<ul><li>Description</li>\n  <li>Values</li>\n  <li>False Positives</li>\n  <li>Required Actions</li>\n  <li>Warning</li>\n</ul><p>The text is meant to accompany any alerts that are triggered via the auto-generated Alerts dashboard. This way Slack, PagerDuty, or email alerts contain some context into why the alert was triggered, what most likely culprits may be involved, and how to resolve the alert.</p>\n<p>Do note that while the other dashboards may have triggers set up, only the Alerts dashboard will fire the triggers since all the dashboards make use of Templating Variables for easy Environment, Data Center, and Host selection, which Grafana alerts do not yet support.</p>\n<p>If there are ever any issues around repair, take a look at the Reaper dashboard as well to monitor the Reaper for Apache Cassandra service.</p>\n<p>The Big Picture dashboard provides a 30,000 foot view of the cluster and is nice to reason about, but ultimately provides very little value other than showing the high-level trends of the Cassandra deployment being monitored.</p>\n<h2 id=\"the-logspout-service\">The <code class=\"highlighter-rouge\">logspout</code> Service</h2>\n<p>The <code class=\"highlighter-rouge\">logspout</code> service is a unique service across multiple aspects. While this section will cover some of the special use cases, feel free to skim this section but do try to intake as much information as possible since these sort of use cases will arise in your future docker-compose.yml creations, even if not today.</p>\n<p>At the top of the <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/docker-compose.yml\">docker-compose.yml</a> under the <code class=\"highlighter-rouge\">logspout</code> section, we define a <code class=\"highlighter-rouge\">build</code> parameter. This <code class=\"highlighter-rouge\">build</code> parameter references <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/logspout/Dockerfile\">logspout/Dockerfile</a> which has no real information since the <code class=\"highlighter-rouge\">FROM</code> image that our local image refers to uses a few <code class=\"highlighter-rouge\">ONBUILD</code> commands. These commands are defined in the <a href=\"https://github.com/gliderlabs/logspout/blob/master/Dockerfile#L9-L11\">parent image</a> and make use of <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/logspout/modules.go\">logspout/modules.go</a>. Our custom logspout/modules.go installs the required Logstash dependencies for use with pre-existing Logstash deployments.</p>\n<p>While <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/logspout/build.sh\">logspout/build.sh</a> was not required to be duplicated since the parent image already had the file pre-baked, I did so for safekeeping.</p>\n<p>The <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/logspout/logspout.env\">logspout/logspout.env</a> file includes a few commented out settings I felt were interesting to look at if I were to experiement with Logstash in the future. These might be a good starting point for further investigation.</p>\n<p>The <code class=\"highlighter-rouge\">logspout</code> service also uses the <code class=\"highlighter-rouge\">restart: always</code> setting to ensure that any possible issues with the log redirection service will automatically be resolved by restarting the service immediately after failure.</p>\n<p>Within the <code class=\"highlighter-rouge\">logspout</code> service, we are redirecting the container’s port <code class=\"highlighter-rouge\">80</code> to our localhost’s port <code class=\"highlighter-rouge\">8000</code>. This allows us to <code class=\"highlighter-rouge\">curl http://localhost:8000/logs</code> from our local machine and grab the logs that the container was displaying on its own REST API under port <code class=\"highlighter-rouge\">80</code>.</p>\n<p>In order for any of the <code class=\"highlighter-rouge\">logspout</code> container magic to work, we need to bind our localhost’s <code class=\"highlighter-rouge\">/var/run/docker.sock</code> into the container as a read-only mount. Even though the mount is read-only, there are still security concerns for doing such an operation. Since this line is still required for allowing this logging redirection to occur, I did include two links to further clarify the security risks involved with including this container within production environments:</p>\n<ul><li>https://raesene.github.io/blog/2016/03/06/The-Dangers-Of-Docker.sock/</li>\n  <li>http://stackoverflow.com/questions/40844197</li>\n</ul><p>Perhaps in the future the Docker/Moby team will provide a more secure workaround.</p>\n<p>The last line that has not been mentioned is the <code class=\"highlighter-rouge\">command</code> option which is commented out by default within the <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/docker-compose.yml\">docker-compose.yml</a>. By uncommenting the <code class=\"highlighter-rouge\">command</code> option, we stop using the default <code class=\"highlighter-rouge\">command</code> provided in the parent Dockerfile which exposes the logs via an REST API and we begin to send our logs to the <a href=\"https://papertrailapp.com\">PaperTrail</a> website which provides 100 MB/month of hosted logs for free.</p>\n<p>In order to isolate checked-in code from each developer’s <code class=\"highlighter-rouge\">$PAPERTRAIL_PORT</code>, each developer must locally create a copy of <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/.env.template\">.env.template</a> within their project’s directory under the filename <code class=\"highlighter-rouge\">.env</code>. Within this <code class=\"highlighter-rouge\">.env</code> file we can define environmental variables that will be used by <code class=\"highlighter-rouge\">docker-compose.yml</code>. Once you have an account for PaperTrail, set <code class=\"highlighter-rouge\">PAPERTRAIL_PORT</code> to the port assigned by PaperTrail to begin seeing your logs within <a href=\"https://papertrailapp.com\">PaperTrail.com</a>. And since <code class=\"highlighter-rouge\">.env</code> is setup to be ignored via <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/.gitignore\">.gitignore</a> we do not have to worry about sending multiple developer’s logs to the same location.</p>\n<h2 id=\"the-prometheus-service\">The <code class=\"highlighter-rouge\">prometheus</code> Service</h2>\n<p>The <code class=\"highlighter-rouge\">prometheus</code> service is one of the simpler services within this Docker Compose environment.</p>\n<p>The <code class=\"highlighter-rouge\">prometheus</code> service:</p>\n<ul><li>Uses a <a href=\"https://hub.docker.com/r/prom/prometheus/\">Docker Hub image</a>.</li>\n  <li>Will require access to both the <code class=\"highlighter-rouge\">cassandra</code> and <code class=\"highlighter-rouge\">cassandra-reaper</code> services to monitor their processes.</li>\n  <li>Will expose the container’s port <code class=\"highlighter-rouge\">9090</code> onto localhost’s port <code class=\"highlighter-rouge\">9090</code>.</li>\n  <li>Will persist all data within the container’s <code class=\"highlighter-rouge\">/prometheus</code> directory onto our local <code class=\"highlighter-rouge\">./data/prometheus</code> directory.</li>\n  <li>Will configure the container using a locally-stored copy of the <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/prometheus/config/prometheus.yml\">prometheus.yml</a>.</li>\n</ul><p>The <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/prometheus/config/prometheus.yml\">prometheus.yml</a> defines which REST endpoints Prometheus will consume to gather metrics from as well as a few other documented configurations. Prometheus will collect metrics from the following services:</p>\n<ul><li>Prometheus.</li>\n  <li>Cassandra.\n    <ul><li>Cassandra internal metrics.</li>\n      <li>collectd metrics.</li>\n    </ul></li>\n  <li>Reaper for Apache Cassandra.</li>\n</ul><h2 id=\"the-pickle-factory-sample-write-application-service\">The <code class=\"highlighter-rouge\">pickle-factory</code> Sample Write Application Service</h2>\n<p>The <code class=\"highlighter-rouge\">pickle-factory</code>, which is a misnomer and should be “pickle-farm” in hindsight, is a sample application that shows an ideal write pattern.</p>\n<p>For production and development purposes, the <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/pickle-factory/Dockerfile\">pickle-factory/Dockerfile</a> uses the line <code class=\"highlighter-rouge\">COPY . .</code> to copy all of the “microservices” code into the Docker image. The <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/docker-compose.yml\">docker-compose.yml</a> uses the <code class=\"highlighter-rouge\">volume</code> key to overwrite any pre-baked code to allow for a simpler development workflow.</p>\n<p>The ideal development to production workflow would be to modify the <code class=\"highlighter-rouge\">pickle-factory/factory.py</code> file directly and have each saved copy refreshed within the running container. Once all changes were tested and committed to <code class=\"highlighter-rouge\">master</code>, a continuous integration (CI) job will build the new Docker images and push those images to Docker Hub for both development and production consumption. The next developer to modify <code class=\"highlighter-rouge\">pickle-factory/factory.py</code> will grab the latest image but use their local copy of <code class=\"highlighter-rouge\">pickle-factory/factory.py</code>. The next time that the production image gets updated it will include the latest copy of <code class=\"highlighter-rouge\">pickle-factory/factory.py</code>, as found in master, directly in the Docker image.</p>\n<p>Included in the Dockerfile is commented out code for installing <code class=\"highlighter-rouge\">gosu</code>, a <code class=\"highlighter-rouge\">sudo</code> replacement for Docker written in Go. <code class=\"highlighter-rouge\">gosu</code> allows the main user to spawn another thread under a non-root user to better contain attackers. Theoretically, if an attacker gains access into the <code class=\"highlighter-rouge\">pickle-factory</code> container, they would do so under a non-privileged user and not be able to potentially breakout of the container into the Docker internals and out onto the host machine. Practically, <code class=\"highlighter-rouge\">gosu</code> limits the possible attack surface on a Docker container. To fully use the provided functionality of <code class=\"highlighter-rouge\">gosu</code>, <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/pickle-factory/docker-entrypoint.sh\">pickle-factory/docker-entrypoint.sh</a> needs to be modified as well.</p>\n<p>The <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/pickle-factory/factory.py\">pickle-factory/factory.py</a> makes a few important design decisions:</p>\n<ul><li>Grabs configurations from environmental variables instead of configuration files for easier Docker-first configurations.</li>\n  <li>The <code class=\"highlighter-rouge\">Cluster()</code> object uses the <code class=\"highlighter-rouge\">DCAwareRoundRobinPolicy</code> in preparation for multiple data centers.</li>\n  <li>The C-based <code class=\"highlighter-rouge\">LibevConnection</code> event loop provider is used since it’s more performant than the Python-native default event loop provider.</li>\n  <li>All statements are prepared in advance to minimize request payload sizes.\n    <ul><li>In the background, the Cassandra nodes will send the statements around to each node in the cluster.</li>\n      <li>The statements will be indexed via a simple integer.</li>\n      <li>All subsequent requests will map the simple integer to the pre-parsed CQL statement.</li>\n    </ul></li>\n  <li>Employee records are generated and written asynchronously instead of synchronously in an effort to improve throughput.</li>\n  <li>Workforce data is denormalized and written asynchronously with a max-buffer to ensure we don’t overload Cassandra with too many in-flight requests.</li>\n  <li>If any requests did not complete successfully the <code class=\"highlighter-rouge\">future.result()</code> call will throw the matching exception.</li>\n</ul><h2 id=\"the-pickle-shop-sample-read-application-service\">The <code class=\"highlighter-rouge\">pickle-shop</code> Sample Read Application Service</h2>\n<p>Much like the <code class=\"highlighter-rouge\">pickle-factory</code> microservice, the <code class=\"highlighter-rouge\">pickle-shop</code> service follows a similar workflow. However, the <code class=\"highlighter-rouge\">pickle-shop</code> service definition within <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/docker-compose.yml\">docker-compose.yml</a> will not overwrite the application code within <code class=\"highlighter-rouge\">/usr/src/app</code> and instead require a rebuilding the local image by using:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>docker-compose build\n</pre></div></div>\n<p>This workflow was chosen as a way to differentiate between a development workflow (<code class=\"highlighter-rouge\">pickle-factory</code>) and a production workflow (<code class=\"highlighter-rouge\">pickle-shop</code>). For production images, all code should ideally be baked into the Docker image and shipped without any external dependencies such as a codebase dependency.</p>\n<p><a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/pickle-shop/shop.py\">pickle-shop/shop.py</a> follows the same overall flow as <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/pickle-factory/factory.py\">pickle-factory/factory.py</a>. After preparing the statements, the read-heavy “microservice” completes the following actions:</p>\n<ul><li>Performs a synchronous read request to grab all the employee IDs.</li>\n  <li>10 asynchronous read queries are performed for 10 employees.</li>\n  <li>We then process all results at the roughly the same time.</li>\n</ul><p>If using a non-asynchronous driver, one would probably follow the alternate workflow:</p>\n<ul><li>Perform 1 synchronous read query for 1 of 10 employees.</li>\n  <li>Process the results of the employee query.</li>\n  <li>Repeat.</li>\n</ul><p>However, following the synchronous workflow will take roughly:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>O(N), where N is the number of employees waiting to be processed\n</pre></div></div>\n<p>Following the asynchronous workflow we will roughly take:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>O(max(N)), where max(N) is the maximum amount of time an employee query will take.\n</pre></div></div>\n<p>Simplified, the asynchronous workflow will roughly take:</p>\n<p>At the end of this series of information, we have covered:</p>\n<ul><li>Docker</li>\n  <li>Docker Compose</li>\n  <li>A few docker-compose.yml settings.</li>\n  <li>Dockerized Cassandra.</li>\n  <li>Helper Docker Compose services.</li>\n  <li>Dockerized Reaper for Apache Cassandra.</li>\n  <li>Dockerized Grafana.</li>\n  <li>Logspout for Docker-driven external logging.</li>\n  <li>Dockerized Prometheus.</li>\n  <li>A sample write-heavy asynchronous application using Cassandra.</li>\n  <li>A sample read-heavy asynchronous application using Cassandra.</li>\n</ul><p>TLP’s hope is that the above documentation and <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap\">minimal Docker Compose ecosystem</a> will provide a starting point for future community Proof of Concepts utilizing Apache Cassandra. With this project, each developer can create, maintain, and develop within their own local environment without any external overhead other than Docker and Docker Compose.</p>\n<p>Once the POC has reached a place of stability, simply adding the project to a Continuous Integration workflow to publish tested images will allow for proper Release Management. After that point, the responsibility typically falls onto the DevOps team to grab the latest Docker image, replace any previously existing Docker containers, and launch the new Docker image. This would all occur without any complicated hand off consisting of dependencies, configuration files (since we’re using environmental variables), and OS-specific requirements.</p>\n<p>Hopefully you will find Docker Compose to be as powerful as I’ve found it to be! Best of luck in cranking out the new POC you’ve been itching to create!</p>",
        "created_at": "2018-10-24T18:04:38+0000",
        "updated_at": "2018-10-24T18:04:45+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 22,
        "domain_name": "thelastpickle.com",
        "preview_picture": "http://thelastpickle.com/android-chrome-192x192.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12462"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 902,
            "label": "kubernetes",
            "slug": "kubernetes"
          },
          {
            "id": 1336,
            "label": "yugabyte",
            "slug": "yugabyte"
          }
        ],
        "is_public": false,
        "id": 12461,
        "uid": null,
        "title": "Understanding How YugaByte DB Runs on Kubernetes – The YugaByte Database Blog",
        "url": "https://blog.yugabyte.com/understanding-how-yugabyte-db-runs-on-kubernetes/",
        "content": "<p>As we reviewed in <a href=\"https://blog.yugabyte.com/docker-kubernetes-and-the-rise-of-cloud-native-databases/\">“Docker, Kubernetes and the Rise of Cloud Native Databases”</a>, Kubernetes has benefited from rapid adoption to become the de-facto choice for container orchestration. This has happened in a short span of only 4 years since Google open sourced the project in 2014. YugaByte DB’s <a href=\"https://blog.yugabyte.com/how-does-the-raft-consensus-based-replication-protocol-work-in-yugabyte-db/\">automated sharding and strongly consistent replication architecture</a> lends itself extremely well to containerized deployments powered by Kubernetes orchestration. In this post we’ll look at the various components involved in getting YugaByte DB up and running as <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\">Kubernetes StatefulSets</a>.</p><h2>YugaByte DB Architecture</h2><p>As shown in the architecture diagram below, YugaByte DB is comprised of two types of distributed services.</p><ul><li><strong>YB-Master:</strong> Responsible for keeping system metadata (such as shard-to-node mapping), coordinating system-wide operations (such as create/alter drop tables), and initiating maintenance operations (such as load-balancing). For fault-tolerance purposes, the number of YB-Masters equals the Replication Factor (RF) of the cluster. The minimum RF needed for fault-tolerance is 3.</li>\n<li><strong>YB-TServer:</strong> The data nodes responsible for hosting/serving user data in shards (also known as tablets). The number of data nodes can be increased or decreased on-demand in a cluster.</li>\n</ul><div id=\"attachment_286\" class=\"wp-caption aligncenter\"><img class=\"wp-image-286\" src=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturea-300x140.png\" alt=\"\" width=\"566\" height=\"264\" srcset=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturea-300x140.png 300w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturea-768x360.png 768w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturea.png 974w\" /><p class=\"wp-caption-text\">Architecture of a YugaByte DB Cluster</p></div><h2>Modeling YugaByte DB as a Workload on Kubernetes</h2><p><a href=\"https://blog.yugabyte.com/orchestrating-stateful-apps-with-kubernetes-statefulsets/\">Orchestrating Stateful Apps with Kubernetes</a> highlights how running stateful applications such as databases in Kubernetes require the use of the <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\">StatefulSets workload API</a>. In this context, YB-Master and YB-TServer are modeled as independent StatefulSets, as shown in the <a href=\"https://github.com/YugaByte/yugabyte-db/tree/master/cloud/kubernetes\">YugaByte DB Kubernetes YAML</a> on Github. Each of these StatefulSet pods instantiate one instance of the same <code>yugabytedb/yugabyte</code> container image but the command used to start the container is changed based on the type of server needed. The next few sections detail how exactly the YugaByte DB StatefulSets are structured in the context of running a four-node RF3 cluster on Kubernetes.</p><h2>Running YB-Master on Kubernetes</h2><p>The YB-Master deployment on Kubernetes needs one StatefulSet and two Services. One of these Services is the headless service that enables discovery of the underlying StatefulSet pods and the other is a LoadBalancer service needed to view the YB-Master Admin UI. YugaByte DB admin clients (such as the YugaByte DB EE Admin Console) connect to the any of the pods using the headless service, while admin users can connect to the LoadBalancer service.</p><div id=\"attachment_287\" class=\"wp-caption aligncenter\"><img class=\"wp-image-287\" src=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Pictureb-300x176.png\" alt=\"\" width=\"598\" height=\"351\" srcset=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Pictureb-300x176.png 300w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Pictureb-768x449.png 768w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Pictureb.png 974w\" /><p class=\"wp-caption-text\">YB-Master on Kubernetes</p></div><h3>yb-master StatefulSet</h3><p>The <a href=\"https://github.com/YugaByte/yugabyte-db/blob/master/cloud/kubernetes/yugabyte-statefulset.yaml\">YugaByte DB Kubernetes YAML</a> has a section for the yb-master StatefulSet. A few points to highlight in the specification.</p><p><strong>Replica count</strong></p><p>As shown in the figure above, to setup a 4-node YugaByte DB cluster with RF3, only 3 yb-master StatefulSet pods are needed. Hence the YAML setting for replicas to 3.</p><p><strong>Pod anti-affinity</strong></p><p>Pod anti-affinity rules are used to ensure no two yb-master pods can be scheduled onto the same Kubernetes node. This enforces maximum fault tolerance possible since a single node failure will only impact one yb-master pod and the cluster will continue functioning normally even with the remaining two yb-master pods on the two other nodes. Additionally, as noted in Kubernetes docs, the <code>preferredDuringSchedulingIgnoredDuringExecution</code> is a soft guarantee from Kubernetes that is better set to <code>requiredDuringSchedulingIgnoredDuringExecution</code> in mission-critical environments such as production.</p><p><strong>Communicating with other yb-masters</strong></p><p>Each yb-master gets to know of the other yb-masters with the –master_addresses flag populated using the fully qualified endpoint of the <code>yb-masters</code> headless service <code>yb-masters.default.svc.cluster.local:7100</code> (see next section).</p><p><strong>Ports</strong></p><p>The rpc port where other yb-masters and yb-tservers communicate is <code>7100</code> while the UI port for checking the current state of the master is <code>7000</code>.</p><p><strong>Volume mounts</strong></p><p>The <code>--fs_data_dirs</code> flag in the command points to the same disk <code>/mnt/data0</code> that is mounted to the container using the <code>datadir</code> volume mount.</p><p><strong>Update strategy</strong></p><p>The <code>RollingUpdate</code> strategy will update all the pods in the yb-master StatefulSet, in reverse ordinal order, while respecting the StatefulSet guarantees.</p><h3>yb-masters Headless service</h3><p>Kubernetes StatefulSets require the use of a headless service so that the StatefulSet pods can be discovered individually and communicated directly by other services (such as client applications). Kubernetes is not responsible for any load balancing across these pods. Such a headless service is created by simply specifying the clusterIP of the service to be <code>None</code>.</p><p>As shown above, the yb-masters headless service yaml is extremely simple. It simply opens up the UI and the rpc ports of the underlying yb-master pods.</p><h3>yb-master-ui LoadBalancer service</h3><p>The clusterwide admin UI for the yb-master can be viewed at the <code>7000</code> port of any yb-master. The <code>yb-master-ui</code> service is of the <code>LoadBalancer</code> type for this port which means that the service will load balance all the incoming requests across all the underlying pods.</p><h2>Running YB-TServer on Kubernetes</h2><p>Assuming you don’t need to view the YB-TServer’s Admin UI, the YB-TServer Kubernetes deployment needs one StatefulSet and one headless service. One important point to note is that the YB-Master service has to be up and running before the YB-TServer service.</p><div id=\"attachment_288\" class=\"wp-caption aligncenter\"><img class=\"wp-image-288\" src=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturec-300x152.png\" alt=\"\" width=\"572\" height=\"290\" srcset=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturec-300x152.png 300w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturec-768x390.png 768w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturec.png 974w\" /><p class=\"wp-caption-text\">YB-TServer and YB-Master on Kubernetes</p></div><h3>yb-tserver StatefulSet</h3><p>The YugaByte DB Kubernetes YAML has a section for the yb-master StatefulSet. A few points to highlight in the specification.</p><p><strong>Replica count</strong></p><p>As shown in the figure above, to setup a 4-node YugaByte DB cluster with RF3, 4 replicas of the yb-master StatefulSet pod are needed.</p><p><strong>Pod anti-affinity</strong></p><p>Pod anti-affinity rules are used to ensure no two yb-tserver pods can be scheduled onto the same Kubernetes node. This enforces maximum fault tolerance possible since a single node failure will only impact one yb-tserver pod and the cluster will continue functioning normally as long as quorum of yb-tserver pods are available. Note that 1 yb-tserver pod and 1 yb-master pod can be located on the same node. Again as noted in the yb-masters section, a stronger guarantee like <code>requiredDuringSchedulingIgnoredDuringExecution</code> is better for mission-critical environments.</p><p><strong>Communicating with yb-masters</strong></p><p>Each yb-tserver gets to know of the other yb-masters with the <code>--tserver_master_addrs</code> flag populated using the fully qualified endpoint of the yb-masters headless service <code>yb-masters.default.svc.cluster.local:7100</code>.</p><p><strong>Ports</strong></p><p>The rpc port where yb-masters and other yb-tservers communicate is <code>9100</code> while the UI port for checking the current state of the tserver is <code>9000</code>. Additionally, YCQL (the Cassandra compatible API) is available at port <code>9042</code> and YEDIS (the Redis compatible API) is available at port <code>6379</code>. PostgreSQL API, currently in beta, can be enabled by adding the port <code>5433</code>.</p><p><strong>Volume mounts</strong></p><p>The <code>--fs_data_dirs</code> flag points to the same disk /mnt/data0 that is mounted to the container using the datadir volume mount.</p><p><strong>Update strategy</strong></p><p>The <code>RollingUpdate</code> update strategy will update all the pods in the yb-tserver StatefulSet, in reverse ordinal order, while respecting the StatefulSet guarantees.</p><h3>yb-tservers Headless service</h3><p>As expected, the yb-tservers headless service yaml is extremely simple. It opens up the UI, the rpc ports as well as client API ports of the underlying yb-tserver pods.</p><h2>YugaByte DB on Kubernetes in Action</h2><p>In order to keep things simple to understand, we will run a 4-node YugaByte DB cluster on <a href=\"https://kubernetes.io/docs/setup/minikube/\">minikube</a>, the preferred method for running Kubernetes on your local environment.</p><h3>Prerequisites</h3><p>Follow the instructions to <a href=\"https://kubernetes.io/docs/tasks/tools/install-minikube/\">install minikube and kubectl</a> if you don’t have them setup already.</p><h3>Step 1 – Download the YugaByte DB Kubernetes YAML</h3><h3>Step 2 – Change the yb-tserver replica count from 3 to 4</h3><p>Open the the YAML in the editor of your choice and set the yb-tserver replica count to 4.</p><h3>Step 3 – Create the YugaByte DB cluster</h3><p>Now you can create the YugaByte DB cluster through the following command.</p><h3>Step 4 – Check status of the pods and services</h3><p>Since Kubernetes has to first pull the yugabytedb/yugabyte image from hub.docker.com, the cluster may take a few minutes to become live. You can check the status using the following commands.</p><p>When the cluster is ready, it will have all the 7 pods (3 for yb-master and 4 for yb-tserver) in the Running status.</p><p>You can also check the status of the 3 services we launched along with the status of the default kubernetes service itself.</p><p><code>NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE<br />kubernetes ClusterIP 10.96.0.1 443/TCP 10m<br />yb-master-ui LoadBalancer 10.102.121.64 7000:31283/TCP 8m<br />yb-masters ClusterIP None 7000/TCP,7100/TCP 8m<br />yb-tservers ClusterIP None 9000/TCP,9100/TCP,9042/TCP,6379/TCP 8m</code></p><p>Finally, you can view the nice UI dashboard provided by Kubernetes that you can launch by the following command.</p><div id=\"attachment_290\" class=\"wp-caption aligncenter\"><img class=\"wp-image-290\" src=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picture1d-300x153.png\" alt=\"\" width=\"626\" height=\"319\" srcset=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picture1d-300x153.png 300w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picture1d-768x392.png 768w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picture1d.png 974w\" /><p class=\"wp-caption-text\">Kubernetes Dashboard with YugaByte DB Installed</p></div><h3>Step 5 – View the YB-Master Admin UI</h3><p>Once the cluster is live, you can launch the YB-Master Admin UI. First use the command below to get the exact URL for the UI and then launch the URL via the browser.</p><div id=\"attachment_289\" class=\"wp-caption aligncenter\"><img class=\"wp-image-289\" src=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturee-300x162.png\" alt=\"\" width=\"603\" height=\"326\" srcset=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturee-300x162.png 300w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturee-768x414.png 768w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturee.png 974w\" /><p class=\"wp-caption-text\">YB-Master Admin UI</p></div><h3>Step 6 – Perform Day 2 Operational Tasks</h3><p>The next few steps show how to perform common day 2 operational tasks such as adding/removing nodes and performing rolling upgrades. All these operations do not impact the availability and performance of client applications thus allowing the applications to continue to operate normally.</p><p><strong>Add a Node</strong></p><p>Horizontal scaling is a breeze with YugaByte DB and with Kubernetes, the process could not be simpler. All we have to do is to let Kubernetes know how many replicas to scale to.</p><p>Now we can check the status of the scaling operation. Note that YugaByte DB automatically moves a few tablet-leaders and a few tablet-followers into the newly added node so that the cluster remains balanced across all the nodes.</p><p><strong>Remove Two Nodes</strong></p><p>Removing nodes is also very simple. Reduce the number of replicas and see the combination of Kubernetes and YugaByte DB do the rest.<br /><code>kubectl scale statefulset yb-tserver --replicas=3</code></p><p>As expected in StatefulSets, we can see that the nodes with the largest ordinal indexes (i.e. 4 and 3) are removed first.</p><p><strong>Perform Rolling Upgrade</strong></p><p>We can also perform rolling upgrades on the YugaByte DB cluster. This involves changing the YugaByte DB container image to a different version first on the yb-master StatefulSet and then on the yb-tserver StatefulSet. As expected in StatefulSets, we can see that the nodes with the largest ordinal indexes are upgraded first.</p><p>Upgrading the yb-master StatefulSet uses the command below. Assuming the new container image is not already available with Kubernetes, the image will be pulled from hub.docker.com first and this may result in the first pod upgrade taking a few minutes.</p><p>Now we can upgrade the yb-tserver StatefulSet as well. This will lead to the yb-tserver pods getting upgraded in the same way we saw for the yb-master pods.</p><h2>Summary</h2><p>Running distributed databases using a distributed orchestration technology such as Kubernetes continues to remain a non-trivial problem. YugaByte DB is a distributed database with a unique sharding and replication architecture that makes it a perfect fit for Kubernetes-based orchestration. In this post, we reviewed the underlying details of how YugaByte DB runs on Kubernetes and how this looks in action in the context of a real cluster. As part of our upcoming 1.1 release, we expect to release additional Kubernetes-related enhancements such as running the YugaByte DB Enterprise Admin Console on the same Kubernetes cluster as YugaByte DB. Subscribe to our blog at the bottom of this page and stay tuned with our progress.</p><h2>What’s Next?</h2><ul><li>Read <a href=\"https://blog.yugabyte.com/orchestrating-stateful-apps-with-kubernetes-statefulsets/\">“Orchestrating Stateful Apps with Kubernetes.”</a></li>\n<li><a href=\"https://docs.yugabyte.com/latest/comparisons/\">Compare YugaByte DB to databases like Amazon DynamoDB, Cassandra, MongoDB and Azure Cosmos DB.</a></li>\n<li><a href=\"https://docs.yugabyte.com/latest/quick-start/\">Get started with YugaByte DB on Kubernetes.</a></li>\n<li><a href=\"https://www.yugabyte.com/about/contact/\">Contact us</a> to learn more about licensing, pricing or to schedule a technical overview.</li>\n</ul>",
        "created_at": "2018-10-24T18:01:47+0000",
        "updated_at": "2018-10-24T18:01:54+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 9,
        "domain_name": "blog.yugabyte.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12461"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          },
          {
            "id": 902,
            "label": "kubernetes",
            "slug": "kubernetes"
          }
        ],
        "is_public": false,
        "id": 12460,
        "uid": null,
        "title": "Deploy a highly-available Cassandra cluster in AWS using Kubernetes",
        "url": "https://medium.com/merapar/deploy-a-high-available-cassandra-cluster-in-aws-using-kubernetes-bd8ba07bfcdd",
        "content": "<div class=\"section-inner sectionLayout--insetColumn\"><figure id=\"9017\" class=\"graf graf--figure graf--leading\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*kdSlB9fHygMjSY-4ZC568A.png\" data-width=\"603\" data-height=\"304\" src=\"https://cdn-images-1.medium.com/max/1600/1*kdSlB9fHygMjSY-4ZC568A.png\" alt=\"image\" /></div></div></figure><h1 id=\"8079\" class=\"graf graf--h3 graf-after--figure graf--title\">Deploy a highly-available Cassandra cluster in AWS using Kubernetes</h1><p id=\"48e3\" class=\"graf graf--p graf-after--h3\">At Merapar, we use Cassandra as the back-bone for some of our highly-available cloud solutions. Cassandra is a highly available, highly performant, truly horizontal scalable NoSQL database. However, deploying Cassandra in the cloud in a highly-available manner is a non-trivial task and needs proper configuration. Fortunately, due to the emergence of new technologies, deploying a Cassandra cluster in AWS is more easy nowadays. This blog will show you how to deploy a Cassandra cluster in AWS, using kops (Kubernetes Operations). It will also show that the setup is highly-available by testing some failure scenarios.</p><p id=\"e322\" class=\"graf graf--p graf-after--p\">This blog assumes basic knowledge of AWS, Kubernetes and Cassandra.</p><h3 id=\"8bab\" class=\"graf graf--h3 graf-after--p\">Deployment</h3><p id=\"1d48\" class=\"graf graf--p graf-after--h3\">The following picture shows what the final Cassandra deployment in AWS looks like:</p></div><div class=\"section-inner sectionLayout--outsetColumn\"><figure id=\"4ae7\" class=\"graf graf--figure graf--layoutOutsetCenter graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*mp72MLMoTnlsnR0VBLomCw.png\" data-width=\"1879\" data-height=\"698\" data-action=\"zoom\" data-action-value=\"1*mp72MLMoTnlsnR0VBLomCw.png\" src=\"https://cdn-images-1.medium.com/max/2000/1*mp72MLMoTnlsnR0VBLomCw.png\" alt=\"image\" /></div></div></figure></div><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"b45b\" class=\"graf graf--p graf-after--figure\">Let’s describe the deployment in detail:</p><ul class=\"postList\"><li id=\"1ae9\" class=\"graf graf--li graf-after--p\">The deployment uses one geographical region: eu-west-1 (Ireland)</li><li id=\"f37e\" class=\"graf graf--li graf-after--li\">Within this region, three availability zones are used. An availability zone is an isolated area within a region, i.e. a failure in one availability zone does not impact operations in another availability zone. Hence, high-availability is achieved by spreading processing and storage across availability zones.</li><li id=\"8ccd\" class=\"graf graf--li graf-after--li\">There are three Kubernetes masters, one in each availability zone. A Kubernetes master schedules the pods (containers), that is, it determines which Kubernetes node runs which pod. By having a master in each availability-zone, the Kubernetes masters can continue to schedule pods, even if one availability-zone is down. The master machines are deployed in an auto-scaling group, so if an EC2 instance in this group terminates, a new EC2 instance (and Kubernetes master) is started automatically.</li><li id=\"11ba\" class=\"graf graf--li graf-after--li\">There are six Kubernetes nodes, two in each availability zone. A Kubernetes node runs your application pods. In this setup, each Kubernetes node will run one Cassandra pod. If one pod fails, a new one will be scheduled by the Kubernetes master. The Kubernetes nodes are also deployed in an auto-scaling group.</li><li id=\"e961\" class=\"graf graf--li graf-after--li\">A Kubernetes stateful set with persistent volumes is used to deploy Cassandra: each Cassandra pod has a known identity (e.g. cassandra-0) and a known volume (e.g. cassandra-storage-cassandra-0). The pod identity and volume identity are tightly coupled. This enables a Cassandra pod to restart on another node and transfer its state. When a Cassandra pod starts, it attaches the same EBS volume as previously, and therefore, has the same state as before. EBS volumes are automatically created the first time a Cassandra pod starts.</li></ul><p id=\"bbe6\" class=\"graf graf--p graf-after--li\">Now let’s discuss how Cassandra has to be configured in order to replicate the data in multiple availability-zones. This is achieved by setting the following properties:</p><ul class=\"postList\"><li id=\"49bd\" class=\"graf graf--li graf-after--p\">Snitch.<br />The snitch determines to which data-center and rack a node belongs. Cassandra uses the terms “data-center” and “rack” to identify the network topology. The EC2Snitch is used. When a Cassandra node starts, the EC2Snitch retrieves the region and availability-zone information from the EC2 meta-data endpoint: the data-center is set to the region and the rack is set to the availability-zone.</li><li id=\"6f31\" class=\"graf graf--li graf-after--li\">Replication factor.<br />This determines the number of data copies. A replication-factor of three is used, i.e. three replicas are stored on different nodes.</li><li id=\"a846\" class=\"graf graf--li graf-after--li\">Replication strategy<br />This property determines which nodes store the replicas. The NetworkTopologyStrategy is used. With this strategy, a replica is stored in each availability-zone. How this is done precisely, is discussed next.</li></ul><p id=\"3fed\" class=\"graf graf--p graf-after--li\">The following picture shows how data is stored. Each Cassandra node creates multiple tokens (32 in our case). Each token is a random number between -2⁶³ to 2⁶³ -1. The Cassandra token ring is a virtual ring in token order of all tokens (192 in our case: 32 tokens * 6 nodes). When a row must be stored, its key is hashed and the result determines where on the ring the record is stored. Imagine the key of a record is hashed to a number between 1829762156858167353 and 1843966738638345890. In the picture below you can see that this corresponds to the top token. This token belongs to cassandra-2. This means cassandra-2 in zone 1b stores the record. Subsequently, the ring is followed clockwise until two additional nodes are found in other availability-zones. In this example, the other replicas are stored on cassandra-3 and cassandra-1.</p><figure id=\"0982\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*p5_BEyYVQ8LK7S6X6gdDqg.png\" data-width=\"1112\" data-height=\"458\" data-action=\"zoom\" data-action-value=\"1*p5_BEyYVQ8LK7S6X6gdDqg.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*p5_BEyYVQ8LK7S6X6gdDqg.png\" alt=\"image\" /></div></div></figure><h3 id=\"cba7\" class=\"graf graf--h3 graf-after--figure\">Step-by-step setup</h3><p id=\"c8bf\" class=\"graf graf--p graf-after--h3\">Next we will install the described deployment from scratch. In order to follow these steps, you only need an AWS account and a Linux machine to run the commands from. We use the following versions for the various components:</p><ul class=\"postList\"><li id=\"424d\" class=\"graf graf--li graf-after--p\">Ubuntu 16.04</li><li id=\"2ef5\" class=\"graf graf--li graf-after--li\">Kops 1.8.1</li><li id=\"3304\" class=\"graf graf--li graf-after--li\">Kubernetes 1.7.16</li><li id=\"8e88\" class=\"graf graf--li graf-after--li\">Cassandra 2.2.9</li></ul><p id=\"3c6f\" class=\"graf graf--p graf-after--li\">We use Cassandra 2.2.9 in production with only a few changes from the default configuration. For details see: <a href=\"https://github.com/merapar/cassandra-docker/tree/master/docker\" data-href=\"https://github.com/merapar/cassandra-docker/tree/master/docker\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://github.com/merapar/cassandra-docker/tree/master/docker</a>. Note that this setup also works perfectly well with newer versions of Cassandra.</p><h4 id=\"01dc\" class=\"graf graf--h4 graf-after--p\">Kubernetes setup</h4><p id=\"bad1\" class=\"graf graf--p graf-after--h4\">First, you need the kops command, which is used to setup the infrastructure in AWS:</p><pre id=\"efd7\" class=\"graf graf--pre graf-after--p\">curl -LO <a href=\"https://github.com/kubernetes/kops/releases/download/1.8.1/kops-linux-amd64\" data-href=\"https://github.com/kubernetes/kops/releases/download/1.8.1/kops-linux-amd64\" class=\"markup--anchor markup--pre-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://github.com/kubernetes/kops/releases/download/1.8.1/kops-linux-amd64</a> <br />sudo mv kops-linux-amd64 /usr/local/bin/kops &amp;&amp; sudo chmod a+x /usr/local/bin/kops</pre><p id=\"f49d\" class=\"graf graf--p graf-after--pre\">You need the kubectl command to interact with the Kubernetes cluster in AWS:</p><pre id=\"6f62\" class=\"graf graf--pre graf-after--p\">curl -LO <a href=\"https://storage.googleapis.com/kubernetes-release/release/v1.7.16/bin/linux/amd64/kubectl\" data-href=\"https://storage.googleapis.com/kubernetes-release/release/v1.7.16/bin/linux/amd64/kubectl\" class=\"markup--anchor markup--pre-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://storage.googleapis.com/kubernetes-release/release/v1.7.16/bin/linux/amd64/kubectl</a><br />sudo mv kubectl /usr/local/bin/kubectl &amp;&amp; sudo chmod a+x /usr/local/bin/kubectl</pre><p id=\"a9d4\" class=\"graf graf--p graf-after--pre\">Kops uses the awscli command to interact with AWS. On Ubuntu, you can install this tool via:</p><pre id=\"20f2\" class=\"graf graf--pre graf-after--p\">apt-get install awscli</pre><p id=\"4265\" class=\"graf graf--p graf-after--pre\">Other means to install awscli can be found here: <a href=\"https://docs.aws.amazon.com/cli/latest/userguide/installing.html\" data-href=\"https://docs.aws.amazon.com/cli/latest/userguide/installing.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://docs.aws.amazon.com/cli/latest/userguide/installing.html</a></p><p id=\"7635\" class=\"graf graf--p graf-after--p\">Now you have to make sure there is an IAM user which kops can use to install the required components in AWS. The IAM user needs programmatic access (i.e. use an access-key and secret-access-key to login). In addition, the IAM user requires the following permissions: AmazonEC2FullAccess AmazonRoute53FullAccess AmazonS3FullAccess IAMFullAccess AmazonVPCFullAccess. More info can be found here: <a href=\"https://github.com/kubernetes/kops/blob/master/docs/aws.md\" data-href=\"https://github.com/kubernetes/kops/blob/master/docs/aws.md\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://github.com/kubernetes/kops/blob/master/docs/aws.md</a></p><p id=\"94fa\" class=\"graf graf--p graf-after--p\">Now let’s configure AWS:</p><pre id=\"3a53\" class=\"graf graf--pre graf-after--p\">aws configure</pre><p id=\"9692\" class=\"graf graf--p graf-after--pre\">Enter the credentials of the IAM user created in the previous step. We use eu-west-1 as the region. Accept the default output format. Next you have to create an S3 bucket. Kops stores the configuration of the deployment in this bucket</p><pre id=\"cadd\" class=\"graf graf--pre graf-after--p\">aws s3api create-bucket --bucket kops-cassandra-blog --region eu-west-1</pre><p id=\"8122\" class=\"graf graf--p graf-after--pre\">Now generate a public/private key-pair:</p><pre id=\"90ce\" class=\"graf graf--pre graf-after--p\">ssh-keygen -f kops-cassandra-blog</pre><p id=\"50ef\" class=\"graf graf--p graf-after--pre\">This key-pair is used to access the EC2 machines. The following command creates the cluster definition:</p><pre id=\"fef7\" class=\"graf graf--pre graf-after--p\">kops create cluster \\<br />--cloud=aws \\<br />--name=kops-cassandra-blog.k8s.local \\<br />--zones=eu-west-1a,eu-west-1b,eu-west-1c \\<br />--master-size=\"t2.small\" \\<br />--master-zones=eu-west-1a,eu-west-1b,eu-west-1c \\<br />--node-size=\"t2.small\" \\<br />--ssh-public-key=\"kops-cassandra-blog.pub\" \\<br />--state=s3://kops-cassandra-blog \\<br />--node-count=6</pre><p id=\"0054\" class=\"graf graf--p graf-after--pre\">Now apply the cluster definition, i.e. create the actual resources in AWS:</p><pre id=\"5302\" class=\"graf graf--pre graf-after--p\">kops update cluster --name=kops-cassandra-blog.k8s.local --state=s3://kops-cassandra-blog --yes</pre><p id=\"cdb3\" class=\"graf graf--p graf-after--pre\">After a few minutes, we will have a high-available Kubernetes cluster in AWS. Kops automatically configures kubectl. Use the following command to check the Kubernetes master nodes (the -L argument shows labels, while the -l argument filters on labels):</p><pre id=\"aa24\" class=\"graf graf--pre graf-after--p\">kubectl get no -L failure-domain.beta.kubernetes.io/zone -l kubernetes.io/role=master</pre><p id=\"b7b1\" class=\"graf graf--p graf-after--pre\">The following output is visible:</p><pre id=\"9841\" class=\"graf graf--pre graf-after--p\">NAME               STATUS  AGE  VERSION  ZONE<br />ip-172-20-112-210  Ready   1m   v1.8.7   eu-west-1c<br />ip-172-20-58-140   Ready   1m   v1.8.7   eu-west-1a<br />ip-172-20-85-234   Ready   1m   v1.8.7   eu-west-1b</pre><p id=\"e324\" class=\"graf graf--p graf-after--pre\">As can be seen, the three Kubernetes masters each reside in a separate availability zone. Now run the same command for the Kubernetes nodes:</p><pre id=\"f986\" class=\"graf graf--pre graf-after--p\">kubectl get no -L failure-domain.beta.kubernetes.io/zone -l kubernetes.io/role=node</pre><p id=\"d73a\" class=\"graf graf--p graf-after--pre\">As can be seen in the output, each availability zone has two Kubernetes nodes:</p><pre id=\"6bd9\" class=\"graf graf--pre graf-after--p\">NAME               STATUS    AGE  VERSION  ZONE<br />ip-172-20-114-66   Ready     1m   v1.8.7   eu-west-1c<br />ip-172-20-116-132  Ready     1m   v1.8.7   eu-west-1c<br />ip-172-20-35-200   Ready     1m   v1.8.7   eu-west-1a<br />ip-172-20-42-220   Ready     1m   v1.8.7   eu-west-1a<br />ip-172-20-94-29    Ready     1m   v1.8.7   eu-west-1b<br />ip-172-20-94-34    Ready     1m   v1.8.7   eu-west-1b</pre><p id=\"c29f\" class=\"graf graf--p graf-after--pre\">You can destroy the environment at all times by running the following command:</p><pre id=\"6aad\" class=\"graf graf--pre graf-after--p\">kops delete cluster --name=kops-cassandra-blog.k8s.local --state=s3://kops-cassandra-blog --yes</pre><h4 id=\"7f99\" class=\"graf graf--h4 graf-after--pre\">Cassandra setup</h4><p id=\"d23d\" class=\"graf graf--p graf-after--h4\">Create a file cassandra.yml, containing the following definitions:</p><pre id=\"1206\" class=\"graf graf--pre graf-after--p\">apiVersion: v1<br />kind: Service<br />metadata:<br />  name: cassandra<br />spec:<br />  clusterIP: None<br />  ports:<br />    - name: cql<br />      port: 9042<br />  selector:<br />    app: cassandra<br />---<br />apiVersion: \"apps/v1beta1\"<br />kind: StatefulSet<br />metadata:<br />  name: cassandra<br />spec:<br />  serviceName: cassandra<br />  replicas: 6<br />  template:<br />    metadata:<br />      labels:<br />        app: cassandra<br />    spec:<br />      affinity:<br />        podAntiAffinity:<br />          requiredDuringSchedulingIgnoredDuringExecution:<br />          - topologyKey: kubernetes.io/hostname<br />            labelSelector:<br />              matchLabels:<br />                app: cassandra<br />      containers:<br />        - env:<br />            - name: MAX_HEAP_SIZE<br />              value: 512M<br />            - name: HEAP_NEWSIZE<br />              value: 512M<br />            - name: POD_IP<br />              valueFrom:<br />                fieldRef:<br />                  fieldPath: status.podIP<br />          image: merapar/cassandra:2.3<br />          name: cassandra<br />          volumeMounts:<br />            - mountPath: /cassandra-storage<br />              name: cassandra-storage<br />  volumeClaimTemplates:<br />  - metadata:<br />      name: cassandra-storage<br />    spec:<br />      accessModes:<br />      - ReadWriteOnce<br />      resources:<br />        requests:<br />          storage: 10Gi</pre><p id=\"c3fd\" class=\"graf graf--p graf-after--pre\">And run</p><pre id=\"d24e\" class=\"graf graf--pre graf-after--p\">kubectl create -f cassandra.yml</pre><p id=\"72a5\" class=\"graf graf--p graf-after--pre\">The following components are installed within the timespan of a few minutes:</p><ul class=\"postList\"><li id=\"40bd\" class=\"graf graf--li graf-after--p\">Service cassandra. This service is used by clients within the Kubernetes cluster to connect to Cassandra. It does not have a cluster-IP. This is on purpose because Cassandra node-discovery and load-balancing is handled by the Cassandra client itself (not via Kubernetes). The client library connects to one contact point only: the cassandra DNS name. This is translated by the DNS pod to the IP address of one of the Cassandra pods. That pod will tell the IP addresses of the other Cassandra pods.</li><li id=\"7b4d\" class=\"graf graf--li graf-after--li\">StatefulSet cassandra. The stateful set makes sure that there are six Cassandra pods running at all times with a fixed identity: cassandra-0 up to and including cassandra-5.</li></ul><p id=\"9d64\" class=\"graf graf--p graf-after--li\">In order to connect to the Cassandra cluster, we use the cqlsh command which is available on each node:</p><pre id=\"0531\" class=\"graf graf--pre graf-after--p\">kubectl exec -ti cassandra-0 cqlsh cassandra-0</pre><p id=\"1ce1\" class=\"graf graf--p graf-after--pre\">This opens a CQL prompt and lets you interact with the cluster using CQL. The command “cqlsh cassandra-0” actually connects to the server listed in the first argument (cassandra-0). So in this case, it connects to itself.</p><p id=\"bae3\" class=\"graf graf--p graf-after--p\">Now we are going to create a key-space, a table and 100 records. First set the consistency level:</p><pre id=\"1b02\" class=\"graf graf--pre graf-after--p\">CONSISTENCY QUORUM;</pre><p id=\"8c24\" class=\"graf graf--p graf-after--pre\">Quorum means that a majority of the replica’s (2 in our case) must be read or written in order for the read or write command to succeed. Now create the key-space:</p><pre id=\"c6b4\" class=\"graf graf--pre graf-after--p\">CREATE KEYSPACE test WITH REPLICATION = { 'class' : 'NetworkTopologyStrategy', 'eu-west' : 3 };</pre><p id=\"7daa\" class=\"graf graf--p graf-after--pre\">Switch to the test key-space:</p><pre id=\"84e8\" class=\"graf graf--pre graf-after--p\">USE test;</pre><p id=\"8856\" class=\"graf graf--p graf-after--pre\">Create a table</p><pre id=\"d230\" class=\"graf graf--pre graf-after--p\">CREATE TABLE persons (id uuid, name text, PRIMARY KEY (id));</pre><p id=\"f17f\" class=\"graf graf--p graf-after--pre\">Now run the following command 100 times to insert 100 records. We need enough records so that each node contains replicas.</p><pre id=\"97e0\" class=\"graf graf--pre graf-after--p\">INSERT INTO persons (id,name) VALUES (uuid(),'name');</pre><p id=\"3d53\" class=\"graf graf--p graf-after--pre\">You can also run a script from the cassandra-0 machine:</p><pre id=\"9e3d\" class=\"graf graf--pre graf-after--p\">kubectl exec -ti cassandra-0 bash</pre><p id=\"a11c\" class=\"graf graf--p graf-after--pre\">With the following loop:</p><pre id=\"b373\" class=\"graf graf--pre graf-after--p\">for i in {1..100}<br /> do<br />   echo \"adding customer $i\"<br />   cqlsh cassandra-0 -e \"USE test; CONSISTENCY QUORUM; INSERT INTO persons (id,name) VALUES (uuid(),'name');\"<br /> done</pre><p id=\"0d92\" class=\"graf graf--p graf-after--pre\">Now run the following command and make sure 100 records are returned:</p><pre id=\"4041\" class=\"graf graf--pre graf-after--p\">SELECT * FROM persons;</pre><h3 id=\"e3e9\" class=\"graf graf--h3 graf-after--pre\">Testing the high-availability</h3><p id=\"a154\" class=\"graf graf--p graf-after--h3\">Now that we have a Cassandra cluster in AWS with some data inside, we can test the high-availability. Note that this setup is only resilient against failures in one availability-zone. In order to be resilient against multiple concurrent availability-zone failures, one should consider using a disaster recovery site in another region. While running the failure scenarios in the following sections, the select query presented earlier on should always return 100 records, i.e. Cassandra should be high-available (all data should be available) at all times.</p><h4 id=\"6473\" class=\"graf graf--h4 graf-after--p\">EC2 instance failure</h4><p id=\"2b6f\" class=\"graf graf--p graf-after--h4\">Note that we will only test Kubernetes-node failure (not Kubernetes-master failure). Let’s terminate an EC2 instance via the AWS console. We should not terminate an EC2 instance running the cassandra-0 pod. Otherwise our CQL prompt terminates. Via the following commands:</p><pre id=\"a6e3\" class=\"graf graf--pre graf-after--p\">kubectl get no -L failure-domain.beta.kubernetes.io/zone<br />kubectl get po -o wide</pre><p id=\"fc8f\" class=\"graf graf--p graf-after--pre\">We can construct the following table:</p><pre id=\"eac3\" class=\"graf graf--pre graf-after--p\">Cassandra-node  EC2 instance       Availability-zone<br />----------------------------------------------------<br />cassandra-0     ip-172-20-94-34    eu-west-1b<br />cassandra-1     ip-172-20-116-132  eu-west-1c<br />cassandra-2     ip-172-20-42-220   eu-west-1a<br />cassandra-3     ip-172-20-94-29    eu-west-1b<br />cassandra-4     ip-172-20-114-66   eu-west-1c<br />cassandra-5     ip-172-20-35-200   eu-west-1a</pre><p id=\"8dca\" class=\"graf graf--p graf-after--pre\">For this test, we will terminate instance ip-172–20–116–132 which will terminate cassandra-1. AWS will try to launch a new EC2 instance in the availability zone with the fewest instances. In our case, the auto-scaling group called “nodes” contains one instance for the eu-west-1c zone, while it contains two for the other zones. Therefore, the new instance is launched in eu-west-1c. Note that this is best effort; if a whole availability-zone is down for an extended period of time, manual intervention is required to recover. This will be discussed later-on. When requesting the pods, the following output is visible:</p><pre id=\"70ec\" class=\"graf graf--pre graf-after--p\">NAME          READY     STATUS    RESTARTS   AGE<br />cassandra-0   1/1       Running   0          1h<br />cassandra-1   0/1       Pending   0          8s<br />cassandra-2   1/1       Running   1          1h<br />cassandra-3   1/1       Running   4          1h<br />cassandra-4   1/1       Running   0          1h<br />cassandra-5   1/1       Running   0          1h</pre><p id=\"52c3\" class=\"graf graf--p graf-after--pre\">While the EC2 instance is starting, the status of the pod is pending. The read-query, as expected, still returns 100 rows. This is because we use quorum reads: we need two of the three replicas and since all zones contain one replica, the read succeeds. Pod cassandra-1 will be rescheduled to the new EC2 instance. The following policies apply during rescheduling:</p><ul class=\"postList\"><li id=\"877e\" class=\"graf graf--li graf-after--p\">When a Kubernetes node is started, it automatically gets a label with availability-zone information. When Kubernetes schedules pods of a stateful set, it tries to spread them across the availability-zones. Persistent volumes (the EBS volume) of a pod are also located in a particular availability-zone. When a persistent volume is created, it too gets a label with availability-zone information. Now, when a pod is scheduled and claims a volume, Kubernetes makes sure the pod is scheduled to a node in the same availability zone as the volume. In our case, cassandra-1 must be rescheduled. This pod claims (wants to link) volume cassandra-storage-cassandra-1. Since this volume is located in zone eu-west-1c, cassandra-1 will get scheduled on a node running in eu-west-1c</li><li id=\"8aea\" class=\"graf graf--li graf-after--li\">We use anti-pod-affinity to make sure a Kubernetes node runs a maximum of one Cassandra pod. Although it is perfectly viable to run without this policy, it has two benefits: While the new EC2 instance is starting, the Cassandra pod is not started on the remaining nodes. Therefore, no manual rescheduling is required afterwards ( in order to balance the pods). The second benefit is that the full resources of the node are available for the Cassandra node. Note that the same can be achieved using other means (e.g. using resource quotas)</li></ul><h4 id=\"a30a\" class=\"graf graf--h4 graf-after--li\">Availability zone failure</h4><p id=\"892a\" class=\"graf graf--p graf-after--h4\">In order to test this scenario, we will terminate Cassandra nodes running in zone eu-west-1a: cassandra-2 (EC2 instance ip-172–20–42–220) and cassandra-5 (EC2 instance ip-172–20–35–200). The “get pods” command now shows only five nodes, of which one is in the pending state:</p><pre id=\"d71d\" class=\"graf graf--pre graf-after--p\">NAME          READY     STATUS    RESTARTS   AGE<br />cassandra-0   1/1       Running   0          2h<br />cassandra-1   1/1       Running   0          18m<br />cassandra-2   0/1       Pending   0          55s<br />cassandra-3   1/1       Running   4          2h<br />cassandra-4   1/1       Running   0          2h</pre><p id=\"386d\" class=\"graf graf--p graf-after--pre\">Since Cassandra replicates data across all zones, all data is still available. This can be confirmed by running the read query which still return 100 records.</p><p id=\"dcc0\" class=\"graf graf--p graf-after--p\">The recovery process for this scenario is basically the same as the single EC2 instance failure scenario describe in the previous scenario. Although very rare, an availability-zone can fail in such a way that instances cannot be restarted in the failing zone, but are started in another zone instead. More information can be found here: <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html\" data-href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html</a></p><p id=\"ab2d\" class=\"graf graf--p graf-after--p\">The previous scenario described how pods with volume claims are scheduled. Because there will be a mismatch between the new EC2 instance zone-info and the volume zone-info, the pods cannot reschedule. Manual intervention is required. When the failing availability-zone is up again, terminate all machines which could previously not start in the failing availability-zone and all should recover automatically.</p><h3 id=\"d549\" class=\"graf graf--h3 graf-after--p\">Final remarks</h3><p id=\"5e58\" class=\"graf graf--p graf-after--h3 graf--trailing\">In this post, we have shown how to deploy a high-available Cassandra cluster in AWS. We have also shown that the deployment automatically recovers from node failures in the same availability zone. In a next post, we will discuss scaling the cluster horizontally. Another topic, not discussed here, is performance, which might also be the subject of a future post.</p></div>",
        "created_at": "2018-10-24T17:55:31+0000",
        "updated_at": "2019-01-15T15:23:13+0000",
        "published_at": "2018-04-20T13:15:18+0000",
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 14,
        "domain_name": "medium.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*kdSlB9fHygMjSY-4ZC568A.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12460"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 15,
            "label": "tutorial",
            "slug": "tutorial"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 902,
            "label": "kubernetes",
            "slug": "kubernetes"
          }
        ],
        "is_public": false,
        "id": 12459,
        "uid": null,
        "title": "Example: Deploying Cassandra with Stateful Sets",
        "url": "https://kubernetes.io/docs/tutorials/stateful-application/cassandra/",
        "content": "<p>Example: Deploying Cassandra with Stateful Sets - Kubernetes</p><header><p></p><h5><a href=\"https://kubernetes.io/docs/home/\">DOCUMENTATION</a>\n\t\t\n\t\t\n\t\t<a href=\"https://kubernetes.io/docs/setup/\">SETUP</a>\n\t\t\n\t\t\n\t\t<a href=\"https://kubernetes.io/docs/concepts/\">CONCEPTS</a>\n\t\t\n\t\t\n\t\t<a href=\"https://kubernetes.io/docs/tasks/\">TASKS</a>\n\t\t\n\t\t\n\t\t<a href=\"https://kubernetes.io/docs/tutorials/\" class=\"YAH\">TUTORIALS</a>\n\t\t\n\t\t\n\t\t<a href=\"https://kubernetes.io/docs/reference/\">REFERENCE</a>\n\t\t\n\t\t\n\t\t<a href=\"https://kubernetes.io/docs/contribute/\">CONTRIBUTE</a>\n\t\t\n\t</h5><section id=\"encyclopedia\"><div id=\"docsContent\"><p><a href=\"https://github.com/kubernetes/website/edit/master/content/en/docs/tutorials/stateful-application/cassandra.md\" id=\"editPageButton\" target=\"_blank\">Edit This Page</a></p><p>This tutorial shows you how to develop a native cloud <a href=\"http://cassandra.apache.org/\" target=\"_blank\">Cassandra</a> deployment on Kubernetes. In this example, a custom Cassandra <em>SeedProvider</em> enables Cassandra to discover new Cassandra nodes as they join the cluster.</p><p><em>StatefulSets</em> make it easier to deploy stateful applications within a clustered environment. For more information on the features used in this tutorial, see the <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\"><em>StatefulSet</em></a> documentation.</p><p><strong>Cassandra on Docker</strong></p><p>The <em>Pods</em> in this tutorial use the <a href=\"https://github.com/kubernetes/examples/blob/master/cassandra/image/Dockerfile\" target=\"_blank\"><code>gcr.io/google-samples/cassandra:v13</code></a>\nimage from Google’s <a href=\"https://cloud.google.com/container-registry/docs/\" target=\"_blank\">container registry</a>.\nThe Docker image above is based on <a href=\"https://github.com/kubernetes/kubernetes/tree/master/build/debian-base\" target=\"_blank\">debian-base</a>\nand includes OpenJDK 8.</p><p>This image includes a standard Cassandra installation from the Apache Debian repo.\nBy using environment variables you can change values that are inserted into <code>cassandra.yaml</code>.</p><table><thead><tr><th>ENV VAR</th>\n<th align=\"center\">DEFAULT VALUE</th>\n</tr></thead><tbody><tr><td><code>CASSANDRA_CLUSTER_NAME</code></td>\n<td align=\"center\"><code>'Test Cluster'</code></td>\n</tr><tr><td><code>CASSANDRA_NUM_TOKENS</code></td>\n<td align=\"center\"><code>32</code></td>\n</tr><tr><td><code>CASSANDRA_RPC_ADDRESS</code></td>\n<td align=\"center\"><code>0.0.0.0</code></td>\n</tr></tbody></table><ul id=\"markdown-toc\"><li><a href=\"#objectives\">Objectives</a></li>\n<li><a href=\"#before-you-begin\">Before you begin</a></li>\n<li><a href=\"#creating-a-cassandra-headless-service\">Creating a Cassandra Headless Service</a></li>\n<li><a href=\"#using-a-statefulset-to-create-a-cassandra-ring\">Using a StatefulSet to Create a Cassandra Ring</a></li>\n<li><a href=\"#validating-the-cassandra-statefulset\">Validating The Cassandra StatefulSet</a></li>\n<li><a href=\"#modifying-the-cassandra-statefulset\">Modifying the Cassandra StatefulSet</a></li>\n<li><a href=\"#cleaning-up\">Cleaning up</a></li>\n<li><a href=\"#what-s-next\">What's next</a></li>\n</ul><h2 id=\"objectives\">Objectives</h2><h2 id=\"before-you-begin\">Before you begin</h2><p>To complete this tutorial, you should already have a basic familiarity with <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/pod/\">Pods</a>, <a href=\"https://kubernetes.io/docs/concepts/services-networking/service/\">Services</a>, and <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\">StatefulSets</a>. In addition, you should:</p><blockquote class=\"note\">\n  <p><strong>Note:</strong> Please read the <a href=\"https://kubernetes.io/docs/setup/pick-right-solution/\">getting started guides</a> if you do not already have a cluster.</p>\n</blockquote><h3 id=\"additional-minikube-setup-instructions\">Additional Minikube Setup Instructions</h3><blockquote class=\"caution\">\n  <div><p><strong>Caution:</strong> <a href=\"https://kubernetes.io/docs/getting-started-guides/minikube/\">Minikube</a> defaults to 1024MB of memory and 1 CPU. Running Minikube with the default resource configuration results in insufficient resource errors during this tutorial. To avoid these errors, start Minikube with the following settings:</p><div class=\"highlight\"><pre class=\"language-shell\" data-lang=\"shell\">minikube start --memory 5120 --cpus=4</pre></div></div>\n</blockquote><p>A Kubernetes <a href=\"https://kubernetes.io/docs/concepts/services-networking/service/\">Service</a> describes a set of <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/pod/\">Pods</a> that perform the same task.</p><p>The following <code>Service</code> is used for DNS lookups between Cassandra Pods and clients within the Kubernetes cluster.</p><table class=\"includecode\" id=\"application-cassandra-cassandra-service-yaml\"><thead><tr><th>\n                <a href=\"https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/application/cassandra/cassandra-service.yaml\" download=\"application/cassandra/cassandra-service.yaml\">\n                    <code>application/cassandra/cassandra-service.yaml</code>\n                </a>\n                <img src=\"https://d33wubrfki0l68.cloudfront.net/951ae1fcc65e28202164b32c13fa7ae04fab4a0b/b77dc/images/copycode.svg\" title=\"Copy application/cassandra/cassandra-service.yaml to clipboard\" alt=\"image\" /></th>\n        </tr></thead><tbody><tr><td><div class=\"highlight\"><pre class=\"language-yaml\" data-lang=\"yaml\">apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: cassandra\n  name: cassandra\nspec:\n  clusterIP: None\n  ports:\n  - port: 9042\n  selector:\n    app: cassandra\n</pre></div>  </td>\n        </tr></tbody></table><ol><li>Launch a terminal window in the directory you downloaded the manifest files.</li>\n<li><p>Create a Service to track all Cassandra StatefulSet nodes from the <code>cassandra-service.yaml</code> file:</p>\n<div class=\"highlight\"><pre class=\"language-shell\" data-lang=\"shell\">kubectl create -f https://k8s.io/examples/application/cassandra/cassandra-service.yaml</pre></div></li>\n</ol><h3 id=\"validating-optional\">Validating (optional)</h3><p>Get the Cassandra Service.</p><div class=\"highlight\"><pre class=\"language-shell\" data-lang=\"shell\">kubectl get svc cassandra</pre></div><p>The response is</p><pre>NAME        TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE\ncassandra   ClusterIP   None         &lt;none&gt;        9042/TCP   45s\n</pre><p>Service creation failed if anything else is returned. Read <a href=\"https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/\">Debug Services</a> for common issues.</p><p>The StatefulSet manifest, included below, creates a Cassandra ring that consists of three Pods.</p><blockquote class=\"note\">\n  <p><strong>Note:</strong> This example uses the default provisioner for Minikube. Please update the following StatefulSet for the cloud you are working with.</p>\n</blockquote><ol><li>Update the StatefulSet if necessary.</li>\n<li><p>Create the Cassandra StatefulSet from the <code>cassandra-statefulset.yaml</code> file:</p>\n<div class=\"highlight\"><pre class=\"language-shell\" data-lang=\"shell\">kubectl create -f https://k8s.io/examples/application/cassandra/cassandra-statefulset.yaml</pre></div></li>\n</ol><ol><li><p>Get the Cassandra StatefulSet:</p>\n<div class=\"highlight\"><pre class=\"language-shell\" data-lang=\"shell\">kubectl get statefulset cassandra</pre></div>\n<p>The response should be:</p>\n<pre>NAME        DESIRED   CURRENT   AGE\ncassandra   3         0         13s\n</pre>\n<p>The <code>StatefulSet</code> resource deploys Pods sequentially.</p></li>\n<li><p>Get the Pods to see the ordered creation status:</p>\n<div class=\"highlight\"><pre class=\"language-shell\" data-lang=\"shell\">kubectl get pods -l=\"app=cassandra\"</pre></div>\n<p>The response should be:</p>\n<div class=\"highlight\"><pre class=\"language-shell\" data-lang=\"shell\">NAME          READY     STATUS              RESTARTS   AGE\ncassandra-0   1/1       Running             0          1m\ncassandra-1   0/1       ContainerCreating   0          8s</pre></div>\n<p>It can take several minutes for all three Pods to deploy. Once they are deployed, the same command returns:</p>\n<pre>NAME          READY     STATUS    RESTARTS   AGE\ncassandra-0   1/1       Running   0          10m\ncassandra-1   1/1       Running   0          9m\ncassandra-2   1/1       Running   0          8m\n</pre></li>\n<li><p>Run the Cassandra <a href=\"https://wiki.apache.org/cassandra/NodeTool\" target=\"_blank\">nodetool</a> to display the status of the ring.</p>\n<div class=\"highlight\"><pre class=\"language-shell\" data-lang=\"shell\">kubectl exec -it cassandra-0 -- nodetool status</pre></div>\n<p>The response should look something like this:</p>\n<pre>Datacenter: DC1-K8Demo\n======================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address     Load       Tokens       Owns (effective)  Host ID                               Rack\nUN  172.17.0.5  83.57 KiB  32           74.0%             e2dd09e6-d9d3-477e-96c5-45094c08db0f  Rack1-K8Demo\nUN  172.17.0.4  101.04 KiB  32           58.8%             f89d6835-3a42-4419-92b3-0e62cae1479c  Rack1-K8Demo\nUN  172.17.0.6  84.74 KiB  32           67.1%             a6a1e8c2-3dc5-4417-b1a0-26507af2aaad  Rack1-K8Demo\n</pre></li>\n</ol><p>Use <code>kubectl edit</code> to modify the size of a Cassandra StatefulSet.</p><ol><li><p>Run the following command:</p>\n<div class=\"highlight\"><pre class=\"language-shell\" data-lang=\"shell\">kubectl edit statefulset cassandra</pre></div>\n<p>This command opens an editor in your terminal. The line you need to change is the <code>replicas</code> field. The following sample is an excerpt of the <code>StatefulSet</code> file:</p>\n<div class=\"highlight\"><pre class=\"language-yaml\" data-lang=\"yaml\"># Please edit the object below. Lines beginning with a '#' will be ignored,\n# and an empty file will abort the edit. If an error occurs while saving this file will be\n# reopened with the relevant failures.\n#\napiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2\nkind: StatefulSet\nmetadata:\n  creationTimestamp: 2016-08-13T18:40:58Z\n  generation: 1\n  labels:\n  app: cassandra\n  name: cassandra\n  namespace: default\n  resourceVersion: \"323\"\n  selfLink: /apis/apps/v1/namespaces/default/statefulsets/cassandra\n  uid: 7a219483-6185-11e6-a910-42010a8a0fc0\nspec:\n  replicas: 3</pre></div></li>\n<li><p>Change the number of replicas to 4, and then save the manifest.</p>\n<p>The <code>StatefulSet</code> now contains 4 Pods.</p></li>\n<li><p>Get the Cassandra StatefulSet to verify:</p>\n<div class=\"highlight\"><pre class=\"language-shell\" data-lang=\"shell\">kubectl get statefulset cassandra</pre></div>\n<p>The response should be</p>\n<pre>NAME        DESIRED   CURRENT   AGE\ncassandra   4         4         36m\n</pre></li>\n</ol><h2 id=\"cleaning-up\">Cleaning up</h2><p>Deleting or scaling a StatefulSet down does not delete the volumes associated with the StatefulSet. This setting is for your safety because your data is more valuable than automatically purging all related StatefulSet resources.</p><blockquote class=\"warning\">\n  <p><strong>Warning:</strong> Depending on the storage class and reclaim policy, deleting the <em>PersistentVolumeClaims</em> may cause the associated volumes to also be deleted. Never assume you’ll be able to access data if its volume claims are deleted.</p>\n</blockquote><ol><li><p>Run the following commands (chained together into a single command) to delete everything in the Cassandra <code>StatefulSet</code>:</p>\n<div class=\"highlight\"><pre class=\"language-shell\" data-lang=\"shell\">grace=$(kubectl get po cassandra-0 -o=jsonpath='{.spec.terminationGracePeriodSeconds}') \\\n  &amp;&amp; kubectl delete statefulset -l app=cassandra \\\n  &amp;&amp; echo \"Sleeping $grace\" \\\n  &amp;&amp; sleep $grace \\\n  &amp;&amp; kubectl delete pvc -l app=cassandra</pre></div></li>\n<li><p>Run the following command to delete the Cassandra Service.</p>\n<div class=\"highlight\"><pre class=\"language-shell\" data-lang=\"shell\">kubectl delete service -l app=cassandra</pre></div></li>\n</ol><h2 id=\"what-s-next\">What's next</h2><ul><li>Learn how to <a href=\"https://kubernetes.io/docs/tasks/run-application/scale-stateful-set/\">Scale a StatefulSet</a>.</li>\n<li>Learn more about the <a href=\"https://github.com/kubernetes/examples/blob/master/cassandra/java/src/main/java/io/k8s/cassandra/KubernetesSeedProvider.java\" target=\"_blank\"><em>KubernetesSeedProvider</em></a></li>\n<li>See more custom <a href=\"https://git.k8s.io/examples/cassandra/java/README.md\" target=\"_blank\">Seed Provider Configurations</a></li>\n</ul></div></section></header>",
        "created_at": "2018-10-24T17:51:48+0000",
        "updated_at": "2018-11-22T14:09:10+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 5,
        "domain_name": "kubernetes.io",
        "preview_picture": "https://kubernetes.io/images/kubernetes-horizontal-color.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12459"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 902,
            "label": "kubernetes",
            "slug": "kubernetes"
          }
        ],
        "is_public": false,
        "id": 12458,
        "uid": null,
        "title": "IBM/Scalable-Cassandra-deployment-on-Kubernetes",
        "url": "https://github.com/IBM/Scalable-Cassandra-deployment-on-Kubernetes/blob/master/README.md",
        "content": "<p><a href=\"https://travis-ci.org/IBM/Scalable-Cassandra-deployment-on-Kubernetes\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/27545c8485cc40771fd0f03ff9844c25de517778/68747470733a2f2f7472617669732d63692e6f72672f49424d2f5363616c61626c652d43617373616e6472612d6465706c6f796d656e742d6f6e2d4b756265726e657465732e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/IBM/Scalable-Cassandra-deployment-on-Kubernetes.svg?branch=master\" /></a></p>\n<p><em>Read this in other languages: <a href=\"https://github.com/IBM/Scalable-Cassandra-deployment-on-Kubernetes/blob/master/README-ko.md\">한국어</a>、<a href=\"https://github.com/IBM/Scalable-Cassandra-deployment-on-Kubernetes/blob/master/README-cn.md\">中国</a>.</em></p>\n<p>This project demonstrates the deployment of a multi-node scalable Cassandra cluster on Kubernetes. Apache Cassandra is a massively scalable open source NoSQL database. Cassandra is perfect for managing large amounts of structured, semi-structured, and unstructured data across multiple datacenters and the cloud.</p>\n<p>Leveraging Kubernetes concepts such as <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/\" rel=\"nofollow\">PersistentVolume</a> and <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\" rel=\"nofollow\">StatefulSets</a>, we can provide a resilient installation of Cassandra and be confident that its data (state) are safe.</p>\n<p>We also utilize a <a href=\"https://kubernetes.io/docs/concepts/services-networking/service/#headless-services\" rel=\"nofollow\">\"headless\" service</a> for Cassandra. This way we can provide a way for applications to access it via <a href=\"https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/\" rel=\"nofollow\">KubeDNS</a> and not expose it to the outside world. To access it from your developer workstation you can use <code>kubectl exec</code> commands against any of the cassandra pods. If you do wish to connect an application to it you can use the KubeDNS value of <code>cassandra.default.svc.cluster.local</code> when configuring your application.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/IBM/Scalable-Cassandra-deployment-on-Kubernetes/blob/master/images/kube-cassandra-code.png\"><img src=\"https://github.com/IBM/Scalable-Cassandra-deployment-on-Kubernetes/raw/master/images/kube-cassandra-code.png\" alt=\"kube-cassandra\" /></a></p>\n<h2><a id=\"user-content-kubernetes-concepts-used\" class=\"anchor\" aria-hidden=\"true\" href=\"#kubernetes-concepts-used\"></a>Kubernetes Concepts Used</h2>\n<ul><li><a href=\"https://kubernetes.io/docs/user-guide/pods\" rel=\"nofollow\">Kubenetes Pods</a></li>\n<li><a href=\"https://kubernetes.io/docs/user-guide/services\" rel=\"nofollow\">Kubenetes Services</a></li>\n<li><a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\" rel=\"nofollow\">Kubernets StatefulSets</a></li>\n</ul><h2><a id=\"user-content-included-components\" class=\"anchor\" aria-hidden=\"true\" href=\"#included-components\"></a>Included Components</h2>\n<ul><li><a href=\"https://console.ng.bluemix.net/docs/containers/cs_ov.html#cs_ov\" rel=\"nofollow\">Kubernetes Clusters</a></li>\n<li><a href=\"https://console.ng.bluemix.net/catalog/?taxonomyNavigation=apps&amp;category=containers\" rel=\"nofollow\">Bluemix container service</a></li>\n<li><a href=\"https://www.ibm.com/cloud-computing/products/ibm-cloud-private/\" rel=\"nofollow\">IBM Cloud Private</a></li>\n<li><a href=\"http://cassandra.apache.org/\" rel=\"nofollow\">Cassandra</a></li>\n</ul><h2><a id=\"user-content-getting-started\" class=\"anchor\" aria-hidden=\"true\" href=\"#getting-started\"></a>Getting Started</h2>\n<p>In order to follow this guide you'll need a Kubernetes cluster. If you do not have access to an existing Kubernetes cluster then follow the instructions (in the link) for one of the following:</p>\n<p><em>The code here is regularly tested against <a href=\"https://console.ng.bluemix.net/docs/containers/cs_ov.html#cs_ov\" rel=\"nofollow\">Kubernetes Cluster from Bluemix Container Service</a> using Travis CI.</em></p>\n<p>After installing (or setting up your access to) Kubernetes ensure that you can access it by running the following and confirming you get version responses for both the Client and the Server:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"7\", GitVersion:\"v1.7.5\", GitCommit:\"17d7182a7ccbb167074be7a87f0a68bd00d58d97\", GitTreeState:\"clean\", BuildDate:\"2017-08-31T09:14:02Z\", GoVersion:\"go1.8.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"7\", GitVersion:\"v1.7.5\", GitCommit:\"17d7182a7ccbb167074be7a87f0a68bd00d58d97\", GitTreeState:\"clean\", BuildDate:\"2017-09-18T20:30:29Z\", GoVersion:\"go1.8.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}</pre></div>\n<h2><a id=\"user-content-create-a-cassandra-service-for-cassandra-cluster-formation-and-discovery\" class=\"anchor\" aria-hidden=\"true\" href=\"#create-a-cassandra-service-for-cassandra-cluster-formation-and-discovery\"></a>Create a Cassandra Service for Cassandra cluster formation and discovery</h2>\n<h3><a id=\"user-content-1-create-a-cassandra-headless-service\" class=\"anchor\" aria-hidden=\"true\" href=\"#1-create-a-cassandra-headless-service\"></a>1. Create a Cassandra Headless Service</h3>\n<p>To allow us to do simple discovery of the cassandra seed node (which we will deploy shortly) we can create a \"headless\" service.  We do this by  specifying <strong>none</strong> for the  <strong>clusterIP</strong> in the <a href=\"https://github.com/IBM/Scalable-Cassandra-deployment-on-Kubernetes/blob/master/cassandra-service.yaml\">cassandra-service.yaml</a>. This headless service  allows us to use KubeDNS for the Pods to discover the IP address of the Cassandra seed.</p>\n<p>You can create the headless service using the <a href=\"https://github.com/IBM/Scalable-Cassandra-deployment-on-Kubernetes/blob/master/cassandra-service.yaml\">cassandra-service.yaml</a> file:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ kubectl create -f cassandra-service.yaml\nservice \"cassandra\" created\n$ kubectl get svc cassandra\nNAME        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE\ncassandra   None         &lt;none&gt;        9042/TCP   10s</pre></div>\n<p>Most applications deployed to Kubernetes should be cloud native and rely on external resources for their data (or state). However since Cassandra is a database we can use Stateful sets and Persistent Volumes to ensure resiliency in our database.</p>\n<h3><a id=\"user-content-2-create-local-volumes\" class=\"anchor\" aria-hidden=\"true\" href=\"#2-create-local-volumes\"></a>2. Create Local Volumes</h3>\n<p>To create persistent Cassandra nodes, we need to provision Persistent Volumes. There are two ways to provision PV's: <strong>dynamically and statically</strong>.</p>\n<p>For the sake of simplicity and compatibility we will use <strong>Static</strong> provisioning where we will create volumes manually using the provided yaml files.</p>\n<p><em>note: You'll need to have the same number of Persistent Volumes as the number of your Cassandra nodes. If you are expecting to have 3 Cassandra nodes, you'll need to create 3 Persistent Volumes.</em></p>\n<p>The provided <a href=\"https://github.com/IBM/Scalable-Cassandra-deployment-on-Kubernetes/blob/master/local-volumes.yaml\">local-volumes.yaml</a> file already has <strong>3</strong> Persistent Volumes defined. Update the file to add more if you expect to have greater than 3 Cassandra nodes. Create the volumes:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ kubectl create -f local-volumes.yaml\n$ kubectl get pv   \nNAME               CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS      CLAIM     STORAGECLASS   REASON    AGE\ncassandra-data-1   1Gi        RWO           Recycle         Available                                      7s\ncassandra-data-2   1Gi        RWO           Recycle         Available                                      7s\ncassandra-data-3   1Gi        RWO           Recycle         Available                                      7s</pre></div>\n<h3><a id=\"user-content-3-create-a-statefulset\" class=\"anchor\" aria-hidden=\"true\" href=\"#3-create-a-statefulset\"></a>3. Create a StatefulSet</h3>\n<p>The <a href=\"https://github.com/IBM/Scalable-Cassandra-deployment-on-Kubernetes/blob/master/cassandra-statefulset.yaml\">StatefulSet</a> is responsible for creating the Pods. It provides ordered deployment, ordered termination and unique network names. Run the following command to start a single Cassandra server:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ kubectl create -f cassandra-statefulset.yaml</pre></div>\n<h3><a id=\"user-content-4-validate-the-statefulset\" class=\"anchor\" aria-hidden=\"true\" href=\"#4-validate-the-statefulset\"></a>4. Validate the StatefulSet</h3>\n<p>You can check if your StatefulSet has deployed using the command below.</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ kubectl get statefulsets\nNAME        DESIRED   CURRENT   AGE\ncassandra   1         1         2h</pre></div>\n<p>If you view the list of the Pods, you should see 1 Pod running. Your Pod name should be cassandra-0 and the next pods would follow the ordinal number (<em>cassandra-1, cassandra-2,..</em>) Use this command to view the Pods created by the StatefulSet:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ kubectl get pods -o wide\nNAME          READY     STATUS    RESTARTS   AGE       IP              NODE\ncassandra-0   1/1       Running   0          1m        172.xxx.xxx.xxx   169.xxx.xxx.xxx</pre></div>\n<p>To check if the Cassandra node is up, perform a <strong>nodetool status:</strong></p>\n<div class=\"highlight highlight-source-shell\"><pre>$ kubectl exec -ti cassandra-0 -- nodetool status\nDatacenter: DC1\n===============\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address          Load       Tokens       Owns (effective)   Host ID                               Rack\nUN  172.xxx.xxx.xxx  109.28 KB  256          100.0%             6402e90d-7995-4ee1-bb9c-36097eb2c9ec  Rack1</pre></div>\n<h3><a id=\"user-content-5-scale-the-statefulset\" class=\"anchor\" aria-hidden=\"true\" href=\"#5-scale-the-statefulset\"></a>5. Scale the StatefulSet</h3>\n<p>To increase or decrease the size of your StatefulSet you can use the scale command:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ kubectl scale --replicas=3 statefulset/cassandra</pre></div>\n<p>Wait a minute or two and check if it worked:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ kubectl get statefulsets\nNAME        DESIRED   CURRENT   AGE\ncassandra   3         3         2h</pre></div>\n<p>If you watch the Cassandra pods deploy, they should be created sequentially.</p>\n<p>You can view the list of the Pods again to confirm that your Pods are up and running.</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ kubectl get pods -o wide\nNAME          READY     STATUS    RESTARTS   AGE       IP                NODE\ncassandra-0   1/1       Running   0          13m       172.xxx.xxx.xxx   169.xxx.xxx.xxx\ncassandra-1   1/1       Running   0          38m       172.xxx.xxx.xxx   169.xxx.xxx.xxx\ncassandra-2   1/1       Running   0          38m       172.xxx.xxx.xxx   169.xxx.xxx.xxx</pre></div>\n<p>You can perform a <strong>nodetool status</strong> to check if the other cassandra nodes have joined and formed a Cassandra cluster.</p>\n<p><em><strong>Note:</strong> It can take around 5 minutes for the Cassandra database to finish its setup.</em></p>\n<div class=\"highlight highlight-source-shell\"><pre>$ kubectl exec -ti cassandra-0 -- nodetool status\nDatacenter: DC1\n===============\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address     Load       Tokens       Owns (effective)  Host ID                               Rack\nUN  172.xxx.xxx.xxx  103.25 KiB  256          68.7%             633ae787-3080-40e8-83cc-d31b62f53582  Rack1\nUN  172.xxx.xxx.xxx  108.62 KiB  256          63.5%             e95fc385-826e-47f5-a46b-f375532607a3  Rack1\nUN  172.xxx.xxx.xxx  177.38 KiB  256          67.8%             66bd8253-3c58-4be4-83ad-3e1c3b334dfd  Rack1</pre></div>\n<p><em>You will need to wait for the status of the nodes to be Up and Normal (UN) to execute the commands in the next steps.</em></p>\n<h3><a id=\"user-content-6-using-cql\" class=\"anchor\" aria-hidden=\"true\" href=\"#6-using-cql\"></a>6. Using CQL</h3>\n<p>You can access the cassandra container using the following command:</p>\n<div class=\"highlight highlight-source-shell\"><pre>kubectl exec -it cassandra-0 cqlsh    \nConnected to Cassandra at 127.0.0.1:9042.\n[cqlsh 5.0.1 | Cassandra 3.11.1 | CQL spec 3.4.4 | Native protocol v4]\nUse HELP for help.\ncqlsh&gt; describe tables\nKeyspace system_traces\n----------------------\nevents  sessions\nKeyspace system_schema\n----------------------\ntables     triggers    views    keyspaces  dropped_columns\nfunctions  aggregates  indexes  types      columns        \nKeyspace system_auth\n--------------------\nresource_role_permissons_index  role_permissions  role_members  roles\nKeyspace system\n---------------\navailable_ranges          peers               batchlog        transferred_ranges\nbatches                   compaction_history  size_estimates  hints             \nprepared_statements       sstable_activity    built_views   \n\"IndexInfo\"               peer_events         range_xfers   \nviews_builds_in_progress  paxos               local         \nKeyspace system_distributed\n---------------------------\nrepair_history  view_build_status  parent_repair_history\n</pre></div>\n<h2><a id=\"user-content-troubleshooting\" class=\"anchor\" aria-hidden=\"true\" href=\"#troubleshooting\"></a>Troubleshooting</h2>\n<ul><li>If your Cassandra instance is not running properly, you may check the logs using\n<ul><li><code>kubectl logs &lt;your-pod-name&gt;</code></li>\n</ul></li>\n<li>To clean/delete your data on your Persistent Volumes, delete your PVCs using\n<ul><li><code>kubectl delete pvc -l app=cassandra</code></li>\n</ul></li>\n<li>If your Cassandra nodes are not joining, delete your controller/statefulset then delete your Cassandra service.\n<ul><li><code>kubectl delete statefulset cassandra</code> if you created the Cassandra StatefulSet</li>\n<li><code>kubectl delete svc cassandra</code></li>\n</ul></li>\n<li>To delete everything:\n<ul><li><code>kubectl delete statefulset,pvc,pv,svc -l app=cassandra</code></li>\n</ul></li>\n</ul><h2><a id=\"user-content-references\" class=\"anchor\" aria-hidden=\"true\" href=\"#references\"></a>References</h2>",
        "created_at": "2018-10-24T17:49:12+0000",
        "updated_at": "2018-10-24T17:49:21+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 6,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/1459110?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12458"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 968,
            "label": "automation",
            "slug": "automation"
          },
          {
            "id": 1208,
            "label": "chef",
            "slug": "chef"
          }
        ],
        "is_public": false,
        "id": 12454,
        "uid": null,
        "title": "michaelklishin/cassandra-chef-cookbook",
        "url": "https://github.com/michaelklishin/cassandra-chef-cookbook",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p><a href=\"https://travis-ci.org/michaelklishin/cassandra-chef-cookbook\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/87f09f29326d3a637955a80bb59facec7a233fae/68747470733a2f2f7472617669732d63692e6f72672f6d69636861656c6b6c697368696e2f63617373616e6472612d636865662d636f6f6b626f6f6b2e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/michaelklishin/cassandra-chef-cookbook.svg?branch=master\" /></a></p>\n<p>This is a Chef cookbook for Apache Cassandra (<a href=\"http://www.datastax.com/products/community\" rel=\"nofollow\">DataStax\nCommunity Edition</a>) as\nwell as DataStax Enterprise.</p>\n<p>It uses officially released packages and provides an Upstart service\nscript. It has fairly complete support for adjustment of Cassandra\nconfiguration parameters using Chef node attributes.</p>\n<p>It was originally created for CI and development environments and now supports cluster discovery using Chef search. <strong>Feel free to contribute</strong> what you find missing!</p>\n<h2><a id=\"user-content-supported-chef-versions\" class=\"anchor\" aria-hidden=\"true\" href=\"#supported-chef-versions\"></a>Supported Chef Versions</h2>\n<p>This cookbook targets Chef 12 and later versions.</p>\n<h2><a id=\"user-content-cookbook-dependencies\" class=\"anchor\" aria-hidden=\"true\" href=\"#cookbook-dependencies\"></a>Cookbook Dependencies</h2>\n<div class=\"highlight highlight-source-ruby\"><pre>depends 'java'\ndepends 'ulimit'\ndepends 'apt'\ndepends 'yum'\ndepends 'ark'</pre></div>\n<h2><a id=\"user-content-cassandra-dependencies\" class=\"anchor\" aria-hidden=\"true\" href=\"#cassandra-dependencies\"></a>Cassandra Dependencies</h2>\n<p>Modern Cassandra versions require OracleJDK 8.</p>\n<h2><a id=\"user-content-berkshelf\" class=\"anchor\" aria-hidden=\"true\" href=\"#berkshelf\"></a>Berkshelf</h2>\n<h3><a id=\"user-content-most-recent-release\" class=\"anchor\" aria-hidden=\"true\" href=\"#most-recent-release\"></a>Most Recent Release</h3>\n<div class=\"highlight highlight-source-ruby\"><pre>cookbook 'cassandra-dse', '~&gt; 4.5.0'</pre></div>\n<h3><a id=\"user-content-from-git\" class=\"anchor\" aria-hidden=\"true\" href=\"#from-git\"></a>From Git</h3>\n<div class=\"highlight highlight-source-ruby\"><pre>cookbook 'cassandra-dse', github: 'michaelklishin/cassandra-chef-cookbook'</pre></div>\n<h2><a id=\"user-content-supported-apache-cassandra-version\" class=\"anchor\" aria-hidden=\"true\" href=\"#supported-apache-cassandra-version\"></a>Supported Apache Cassandra Version</h2>\n<p>This cookbook currently provides</p>\n<ul><li>Cassandra via tarballs</li>\n<li>Cassandra (DataStax Community Edition) via apt and yum packages</li>\n<li>DataStax Enterprise (DSE) via packages</li>\n</ul><h2><a id=\"user-content-supported-os-distributions\" class=\"anchor\" aria-hidden=\"true\" href=\"#supported-os-distributions\"></a>Supported OS Distributions</h2>\n<ul><li>Ubuntu 12.04 through 17.101 via DataStax apt repo.</li>\n<li>RHEL/CentOS via DataStax yum repo.</li>\n<li>RHEL/CentOS/Amazon via tarball</li>\n</ul><h2><a id=\"user-content-support-jdk-versions\" class=\"anchor\" aria-hidden=\"true\" href=\"#support-jdk-versions\"></a>Support JDK Versions</h2>\n<p>Cassandra 2.x requires JDK 7+, later versions require Oracle JDK 8+.</p>\n<h2><a id=\"user-content-recipes\" class=\"anchor\" aria-hidden=\"true\" href=\"#recipes\"></a>Recipes</h2>\n<p>The main recipe is <code>cassandra-dse::default</code> which together with the <code>node[:cassandra][:install_method]</code> attribute will be responsible for including the proper installation recipe and recipe <code>cassandra-dse::config</code> for configuring both <code>datastax</code> and <code>tarball</code> C* installation.</p>\n<p>Two actual installation recipes are <code>cassandra-dse::tarball</code> and <code>cassandra-dse::datastax</code>. The former uses official tarball\nand thus can be used to provision any specific version.</p>\n<p>The latter uses DataStax repository via packages. You can install different versions (ex. <code>dsc20</code> for v2.0) available in the repository by altering <code>:package_name</code> attribute (<code>dsc20</code> by default).</p>\n<p>Recently we have moved all the configuration resources to a separate recipe <code>cassandra-des::config</code>, which means recipes <code>cassandra-dse::tarball</code> and <code>cassandra-dse::datastax</code> are only responsible for C* installation.</p>\n<blockquote>\n<blockquote>\n<p>Users with cookbook version <code>=&lt;3.5.0</code> needs to update the <code>run_list</code>, in case of not using <code>cassandra-dse::default</code> recipe.</p>\n</blockquote>\n</blockquote>\n<p>include_recipe <code>cassandra-dse</code> uses <code>cassandra-dse::datastax</code> as the default.</p>\n<h3><a id=\"user-content-datastax-enterprise\" class=\"anchor\" aria-hidden=\"true\" href=\"#datastax-enterprise\"></a>DataStax Enterprise</h3>\n<p>You can also install the DataStax Enterprise edition by adding <code>node[:cassandra][:dse]</code> attributes according to the datastax.rb.</p>\n<ul><li><code>node[:cassandra][:package_name]</code>: Override default value to 'dse-full'.</li>\n<li><code>node[:cassandra][:service_name]</code>: Override default value to 'dse'.</li>\n</ul><p>Unencrypted Credentials:</p>\n<ul><li><code>node[:cassandra][:dse][:credentials][:username]</code>: Your username from Datastax website.</li>\n<li><code>node[:cassandra][:dse][:credentials][:password]</code>: Your password from Datastax website.</li>\n</ul><p>Encrypted Credentials:</p>\n<ul><li><code>node[:cassandra][:dse][:credentials][:databag][:name]</code>: Databag name, i.e. the value 'cassandra' will reference to <code>/data_bags/cassandra</code>.</li>\n<li><code>node[:cassandra][:dse][:credentials][:databag][:item]</code>: Databag item, i.e. the value 'main' will reference to <code>/data_bags/cassandra/main.json</code>.</li>\n<li><code>node[:cassandra][:dse][:credentials][:databag][:entry]</code>: The field name in the databag item, in which the credetials are written. i.e. the data_bag:</li>\n</ul><pre>{\n  \"id\": \"main\",\n  \"entry\": {\n    \"username\": \"%USERNAME%\",\n    \"password\": \"%PASSWORD%\"\n  }\n}\n</pre>\n<p>There are also recipes for DataStax opscenter installation (\n<code>cassandra-dse::opscenter_agent_tarball</code>,\n<code>cassandra-dse::opscenter_agent_datastax</code>, and\n<code>cassandra-dse::opscenter_server</code> ) along with attributes available\nfor override (see below).</p>\n<h3><a id=\"user-content-jna-support-for-c-versions-prior-to-210\" class=\"anchor\" aria-hidden=\"true\" href=\"#jna-support-for-c-versions-prior-to-210\"></a>JNA Support (for C* Versions Prior to 2.1.0)</h3>\n<p>The <code>node[:cassandra][:setup_jna]</code> attribute will install the jna.jar in the\n<code>/usr/share/java/jna.jar</code>, and create a symbolic link to it on\n<code>#{cassandra.lib\\_dir}/jna.jar</code>, according to the <a href=\"http://www.datastax.com/documentation/cassandra/1.2/webhelp/cassandra/install/installJnaDeb.html\" rel=\"nofollow\">DataStax\ndocumentation</a>.</p>\n<h2><a id=\"user-content-node-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#node-attributes\"></a>Node Attributes</h2>\n<p>Please note that the maintainers try to keep the list below up-to-date but it fairly often misses\nsome recently added attributes. Please refer to the <a href=\"https://github.com/michaelklishin/cassandra-chef-cookbook/tree/master/attributes\">attributes files</a> if an attribute you are looking for isn't listed.</p>\n<h3><a id=\"user-content-core-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#core-attributes\"></a>Core Attributes</h3>\n<ul><li><code>node[:cassandra][:install_method]</code> (default: datastax): The installation method to use (either 'datastax' or 'tarball').</li>\n<li><code>node[:cassandra][:config][:cluster_name]</code> (default: none): Name of the cluster to create. This is required.</li>\n<li><code>node[:cassandra][:version]</code> (default: a recent patch version): version to provision</li>\n<li><code>node[:cassandra][:tarball][:url]</code> and <code>node[:cassandra][:tarball][:sha256sum]</code> specify tarball URL and SHA256 check sum used by the <code>cassandra::tarball</code> recipe.</li>\n<li>Setting <code>node[:cassandra][:tarball][:url]</code> to \"auto\" (default) will download the tarball of the specified version from the Apache repository.</li>\n<li><code>node[:cassandra][:setup_user]</code> (default: true): create user/group for Cassandra node process</li>\n<li><code>node[:cassandra][:setup_user_limits]</code> (default: true): setup Cassandra user limits</li>\n<li><code>node[:cassandra][:user]</code>: username Cassandra node process will use</li>\n<li><code>node[:cassandra][:group]</code>: groupname Cassandra node process will use</li>\n<li><code>node[:cassandra][:heap_new_size]</code> set JVM <code>-Xmn</code>. If set, <code>node[:cassandra][:max_heap_size]</code> must also be set; if nil, defaults to <code>min(100MB * num_cores, 1/4 * heap size)</code></li>\n<li><code>node[:cassandra][:max_heap_size]</code> set JVM <code>-Xms</code> and <code>-Xmx</code>. If set, <code>node[:cassandra][:heap_new_size]</code> must also be set; if nil, defaults to <code>max(min(1/2 ram, 1024MB), min(1/4 ram, 8GB))</code></li>\n<li><code>node[:cassandra][:installation_dir]</code> (default: <code>/usr/local/cassandra</code>): installation directory</li>\n<li><code>node[:cassandra][:root_dir]</code> (default: <code>/var/lib/cassandra</code>): data directory root</li>\n<li><code>node[:cassandra][:log_dir]</code> (default: <code>/var/log/cassandra</code>): log directory</li>\n<li><code>node[:cassandra][:tmp_dir]</code> (default: none): tmp directory. Be careful what you set this to, as the cassandra user will be given ownership of that directory.</li>\n<li><code>node[:cassandra][:local_jmx]</code> (default: true): bind JMX listener to localhost</li>\n<li><code>node[:cassandra][:jmx_port]</code> (default: 7199): port to listen for JMX</li>\n<li><code>node[:cassandra][:jmx_remote_rmi_port]</code> (default: $JMX_PORT): port for jmx remote method invocation. If using internode SSL, there is a bug requiring this to be different than <code>node[:cassandra][:jmx_port]</code></li>\n<li><code>node[:cassandra][:jmx_remote_authenticate]</code> (default: false): turn on to require username/password for jmx operations including nodetool. To turn on requires <code>node[:cassandra][:local_jmx]</code> to be false</li>\n<li><code>node[:cassandra][:jmx][:user]</code> (default: cassandra): username for jmx authentication</li>\n<li><code>node[:cassandra][:jmx][:password]</code> (default: cassandra): password for jmx authentication.</li>\n<li><code>node[:cassandra][:notify_restart]</code> (default: false): notify Cassandra service restart upon resource update</li>\n<li>Setting <code>node[:cassandra][:notify_restart]</code> to true will restart Cassandra service upon resource change</li>\n<li><code>node[:cassandra][:setup_jna]</code> (default: true): installs jna.jar</li>\n<li><code>node[:cassandra][:skip_jna]</code> (default: false): (2.1.0 and up only) removes jna.jar, adding '-Dcassandra.boot_without_jna=true' for low-memory C* installations</li>\n<li><code>node[:cassandra][:pid_dir]</code> (default: true): pid directory for Cassandra node process for <code>cassandra::tarball</code> recipe</li>\n<li><code>node[:cassandra][:dir_mode]</code> (default: 0755): default permission set for Cassandra node directory / files</li>\n<li><code>node[:cassandra][:service_action]</code> (default: [:enable, :start]): default service actions for the service</li>\n<li><code>node[:cassandra][:install_java]</code> (default: true): whether to run the open source java cookbook</li>\n<li><code>node[:cassandra][:cassandra_old_version_20]</code> (default: ): attribute used in cookbook to determine C* version older or newer than 2.1</li>\n<li><code>node[:cassandra][:log_config_files]</code> (default: calculated): log framework configuration files name array</li>\n<li><code>node[:cassandra][:xss]</code>  JVM per thread stack-size (-Xss option) (default: 256k).</li>\n<li><code>node[:cassandra][:jmx_server_hostname]</code> java.rmi.server.hostname option for JMX interface, necessary to set when you have problems connecting to JMX) (default: false)</li>\n<li><code>node[:cassandra][:heap_dump]</code> -XX:+HeapDumpOnOutOfMemoryError JVM parameter (default: true)</li>\n<li><code>node[:cassandra][:heap_dump_dir]</code> Directory where heap dumps will be placed (default: nil, which will use cwd)</li>\n<li><code>node[:cassandra][:vnodes]</code> enable vnodes. (default: true)</li>\n</ul><p>For the complete set of supported attributes, please consult <a href=\"https://github.com/michaelklishin/cassandra-chef-cookbook/tree/master/attributes\">the source</a>.</p>\n<p>Attributes used to define JBOD functionality</p>\n<ul><li><code>default['cassandra']['jbod']['slices']</code> - defines the number of jbod slices while each represents data directory. By default disables with nil.</li>\n<li><code>default['cassandra']['jbod']['dir_name_prefix']</code> - defines the data directory prefix\nFor example if you want to connect 4 EBS disks as a JBOD slices the names will be in the following format: data1,data2,data3,data4\ncassandra.yaml.erb will generate automatically entry per data_dir location\nPlease note: this functionality is not creating volumes or directories. It takes care of configuration. You can use same parameters with AWS cookbook to create EBS volumes and map to directories.</li>\n</ul><p>Attributes for fine tuning CMS/ParNew, the GC algorithm recommended for Cassandra deployments:</p>\n<ul><li><code>node[:cassandra][:gc_survivor_ratio]</code> -XX:SurvivorRatio JVM parameter (default: 8)</li>\n<li><code>node[:cassandra][:gc_max_tenuring_threshold]</code> -XX:MaxTenuringThreshold JVM parameter (default: 1)</li>\n<li><code>node[:cassandra][:gc_cms_initiating_occupancy_fraction]</code> -XX:CMSInitiatingOccupancyFraction JVM parameter (default: 75)</li>\n</ul><p>Descriptions for these JVM parameters can be found <a href=\"http://www.oracle.com/technetwork/java/javase/tech/vmoptions-jsp-140102.html#PerformanceTuning\" rel=\"nofollow\">here</a> and <a href=\"http://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html#cms.starting_a_cycle\" rel=\"nofollow\">here</a>.</p>\n<p>Attributes for enabling G1 GC.</p>\n<ul><li><code>node[:cassandra][:jvm][:g1]</code> (default: false)</li>\n</ul><p>Attributes for enabling GC detail/logging.</p>\n<ul><li><code>node[:cassandra][:jvm][:gcdetail]</code> (default: false)</li>\n</ul><p>Attributes for fine tuning the G1 GC algorithm:</p>\n<ul><li><code>node[:cassandra][:jvm][:g1_rset_updating_pause_time_percent]</code> (default: 10)</li>\n<li><code>node[:cassandra][:jvm][:g1_heap_region_size]</code> -XX:G1HeapRegionSize (default: 0)</li>\n<li><code>node[:cassandra][:jvm][:max_gc_pause_millis]</code> -XX:MaxGCPauseMillis (default: 200)</li>\n<li><code>node[:cassandra][:jvm][:heap_occupancy_threshold]</code> -XX:InitiatingHeapOccupancyPercent (default: 45)</li>\n<li><code>node[:cassandra][:jvm][:max_parallel_gc_threads]</code> This will set -XX:ParallelGCThreads to the number of cores on the machine (default: false)</li>\n<li><code>node[:cassandra][:jvm][:max_conc_gc_threads]</code> This will set -XX:ConcGCThreads to the number of cores on the machine (default: false)</li>\n<li><code>node[:cassandra][:jvm][:parallel_ref_proc]</code> -XX:ParallelRefProcEnabled (default: false)</li>\n<li><code>node[:cassandra][:jvm][:always_pre_touch]</code> -XX:AlwaysPreTouch (default: false)</li>\n<li><code>node[:cassandra][:jvm][:use_biased_locking]</code> -XX:UseBiasedLocking  (default: true)</li>\n<li><code>node[:cassandra][:jvm][:use_tlab]</code> -XX:UseTLAB (default: true)</li>\n<li><code>node[:cassandra][:jvm][:resize_tlab]</code> -XX:ResizeTLAB (default: true)</li>\n</ul><p>Oracle JVM 8 tuning parameters: <a href=\"https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/\" rel=\"nofollow\">here</a></p>\n<h3><a id=\"user-content-seed-discovery-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#seed-discovery-attributes\"></a>Seed Discovery Attributes</h3>\n<ul><li><code>node[:cassandra][:seeds]</code> (default: <code>[node[:ipaddress]]</code>): an array of nodes this node will contact to discover cluster topology</li>\n<li><code>node[:cassandra][:seed_discovery][:use_chef_search]</code> (default: false): enabled seed discovery using Chef search</li>\n<li><code>node[:cassandra][:seed_discovery][:search_role]</code> (default: <code>\"cassandra-seed\"</code>): role to use in search query</li>\n<li><code>node[:cassandra][:seed_discovery][:search_query]</code> (default: uses <code>node[:cassandra][:seed_discovery][:search_role]</code>): allows\nfor overriding the entire Chef search query</li>\n<li><code>node[:cassandra][:seed_discovery][:count]</code> (default: <code>3</code>): how many nodes to include into seed list. First N nodes are\ntaken in the order Chef search returns them. IP addresses of the nodes are sorted lexographically.</li>\n</ul><h3><a id=\"user-content-cassandrayaml-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#cassandrayaml-attributes\"></a>cassandra.yaml Attributes</h3>\n<ul><li><code>node[:cassandra][:config][:num_tokens]</code> set the desired number of tokens. (default: 256)</li>\n<li><code>node[:cassandra][:config][:listen_address]</code> (default: node[:ipaddress]): address clients will use to connect to the node</li>\n<li><code>node[:cassandra][:config][:broadcast_address]</code> (default: node IP address): address to broadcast to other Cassandra nodes</li>\n<li><code>node[:cassandra][:config][:rpc_address]</code> (default: 0.0.0.0): address to bind the RPC interface.  Leave blank to lookup IP from hostname.</li>\n<li><code>node[:cassandra][:config][:hinted_handoff_enabled]</code> see <a href=\"http://wiki.apache.org/cassandra/HintedHandoff\" rel=\"nofollow\">http://wiki.apache.org/cassandra/HintedHandoff</a> (default: true)</li>\n<li><code>node[:cassandra][:config][:max_hint_window_in_ms]</code> The maximum amount of time a dead host will have hints generated (default: 10800000).</li>\n<li><code>node[:cassandra][:config][:hinted_handoff_throttle_in_kb]</code> throttle in KB's per second, per delivery thread (default: 1024)</li>\n<li><code>node[:cassandra][:config][:max_hints_delivery_threads]</code> Number of threads with which to deliver hints (default: 2)</li>\n<li><code>node[:cassandra][:config][:authenticator]</code> Authentication backend (default: org.apache.cassandra.auth.AllowAllAuthenticator)</li>\n<li><code>node[:cassandra][:config][:authorizer]</code> Authorization backend (default: org.apache.cassandra.auth.AllowAllAuthorizer)</li>\n<li><code>node[:cassandra][:config][:permissions_validity_in_ms]</code> Validity period for permissions cache, set to0 to disable (default: 2000)</li>\n<li><code>node[:cassandra][:config][:partitioner]</code> The partitioner to distribute keys across the cluster (default: org.apache.cassandra.dht.Murmur3Partitioner).</li>\n<li><code>node[:cassandra][:config][:disk_failure_policy]</code> policy for data disk failures: stop, best_effort, or ignore (default: stop)</li>\n<li><code>node[:cassandra][:config][:key_cache_size_in_mb]</code> Maximum size of the key cache in memory. Set to 0 to disable, or \"\" for auto = (min(5% of Heap (in MB), 100MB)) (default: \"\", auto).</li>\n<li><code>node[:cassandra][:config][:key_cache_save_period]</code> Duration in seconds after which key cache is saved to saved_caches_directory. (default: 14400)</li>\n<li><code>node[:cassandra][:config][:row_cache_size_in_mb]</code> Maximum size of the row cache in memory, 0 to disable (default: 0)</li>\n<li><code>node[:cassandra][:config][:row_cache_save_period]</code> Duration in seconds after which row cache is saved to saved_caches_directory, 0 to disable cache save. (default: 0)</li>\n<li><code>node[:cassandra][:config][:row_cache_provider]</code> The provider for the row cache to use (default: SerializingCacheProvider)</li>\n<li><code>node[:cassandra][:config][:commitlog_sync]</code> periodic to ack writes immediately with periodic fsyncs, or batch to wait until fsync to ack writes (default: periodic)</li>\n<li><code>node[:cassandra][:config][:commitlog_sync_period_in_ms]</code> period for commitlog fsync when commitlog_sync = periodic (default: 10000)</li>\n<li><code>node[:cassandra][:config][:commitlog_sync_batch_window_in_ms]</code> batch window for fsync when commitlog_sync = batch (default: 50)</li>\n<li><code>node[:cassandra][:config][:commitlog_segment_size_in_mb]</code> Size of individual commitlog file segments (default: 32)</li>\n<li><code>node[:cassandra][:config][:commitlog_total_space_in_mb]</code> If space gets above this value (it will round up to the next nearest segment multiple), Cassandra will flush every dirty CF in the oldest segment and remove it. (default: 4096)</li>\n<li><code>node[:cassandra][:config][:concurrent_reads]</code> Should be set to 16 * drives (default: 32)</li>\n<li><code>node[:cassandra][:config][:concurrent_writes]</code> Should be set to 8 * cpu cores (default: 32)</li>\n<li><code>node[:cassandra][:config][:trickle_fsync]</code> Enable this to avoid sudden dirty buffer flushing from impacting read latencies.  Almost always a good idea on SSDs; not necessary on platters (default: false)</li>\n<li><code>node[:cassandra][:config][:trickle_fsync_interval_in_kb]</code> Interval for fsync when doing sequential writes (default: 10240)</li>\n<li><code>node[:cassandra][:config][:storage_port]</code> TCP port, for commands and data (default: 7000)</li>\n<li><code>node[:cassandra][:config][:ssl_storage_port]</code> SSL port, unused unless enabled in encryption options (default: 7001)</li>\n<li><code>node[:cassandra][:config][:listen_address]</code> Address to bind for communication with other nodes. Leave blank to lookup IP from hostname. 0.0.0.0 is always wrong. (default: node[:ipaddress]).</li>\n<li><code>node[:cassandra][:config][:broadcast_address]</code> Address to broadcast to other Cassandra nodes.  If '', will use listen_address (default: '')</li>\n<li><code>node[:cassandra][:config][:start_native_transport]</code> Whether to start the native transport server (default: true)</li>\n<li><code>node[:cassandra][:config][:native_transport_port]</code> Port for the CQL native transport to listen for clients on (default: 9042)</li>\n<li><code>node[:cassandra][:config][:start_rpc]</code> Whether to start the Thrift RPC server (default: true)</li>\n<li><code>node[:cassandra][:config][:rpc_port]</code> Port for Thrift RPC server to listen for clients on (default: 9160)</li>\n<li><code>node[:cassandra][:config][:rpc_keepalive]</code> Enable keepalive on RPC connections (default: true)</li>\n<li><code>node[:cassandra][:config][:rpc_server_type]</code> sync for one thread per connection; hsha for \"half synchronous, half asynchronous\" (default: sync)</li>\n<li><code>node[:cassandra][:config][:thrift_framed_transport_size_in_mb]</code> Frame size for Thrift (maximum field length) (default: 15)</li>\n<li><code>node[:cassandra][:config][:thrift_max_message_length_in_mb]</code> Max length of a Thrift message, including all fields and internal Thrift overhead (default: 16)</li>\n<li><code>node[:cassandra][:config][:incremental_backups]</code> Enable hardlinks in backups/ for each sstable flushed or streamed locally. Removing these links is the operator's responsibility (default: false)</li>\n<li><code>node[:cassandra][:config][:snapshot_before_compaction]</code> Take a snapshot before each compaction (default: false)</li>\n<li><code>node[:cassandra][:config][:auto_snapshot]</code> Take a snapshot before keyspace truncation or dropping of column families.  If you set this value to false, you will lose data on truncation or drop (default: true)</li>\n<li><code>node[:cassandra][:config][:column_index_size_in_kb]</code> Add column indexes to a row after its contents reach this size (default: 64)</li>\n<li><code>node[:cassandra][:config][:compaction_throughput_mb_per_sec]</code> Throttle compaction to this total system throughput. Generally should be 16-32 times data insertion rate (default: 16)</li>\n<li><code>node[:cassandra][:config][:read_request_timeout_in_ms]</code> How long the coordinator should wait for read operations to complete (default: 10000)</li>\n<li><code>node[:cassandra][:config][:range_request_timeout_in_ms]</code> How long the coordinator should wait for seq or index scans to complete (default: 10000).</li>\n<li><code>node[:cassandra][:config][:write_request_timeout_in_ms]</code> How long the coordinator should wait for writes to complete (default: 10000)</li>\n<li><code>node[:cassandra][:config][:truncate_request_timeout_in_ms]</code> How long the coordinator should wait for truncates to complete (default: 60000)</li>\n<li><code>node[:cassandra][:config][:request_timeout_in_ms]</code> Default timeout for other, miscellaneous operations (default: 10000)</li>\n<li><code>node[:cassandra][:config][:cross_node_timeout]</code> Enable operation timeout information exchange between nodes to accurately measure request timeouts. Be sure ntp is installed and node times are synchronized before enabling. (default: false)</li>\n<li><code>node[:cassandra][:config][:streaming_socket_timeout_in_ms]</code> Enable socket timeout for streaming operation (default: 3600000 - 1 hour)</li>\n<li><code>node[:cassandra][:config][:phi_convict_threshold]</code> Adjusts the sensitivity of the failure detector on an exponential scale (default: 8)</li>\n<li><code>node[:cassandra][:config][:endpoint_snitch]</code> SimpleSnitch, PropertyFileSnitch, GossipingPropertyFileSnitch, RackInferringSnitch, Ec2Snitch, Ec2MultiRegionSnitch (default: SimpleSnitch)</li>\n<li><code>node[:cassandra][:config][:dynamic_snitch_update_interval_in_ms]</code> How often to perform the more expensive part of host score calculation (default: 100)</li>\n<li><code>node[:cassandra][:config][:dynamic_snitch_reset_interval_in_ms]</code> How often to reset all host scores, allowing a bad host to possibly recover (default: 600000)</li>\n<li><code>node[:cassandra][:config][:dynamic_snitch_badness_threshold]</code> Allow 'pinning' of replicas to hosts in order to increase cache capacity. (default: 0.1)</li>\n<li><code>node[:cassandra][:config][:request_scheduler]</code> Class to schedule incoming client requests (default: org.apache.cassandra.scheduler.NoScheduler)</li>\n<li><code>node[:cassandra][:config][:index_interval]</code> index_interval controls the sampling of entries from the primary row index in terms of space versus time (default: 128).</li>\n<li><code>node[:cassandra][:config][:auto_bootstrap]</code> Setting this parameter to false prevents the new nodes from attempting to get all the data from the other nodes in the data center. (default: true).</li>\n<li><code>node[:cassandra][:config][:enable_assertions]</code> Enable JVM assertions.  Disabling this in production will give a modest performance benefit (around 5%) (default: true).</li>\n<li><code>node[:cassandra][:config][:data_file_directories]</code> (default: node['cassandra']['data_dir']): C* data cirectories</li>\n<li><code>node[:cassandra][:config][:saved_caches_directory]</code> (default: saved_caches_directory): C* saved cache directory</li>\n<li><code>node[:cassandra][:config][:commitlog_directory]</code> (default: node['cassandra']['commitlog_dir']) *C commit log directory</li>\n</ul><h4><a id=\"user-content-c-v20-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#c-v20-attributes\"></a>C* &lt;v2.0 Attributes</h4>\n<ul><li><code>node[:cassandra][:config][:memtable_flush_queue_size]</code> Number of full memtables to allow pending flush, i.e., waiting for a writer thread (default: 4)</li>\n<li><code>node[:cassandra][:config][:in_memory_compaction_limit_in_mb]</code> Size limit for rows being compacted in memory (default: 64)</li>\n<li><code>node[:cassandra][:config][:concurrent_compactors]</code> Sets the number of concurrent compaction processes allowed to run simultaneously on a node. (default: nil, which will result in one compaction process per CPU core)</li>\n<li><code>node[:cassandra][:config][:multithreaded_compaction]</code> Enable multithreaded compaction. Uses one thread per core, plus one thread per sstable being merged. (default: false)</li>\n<li><code>node[:cassandra][:config][:compaction_preheat_key_cache]</code> Track cached row keys during compaction and re-cache their new positions in the compacted sstable. Disable if you use really large key caches (default: true)</li>\n<li><code>node[:cassandra][:config][:native_transport_min_threads]</code> Min number of threads for handling transport requests when the native protocol is used (default: nil)</li>\n<li><code>node[:cassandra][:config][:native_transport_max_threads]</code> Max number of threads for handling transport requests when the native protocol is used (default: nil)</li>\n</ul><h4><a id=\"user-content-c-v21-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#c-v21-attributes\"></a>C* &gt;v2.1 Attributes</h4>\n<ul><li><code>node[:cassandra][:config][:broadcast_rpc_address]</code> RPC address to broadcast to drivers and other Cassandra nodes (default: node[:ipaddress])</li>\n<li><code>node[:cassandra][:config][:tombstone_failure_threshold]</code> tombstone attribute, check C* documentation for more info (default: 100000)</li>\n<li><code>node[:cassandra][:config][:tombstone_warn_threshold]</code> tombstone attribute, check C* documentation for more info (default: 1000)</li>\n<li><code>node[:cassandra][:config][:sstable_preemptive_open_interval_in_mb]</code> This helps to smoothly transfer reads between the sstables, reducing page cache churn and keeping hot rows hot (default: 50)</li>\n<li><code>node[:cassandra][:config][:memtable_allocation_type]</code> Specify the way Cassandra allocates and manages memtable memory (default: heap_buffers)</li>\n<li><code>node[:cassandra][:config][:index_summary_capacity_in_mb]</code> A fixed memory pool size in MB for for SSTable index summaries. If left empty, this will default to 5% of the heap size (default: nil)</li>\n<li><code>node[:cassandra][:config][:index_summary_resize_interval_in_minutes]</code> How frequently index summaries should be resampled (default: 60)</li>\n<li><code>node[:cassandra][:config][:concurrent_counter_writes]</code> Concurrent writes, since writes are almost never IO bound, the ideal number of \"concurrent_writes\" is dependent on the number of cores in your system; (8 * number_of_cores) (default: 32)</li>\n<li><code>node[:cassandra][:config][:counter_cache_save_period]</code> Duration in seconds after which Cassandra should save the counter cache (keys only) (default: 7200)</li>\n<li><code>node[:cassandra][:config][:counter_cache_size_in_mb]</code> Counter cache helps to reduce counter locks' contention for hot counter cells. Default value is empty to make it \"auto\" (min(2.5% of Heap (in MB), 50MB)). Set to 0 to disable counter cache. (default: nil)</li>\n<li><code>node[:cassandra][:config][:counter_write_request_timeout_in_ms]</code> How long the coordinator should wait for counter writes to complete (default: 5000)</li>\n<li><code>node[:cassandra][:config][:commit_failure_policy]</code> policy for commit disk failures (default: stop)</li>\n<li><code>node[:cassandra][:config][:cas_contention_timeout_in_ms]</code> How long a coordinator should continue to retry a CAS operation that contends with other proposals for the same row (default: 1000)</li>\n<li><code>node[:cassandra][:config][:batch_size_warn_threshold_in_kb]</code> Log WARN on any batch size exceeding this value. 5kb per batch by default (default: 5)</li>\n<li><code>node[:cassandra][:config][:batchlog_replay_throttle_in_kb]</code> Maximum throttle in KBs per second, total. This will be reduced proportionally to the number of nodes in the cluster (default: 1024)</li>\n</ul><h3><a id=\"user-content-jamm-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#jamm-attributes\"></a>JAMM Attributes</h3>\n<ul><li><code>node[:cassandra][:setup_jamm]</code> (default: false): install the jamm jar file and use it to set java option <code>-javaagent</code>, obsolete for C* versions <code>&gt;v0.8.0</code></li>\n<li><code>node[:cassandra][:jamm][:sha256sum]</code> (default: calculated): jamm lib sha256sum for calculated version</li>\n<li><code>node[:cassandra][:jamm][:base_url]</code> (default: calculated): jamm lib jar url</li>\n<li><code>node[:cassandra][:jamm][:jar_name]</code> (default: calculated): jamm lib jar name</li>\n<li><code>node[:cassandra][:jamm][:version]</code> (default: calculated): jamm lib version</li>\n</ul><h3><a id=\"user-content-jna-attributes-prior-c-version-210\" class=\"anchor\" aria-hidden=\"true\" href=\"#jna-attributes-prior-c-version-210\"></a>JNA Attributes (Prior C* version 2.1.0)</h3>\n<ul><li><code>node[:cassandra][:jna][:base_url]</code> The base url to fetch the JNA jar (default: <a href=\"https://github.com/twall/jna/tree/4.0/dist\">https://github.com/twall/jna/tree/4.0/dist</a>)</li>\n<li><code>node[:cassandra][:jna][:jar_name]</code> The name of the jar to download from the base url. (default: jna.jar)</li>\n<li><code>node[:cassandra][:jna][:sha256sum]</code> The SHA-256 checksum of the file. If the local jna.jar file matches the checksum, the chef-client will not re-download it. (default: dac270b6441ce24d93a96ddb6e8f93d8df099192738799a6f6fcfc2b2416ca19)</li>\n</ul><h3><a id=\"user-content-priam-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#priam-attributes\"></a>Priam Attributes</h3>\n<ul><li><code>node[:cassandra][:setup_priam]</code> (default: false): install the priam jar file and use it to set java option <code>-javaagent</code>, uses the priam version corresponding to the cassandra version</li>\n<li><code>node[:cassandra][:priam][:sha256sum]</code> (default: 9fde9a40dc5c538adee54f40fa9027cf3ebb7fd42e3592b3e6fdfe3f7aff81e1): priam lib sha256sum for version <code>2.2.0</code></li>\n<li><code>node[:cassandra][:priam][:base_url]</code> (default: priam url on maven.org): priam lib jar url</li>\n<li><code>node[:cassandra][:priam][:jar_name]</code> (default: calculated): priam lib jar name</li>\n</ul><h3><a id=\"user-content-logback-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#logback-attributes\"></a>Logback Attributes</h3>\n<ul><li><code>node[:cassandra][:logback][:file][:max_file_size]</code> (default: \"20MB\"): logback File appender log file rotation size</li>\n<li><code>node[:cassandra][:logback][:file][:max_index]</code> (default: 20): logback File appender log files max_index</li>\n<li><code>node[:cassandra][:logback][:file][:min_index]</code> (default: 1): logback File appender log files min_index</li>\n<li><code>node[:cassandra][:logback][:file][:pattern]</code> (default: \"%-5level [%thread] %date{ISO8601} %F:%L - %msg%n\"): logback File appender log pattern</li>\n<li><code>node[:cassandra][:logback][:debug][:enable]</code> (default: false): enable logback File appender log debug</li>\n<li><code>node[:cassandra][:logback][:debug][:max_file_size]</code> (default: \"20MB\"): logback File appender log file rotation size</li>\n<li><code>node[:cassandra][:logback][:debug][:max_index]</code> (default: 20): logback File appender log files max_index</li>\n<li><code>node[:cassandra][:logback][:debug][:min_index]</code> (default: 1): logback File appender log files min_index</li>\n<li><code>node[:cassandra][:logback][:debug][:pattern]</code> (default: \"%-5level [%thread] %date{ISO8601} %F:%L - %msg%n\"): logback File appender log pattern</li>\n<li><code>node[:cassandra][:logback][:stdout][:enable]</code> (default: true): enable logback STDOUT appender</li>\n<li><code>node[:cassandra][:logback][:stdout][:pattern]</code> (default: \"%-5level %date{HH:mm:ss,SSS} %msg%n\"): logback STDOUT appender log pattern</li>\n<li><code>node[:cassandra][:logback][:syslog][:enable]</code> (default: false): enable logback SYSLOG appender. Requires RSYSLOG be installed and running on the node.</li>\n<li><code>node[:cassandra][:logback][:syslog][:host]</code> (default: localhost): The host name the syslog is written to.</li>\n<li><code>node[:cassandra][:logback][:syslog][:facility]</code> (default: USER) The facility specified for the appender.</li>\n<li><code>node[:cassandra][:logback][:syslog][:pattern]</code> (default: \"%-5level [%thread] %F:%L - %msg%n\") lockback SYSLOG appender log pattern</li>\n<li><code>node[:cassandra][:logback][:override_loggers]</code> (default: {}) Override log level of specific logger (i.e { 'org.apache.cassandra.utils.StatusLogger' =&gt; 'WARN' })</li>\n</ul><h3><a id=\"user-content-ulimit-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#ulimit-attributes\"></a>Ulimit Attributes</h3>\n<ul><li><code>node[:cassandra][:limits][:memlock]</code> (default: \"unlimited\"): memory ulimit for Cassandra node process</li>\n<li><code>node[:cassandra][:limits][:nofile]</code> (default: 48000): file ulimit for Cassandra node process</li>\n<li><code>node[:cassandra][:limits][:nproc]</code> (default: \"unlimited\"): process ulimit for Cassandra node process</li>\n</ul><h3><a id=\"user-content-yum-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#yum-attributes\"></a>Yum Attributes</h3>\n<ul><li><code>node[:cassandra][:yum][:repo]</code> (default: datastax): name of the repo from which to install</li>\n<li><code>node[:cassandra][:yum][:description]</code> (default: \"DataStax Repo for Apache Cassandra\"): description of the repo</li>\n<li><code>node[:cassandra][:yum][:baseurl]</code> (default: \"<a href=\"http://rpm.datastax.com/community\" rel=\"nofollow\">http://rpm.datastax.com/community</a>\"): repo url</li>\n<li><code>node[:cassandra][:yum][:mirrorlist]</code> (default: nil): a mirrorlist file</li>\n<li><code>node[:cassandra][:yum][:gpgcheck]</code> (default: false): whether to use <code>gpgcheck</code></li>\n<li><code>node[:cassandra][:yum][:enabled]</code> (default: true): whether the repo is enabled by default</li>\n<li><code>node[:cassandra][:yum][:options]</code> (default: \"\"): Additional options to pass to <code>yum_package</code></li>\n</ul><h3><a id=\"user-content-opscenter-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#opscenter-attributes\"></a>OpsCenter Attributes</h3>\n<h4><a id=\"user-content-datastax-ops-center-server-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#datastax-ops-center-server-attributes\"></a>DataStax Ops Center Server attributes</h4>\n<ul><li><code>node[:cassandra][:opscenter][:server][:package_name]</code> (default: opscenter-free)</li>\n<li><code>node[:cassandra][:opscenter][:server][:port]</code> (default: 8888)</li>\n<li><code>node[:cassandra][:opscenter][:server][:interface]</code> (default: 0.0.0.0)</li>\n<li><code>node[:cassandra][:opscenter][:server][:authentication]</code> (default: false)</li>\n<li><code>node[:cassandra][:opscenter][:cassandra_metrics][:ignored_keyspaces]</code> (default: [system, OpsCenter])</li>\n<li><code>node[:cassandra][:opscenter][:cassandra_metrics][:ignored_column_families]</code> (default: [])</li>\n<li><code>node[:cassandra][:opscenter][:cassandra_metrics][:1min_ttl]</code> (default: 604800)</li>\n<li><code>node[:cassandra][:opscenter][:cassandra_metrics][:5min_ttl]</code> (default: 2419200)</li>\n<li><code>node[:cassandra][:opscenter][:cassandra_metrics][:2hr_ttl]</code> (default: 31536000)</li>\n<li><code>node[:cassandra][:opscenter][:custom_configuration]</code> (default: {}) a hash of custom configuration sections to add to <a href=\"https://docs.datastax.com/en/opscenter/6.0/opsc/configure/opscConfigProps_r.html\" rel=\"nofollow\">opscenterd.conf</a>, e.g.:</li>\n</ul><pre>{\n 'ui' =&gt; {\n   'default_api_timeout' =&gt; 300\n },\n 'stat_reporter' =&gt; {\n   'interval' =&gt; 1\n }\n}\n</pre>\n<h4><a id=\"user-content-datastax-ops-center-agent-tarball-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#datastax-ops-center-agent-tarball-attributes\"></a>DataStax Ops Center Agent Tarball attributes</h4>\n<ul><li><code>node[:cassandra][:opscenter][:agent][:download_url]</code> (default: \"\") Required. You need to specify\nagent download url, because that could be different for each opscenter server version. ( S3 is a great\nplace to store packages )</li>\n<li><code>node[:cassandra][:opscenter][:agent][:checksum]</code> (default: <code>nil</code>)</li>\n<li><code>node[:cassandra][:opscenter][:agent][:install_dir]</code> (default: <code>/opt</code>)</li>\n<li><code>node[:cassandra][:opscenter][:agent][:install_folder_name]</code> (default: <code>opscenter_agent</code>)</li>\n<li><code>node[:cassandra][:opscenter][:agent][:binary_name]</code> (default: <code>opscenter-agent</code>) Introduced since Datastax changed agent binary name from opscenter-agent to datastax-agent. <strong>Make sure to set it right if you are updating to 4.0.2</strong></li>\n<li><code>node[:cassandra][:opscenter][:agent][:server_host]</code> (default: \"\" ). If left empty, will use search to get IP by opscenter <code>server_role</code> role.</li>\n<li><code>node[:cassandra][:opscenter][:agent][:server_role]</code> (default: <code>opscenter_server</code>). Will be use for opscenter server IP lookup if <code>:server_host</code> is not set.</li>\n<li><code>node[:cassandra][:opscenter][:agent][:use_chef_search]</code> (default: <code>true</code>). Determines whether chef search will be used for locating the data agent server.</li>\n<li><code>node[:cassandra][:opscenter][:agent][:use_ssl]</code> (default: <code>false</code>)</li>\n</ul><h4><a id=\"user-content-datastax-ops-center-agent-datastax-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#datastax-ops-center-agent-datastax-attributes\"></a>DataStax Ops Center Agent Datastax attributes</h4>\n<ul><li><code>node[:cassandra][:opscenter][:agent][:package_name]</code> (default: \"datastax-agent\" ).</li>\n<li><code>node[:cassandra][:opscenter][:agent][:server_host]</code> (default: \"\" ). If left empty, will use search to get IP by opscenter <code>server_role</code> role.</li>\n<li><code>node[:cassandra][:opscenter][:agent][:server_role]</code> (default: <code>opscenter_server</code>). Will be use for opscenter server IP lookup if <code>:server_host</code> is not set.</li>\n<li><code>node[:cassandra][:opscenter][:agent][:use_ssl]</code> (default: <code>false</code>)</li>\n</ul><h3><a id=\"user-content-data-center-and-rack-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#data-center-and-rack-attributes\"></a>Data Center and Rack Attributes</h3>\n<ul><li><code>node[:cassandra][:rackdc][:dc]</code> (default: \"\") The datacenter to specify in the cassandra-rackdc.properties file. (GossipingPropertyFileSnitch only)</li>\n<li><code>node[:cassandra][:rackdc][:rack]</code> (default: \"\") The rack to specify in the cassandra-rackdc.properties file (GossipingPropertyFileSnitch only)</li>\n<li><code>node[:cassandra][:rackdc][:prefer_local]</code> (default: \"false\") Whether the snitch will prefer the internal ip when possible, as the Ec2MultiRegionSnitch does. (GossipingPropertyFileSnitch only)</li>\n</ul><h2><a id=\"user-content-contributing\" class=\"anchor\" aria-hidden=\"true\" href=\"#contributing\"></a>Contributing</h2>\n<p>See <a href=\"https://github.com/michaelklishin/cassandra-chef-cookbook/blob/master/CONTRIBUTING.md\">CONTRIBUTING.md</a> and <a href=\"https://github.com/michaelklishin/cassandra-chef-cookbook/blob/master/TESTING.md\">TESTING.md</a>.</p>\n<h2><a id=\"user-content-copyright--license\" class=\"anchor\" aria-hidden=\"true\" href=\"#copyright--license\"></a>Copyright &amp; License</h2>\n<p>Michael S. Klishin, Travis CI Development Team, and <a href=\"https://github.com/michaelklishin/cassandra-chef-cookbook/graphs/contributors\">contributors</a>,\n2012-2018.</p>\n<p>Released under the <a href=\"http://www.apache.org/licenses/LICENSE-2.0.html\" rel=\"nofollow\">Apache 2.0 License</a>.</p>\n</article>",
        "created_at": "2018-10-24T13:39:10+0000",
        "updated_at": "2018-10-24T13:39:17+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 22,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/1090?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12454"
          }
        }
      }
    ]
  }
}