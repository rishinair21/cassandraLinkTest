{
  "page": 1,
  "limit": 100,
  "pages": 5,
  "total": 418,
  "_links": {
    "self": {
      "href": "http://leaves.anant.us:82/api/entries?sort=created&order=desc&tags=cassandra&since=0&page=1&perPage=100"
    },
    "first": {
      "href": "http://leaves.anant.us:82/api/entries?sort=created&order=desc&tags=cassandra&since=0&page=1&perPage=100"
    },
    "last": {
      "href": "http://leaves.anant.us:82/api/entries?sort=created&order=desc&tags=cassandra&since=0&page=5&perPage=100"
    },
    "next": {
      "href": "http://leaves.anant.us:82/api/entries?sort=created&order=desc&tags=cassandra&since=0&page=2&perPage=100"
    }
  },
  "_embedded": {
    "items": [
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 12,
            "label": "video",
            "slug": "video"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 568,
            "label": "azure",
            "slug": "azure"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          },
          {
            "id": 994,
            "label": "dynamo",
            "slug": "dynamo"
          },
          {
            "id": 1298,
            "label": "google.cloud",
            "slug": "google-cloud"
          },
          {
            "id": 1385,
            "label": "dse",
            "slug": "dse"
          }
        ],
        "is_public": false,
        "id": 12757,
        "uid": null,
        "title": "Apache Cassandra vs Other Cloud Databases | DataStax",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=G8xHwnCevYo",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/G8xHwnCevYo?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-11-14T21:22:27+0000",
        "updated_at": "2018-11-14T21:22:27+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/G8xHwnCevYo/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12757"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 12,
            "label": "video",
            "slug": "video"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12756,
        "uid": null,
        "title": "Apache Cassandra Best Practices - Live Webinar",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=dhu9qzetnY8",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/dhu9qzetnY8?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-11-14T21:22:26+0000",
        "updated_at": "2018-11-14T21:22:26+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/dhu9qzetnY8/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12756"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 995,
            "label": "testing",
            "slug": "testing"
          },
          {
            "id": 1232,
            "label": "stress",
            "slug": "stress"
          },
          {
            "id": 1430,
            "label": "gatling",
            "slug": "gatling"
          }
        ],
        "is_public": false,
        "id": 12754,
        "uid": null,
        "title": "yabinmeng/cassgatling",
        "url": "https://github.com/yabinmeng/cassgatling",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>This is a sample code of utilizing <a href=\"https://gatling.io/\" rel=\"nofollow\">Gatling</a> testing framework for stress testing with DSE</p>\n<h3><a id=\"user-content-pre-requisites\" class=\"anchor\" aria-hidden=\"true\" href=\"#pre-requisites\"></a>Pre-requisites</h3>\n<p>Note: At the moment, the latest version of Gatling framework is 2.3.1. However, the CQL plugin 0.0.7 is not compatible with Gatling 2.3.1 yet.</p>\n<ol><li>Gatling high charts bundle version 2.2.5:</li>\n</ol><ul><li><a href=\"https://repo1.maven.org/maven2/io/gatling/highcharts/gatling-charts-highcharts-bundle/2.2.5/\" rel=\"nofollow\">https://repo1.maven.org/maven2/io/gatling/highcharts/gatling-charts-highcharts-bundle/2.2.5/</a></li>\n</ul><ol start=\"2\"><li>Gatling CQL plugin v0.0.7 (latest version as of writing)</li>\n</ol><p><strong>[NOTES]</strong> - as of April 10, 2018, Gatling CQL plugin has released version 0.0.8 which works with the latest Gatling framework 2.3.1. Please feel free to download these latest versions for your test. Please also be aware that Gatling 2.3.1 requires scala 2.1.2 and make sure your scala version is upgraded first.</p>\n<h3><a id=\"user-content-notes-about-the-example-simulation-scenario\" class=\"anchor\" aria-hidden=\"true\" href=\"#notes-about-the-example-simulation-scenario\"></a>Notes about the Example Simulation Scenario</h3>\n<p>The simulation scenario (MyTestSimu.scala) as included in this example simulates a mixed read/write workload. The core steps of the scenario are summarized below and you can follow the same steps when creating your own scenario:</p>\n<ol><li>Set up connection to Cassandra/DSE cluster with proper properties, such as \"contact points\", \"load balancing policy\", etc.</li>\n<li>Create the application keyspace and table schema</li>\n<li>Define the (random) value generator for table columns based on their types</li>\n<li>Define the Read/Write statements to be used in the simulation, including the Consistency Level associated with the statements</li>\n<li>Set up the user simulation behavior for Read and Write. The key parts include:</li>\n</ol><ul><li>How many concurrent users (can be constant or some variance) to be simulated per second.</li>\n<li>How long does the simulation executes</li>\n</ul><h3><a id=\"user-content-procedure\" class=\"anchor\" aria-hidden=\"true\" href=\"#procedure\"></a>Procedure</h3>\n<ol><li>Set up Gatling framework and the CQL plug-in (just unzip, as per description found <a href=\"https://github.com/gatling-cql/GatlingCql\">here</a>)</li>\n<li>Download the MyTestSimu.scala file and put it under folder &lt;GATLING_HOME&gt;/user-files/simulations/</li>\n<li>Execute the Gatling simulation (stress-testing) scenario by running command: &lt;GATLING_HOME&gt;/bin/gatling.sh. Follow the instructions on the command-line output.</li>\n</ol><p>NOTE: The simulation scenario (MyTestSimu.scala) is tested against a DSE cluster (version 5.1.6) with UserName/Password authentication. Please adjust accordingly for your case.</p>\n<hr /><p>An example is as below. Simulation number 1 is the Cassandra stres-testing scenario as defined by this example.</p>\n<pre>$ bin/gatling.sh\nGATLING_HOME is set to /home/automaton/gatling-charts-highcharts-bundle-2.2.5\nChoose a simulation number:\n     [0] cassandra.CassandraSimulation\n     [1] cassandra.MyTestSimu\n     [2] computerdatabase.BasicSimulation\n     [3] computerdatabase.advanced.AdvancedSimulationStep01\n     [4] computerdatabase.advanced.AdvancedSimulationStep02\n     [5] computerdatabase.advanced.AdvancedSimulationStep03\n     [6] computerdatabase.advanced.AdvancedSimulationStep04\n     [7] computerdatabase.advanced.AdvancedSimulationStep05\n1\nSelect simulation id (default is 'mytestsimu'). Accepted characters are a-z, A-Z, 0-9, - and _\nSelect run description (optional)\nSimulation cassandra.MyTestSimu started...\n================================================================================\n2018-04-04 15:33:43                                           5s elapsed\n---- Requests ------------------------------------------------------------------\n&gt; Global                                                   (OK=232    KO=0     )\n&gt; upsertStmt                                               (OK=93     KO=0     )\n&gt; readStmt                                                 (OK=139    KO=0     )\n---- Read Workload Scenario ----------------------------------------------------\n[#                                                                         ]  1%\n          waiting: 8861   / active: 0      / done:139\n---- Write Workload Scenario ---------------------------------------------------\n[                                                                          ]  0%\n          waiting: 20907  / active: 0      / done:93\n================================================================================\n... ...\n================================================================================\n2018-04-04 15:43:39                                         600s elapsed\n---- Requests ------------------------------------------------------------------\n&gt; Global                                                   (OK=30000  KO=0     )\n&gt; upsertStmt                                               (OK=21000  KO=0     )\n&gt; readStmt                                                 (OK=9000   KO=0     )\n---- Read Workload Scenario ----------------------------------------------------\n[##########################################################################]100%\n          waiting: 0      / active: 0      / done:9000\n---- Write Workload Scenario ---------------------------------------------------\n[##########################################################################]100%\n          waiting: 0      / active: 0      / done:21000\n================================================================================\nSimulation cassandra.MyTestSimu completed in 600 seconds\nParsing log file(s)...\nParsing log file(s) done\nGenerating reports...\n================================================================================\n---- Global Information --------------------------------------------------------\n&gt; request count                                      30000 (OK=30000  KO=0     )\n&gt; min response time                                      0 (OK=0      KO=-     )\n&gt; max response time                                    224 (OK=224    KO=-     )\n&gt; mean response time                                     2 (OK=2      KO=-     )\n&gt; std deviation                                          5 (OK=5      KO=-     )\n&gt; response time 50th percentile                          1 (OK=1      KO=-     )\n&gt; response time 75th percentile                          2 (OK=2      KO=-     )\n&gt; response time 95th percentile                          3 (OK=3      KO=-     )\n&gt; response time 99th percentile                          7 (OK=7      KO=-     )\n&gt; mean requests/sec                                     50 (OK=50     KO=-     )\n---- Response Time Distribution ------------------------------------------------\n&gt; t &lt; 800 ms                                         30000 (100%)\n&gt; 800 ms &lt; t &lt; 1200 ms                                   0 (  0%)\n&gt; t &gt; 1200 ms                                            0 (  0%)\n&gt; failed                                                 0 (  0%)\n================================================================================\nReports generated in 4s.\nPlease open the following file: /home/automaton/gatling-charts-highcharts-bundle-2.2.5/results/mytestsimu-1522856018600/index.html\n</pre>\n</article>",
        "created_at": "2018-11-14T17:34:32+0000",
        "updated_at": "2018-11-14T17:34:41+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/16789452?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12754"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 217,
            "label": "tool",
            "slug": "tool"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          }
        ],
        "is_public": false,
        "id": 12753,
        "uid": null,
        "title": "yabinmeng/dseutilities",
        "url": "https://github.com/yabinmeng/dseutilities",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\"><h2><a id=\"user-content-description\" class=\"anchor\" aria-hidden=\"true\" href=\"#description\"></a>Description</h2>\n<p>This repository contains a list of Misc. utilities, sample codes, and blog posts that I wrote in order to ease the learning and working with DataStax Enterprise (DSE).</p>\n<p><strong>Disclaimer</strong>: Most, if not all, of the utilities/sample codes/blog posts are based on DSE 5.1.x version (the latest major DSE release as of writing). For older DSE versions such as 5.0.x or 4.8.x, please use at your own discretion and feel free to test out by yourself.</p>\n<hr /><h2><a id=\"user-content-1-utilities\" class=\"anchor\" aria-hidden=\"true\" href=\"#1-utilities\"></a>1. Utilities</h2>\n<h4><a id=\"user-content-11-wrapper-utility-to-encrypt-dse-configuration\" class=\"anchor\" aria-hidden=\"true\" href=\"#11-wrapper-utility-to-encrypt-dse-configuration\"></a>1.1. <a href=\"https://github.com/yabinmeng/dseutilities/tree/master/dseconfenc\">Wrapper Utility to Encrypt DSE Configuration</a></h4>\n<h4><a id=\"user-content-12-wrapper-utility-to-encrypt-opscenter-configuration\" class=\"anchor\" aria-hidden=\"true\" href=\"#12-wrapper-utility-to-encrypt-opscenter-configuration\"></a>1.2. <a href=\"https://github.com/yabinmeng/dseutilities/tree/master/opsconfenc\">Wrapper Utility to Encrypt OpsCenter Configuration</a></h4>\n<h4><a id=\"user-content-13-cassandra-tombstone-counter\" class=\"anchor\" aria-hidden=\"true\" href=\"#13-cassandra-tombstone-counter\"></a>1.3. <a href=\"https://github.com/yabinmeng/tombstone_counter\">Cassandra Tombstone Counter</a></h4>\n<h4><a id=\"user-content-14-ansible-playbook-for-multi-dc-dse-cluster-provisioning\" class=\"anchor\" aria-hidden=\"true\" href=\"#14-ansible-playbook-for-multi-dc-dse-cluster-provisioning\"></a>1.4. <a href=\"https://github.com/yabinmeng/dseansible\">Ansible Playbook for multi-DC DSE Cluster Provisioning</a></h4>\n<h4><a id=\"user-content-15-terraform-and-ansible-automation-to-provision-multi-dc-dse-cluster-on-aws\" class=\"anchor\" aria-hidden=\"true\" href=\"#15-terraform-and-ansible-automation-to-provision-multi-dc-dse-cluster-on-aws\"></a>1.5. <a href=\"https://github.com/yabinmeng/terradse\">Terraform and Ansible Automation to Provision multi-DC DSE Cluster on AWS</a></h4>\n<h4><a id=\"user-content-16-restore-dse-backup-data-from-aws-s3\" class=\"anchor\" aria-hidden=\"true\" href=\"#16-restore-dse-backup-data-from-aws-s3\"></a>1.6. <a href=\"https://github.com/yabinmeng/opscs3restore\">Restore DSE backup data from AWS S3</a></h4>\n<h4><a id=\"user-content-17-oss-gatling-stress-testing-scenario-for-dse\" class=\"anchor\" aria-hidden=\"true\" href=\"#17-oss-gatling-stress-testing-scenario-for-dse\"></a>1.7. <a href=\"https://github.com/yabinmeng/cassgatling\">OSS Gatling Stress Testing Scenario for DSE</a></h4>\n<hr /><h2><a id=\"user-content-2-blog-posts\" class=\"anchor\" aria-hidden=\"true\" href=\"#2-blog-posts\"></a>2. Blog Posts</h2>\n<hr /><h2><a id=\"user-content-3-sample-codes\" class=\"anchor\" aria-hidden=\"true\" href=\"#3-sample-codes\"></a>3. Sample Codes</h2>\n<h4><a id=\"user-content-31-writting-udt-into-cassandra\" class=\"anchor\" aria-hidden=\"true\" href=\"#31-writting-udt-into-cassandra\"></a>3.1. <a href=\"https://github.com/yabinmeng/dseudt\">Writting UDT into Cassandra</a></h4>\n</article>",
        "created_at": "2018-11-14T17:34:02+0000",
        "updated_at": "2018-11-14T17:34:15+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/16789452?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12753"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1233,
            "label": "data.modeling",
            "slug": "data-modeling"
          }
        ],
        "is_public": false,
        "id": 12752,
        "uid": null,
        "title": "yabinmeng/dseutilities",
        "url": "https://github.com/yabinmeng/dseutilities/blob/master/documents/Yabin.meng-CassandraTombstoneInDepth.pdf",
        "content": "<div role=\"main\" class=\"application-main\" data-organization-hovercards-enabled=\"\"><div itemscope=\"itemscope\" itemtype=\"http://schema.org/SoftwareSourceCode\" class=\"\"><div id=\"js-repo-pjax-container\" data-pjax-container=\"\"><div class=\"container new-discussion-timeline experiment-repo-nav\"><div class=\"repository-content\"><a class=\"d-none js-permalink-shortcut\" data-hotkey=\"y\" href=\"https://github.com/yabinmeng/dseutilities/blob/32dd15bbd4090e34bd61c85675fef4045df6d1a3/documents/Yabin.meng-CassandraTombstoneInDepth.pdf\">Permalink</a><div class=\"commit-tease\"><a class=\"commit-tease-sha\" href=\"https://github.com/yabinmeng/dseutilities/commit/f9be5c04edf81c7cd80623ab7d4e075e17b32550\" data-pjax=\"\">\n          f9be5c0\n        </a>Jul 8, 2018<p><a class=\"user-mention\" rel=\"author\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16789452\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yabinmeng\">yabinmeng</a>\n          <a data-pjax=\"true\" title=\"Initial documents upload\" class=\"message\" href=\"https://github.com/yabinmeng/dseutilities/commit/f9be5c04edf81c7cd80623ab7d4e075e17b32550\">Initial documents upload</a></p><div class=\"commit-tease-contributors\"><details class=\"details-reset details-overlay details-overlay-dark lh-default text-gray-dark float-left mr-2\" id=\"blob_contributors_box\"><summary class=\"btn-link\" aria-haspopup=\"dialog\"><strong>1</strong> contributor\n  </summary><details class=\"Box Box--overlay d-flex flex-column anim-fade-in fast\" aria-label=\"Users who have contributed to this file\">\n    \n        <ul class=\"list-style-none overflow-auto\"><li class=\"Box-row\">\n              <a class=\"link-gray-dark no-underline\" href=\"https://github.com/yabinmeng\">\n                <img class=\"avatar mr-2\" alt=\"\" src=\"https://avatars2.githubusercontent.com/u/16789452?s=40&amp;v=4\" width=\"20\" height=\"20\" />\n                yabinmeng\n</a>            </li>\n        </ul></details></details></div></div><details class=\"details-reset details-overlay details-overlay-dark\"><summary data-hotkey=\"l\" aria-label=\"Jump to line\">    </summary></details></div></div></div></div></div><div id=\"ajax-error-message\" class=\"ajax-error-message flash flash-error\">You can’t perform that action at this time.</div>\n    \n  \n  \n  <details class=\"details-reset details-overlay details-overlay-dark lh-default text-gray-dark\" open=\"open\"><summary aria-haspopup=\"dialog\" aria-label=\"Close dialog\">\n      \n    </summary></details>\n<p>Press h to open a hovercard with more details.</p>",
        "created_at": "2018-11-14T17:33:42+0000",
        "updated_at": "2018-11-14T17:33:49+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/16789452?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12752"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          }
        ],
        "is_public": false,
        "id": 12741,
        "uid": null,
        "title": "DataStax Distribution of Apache Cassandra",
        "url": "https://www.datastax.com/products/datastax-distribution-of-apache-cassandra",
        "content": "DataStax Distribution of Apache Cassandra | DataStax \n\n<noscript>\n\n\n\n\n<div class=\"DS17\"><div class=\"connect-us\"><a href=\"https://www.datastax.com/contactus\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Mail.svg\" alt=\"email icon\" />email</a><a href=\"https://www.datastax.com/company#offices\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Phone.svg\" alt=\"phone icon\" />call</a></div></div><section class=\"intro-with-content\"><div class=\"container\"><div class=\"wrapper text-center\"><div class=\"content wow fadeInUp\"><h3>DataStax and Apache Cassandra</h3><p>DataStax and Apache Cassandra are tied at the hip - literally and figuratively. Cassandra is the backbone of DataStax technology and the reason we love data management. From its inception, Apache Cassandra has been the premier distributed database on the market, and here at DataStax we remain committed to continuing that legacy and expanding it with world-class enterprise features delivered via DataStax Enterprise 6 - the best distribution of Apache Cassandra.</p></div><div class=\"col-wrapper\"><div class=\"three-col wow fadeInUp\"><h6>#1 OSS Committer</h6><p>DataStax provides over 80% of the commits to Apache Cassandra Open Source and we are one of the driving forces behind Apache Cassandra 4.0.</p></div><div class=\"three-col wow fadeInUp\"><h6>Apache Cassandra Experts</h6><p>DataStax Enterprise is developed and updated from the open source Cassandra project, and the DataStax team has been an integral part of the Cassandra project since its inception (check out our history <a href=\"https://www.datastax.com/why-datastax/apache-cassandra\" target=\"_self\">here</a>).&#13;\n</p></div><div class=\"three-col wow fadeInUp\"><h6>Open Source Leadership</h6><p>DataStax provides open source leadership in other database-related projects (like Apache TinkerPop™) as part of our commitment to open source.</p></div></div></div></div></section><section class=\"cards wow fadeInUp dse-cards no-padding-bottom   bg-gray\" data-wow-delay=\"0.5s\"><div class=\"container\"><p>&#13;\n        </p><h3>Why DataStax Distribution of Apache Cassandra?</h3>&#13;<div class=\"column-wrapper text-center\"><div class=\"three-col wow fadeInUp col card-blue\"><p>&#13;\n              </p><h4>Production Certified Apache Cassandra</h4>&#13;<div class=\"icon-box\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Graph_Powered_Insights.svg\" alt=\"circular icon with customer surrounded by dots\" /></div><p>100% OSS compatible but also production certified with hot fixes, bug escalation, and nightly regression testing included as standard.</p></div><div class=\"three-col wow fadeInUp col card-blue\"><p>&#13;\n              </p><h4>Backed by the Apache Cassandra Experts</h4>&#13;<div class=\"icon-box\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Graph_Your_way.svg\" alt=\"circular icon showing datastax graph personalization\" /></div><p>Full access to the best-in-class Apache Cassandra software, support, and services from the experts.</p></div><div class=\"three-col wow fadeInUp col card-blue\"><p>&#13;\n              </p><h4>Streamline Operations and Support</h4>&#13;<div class=\"icon-box\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Graph_Available_Always.svg\" alt=\"circular icon showing datastax graphs are available around the clock\" /></div><p>Remove your patchwork quilt of in house, regional SI’s and third party vendors providing support and services with a single, expert team.</p></div></div></div></section><section class=\"database-table  bg-gray\" id=\"databaseTable\"><div class=\"container\"><div class=\"full-width-table\"><table><thead><tr><td><h5>Open Source Compatible</h5></td>\n          <td><div class=\"img-box\"><img src=\"https://www.datastax.com/templates/dist/images/check_link.png\" alt=\"check-arrow\" /></div></td>\n          <td><div class=\"img-box\"><img src=\"https://www.datastax.com/templates/dist/images/check_link.png\" alt=\"check-arrow\" /></div></td>\n          </tr><tr><td><h5>Production Certified</h5></td>\n          <td>&#13;\n          </td><td><div class=\"img-box\"><img src=\"https://www.datastax.com/templates/dist/images/check_link.png\" alt=\"check-arrow\" /></div></td>\n          </tr><tr><td><h5>Drivers</h5></td>\n          <td>&#13;\n          </td><td><div class=\"img-box\"><img src=\"https://www.datastax.com/templates/dist/images/check_link.png\" alt=\"check-arrow\" /></div></td>\n          </tr><tr></tr><tr><td><h5>DataStax Bulk Loader</h5></td>\n          <td>&#13;\n          </td><td><div class=\"img-box\"><img src=\"https://www.datastax.com/templates/dist/images/check_link.png\" alt=\"check-arrow\" /></div></td>\n          </tr><tr></tr><tr><td><h5>Technical Support</h5></td>\n          <td>&#13;\n          </td><td><div class=\"img-box\"><img src=\"https://www.datastax.com/templates/dist/images/check_link.png\" alt=\"check-arrow\" /></div></td>\n          </tr><tr></tr><tr><td><h5>Professional Services</h5></td>\n          <td>&#13;\n          </td><td><div class=\"img-box\"><img src=\"https://www.datastax.com/templates/dist/images/check_link.png\" alt=\"check-arrow\" /></div></td>\n          </tr><tr></tr><tr><td><h5>Hot Fixes &amp; Bug Escalation</h5></td>\n          <td>&#13;\n          </td><td><div class=\"img-box\"><img src=\"https://www.datastax.com/templates/dist/images/check_link.png\" alt=\"check-arrow\" /></div></td>\n          </tr><tr></tr></thead></table></div></div></section><section class=\"content-with-media-box\"><section class=\"latest-block  bg-gray\"><section class=\"cta round-left-bottom no-padding\"><div class=\"DS17\"><div class=\"use-case\"><div class=\"wrapper\"><div class=\"two-col text-light-blue\"><h6>Customer Experience</h6><ul><li><a href=\"https://www.datastax.com/use-cases/customer-360\">Customer 360</a></li>\n          <li><a href=\"https://www.datastax.com/personalization\">Personalization &amp; Recommendations</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/loyalty-programs\">Loyalty Programs</a></li>\n          <li><a href=\"https://www.datastax.com/fraud-detection\">Consumer Fraud Detection</a></li>\n        </ul></div><div class=\"two-col text-light-green\"><h6><a href=\"#\">Enterprise Optimization</a></h6><ul><li><a href=\"https://www.datastax.com/use-cases/ecommerce\">eCommerce</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/identity-management\">Identity Management</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/security\">Security and Compliance</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/supply-chain\">Supply Chain</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/inventory-management\">Inventory Management</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/asset-monitoring\">Asset Monitoring</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/logistics\">Logistics</a></li>\n        </ul></div></div></div></div>\n\t\n\t\n\t\n\t\n</section></section></section></noscript>",
        "created_at": "2018-11-13T21:40:01+0000",
        "updated_at": "2018-11-13T21:40:11+0000",
        "published_at": "2018-11-02T20:17:48+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 1,
        "domain_name": "www.datastax.com",
        "preview_picture": "https://www.datastax.com/wp-content/uploads/resources/social/Generic_DataStax_FB.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12741"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          },
          {
            "id": 956,
            "label": "streaming",
            "slug": "streaming"
          },
          {
            "id": 1385,
            "label": "dse",
            "slug": "dse"
          }
        ],
        "is_public": false,
        "id": 12729,
        "uid": null,
        "title": "phact/SimpleSparkStreaming",
        "url": "https://github.com/phact/SimpleSparkStreaming",
        "content": "<p>This is a guide for how to use the Streaming Analytics Proofshop asset brought to you by the Vanguard team. The Proofshop consists of demonstrating real time data processing and analytics with the DSE paltform at high throughput, low latency, and scale.</p><h3>Motivation</h3><p>DSE is often seen as a serving layer for the output of analytical applications on large datasets. Often the analytics themselves are performed in slow batch data layers and persisted to DSE only when they are ready to be exposed to a front end or downstream system. In many cases, as we look to satisfy end user requirements, it makes more sense to perform some of these operational analytics in real time using DSE's streaming analytics capabilities.</p><h3>What is included?</h3><p>This field asset includes sample usage of DSE Streaming Analytics in the following contexts:</p><ul><li>Data generation using EBDSE served via TCP</li>\n<li>Spark streaming application that persists raw events into DSE and rollups into summary tables</li>\n<li>Tuning capabilities for throughput and latencies</li>\n<li>Checkpointing into DSEFS</li>\n</ul><h3>Business Take Aways</h3><p>In the right now economy businesses need analytics that are up to date. Monthly, weekly, and hourly reports are often too old to be actionable. DSE Streaming analytics allows businesses to operationalize structured analytics to obtain real time insights to power their decision making.</p><p>Out of the Five Dimensions, this asset focuses on Relevancy and Responsiveness without ignoring the remaining dimensions (Availability, Accessibility, Engagement).</p><p>If discussing this asset with a business stakeholder it may be relevant to walk them through <a href=\"https://docs.google.com/presentation/d/1z_wGENm2RNX1oqwUkSzDg3P03xPG2P9lKEjj8UiZOQM/edit?usp=sharing\" rel=\"nofollow\">The DataStax Story</a></p><h3>Technical Take Aways</h3><p>DSE Streaming Analytics is DataStax's version of Apache Spark (TM) which ships with DSE and is modified to match the design principles that our engineering team has always focused on when building C* and DSE (CARDS). DSE Spark is optimized for high availability and operational simplicity by removing its dependency on zookeeper and using LWTs for leader elections. Furthermore, DSE Spark is optimized for performance against the DSE backend with features that include Contiunous Paging and Direct Joins.</p><p>Building Streaming Analytics applications on DSE requires:</p><ul><li>Having a streaming source - in this case we use EBDSE's tcpserver functionality and data generation capabilities as our streaming source</li>\n<li>A Spark Streaming Application - sources for the app included as part of the asset</li>\n</ul><p>For more general information on how to use Spark Streaming check out the <a href=\"https://spark.apache.org/docs/latest/streaming-programming-guide.html\" rel=\"nofollow\">Programming Guide</a> in the Spark Docs.</p><p>These docs will dive deeper on the following functionality:</p><ul><li>Cumulative Streaming Calculations</li>\n<li>Monitoring and Tuning Streaming Jobs</li>\n</ul>",
        "created_at": "2018-11-13T20:26:31+0000",
        "updated_at": "2018-11-13T20:27:26+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "github.com",
        "preview_picture": "https://avatars3.githubusercontent.com/u/1313220?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12729"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 568,
            "label": "azure",
            "slug": "azure"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          },
          {
            "id": 995,
            "label": "testing",
            "slug": "testing"
          },
          {
            "id": 1232,
            "label": "stress",
            "slug": "stress"
          },
          {
            "id": 1298,
            "label": "google.cloud",
            "slug": "google-cloud"
          },
          {
            "id": 1385,
            "label": "dse",
            "slug": "dse"
          }
        ],
        "is_public": false,
        "id": 12722,
        "uid": null,
        "title": "justinbreese/dse-cassandra-stress",
        "url": "https://github.com/justinbreese/dse-cassandra-stress",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<ul><li>I wanted to run some <code>cassandra-stress</code> testing on the three main cloud providers (Google, Amazon, and Microsoft) on similarly sized DataStax Enterprise clusters and see what the results would be</li>\n<li>Why did I want to do this? When running on a cluster of similarly-sized VMs, networking starts to come into play for replication, storage, and stronger consistency levels. This type of an exercise exposes the network so you can understand what you may have to contend with for a specific cloud provider versus another</li>\n<li>This is also a tutorial on some of the fun switches that you can use to tune your <code>cassandra-stress</code> test</li>\n<li>This is not a tutorial on how to deploy the VMs, if you want that information, please go over to another repo of mine as I repurposed it. It is 99% similar; I just didn't enable SSL and installed <code>dse-full</code>: <a href=\"https://github.com/justinbreese/dse-multi-cloud-demo\">https://github.com/justinbreese/dse-multi-cloud-demo</a></li>\n</ul><h2><a id=\"user-content-methodology\" class=\"anchor\" aria-hidden=\"true\" href=\"#methodology\"></a>Methodology</h2>\n<ul><li>Setup 4 VMs on each of the cloud providers; VM size was as close as possible with similar storage provisioned: i3.4xl, Standard_DS14_v2, and n1-highmem-16</li>\n<li>3 of the VMs will be running DataStax Enterprise as their own cluster; again each cloud is their own cluster</li>\n<li>The 4th VM runs DataStax OpsCenter and acts as a <code>cassandra-stress</code> client\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/justinbreese/dse-cassandra-stress/blob/master/architecture.png?raw=true\"><img src=\"https://github.com/justinbreese/dse-cassandra-stress/raw/master/architecture.png?raw=true\" alt=\"Screenshot\" /></a></li>\n<li>Next, I ran this test on each of the clients 35 times: <code>cassandra-stress user profile=stress.yaml n=1000000 ops\\(insert=3,likelyquery1=1\\) cl=QUORUM -mode native cql3 user=cassandra password=datastax -node 104.196.140.126 -rate threads\\&gt;=121</code>\n<ul><li>I then captured the results into an Excel file</li>\n</ul></li>\n<li>Then I decided to really turn it up and do 1B operations; and it led to this test that was run once on each of the clients: <code>nohup cassandra-stress user profile=stress.yaml n=1000000000 ops\\(insert=3,likelyquery1=1\\) cl=QUORUM -mode native cql3 user=cassandra password=datastax -node 40.118.149.27 -rate threads=300 -log file=output.txt -errors ignore</code>\n<ul><li>Again, I captured the results in an Excel file</li>\n</ul></li>\n</ul><h2><a id=\"user-content-breakdown-the-switches\" class=\"anchor\" aria-hidden=\"true\" href=\"#breakdown-the-switches\"></a>Breakdown the switches!</h2>\n<p>I told you that this would be educational!</p>\n<h3><a id=\"user-content-first-test\" class=\"anchor\" aria-hidden=\"true\" href=\"#first-test\"></a>First test</h3>\n<p><code>cassandra-stress user profile=stress.yaml n=1000000 ops\\(insert=3,likelyquery1=1\\) cl=QUORUM -mode native cql3 user=cassandra password=datastax -node 104.196.140.126 -rate threads\\&gt;=121</code></p>\n<ul><li><code>profile=stress.yaml</code> --&gt; take a look at the one that I created in the root of this repo. I went to a colleague's website and was able to build it in a few minutes (source below). Do this to create the actual data model that you want to use in production!</li>\n<li><code>n=1000000</code> --&gt; the amount of times that you want to run operations</li>\n<li><code>ops\\(insert=3,likelyquery1=1\\)</code> --&gt; the ratio of writes:reads that you want done for your test. This is a 3:1 ratio. You can be specific about the reads that you want done. In <code>stress.yaml</code>, towards the bottom, there is a query called <code>likelyquery</code> that I want to run for my reads. You can customize this to read whatever query you prefer; super useful!</li>\n<li><code>cl=QUORUM</code> --&gt; consistency level. I see way too many stress tests done just at a consistency level of <code>CL=ONE</code>. The reality is that most don't run with a <code>CL=ONE</code> when they're in production; so why would they test against it now. Run your test against the consistency level that you want to run in production!</li>\n<li><code>-mode native cql3 user=cassandra password=datastax</code> --&gt; you need the client to be able to access DataStax Enterprise, so ensure it has the correct username and password to do so</li>\n<li><code>-node 104.196.140.126</code> --&gt; as I am running on a client, I need to be able to connect to the cluster. This IP address was of one of the seed nodes for the cluster that I was targeting. Once I point it towards one node, the client will become aware of all of the other nodes via gossip.</li>\n<li><code>-rate threads\\&gt;=121</code> --&gt; this runs the test at thread levels that are greater than or equal to 121. You can run at less than that, but I wanted to run at realistic levels.\n<ul><li>A great first test is to run at <code>-rate auto</code> which will will start at a single digit thread count and work its way up as high as it can go. This is great for understanding what is going on so you can understand the relationship for latency, operations/second, threadcount, etc.</li>\n</ul></li>\n</ul><h2><a id=\"user-content-second-test\" class=\"anchor\" aria-hidden=\"true\" href=\"#second-test\"></a>Second test</h2>\n<p>I am not going to go over the flags/arguments that I went over in the previous example. Instead, I'll go over the new ones or changes.</p>\n<p><code>nohup cassandra-stress user profile=stress.yaml n=1000000000 ops\\(insert=3,likelyquery1=1\\) cl=QUORUM -mode native cql3 user=cassandra password=datastax -node 40.118.149.27 -rate threads=300 -log file=output.txt -errors ignore</code></p>\n<ul><li><code>nohup</code> --&gt; I nohup'd because I wanted this to run as its own process. The advantage here is that it will run after I exit the session, computer goes to sleep, etc.</li>\n<li><code>n=1000000000</code> --&gt; increased the ops to 1B</li>\n<li><code>-rate threads=300</code> --&gt; set the thread count to an even 300. This is generally a point at which we start to see saturation, contention, etc.; so I wanted to stay in that range for an extended period of time</li>\n<li><code>-log file=output.txt</code> --&gt; as this would be a long running test, I wanted to be able to save the output all into a file that way I could <code>SCP</code> it back to my laptop for analysis in Excel. Otherwise it will just print to the screen; and after 1B transactions, that gets messy!</li>\n<li><code>-errors ignore</code> --&gt; after you run 1B transactions, you will get errors eventually. Cassandra handles them just fine, but you don't want it to stop the test. Do this for your tests! Otherwise, you could have your test stop half-way through; ask me how I know...</li>\n</ul><h2><a id=\"user-content-results\" class=\"anchor\" aria-hidden=\"true\" href=\"#results\"></a>Results</h2>\n<p>to be continued... maybe, if I can.</p>\n<ul><li>If you want to learn more about <code>cassandra-stress</code>, then look no further than the excellent work of my colleague: <a href=\"https://www.datastax.com/dev/blog/data-modeler\" rel=\"nofollow\">https://www.datastax.com/dev/blog/data-modeler</a></li>\n<li>The actual tool that builds out the <code>stress.yaml</code>: <a href=\"https://www.sestevez.com/sestevez/CassandraDataModeler/\" rel=\"nofollow\">https://www.sestevez.com/sestevez/CassandraDataModeler/</a></li>\n</ul>\n<p>Thanks for taking a look at this repo and let me know if you have any questions.</p>\n</article>",
        "created_at": "2018-11-13T20:12:27+0000",
        "updated_at": "2018-11-13T20:12:50+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 5,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/16208950?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12722"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          },
          {
            "id": 1385,
            "label": "dse",
            "slug": "dse"
          }
        ],
        "is_public": false,
        "id": 12719,
        "uid": null,
        "title": "phact/dse-cluster-migration",
        "url": "https://github.com/phact/dse-cluster-migration",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>NOTE: If you are looking to perform a migration across two clusters, make sure\nyou deploy two clusters!</p>\n<p>This is a Demo for Migrating DSE and cassandra clusters (or even tables within the same cluster) using DSE Analytics / Spark.</p>\n<h3><a id=\"user-content-motivation\" class=\"anchor\" aria-hidden=\"true\" href=\"#motivation\"></a>Motivation</h3>\n<p>Moving data across clusters for one time migrations or bulk migrations are relatively common. DSE Analytics Makes this process almost trivial for users that are well versed in Spark. In a <a href=\"http://www.sestevez.com/cluster-migration-keeping-simple-things-simple/\" rel=\"nofollow\">previous blog post</a> I attempted to make this process simpler for users with minimal spark experience. This asset aims to make this process easy even for users that with no spark experience at all.</p>\n<h3><a id=\"user-content-what-is-included\" class=\"anchor\" aria-hidden=\"true\" href=\"#what-is-included\"></a>What is included?</h3>\n<p>This field asset (demo) includes the following:</p>\n<ul><li>dse-cluster-migration Spark Job</li>\n<li>Performs migration using massively parallel Spark compute</li>\n<li>Code is visible in the Che web IDE which ships with this asset</li>\n<li>MigUI (Migration UI) web appication</li>\n<li>React Redux frontend</li>\n<li>Dropwizard backend</li>\n<li>Uses webhdfs to programatically upload the spark job jar to DSEFS</li>\n<li>Uses the DSE only CQL Spark interface (currently an internal undocumented API) to submit the spark job to DSE</li>\n</ul><h3><a id=\"user-content-business-take-aways\" class=\"anchor\" aria-hidden=\"true\" href=\"#business-take-aways\"></a>Business Take Aways</h3>\n<p>DSE is the best distribution of Apache Cassandra and the easiest to use. By taking advantage of the migration capabilities in DSE analytics, projects can get off the ground faster and complex business requirements have a shorter time to Value.</p>\n<h3><a id=\"user-content-technical-take-aways\" class=\"anchor\" aria-hidden=\"true\" href=\"#technical-take-aways\"></a>Technical Take Aways</h3>\n<p>In some DSE to DSE or c* to DSE scenarios, there are a few cases in which a cluster migration is easier to perform than an upgrade. When the source cluster needs to remain place (i.e. data migrations accross environments, DEV &lt;-&gt; SIT &lt;-&gt; UAT &lt;-&gt; PROD) DSE Analytics can be the right solution.</p>\n<p>Look at this asset if you are interested in:\nUsing Spark to migrate data from one cluster to another\nUsing Spark to move a table to another table in the same cluster\nProgramatically writing to DSEFS from Java\nProgramatically using CQL to kick off DSE Analytics jobs (NOTE this is unsupported and undocumented at this time)</p>\n</article>",
        "created_at": "2018-11-13T18:41:41+0000",
        "updated_at": "2018-11-13T18:41:50+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://avatars3.githubusercontent.com/u/1313220?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12719"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 217,
            "label": "tool",
            "slug": "tool"
          },
          {
            "id": 1233,
            "label": "data.modeling",
            "slug": "data-modeling"
          }
        ],
        "is_public": false,
        "id": 12716,
        "uid": null,
        "title": "CQL Data Modeler",
        "url": "https://www.sestevez.com/sestevez/CassandraDataModeler/",
        "content": "<p>This tool is meant to improve your experience with Apache Cassandra.\n                Provide some information about your table definition, and the size, population, and cluster distributions of each of your fields and the tool will give you a script you can use to benchmark your data model and a storage engine visualization.\n                Please input your table definition for one table, keeping the Create statement, field definitions, and Primary Key definition on separate lines.</p><textarea id=\"tableDef\" placeholder=\"Create Table Statement\" rows=\"10\" cols=\"50\">\nCREATE TABLE reviews_by_day (\n    userid text,\n    day int,\n    productid text,\n    reviewid uuid,\n    profilename text,\n    helpfulness text,\n    score text,\n    summary text,\n    review text,\n    time timestamp,\n    PRIMARY KEY (userid, productid, time, reviewid))\n                </textarea><a href=\"https://www.sestevez.com/sestevez/CassandraDataModeler/\" class=\"ui-btn\" id=\"submit-button\">Verify Table</a>",
        "created_at": "2018-11-13T18:31:33+0000",
        "updated_at": "2018-11-13T18:31:44+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.sestevez.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12716"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 12,
            "label": "video",
            "slug": "video"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12713,
        "uid": null,
        "title": "Apache Cassandra Best Practices - Live Webinar",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=dhu9qzetnY8",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/dhu9qzetnY8?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-11-11T19:58:06+0000",
        "updated_at": "2018-11-11T19:58:06+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/dhu9qzetnY8/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12713"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 12,
            "label": "video",
            "slug": "video"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 568,
            "label": "azure",
            "slug": "azure"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          },
          {
            "id": 994,
            "label": "dynamo",
            "slug": "dynamo"
          },
          {
            "id": 1298,
            "label": "google.cloud",
            "slug": "google-cloud"
          },
          {
            "id": 1385,
            "label": "dse",
            "slug": "dse"
          }
        ],
        "is_public": false,
        "id": 12712,
        "uid": null,
        "title": "Apache Cassandra vs Other Cloud Databases | DataStax",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=G8xHwnCevYo",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/G8xHwnCevYo?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-11-11T19:58:04+0000",
        "updated_at": "2018-11-11T19:58:04+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/G8xHwnCevYo/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12712"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 12,
            "label": "video",
            "slug": "video"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12709,
        "uid": null,
        "title": "Cassandra with Tim Berglund",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=ifJK1jMEXh0",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/ifJK1jMEXh0?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-11-09T20:24:22+0000",
        "updated_at": "2018-11-09T20:24:27+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/ifJK1jMEXh0/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12709"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 12,
            "label": "video",
            "slug": "video"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 568,
            "label": "azure",
            "slug": "azure"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          },
          {
            "id": 994,
            "label": "dynamo",
            "slug": "dynamo"
          },
          {
            "id": 1298,
            "label": "google.cloud",
            "slug": "google-cloud"
          },
          {
            "id": 1420,
            "label": "spanner",
            "slug": "spanner"
          },
          {
            "id": 1421,
            "label": "strata",
            "slug": "strata"
          }
        ],
        "is_public": false,
        "id": 12708,
        "uid": null,
        "title": "Apache Cassandra vs Other Cloud Databases | DataStax",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=G8xHwnCevYo",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/G8xHwnCevYo?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-11-09T20:21:28+0000",
        "updated_at": "2018-11-09T20:21:56+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/G8xHwnCevYo/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12708"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 568,
            "label": "azure",
            "slug": "azure"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          },
          {
            "id": 994,
            "label": "dynamo",
            "slug": "dynamo"
          },
          {
            "id": 1017,
            "label": "cosmos",
            "slug": "cosmos"
          },
          {
            "id": 1420,
            "label": "spanner",
            "slug": "spanner"
          }
        ],
        "is_public": false,
        "id": 12707,
        "uid": null,
        "title": "Cassandra%20versus%20cloud%20databases%20Presentation.pdf",
        "url": "https://cdn.oreillystatic.com/en/assets/1/event/278/Cassandra%20versus%20cloud%20databases%20Presentation.pdf",
        "content": null,
        "created_at": "2018-11-09T20:19:13+0000",
        "updated_at": "2018-11-09T20:19:27+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": null,
        "language": null,
        "reading_time": 0,
        "domain_name": "cdn.oreillystatic.com",
        "preview_picture": null,
        "http_status": null,
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12707"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 11,
            "label": "database",
            "slug": "database"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 894,
            "label": "distributed",
            "slug": "distributed"
          }
        ],
        "is_public": false,
        "id": 12706,
        "uid": null,
        "title": "scalar-labs/scalardb",
        "url": "https://github.com/scalar-labs/scalardb",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\"><h2><a id=\"user-content-scalar-db\" class=\"anchor\" aria-hidden=\"true\" href=\"#scalar-db\"></a>Scalar DB</h2>\n<p><a href=\"https://circleci.com/gh/scalar-labs/scalardb/tree/master\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/684963021e0f453f74ed8e49e66799d1e938248b/68747470733a2f2f636972636c6563692e636f6d2f67682f7363616c61722d6c6162732f7363616c617264622f747265652f6d61737465722e7376673f7374796c653d73766726636972636c652d746f6b656e3d36373266373063653766326334663864396537316637633964623861653832346532636661656361\" alt=\"CircleCI\" data-canonical-src=\"https://circleci.com/gh/scalar-labs/scalardb/tree/master.svg?style=svg&amp;circle-token=672f70ce7f2c4f8d9e71f7c9db8ae824e2cfaeca\" /></a></p>\n<p>A library that provides a distributed storage abstraction and client-coordinated distributed transaction manager on the storage, and makes non-ACID distributed database/storage ACID-compliant.</p>\n<h2><a id=\"user-content-docs\" class=\"anchor\" aria-hidden=\"true\" href=\"#docs\"></a>Docs</h2>\n<ul><li><a href=\"https://github.com/scalar-labs/scalardb/blob/master/docs/getting-started.md\">Getting started</a></li>\n<li><a href=\"https://github.com/scalar-labs/scalardb/blob/master/docs/design.md\">Design document</a></li>\n<li><a href=\"https://scalar-labs.github.io/scalardb/javadoc/\" rel=\"nofollow\">Javadoc</a></li>\n<li>Sample applications by 3rd party developers (coming soon)</li>\n</ul><h2><a id=\"user-content-contributing\" class=\"anchor\" aria-hidden=\"true\" href=\"#contributing\"></a>Contributing</h2>\n<p>This library is mainly maintained by the Scalar Engineering Team, but of course we appreciate any help.</p>\n<ul><li>For asking questions, finding answers and helping other users, please use the public mailing list <a href=\"mailto:scalardb-user@googlegroups.com\">scalardb-user@googlegroups.com</a> or go to <a href=\"https://groups.google.com/forum/#!forum/scalardb-user\" rel=\"nofollow\">https://groups.google.com/forum/#!forum/scalardb-user</a>.</li>\n<li>For filing bugs, suggesting improvements, or requesting new features, help us out by opening an issue.</li>\n</ul><h2><a id=\"user-content-license\" class=\"anchor\" aria-hidden=\"true\" href=\"#license\"></a>License</h2>\n<p>Scalar DB is dual-licensed under both the AGPL (found in the LICENSE file in the root directory) and commercial license. You may select, at your option, one of the above-listed licenses. Regarding the commercial license, <a href=\"https://scalar-labs.com/contact_us/\" rel=\"nofollow\">contact us</a> for more information.</p>\n</article>",
        "created_at": "2018-11-09T14:49:44+0000",
        "updated_at": "2018-11-09T14:49:55+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/40349669?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12706"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12528,
        "uid": null,
        "title": "apache-cassandra",
        "url": "https://discovery.hgdata.com/product/apache-cassandra",
        "content": null,
        "created_at": "2018-10-30T18:46:18+0000",
        "updated_at": "2018-10-30T20:19:24+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": null,
        "language": null,
        "reading_time": 0,
        "domain_name": "discovery.hgdata.com",
        "preview_picture": null,
        "http_status": null,
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12528"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1347,
            "label": "architecture",
            "slug": "architecture"
          }
        ],
        "is_public": false,
        "id": 12466,
        "uid": null,
        "title": "Cassandra Architecture and Write Path Anatomy | Jorge Acetozi - Books, Courses and Notes",
        "url": "https://www.jorgeacetozi.com/single-post/cassandra-architecture-and-write-path-anatomy?_escaped_fragment_=",
        "content": "<img itemprop=\"image\" width=\"570.35\" title=\"Cassandra\" src=\"https://static.wixstatic.com/media/fcd55e_3923bbb56b00476692f2bb04af444cee~mv2.png/v1/fill/w_570,h_382/fcd55e_3923bbb56b00476692f2bb04af444cee~mv2.png\" height=\"382.2666666666667\" alt=\"Cassandra\" /><p>Cassandra is a NoSQL database that belongs to the Column Family NoSQL database category. It's an\n                        \n                            <a target=\"_blank\" href=\"http://cassandra.apache.org/\" shape=\"rect\">\n                                Apache project\n                            </a>\n                        \n                        and it has an Enterprise version maintained by\n                        \n                            <a target=\"_blank\" href=\"https://www.datastax.com/\" shape=\"rect\">\n                                DataStax\n                            </a>\n                        \n                        . Cassandra is written in Java and it's mainly used for time-series data such as metrics, IoT (Internet of Things), logs, chat messages, and so on. It is able to handle a massive amount of writes and reads and scale to thousands of nodes. Let's list out here some important Cassandra characteristics. Basically, Cassandra...</p><ul class=\"font_8\"><li>\n                        <p>mix ideas from Google's Big Table and Amazon's Dynamo.</p>\n                    </li>\n                    <li>\n                        <p>is based on\n                                \n                                    <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Peer-to-peer\" shape=\"rect\">\n                                        peer-to-peer\n                                    </a>\n                                \n                                architecture. Every node is equal (can perform both reads and writes), therefore there is no master or slave node, that is, there is no master single point of failure.</p>\n                    </li>\n                    <li>\n                        <p>does automatic partitioning and replication.</p>\n                    </li>\n                    <li>\n                        <p>has tunable write and read consistency for both read and write operations.</p>\n                    </li>\n                    <li>\n                        <p>is able to horizontally scale keeping\n                                \n                                    <a target=\"_blank\" href=\"https://medium.com/netflix-techblog/benchmarking-cassandra-scalability-on-aws-over-a-million-writes-per-second-39f45f066c9e\" shape=\"rect\">\n                                        linear scalability\n                                    </a>\n                                \n                                for both reads and writes.</p>\n                    </li>\n                    <li>\n                        <p>handles inter-node communication through the\n                                \n                                    <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Gossip_protocol\" shape=\"rect\">\n                                        Gossip protocol\n                                    </a>\n                                \n                                .</p>\n                    </li>\n                    <li>\n                        <p>handles client communication through the CQL (Cassandra Query Language), which is very similar to SQL.</p>\n                    </li>\n                </ul><p>Coordinator</p><p>When a request is sent to\n                        any\n                        Cassandra node, this node acts as a proxy between the application (actually, the Cassandra driver) and the nodes involved in the request flow. This proxy node is called as the\n                        coordinator\n                        . The coordinator is responsible for managing the entire request path and to respond back to the client.</p><div><img width=\"738.65\" title=\"Coordinator\" src=\"https://static.wixstatic.com/media/fcd55e_d58f6f89b5bd47478c819e946e7928b3~mv2.png/v1/fill/w_739,h_416/fcd55e_d58f6f89b5bd47478c819e946e7928b3~mv2.png\" height=\"415.7375\" alt=\"Coordinator\" /></div><p>Figure 1 - Coordinator</p><p>Besides, sometimes when the coordinator forwards a write request to the replica nodes, they may happen to be unavailable at this very moment. In this case, the coordinator plays an important role implementing a mechanism called Hinted Handoff, which will be described in details later.</p><p>Partitioner</p><p>Basically, for each node in the Cassandra cluster (Cassandra ring) is assigned a range of tokens as shown in Figure 2 for a 6-node cluster (with imaginary tokens, of course).</p><div><img width=\"589.05\" title=\"Token Ranges\" src=\"https://static.wixstatic.com/media/fcd55e_c3532c858e644de0a3cc9dbb718ed5a6~mv2.png/v1/fill/w_589,h_460/fcd55e_c3532c858e644de0a3cc9dbb718ed5a6~mv2.png\" height=\"459.8723684210526\" alt=\"\" /></div><p>Figure 2 - Token Ranges</p><p>Cassandra distributes data across the cluster using a\n                        \n                            <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Consistent_hashing\" shape=\"rect\">\n                                Consistent Hashing\n                            </a>\n                        \n                        algorithm and, starting from version 1.2, it also implements the concept of\n                        virtual nodes\n                        (vnodes), where each node owns a large number of small token ranges in order to improve token reorganization and avoid hotspots in the cluster, that is, some nodes storing much more data than the others. Virtual nodes also allows to add and remove nodes in the cluster more easily and manages the token assignment automatically for you so that you can enjoy a nice coffee when adding or removing a node instead of calculating and assigning new token ranges for each node (which is a very error prone operation, by the way). </p><p><br style=\"clear: none;\" />\n                    \n                        Well, that said, the partitioner is the component responsible for determining how to distribute the data across the nodes in the cluster given the\n                        \n                            <a target=\"_blank\" href=\"https://dzone.com/articles/cassandra-data-modeling-primary-clustering-partiti\" shape=\"rect\">\n                                partition key\n                            </a>\n                        \n                         of a row. Basically, it is a hash function for computing a token given the partition key.</p><p>Once the partitioner applies the hash function to the partition key and gets the token, it knows exactly which node is going to handle the request.</p><p>Let's consider a simple example: suppose a request is issued to node6 (that is, node6 is the coordinator for this request) with a row containing the partition key \"jorge_acetozi\". Suppose the partitioner applies the hash function to the partition key \"jorge_acetozi\" and gets the token -17. As figure 3 shows, node2 token ranges includes -17, so this node will be the one handling the request.</p><div><img width=\"897.6\" title=\"Partitioner\" src=\"https://static.wixstatic.com/media/fcd55e_11ca42aad78744a48759dd9908cdbef5~mv2.png/v1/fill/w_898,h_485/fcd55e_11ca42aad78744a48759dd9908cdbef5~mv2.png\" height=\"485.1891891891892\" alt=\"Partitioner\" /></div><p>Figure 3 - Partitioner</p><p>Cassandra offers three types of partitioners:\n                        \n                            <a target=\"_blank\" href=\"https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/dht/Murmur3Partitioner.java\" shape=\"rect\">\n                                Murmur3Partitioner\n                            </a>\n                        \n                        (which is the default),\n                        \n                            <a target=\"_blank\" href=\"https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/dht/RandomPartitioner.java\" shape=\"rect\">\n                                RandomPartitioner\n                            </a>\n                        \n                        , and\n                        \n                            <a target=\"_blank\" href=\"https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/dht/ByteOrderedPartitioner.java\" shape=\"rect\">\n                                ByteOrderedPartitioner\n                            </a>\n                        \n                        .</p><p>Replication</p><p>Life would be much easier if...</p><ul class=\"font_8\"><li>\n                        <p>Nodes never fail</p>\n                    </li>\n                    <li>\n                        <p>Networks had no latency</p>\n                    </li>\n                    <li>\n                        <p>People did not stumble on cables</p>\n                    </li>\n                    <li>\n                        <p>Amazon did not restart your instances</p>\n                    </li>\n                    <li>\n                        <p>Full GC meant \"Full Guitar Concert\"</p>\n                    </li>\n                </ul><p>And so on. Unfortunately, these things happen all the time and you already chose a software engineer career (your mother used to advise you to study hard and to become a doctor, but you chose to keep playing Counter-Strike instead. Now you are a software engineer, know what AK-47 means and have to care about stuff like that).</p><p>Fortunately, Cassandra offers automatic data replication and keeps your data redundant throughout different nodes in the cluster. This means that (in certain levels) you can even resist to node failure scenarios and your data would still be safe and available. But everything comes at a price, and the price of replication is consistency.</p><p>Replication Strategy</p><p>Basically, the coordinator uses the Replication Strategy to find out which nodes will be the replica nodes for a given request.</p><p>There are two replication strategies available:</p><p><a target=\"_blank\" href=\"https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/locator/SimpleStrategy.java\" shape=\"rect\">\n                                SimpleStrategy\n                            </a>\n                        \n                        : used for a single data center deployment (not recommended for production environment). It doesn't consider the network topology. Basically, it just takes the partitioner's decision (that is, the node that will handle the request first based on the token range) and places the remaining replicas clockwise in relation to this node. For example, in Figure 3, if the table replication factor was 3, which nodes would have been chosen by the SimpleStrategy to act as replicas (besides node2, which was already chosen by the partitioner)? That's correct, node3 and node4! What if the replication factor was 4? Well, then node5 would also be included.</p><p><a target=\"_blank\" href=\"https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/locator/NetworkTopologyStrategy.java\" shape=\"rect\">\n                                NetworkTopologyStrategy\n                            </a>\n                        \n                        : used for multiple data centers deployment (recommended for production environment). It also takes the partitioner's decision and places the remaining replicas clockwise, but it also takes into consideration the rack and data centers configuration.</p><p>Replication Factor</p><p>When you create a table (Column Family) in Cassandra, you specify the replication factor. The replication factor is the number of replicas that Cassandra will hold for this table in different nodes. If you specify REPLICATION_FACTOR=3, then your data will be replicated to 3 different nodes throughout the cluster. That provides fault tolerance and resilience because even if some nodes fail your data would still be safe and available.</p><p>Write Consistency Level</p><p>Do you still remember that when the client sends a request to a Cassandra node, this node is called a coordinator and acts as a proxy between the client and the replica nodes?</p><p>Well, when you write to a table in Cassandra (inserting data, for example), you can specify the write consistency level. The write consistency level is the number of replica nodes that have to acknowledge the coordinator that its local insert was successful (success here means that the data was appended to the commit log and written to the memtable). As soon as the coordinator gets WRITE_CONSISTENCY_LEVEL success acknowledgments from the replica nodes, it returns success back to the client and doesn't wait for the remaining replicas to acknowledge success.</p><p>For example, if an application issue an insert request with WRITE_CONSISTENCY_LEVEL=TWO to a table that is configured with REPLICATION_FACTOR=3, the coordinator will only return success to the application when two of the three replicas acknowledge success. Of course, this doesn't mean that the third replica will not write the data too; it will, but at this point, the coordinator would already have sent success back to the client.</p><p>There are many different types of write consistency levels you can specify in your write request. From the less consistent to full consistency: ANY, ONE, TWO, THREE, QUORUM, LOCAL_QUORUM, EACH_QUORUM, ALL. </p><p>Write Flow Example</p><p>For simplicity, suppose a write request is issued to a 6-node Cassandra cluster with the following characteristics:</p><p>WRITE_CONSISTENCY_LEVEL=TWO\n                            <br style=\"clear: none;\" />\n                            TABLE_REPLICATION_FACTOR=3\n                            <br style=\"clear: none;\" />\n                            REPLICATION_STRATEGY=SimpleStrategy\n                        \n                    \n                    <br style=\"clear: none;\" />\n                     </p><p>First, the client sends the write request to the Cassandra cluster using the driver. We haven't discussed the role of the driver in this post (maybe in another post), but it plays a very important role as well. The driver is responsible for a lot of features such as asynchronous IO, parallel execution, request pipelining, connection pooling, auto node discovery, automatic reconnection, token awareness, and so on. For example, by using a driver that implements a token-aware policy, the driver reduces network hops by sending requests directly to the node that owns the data instead of sending it to a \"random\" coordinator.</p><p>As soon as the coordinator gets the write request, it applies the partitioner hash function to the partition key\n                        and\n                        uses the configured Replication Strategy in order to determine the TABLE_REPLICATION_FACTOR replica nodes that will actually write the data (in this sentence, replace TABLE_REPLICATION_FACTOR with the number 3). Figure 4 shows the replica nodes (in green) that will handle the write request.</p><div><img width=\"897.6\" title=\"Replica Nodes\" src=\"https://static.wixstatic.com/media/fcd55e_e2d1c28b9b494dea9c6372622f09dd19~mv2.png/v1/fill/w_898,h_485/fcd55e_e2d1c28b9b494dea9c6372622f09dd19~mv2.png\" height=\"485.1891891891892\" alt=\"Replica Nodes\" /></div><p>Figure 4 - Replica Nodes</p><p>Now, before the coordinator forwards the write request to all the 3 replica nodes, it will ask to the\n                        \n                            <a target=\"_blank\" href=\"https://github.com/jorgeacetozi/cassandra/blob/trunk/src/java/org/apache/cassandra/gms/FailureDetector.java\" shape=\"rect\">\n                                Failure Detector component\n                            </a>\n                        \n                        how many of these replica nodes are actually available and compare it to the WRITE_CONSISTENCY_LEVEL provided in the request. If the number of replica nodes available is less than the WRITE_CONSISTENCY_LEVEL provided, the Failure Detector will immediately throw an Exception.</p><p>For our example, suppose the 3 replica nodes are available (that is, the Failure Detector will allow the request to continue) such as shown in Figure 5. Now, the coordinator will asynchronously forward the write request to\n                        all\n                        the replica nodes (in these case, the 3 replica nodes that were figured in the first step). As soon as WRITE_CONSISTENCY_LEVEL replica nodes acknowledge success (node2 and node4), the coordinator returns success back to the driver.</p><div><img width=\"897.6\" title=\"Write Success\" src=\"https://static.wixstatic.com/media/fcd55e_94c355b7090f4a85b15f5ecc50c499f2~mv2.png/v1/fill/w_898,h_485/fcd55e_94c355b7090f4a85b15f5ecc50c499f2~mv2.png\" height=\"485.1891891891892\" alt=\"Write Success\" /></div><p>Figure 5 - Write Success</p><p>If the WRITE_CONSISTENCY_LEVEL for this request was THREE (or ALL), the coordinator would have to wait until node3 acknowledges success too, and of course that this write request would be slower.</p><p>So, basically...</p><ul class=\"font_8\"><li>\n                        <p>Do you need fault tolerance and high availability? Use replication.</p>\n                    </li>\n                    <li>\n                        <p>Just keep in mind that using replication means you will pay with consistency (for most of the cases, this is not a problem. Availability is often more important than consistency).</p>\n                    </li>\n                    <li>\n                        <p>If consistency is not an issue for your domain, perfect. If it is, just increase the consistency level, but then you will pay with higher latency.</p>\n                    </li>\n                    <li>\n                        <p>If you want fault tolerance and high availability, strong consistency and low latency, then you should be the client, not the software engineer (Lol).</p>\n                    </li>\n                </ul><p>Hinted Handoff</p><p>Suppose in the last example that only 2 of 3 replica nodes were available. In this case, the Failure Detector would still allow the request to continue as the number of available replica nodes is not less than the WRITE_CONSISTENCY_LEVEL provided. In this case, the coordinator would behave exactly as described before but there would be one additional step. The coordinator would write locally the hint (the write request blob along with some metadata) in the disk (hints\n                         \n                        directory) and would keep the hint there for 3 hours (by default) waiting for the replica node to become available again. If the replica node recovers within this period, the coordinator will send the hint to the replica node so that it can update itself and become consistent with the other replicas. If the replica node is offline for more than 3 hours, then a read repair is needed. This process is referred as Hinted Handoff.\n                    \n                    <br style=\"clear: none;\" />\n                    \n                        \n                            Write Internals</p><p>In short, when a write request reaches a node, mainly two things happen:</p><ol class=\"font_8\"><li>\n                        <p>The write request is appended to the commit log in the disk. This ensures data durability (the write request data would permanently survive even in a node failure scenario)</p>\n                    </li>\n                    <li>\n                        <p>The write request is sent to the memtable (a structure stored in the memory). When the memtable is full, the data is flushed to a SSTable on disk using sequential I/O and the data in the commit log is purged.</p>\n                    </li>\n                </ol><div><img width=\"570.35\" title=\"Cassandra Node Internals\" src=\"https://static.wixstatic.com/media/fcd55e_7db3565284b1438b90ab7f3459df375b~mv2_d_1717_1371_s_2.png/v1/fill/w_570,h_456/fcd55e_7db3565284b1438b90ab7f3459df375b~mv2_d_1717_1371_s_2.png\" height=\"456.4833333333334\" alt=\"\" /></div><p>Figure 6 - Cassandra Node Internals</p><p>I really hope this article has been useful to you. If you enjoyed reading it, let me know if you would like to read another article diving into the Read Request Path. The Read Request Path is a little bit more complicated as it involves Snitches, Bloom Filter, Indexes, and so on, but it's pretty interesting as well.</p><p>If you are a software developer interested in how to use Cassandra in a realistic scenario coding a real-time chat application from the scratch, please take a look at my book: \n                        \n                            <a target=\"_blank\" href=\"https://www.jorgeacetozi.com/books\" shape=\"rect\">\n                                Pro Java Clustering and Scalability: Building Real-Time Apps with Spring, Cassandra, Redis, WebSocket and RabbitMQ\n                            </a></p><p>Thank you very much.</p>",
        "created_at": "2018-10-24T18:11:57+0000",
        "updated_at": "2018-10-24T18:12:10+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 10,
        "domain_name": "www.jorgeacetozi.com",
        "preview_picture": "https://static.wixstatic.com/media/fcd55e_3923bbb56b00476692f2bb04af444cee%7Emv2.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12466"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 901,
            "label": "security",
            "slug": "security"
          },
          {
            "id": 1303,
            "label": "ssl",
            "slug": "ssl"
          }
        ],
        "is_public": false,
        "id": 12465,
        "uid": null,
        "title": "cloudurable/cassandra-image",
        "url": "https://github.com/cloudurable/cassandra-image/wiki/Cassandra-Tutorial-2:-Setting-up-client-and-cluster-SSL-transport-in-Cassandra",
        "content": "<div class=\"markdown-body\">\n          <p>Cassandra allows you to secure the <code>client transport</code> (CQL) as well as the cluster transport (storage transport).</p>\n<p>SSL/TLS have some overhead. This is especially true in the JVM world which is not as performant for handling SSL/TLS unless you are using Netty/OpenSSl integration.</p>\n<p>If possible, use no encryption for the <code>cluster transport</code> (<code>storage transport</code>), and deploy your Cassandra nodes in a private subnet, and limit access to this subnet to the <code>client transport</code>. Also if possible avoid using TLS/SSL on the client transport and do client operations from your app tier, which is located in a non-public subnet.</p>\n<p>However, that is not always possible. You may work in an industry that requires the use of encrypted transports like the U.S. Health Insurance Portability and Accountability Act (HIPAA), Germany’s Federal Data Protection Act,\nThe Payment Card Industry Data Security Standard (PCI DSS), or U.S. Sarbanes-Oxley Act of 2002. Or you might work for a bank or other financial institution. Or it just might be a corporate policy to encrypt such transports.</p>\n<p>Another area of concern is for compliance is authorization, and encrypted data at rest. Cassandra’s has essential security features: authentication, role-based authorization, transport encryption (JMX, client transport, cluster transport), as well as data at rest encryption (encrypting SSTables).</p>\n<p>This article will focus just on setting up encryption for the Cassandra <code>client transport</code> (CQL) and the <code>cluster transport</code>. Later articles will cover various aspects of compliance and encryption.</p>\n<h4>\n<a id=\"user-content-encrypting-the-transports\" class=\"anchor\" href=\"#encrypting-the-transports\" aria-hidden=\"true\"></a>Encrypting the transports</h4>\n<p>Data that travels over the <code>client transport</code> across a network could be accessed by someone you don't want accessing said data with tools like <a href=\"https://www.wireshark.org/\" rel=\"nofollow\">wire shark</a>. If data includes private information, SSN number, credentials (password, username), credit card numbers or account numbers, then we want to make that data unintelligible (encrypted) to any and all 3rd parties. This is especially important if we don't control the network. You can also use TLS to make sure the data has not been tampered with whilst traveling the network. The Secure Sockets Layer (SSL) and Transport Layer Security (TLS) protocols are designed to provide these features (SSL is the old name for what became TLS but many people still refer to TLS as SSL).</p>\n<p>Cassandra is written in Java. Java defines the JSSE framework which in turn uses the <a href=\"http://docs.oracle.com/javase/8/docs/technotes/guides/security/crypto/CryptoSpec.html#Design\" rel=\"nofollow\">Java Cryptography Architecture (JCA)</a>. JSSE uses cryptographic service providers from JCA.</p>\n<p>If any of the above is new to you, please take a few minutes to read through the [TLS/SSL Java guide] (<a href=\"http://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/JSSERefGuide.html\" rel=\"nofollow\">http://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/JSSERefGuide.html</a>).</p>\n<h2>\n<a id=\"user-content-setting-up-client-config\" class=\"anchor\" href=\"#setting-up-client-config\" aria-hidden=\"true\"></a>Setting up client config</h2>\n<p>The client transport encryption protects data as it moves from clients to server nodes in the cluster.</p>\n<p>The <code>client_encryption_options</code> are stored in the cassandra.yaml.\nHere is an example config.</p>\n<h4>\n<a id=\"user-content-sample-config\" class=\"anchor\" href=\"#sample-config\" aria-hidden=\"true\"></a>Sample config</h4>\n<pre>\n# enable or disable client/server encryption.\nclient_encryption_options:\n    enabled: false\n    # If enabled and optional is set to true encrypted and unencrypted connections are handled.\n    optional: false\n    keystore: conf/.keystore\n    keystore_password: cassandra\n    require_client_auth: false\n    # Set trustore and truststore_password if require_client_auth is true\n    # truststore: conf/.truststore\n    # truststore_password: cassandra\n    protocol: TLS\n    algorithm: SunX509\n    store_type: JKS\n    cipher_suites: [TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDH_anon_WITH_AES_256_CBC_SHA]\n</pre>\n<h2>\n<a id=\"user-content-setup-keys\" class=\"anchor\" href=\"#setup-keys\" aria-hidden=\"true\"></a>Setup keys</h2>\n<p>Before we go into the details of setting up the cassandra.yaml file, let's create some trust stores, key stores, and export some keys. The following script generates cluster and client keys.</p>\n<h4>\n<a id=\"user-content-setupkeys-cassandra-securitysh\" class=\"anchor\" href=\"#setupkeys-cassandra-securitysh\" aria-hidden=\"true\"></a>setupkeys-cassandra-security.sh</h4>\n<div class=\"highlight highlight-source-shell\"><pre>#!/bin/bash\nKEY_STORE_PATH=\"$PWD/resources/opt/cassandra/conf/certs\"\nmkdir -p \"$KEY_STORE_PATH\"\nKEY_STORE=\"$KEY_STORE_PATH/cassandra.keystore\"\nPKS_KEY_STORE=\"$KEY_STORE_PATH/cassandra.pks12.keystore\"\nTRUST_STORE=\"$KEY_STORE_PATH/cassandra.truststore\"\nPASSWORD=cassandra\nCLUSTER_NAME=test\nCLUSTER_PUBLIC_CERT=\"$KEY_STORE_PATH/CLUSTER_${CLUSTER_NAME}_PUBLIC.cer\"\nCLIENT_PUBLIC_CERT=\"$KEY_STORE_PATH/CLIENT_${CLUSTER_NAME}_PUBLIC.cer\"\n### Cluster key setup.\n# Create the cluster key for cluster communication.\nkeytool -genkey -keyalg RSA -alias \"${CLUSTER_NAME}_CLUSTER\" -keystore \"$KEY_STORE\" -storepass \"$PASSWORD\" -keypass \"$PASSWORD\" \\\n-dname \"CN=CloudDurable Image $CLUSTER_NAME cluster, OU=Cloudurable, O=Cloudurable, L=San Francisco, ST=CA, C=USA, DC=cloudurable, DC=com\" \\\n-validity 36500\n# Create the public key for the cluster which is used to identify nodes.\nkeytool -export -alias \"${CLUSTER_NAME}_CLUSTER\" -file \"$CLUSTER_PUBLIC_CERT\" -keystore \"$KEY_STORE\" \\\n-storepass \"$PASSWORD\" -keypass \"$PASSWORD\" -noprompt\n# Import the identity of the cluster public cluster key into the trust store so that nodes can identify each other.\nkeytool -import -v -trustcacerts -alias \"${CLUSTER_NAME}_CLUSTER\" -file \"$CLUSTER_PUBLIC_CERT\" -keystore \"$TRUST_STORE\" \\\n-storepass \"$PASSWORD\" -keypass \"$PASSWORD\" -noprompt\n### Client key setup.\n# Create the client key for CQL.\nkeytool -genkey -keyalg RSA -alias \"${CLUSTER_NAME}_CLIENT\" -keystore \"$KEY_STORE\" -storepass \"$PASSWORD\" -keypass \"$PASSWORD\" \\\n-dname \"CN=CloudDurable Image $CLUSTER_NAME client, OU=Cloudurable, O=Cloudurable, L=San Francisco, ST=CA, C=USA, DC=cloudurable, DC=com\" \\\n-validity 36500\n# Create the public key for the client to identify itself.\nkeytool -export -alias \"${CLUSTER_NAME}_CLIENT\" -file \"$CLIENT_PUBLIC_CERT\" -keystore \"$KEY_STORE\" \\\n-storepass \"$PASSWORD\" -keypass \"$PASSWORD\" -noprompt\n# Import the identity of the client pub  key into the trust store so nodes can identify this client.\nkeytool -importcert -v -trustcacerts -alias \"${CLUSTER_NAME}_CLIENT\" -file \"$CLIENT_PUBLIC_CERT\" -keystore \"$TRUST_STORE\" \\\n-storepass \"$PASSWORD\" -keypass \"$PASSWORD\" -noprompt\nkeytool -importkeystore -srckeystore \"$KEY_STORE\" -destkeystore \"$PKS_KEY_STORE\" -deststoretype PKCS12 \\\n-srcstorepass \"$PASSWORD\" -deststorepass \"$PASSWORD\"\nopenssl pkcs12 -in \"$PKS_KEY_STORE\" -nokeys -out \"${CLUSTER_NAME}_CLIENT.cer.pem\" -passin pass:cassandra\nopenssl pkcs12 -in \"$PKS_KEY_STORE\" -nodes -nocerts -out \"${CLUSTER_NAME}_CLIENT.key.pem\" -passin pass:cassandra\n</pre></div>\n<p>The <a href=\"https://docs.oracle.com/javase/8/docs/technotes/tools/unix/keytool.html\" rel=\"nofollow\">keytool</a> utility ships with Java SDK. We use this keytool command to create the cluster key. Let's break down the script that generates the keys and certificates.</p>\n<h4>\n<a id=\"user-content-create-the-cluster-key\" class=\"anchor\" href=\"#create-the-cluster-key\" aria-hidden=\"true\"></a>Create the cluster key</h4>\n<pre>keytool -genkey -keyalg RSA -alias \"${CLUSTER_NAME}_CLUSTER\" -keystore \"$KEY_STORE\" -storepass \"$PASSWORD\" -keypass \"$PASSWORD\" \\\n-dname \"CN=CloudDurable Image $CLUSTER_NAME cluster, OU=Cloudurable, O=Cloudurable, L=San Francisco, ST=CA, C=USA, DC=cloudurable, DC=com\" \\\n-validity 36500\n</pre>\n<p>Once we create the cluster key, we will want to export a public key from it.</p>\n<h4>\n<a id=\"user-content-export-a-public-key-for-the-cluster-key\" class=\"anchor\" href=\"#export-a-public-key-for-the-cluster-key\" aria-hidden=\"true\"></a>Export a public key for the cluster key.</h4>\n<pre># Create the public key for the client to identify itself.\nkeytool -export -alias \"${CLUSTER_NAME}_CLIENT\" -file \"$CLIENT_PUBLIC_CERT\" -keystore \"$KEY_STORE\" \\\n-storepass \"$PASSWORD\" -keypass \"$PASSWORD\" -noprompt\n</pre>\n<p>Then we will import the public key into the trust store so that nodes can identify each other.</p>\n<h4>\n<a id=\"user-content-import-public-key-for-the-cluster-key-into-the-trust-store-so-nodes-can-identify-each-other\" class=\"anchor\" href=\"#import-public-key-for-the-cluster-key-into-the-trust-store-so-nodes-can-identify-each-other\" aria-hidden=\"true\"></a>Import public key for the cluster key into the trust store so nodes can identify each other</h4>\n<pre># Import the identity of the cluster public cluster key into the trust store so that nodes can identify each other.\nkeytool -import -v -trustcacerts -alias \"${CLUSTER_NAME}_CLUSTER\" -file \"$CLUSTER_PUBLIC_CERT\" -keystore \"$TRUST_STORE\" \\\n-storepass \"$PASSWORD\" -keypass \"$PASSWORD\" -noprompt\n</pre>\n<p>We perform the same three tasks for the client keys. Then lastly we create pem files for the client keys by exporting our Java JKS keystore as a PKCS12 trust store.</p>\n<h4>\n<a id=\"user-content-creating-client-pem-files\" class=\"anchor\" href=\"#creating-client-pem-files\" aria-hidden=\"true\"></a>Creating client pem files</h4>\n<pre>keytool -importkeystore -srcalias \"${CLUSTER_NAME}_CLIENT\" -srckeystore \"$KEY_STORE\" -destkeystore \"$PKS_KEY_STORE\" -deststoretype PKCS12 \\\n-srcstorepass \"$PASSWORD\" -deststorepass \"$PASSWORD\"\nopenssl pkcs12 -in \"$PKS_KEY_STORE\" -nokeys -out \"$KEY_STORE_PATH/${CLUSTER_NAME}_CLIENT.cer.pem\" -passin pass:cassandra\nopenssl pkcs12 -in \"$PKS_KEY_STORE\" -nodes -nocerts -out \"$KEY_STORE_PATH/${CLUSTER_NAME}_CLIENT.key.pem\" -passin pass:cassandra\n</pre>\n<p>Here are the files that get generated.</p>\n<h4>\n<a id=\"user-content-cert-files-stores-private-keys\" class=\"anchor\" href=\"#cert-files-stores-private-keys\" aria-hidden=\"true\"></a>Cert files, stores, private keys</h4>\n<pre>$ pwd\n~/github/cassandra-image\n$ ls resources/opt/cassandra/conf/certs/\nCLIENT_test_PUBLIC.cer\t\tcassandra.pks12.keystore\ttest_CLIENT.key.pem\nCLUSTER_test_PUBLIC.cer\t\tcassandra.truststore\ncassandra.keystore\t\ttest_CLIENT.cer.pem\n</pre>\n<ul><li>\n<code>CLIENT_test_PUBLIC.cer</code>     public client key for the test cluster.</li>\n<li>\n<code>cassandra.pks12.keystore</code>   PKS12 keystore for client used to generate pem</li>\n<li>\n<code>test_CLIENT.key.pem</code>        private client key in pem format used by csqlsh</li>\n<li>\n<code>CLUSTER_test_PUBLIC.cer</code>    public cluster key for the test cluster</li>\n<li>\n<code>cassandra.truststore</code>       Trust store used by cassandra</li>\n<li>\n<code>cassandra.keystore</code>         Key store used by cassandra</li>\n<li>\n<code>test_CLIENT.cer.pem</code>        public client key in pem format used by csqlsh</li>\n</ul><p>For the <a href=\"https://github.com/cloudurable/cassandra-image\">cassandra_image</a> project, these files are copied to <code>/opt/cassandra/conf/cert</code>. To learn more about our Vagrant project see <a href=\"http://cloudurable.com/blog/cassandra-image-vagrant-cluster-example/index.html\" rel=\"nofollow\">Setting up a Cassandra cluster with cassandra image and cassandra cloud project with Vagrant</a>.</p>\n<h2>\n<a id=\"user-content-use-the-keys-that-we-setup\" class=\"anchor\" href=\"#use-the-keys-that-we-setup\" aria-hidden=\"true\"></a>Use the keys that we setup.</h2>\n<p>As part of the provision script for <a href=\"https://github.com/cloudurable/cassandra-image\">cassandra_image</a>(see <a href=\"http://cloudurable.com/blog/cassandra-image-vagrant-cluster-example/index.html\" rel=\"nofollow\">Setting up a Cassandra cluster with cassandra image and cassandra cloud project with Vagrant</a>). We added the following:</p>\n<h4>\n<a id=\"user-content-cassandra-imagescripts040-install-certssh\" class=\"anchor\" href=\"#cassandra-imagescripts040-install-certssh\" aria-hidden=\"true\"></a>cassandra-image/scripts/040-install-certs.sh</h4>\n<pre>#!/bin/bash\nset -e\nDESTINATION_DIRECTORY=/opt/cassandra/conf/certs\nSOURCE_DIRECTORY=\"~/resources$DESTINATION_DIRECTORY\"\nif [ -d \"$SOURCE_DIRECTORY\" ]; then\n    mkdir -p \"$DESTINATION_DIRECTORY\"\n    cp -r \"$SOURCE_DIRECTORY\" \"$DESTINATION_DIRECTORY\"\nfi\nif [ ! -d \"$SOURCE_DIRECTORY\" ]; then\n    echo \"UNABLE TO INSTALL CERTS AS THEY WERE NOT FOUND\"\nfi\n</pre>\n<p>This will copy the certs to the right location if you generated a folder in resources (cassandra_image/resources/opt/cassandra/conf/cert), which the last <a href=\"https://github.com/cloudurable/cassandra-image/wiki/setupkeys-cassandra-security.sh\">script that we covered does</a>.</p>\n<h2>\n<a id=\"user-content-configure-cassandra-to-use-the-keys\" class=\"anchor\" href=\"#configure-cassandra-to-use-the-keys\" aria-hidden=\"true\"></a>Configure Cassandra to use the keys.</h2>\n<h4>\n<a id=\"user-content-optcassandraconf\" class=\"anchor\" href=\"#optcassandraconf\" aria-hidden=\"true\"></a>/opt/cassandra/conf</h4>\n<pre>\nserver_encryption_options:\n    internode_encryption: all\n    keystore: /opt/cassandra/conf/certs/cassandra.keystore\n    keystore_password: cassandra\n    truststore: /opt/cassandra/conf/certs/cassandra.truststore\n    truststore_password: cassandra\n    # More advanced defaults below:\n    protocol: TLS\nclient_encryption_options:\n    enabled: true\n    # If enabled and optional is set to true encrypted and unencrypted connections are handled.\n    optional: false\n    keystore: /opt/cassandra/conf/certs/cassandra.keystore\n    keystore_password: cassandra\n    truststore: /opt/cassandra/conf/certs/cassandra.truststore\n    truststore_password: cassandra\n    require_client_auth: true\n    protocol: TLS\n</pre>\n<p>Now let's test it.</p>\n<h4>\n<a id=\"user-content-testing-that-our-cassandra-nodes-can-talk-to-each-other\" class=\"anchor\" href=\"#testing-that-our-cassandra-nodes-can-talk-to-each-other\" aria-hidden=\"true\"></a>Testing that our Cassandra nodes can talk to each other</h4>\n<pre>$ vagrant up\n# Get a coffee and otherwise relax for a minute.\n# Now log into one of the nodes.\n$ vagrant ssh node0\n# Now check to see if the cluster is formed. \n[vagrant@localhost ~]$ /opt/cassandra/bin/nodetool describecluster\nCluster Information:\n\tName: test\n\tSnitch: org.apache.cassandra.locator.DynamicEndpointSnitch\n\tPartitioner: org.apache.cassandra.dht.Murmur3Partitioner\n\tSchema versions:\n\t\t86afa796-d883-3932-aa73-6b017cef0d19: [192.168.50.4, 192.168.50.5, 192.168.50.6]\n</pre>\n<p>We can see that the servers in the cluster can connect to each other (see <a href=\"http://cloudurable.com/blog/cassandra-image-vagrant-cluster-example/index.html\" rel=\"nofollow\">Setting up a Cassandra cluster with cassandra image and cassandra cloud project with Vagrant</a>).</p>\n<h2>\n<a id=\"user-content-setup-csql-client\" class=\"anchor\" href=\"#setup-csql-client\" aria-hidden=\"true\"></a>Setup csql client</h2>\n<p>We are doing this in OSX. In this example, we have the virtual machines running <em><strong>CentOS 7</strong></em> with <em><strong>Vagrant</strong></em> on <em><strong>VirtualBox</strong></em>. We can connect to those instances with <em><strong>Cassandra</strong></em> <code>cqlsh</code>.</p>\n<p>Let's copy cert files so we can access them from the client (MacBook pro / OSX).</p>\n<h4>\n<a id=\"user-content-copy-cert-files-created-earlier\" class=\"anchor\" href=\"#copy-cert-files-created-earlier\" aria-hidden=\"true\"></a>Copy cert files created earlier.</h4>\n<pre>$ cd ~/github/cassandra-image/resources/opt/cassandra/conf/certs\n$ mkdir /opt/cassandra/conf/certs\n$ cp * /opt/cassandra/conf/certs\n</pre>\n<p>Now we will create a <a href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_reference/cqlshUsingCqlshrc.html\" rel=\"nofollow\">cqlshrc</a> which is a file that dictates how we connect to Cassandra.</p>\n<h4>\n<a id=\"user-content-create-the-cqlshrc-in--cassandra\" class=\"anchor\" href=\"#create-the-cqlshrc-in--cassandra\" aria-hidden=\"true\"></a>Create the cqlshrc in  ~/.cassandra</h4>\n<pre>$ mkdir ~/.cassandra\n$ cd ~/.cassandra\n$ touch cqlshrc\n# edit this file\n</pre>\n<h4>\n<a id=\"user-content-cassandracqlshrc-contents\" class=\"anchor\" href=\"#cassandracqlshrc-contents\" aria-hidden=\"true\"></a>~/.cassandra/cqlshrc contents</h4>\n<pre>\n[connection]\nhostname = 192.168.50.4\nport = 9042\nfactory = cqlshlib.ssl.ssl_transport_factory\n[ssl]\ncertfile =  /opt/cassandra/conf/certs/test_CLIENT.cer.pem\nvalidate = false\n# Next 2 lines must be provided when require_client_auth = true in the cassandra.yaml file\nuserkey = /opt/cassandra/conf/certs/test_CLIENT.key.pem\nusercert = /opt/cassandra/conf/certs/test_CLIENT.cer.pem\n</pre>\n<p>Note we specify the nodes and we are using the pem file as our credentials via SSL to prove who we are instead of a username/password. (We could use both.) We need the <code>userkey</code> and <code>usercert</code> in the <code>cqlshrc</code> because we set <code>require_client_auth = true</code> in the <code>cassandra.yaml</code> file for the cluster nodes.</p>\n<p>Now let's test that the client connection works with SSL via <em><strong>cqlsh</strong></em>.</p>\n<h4>\n<a id=\"user-content-testing-client-connection-using-cqlsh\" class=\"anchor\" href=\"#testing-client-connection-using-cqlsh\" aria-hidden=\"true\"></a>Testing client connection using cqlsh</h4>\n<pre>$ /opt/cassandra/bin/cqlsh --ssl \nConnected to test at 192.168.50.4:9042.\n[cqlsh 5.0.1 | Cassandra 3.9 | CQL spec 3.4.2 | Native protocol v4]\nUse HELP for help.\n</pre>\n<p>Notice we can connect to Cassandra using SSL.  Notice that we are connected to the <code>test</code> cluster, which\nis the cluster we setup in <a href=\"http://cloudurable.com/blog/cassandra-image-vagrant-cluster-example/index.html\" rel=\"nofollow\">Setting up a Cassandra cluster with cassandra image and cassandra cloud project with Vagrant</a>.</p>\n<h2>\n<a id=\"user-content-review\" class=\"anchor\" href=\"#review\" aria-hidden=\"true\"></a>Review</h2>\n<p>We setup keys for client and clustering. We deployed keys to three Linux boxes using Vagrant provisioning.\nWe then setup <code>cqlsh</code> to use SSL. We then logged into one of the nodes and checked that the network was setup with the <code>nodetool describecluster</code>. Then we locally setup <code>csqlsh</code> to connect to the cluster using SSL.</p>\n<h2>\n<a id=\"user-content-more-to-come\" class=\"anchor\" href=\"#more-to-come\" aria-hidden=\"true\"></a>More to come.</h2>\n<p>Check back with us at the <a href=\"http://cloudurable.com/blog/index.html\" rel=\"nofollow\">Cloudurable blog</a> to find out more about <code>cassandra-image</code> and <code>cassandra-cloud</code>.</p>\n<h2>\n<a id=\"user-content-about-cloudurable\" class=\"anchor\" href=\"#about-cloudurable\" aria-hidden=\"true\"></a>About Cloudurable</h2>\n<p><a href=\"http://cloudurable.com/\" rel=\"nofollow\">Cloudurable</a> provides AMIs, cloudformation templates and monitoring tools\nto support <a href=\"http://cloudurable.com/services/index.html\" rel=\"nofollow\">Cassandra in production running in EC2</a>.\nWe also teach advanced <a href=\"http://cloudurable.com/cassandra-course/index.html\" rel=\"nofollow\">Cassandra courses which teaches how one could develop, support and deploy Cassandra to production in AWS EC2</a>.</p>\n<h4>\n<a id=\"user-content-references\" class=\"anchor\" href=\"#references\" aria-hidden=\"true\"></a>References</h4>\n<ul><li>Carpenter, Jeff; Hewitt, Eben (2016-06-29). <em><strong>Cassandra: The Definitive Guide: Distributed Data at Web Scale.</strong></em> O'Reilly Media.</li>\n<li><a href=\"https://developer.jboss.org/wiki/KeystoreFormatsJKSAndPEMCheatsheet\" rel=\"nofollow\">https://developer.jboss.org/wiki/KeystoreFormatsJKSAndPEMCheatsheet</a></li>\n<li><a href=\"http://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/JSSERefGuide.html\" rel=\"nofollow\">Java SSL support</a></li>\n<li><a href=\"https://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/FIPS.html\" rel=\"nofollow\">https://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/FIPS.html</a></li>\n<li><a href=\"http://docs.datastax.com/en/archived/cassandra/2.0/cassandra/security/secureSSLNodeToNode_t.html\" rel=\"nofollow\">http://docs.datastax.com/en/archived/cassandra/2.0/cassandra/security/secureSSLNodeToNode_t.html</a></li>\n<li><a href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/security/secureSSLCertificates_t.html\" rel=\"nofollow\">https://docs.datastax.com/en/cassandra/2.1/cassandra/security/secureSSLCertificates_t.html</a></li>\n<li><a href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/security/secureSSLClientToNode_t.html\" rel=\"nofollow\">https://docs.datastax.com/en/cassandra/2.1/cassandra/security/secureSSLClientToNode_t.html</a></li>\n<li><a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/configuration/secureCqlshSSL.html\" rel=\"nofollow\">Using cqlsh with SSL</a></li>\n<li><a href=\"http://cloudurable.com/blog/cassandra-image-vagrant-cluster-example/index.html\" rel=\"nofollow\">Setting up a Cassandra cluster with cassandra image and cassandra cloud project with Vagrant</a></li>\n<li>\n<a href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_reference/cqlshUsingCqlshrc.html\" rel=\"nofollow\">Configuring cqlsh from a file</a>.</li>\n</ul></div><div id=\"wiki-footer\" class=\"mt-5 mb-0 wiki-footer gollum-markdown-content\">\n          <div class=\"Box Box--condensed bg-gray box-shadow\">\n            <div class=\"Box-body  markdown-body\">\n              <h2>\n<a id=\"user-content-about-us\" class=\"anchor\" href=\"#about-us\" aria-hidden=\"true\"></a>About us</h2>\n<p><a href=\"http://cloudurable.com/\" rel=\"nofollow\">Cloudurable™: streamline DevOps for Cassandra running on AWS</a> provides AMIs, CloudWatch Monitoring, CloudFormation templates and monitoring tools\nto support <a href=\"http://cloudurable.com/services/index.html\" rel=\"nofollow\">Cassandra in production running in EC2</a>.\nWe also teach advanced <a href=\"http://cloudurable.com/services/index.html\" rel=\"nofollow\">Cassandra courses which teaches how one could develop, support and deploy Cassandra to production in AWS EC2 for Developers and DevOps</a>.</p>\n<h4>\n<a id=\"user-content-more-info\" class=\"anchor\" href=\"#more-info\" aria-hidden=\"true\"></a>More info</h4>\n<p>Please take some time to read the <a href=\"http://cloudurable.com/advantages/index.html\" rel=\"nofollow\">Advantage of using Cloudurable™</a>.</p>\n<p>Cloudurable provides:</p>\n<ul><li>\n<a href=\"http://cloudurable.com/subscription_support_benefits_cassandra/index.html\" rel=\"nofollow\">Subscription Amazon Cassandra support to streamline DevOps</a> (<a href=\"http://cloudurable.com/subscription_support/index.html\" rel=\"nofollow\">Support subscription pricing for Cassandra and Kafka in AWS</a>)</li>\n<li><a href=\"http://cloudurable.com/cassandra-course/index.html\" rel=\"nofollow\">Cassandra Course</a></li>\n<li><a href=\"http://cloudurable.com/service-quick-start-mentoring-cassandra-or-kafka-aws-ec2/index.html\" rel=\"nofollow\">Cassandra Consulting: Quick Start</a></li>\n<li><a href=\"http://cloudurable.com/service-architecture-analysis-cassandra-or-kafka-aws-ec2/index.html\" rel=\"nofollow\">Cassandra Consulting: Architecture Analysis</a></li>\n</ul><p><a href=\"http://cloudurable.com/kafka-training/index.html\" rel=\"nofollow\">Kafka training</a>, <a href=\"http://cloudurable.com/kafka-aws-consulting/index.html\" rel=\"nofollow\">Kafka consulting</a>,\n<a href=\"http://cloudurable.com/cassandra-course/index.html\" rel=\"nofollow\">Cassandra training</a>, <a href=\"http://cloudurable.com/kafka-aws-consulting/index.html\" rel=\"nofollow\">Cassandra consulting</a>,\n<a href=\"http://cloudurable.com/spark-aws-emr-training/index.html\" rel=\"nofollow\">Spark training</a>, <a href=\"http://cloudurable.com/spark-aws-emr-consulting/index.html\" rel=\"nofollow\">Spark consulting</a></p>\n            </div>\n          </div>\n        </div>",
        "created_at": "2018-10-24T18:10:13+0000",
        "updated_at": "2018-10-24T18:10:22+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 11,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/24500753?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12465"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          }
        ],
        "is_public": false,
        "id": 12464,
        "uid": null,
        "title": "Setting Up a Cassandra Cluster in AWS - DZone Cloud",
        "url": "https://dzone.com/articles/setting-up-a-cassandra-cluster-in-aws",
        "content": "<div class=\"content-html\" itemprop=\"text\"><article class=\"post-section post-2624 post type-post status-publish format-standard hentry category-developer-tips\" id=\"post-2624\"><div class=\"sw-wrapper clearfix\"> \n  <div class=\"post-detail\"> \n   <p>Apache Cassandra is a NoSQL database that allows for easy horizontal scaling, <a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archDataDistributeHashing.html\" rel=\"nofollow\">using the consistent hashing mechanism</a>. <a href=\"https://techblog.bozho.net/why-i-decided-not-to-use-cassandra/\" target=\"_blank\" rel=\"nofollow\">Seven years ago, I tried it and decided not use it for a side-project of mine</a> because it was too new. But things are different now — Cassandra is well-established, there’s a company behind it (DataStax), and there are a lot more tools, documentation, and community support. So once again, I decided to try Cassandra.</p> \n   <p>This time, I need it to run in a cluster on AWS, so I went on to set up such a cluster. Googling how to do it gives several interesting results, like <a href=\"http://highscalability.com/blog/2016/8/1/how-to-setup-a-highly-available-multi-az-cassandra-cluster-o.html\" target=\"_blank\" rel=\"nofollow\">this</a>, <a href=\"https://github.com/cloudurable/cassandra-image/wiki/Cassandra-Tutorial-6:-Setting-up-Cassandra-Cluster-in-EC2-Part-2-Multi-AZs-with-Ec2Snitch\" target=\"_blank\" rel=\"nofollow\">this</a>, and <a href=\"https://wiki.apache.org/cassandra/CloudConfig\" target=\"_blank\" rel=\"nofollow\">this</a>, but they are incomplete, outdated, or have too many irrelevant details. So they are only of moderate help.</p> \n   <p>My goal is to use CloudFormation (or Terraform potentially) to launch a stack that has a Cassandra auto-scaling group (in a single region) that can grow as easily as increasing the number of nodes in the group.</p> \n   <p>Also, in order to have the web application connect to Cassandra without hardcoding the node IPs, I wanted to have a load balancer in front of all Cassandra nodes to do the round-robin for me. The alternative to that would be to have a <a href=\"http://docs.datastax.com/en/drivers/java/2.2/com/datastax/driver/core/policies/DCAwareRoundRobinPolicy.html\" target=\"_blank\" rel=\"nofollow\">client-side round-robin</a>, but that would mean some extra complexity on the client, which seems avoidable with a load balancer in front of the Cassandra auto-scaling group.</p> \n   <p>The relevant bits from my <a href=\"https://gist.github.com/Glamdring/5139bc49037f3b5c1f8cd008ba046df0\" target=\"_blank\" rel=\"nofollow\">CloudFormation JSON can be seen here</a>. Here's what it does:</p> \n   <ul><li>Sets up three private subnets (1 per availability zone in the eu-west region)</li> \n    <li>Creates a security group that allows incoming and outgoing ports that allow Cassandra to accept connections (9042) and for the nodes to gossip (7000/7001). Note that the ports are only accessible from within the VPC — no external connection is allowed. SSH goes only through a <a href=\"https://en.wikipedia.org/wiki/Bastion_host\" target=\"_blank\" rel=\"nofollow\">bastion host</a>.</li> \n    <li>Defines a TCP load balancer for port 9042 where all clients will connect. The load balancer requires a so-called “Target group,” which is defined as well.</li> \n    <li>Configures an auto-scaling group with a pre-configured number of nodes. The autoscaling group has a reference to the “target group”so that the load balancer always sees all nodes in the auto-scaling group.</li> \n    <li>Each node in the auto-scaling group is identical based on a launch configuration. The launch configuration runs a few scripts on initialization. These scripts will be run for every node – either initially, when case a node dies and another one is spawned in its place, or when the cluster has to grow. The scripts are fetched from S3, where you can publish them (and version them) either manually or with an automated process.</li> \n    <li>Note: This does not configure specific EBS volumes and, in reality, you may need to configure and attach them if the instance storage is insufficient. Don’t worry about nodes dying, though, as data is safely replicated.</li> \n   </ul><p>That was the easy part – a bunch of AWS resources and port configurations. The Cassandra-specific setup is a bit harder, as it requires understanding of how Cassandra functions.</p> \n   <p>The two scripts are <a href=\"https://gist.github.com/Glamdring/ee5f24dbef3795a860ac91c9c14255a2\" target=\"_blank\" rel=\"nofollow\">setup-cassandra.sh</a> and <a href=\"https://gist.github.com/Glamdring/d317ff191a223d2bcf7c92fa5fdf3476\" target=\"_blank\" rel=\"nofollow\">update-cassandra-cluster-config.py</a>, so bash and Python: bash for setting-up the machine, and Python for Cassandra-specific stuff. Instead of the bash script, one could use a pre-built AMI (image), e.g. with Packer, but since only two pieces of software are installed, I thought it was a bit of an overhead to support AMIs.</p> \n   <p><a href=\"https://gist.github.com/Glamdring/ee5f24dbef3795a860ac91c9c14255a2\" target=\"_blank\" rel=\"nofollow\">The bash script can be seen here</a>, and simply installs Java 8 and the latest Cassandra, runs the Python script, runs the Cassandra services, and creates (if needed) a keyspace with proper replication configuration. A few notes here – the cassandra.yaml.template could be supplied via the CloudFormation script instead of having it fetched via bash (and having to pass the bucket name); you could also have it fetched in the Python script itself – it’s a matter of preference.</p> \n   <p>Cassandra is not configured for use with SSL, which is generally a bad idea, but the <a href=\"https://github.com/cloudurable/cassandra-image/wiki/Cassandra-Tutorial-2:-Setting-up-client-and-cluster-SSL-transport-in-Cassandra\" target=\"_blank\" rel=\"nofollow\">SSL configuration</a> is out of the scope of this basic setup. Finally, the script waits for the Cassandra process to run (using a while/sleep loop) and then creates the keyspace if needed. The keyspace (=database) has to be created with a NetworkTopologyStrategy, and the number of replicas for the particular datacenter (=AWS region) has to be configured. The value is 3, for the 3 availability zones where we’ll have nodes. That means there’s a copy in each AZ (which is seen like a “rack”, although it’s exactly that).</p> \n   <p><a href=\"https://gist.github.com/Glamdring/d317ff191a223d2bcf7c92fa5fdf3476\" target=\"_blank\" rel=\"nofollow\">The Python script </a>does some very important configurations – without them, the cluster won’t work. (I don’t work with Python normally, so feel free to criticize my Python code). The script does the following:</p> \n   <ul><li>Gets the current auto-scaling group details (using AWS EC2 APIs).</li> \n    <li>Sorts the instances by time.</li> \n    <li>Fetches the first instance in the group in order to assign it as seed node.</li> \n    <li>Sets the seed node in the configuration file (by replacing a placeholder).</li> \n    <li>Sets the listen_address (and therefore rpc_address) to the private IP of the node in order to allow Cassandra to listen for incoming connections.</li> \n   </ul><p>Designating the seed node is important, as all cluster nodes have to join the cluster by specifying at least one seed. You can get the first two nodes instead of just one, but it shouldn’t matter. Note that the seed node is not always fixed – it’s just the oldest node in the cluster. If at some point the oldest node is terminated, each new node will use the second oldest as seed.</p> \n   <p>What I haven’t shown is the cassandra.yaml.template file. It is basically a copy of the cassandra.yaml file from a standard Cassandra installation, with a few changes:</p> \n   <ul><li><code>cluster_name</code> is modified to match your application name. This is just for human-readable purposes, so it doesn’t matter what you set it to.</li> \n    <li><code>allocate_tokens_for_keyspace: your_keyspace</code> is uncommented and the keyspace is set to match your main keyspace. This enables the <a href=\"https://www.datastax.com/dev/blog/token-allocation-algorithm\" target=\"_blank\" rel=\"nofollow\">new token distribution algorithm in Cassandra 3.0</a>. It allows for evenly distributing the data across nodes.</li> \n    <li><code>endpoint_snitch: Ec2Snitch</code> is set instead of the SimpleSnitch to make use of AWS metadata APIs. Note that this setup is in a single region. For multi-region, there’s another snitch and some additional complications of exposing ports and changing the broadcast address.</li> \n    <li>As mentioned above, ${private_ip} and ${seeds} placeholders are placed in the appropriate places (listen_address and rpc_address for the IP) in order to allow substitution.</li> \n   </ul><p>The lets you run a Cassandra cluster as part of your AWS stack, which is auto-scalable and doesn’t require any manual intervention – neither on setup nor on scaling up. Well, allegedly – there may be issues that have to be resolved once you hit the use cases of reality. And for clients to connect to the cluster, simply use the load balancer DNS name (you can print it in a config file on each application node).</p> \n  </div> \n </div> \n</article></div><div class=\"content-html\" itemprop=\"text\"><a>\n                        <img class=\"pub-image\" width=\"420\" itemprop=\"image\" src=\"src\" alt=\"image\" /></a></div>",
        "created_at": "2018-10-24T18:08:00+0000",
        "updated_at": "2018-10-24T18:08:09+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 5,
        "domain_name": "dzone.com",
        "preview_picture": "https://dz2cdn3.dzone.com/storage/article-thumb/6858798-thumb.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12464"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 35,
            "label": "docker",
            "slug": "docker"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12462,
        "uid": null,
        "title": "Docker Meet Cassandra. Cassandra Meet Docker.",
        "url": "http://thelastpickle.com/blog/2018/01/23/docker-meet-cassandra.html",
        "content": "<p>After having spent quite a bit of time learning Docker and after hearing strong community interest for the technology even though few have played with it, I figured it’d be be best to share what I’ve learned. Hopefully the knowledge transfer helps newcomers get up and running with Cassandra in a concise, yet deeply informed manner.</p><p>A few years ago I finally started playing with Docker by way of Vagrant. That entire experience was weird. Don’t do it.</p>\n<p>Later Docker Compose was released and all the roadblocks I previously encountered immediately melted away and the power of Docker was made very aware to me. Since then I’ve been like my cat, but instead of “Tuna Tuna Tuna Tuna” it’s more like: “Docker Docker Docker Docker.”</p>\n<p>But the more I spoke about Docker and asked around about Docker, the sadder I became since:</p>\n<ul><li>Few really used Docker.</li>\n  <li>Fewer had even heard of Docker Compose.</li>\n  <li>Everyone was worried about how Docker performance would be in production.</li>\n  <li>Some were waiting for the Mesos and Kubernetes war to play out.\n    <ul><li>Kubernetes won by the way. Read any news around Docker-Kubernetes and AWS-Kubernetes to make your own judgements.</li>\n    </ul></li>\n</ul><p>Within The Last Pickle, I advocate for Docker as best I can. Development project? “Why not use Docker?” Quick test? “<em>cough</em> Docker <em>cough</em>.” Learn everything you can about Grafana, Graphite, and monitoring dashboards you ask? “Okay, Docker it is!”</p>\n<p>About a year later, we’re here and guess what? Now you get to be here with me as well! :tada:</p>\n<h2 id=\"docker-cassandra-bootstrap\">Docker Cassandra Bootstrap</h2>\n<p>In October, <a href=\"https://www.linkedin.com/in/nickmbailey/\">Nick Bailey</a> invited me to present at the local <a href=\"https://www.meetup.com/Austin-Cassandra-Users/\">Austin Cassandra Users Meetup</a> and I figured this was the time to consolidate my recent knowledge and learnings into a simplified project. I figured if I had already spent time on such an intricate project I could save others time and give them a clean environment they could play with, develop on, then move into production.</p>\n<p>That’s how the <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap\">docker-cassandra-bootstrap</a> project was born.</p>\n<p>I will stay away from how to run the Docker Cassandra Bootstrap project within this blog post since the instructions are already within the Github project’s <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap\">README.md</a>. Instead, I’ll focus on the individual components, what’s hidden in which nooks, and which stubs are commented out in which crannies for future use and development.</p>\n<h2 id=\"docker-composeyml\">docker-compose.yml</h2>\n<p>The <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/docker-compose.yml\">docker-compose.yml</a> is the core building block for any Docker Compose project.</p>\n<h3 id=\"building\">Building</h3>\n<p>What Docker provided was the <code class=\"highlighter-rouge\">Dockerfile</code> which allowed <em>image</em> definitions to run <em>containers</em>. (Note the differentiation that <em>containers</em> are <em>images</em> that are running.) Building an image using Docker was pretty straight forward:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>docker build .\n</pre></div></div>\n<p>However, building many images would require tagging and additional <code class=\"highlighter-rouge\">docker</code> parameters. And that can get confusing really quickly and definitely isn’t user-friendly.</p>\n<p>Instead, Docker Compose lets you build entire ecosystems of <em>services</em> with a simple command:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>docker-compose build\n</pre></div></div>\n<p>Now with Docker Compose, you don’t have to keep track of image tagging, <code class=\"highlighter-rouge\">Dockerfile</code> location, or anything else that I gratefully have too little experience with. Instead, <em>images</em> are defined by the <code class=\"highlighter-rouge\">image</code> (from Docker Hub) and <code class=\"highlighter-rouge\">build</code> (from a local <code class=\"highlighter-rouge\">Dockerfile</code>) parameters within a Docker Compose <em>service</em>.</p>\n<h3 id=\"environmental-variables\">Environmental Variables</h3>\n<p>Along with the simplification of image definitions, Docker Compose introduces the <code class=\"highlighter-rouge\">env_file</code> parameter which is a not-quite-bash environmental definition file. It’s slightly different, bash commands will not resolve, you can’t use an envar within another’s definition, and don’t use quotes since those will be considered part of the envar’s value. While these <code class=\"highlighter-rouge\">env_files</code> come with their own limitations, <code class=\"highlighter-rouge\">env_files</code> means I no longer have to have ugly, long, complicated lines like:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>docker -e KEY=VAL -e KEY2=VAL2 -e WHAT=AMI -e DOING=NOW ...\n</pre></div></div>\n<p>Through the use of <a href=\"https://docs.docker.com/compose/extends/\">multiple <code class=\"highlighter-rouge\">docker-compose.yml</code> files</a>, one can create:</p>\n<ul><li>An <code class=\"highlighter-rouge\">env_file</code> called <code class=\"highlighter-rouge\">settings.env</code> which has all generalized settings.</li>\n  <li>An <code class=\"highlighter-rouge\">env_file</code> called <code class=\"highlighter-rouge\">dev.env</code> which has dev-specific settings.</li>\n  <li>An <code class=\"highlighter-rouge\">env_file</code> called <code class=\"highlighter-rouge\">prod.env</code> which has production-specific settings.</li>\n  <li>An <code class=\"highlighter-rouge\">env_file</code> called <code class=\"highlighter-rouge\">nz.env</code> which will define variables to flip all gifs by 180-degrees to counteract the fact that New Zealanders are upside down.</li>\n</ul><p>At the end of the day, the filenames, segregation, and environmental variable values are for you to use and define within your own ecosystem.</p>\n<p>But that’s getting ahead of ourselves. Just know that you can place whatever environmental variables you want in these files as you create production-specific <code class=\"highlighter-rouge\">env_files</code> which may not get used today, but will be utilized when you move into production.</p>\n<h3 id=\"volumes\">Volumes</h3>\n<p>Within Docker everything within a container that is not stored in <code class=\"highlighter-rouge\">volumes</code> is temporary. This means that if we launch a new container using any given static Docker image, we can manipulate multiple aspects of system configuration, data, file placement, etc, without a concern on changing our stable static environment. If something ever breaks, we simply kill the container, launch a new container based off the same image, and we’re back to our familiar static and stable state. However, if we ever want to persist data, storing this valuable data within Docker <code class=\"highlighter-rouge\">volumes</code> ensures that the data is accessible across container restarts.</p>\n<p>The above statement probably makes up about 90% of my love for Docker (image layer caching probably makes up a majority of the remaining 10%).</p>\n<p>What my declaration of love means is that while a container is running: it is your pet. It does what you ask of it (unless it’s a cat) and it will live with you by your side crunching on the code you fed it. But once your pet has finished processing the provided code, it will vanish into the ether and you can replace it like you would cattle.</p>\n<p>This methodology is a perfect fit for short-lived microservice workers. However, if you want to persist data, or provide configuration files to the worker microservices, you’ll want to use <code class=\"highlighter-rouge\">volumes</code>.</p>\n<p>While you can use <a href=\"https://docs.docker.com/compose/compose-file/#volume-configuration-reference\"><em>named volumes</em></a> to allow Docker to handle the data location for you, not using named volumes will put you in full control of where the data directory will resolve to, can provide performance benefits, and will remove one layer of indirection in case anything should go wrong.</p>\n<p>When people ask about the performance of Docker in production, <code class=\"highlighter-rouge\">volumes</code> are the key component. If your service relies on disk access, use <code class=\"highlighter-rouge\">volumes</code> to ensure a higher level of performance. For all other cases, if there even is a CPU-based performance hit, it should be mild and you can always scale horizontally. The ultimate performance benefit of using Docker is that applications are containerized and are extremely effective at horizontal scaling. Also, consider the time and effort costs to test, deploy, and rollback any production changes when using containers. Although this isn’t essentially performance related, it does increase codebase velocity which may be as valuable as raw performance metrics.</p>\n<h3 id=\"entrypoints\">Entrypoints</h3>\n<p>Back to looking at the <code class=\"highlighter-rouge\">docker-compose.yml</code>, Docker <code class=\"highlighter-rouge\">entrypoints</code> define which program will be executed when a machine begins running. You can think of SSH’s entrypoint as being <code class=\"highlighter-rouge\">bash</code>, or <code class=\"highlighter-rouge\">zsh</code>.</p>\n<p>Under the <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/docker-compose.yml\"><code class=\"highlighter-rouge\">cqlsh</code> service</a>, the default <code class=\"highlighter-rouge\">entrypoint</code> is overwritten by <code class=\"highlighter-rouge\">cqlsh cassandra</code>, where <code class=\"highlighter-rouge\">cassandra</code> is the name of the Cassandra Docker Compose service. This means that we want to use the <code class=\"highlighter-rouge\">cassandra:3.11</code> image, but not use the bash script that sets up the <code class=\"highlighter-rouge\">cassandra.yaml</code> and other Cassandra settings. Instead, the service will utilize <code class=\"highlighter-rouge\">cassandra:3.11</code>’s image and start the container with <code class=\"highlighter-rouge\">cqlsh cassandra</code>. This allows the following shorthand command to be run, all within a Docker container, without any local dependencies other than Docker and Docker Compose:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>docker-compose run cqlsh\n</pre></div></div>\n<p>The above command starts up the <code class=\"highlighter-rouge\">cqlsh</code> service, calls the <code class=\"highlighter-rouge\">cqlsh</code> binary, and provides the <code class=\"highlighter-rouge\">cassandra</code> hostname as the contact point.</p>\n<p>The <code class=\"highlighter-rouge\">nodetool</code> service is good example of creating a shorthand command for an otherwise complicated process. Instead of having:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>docker-compose run --entrypoint bash cassandra\n$ nodetool -h cassandra -u cassandraUser -pw cassandraPass status\n</pre></div></div>\n<p>We can simply run:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>docker-compose run nodetool status\n</pre></div></div>\n<p>Any parameters following the service name are appended to the defined <code class=\"highlighter-rouge\">entrypoint</code>’s command and replaces the service’s <code class=\"highlighter-rouge\">command</code> parameter. For the <code class=\"highlighter-rouge\">nodetool</code> service the default <code class=\"highlighter-rouge\">command</code> is <code class=\"highlighter-rouge\">help</code>, but in the above line, the <code class=\"highlighter-rouge\">command</code> that is appended to the <code class=\"highlighter-rouge\">entrypoint</code> is <code class=\"highlighter-rouge\">status</code>.</p>\n<h3 id=\"links\">Links</h3>\n<p>If we have a service that will rely on communication with another service, the way that the <code class=\"highlighter-rouge\">cassandra-reaper</code> service must be in contact with the <code class=\"highlighter-rouge\">cassandra</code> service that Reaper will be monitoring, we can use the <code class=\"highlighter-rouge\">links</code> service parameter to define both the hostname and servicename within this link.</p>\n<p>I like to be both simple and explicit, which is why I use the same name for the hostname and service name like:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>links:\n  - cassandra:cassandra\n</pre></div></div>\n<p>The above line will allow the <code class=\"highlighter-rouge\">cassandra-reaper</code> service’s container to contact the <code class=\"highlighter-rouge\">cassandra</code> service by way of the <code class=\"highlighter-rouge\">cassandra</code> hostname.</p>\n<h3 id=\"ports\">Ports</h3>\n<p>Ports are a way to expose a service’s port to another service, or bind a service’s port to a local port.</p>\n<p>Because the <code class=\"highlighter-rouge\">cassandra-reaper</code> service is meant to be used via its web UI, we bind the service’s <code class=\"highlighter-rouge\">8080</code> and <code class=\"highlighter-rouge\">8081</code> ports from within the service onto the local machine’s <code class=\"highlighter-rouge\">8080</code> and <code class=\"highlighter-rouge\">8081</code> ports using the following lines:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>ports:\n  - \"8080:8080\"\n  - \"8081:8081\"\n</pre></div></div>\n<p>This means if we visit http://localhost:8080/webui/ from our local machine, we’ll be processing code from within a container to service that web request.</p>\n<h3 id=\"restart\">Restart</h3>\n<p>The <code class=\"highlighter-rouge\">restart</code> command is more of a Docker Compose-specific scheduler that dictates what a service should do if the container is abruptly terminated.</p>\n<p>In the case of the <code class=\"highlighter-rouge\">grafana</code> and <code class=\"highlighter-rouge\">logspout</code> services, if Grafana or Logspout ever die, the containers will exit and the <code class=\"highlighter-rouge\">grafana</code> or <code class=\"highlighter-rouge\">logspout</code> services will automatically restart and come back online.</p>\n<p>While this parameter may be ideal for some microservices, it may not be ideal for services that power data stores.</p>\n<h2 id=\"the-cassandra-service\">The <code class=\"highlighter-rouge\">cassandra</code> service</h2>\n<p>The <code class=\"highlighter-rouge\">docker-compose.yml</code> defines the <code class=\"highlighter-rouge\">cassandra</code> service as having a few mounted configuration files.</p>\n<p>The two configuration files that are enabled by default include configurations for:</p>\n<ul><li>collectd.</li>\n  <li>Prometheus JMX exporter.</li>\n</ul><p>The two configuration files that are disabled by default are for:</p>\n<ul><li>The Graphite metrics reporter.</li>\n  <li>The Filebeat log reporter for ELK.</li>\n</ul><h3 id=\"collectd\">collectd</h3>\n<p>The <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/config/collectd.cassandra.conf\">cassandra/config/collectd.cassandra.conf</a> configuration file loads a few plugins that TLP has found to be useful for enterprise metric dashboards.</p>\n<p><a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/Dockerfile#L62-L74\">A few packages</a> need to be installed for the collectd to be fully installed with the referenced plugins. The service must also be started from within the <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/docker-entrypoint.sh#L11\">cassandra/docker-entrypoint.sh</a> or just by using <code class=\"highlighter-rouge\">service collectd start</code> on hardware.</p>\n<p>The metrics that are collected include information on:</p>\n<ul><li>Disk.</li>\n  <li>CPU load.</li>\n  <li>Network traffic.</li>\n  <li>Memory usage.</li>\n  <li>System logs.</li>\n</ul><p>collectd is then configured to write to a Prometheus backend by default. Commented code is included for writing to a Graphite backend.</p>\n<p>For further information on each of the plugins, visit the <a href=\"https://collectd.org/wiki/index.php/Table_of_Plugins\">collectd wiki</a>.</p>\n<h3 id=\"prometheus-jmx-exporter\">Prometheus JMX Exporter</h3>\n<p>The <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/config/prometheus.yml\">cassandra/config/prometheus.yml</a> configuration file defines the JMX endpoints that will be collected by the JMX exporter and exposed via a REST API for Prometheus ingestion.</p>\n<p>The following jar needs to be used and referenced within <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/Dockerfile#L78-L85\">cassandra-env.sh</a>:</p>\n<ul><li><a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/lib/jmx_prometheus_javaagent-0.9.jar\">cassandra/lib/jmx_prometheus_javaagent-0.9.jar</a></li>\n</ul><p>Only the most important enterprise-centric list of metrics are being collected for Prometheus ingestion.</p>\n<p>The resulting <code class=\"highlighter-rouge\">name</code> formats did not follow the standard Prometheus naming scheme, but instead something between that of the Graphite dot-naming scheme with the use of Prometheus-styled labels when relevant. Do note that Prometheus will automatically convert all dot-separated metrics to underscore-separated metrics since Prometheus does not understand the concept of hierarchical metric keys.</p>\n<h3 id=\"graphite-metrics-reporter\">Graphite Metrics Reporter</h3>\n<p>The <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/config/graphite.cassandra.yaml\">cassandra/config/graphite.cassandra.yaml</a> configuration file is included and commented out by default.</p>\n<p>For reporting to work correctly, this file requires the following jars to be placed into <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/Dockerfile#L10-L35\">Cassandra’s lib directory and have Java 8 installed</a>:</p>\n<ul><li><a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/lib/metrics-core-3.1.2.jar\">cassandra/lib/metrics-core-3.1.2.jar</a></li>\n  <li><a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/lib/metrics-graphite-3.1.2.jar\">cassandra/lib/metrics-graphite-3.1.2.jar</a></li>\n  <li><a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/lib/reporter-config-base-3.0.3.jar\">cassandra/lib/reporter-config-base-3.0.3.jar</a></li>\n  <li><a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/lib/reporter-config3-3.0.3.jar\">cassandra/lib/reporter-config3-3.0.3.jar</a></li>\n</ul><p>Once properly enabled, the <code class=\"highlighter-rouge\">cassandra</code> service can contact a Graphite host with enterprise-centric metrics. However, we did not include the <code class=\"highlighter-rouge\">graphite</code> service within this project because our experience with Prometheus has been smoother than dealing with Graphite and because Prometheus seems to scale better than Graphite in production environments. As of the most recent Grafana releases, Prometheus has also received plenty of Prometheus-centric features and support as well.</p>\n<h3 id=\"filebeat-log-reporter\">Filebeat Log Reporter</h3>\n<p>The <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/config/filebeat.yml\">cassandra/config/filebeat.yml</a> configuration file is set up to contact a pre-existing ELK stack.</p>\n<p>The Filebeat package is required and can be started within a Docker container by referencing the Filebeat service from within <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/Dockerfile#L37-L52\">cassandra-env.sh</a>.</p>\n<p>The Filebeat configuration file is by no means complete (and assume incorrect!), but it does provide a good starting point for learning how to ingest log files into Logstash using Filebeat. Please consider this configuration file as <em>fully experimental</em>.</p>\n<h3 id=\"jmx-authentication\">JMX Authentication</h3>\n<p>Because Reaper for Apache Cassandra will be contacting the JMX port of this <code class=\"highlighter-rouge\">cassandra</code> service we will also need to add <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/Dockerfile#L87-L95\">authentication files for JMX in two locations</a> and set the <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra/cassandra.env#L15-L17\"><code class=\"highlighter-rouge\">LOCAL_JMX</code> variable to <code class=\"highlighter-rouge\">no</code></a> to expose the JMX port externally while requiring authentication.</p>\n<h2 id=\"the-cqlsh-service\">The <code class=\"highlighter-rouge\">cqlsh</code> Service</h2>\n<p>The <code class=\"highlighter-rouge\">cqlsh</code> service is a helper service that simply uses the <code class=\"highlighter-rouge\">cassandra:3.11</code> image hosted on Docker Hub to provide the cqlsh binary while mounting the local <code class=\"highlighter-rouge\">./cassandra/schema.cql</code> file into the container for simple schema creation and data querying.</p>\n<p>The defined setup allows us to run the following command with ease:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>docker-compose run cqlsh -f /schema.cql\n</pre></div></div>\n<p>The <code class=\"highlighter-rouge\">nodetool</code> service is another helper service that is provided to automatically fill in the host, username, and password parameters.</p>\n<p>By default, it includes the <code class=\"highlighter-rouge\">help</code> <code class=\"highlighter-rouge\">command</code> to provide a list of options for <code class=\"highlighter-rouge\">nodetool</code>. Running the following command will contact the <code class=\"highlighter-rouge\">cassandra</code> node, automatically authenticate, and show a list of options:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>docker-compose run nodetool\n</pre></div></div>\n<p>By including an additional parameter within the command line we will overwrite the <code class=\"highlighter-rouge\">help</code> <code class=\"highlighter-rouge\">command</code> and run that requested command instead, like <code class=\"highlighter-rouge\">status</code>:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>docker-compose run nodetool status\n</pre></div></div>\n<h2 id=\"the-cassandra-reaper-service\">The <code class=\"highlighter-rouge\">cassandra-reaper</code> Service</h2>\n<p>The <code class=\"highlighter-rouge\">cassandra-reaper</code> service does not use a locally built and customized image, but instead uses an image hosted on <a href=\"https://hub.docker.com/r/thelastpickle/cassandra-reaper/\">Docker Hub</a>. The image that is specifically used is <code class=\"highlighter-rouge\">ab0fff2</code>. However, you can choose to use the <code class=\"highlighter-rouge\">latest</code> release or <code class=\"highlighter-rouge\">master</code> image if you want the bleeding edge version.</p>\n<p>The configuration is all handled via environmental variables for easy Docker consumption within <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/cassandra-reaper/cassandra-reaper.env\">cassandra-reaper/cassandra-reaper.env</a>. For a list of all configuration options, see the <a href=\"http://cassandra-reaper.io/docs/configuration/docker_vars/\">Reaper documentation</a>.</p>\n<p>The ports that are exposed onto <code class=\"highlighter-rouge\">localhost</code> are <code class=\"highlighter-rouge\">8080</code> and <code class=\"highlighter-rouge\">8181</code> for the web UI and administration UI, respectively.</p>\n<h2 id=\"the-grafana-service\">The <code class=\"highlighter-rouge\">grafana</code> Service</h2>\n<p>The <code class=\"highlighter-rouge\">grafana</code> service uses the <a href=\"https://hub.docker.com/r/grafana/grafana/\">grafana/grafana</a> image from Docker Hub and is configured using the <code class=\"highlighter-rouge\">grafana/grafana.env</code>. For a list of all configuration options via environmental variables, visit the <a href=\"http://docs.grafana.org/installation/configuration/#using-environment-variables\">Grafana documentation</a>.</p>\n<p>The two scripts included in <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/tree/master/grafana/bin\">grafana/bin/</a> are referenced in the <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap#meetup-workflow\">README.md</a> and are used to create data sources that rely on the Prometheus data store and uploads all the <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/tree/master/grafana/dashboards\">grafana/dashboard</a> JSON files into Grafana.</p>\n<p>The JSON files were meticulously created for two of our enterprise customer’s production deployments (shared here with permission). They include 7 dashboards that highlight specific Cassandra metrics in a drill-down fashion:</p>\n<ul><li>Overview</li>\n  <li>Read Path</li>\n  <li>Write Path</li>\n  <li>Client Connections</li>\n  <li>Alerts</li>\n  <li>Reaper</li>\n  <li>Big Picture</li>\n</ul><p>At TLP we typically always start off with the Overview dashboard. Depending on our metrics we’ll switch to look at the <a href=\"http://thelastpickle.com/blog/2017/12/05/datadog-tlp-dashboards.html\">Read Path, Write Path, or Client Connection dashboards</a> for further investigation. Do keep in mind that while these dashboards are a work in progress, they do include proper x/y-axis labeling, units, and tooltips/descriptions. The tooltips/descriptions should provide info in the following order, when relevant:</p>\n<ul><li>Description</li>\n  <li>Values</li>\n  <li>False Positives</li>\n  <li>Required Actions</li>\n  <li>Warning</li>\n</ul><p>The text is meant to accompany any alerts that are triggered via the auto-generated Alerts dashboard. This way Slack, PagerDuty, or email alerts contain some context into why the alert was triggered, what most likely culprits may be involved, and how to resolve the alert.</p>\n<p>Do note that while the other dashboards may have triggers set up, only the Alerts dashboard will fire the triggers since all the dashboards make use of Templating Variables for easy Environment, Data Center, and Host selection, which Grafana alerts do not yet support.</p>\n<p>If there are ever any issues around repair, take a look at the Reaper dashboard as well to monitor the Reaper for Apache Cassandra service.</p>\n<p>The Big Picture dashboard provides a 30,000 foot view of the cluster and is nice to reason about, but ultimately provides very little value other than showing the high-level trends of the Cassandra deployment being monitored.</p>\n<h2 id=\"the-logspout-service\">The <code class=\"highlighter-rouge\">logspout</code> Service</h2>\n<p>The <code class=\"highlighter-rouge\">logspout</code> service is a unique service across multiple aspects. While this section will cover some of the special use cases, feel free to skim this section but do try to intake as much information as possible since these sort of use cases will arise in your future docker-compose.yml creations, even if not today.</p>\n<p>At the top of the <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/docker-compose.yml\">docker-compose.yml</a> under the <code class=\"highlighter-rouge\">logspout</code> section, we define a <code class=\"highlighter-rouge\">build</code> parameter. This <code class=\"highlighter-rouge\">build</code> parameter references <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/logspout/Dockerfile\">logspout/Dockerfile</a> which has no real information since the <code class=\"highlighter-rouge\">FROM</code> image that our local image refers to uses a few <code class=\"highlighter-rouge\">ONBUILD</code> commands. These commands are defined in the <a href=\"https://github.com/gliderlabs/logspout/blob/master/Dockerfile#L9-L11\">parent image</a> and make use of <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/logspout/modules.go\">logspout/modules.go</a>. Our custom logspout/modules.go installs the required Logstash dependencies for use with pre-existing Logstash deployments.</p>\n<p>While <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/logspout/build.sh\">logspout/build.sh</a> was not required to be duplicated since the parent image already had the file pre-baked, I did so for safekeeping.</p>\n<p>The <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/logspout/logspout.env\">logspout/logspout.env</a> file includes a few commented out settings I felt were interesting to look at if I were to experiement with Logstash in the future. These might be a good starting point for further investigation.</p>\n<p>The <code class=\"highlighter-rouge\">logspout</code> service also uses the <code class=\"highlighter-rouge\">restart: always</code> setting to ensure that any possible issues with the log redirection service will automatically be resolved by restarting the service immediately after failure.</p>\n<p>Within the <code class=\"highlighter-rouge\">logspout</code> service, we are redirecting the container’s port <code class=\"highlighter-rouge\">80</code> to our localhost’s port <code class=\"highlighter-rouge\">8000</code>. This allows us to <code class=\"highlighter-rouge\">curl http://localhost:8000/logs</code> from our local machine and grab the logs that the container was displaying on its own REST API under port <code class=\"highlighter-rouge\">80</code>.</p>\n<p>In order for any of the <code class=\"highlighter-rouge\">logspout</code> container magic to work, we need to bind our localhost’s <code class=\"highlighter-rouge\">/var/run/docker.sock</code> into the container as a read-only mount. Even though the mount is read-only, there are still security concerns for doing such an operation. Since this line is still required for allowing this logging redirection to occur, I did include two links to further clarify the security risks involved with including this container within production environments:</p>\n<ul><li>https://raesene.github.io/blog/2016/03/06/The-Dangers-Of-Docker.sock/</li>\n  <li>http://stackoverflow.com/questions/40844197</li>\n</ul><p>Perhaps in the future the Docker/Moby team will provide a more secure workaround.</p>\n<p>The last line that has not been mentioned is the <code class=\"highlighter-rouge\">command</code> option which is commented out by default within the <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/docker-compose.yml\">docker-compose.yml</a>. By uncommenting the <code class=\"highlighter-rouge\">command</code> option, we stop using the default <code class=\"highlighter-rouge\">command</code> provided in the parent Dockerfile which exposes the logs via an REST API and we begin to send our logs to the <a href=\"https://papertrailapp.com\">PaperTrail</a> website which provides 100 MB/month of hosted logs for free.</p>\n<p>In order to isolate checked-in code from each developer’s <code class=\"highlighter-rouge\">$PAPERTRAIL_PORT</code>, each developer must locally create a copy of <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/.env.template\">.env.template</a> within their project’s directory under the filename <code class=\"highlighter-rouge\">.env</code>. Within this <code class=\"highlighter-rouge\">.env</code> file we can define environmental variables that will be used by <code class=\"highlighter-rouge\">docker-compose.yml</code>. Once you have an account for PaperTrail, set <code class=\"highlighter-rouge\">PAPERTRAIL_PORT</code> to the port assigned by PaperTrail to begin seeing your logs within <a href=\"https://papertrailapp.com\">PaperTrail.com</a>. And since <code class=\"highlighter-rouge\">.env</code> is setup to be ignored via <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/.gitignore\">.gitignore</a> we do not have to worry about sending multiple developer’s logs to the same location.</p>\n<h2 id=\"the-prometheus-service\">The <code class=\"highlighter-rouge\">prometheus</code> Service</h2>\n<p>The <code class=\"highlighter-rouge\">prometheus</code> service is one of the simpler services within this Docker Compose environment.</p>\n<p>The <code class=\"highlighter-rouge\">prometheus</code> service:</p>\n<ul><li>Uses a <a href=\"https://hub.docker.com/r/prom/prometheus/\">Docker Hub image</a>.</li>\n  <li>Will require access to both the <code class=\"highlighter-rouge\">cassandra</code> and <code class=\"highlighter-rouge\">cassandra-reaper</code> services to monitor their processes.</li>\n  <li>Will expose the container’s port <code class=\"highlighter-rouge\">9090</code> onto localhost’s port <code class=\"highlighter-rouge\">9090</code>.</li>\n  <li>Will persist all data within the container’s <code class=\"highlighter-rouge\">/prometheus</code> directory onto our local <code class=\"highlighter-rouge\">./data/prometheus</code> directory.</li>\n  <li>Will configure the container using a locally-stored copy of the <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/prometheus/config/prometheus.yml\">prometheus.yml</a>.</li>\n</ul><p>The <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/prometheus/config/prometheus.yml\">prometheus.yml</a> defines which REST endpoints Prometheus will consume to gather metrics from as well as a few other documented configurations. Prometheus will collect metrics from the following services:</p>\n<ul><li>Prometheus.</li>\n  <li>Cassandra.\n    <ul><li>Cassandra internal metrics.</li>\n      <li>collectd metrics.</li>\n    </ul></li>\n  <li>Reaper for Apache Cassandra.</li>\n</ul><h2 id=\"the-pickle-factory-sample-write-application-service\">The <code class=\"highlighter-rouge\">pickle-factory</code> Sample Write Application Service</h2>\n<p>The <code class=\"highlighter-rouge\">pickle-factory</code>, which is a misnomer and should be “pickle-farm” in hindsight, is a sample application that shows an ideal write pattern.</p>\n<p>For production and development purposes, the <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/pickle-factory/Dockerfile\">pickle-factory/Dockerfile</a> uses the line <code class=\"highlighter-rouge\">COPY . .</code> to copy all of the “microservices” code into the Docker image. The <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/docker-compose.yml\">docker-compose.yml</a> uses the <code class=\"highlighter-rouge\">volume</code> key to overwrite any pre-baked code to allow for a simpler development workflow.</p>\n<p>The ideal development to production workflow would be to modify the <code class=\"highlighter-rouge\">pickle-factory/factory.py</code> file directly and have each saved copy refreshed within the running container. Once all changes were tested and committed to <code class=\"highlighter-rouge\">master</code>, a continuous integration (CI) job will build the new Docker images and push those images to Docker Hub for both development and production consumption. The next developer to modify <code class=\"highlighter-rouge\">pickle-factory/factory.py</code> will grab the latest image but use their local copy of <code class=\"highlighter-rouge\">pickle-factory/factory.py</code>. The next time that the production image gets updated it will include the latest copy of <code class=\"highlighter-rouge\">pickle-factory/factory.py</code>, as found in master, directly in the Docker image.</p>\n<p>Included in the Dockerfile is commented out code for installing <code class=\"highlighter-rouge\">gosu</code>, a <code class=\"highlighter-rouge\">sudo</code> replacement for Docker written in Go. <code class=\"highlighter-rouge\">gosu</code> allows the main user to spawn another thread under a non-root user to better contain attackers. Theoretically, if an attacker gains access into the <code class=\"highlighter-rouge\">pickle-factory</code> container, they would do so under a non-privileged user and not be able to potentially breakout of the container into the Docker internals and out onto the host machine. Practically, <code class=\"highlighter-rouge\">gosu</code> limits the possible attack surface on a Docker container. To fully use the provided functionality of <code class=\"highlighter-rouge\">gosu</code>, <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/pickle-factory/docker-entrypoint.sh\">pickle-factory/docker-entrypoint.sh</a> needs to be modified as well.</p>\n<p>The <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/pickle-factory/factory.py\">pickle-factory/factory.py</a> makes a few important design decisions:</p>\n<ul><li>Grabs configurations from environmental variables instead of configuration files for easier Docker-first configurations.</li>\n  <li>The <code class=\"highlighter-rouge\">Cluster()</code> object uses the <code class=\"highlighter-rouge\">DCAwareRoundRobinPolicy</code> in preparation for multiple data centers.</li>\n  <li>The C-based <code class=\"highlighter-rouge\">LibevConnection</code> event loop provider is used since it’s more performant than the Python-native default event loop provider.</li>\n  <li>All statements are prepared in advance to minimize request payload sizes.\n    <ul><li>In the background, the Cassandra nodes will send the statements around to each node in the cluster.</li>\n      <li>The statements will be indexed via a simple integer.</li>\n      <li>All subsequent requests will map the simple integer to the pre-parsed CQL statement.</li>\n    </ul></li>\n  <li>Employee records are generated and written asynchronously instead of synchronously in an effort to improve throughput.</li>\n  <li>Workforce data is denormalized and written asynchronously with a max-buffer to ensure we don’t overload Cassandra with too many in-flight requests.</li>\n  <li>If any requests did not complete successfully the <code class=\"highlighter-rouge\">future.result()</code> call will throw the matching exception.</li>\n</ul><h2 id=\"the-pickle-shop-sample-read-application-service\">The <code class=\"highlighter-rouge\">pickle-shop</code> Sample Read Application Service</h2>\n<p>Much like the <code class=\"highlighter-rouge\">pickle-factory</code> microservice, the <code class=\"highlighter-rouge\">pickle-shop</code> service follows a similar workflow. However, the <code class=\"highlighter-rouge\">pickle-shop</code> service definition within <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/docker-compose.yml\">docker-compose.yml</a> will not overwrite the application code within <code class=\"highlighter-rouge\">/usr/src/app</code> and instead require a rebuilding the local image by using:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>docker-compose build\n</pre></div></div>\n<p>This workflow was chosen as a way to differentiate between a development workflow (<code class=\"highlighter-rouge\">pickle-factory</code>) and a production workflow (<code class=\"highlighter-rouge\">pickle-shop</code>). For production images, all code should ideally be baked into the Docker image and shipped without any external dependencies such as a codebase dependency.</p>\n<p><a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/pickle-shop/shop.py\">pickle-shop/shop.py</a> follows the same overall flow as <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap/blob/master/pickle-factory/factory.py\">pickle-factory/factory.py</a>. After preparing the statements, the read-heavy “microservice” completes the following actions:</p>\n<ul><li>Performs a synchronous read request to grab all the employee IDs.</li>\n  <li>10 asynchronous read queries are performed for 10 employees.</li>\n  <li>We then process all results at the roughly the same time.</li>\n</ul><p>If using a non-asynchronous driver, one would probably follow the alternate workflow:</p>\n<ul><li>Perform 1 synchronous read query for 1 of 10 employees.</li>\n  <li>Process the results of the employee query.</li>\n  <li>Repeat.</li>\n</ul><p>However, following the synchronous workflow will take roughly:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>O(N), where N is the number of employees waiting to be processed\n</pre></div></div>\n<p>Following the asynchronous workflow we will roughly take:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>O(max(N)), where max(N) is the maximum amount of time an employee query will take.\n</pre></div></div>\n<p>Simplified, the asynchronous workflow will roughly take:</p>\n<p>At the end of this series of information, we have covered:</p>\n<ul><li>Docker</li>\n  <li>Docker Compose</li>\n  <li>A few docker-compose.yml settings.</li>\n  <li>Dockerized Cassandra.</li>\n  <li>Helper Docker Compose services.</li>\n  <li>Dockerized Reaper for Apache Cassandra.</li>\n  <li>Dockerized Grafana.</li>\n  <li>Logspout for Docker-driven external logging.</li>\n  <li>Dockerized Prometheus.</li>\n  <li>A sample write-heavy asynchronous application using Cassandra.</li>\n  <li>A sample read-heavy asynchronous application using Cassandra.</li>\n</ul><p>TLP’s hope is that the above documentation and <a href=\"https://github.com/thelastpickle/docker-cassandra-bootstrap\">minimal Docker Compose ecosystem</a> will provide a starting point for future community Proof of Concepts utilizing Apache Cassandra. With this project, each developer can create, maintain, and develop within their own local environment without any external overhead other than Docker and Docker Compose.</p>\n<p>Once the POC has reached a place of stability, simply adding the project to a Continuous Integration workflow to publish tested images will allow for proper Release Management. After that point, the responsibility typically falls onto the DevOps team to grab the latest Docker image, replace any previously existing Docker containers, and launch the new Docker image. This would all occur without any complicated hand off consisting of dependencies, configuration files (since we’re using environmental variables), and OS-specific requirements.</p>\n<p>Hopefully you will find Docker Compose to be as powerful as I’ve found it to be! Best of luck in cranking out the new POC you’ve been itching to create!</p>",
        "created_at": "2018-10-24T18:04:38+0000",
        "updated_at": "2018-10-24T18:04:45+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 22,
        "domain_name": "thelastpickle.com",
        "preview_picture": "http://thelastpickle.com/android-chrome-192x192.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12462"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 902,
            "label": "kubernetes",
            "slug": "kubernetes"
          },
          {
            "id": 1336,
            "label": "yugabyte",
            "slug": "yugabyte"
          }
        ],
        "is_public": false,
        "id": 12461,
        "uid": null,
        "title": "Understanding How YugaByte DB Runs on Kubernetes – The YugaByte Database Blog",
        "url": "https://blog.yugabyte.com/understanding-how-yugabyte-db-runs-on-kubernetes/",
        "content": "<p>As we reviewed in <a href=\"https://blog.yugabyte.com/docker-kubernetes-and-the-rise-of-cloud-native-databases/\">“Docker, Kubernetes and the Rise of Cloud Native Databases”</a>, Kubernetes has benefited from rapid adoption to become the de-facto choice for container orchestration. This has happened in a short span of only 4 years since Google open sourced the project in 2014. YugaByte DB’s <a href=\"https://blog.yugabyte.com/how-does-the-raft-consensus-based-replication-protocol-work-in-yugabyte-db/\">automated sharding and strongly consistent replication architecture</a> lends itself extremely well to containerized deployments powered by Kubernetes orchestration. In this post we’ll look at the various components involved in getting YugaByte DB up and running as <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\">Kubernetes StatefulSets</a>.</p><h2>YugaByte DB Architecture</h2><p>As shown in the architecture diagram below, YugaByte DB is comprised of two types of distributed services.</p><ul><li><strong>YB-Master:</strong> Responsible for keeping system metadata (such as shard-to-node mapping), coordinating system-wide operations (such as create/alter drop tables), and initiating maintenance operations (such as load-balancing). For fault-tolerance purposes, the number of YB-Masters equals the Replication Factor (RF) of the cluster. The minimum RF needed for fault-tolerance is 3.</li>\n<li><strong>YB-TServer:</strong> The data nodes responsible for hosting/serving user data in shards (also known as tablets). The number of data nodes can be increased or decreased on-demand in a cluster.</li>\n</ul><div id=\"attachment_286\" class=\"wp-caption aligncenter\"><img class=\"wp-image-286\" src=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturea-300x140.png\" alt=\"\" width=\"566\" height=\"264\" srcset=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturea-300x140.png 300w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturea-768x360.png 768w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturea.png 974w\" /><p class=\"wp-caption-text\">Architecture of a YugaByte DB Cluster</p></div><h2>Modeling YugaByte DB as a Workload on Kubernetes</h2><p><a href=\"https://blog.yugabyte.com/orchestrating-stateful-apps-with-kubernetes-statefulsets/\">Orchestrating Stateful Apps with Kubernetes</a> highlights how running stateful applications such as databases in Kubernetes require the use of the <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\">StatefulSets workload API</a>. In this context, YB-Master and YB-TServer are modeled as independent StatefulSets, as shown in the <a href=\"https://github.com/YugaByte/yugabyte-db/tree/master/cloud/kubernetes\">YugaByte DB Kubernetes YAML</a> on Github. Each of these StatefulSet pods instantiate one instance of the same <code>yugabytedb/yugabyte</code> container image but the command used to start the container is changed based on the type of server needed. The next few sections detail how exactly the YugaByte DB StatefulSets are structured in the context of running a four-node RF3 cluster on Kubernetes.</p><h2>Running YB-Master on Kubernetes</h2><p>The YB-Master deployment on Kubernetes needs one StatefulSet and two Services. One of these Services is the headless service that enables discovery of the underlying StatefulSet pods and the other is a LoadBalancer service needed to view the YB-Master Admin UI. YugaByte DB admin clients (such as the YugaByte DB EE Admin Console) connect to the any of the pods using the headless service, while admin users can connect to the LoadBalancer service.</p><div id=\"attachment_287\" class=\"wp-caption aligncenter\"><img class=\"wp-image-287\" src=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Pictureb-300x176.png\" alt=\"\" width=\"598\" height=\"351\" srcset=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Pictureb-300x176.png 300w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Pictureb-768x449.png 768w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Pictureb.png 974w\" /><p class=\"wp-caption-text\">YB-Master on Kubernetes</p></div><h3>yb-master StatefulSet</h3><p>The <a href=\"https://github.com/YugaByte/yugabyte-db/blob/master/cloud/kubernetes/yugabyte-statefulset.yaml\">YugaByte DB Kubernetes YAML</a> has a section for the yb-master StatefulSet. A few points to highlight in the specification.</p><p><strong>Replica count</strong></p><p>As shown in the figure above, to setup a 4-node YugaByte DB cluster with RF3, only 3 yb-master StatefulSet pods are needed. Hence the YAML setting for replicas to 3.</p><p><strong>Pod anti-affinity</strong></p><p>Pod anti-affinity rules are used to ensure no two yb-master pods can be scheduled onto the same Kubernetes node. This enforces maximum fault tolerance possible since a single node failure will only impact one yb-master pod and the cluster will continue functioning normally even with the remaining two yb-master pods on the two other nodes. Additionally, as noted in Kubernetes docs, the <code>preferredDuringSchedulingIgnoredDuringExecution</code> is a soft guarantee from Kubernetes that is better set to <code>requiredDuringSchedulingIgnoredDuringExecution</code> in mission-critical environments such as production.</p><p><strong>Communicating with other yb-masters</strong></p><p>Each yb-master gets to know of the other yb-masters with the –master_addresses flag populated using the fully qualified endpoint of the <code>yb-masters</code> headless service <code>yb-masters.default.svc.cluster.local:7100</code> (see next section).</p><p><strong>Ports</strong></p><p>The rpc port where other yb-masters and yb-tservers communicate is <code>7100</code> while the UI port for checking the current state of the master is <code>7000</code>.</p><p><strong>Volume mounts</strong></p><p>The <code>--fs_data_dirs</code> flag in the command points to the same disk <code>/mnt/data0</code> that is mounted to the container using the <code>datadir</code> volume mount.</p><p><strong>Update strategy</strong></p><p>The <code>RollingUpdate</code> strategy will update all the pods in the yb-master StatefulSet, in reverse ordinal order, while respecting the StatefulSet guarantees.</p><h3>yb-masters Headless service</h3><p>Kubernetes StatefulSets require the use of a headless service so that the StatefulSet pods can be discovered individually and communicated directly by other services (such as client applications). Kubernetes is not responsible for any load balancing across these pods. Such a headless service is created by simply specifying the clusterIP of the service to be <code>None</code>.</p><p>As shown above, the yb-masters headless service yaml is extremely simple. It simply opens up the UI and the rpc ports of the underlying yb-master pods.</p><h3>yb-master-ui LoadBalancer service</h3><p>The clusterwide admin UI for the yb-master can be viewed at the <code>7000</code> port of any yb-master. The <code>yb-master-ui</code> service is of the <code>LoadBalancer</code> type for this port which means that the service will load balance all the incoming requests across all the underlying pods.</p><h2>Running YB-TServer on Kubernetes</h2><p>Assuming you don’t need to view the YB-TServer’s Admin UI, the YB-TServer Kubernetes deployment needs one StatefulSet and one headless service. One important point to note is that the YB-Master service has to be up and running before the YB-TServer service.</p><div id=\"attachment_288\" class=\"wp-caption aligncenter\"><img class=\"wp-image-288\" src=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturec-300x152.png\" alt=\"\" width=\"572\" height=\"290\" srcset=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturec-300x152.png 300w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturec-768x390.png 768w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturec.png 974w\" /><p class=\"wp-caption-text\">YB-TServer and YB-Master on Kubernetes</p></div><h3>yb-tserver StatefulSet</h3><p>The YugaByte DB Kubernetes YAML has a section for the yb-master StatefulSet. A few points to highlight in the specification.</p><p><strong>Replica count</strong></p><p>As shown in the figure above, to setup a 4-node YugaByte DB cluster with RF3, 4 replicas of the yb-master StatefulSet pod are needed.</p><p><strong>Pod anti-affinity</strong></p><p>Pod anti-affinity rules are used to ensure no two yb-tserver pods can be scheduled onto the same Kubernetes node. This enforces maximum fault tolerance possible since a single node failure will only impact one yb-tserver pod and the cluster will continue functioning normally as long as quorum of yb-tserver pods are available. Note that 1 yb-tserver pod and 1 yb-master pod can be located on the same node. Again as noted in the yb-masters section, a stronger guarantee like <code>requiredDuringSchedulingIgnoredDuringExecution</code> is better for mission-critical environments.</p><p><strong>Communicating with yb-masters</strong></p><p>Each yb-tserver gets to know of the other yb-masters with the <code>--tserver_master_addrs</code> flag populated using the fully qualified endpoint of the yb-masters headless service <code>yb-masters.default.svc.cluster.local:7100</code>.</p><p><strong>Ports</strong></p><p>The rpc port where yb-masters and other yb-tservers communicate is <code>9100</code> while the UI port for checking the current state of the tserver is <code>9000</code>. Additionally, YCQL (the Cassandra compatible API) is available at port <code>9042</code> and YEDIS (the Redis compatible API) is available at port <code>6379</code>. PostgreSQL API, currently in beta, can be enabled by adding the port <code>5433</code>.</p><p><strong>Volume mounts</strong></p><p>The <code>--fs_data_dirs</code> flag points to the same disk /mnt/data0 that is mounted to the container using the datadir volume mount.</p><p><strong>Update strategy</strong></p><p>The <code>RollingUpdate</code> update strategy will update all the pods in the yb-tserver StatefulSet, in reverse ordinal order, while respecting the StatefulSet guarantees.</p><h3>yb-tservers Headless service</h3><p>As expected, the yb-tservers headless service yaml is extremely simple. It opens up the UI, the rpc ports as well as client API ports of the underlying yb-tserver pods.</p><h2>YugaByte DB on Kubernetes in Action</h2><p>In order to keep things simple to understand, we will run a 4-node YugaByte DB cluster on <a href=\"https://kubernetes.io/docs/setup/minikube/\">minikube</a>, the preferred method for running Kubernetes on your local environment.</p><h3>Prerequisites</h3><p>Follow the instructions to <a href=\"https://kubernetes.io/docs/tasks/tools/install-minikube/\">install minikube and kubectl</a> if you don’t have them setup already.</p><h3>Step 1 – Download the YugaByte DB Kubernetes YAML</h3><h3>Step 2 – Change the yb-tserver replica count from 3 to 4</h3><p>Open the the YAML in the editor of your choice and set the yb-tserver replica count to 4.</p><h3>Step 3 – Create the YugaByte DB cluster</h3><p>Now you can create the YugaByte DB cluster through the following command.</p><h3>Step 4 – Check status of the pods and services</h3><p>Since Kubernetes has to first pull the yugabytedb/yugabyte image from hub.docker.com, the cluster may take a few minutes to become live. You can check the status using the following commands.</p><p>When the cluster is ready, it will have all the 7 pods (3 for yb-master and 4 for yb-tserver) in the Running status.</p><p>You can also check the status of the 3 services we launched along with the status of the default kubernetes service itself.</p><p><code>NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE<br />kubernetes ClusterIP 10.96.0.1 443/TCP 10m<br />yb-master-ui LoadBalancer 10.102.121.64 7000:31283/TCP 8m<br />yb-masters ClusterIP None 7000/TCP,7100/TCP 8m<br />yb-tservers ClusterIP None 9000/TCP,9100/TCP,9042/TCP,6379/TCP 8m</code></p><p>Finally, you can view the nice UI dashboard provided by Kubernetes that you can launch by the following command.</p><div id=\"attachment_290\" class=\"wp-caption aligncenter\"><img class=\"wp-image-290\" src=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picture1d-300x153.png\" alt=\"\" width=\"626\" height=\"319\" srcset=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picture1d-300x153.png 300w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picture1d-768x392.png 768w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picture1d.png 974w\" /><p class=\"wp-caption-text\">Kubernetes Dashboard with YugaByte DB Installed</p></div><h3>Step 5 – View the YB-Master Admin UI</h3><p>Once the cluster is live, you can launch the YB-Master Admin UI. First use the command below to get the exact URL for the UI and then launch the URL via the browser.</p><div id=\"attachment_289\" class=\"wp-caption aligncenter\"><img class=\"wp-image-289\" src=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturee-300x162.png\" alt=\"\" width=\"603\" height=\"326\" srcset=\"https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturee-300x162.png 300w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturee-768x414.png 768w, https://3lr6t13cowm230cj0q42yphj-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Picturee.png 974w\" /><p class=\"wp-caption-text\">YB-Master Admin UI</p></div><h3>Step 6 – Perform Day 2 Operational Tasks</h3><p>The next few steps show how to perform common day 2 operational tasks such as adding/removing nodes and performing rolling upgrades. All these operations do not impact the availability and performance of client applications thus allowing the applications to continue to operate normally.</p><p><strong>Add a Node</strong></p><p>Horizontal scaling is a breeze with YugaByte DB and with Kubernetes, the process could not be simpler. All we have to do is to let Kubernetes know how many replicas to scale to.</p><p>Now we can check the status of the scaling operation. Note that YugaByte DB automatically moves a few tablet-leaders and a few tablet-followers into the newly added node so that the cluster remains balanced across all the nodes.</p><p><strong>Remove Two Nodes</strong></p><p>Removing nodes is also very simple. Reduce the number of replicas and see the combination of Kubernetes and YugaByte DB do the rest.<br /><code>kubectl scale statefulset yb-tserver --replicas=3</code></p><p>As expected in StatefulSets, we can see that the nodes with the largest ordinal indexes (i.e. 4 and 3) are removed first.</p><p><strong>Perform Rolling Upgrade</strong></p><p>We can also perform rolling upgrades on the YugaByte DB cluster. This involves changing the YugaByte DB container image to a different version first on the yb-master StatefulSet and then on the yb-tserver StatefulSet. As expected in StatefulSets, we can see that the nodes with the largest ordinal indexes are upgraded first.</p><p>Upgrading the yb-master StatefulSet uses the command below. Assuming the new container image is not already available with Kubernetes, the image will be pulled from hub.docker.com first and this may result in the first pod upgrade taking a few minutes.</p><p>Now we can upgrade the yb-tserver StatefulSet as well. This will lead to the yb-tserver pods getting upgraded in the same way we saw for the yb-master pods.</p><h2>Summary</h2><p>Running distributed databases using a distributed orchestration technology such as Kubernetes continues to remain a non-trivial problem. YugaByte DB is a distributed database with a unique sharding and replication architecture that makes it a perfect fit for Kubernetes-based orchestration. In this post, we reviewed the underlying details of how YugaByte DB runs on Kubernetes and how this looks in action in the context of a real cluster. As part of our upcoming 1.1 release, we expect to release additional Kubernetes-related enhancements such as running the YugaByte DB Enterprise Admin Console on the same Kubernetes cluster as YugaByte DB. Subscribe to our blog at the bottom of this page and stay tuned with our progress.</p><h2>What’s Next?</h2><ul><li>Read <a href=\"https://blog.yugabyte.com/orchestrating-stateful-apps-with-kubernetes-statefulsets/\">“Orchestrating Stateful Apps with Kubernetes.”</a></li>\n<li><a href=\"https://docs.yugabyte.com/latest/comparisons/\">Compare YugaByte DB to databases like Amazon DynamoDB, Cassandra, MongoDB and Azure Cosmos DB.</a></li>\n<li><a href=\"https://docs.yugabyte.com/latest/quick-start/\">Get started with YugaByte DB on Kubernetes.</a></li>\n<li><a href=\"https://www.yugabyte.com/about/contact/\">Contact us</a> to learn more about licensing, pricing or to schedule a technical overview.</li>\n</ul>",
        "created_at": "2018-10-24T18:01:47+0000",
        "updated_at": "2018-10-24T18:01:54+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 9,
        "domain_name": "blog.yugabyte.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12461"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          },
          {
            "id": 902,
            "label": "kubernetes",
            "slug": "kubernetes"
          }
        ],
        "is_public": false,
        "id": 12460,
        "uid": null,
        "title": "Deploy a highly-available Cassandra cluster in AWS using Kubernetes",
        "url": "https://medium.com/merapar/deploy-a-high-available-cassandra-cluster-in-aws-using-kubernetes-bd8ba07bfcdd",
        "content": "<div class=\"section-inner sectionLayout--insetColumn\"><figure id=\"9017\" class=\"graf graf--figure graf--leading\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*kdSlB9fHygMjSY-4ZC568A.png\" data-width=\"603\" data-height=\"304\" src=\"https://cdn-images-1.medium.com/max/1600/1*kdSlB9fHygMjSY-4ZC568A.png\" alt=\"image\" /></div></div></figure><h1 id=\"8079\" class=\"graf graf--h3 graf-after--figure graf--title\">Deploy a highly-available Cassandra cluster in AWS using Kubernetes</h1><p id=\"48e3\" class=\"graf graf--p graf-after--h3\">At Merapar, we use Cassandra as the back-bone for some of our highly-available cloud solutions. Cassandra is a highly available, highly performant, truly horizontal scalable NoSQL database. However, deploying Cassandra in the cloud in a highly-available manner is a non-trivial task and needs proper configuration. Fortunately, due to the emergence of new technologies, deploying a Cassandra cluster in AWS is more easy nowadays. This blog will show you how to deploy a Cassandra cluster in AWS, using kops (Kubernetes Operations). It will also show that the setup is highly-available by testing some failure scenarios.</p><p id=\"e322\" class=\"graf graf--p graf-after--p\">This blog assumes basic knowledge of AWS, Kubernetes and Cassandra.</p><h3 id=\"8bab\" class=\"graf graf--h3 graf-after--p\">Deployment</h3><p id=\"1d48\" class=\"graf graf--p graf-after--h3\">The following picture shows what the final Cassandra deployment in AWS looks like:</p></div><div class=\"section-inner sectionLayout--outsetColumn\"><figure id=\"4ae7\" class=\"graf graf--figure graf--layoutOutsetCenter graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*mp72MLMoTnlsnR0VBLomCw.png\" data-width=\"1879\" data-height=\"698\" data-action=\"zoom\" data-action-value=\"1*mp72MLMoTnlsnR0VBLomCw.png\" src=\"https://cdn-images-1.medium.com/max/2000/1*mp72MLMoTnlsnR0VBLomCw.png\" alt=\"image\" /></div></div></figure></div><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"b45b\" class=\"graf graf--p graf-after--figure\">Let’s describe the deployment in detail:</p><ul class=\"postList\"><li id=\"1ae9\" class=\"graf graf--li graf-after--p\">The deployment uses one geographical region: eu-west-1 (Ireland)</li><li id=\"f37e\" class=\"graf graf--li graf-after--li\">Within this region, three availability zones are used. An availability zone is an isolated area within a region, i.e. a failure in one availability zone does not impact operations in another availability zone. Hence, high-availability is achieved by spreading processing and storage across availability zones.</li><li id=\"8ccd\" class=\"graf graf--li graf-after--li\">There are three Kubernetes masters, one in each availability zone. A Kubernetes master schedules the pods (containers), that is, it determines which Kubernetes node runs which pod. By having a master in each availability-zone, the Kubernetes masters can continue to schedule pods, even if one availability-zone is down. The master machines are deployed in an auto-scaling group, so if an EC2 instance in this group terminates, a new EC2 instance (and Kubernetes master) is started automatically.</li><li id=\"11ba\" class=\"graf graf--li graf-after--li\">There are six Kubernetes nodes, two in each availability zone. A Kubernetes node runs your application pods. In this setup, each Kubernetes node will run one Cassandra pod. If one pod fails, a new one will be scheduled by the Kubernetes master. The Kubernetes nodes are also deployed in an auto-scaling group.</li><li id=\"e961\" class=\"graf graf--li graf-after--li\">A Kubernetes stateful set with persistent volumes is used to deploy Cassandra: each Cassandra pod has a known identity (e.g. cassandra-0) and a known volume (e.g. cassandra-storage-cassandra-0). The pod identity and volume identity are tightly coupled. This enables a Cassandra pod to restart on another node and transfer its state. When a Cassandra pod starts, it attaches the same EBS volume as previously, and therefore, has the same state as before. EBS volumes are automatically created the first time a Cassandra pod starts.</li></ul><p id=\"bbe6\" class=\"graf graf--p graf-after--li\">Now let’s discuss how Cassandra has to be configured in order to replicate the data in multiple availability-zones. This is achieved by setting the following properties:</p><ul class=\"postList\"><li id=\"49bd\" class=\"graf graf--li graf-after--p\">Snitch.<br />The snitch determines to which data-center and rack a node belongs. Cassandra uses the terms “data-center” and “rack” to identify the network topology. The EC2Snitch is used. When a Cassandra node starts, the EC2Snitch retrieves the region and availability-zone information from the EC2 meta-data endpoint: the data-center is set to the region and the rack is set to the availability-zone.</li><li id=\"6f31\" class=\"graf graf--li graf-after--li\">Replication factor.<br />This determines the number of data copies. A replication-factor of three is used, i.e. three replicas are stored on different nodes.</li><li id=\"a846\" class=\"graf graf--li graf-after--li\">Replication strategy<br />This property determines which nodes store the replicas. The NetworkTopologyStrategy is used. With this strategy, a replica is stored in each availability-zone. How this is done precisely, is discussed next.</li></ul><p id=\"3fed\" class=\"graf graf--p graf-after--li\">The following picture shows how data is stored. Each Cassandra node creates multiple tokens (32 in our case). Each token is a random number between -2⁶³ to 2⁶³ -1. The Cassandra token ring is a virtual ring in token order of all tokens (192 in our case: 32 tokens * 6 nodes). When a row must be stored, its key is hashed and the result determines where on the ring the record is stored. Imagine the key of a record is hashed to a number between 1829762156858167353 and 1843966738638345890. In the picture below you can see that this corresponds to the top token. This token belongs to cassandra-2. This means cassandra-2 in zone 1b stores the record. Subsequently, the ring is followed clockwise until two additional nodes are found in other availability-zones. In this example, the other replicas are stored on cassandra-3 and cassandra-1.</p><figure id=\"0982\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*p5_BEyYVQ8LK7S6X6gdDqg.png\" data-width=\"1112\" data-height=\"458\" data-action=\"zoom\" data-action-value=\"1*p5_BEyYVQ8LK7S6X6gdDqg.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*p5_BEyYVQ8LK7S6X6gdDqg.png\" alt=\"image\" /></div></div></figure><h3 id=\"cba7\" class=\"graf graf--h3 graf-after--figure\">Step-by-step setup</h3><p id=\"c8bf\" class=\"graf graf--p graf-after--h3\">Next we will install the described deployment from scratch. In order to follow these steps, you only need an AWS account and a Linux machine to run the commands from. We use the following versions for the various components:</p><ul class=\"postList\"><li id=\"424d\" class=\"graf graf--li graf-after--p\">Ubuntu 16.04</li><li id=\"2ef5\" class=\"graf graf--li graf-after--li\">Kops 1.8.1</li><li id=\"3304\" class=\"graf graf--li graf-after--li\">Kubernetes 1.7.16</li><li id=\"8e88\" class=\"graf graf--li graf-after--li\">Cassandra 2.2.9</li></ul><p id=\"3c6f\" class=\"graf graf--p graf-after--li\">We use Cassandra 2.2.9 in production with only a few changes from the default configuration. For details see: <a href=\"https://github.com/merapar/cassandra-docker/tree/master/docker\" data-href=\"https://github.com/merapar/cassandra-docker/tree/master/docker\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://github.com/merapar/cassandra-docker/tree/master/docker</a>. Note that this setup also works perfectly well with newer versions of Cassandra.</p><h4 id=\"01dc\" class=\"graf graf--h4 graf-after--p\">Kubernetes setup</h4><p id=\"bad1\" class=\"graf graf--p graf-after--h4\">First, you need the kops command, which is used to setup the infrastructure in AWS:</p><pre id=\"efd7\" class=\"graf graf--pre graf-after--p\">curl -LO <a href=\"https://github.com/kubernetes/kops/releases/download/1.8.1/kops-linux-amd64\" data-href=\"https://github.com/kubernetes/kops/releases/download/1.8.1/kops-linux-amd64\" class=\"markup--anchor markup--pre-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://github.com/kubernetes/kops/releases/download/1.8.1/kops-linux-amd64</a> <br />sudo mv kops-linux-amd64 /usr/local/bin/kops &amp;&amp; sudo chmod a+x /usr/local/bin/kops</pre><p id=\"f49d\" class=\"graf graf--p graf-after--pre\">You need the kubectl command to interact with the Kubernetes cluster in AWS:</p><pre id=\"6f62\" class=\"graf graf--pre graf-after--p\">curl -LO <a href=\"https://storage.googleapis.com/kubernetes-release/release/v1.7.16/bin/linux/amd64/kubectl\" data-href=\"https://storage.googleapis.com/kubernetes-release/release/v1.7.16/bin/linux/amd64/kubectl\" class=\"markup--anchor markup--pre-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://storage.googleapis.com/kubernetes-release/release/v1.7.16/bin/linux/amd64/kubectl</a><br />sudo mv kubectl /usr/local/bin/kubectl &amp;&amp; sudo chmod a+x /usr/local/bin/kubectl</pre><p id=\"a9d4\" class=\"graf graf--p graf-after--pre\">Kops uses the awscli command to interact with AWS. On Ubuntu, you can install this tool via:</p><pre id=\"20f2\" class=\"graf graf--pre graf-after--p\">apt-get install awscli</pre><p id=\"4265\" class=\"graf graf--p graf-after--pre\">Other means to install awscli can be found here: <a href=\"https://docs.aws.amazon.com/cli/latest/userguide/installing.html\" data-href=\"https://docs.aws.amazon.com/cli/latest/userguide/installing.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://docs.aws.amazon.com/cli/latest/userguide/installing.html</a></p><p id=\"7635\" class=\"graf graf--p graf-after--p\">Now you have to make sure there is an IAM user which kops can use to install the required components in AWS. The IAM user needs programmatic access (i.e. use an access-key and secret-access-key to login). In addition, the IAM user requires the following permissions: AmazonEC2FullAccess AmazonRoute53FullAccess AmazonS3FullAccess IAMFullAccess AmazonVPCFullAccess. More info can be found here: <a href=\"https://github.com/kubernetes/kops/blob/master/docs/aws.md\" data-href=\"https://github.com/kubernetes/kops/blob/master/docs/aws.md\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://github.com/kubernetes/kops/blob/master/docs/aws.md</a></p><p id=\"94fa\" class=\"graf graf--p graf-after--p\">Now let’s configure AWS:</p><pre id=\"3a53\" class=\"graf graf--pre graf-after--p\">aws configure</pre><p id=\"9692\" class=\"graf graf--p graf-after--pre\">Enter the credentials of the IAM user created in the previous step. We use eu-west-1 as the region. Accept the default output format. Next you have to create an S3 bucket. Kops stores the configuration of the deployment in this bucket</p><pre id=\"cadd\" class=\"graf graf--pre graf-after--p\">aws s3api create-bucket --bucket kops-cassandra-blog --region eu-west-1</pre><p id=\"8122\" class=\"graf graf--p graf-after--pre\">Now generate a public/private key-pair:</p><pre id=\"90ce\" class=\"graf graf--pre graf-after--p\">ssh-keygen -f kops-cassandra-blog</pre><p id=\"50ef\" class=\"graf graf--p graf-after--pre\">This key-pair is used to access the EC2 machines. The following command creates the cluster definition:</p><pre id=\"fef7\" class=\"graf graf--pre graf-after--p\">kops create cluster \\<br />--cloud=aws \\<br />--name=kops-cassandra-blog.k8s.local \\<br />--zones=eu-west-1a,eu-west-1b,eu-west-1c \\<br />--master-size=\"t2.small\" \\<br />--master-zones=eu-west-1a,eu-west-1b,eu-west-1c \\<br />--node-size=\"t2.small\" \\<br />--ssh-public-key=\"kops-cassandra-blog.pub\" \\<br />--state=s3://kops-cassandra-blog \\<br />--node-count=6</pre><p id=\"0054\" class=\"graf graf--p graf-after--pre\">Now apply the cluster definition, i.e. create the actual resources in AWS:</p><pre id=\"5302\" class=\"graf graf--pre graf-after--p\">kops update cluster --name=kops-cassandra-blog.k8s.local --state=s3://kops-cassandra-blog --yes</pre><p id=\"cdb3\" class=\"graf graf--p graf-after--pre\">After a few minutes, we will have a high-available Kubernetes cluster in AWS. Kops automatically configures kubectl. Use the following command to check the Kubernetes master nodes (the -L argument shows labels, while the -l argument filters on labels):</p><pre id=\"aa24\" class=\"graf graf--pre graf-after--p\">kubectl get no -L failure-domain.beta.kubernetes.io/zone -l kubernetes.io/role=master</pre><p id=\"b7b1\" class=\"graf graf--p graf-after--pre\">The following output is visible:</p><pre id=\"9841\" class=\"graf graf--pre graf-after--p\">NAME               STATUS  AGE  VERSION  ZONE<br />ip-172-20-112-210  Ready   1m   v1.8.7   eu-west-1c<br />ip-172-20-58-140   Ready   1m   v1.8.7   eu-west-1a<br />ip-172-20-85-234   Ready   1m   v1.8.7   eu-west-1b</pre><p id=\"e324\" class=\"graf graf--p graf-after--pre\">As can be seen, the three Kubernetes masters each reside in a separate availability zone. Now run the same command for the Kubernetes nodes:</p><pre id=\"f986\" class=\"graf graf--pre graf-after--p\">kubectl get no -L failure-domain.beta.kubernetes.io/zone -l kubernetes.io/role=node</pre><p id=\"d73a\" class=\"graf graf--p graf-after--pre\">As can be seen in the output, each availability zone has two Kubernetes nodes:</p><pre id=\"6bd9\" class=\"graf graf--pre graf-after--p\">NAME               STATUS    AGE  VERSION  ZONE<br />ip-172-20-114-66   Ready     1m   v1.8.7   eu-west-1c<br />ip-172-20-116-132  Ready     1m   v1.8.7   eu-west-1c<br />ip-172-20-35-200   Ready     1m   v1.8.7   eu-west-1a<br />ip-172-20-42-220   Ready     1m   v1.8.7   eu-west-1a<br />ip-172-20-94-29    Ready     1m   v1.8.7   eu-west-1b<br />ip-172-20-94-34    Ready     1m   v1.8.7   eu-west-1b</pre><p id=\"c29f\" class=\"graf graf--p graf-after--pre\">You can destroy the environment at all times by running the following command:</p><pre id=\"6aad\" class=\"graf graf--pre graf-after--p\">kops delete cluster --name=kops-cassandra-blog.k8s.local --state=s3://kops-cassandra-blog --yes</pre><h4 id=\"7f99\" class=\"graf graf--h4 graf-after--pre\">Cassandra setup</h4><p id=\"d23d\" class=\"graf graf--p graf-after--h4\">Create a file cassandra.yml, containing the following definitions:</p><pre id=\"1206\" class=\"graf graf--pre graf-after--p\">apiVersion: v1<br />kind: Service<br />metadata:<br />  name: cassandra<br />spec:<br />  clusterIP: None<br />  ports:<br />    - name: cql<br />      port: 9042<br />  selector:<br />    app: cassandra<br />---<br />apiVersion: \"apps/v1beta1\"<br />kind: StatefulSet<br />metadata:<br />  name: cassandra<br />spec:<br />  serviceName: cassandra<br />  replicas: 6<br />  template:<br />    metadata:<br />      labels:<br />        app: cassandra<br />    spec:<br />      affinity:<br />        podAntiAffinity:<br />          requiredDuringSchedulingIgnoredDuringExecution:<br />          - topologyKey: kubernetes.io/hostname<br />            labelSelector:<br />              matchLabels:<br />                app: cassandra<br />      containers:<br />        - env:<br />            - name: MAX_HEAP_SIZE<br />              value: 512M<br />            - name: HEAP_NEWSIZE<br />              value: 512M<br />            - name: POD_IP<br />              valueFrom:<br />                fieldRef:<br />                  fieldPath: status.podIP<br />          image: merapar/cassandra:2.3<br />          name: cassandra<br />          volumeMounts:<br />            - mountPath: /cassandra-storage<br />              name: cassandra-storage<br />  volumeClaimTemplates:<br />  - metadata:<br />      name: cassandra-storage<br />    spec:<br />      accessModes:<br />      - ReadWriteOnce<br />      resources:<br />        requests:<br />          storage: 10Gi</pre><p id=\"c3fd\" class=\"graf graf--p graf-after--pre\">And run</p><pre id=\"d24e\" class=\"graf graf--pre graf-after--p\">kubectl create -f cassandra.yml</pre><p id=\"72a5\" class=\"graf graf--p graf-after--pre\">The following components are installed within the timespan of a few minutes:</p><ul class=\"postList\"><li id=\"40bd\" class=\"graf graf--li graf-after--p\">Service cassandra. This service is used by clients within the Kubernetes cluster to connect to Cassandra. It does not have a cluster-IP. This is on purpose because Cassandra node-discovery and load-balancing is handled by the Cassandra client itself (not via Kubernetes). The client library connects to one contact point only: the cassandra DNS name. This is translated by the DNS pod to the IP address of one of the Cassandra pods. That pod will tell the IP addresses of the other Cassandra pods.</li><li id=\"7b4d\" class=\"graf graf--li graf-after--li\">StatefulSet cassandra. The stateful set makes sure that there are six Cassandra pods running at all times with a fixed identity: cassandra-0 up to and including cassandra-5.</li></ul><p id=\"9d64\" class=\"graf graf--p graf-after--li\">In order to connect to the Cassandra cluster, we use the cqlsh command which is available on each node:</p><pre id=\"0531\" class=\"graf graf--pre graf-after--p\">kubectl exec -ti cassandra-0 cqlsh cassandra-0</pre><p id=\"1ce1\" class=\"graf graf--p graf-after--pre\">This opens a CQL prompt and lets you interact with the cluster using CQL. The command “cqlsh cassandra-0” actually connects to the server listed in the first argument (cassandra-0). So in this case, it connects to itself.</p><p id=\"bae3\" class=\"graf graf--p graf-after--p\">Now we are going to create a key-space, a table and 100 records. First set the consistency level:</p><pre id=\"1b02\" class=\"graf graf--pre graf-after--p\">CONSISTENCY QUORUM;</pre><p id=\"8c24\" class=\"graf graf--p graf-after--pre\">Quorum means that a majority of the replica’s (2 in our case) must be read or written in order for the read or write command to succeed. Now create the key-space:</p><pre id=\"c6b4\" class=\"graf graf--pre graf-after--p\">CREATE KEYSPACE test WITH REPLICATION = { 'class' : 'NetworkTopologyStrategy', 'eu-west' : 3 };</pre><p id=\"7daa\" class=\"graf graf--p graf-after--pre\">Switch to the test key-space:</p><pre id=\"84e8\" class=\"graf graf--pre graf-after--p\">USE test;</pre><p id=\"8856\" class=\"graf graf--p graf-after--pre\">Create a table</p><pre id=\"d230\" class=\"graf graf--pre graf-after--p\">CREATE TABLE persons (id uuid, name text, PRIMARY KEY (id));</pre><p id=\"f17f\" class=\"graf graf--p graf-after--pre\">Now run the following command 100 times to insert 100 records. We need enough records so that each node contains replicas.</p><pre id=\"97e0\" class=\"graf graf--pre graf-after--p\">INSERT INTO persons (id,name) VALUES (uuid(),'name');</pre><p id=\"3d53\" class=\"graf graf--p graf-after--pre\">You can also run a script from the cassandra-0 machine:</p><pre id=\"9e3d\" class=\"graf graf--pre graf-after--p\">kubectl exec -ti cassandra-0 bash</pre><p id=\"a11c\" class=\"graf graf--p graf-after--pre\">With the following loop:</p><pre id=\"b373\" class=\"graf graf--pre graf-after--p\">for i in {1..100}<br /> do<br />   echo \"adding customer $i\"<br />   cqlsh cassandra-0 -e \"USE test; CONSISTENCY QUORUM; INSERT INTO persons (id,name) VALUES (uuid(),'name');\"<br /> done</pre><p id=\"0d92\" class=\"graf graf--p graf-after--pre\">Now run the following command and make sure 100 records are returned:</p><pre id=\"4041\" class=\"graf graf--pre graf-after--p\">SELECT * FROM persons;</pre><h3 id=\"e3e9\" class=\"graf graf--h3 graf-after--pre\">Testing the high-availability</h3><p id=\"a154\" class=\"graf graf--p graf-after--h3\">Now that we have a Cassandra cluster in AWS with some data inside, we can test the high-availability. Note that this setup is only resilient against failures in one availability-zone. In order to be resilient against multiple concurrent availability-zone failures, one should consider using a disaster recovery site in another region. While running the failure scenarios in the following sections, the select query presented earlier on should always return 100 records, i.e. Cassandra should be high-available (all data should be available) at all times.</p><h4 id=\"6473\" class=\"graf graf--h4 graf-after--p\">EC2 instance failure</h4><p id=\"2b6f\" class=\"graf graf--p graf-after--h4\">Note that we will only test Kubernetes-node failure (not Kubernetes-master failure). Let’s terminate an EC2 instance via the AWS console. We should not terminate an EC2 instance running the cassandra-0 pod. Otherwise our CQL prompt terminates. Via the following commands:</p><pre id=\"a6e3\" class=\"graf graf--pre graf-after--p\">kubectl get no -L failure-domain.beta.kubernetes.io/zone<br />kubectl get po -o wide</pre><p id=\"fc8f\" class=\"graf graf--p graf-after--pre\">We can construct the following table:</p><pre id=\"eac3\" class=\"graf graf--pre graf-after--p\">Cassandra-node  EC2 instance       Availability-zone<br />----------------------------------------------------<br />cassandra-0     ip-172-20-94-34    eu-west-1b<br />cassandra-1     ip-172-20-116-132  eu-west-1c<br />cassandra-2     ip-172-20-42-220   eu-west-1a<br />cassandra-3     ip-172-20-94-29    eu-west-1b<br />cassandra-4     ip-172-20-114-66   eu-west-1c<br />cassandra-5     ip-172-20-35-200   eu-west-1a</pre><p id=\"8dca\" class=\"graf graf--p graf-after--pre\">For this test, we will terminate instance ip-172–20–116–132 which will terminate cassandra-1. AWS will try to launch a new EC2 instance in the availability zone with the fewest instances. In our case, the auto-scaling group called “nodes” contains one instance for the eu-west-1c zone, while it contains two for the other zones. Therefore, the new instance is launched in eu-west-1c. Note that this is best effort; if a whole availability-zone is down for an extended period of time, manual intervention is required to recover. This will be discussed later-on. When requesting the pods, the following output is visible:</p><pre id=\"70ec\" class=\"graf graf--pre graf-after--p\">NAME          READY     STATUS    RESTARTS   AGE<br />cassandra-0   1/1       Running   0          1h<br />cassandra-1   0/1       Pending   0          8s<br />cassandra-2   1/1       Running   1          1h<br />cassandra-3   1/1       Running   4          1h<br />cassandra-4   1/1       Running   0          1h<br />cassandra-5   1/1       Running   0          1h</pre><p id=\"52c3\" class=\"graf graf--p graf-after--pre\">While the EC2 instance is starting, the status of the pod is pending. The read-query, as expected, still returns 100 rows. This is because we use quorum reads: we need two of the three replicas and since all zones contain one replica, the read succeeds. Pod cassandra-1 will be rescheduled to the new EC2 instance. The following policies apply during rescheduling:</p><ul class=\"postList\"><li id=\"877e\" class=\"graf graf--li graf-after--p\">When a Kubernetes node is started, it automatically gets a label with availability-zone information. When Kubernetes schedules pods of a stateful set, it tries to spread them across the availability-zones. Persistent volumes (the EBS volume) of a pod are also located in a particular availability-zone. When a persistent volume is created, it too gets a label with availability-zone information. Now, when a pod is scheduled and claims a volume, Kubernetes makes sure the pod is scheduled to a node in the same availability zone as the volume. In our case, cassandra-1 must be rescheduled. This pod claims (wants to link) volume cassandra-storage-cassandra-1. Since this volume is located in zone eu-west-1c, cassandra-1 will get scheduled on a node running in eu-west-1c</li><li id=\"8aea\" class=\"graf graf--li graf-after--li\">We use anti-pod-affinity to make sure a Kubernetes node runs a maximum of one Cassandra pod. Although it is perfectly viable to run without this policy, it has two benefits: While the new EC2 instance is starting, the Cassandra pod is not started on the remaining nodes. Therefore, no manual rescheduling is required afterwards ( in order to balance the pods). The second benefit is that the full resources of the node are available for the Cassandra node. Note that the same can be achieved using other means (e.g. using resource quotas)</li></ul><h4 id=\"a30a\" class=\"graf graf--h4 graf-after--li\">Availability zone failure</h4><p id=\"892a\" class=\"graf graf--p graf-after--h4\">In order to test this scenario, we will terminate Cassandra nodes running in zone eu-west-1a: cassandra-2 (EC2 instance ip-172–20–42–220) and cassandra-5 (EC2 instance ip-172–20–35–200). The “get pods” command now shows only five nodes, of which one is in the pending state:</p><pre id=\"d71d\" class=\"graf graf--pre graf-after--p\">NAME          READY     STATUS    RESTARTS   AGE<br />cassandra-0   1/1       Running   0          2h<br />cassandra-1   1/1       Running   0          18m<br />cassandra-2   0/1       Pending   0          55s<br />cassandra-3   1/1       Running   4          2h<br />cassandra-4   1/1       Running   0          2h</pre><p id=\"386d\" class=\"graf graf--p graf-after--pre\">Since Cassandra replicates data across all zones, all data is still available. This can be confirmed by running the read query which still return 100 records.</p><p id=\"dcc0\" class=\"graf graf--p graf-after--p\">The recovery process for this scenario is basically the same as the single EC2 instance failure scenario describe in the previous scenario. Although very rare, an availability-zone can fail in such a way that instances cannot be restarted in the failing zone, but are started in another zone instead. More information can be found here: <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html\" data-href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html</a></p><p id=\"ab2d\" class=\"graf graf--p graf-after--p\">The previous scenario described how pods with volume claims are scheduled. Because there will be a mismatch between the new EC2 instance zone-info and the volume zone-info, the pods cannot reschedule. Manual intervention is required. When the failing availability-zone is up again, terminate all machines which could previously not start in the failing availability-zone and all should recover automatically.</p><h3 id=\"d549\" class=\"graf graf--h3 graf-after--p\">Final remarks</h3><p id=\"5e58\" class=\"graf graf--p graf-after--h3 graf--trailing\">In this post, we have shown how to deploy a high-available Cassandra cluster in AWS. We have also shown that the deployment automatically recovers from node failures in the same availability zone. In a next post, we will discuss scaling the cluster horizontally. Another topic, not discussed here, is performance, which might also be the subject of a future post.</p></div>",
        "created_at": "2018-10-24T17:55:31+0000",
        "updated_at": "2018-10-24T17:59:00+0000",
        "published_at": "2018-04-20T13:15:18+0000",
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 14,
        "domain_name": "medium.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*kdSlB9fHygMjSY-4ZC568A.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12460"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 15,
            "label": "tutorial",
            "slug": "tutorial"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 902,
            "label": "kubernetes",
            "slug": "kubernetes"
          }
        ],
        "is_public": false,
        "id": 12459,
        "uid": null,
        "title": "Example: Deploying Cassandra with Stateful Sets",
        "url": "https://kubernetes.io/docs/tutorials/stateful-application/cassandra/",
        "content": "Example: Deploying Cassandra with Stateful Sets - Kubernetes<header>\n    \n    </header><section id=\"hero\" class=\"light-text no-sub\"><h5><a href=\"https://kubernetes.io/docs/home/\">DOCUMENTATION</a>\n\t\t\n\t\t\n\t\t<a href=\"https://kubernetes.io/docs/setup/\">SETUP</a>\n\t\t\n\t\t\n\t\t<a href=\"https://kubernetes.io/docs/concepts/\">CONCEPTS</a>\n\t\t\n\t\t\n\t\t<a href=\"https://kubernetes.io/docs/tasks/\">TASKS</a>\n\t\t\n\t\t\n\t\t<a href=\"https://kubernetes.io/docs/tutorials/\" class=\"YAH\">TUTORIALS</a>\n\t\t\n\t\t\n\t\t<a href=\"https://kubernetes.io/docs/reference/\">REFERENCE</a>\n\t\t\n\t\t\n\t\t<a href=\"https://kubernetes.io/docs/contribute/\">CONTRIBUTE</a>\n\t\t\n\t</h5></section><section id=\"encyclopedia\"><div id=\"docsContent\"><p><a href=\"https://github.com/kubernetes/website/edit/master/content/en/docs/tutorials/stateful-application/cassandra.md\" id=\"editPageButton\" target=\"_blank\">Edit This Page</a></p><p>This tutorial shows you how to develop a native cloud <a href=\"http://cassandra.apache.org/\" target=\"_blank\">Cassandra</a> deployment on Kubernetes. In this example, a custom Cassandra <em>SeedProvider</em> enables Cassandra to discover new Cassandra nodes as they join the cluster.</p><p><em>StatefulSets</em> make it easier to deploy stateful applications within a clustered environment. For more information on the features used in this tutorial, see the <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\"><em>StatefulSet</em></a> documentation.</p><p><strong>Cassandra on Docker</strong></p><p>The <em>Pods</em> in this tutorial use the <a href=\"https://github.com/kubernetes/examples/blob/master/cassandra/image/Dockerfile\" target=\"_blank\"><code>gcr.io/google-samples/cassandra:v13</code></a>\nimage from Google’s <a href=\"https://cloud.google.com/container-registry/docs/\" target=\"_blank\">container registry</a>.\nThe Docker image above is based on <a href=\"https://github.com/kubernetes/kubernetes/tree/master/build/debian-base\" target=\"_blank\">debian-base</a>\nand includes OpenJDK 8.</p><p>This image includes a standard Cassandra installation from the Apache Debian repo.\nBy using environment variables you can change values that are inserted into <code>cassandra.yaml</code>.</p><table><thead><tr><th>ENV VAR</th>\n<th align=\"center\">DEFAULT VALUE</th>\n</tr></thead><tbody><tr><td><code>CASSANDRA_CLUSTER_NAME</code></td>\n<td align=\"center\"><code>'Test Cluster'</code></td>\n</tr><tr><td><code>CASSANDRA_NUM_TOKENS</code></td>\n<td align=\"center\"><code>32</code></td>\n</tr><tr><td><code>CASSANDRA_RPC_ADDRESS</code></td>\n<td align=\"center\"><code>0.0.0.0</code></td>\n</tr></tbody></table><ul id=\"markdown-toc\"><li><a href=\"#objectives\">Objectives</a></li>\n<li><a href=\"#before-you-begin\">Before you begin</a></li>\n<li><a href=\"#creating-a-cassandra-headless-service\">Creating a Cassandra Headless Service</a></li>\n<li><a href=\"#using-a-statefulset-to-create-a-cassandra-ring\">Using a StatefulSet to Create a Cassandra Ring</a></li>\n<li><a href=\"#validating-the-cassandra-statefulset\">Validating The Cassandra StatefulSet</a></li>\n<li><a href=\"#modifying-the-cassandra-statefulset\">Modifying the Cassandra StatefulSet</a></li>\n<li><a href=\"#cleaning-up\">Cleaning up</a></li>\n<li><a href=\"#what-s-next\">What's next</a></li>\n</ul><h2 id=\"objectives\">Objectives</h2><ul><li>Create and validate a Cassandra headless <a href=\"https://kubernetes.io/docs/concepts/services-networking/service/\"><em>Service</em></a>.</li>\n<li>Use a <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\">StatefulSet</a> to create a Cassandra ring.</li>\n<li>Validate the <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\">StatefulSet</a>.</li>\n<li>Modify the <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\">StatefulSet</a>.</li>\n<li>Delete the <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\">StatefulSet</a> and its <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/pod/\">Pods</a>.</li>\n</ul><h2 id=\"before-you-begin\">Before you begin</h2><p>To complete this tutorial, you should already have a basic familiarity with <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/pod/\">Pods</a>, <a href=\"https://kubernetes.io/docs/concepts/services-networking/service/\">Services</a>, and <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\">StatefulSets</a>. In addition, you should:</p><ul><li><p><a href=\"https://kubernetes.io/docs/tasks/tools/install-kubectl/\">Install and Configure</a> the <em>kubectl</em> command-line tool</p></li>\n<li><p>Download <a href=\"https://kubernetes.io/examples/application/cassandra/cassandra-service.yaml\"><code>cassandra-service.yaml</code></a>\nand <a href=\"https://kubernetes.io/examples/application/cassandra/cassandra-statefulset.yaml\"><code>cassandra-statefulset.yaml</code></a></p></li>\n<li><p>Have a supported Kubernetes cluster running</p></li>\n</ul><blockquote class=\"note\">\n  <p><strong>Note:</strong> Please read the <a href=\"https://kubernetes.io/docs/setup/pick-right-solution/\">getting started guides</a> if you do not already have a cluster.</p>\n</blockquote><h3 id=\"additional-minikube-setup-instructions\">Additional Minikube Setup Instructions</h3><blockquote class=\"caution\">\n  <div><p><strong>Caution:</strong> <a href=\"https://kubernetes.io/docs/getting-started-guides/minikube/\">Minikube</a> defaults to 1024MB of memory and 1 CPU. Running Minikube with the default resource configuration results in insufficient resource errors during this tutorial. To avoid these errors, start Minikube with the following settings:</p><div class=\"highlight\"><pre class=\"language-shell\" data-lang=\"shell\">minikube start --memory 5120 --cpus=4</pre></div></div>\n</blockquote><p>A Kubernetes <a href=\"https://kubernetes.io/docs/concepts/services-networking/service/\">Service</a> describes a set of <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/pod/\">Pods</a> that perform the same task.</p><p>The following <code>Service</code> is used for DNS lookups between Cassandra Pods and clients within the Kubernetes cluster.</p><table class=\"includecode\" id=\"application-cassandra-cassandra-service-yaml\"><thead><tr><th>\n                <a href=\"https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/application/cassandra/cassandra-service.yaml\" download=\"application/cassandra/cassandra-service.yaml\">\n                    <code>application/cassandra/cassandra-service.yaml</code>\n                </a>\n                <img src=\"https://d33wubrfki0l68.cloudfront.net/951ae1fcc65e28202164b32c13fa7ae04fab4a0b/b77dc/images/copycode.svg\" title=\"Copy application/cassandra/cassandra-service.yaml to clipboard\" alt=\"image\" /></th>\n        </tr></thead><tbody><tr><td><div class=\"highlight\"><pre class=\"language-yaml\" data-lang=\"yaml\">apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: cassandra\n  name: cassandra\nspec:\n  clusterIP: None\n  ports:\n  - port: 9042\n  selector:\n    app: cassandra\n</pre></div>  </td>\n        </tr></tbody></table><ol><li>Launch a terminal window in the directory you downloaded the manifest files.</li>\n<li><p>Create a Service to track all Cassandra StatefulSet nodes from the <code>cassandra-service.yaml</code> file:</p>\n<div class=\"highlight\"><pre class=\"language-shell\" data-lang=\"shell\">kubectl create -f https://k8s.io/examples/application/cassandra/cassandra-service.yaml</pre></div></li>\n</ol><h3 id=\"validating-optional\">Validating (optional)</h3><p>Get the Cassandra Service.</p><div class=\"highlight\"><pre class=\"language-shell\" data-lang=\"shell\">kubectl get svc cassandra</pre></div><p>The response is</p><pre>NAME        TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE\ncassandra   ClusterIP   None         &lt;none&gt;        9042/TCP   45s\n</pre><p>Service creation failed if anything else is returned. Read <a href=\"https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/\">Debug Services</a> for common issues.</p><p>The StatefulSet manifest, included below, creates a Cassandra ring that consists of three Pods.</p><blockquote class=\"note\">\n  <p><strong>Note:</strong> This example uses the default provisioner for Minikube. Please update the following StatefulSet for the cloud you are working with.</p>\n</blockquote><ol><li>Update the StatefulSet if necessary.</li>\n<li><p>Create the Cassandra StatefulSet from the <code>cassandra-statefulset.yaml</code> file:</p>\n<div class=\"highlight\"><pre class=\"language-shell\" data-lang=\"shell\">kubectl create -f https://k8s.io/examples/application/cassandra/cassandra-statefulset.yaml</pre></div></li>\n</ol><ol><li><p>Get the Cassandra StatefulSet:</p>\n<div class=\"highlight\"><pre class=\"language-shell\" data-lang=\"shell\">kubectl get statefulset cassandra</pre></div>\n<p>The response should be:</p>\n<pre>NAME        DESIRED   CURRENT   AGE\ncassandra   3         0         13s\n</pre>\n<p>The <code>StatefulSet</code> resource deploys Pods sequentially.</p></li>\n<li><p>Get the Pods to see the ordered creation status:</p>\n<div class=\"highlight\"><pre class=\"language-shell\" data-lang=\"shell\">kubectl get pods -l=\"app=cassandra\"</pre></div>\n<p>The response should be:</p>\n<div class=\"highlight\"><pre class=\"language-shell\" data-lang=\"shell\">NAME          READY     STATUS              RESTARTS   AGE\ncassandra-0   1/1       Running             0          1m\ncassandra-1   0/1       ContainerCreating   0          8s</pre></div>\n<p>It can take several minutes for all three Pods to deploy. Once they are deployed, the same command returns:</p>\n<pre>NAME          READY     STATUS    RESTARTS   AGE\ncassandra-0   1/1       Running   0          10m\ncassandra-1   1/1       Running   0          9m\ncassandra-2   1/1       Running   0          8m\n</pre></li>\n<li><p>Run the Cassandra <a href=\"https://wiki.apache.org/cassandra/NodeTool\" target=\"_blank\">nodetool</a> to display the status of the ring.</p>\n<div class=\"highlight\"><pre class=\"language-shell\" data-lang=\"shell\">kubectl exec -it cassandra-0 -- nodetool status</pre></div>\n<p>The response should look something like this:</p>\n<pre>Datacenter: DC1-K8Demo\n======================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address     Load       Tokens       Owns (effective)  Host ID                               Rack\nUN  172.17.0.5  83.57 KiB  32           74.0%             e2dd09e6-d9d3-477e-96c5-45094c08db0f  Rack1-K8Demo\nUN  172.17.0.4  101.04 KiB  32           58.8%             f89d6835-3a42-4419-92b3-0e62cae1479c  Rack1-K8Demo\nUN  172.17.0.6  84.74 KiB  32           67.1%             a6a1e8c2-3dc5-4417-b1a0-26507af2aaad  Rack1-K8Demo\n</pre></li>\n</ol><p>Use <code>kubectl edit</code> to modify the size of a Cassandra StatefulSet.</p><ol><li><p>Run the following command:</p>\n<div class=\"highlight\"><pre class=\"language-shell\" data-lang=\"shell\">kubectl edit statefulset cassandra</pre></div>\n<p>This command opens an editor in your terminal. The line you need to change is the <code>replicas</code> field. The following sample is an excerpt of the <code>StatefulSet</code> file:</p>\n<div class=\"highlight\"><pre class=\"language-yaml\" data-lang=\"yaml\"># Please edit the object below. Lines beginning with a '#' will be ignored,\n# and an empty file will abort the edit. If an error occurs while saving this file will be\n# reopened with the relevant failures.\n#\napiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2\nkind: StatefulSet\nmetadata:\n  creationTimestamp: 2016-08-13T18:40:58Z\n  generation: 1\n  labels:\n  app: cassandra\n  name: cassandra\n  namespace: default\n  resourceVersion: \"323\"\n  selfLink: /apis/apps/v1/namespaces/default/statefulsets/cassandra\n  uid: 7a219483-6185-11e6-a910-42010a8a0fc0\nspec:\n  replicas: 3</pre></div></li>\n<li><p>Change the number of replicas to 4, and then save the manifest.</p>\n<p>The <code>StatefulSet</code> now contains 4 Pods.</p></li>\n<li><p>Get the Cassandra StatefulSet to verify:</p>\n<div class=\"highlight\"><pre class=\"language-shell\" data-lang=\"shell\">kubectl get statefulset cassandra</pre></div>\n<p>The response should be</p>\n<pre>NAME        DESIRED   CURRENT   AGE\ncassandra   4         4         36m\n</pre></li>\n</ol><h2 id=\"cleaning-up\">Cleaning up</h2><p>Deleting or scaling a StatefulSet down does not delete the volumes associated with the StatefulSet. This setting is for your safety because your data is more valuable than automatically purging all related StatefulSet resources.</p><blockquote class=\"warning\">\n  <p><strong>Warning:</strong> Depending on the storage class and reclaim policy, deleting the <em>PersistentVolumeClaims</em> may cause the associated volumes to also be deleted. Never assume you’ll be able to access data if its volume claims are deleted.</p>\n</blockquote><ol><li><p>Run the following commands (chained together into a single command) to delete everything in the Cassandra <code>StatefulSet</code>:</p>\n<div class=\"highlight\"><pre class=\"language-shell\" data-lang=\"shell\">grace=$(kubectl get po cassandra-0 -o=jsonpath='{.spec.terminationGracePeriodSeconds}') \\\n  &amp;&amp; kubectl delete statefulset -l app=cassandra \\\n  &amp;&amp; echo \"Sleeping $grace\" \\\n  &amp;&amp; sleep $grace \\\n  &amp;&amp; kubectl delete pvc -l app=cassandra</pre></div></li>\n<li><p>Run the following command to delete the Cassandra Service.</p>\n<div class=\"highlight\"><pre class=\"language-shell\" data-lang=\"shell\">kubectl delete service -l app=cassandra</pre></div></li>\n</ol><h2 id=\"what-s-next\">What's next</h2><ul><li>Learn how to <a href=\"https://kubernetes.io/docs/tasks/run-application/scale-stateful-set/\">Scale a StatefulSet</a>.</li>\n<li>Learn more about the <a href=\"https://github.com/kubernetes/examples/blob/master/cassandra/java/src/main/java/io/k8s/cassandra/KubernetesSeedProvider.java\" target=\"_blank\"><em>KubernetesSeedProvider</em></a></li>\n<li>See more custom <a href=\"https://git.k8s.io/examples/cassandra/java/README.md\" target=\"_blank\">Seed Provider Configurations</a></li>\n</ul></div></section>",
        "created_at": "2018-10-24T17:51:48+0000",
        "updated_at": "2018-10-24T17:51:53+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 5,
        "domain_name": "kubernetes.io",
        "preview_picture": "https://kubernetes.io/images/kubernetes-horizontal-color.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12459"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 902,
            "label": "kubernetes",
            "slug": "kubernetes"
          }
        ],
        "is_public": false,
        "id": 12458,
        "uid": null,
        "title": "IBM/Scalable-Cassandra-deployment-on-Kubernetes",
        "url": "https://github.com/IBM/Scalable-Cassandra-deployment-on-Kubernetes/blob/master/README.md",
        "content": "<p><a href=\"https://travis-ci.org/IBM/Scalable-Cassandra-deployment-on-Kubernetes\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/27545c8485cc40771fd0f03ff9844c25de517778/68747470733a2f2f7472617669732d63692e6f72672f49424d2f5363616c61626c652d43617373616e6472612d6465706c6f796d656e742d6f6e2d4b756265726e657465732e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/IBM/Scalable-Cassandra-deployment-on-Kubernetes.svg?branch=master\" /></a></p>\n<p><em>Read this in other languages: <a href=\"https://github.com/IBM/Scalable-Cassandra-deployment-on-Kubernetes/blob/master/README-ko.md\">한국어</a>、<a href=\"https://github.com/IBM/Scalable-Cassandra-deployment-on-Kubernetes/blob/master/README-cn.md\">中国</a>.</em></p>\n<p>This project demonstrates the deployment of a multi-node scalable Cassandra cluster on Kubernetes. Apache Cassandra is a massively scalable open source NoSQL database. Cassandra is perfect for managing large amounts of structured, semi-structured, and unstructured data across multiple datacenters and the cloud.</p>\n<p>Leveraging Kubernetes concepts such as <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/\" rel=\"nofollow\">PersistentVolume</a> and <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\" rel=\"nofollow\">StatefulSets</a>, we can provide a resilient installation of Cassandra and be confident that its data (state) are safe.</p>\n<p>We also utilize a <a href=\"https://kubernetes.io/docs/concepts/services-networking/service/#headless-services\" rel=\"nofollow\">\"headless\" service</a> for Cassandra. This way we can provide a way for applications to access it via <a href=\"https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/\" rel=\"nofollow\">KubeDNS</a> and not expose it to the outside world. To access it from your developer workstation you can use <code>kubectl exec</code> commands against any of the cassandra pods. If you do wish to connect an application to it you can use the KubeDNS value of <code>cassandra.default.svc.cluster.local</code> when configuring your application.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/IBM/Scalable-Cassandra-deployment-on-Kubernetes/blob/master/images/kube-cassandra-code.png\"><img src=\"https://github.com/IBM/Scalable-Cassandra-deployment-on-Kubernetes/raw/master/images/kube-cassandra-code.png\" alt=\"kube-cassandra\" /></a></p>\n<h2><a id=\"user-content-kubernetes-concepts-used\" class=\"anchor\" aria-hidden=\"true\" href=\"#kubernetes-concepts-used\"></a>Kubernetes Concepts Used</h2>\n<ul><li><a href=\"https://kubernetes.io/docs/user-guide/pods\" rel=\"nofollow\">Kubenetes Pods</a></li>\n<li><a href=\"https://kubernetes.io/docs/user-guide/services\" rel=\"nofollow\">Kubenetes Services</a></li>\n<li><a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\" rel=\"nofollow\">Kubernets StatefulSets</a></li>\n</ul><h2><a id=\"user-content-included-components\" class=\"anchor\" aria-hidden=\"true\" href=\"#included-components\"></a>Included Components</h2>\n<ul><li><a href=\"https://console.ng.bluemix.net/docs/containers/cs_ov.html#cs_ov\" rel=\"nofollow\">Kubernetes Clusters</a></li>\n<li><a href=\"https://console.ng.bluemix.net/catalog/?taxonomyNavigation=apps&amp;category=containers\" rel=\"nofollow\">Bluemix container service</a></li>\n<li><a href=\"https://www.ibm.com/cloud-computing/products/ibm-cloud-private/\" rel=\"nofollow\">IBM Cloud Private</a></li>\n<li><a href=\"http://cassandra.apache.org/\" rel=\"nofollow\">Cassandra</a></li>\n</ul><h2><a id=\"user-content-getting-started\" class=\"anchor\" aria-hidden=\"true\" href=\"#getting-started\"></a>Getting Started</h2>\n<p>In order to follow this guide you'll need a Kubernetes cluster. If you do not have access to an existing Kubernetes cluster then follow the instructions (in the link) for one of the following:</p>\n<p><em>The code here is regularly tested against <a href=\"https://console.ng.bluemix.net/docs/containers/cs_ov.html#cs_ov\" rel=\"nofollow\">Kubernetes Cluster from Bluemix Container Service</a> using Travis CI.</em></p>\n<p>After installing (or setting up your access to) Kubernetes ensure that you can access it by running the following and confirming you get version responses for both the Client and the Server:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"7\", GitVersion:\"v1.7.5\", GitCommit:\"17d7182a7ccbb167074be7a87f0a68bd00d58d97\", GitTreeState:\"clean\", BuildDate:\"2017-08-31T09:14:02Z\", GoVersion:\"go1.8.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"7\", GitVersion:\"v1.7.5\", GitCommit:\"17d7182a7ccbb167074be7a87f0a68bd00d58d97\", GitTreeState:\"clean\", BuildDate:\"2017-09-18T20:30:29Z\", GoVersion:\"go1.8.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}</pre></div>\n<h2><a id=\"user-content-create-a-cassandra-service-for-cassandra-cluster-formation-and-discovery\" class=\"anchor\" aria-hidden=\"true\" href=\"#create-a-cassandra-service-for-cassandra-cluster-formation-and-discovery\"></a>Create a Cassandra Service for Cassandra cluster formation and discovery</h2>\n<h3><a id=\"user-content-1-create-a-cassandra-headless-service\" class=\"anchor\" aria-hidden=\"true\" href=\"#1-create-a-cassandra-headless-service\"></a>1. Create a Cassandra Headless Service</h3>\n<p>To allow us to do simple discovery of the cassandra seed node (which we will deploy shortly) we can create a \"headless\" service.  We do this by  specifying <strong>none</strong> for the  <strong>clusterIP</strong> in the <a href=\"https://github.com/IBM/Scalable-Cassandra-deployment-on-Kubernetes/blob/master/cassandra-service.yaml\">cassandra-service.yaml</a>. This headless service  allows us to use KubeDNS for the Pods to discover the IP address of the Cassandra seed.</p>\n<p>You can create the headless service using the <a href=\"https://github.com/IBM/Scalable-Cassandra-deployment-on-Kubernetes/blob/master/cassandra-service.yaml\">cassandra-service.yaml</a> file:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ kubectl create -f cassandra-service.yaml\nservice \"cassandra\" created\n$ kubectl get svc cassandra\nNAME        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE\ncassandra   None         &lt;none&gt;        9042/TCP   10s</pre></div>\n<p>Most applications deployed to Kubernetes should be cloud native and rely on external resources for their data (or state). However since Cassandra is a database we can use Stateful sets and Persistent Volumes to ensure resiliency in our database.</p>\n<h3><a id=\"user-content-2-create-local-volumes\" class=\"anchor\" aria-hidden=\"true\" href=\"#2-create-local-volumes\"></a>2. Create Local Volumes</h3>\n<p>To create persistent Cassandra nodes, we need to provision Persistent Volumes. There are two ways to provision PV's: <strong>dynamically and statically</strong>.</p>\n<p>For the sake of simplicity and compatibility we will use <strong>Static</strong> provisioning where we will create volumes manually using the provided yaml files.</p>\n<p><em>note: You'll need to have the same number of Persistent Volumes as the number of your Cassandra nodes. If you are expecting to have 3 Cassandra nodes, you'll need to create 3 Persistent Volumes.</em></p>\n<p>The provided <a href=\"https://github.com/IBM/Scalable-Cassandra-deployment-on-Kubernetes/blob/master/local-volumes.yaml\">local-volumes.yaml</a> file already has <strong>3</strong> Persistent Volumes defined. Update the file to add more if you expect to have greater than 3 Cassandra nodes. Create the volumes:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ kubectl create -f local-volumes.yaml\n$ kubectl get pv   \nNAME               CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS      CLAIM     STORAGECLASS   REASON    AGE\ncassandra-data-1   1Gi        RWO           Recycle         Available                                      7s\ncassandra-data-2   1Gi        RWO           Recycle         Available                                      7s\ncassandra-data-3   1Gi        RWO           Recycle         Available                                      7s</pre></div>\n<h3><a id=\"user-content-3-create-a-statefulset\" class=\"anchor\" aria-hidden=\"true\" href=\"#3-create-a-statefulset\"></a>3. Create a StatefulSet</h3>\n<p>The <a href=\"https://github.com/IBM/Scalable-Cassandra-deployment-on-Kubernetes/blob/master/cassandra-statefulset.yaml\">StatefulSet</a> is responsible for creating the Pods. It provides ordered deployment, ordered termination and unique network names. Run the following command to start a single Cassandra server:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ kubectl create -f cassandra-statefulset.yaml</pre></div>\n<h3><a id=\"user-content-4-validate-the-statefulset\" class=\"anchor\" aria-hidden=\"true\" href=\"#4-validate-the-statefulset\"></a>4. Validate the StatefulSet</h3>\n<p>You can check if your StatefulSet has deployed using the command below.</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ kubectl get statefulsets\nNAME        DESIRED   CURRENT   AGE\ncassandra   1         1         2h</pre></div>\n<p>If you view the list of the Pods, you should see 1 Pod running. Your Pod name should be cassandra-0 and the next pods would follow the ordinal number (<em>cassandra-1, cassandra-2,..</em>) Use this command to view the Pods created by the StatefulSet:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ kubectl get pods -o wide\nNAME          READY     STATUS    RESTARTS   AGE       IP              NODE\ncassandra-0   1/1       Running   0          1m        172.xxx.xxx.xxx   169.xxx.xxx.xxx</pre></div>\n<p>To check if the Cassandra node is up, perform a <strong>nodetool status:</strong></p>\n<div class=\"highlight highlight-source-shell\"><pre>$ kubectl exec -ti cassandra-0 -- nodetool status\nDatacenter: DC1\n===============\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address          Load       Tokens       Owns (effective)   Host ID                               Rack\nUN  172.xxx.xxx.xxx  109.28 KB  256          100.0%             6402e90d-7995-4ee1-bb9c-36097eb2c9ec  Rack1</pre></div>\n<h3><a id=\"user-content-5-scale-the-statefulset\" class=\"anchor\" aria-hidden=\"true\" href=\"#5-scale-the-statefulset\"></a>5. Scale the StatefulSet</h3>\n<p>To increase or decrease the size of your StatefulSet you can use the scale command:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ kubectl scale --replicas=3 statefulset/cassandra</pre></div>\n<p>Wait a minute or two and check if it worked:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ kubectl get statefulsets\nNAME        DESIRED   CURRENT   AGE\ncassandra   3         3         2h</pre></div>\n<p>If you watch the Cassandra pods deploy, they should be created sequentially.</p>\n<p>You can view the list of the Pods again to confirm that your Pods are up and running.</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ kubectl get pods -o wide\nNAME          READY     STATUS    RESTARTS   AGE       IP                NODE\ncassandra-0   1/1       Running   0          13m       172.xxx.xxx.xxx   169.xxx.xxx.xxx\ncassandra-1   1/1       Running   0          38m       172.xxx.xxx.xxx   169.xxx.xxx.xxx\ncassandra-2   1/1       Running   0          38m       172.xxx.xxx.xxx   169.xxx.xxx.xxx</pre></div>\n<p>You can perform a <strong>nodetool status</strong> to check if the other cassandra nodes have joined and formed a Cassandra cluster.</p>\n<p><em><strong>Note:</strong> It can take around 5 minutes for the Cassandra database to finish its setup.</em></p>\n<div class=\"highlight highlight-source-shell\"><pre>$ kubectl exec -ti cassandra-0 -- nodetool status\nDatacenter: DC1\n===============\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address     Load       Tokens       Owns (effective)  Host ID                               Rack\nUN  172.xxx.xxx.xxx  103.25 KiB  256          68.7%             633ae787-3080-40e8-83cc-d31b62f53582  Rack1\nUN  172.xxx.xxx.xxx  108.62 KiB  256          63.5%             e95fc385-826e-47f5-a46b-f375532607a3  Rack1\nUN  172.xxx.xxx.xxx  177.38 KiB  256          67.8%             66bd8253-3c58-4be4-83ad-3e1c3b334dfd  Rack1</pre></div>\n<p><em>You will need to wait for the status of the nodes to be Up and Normal (UN) to execute the commands in the next steps.</em></p>\n<h3><a id=\"user-content-6-using-cql\" class=\"anchor\" aria-hidden=\"true\" href=\"#6-using-cql\"></a>6. Using CQL</h3>\n<p>You can access the cassandra container using the following command:</p>\n<div class=\"highlight highlight-source-shell\"><pre>kubectl exec -it cassandra-0 cqlsh    \nConnected to Cassandra at 127.0.0.1:9042.\n[cqlsh 5.0.1 | Cassandra 3.11.1 | CQL spec 3.4.4 | Native protocol v4]\nUse HELP for help.\ncqlsh&gt; describe tables\nKeyspace system_traces\n----------------------\nevents  sessions\nKeyspace system_schema\n----------------------\ntables     triggers    views    keyspaces  dropped_columns\nfunctions  aggregates  indexes  types      columns        \nKeyspace system_auth\n--------------------\nresource_role_permissons_index  role_permissions  role_members  roles\nKeyspace system\n---------------\navailable_ranges          peers               batchlog        transferred_ranges\nbatches                   compaction_history  size_estimates  hints             \nprepared_statements       sstable_activity    built_views   \n\"IndexInfo\"               peer_events         range_xfers   \nviews_builds_in_progress  paxos               local         \nKeyspace system_distributed\n---------------------------\nrepair_history  view_build_status  parent_repair_history\n</pre></div>\n<h2><a id=\"user-content-troubleshooting\" class=\"anchor\" aria-hidden=\"true\" href=\"#troubleshooting\"></a>Troubleshooting</h2>\n<ul><li>If your Cassandra instance is not running properly, you may check the logs using\n<ul><li><code>kubectl logs &lt;your-pod-name&gt;</code></li>\n</ul></li>\n<li>To clean/delete your data on your Persistent Volumes, delete your PVCs using\n<ul><li><code>kubectl delete pvc -l app=cassandra</code></li>\n</ul></li>\n<li>If your Cassandra nodes are not joining, delete your controller/statefulset then delete your Cassandra service.\n<ul><li><code>kubectl delete statefulset cassandra</code> if you created the Cassandra StatefulSet</li>\n<li><code>kubectl delete svc cassandra</code></li>\n</ul></li>\n<li>To delete everything:\n<ul><li><code>kubectl delete statefulset,pvc,pv,svc -l app=cassandra</code></li>\n</ul></li>\n</ul><h2><a id=\"user-content-references\" class=\"anchor\" aria-hidden=\"true\" href=\"#references\"></a>References</h2>",
        "created_at": "2018-10-24T17:49:12+0000",
        "updated_at": "2018-10-24T17:49:21+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 6,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/1459110?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12458"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 968,
            "label": "automation",
            "slug": "automation"
          },
          {
            "id": 1208,
            "label": "chef",
            "slug": "chef"
          }
        ],
        "is_public": false,
        "id": 12454,
        "uid": null,
        "title": "michaelklishin/cassandra-chef-cookbook",
        "url": "https://github.com/michaelklishin/cassandra-chef-cookbook",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p><a href=\"https://travis-ci.org/michaelklishin/cassandra-chef-cookbook\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/87f09f29326d3a637955a80bb59facec7a233fae/68747470733a2f2f7472617669732d63692e6f72672f6d69636861656c6b6c697368696e2f63617373616e6472612d636865662d636f6f6b626f6f6b2e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/michaelklishin/cassandra-chef-cookbook.svg?branch=master\" /></a></p>\n<p>This is a Chef cookbook for Apache Cassandra (<a href=\"http://www.datastax.com/products/community\" rel=\"nofollow\">DataStax\nCommunity Edition</a>) as\nwell as DataStax Enterprise.</p>\n<p>It uses officially released packages and provides an Upstart service\nscript. It has fairly complete support for adjustment of Cassandra\nconfiguration parameters using Chef node attributes.</p>\n<p>It was originally created for CI and development environments and now supports cluster discovery using Chef search. <strong>Feel free to contribute</strong> what you find missing!</p>\n<h2><a id=\"user-content-supported-chef-versions\" class=\"anchor\" aria-hidden=\"true\" href=\"#supported-chef-versions\"></a>Supported Chef Versions</h2>\n<p>This cookbook targets Chef 12 and later versions.</p>\n<h2><a id=\"user-content-cookbook-dependencies\" class=\"anchor\" aria-hidden=\"true\" href=\"#cookbook-dependencies\"></a>Cookbook Dependencies</h2>\n<div class=\"highlight highlight-source-ruby\"><pre>depends 'java'\ndepends 'ulimit'\ndepends 'apt'\ndepends 'yum'\ndepends 'ark'</pre></div>\n<h2><a id=\"user-content-cassandra-dependencies\" class=\"anchor\" aria-hidden=\"true\" href=\"#cassandra-dependencies\"></a>Cassandra Dependencies</h2>\n<p>Modern Cassandra versions require OracleJDK 8.</p>\n<h2><a id=\"user-content-berkshelf\" class=\"anchor\" aria-hidden=\"true\" href=\"#berkshelf\"></a>Berkshelf</h2>\n<h3><a id=\"user-content-most-recent-release\" class=\"anchor\" aria-hidden=\"true\" href=\"#most-recent-release\"></a>Most Recent Release</h3>\n<div class=\"highlight highlight-source-ruby\"><pre>cookbook 'cassandra-dse', '~&gt; 4.5.0'</pre></div>\n<h3><a id=\"user-content-from-git\" class=\"anchor\" aria-hidden=\"true\" href=\"#from-git\"></a>From Git</h3>\n<div class=\"highlight highlight-source-ruby\"><pre>cookbook 'cassandra-dse', github: 'michaelklishin/cassandra-chef-cookbook'</pre></div>\n<h2><a id=\"user-content-supported-apache-cassandra-version\" class=\"anchor\" aria-hidden=\"true\" href=\"#supported-apache-cassandra-version\"></a>Supported Apache Cassandra Version</h2>\n<p>This cookbook currently provides</p>\n<ul><li>Cassandra via tarballs</li>\n<li>Cassandra (DataStax Community Edition) via apt and yum packages</li>\n<li>DataStax Enterprise (DSE) via packages</li>\n</ul><h2><a id=\"user-content-supported-os-distributions\" class=\"anchor\" aria-hidden=\"true\" href=\"#supported-os-distributions\"></a>Supported OS Distributions</h2>\n<ul><li>Ubuntu 12.04 through 17.101 via DataStax apt repo.</li>\n<li>RHEL/CentOS via DataStax yum repo.</li>\n<li>RHEL/CentOS/Amazon via tarball</li>\n</ul><h2><a id=\"user-content-support-jdk-versions\" class=\"anchor\" aria-hidden=\"true\" href=\"#support-jdk-versions\"></a>Support JDK Versions</h2>\n<p>Cassandra 2.x requires JDK 7+, later versions require Oracle JDK 8+.</p>\n<h2><a id=\"user-content-recipes\" class=\"anchor\" aria-hidden=\"true\" href=\"#recipes\"></a>Recipes</h2>\n<p>The main recipe is <code>cassandra-dse::default</code> which together with the <code>node[:cassandra][:install_method]</code> attribute will be responsible for including the proper installation recipe and recipe <code>cassandra-dse::config</code> for configuring both <code>datastax</code> and <code>tarball</code> C* installation.</p>\n<p>Two actual installation recipes are <code>cassandra-dse::tarball</code> and <code>cassandra-dse::datastax</code>. The former uses official tarball\nand thus can be used to provision any specific version.</p>\n<p>The latter uses DataStax repository via packages. You can install different versions (ex. <code>dsc20</code> for v2.0) available in the repository by altering <code>:package_name</code> attribute (<code>dsc20</code> by default).</p>\n<p>Recently we have moved all the configuration resources to a separate recipe <code>cassandra-des::config</code>, which means recipes <code>cassandra-dse::tarball</code> and <code>cassandra-dse::datastax</code> are only responsible for C* installation.</p>\n<blockquote>\n<blockquote>\n<p>Users with cookbook version <code>=&lt;3.5.0</code> needs to update the <code>run_list</code>, in case of not using <code>cassandra-dse::default</code> recipe.</p>\n</blockquote>\n</blockquote>\n<p>include_recipe <code>cassandra-dse</code> uses <code>cassandra-dse::datastax</code> as the default.</p>\n<h3><a id=\"user-content-datastax-enterprise\" class=\"anchor\" aria-hidden=\"true\" href=\"#datastax-enterprise\"></a>DataStax Enterprise</h3>\n<p>You can also install the DataStax Enterprise edition by adding <code>node[:cassandra][:dse]</code> attributes according to the datastax.rb.</p>\n<ul><li><code>node[:cassandra][:package_name]</code>: Override default value to 'dse-full'.</li>\n<li><code>node[:cassandra][:service_name]</code>: Override default value to 'dse'.</li>\n</ul><p>Unencrypted Credentials:</p>\n<ul><li><code>node[:cassandra][:dse][:credentials][:username]</code>: Your username from Datastax website.</li>\n<li><code>node[:cassandra][:dse][:credentials][:password]</code>: Your password from Datastax website.</li>\n</ul><p>Encrypted Credentials:</p>\n<ul><li><code>node[:cassandra][:dse][:credentials][:databag][:name]</code>: Databag name, i.e. the value 'cassandra' will reference to <code>/data_bags/cassandra</code>.</li>\n<li><code>node[:cassandra][:dse][:credentials][:databag][:item]</code>: Databag item, i.e. the value 'main' will reference to <code>/data_bags/cassandra/main.json</code>.</li>\n<li><code>node[:cassandra][:dse][:credentials][:databag][:entry]</code>: The field name in the databag item, in which the credetials are written. i.e. the data_bag:</li>\n</ul><pre>{\n  \"id\": \"main\",\n  \"entry\": {\n    \"username\": \"%USERNAME%\",\n    \"password\": \"%PASSWORD%\"\n  }\n}\n</pre>\n<p>There are also recipes for DataStax opscenter installation (\n<code>cassandra-dse::opscenter_agent_tarball</code>,\n<code>cassandra-dse::opscenter_agent_datastax</code>, and\n<code>cassandra-dse::opscenter_server</code> ) along with attributes available\nfor override (see below).</p>\n<h3><a id=\"user-content-jna-support-for-c-versions-prior-to-210\" class=\"anchor\" aria-hidden=\"true\" href=\"#jna-support-for-c-versions-prior-to-210\"></a>JNA Support (for C* Versions Prior to 2.1.0)</h3>\n<p>The <code>node[:cassandra][:setup_jna]</code> attribute will install the jna.jar in the\n<code>/usr/share/java/jna.jar</code>, and create a symbolic link to it on\n<code>#{cassandra.lib\\_dir}/jna.jar</code>, according to the <a href=\"http://www.datastax.com/documentation/cassandra/1.2/webhelp/cassandra/install/installJnaDeb.html\" rel=\"nofollow\">DataStax\ndocumentation</a>.</p>\n<h2><a id=\"user-content-node-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#node-attributes\"></a>Node Attributes</h2>\n<p>Please note that the maintainers try to keep the list below up-to-date but it fairly often misses\nsome recently added attributes. Please refer to the <a href=\"https://github.com/michaelklishin/cassandra-chef-cookbook/tree/master/attributes\">attributes files</a> if an attribute you are looking for isn't listed.</p>\n<h3><a id=\"user-content-core-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#core-attributes\"></a>Core Attributes</h3>\n<ul><li><code>node[:cassandra][:install_method]</code> (default: datastax): The installation method to use (either 'datastax' or 'tarball').</li>\n<li><code>node[:cassandra][:config][:cluster_name]</code> (default: none): Name of the cluster to create. This is required.</li>\n<li><code>node[:cassandra][:version]</code> (default: a recent patch version): version to provision</li>\n<li><code>node[:cassandra][:tarball][:url]</code> and <code>node[:cassandra][:tarball][:sha256sum]</code> specify tarball URL and SHA256 check sum used by the <code>cassandra::tarball</code> recipe.</li>\n<li>Setting <code>node[:cassandra][:tarball][:url]</code> to \"auto\" (default) will download the tarball of the specified version from the Apache repository.</li>\n<li><code>node[:cassandra][:setup_user]</code> (default: true): create user/group for Cassandra node process</li>\n<li><code>node[:cassandra][:setup_user_limits]</code> (default: true): setup Cassandra user limits</li>\n<li><code>node[:cassandra][:user]</code>: username Cassandra node process will use</li>\n<li><code>node[:cassandra][:group]</code>: groupname Cassandra node process will use</li>\n<li><code>node[:cassandra][:heap_new_size]</code> set JVM <code>-Xmn</code>. If set, <code>node[:cassandra][:max_heap_size]</code> must also be set; if nil, defaults to <code>min(100MB * num_cores, 1/4 * heap size)</code></li>\n<li><code>node[:cassandra][:max_heap_size]</code> set JVM <code>-Xms</code> and <code>-Xmx</code>. If set, <code>node[:cassandra][:heap_new_size]</code> must also be set; if nil, defaults to <code>max(min(1/2 ram, 1024MB), min(1/4 ram, 8GB))</code></li>\n<li><code>node[:cassandra][:installation_dir]</code> (default: <code>/usr/local/cassandra</code>): installation directory</li>\n<li><code>node[:cassandra][:root_dir]</code> (default: <code>/var/lib/cassandra</code>): data directory root</li>\n<li><code>node[:cassandra][:log_dir]</code> (default: <code>/var/log/cassandra</code>): log directory</li>\n<li><code>node[:cassandra][:tmp_dir]</code> (default: none): tmp directory. Be careful what you set this to, as the cassandra user will be given ownership of that directory.</li>\n<li><code>node[:cassandra][:local_jmx]</code> (default: true): bind JMX listener to localhost</li>\n<li><code>node[:cassandra][:jmx_port]</code> (default: 7199): port to listen for JMX</li>\n<li><code>node[:cassandra][:jmx_remote_rmi_port]</code> (default: $JMX_PORT): port for jmx remote method invocation. If using internode SSL, there is a bug requiring this to be different than <code>node[:cassandra][:jmx_port]</code></li>\n<li><code>node[:cassandra][:jmx_remote_authenticate]</code> (default: false): turn on to require username/password for jmx operations including nodetool. To turn on requires <code>node[:cassandra][:local_jmx]</code> to be false</li>\n<li><code>node[:cassandra][:jmx][:user]</code> (default: cassandra): username for jmx authentication</li>\n<li><code>node[:cassandra][:jmx][:password]</code> (default: cassandra): password for jmx authentication.</li>\n<li><code>node[:cassandra][:notify_restart]</code> (default: false): notify Cassandra service restart upon resource update</li>\n<li>Setting <code>node[:cassandra][:notify_restart]</code> to true will restart Cassandra service upon resource change</li>\n<li><code>node[:cassandra][:setup_jna]</code> (default: true): installs jna.jar</li>\n<li><code>node[:cassandra][:skip_jna]</code> (default: false): (2.1.0 and up only) removes jna.jar, adding '-Dcassandra.boot_without_jna=true' for low-memory C* installations</li>\n<li><code>node[:cassandra][:pid_dir]</code> (default: true): pid directory for Cassandra node process for <code>cassandra::tarball</code> recipe</li>\n<li><code>node[:cassandra][:dir_mode]</code> (default: 0755): default permission set for Cassandra node directory / files</li>\n<li><code>node[:cassandra][:service_action]</code> (default: [:enable, :start]): default service actions for the service</li>\n<li><code>node[:cassandra][:install_java]</code> (default: true): whether to run the open source java cookbook</li>\n<li><code>node[:cassandra][:cassandra_old_version_20]</code> (default: ): attribute used in cookbook to determine C* version older or newer than 2.1</li>\n<li><code>node[:cassandra][:log_config_files]</code> (default: calculated): log framework configuration files name array</li>\n<li><code>node[:cassandra][:xss]</code>  JVM per thread stack-size (-Xss option) (default: 256k).</li>\n<li><code>node[:cassandra][:jmx_server_hostname]</code> java.rmi.server.hostname option for JMX interface, necessary to set when you have problems connecting to JMX) (default: false)</li>\n<li><code>node[:cassandra][:heap_dump]</code> -XX:+HeapDumpOnOutOfMemoryError JVM parameter (default: true)</li>\n<li><code>node[:cassandra][:heap_dump_dir]</code> Directory where heap dumps will be placed (default: nil, which will use cwd)</li>\n<li><code>node[:cassandra][:vnodes]</code> enable vnodes. (default: true)</li>\n</ul><p>For the complete set of supported attributes, please consult <a href=\"https://github.com/michaelklishin/cassandra-chef-cookbook/tree/master/attributes\">the source</a>.</p>\n<p>Attributes used to define JBOD functionality</p>\n<ul><li><code>default['cassandra']['jbod']['slices']</code> - defines the number of jbod slices while each represents data directory. By default disables with nil.</li>\n<li><code>default['cassandra']['jbod']['dir_name_prefix']</code> - defines the data directory prefix\nFor example if you want to connect 4 EBS disks as a JBOD slices the names will be in the following format: data1,data2,data3,data4\ncassandra.yaml.erb will generate automatically entry per data_dir location\nPlease note: this functionality is not creating volumes or directories. It takes care of configuration. You can use same parameters with AWS cookbook to create EBS volumes and map to directories.</li>\n</ul><p>Attributes for fine tuning CMS/ParNew, the GC algorithm recommended for Cassandra deployments:</p>\n<ul><li><code>node[:cassandra][:gc_survivor_ratio]</code> -XX:SurvivorRatio JVM parameter (default: 8)</li>\n<li><code>node[:cassandra][:gc_max_tenuring_threshold]</code> -XX:MaxTenuringThreshold JVM parameter (default: 1)</li>\n<li><code>node[:cassandra][:gc_cms_initiating_occupancy_fraction]</code> -XX:CMSInitiatingOccupancyFraction JVM parameter (default: 75)</li>\n</ul><p>Descriptions for these JVM parameters can be found <a href=\"http://www.oracle.com/technetwork/java/javase/tech/vmoptions-jsp-140102.html#PerformanceTuning\" rel=\"nofollow\">here</a> and <a href=\"http://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html#cms.starting_a_cycle\" rel=\"nofollow\">here</a>.</p>\n<p>Attributes for enabling G1 GC.</p>\n<ul><li><code>node[:cassandra][:jvm][:g1]</code> (default: false)</li>\n</ul><p>Attributes for enabling GC detail/logging.</p>\n<ul><li><code>node[:cassandra][:jvm][:gcdetail]</code> (default: false)</li>\n</ul><p>Attributes for fine tuning the G1 GC algorithm:</p>\n<ul><li><code>node[:cassandra][:jvm][:g1_rset_updating_pause_time_percent]</code> (default: 10)</li>\n<li><code>node[:cassandra][:jvm][:g1_heap_region_size]</code> -XX:G1HeapRegionSize (default: 0)</li>\n<li><code>node[:cassandra][:jvm][:max_gc_pause_millis]</code> -XX:MaxGCPauseMillis (default: 200)</li>\n<li><code>node[:cassandra][:jvm][:heap_occupancy_threshold]</code> -XX:InitiatingHeapOccupancyPercent (default: 45)</li>\n<li><code>node[:cassandra][:jvm][:max_parallel_gc_threads]</code> This will set -XX:ParallelGCThreads to the number of cores on the machine (default: false)</li>\n<li><code>node[:cassandra][:jvm][:max_conc_gc_threads]</code> This will set -XX:ConcGCThreads to the number of cores on the machine (default: false)</li>\n<li><code>node[:cassandra][:jvm][:parallel_ref_proc]</code> -XX:ParallelRefProcEnabled (default: false)</li>\n<li><code>node[:cassandra][:jvm][:always_pre_touch]</code> -XX:AlwaysPreTouch (default: false)</li>\n<li><code>node[:cassandra][:jvm][:use_biased_locking]</code> -XX:UseBiasedLocking  (default: true)</li>\n<li><code>node[:cassandra][:jvm][:use_tlab]</code> -XX:UseTLAB (default: true)</li>\n<li><code>node[:cassandra][:jvm][:resize_tlab]</code> -XX:ResizeTLAB (default: true)</li>\n</ul><p>Oracle JVM 8 tuning parameters: <a href=\"https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/\" rel=\"nofollow\">here</a></p>\n<h3><a id=\"user-content-seed-discovery-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#seed-discovery-attributes\"></a>Seed Discovery Attributes</h3>\n<ul><li><code>node[:cassandra][:seeds]</code> (default: <code>[node[:ipaddress]]</code>): an array of nodes this node will contact to discover cluster topology</li>\n<li><code>node[:cassandra][:seed_discovery][:use_chef_search]</code> (default: false): enabled seed discovery using Chef search</li>\n<li><code>node[:cassandra][:seed_discovery][:search_role]</code> (default: <code>\"cassandra-seed\"</code>): role to use in search query</li>\n<li><code>node[:cassandra][:seed_discovery][:search_query]</code> (default: uses <code>node[:cassandra][:seed_discovery][:search_role]</code>): allows\nfor overriding the entire Chef search query</li>\n<li><code>node[:cassandra][:seed_discovery][:count]</code> (default: <code>3</code>): how many nodes to include into seed list. First N nodes are\ntaken in the order Chef search returns them. IP addresses of the nodes are sorted lexographically.</li>\n</ul><h3><a id=\"user-content-cassandrayaml-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#cassandrayaml-attributes\"></a>cassandra.yaml Attributes</h3>\n<ul><li><code>node[:cassandra][:config][:num_tokens]</code> set the desired number of tokens. (default: 256)</li>\n<li><code>node[:cassandra][:config][:listen_address]</code> (default: node[:ipaddress]): address clients will use to connect to the node</li>\n<li><code>node[:cassandra][:config][:broadcast_address]</code> (default: node IP address): address to broadcast to other Cassandra nodes</li>\n<li><code>node[:cassandra][:config][:rpc_address]</code> (default: 0.0.0.0): address to bind the RPC interface.  Leave blank to lookup IP from hostname.</li>\n<li><code>node[:cassandra][:config][:hinted_handoff_enabled]</code> see <a href=\"http://wiki.apache.org/cassandra/HintedHandoff\" rel=\"nofollow\">http://wiki.apache.org/cassandra/HintedHandoff</a> (default: true)</li>\n<li><code>node[:cassandra][:config][:max_hint_window_in_ms]</code> The maximum amount of time a dead host will have hints generated (default: 10800000).</li>\n<li><code>node[:cassandra][:config][:hinted_handoff_throttle_in_kb]</code> throttle in KB's per second, per delivery thread (default: 1024)</li>\n<li><code>node[:cassandra][:config][:max_hints_delivery_threads]</code> Number of threads with which to deliver hints (default: 2)</li>\n<li><code>node[:cassandra][:config][:authenticator]</code> Authentication backend (default: org.apache.cassandra.auth.AllowAllAuthenticator)</li>\n<li><code>node[:cassandra][:config][:authorizer]</code> Authorization backend (default: org.apache.cassandra.auth.AllowAllAuthorizer)</li>\n<li><code>node[:cassandra][:config][:permissions_validity_in_ms]</code> Validity period for permissions cache, set to0 to disable (default: 2000)</li>\n<li><code>node[:cassandra][:config][:partitioner]</code> The partitioner to distribute keys across the cluster (default: org.apache.cassandra.dht.Murmur3Partitioner).</li>\n<li><code>node[:cassandra][:config][:disk_failure_policy]</code> policy for data disk failures: stop, best_effort, or ignore (default: stop)</li>\n<li><code>node[:cassandra][:config][:key_cache_size_in_mb]</code> Maximum size of the key cache in memory. Set to 0 to disable, or \"\" for auto = (min(5% of Heap (in MB), 100MB)) (default: \"\", auto).</li>\n<li><code>node[:cassandra][:config][:key_cache_save_period]</code> Duration in seconds after which key cache is saved to saved_caches_directory. (default: 14400)</li>\n<li><code>node[:cassandra][:config][:row_cache_size_in_mb]</code> Maximum size of the row cache in memory, 0 to disable (default: 0)</li>\n<li><code>node[:cassandra][:config][:row_cache_save_period]</code> Duration in seconds after which row cache is saved to saved_caches_directory, 0 to disable cache save. (default: 0)</li>\n<li><code>node[:cassandra][:config][:row_cache_provider]</code> The provider for the row cache to use (default: SerializingCacheProvider)</li>\n<li><code>node[:cassandra][:config][:commitlog_sync]</code> periodic to ack writes immediately with periodic fsyncs, or batch to wait until fsync to ack writes (default: periodic)</li>\n<li><code>node[:cassandra][:config][:commitlog_sync_period_in_ms]</code> period for commitlog fsync when commitlog_sync = periodic (default: 10000)</li>\n<li><code>node[:cassandra][:config][:commitlog_sync_batch_window_in_ms]</code> batch window for fsync when commitlog_sync = batch (default: 50)</li>\n<li><code>node[:cassandra][:config][:commitlog_segment_size_in_mb]</code> Size of individual commitlog file segments (default: 32)</li>\n<li><code>node[:cassandra][:config][:commitlog_total_space_in_mb]</code> If space gets above this value (it will round up to the next nearest segment multiple), Cassandra will flush every dirty CF in the oldest segment and remove it. (default: 4096)</li>\n<li><code>node[:cassandra][:config][:concurrent_reads]</code> Should be set to 16 * drives (default: 32)</li>\n<li><code>node[:cassandra][:config][:concurrent_writes]</code> Should be set to 8 * cpu cores (default: 32)</li>\n<li><code>node[:cassandra][:config][:trickle_fsync]</code> Enable this to avoid sudden dirty buffer flushing from impacting read latencies.  Almost always a good idea on SSDs; not necessary on platters (default: false)</li>\n<li><code>node[:cassandra][:config][:trickle_fsync_interval_in_kb]</code> Interval for fsync when doing sequential writes (default: 10240)</li>\n<li><code>node[:cassandra][:config][:storage_port]</code> TCP port, for commands and data (default: 7000)</li>\n<li><code>node[:cassandra][:config][:ssl_storage_port]</code> SSL port, unused unless enabled in encryption options (default: 7001)</li>\n<li><code>node[:cassandra][:config][:listen_address]</code> Address to bind for communication with other nodes. Leave blank to lookup IP from hostname. 0.0.0.0 is always wrong. (default: node[:ipaddress]).</li>\n<li><code>node[:cassandra][:config][:broadcast_address]</code> Address to broadcast to other Cassandra nodes.  If '', will use listen_address (default: '')</li>\n<li><code>node[:cassandra][:config][:start_native_transport]</code> Whether to start the native transport server (default: true)</li>\n<li><code>node[:cassandra][:config][:native_transport_port]</code> Port for the CQL native transport to listen for clients on (default: 9042)</li>\n<li><code>node[:cassandra][:config][:start_rpc]</code> Whether to start the Thrift RPC server (default: true)</li>\n<li><code>node[:cassandra][:config][:rpc_port]</code> Port for Thrift RPC server to listen for clients on (default: 9160)</li>\n<li><code>node[:cassandra][:config][:rpc_keepalive]</code> Enable keepalive on RPC connections (default: true)</li>\n<li><code>node[:cassandra][:config][:rpc_server_type]</code> sync for one thread per connection; hsha for \"half synchronous, half asynchronous\" (default: sync)</li>\n<li><code>node[:cassandra][:config][:thrift_framed_transport_size_in_mb]</code> Frame size for Thrift (maximum field length) (default: 15)</li>\n<li><code>node[:cassandra][:config][:thrift_max_message_length_in_mb]</code> Max length of a Thrift message, including all fields and internal Thrift overhead (default: 16)</li>\n<li><code>node[:cassandra][:config][:incremental_backups]</code> Enable hardlinks in backups/ for each sstable flushed or streamed locally. Removing these links is the operator's responsibility (default: false)</li>\n<li><code>node[:cassandra][:config][:snapshot_before_compaction]</code> Take a snapshot before each compaction (default: false)</li>\n<li><code>node[:cassandra][:config][:auto_snapshot]</code> Take a snapshot before keyspace truncation or dropping of column families.  If you set this value to false, you will lose data on truncation or drop (default: true)</li>\n<li><code>node[:cassandra][:config][:column_index_size_in_kb]</code> Add column indexes to a row after its contents reach this size (default: 64)</li>\n<li><code>node[:cassandra][:config][:compaction_throughput_mb_per_sec]</code> Throttle compaction to this total system throughput. Generally should be 16-32 times data insertion rate (default: 16)</li>\n<li><code>node[:cassandra][:config][:read_request_timeout_in_ms]</code> How long the coordinator should wait for read operations to complete (default: 10000)</li>\n<li><code>node[:cassandra][:config][:range_request_timeout_in_ms]</code> How long the coordinator should wait for seq or index scans to complete (default: 10000).</li>\n<li><code>node[:cassandra][:config][:write_request_timeout_in_ms]</code> How long the coordinator should wait for writes to complete (default: 10000)</li>\n<li><code>node[:cassandra][:config][:truncate_request_timeout_in_ms]</code> How long the coordinator should wait for truncates to complete (default: 60000)</li>\n<li><code>node[:cassandra][:config][:request_timeout_in_ms]</code> Default timeout for other, miscellaneous operations (default: 10000)</li>\n<li><code>node[:cassandra][:config][:cross_node_timeout]</code> Enable operation timeout information exchange between nodes to accurately measure request timeouts. Be sure ntp is installed and node times are synchronized before enabling. (default: false)</li>\n<li><code>node[:cassandra][:config][:streaming_socket_timeout_in_ms]</code> Enable socket timeout for streaming operation (default: 3600000 - 1 hour)</li>\n<li><code>node[:cassandra][:config][:phi_convict_threshold]</code> Adjusts the sensitivity of the failure detector on an exponential scale (default: 8)</li>\n<li><code>node[:cassandra][:config][:endpoint_snitch]</code> SimpleSnitch, PropertyFileSnitch, GossipingPropertyFileSnitch, RackInferringSnitch, Ec2Snitch, Ec2MultiRegionSnitch (default: SimpleSnitch)</li>\n<li><code>node[:cassandra][:config][:dynamic_snitch_update_interval_in_ms]</code> How often to perform the more expensive part of host score calculation (default: 100)</li>\n<li><code>node[:cassandra][:config][:dynamic_snitch_reset_interval_in_ms]</code> How often to reset all host scores, allowing a bad host to possibly recover (default: 600000)</li>\n<li><code>node[:cassandra][:config][:dynamic_snitch_badness_threshold]</code> Allow 'pinning' of replicas to hosts in order to increase cache capacity. (default: 0.1)</li>\n<li><code>node[:cassandra][:config][:request_scheduler]</code> Class to schedule incoming client requests (default: org.apache.cassandra.scheduler.NoScheduler)</li>\n<li><code>node[:cassandra][:config][:index_interval]</code> index_interval controls the sampling of entries from the primary row index in terms of space versus time (default: 128).</li>\n<li><code>node[:cassandra][:config][:auto_bootstrap]</code> Setting this parameter to false prevents the new nodes from attempting to get all the data from the other nodes in the data center. (default: true).</li>\n<li><code>node[:cassandra][:config][:enable_assertions]</code> Enable JVM assertions.  Disabling this in production will give a modest performance benefit (around 5%) (default: true).</li>\n<li><code>node[:cassandra][:config][:data_file_directories]</code> (default: node['cassandra']['data_dir']): C* data cirectories</li>\n<li><code>node[:cassandra][:config][:saved_caches_directory]</code> (default: saved_caches_directory): C* saved cache directory</li>\n<li><code>node[:cassandra][:config][:commitlog_directory]</code> (default: node['cassandra']['commitlog_dir']) *C commit log directory</li>\n</ul><h4><a id=\"user-content-c-v20-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#c-v20-attributes\"></a>C* &lt;v2.0 Attributes</h4>\n<ul><li><code>node[:cassandra][:config][:memtable_flush_queue_size]</code> Number of full memtables to allow pending flush, i.e., waiting for a writer thread (default: 4)</li>\n<li><code>node[:cassandra][:config][:in_memory_compaction_limit_in_mb]</code> Size limit for rows being compacted in memory (default: 64)</li>\n<li><code>node[:cassandra][:config][:concurrent_compactors]</code> Sets the number of concurrent compaction processes allowed to run simultaneously on a node. (default: nil, which will result in one compaction process per CPU core)</li>\n<li><code>node[:cassandra][:config][:multithreaded_compaction]</code> Enable multithreaded compaction. Uses one thread per core, plus one thread per sstable being merged. (default: false)</li>\n<li><code>node[:cassandra][:config][:compaction_preheat_key_cache]</code> Track cached row keys during compaction and re-cache their new positions in the compacted sstable. Disable if you use really large key caches (default: true)</li>\n<li><code>node[:cassandra][:config][:native_transport_min_threads]</code> Min number of threads for handling transport requests when the native protocol is used (default: nil)</li>\n<li><code>node[:cassandra][:config][:native_transport_max_threads]</code> Max number of threads for handling transport requests when the native protocol is used (default: nil)</li>\n</ul><h4><a id=\"user-content-c-v21-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#c-v21-attributes\"></a>C* &gt;v2.1 Attributes</h4>\n<ul><li><code>node[:cassandra][:config][:broadcast_rpc_address]</code> RPC address to broadcast to drivers and other Cassandra nodes (default: node[:ipaddress])</li>\n<li><code>node[:cassandra][:config][:tombstone_failure_threshold]</code> tombstone attribute, check C* documentation for more info (default: 100000)</li>\n<li><code>node[:cassandra][:config][:tombstone_warn_threshold]</code> tombstone attribute, check C* documentation for more info (default: 1000)</li>\n<li><code>node[:cassandra][:config][:sstable_preemptive_open_interval_in_mb]</code> This helps to smoothly transfer reads between the sstables, reducing page cache churn and keeping hot rows hot (default: 50)</li>\n<li><code>node[:cassandra][:config][:memtable_allocation_type]</code> Specify the way Cassandra allocates and manages memtable memory (default: heap_buffers)</li>\n<li><code>node[:cassandra][:config][:index_summary_capacity_in_mb]</code> A fixed memory pool size in MB for for SSTable index summaries. If left empty, this will default to 5% of the heap size (default: nil)</li>\n<li><code>node[:cassandra][:config][:index_summary_resize_interval_in_minutes]</code> How frequently index summaries should be resampled (default: 60)</li>\n<li><code>node[:cassandra][:config][:concurrent_counter_writes]</code> Concurrent writes, since writes are almost never IO bound, the ideal number of \"concurrent_writes\" is dependent on the number of cores in your system; (8 * number_of_cores) (default: 32)</li>\n<li><code>node[:cassandra][:config][:counter_cache_save_period]</code> Duration in seconds after which Cassandra should save the counter cache (keys only) (default: 7200)</li>\n<li><code>node[:cassandra][:config][:counter_cache_size_in_mb]</code> Counter cache helps to reduce counter locks' contention for hot counter cells. Default value is empty to make it \"auto\" (min(2.5% of Heap (in MB), 50MB)). Set to 0 to disable counter cache. (default: nil)</li>\n<li><code>node[:cassandra][:config][:counter_write_request_timeout_in_ms]</code> How long the coordinator should wait for counter writes to complete (default: 5000)</li>\n<li><code>node[:cassandra][:config][:commit_failure_policy]</code> policy for commit disk failures (default: stop)</li>\n<li><code>node[:cassandra][:config][:cas_contention_timeout_in_ms]</code> How long a coordinator should continue to retry a CAS operation that contends with other proposals for the same row (default: 1000)</li>\n<li><code>node[:cassandra][:config][:batch_size_warn_threshold_in_kb]</code> Log WARN on any batch size exceeding this value. 5kb per batch by default (default: 5)</li>\n<li><code>node[:cassandra][:config][:batchlog_replay_throttle_in_kb]</code> Maximum throttle in KBs per second, total. This will be reduced proportionally to the number of nodes in the cluster (default: 1024)</li>\n</ul><h3><a id=\"user-content-jamm-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#jamm-attributes\"></a>JAMM Attributes</h3>\n<ul><li><code>node[:cassandra][:setup_jamm]</code> (default: false): install the jamm jar file and use it to set java option <code>-javaagent</code>, obsolete for C* versions <code>&gt;v0.8.0</code></li>\n<li><code>node[:cassandra][:jamm][:sha256sum]</code> (default: calculated): jamm lib sha256sum for calculated version</li>\n<li><code>node[:cassandra][:jamm][:base_url]</code> (default: calculated): jamm lib jar url</li>\n<li><code>node[:cassandra][:jamm][:jar_name]</code> (default: calculated): jamm lib jar name</li>\n<li><code>node[:cassandra][:jamm][:version]</code> (default: calculated): jamm lib version</li>\n</ul><h3><a id=\"user-content-jna-attributes-prior-c-version-210\" class=\"anchor\" aria-hidden=\"true\" href=\"#jna-attributes-prior-c-version-210\"></a>JNA Attributes (Prior C* version 2.1.0)</h3>\n<ul><li><code>node[:cassandra][:jna][:base_url]</code> The base url to fetch the JNA jar (default: <a href=\"https://github.com/twall/jna/tree/4.0/dist\">https://github.com/twall/jna/tree/4.0/dist</a>)</li>\n<li><code>node[:cassandra][:jna][:jar_name]</code> The name of the jar to download from the base url. (default: jna.jar)</li>\n<li><code>node[:cassandra][:jna][:sha256sum]</code> The SHA-256 checksum of the file. If the local jna.jar file matches the checksum, the chef-client will not re-download it. (default: dac270b6441ce24d93a96ddb6e8f93d8df099192738799a6f6fcfc2b2416ca19)</li>\n</ul><h3><a id=\"user-content-priam-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#priam-attributes\"></a>Priam Attributes</h3>\n<ul><li><code>node[:cassandra][:setup_priam]</code> (default: false): install the priam jar file and use it to set java option <code>-javaagent</code>, uses the priam version corresponding to the cassandra version</li>\n<li><code>node[:cassandra][:priam][:sha256sum]</code> (default: 9fde9a40dc5c538adee54f40fa9027cf3ebb7fd42e3592b3e6fdfe3f7aff81e1): priam lib sha256sum for version <code>2.2.0</code></li>\n<li><code>node[:cassandra][:priam][:base_url]</code> (default: priam url on maven.org): priam lib jar url</li>\n<li><code>node[:cassandra][:priam][:jar_name]</code> (default: calculated): priam lib jar name</li>\n</ul><h3><a id=\"user-content-logback-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#logback-attributes\"></a>Logback Attributes</h3>\n<ul><li><code>node[:cassandra][:logback][:file][:max_file_size]</code> (default: \"20MB\"): logback File appender log file rotation size</li>\n<li><code>node[:cassandra][:logback][:file][:max_index]</code> (default: 20): logback File appender log files max_index</li>\n<li><code>node[:cassandra][:logback][:file][:min_index]</code> (default: 1): logback File appender log files min_index</li>\n<li><code>node[:cassandra][:logback][:file][:pattern]</code> (default: \"%-5level [%thread] %date{ISO8601} %F:%L - %msg%n\"): logback File appender log pattern</li>\n<li><code>node[:cassandra][:logback][:debug][:enable]</code> (default: false): enable logback File appender log debug</li>\n<li><code>node[:cassandra][:logback][:debug][:max_file_size]</code> (default: \"20MB\"): logback File appender log file rotation size</li>\n<li><code>node[:cassandra][:logback][:debug][:max_index]</code> (default: 20): logback File appender log files max_index</li>\n<li><code>node[:cassandra][:logback][:debug][:min_index]</code> (default: 1): logback File appender log files min_index</li>\n<li><code>node[:cassandra][:logback][:debug][:pattern]</code> (default: \"%-5level [%thread] %date{ISO8601} %F:%L - %msg%n\"): logback File appender log pattern</li>\n<li><code>node[:cassandra][:logback][:stdout][:enable]</code> (default: true): enable logback STDOUT appender</li>\n<li><code>node[:cassandra][:logback][:stdout][:pattern]</code> (default: \"%-5level %date{HH:mm:ss,SSS} %msg%n\"): logback STDOUT appender log pattern</li>\n<li><code>node[:cassandra][:logback][:syslog][:enable]</code> (default: false): enable logback SYSLOG appender. Requires RSYSLOG be installed and running on the node.</li>\n<li><code>node[:cassandra][:logback][:syslog][:host]</code> (default: localhost): The host name the syslog is written to.</li>\n<li><code>node[:cassandra][:logback][:syslog][:facility]</code> (default: USER) The facility specified for the appender.</li>\n<li><code>node[:cassandra][:logback][:syslog][:pattern]</code> (default: \"%-5level [%thread] %F:%L - %msg%n\") lockback SYSLOG appender log pattern</li>\n<li><code>node[:cassandra][:logback][:override_loggers]</code> (default: {}) Override log level of specific logger (i.e { 'org.apache.cassandra.utils.StatusLogger' =&gt; 'WARN' })</li>\n</ul><h3><a id=\"user-content-ulimit-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#ulimit-attributes\"></a>Ulimit Attributes</h3>\n<ul><li><code>node[:cassandra][:limits][:memlock]</code> (default: \"unlimited\"): memory ulimit for Cassandra node process</li>\n<li><code>node[:cassandra][:limits][:nofile]</code> (default: 48000): file ulimit for Cassandra node process</li>\n<li><code>node[:cassandra][:limits][:nproc]</code> (default: \"unlimited\"): process ulimit for Cassandra node process</li>\n</ul><h3><a id=\"user-content-yum-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#yum-attributes\"></a>Yum Attributes</h3>\n<ul><li><code>node[:cassandra][:yum][:repo]</code> (default: datastax): name of the repo from which to install</li>\n<li><code>node[:cassandra][:yum][:description]</code> (default: \"DataStax Repo for Apache Cassandra\"): description of the repo</li>\n<li><code>node[:cassandra][:yum][:baseurl]</code> (default: \"<a href=\"http://rpm.datastax.com/community\" rel=\"nofollow\">http://rpm.datastax.com/community</a>\"): repo url</li>\n<li><code>node[:cassandra][:yum][:mirrorlist]</code> (default: nil): a mirrorlist file</li>\n<li><code>node[:cassandra][:yum][:gpgcheck]</code> (default: false): whether to use <code>gpgcheck</code></li>\n<li><code>node[:cassandra][:yum][:enabled]</code> (default: true): whether the repo is enabled by default</li>\n<li><code>node[:cassandra][:yum][:options]</code> (default: \"\"): Additional options to pass to <code>yum_package</code></li>\n</ul><h3><a id=\"user-content-opscenter-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#opscenter-attributes\"></a>OpsCenter Attributes</h3>\n<h4><a id=\"user-content-datastax-ops-center-server-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#datastax-ops-center-server-attributes\"></a>DataStax Ops Center Server attributes</h4>\n<ul><li><code>node[:cassandra][:opscenter][:server][:package_name]</code> (default: opscenter-free)</li>\n<li><code>node[:cassandra][:opscenter][:server][:port]</code> (default: 8888)</li>\n<li><code>node[:cassandra][:opscenter][:server][:interface]</code> (default: 0.0.0.0)</li>\n<li><code>node[:cassandra][:opscenter][:server][:authentication]</code> (default: false)</li>\n<li><code>node[:cassandra][:opscenter][:cassandra_metrics][:ignored_keyspaces]</code> (default: [system, OpsCenter])</li>\n<li><code>node[:cassandra][:opscenter][:cassandra_metrics][:ignored_column_families]</code> (default: [])</li>\n<li><code>node[:cassandra][:opscenter][:cassandra_metrics][:1min_ttl]</code> (default: 604800)</li>\n<li><code>node[:cassandra][:opscenter][:cassandra_metrics][:5min_ttl]</code> (default: 2419200)</li>\n<li><code>node[:cassandra][:opscenter][:cassandra_metrics][:2hr_ttl]</code> (default: 31536000)</li>\n<li><code>node[:cassandra][:opscenter][:custom_configuration]</code> (default: {}) a hash of custom configuration sections to add to <a href=\"https://docs.datastax.com/en/opscenter/6.0/opsc/configure/opscConfigProps_r.html\" rel=\"nofollow\">opscenterd.conf</a>, e.g.:</li>\n</ul><pre>{\n 'ui' =&gt; {\n   'default_api_timeout' =&gt; 300\n },\n 'stat_reporter' =&gt; {\n   'interval' =&gt; 1\n }\n}\n</pre>\n<h4><a id=\"user-content-datastax-ops-center-agent-tarball-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#datastax-ops-center-agent-tarball-attributes\"></a>DataStax Ops Center Agent Tarball attributes</h4>\n<ul><li><code>node[:cassandra][:opscenter][:agent][:download_url]</code> (default: \"\") Required. You need to specify\nagent download url, because that could be different for each opscenter server version. ( S3 is a great\nplace to store packages )</li>\n<li><code>node[:cassandra][:opscenter][:agent][:checksum]</code> (default: <code>nil</code>)</li>\n<li><code>node[:cassandra][:opscenter][:agent][:install_dir]</code> (default: <code>/opt</code>)</li>\n<li><code>node[:cassandra][:opscenter][:agent][:install_folder_name]</code> (default: <code>opscenter_agent</code>)</li>\n<li><code>node[:cassandra][:opscenter][:agent][:binary_name]</code> (default: <code>opscenter-agent</code>) Introduced since Datastax changed agent binary name from opscenter-agent to datastax-agent. <strong>Make sure to set it right if you are updating to 4.0.2</strong></li>\n<li><code>node[:cassandra][:opscenter][:agent][:server_host]</code> (default: \"\" ). If left empty, will use search to get IP by opscenter <code>server_role</code> role.</li>\n<li><code>node[:cassandra][:opscenter][:agent][:server_role]</code> (default: <code>opscenter_server</code>). Will be use for opscenter server IP lookup if <code>:server_host</code> is not set.</li>\n<li><code>node[:cassandra][:opscenter][:agent][:use_chef_search]</code> (default: <code>true</code>). Determines whether chef search will be used for locating the data agent server.</li>\n<li><code>node[:cassandra][:opscenter][:agent][:use_ssl]</code> (default: <code>false</code>)</li>\n</ul><h4><a id=\"user-content-datastax-ops-center-agent-datastax-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#datastax-ops-center-agent-datastax-attributes\"></a>DataStax Ops Center Agent Datastax attributes</h4>\n<ul><li><code>node[:cassandra][:opscenter][:agent][:package_name]</code> (default: \"datastax-agent\" ).</li>\n<li><code>node[:cassandra][:opscenter][:agent][:server_host]</code> (default: \"\" ). If left empty, will use search to get IP by opscenter <code>server_role</code> role.</li>\n<li><code>node[:cassandra][:opscenter][:agent][:server_role]</code> (default: <code>opscenter_server</code>). Will be use for opscenter server IP lookup if <code>:server_host</code> is not set.</li>\n<li><code>node[:cassandra][:opscenter][:agent][:use_ssl]</code> (default: <code>false</code>)</li>\n</ul><h3><a id=\"user-content-data-center-and-rack-attributes\" class=\"anchor\" aria-hidden=\"true\" href=\"#data-center-and-rack-attributes\"></a>Data Center and Rack Attributes</h3>\n<ul><li><code>node[:cassandra][:rackdc][:dc]</code> (default: \"\") The datacenter to specify in the cassandra-rackdc.properties file. (GossipingPropertyFileSnitch only)</li>\n<li><code>node[:cassandra][:rackdc][:rack]</code> (default: \"\") The rack to specify in the cassandra-rackdc.properties file (GossipingPropertyFileSnitch only)</li>\n<li><code>node[:cassandra][:rackdc][:prefer_local]</code> (default: \"false\") Whether the snitch will prefer the internal ip when possible, as the Ec2MultiRegionSnitch does. (GossipingPropertyFileSnitch only)</li>\n</ul><h2><a id=\"user-content-contributing\" class=\"anchor\" aria-hidden=\"true\" href=\"#contributing\"></a>Contributing</h2>\n<p>See <a href=\"https://github.com/michaelklishin/cassandra-chef-cookbook/blob/master/CONTRIBUTING.md\">CONTRIBUTING.md</a> and <a href=\"https://github.com/michaelklishin/cassandra-chef-cookbook/blob/master/TESTING.md\">TESTING.md</a>.</p>\n<h2><a id=\"user-content-copyright--license\" class=\"anchor\" aria-hidden=\"true\" href=\"#copyright--license\"></a>Copyright &amp; License</h2>\n<p>Michael S. Klishin, Travis CI Development Team, and <a href=\"https://github.com/michaelklishin/cassandra-chef-cookbook/graphs/contributors\">contributors</a>,\n2012-2018.</p>\n<p>Released under the <a href=\"http://www.apache.org/licenses/LICENSE-2.0.html\" rel=\"nofollow\">Apache 2.0 License</a>.</p>\n</article>",
        "created_at": "2018-10-24T13:39:10+0000",
        "updated_at": "2018-10-24T13:39:17+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 22,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/1090?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12454"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1209,
            "label": "puppet",
            "slug": "puppet"
          }
        ],
        "is_public": false,
        "id": 12452,
        "uid": null,
        "title": "voxpupuli/puppet-cassandra",
        "url": "https://github.com/voxpupuli/puppet-cassandra",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p><a href=\"https://travis-ci.org/voxpupuli/puppet-cassandra\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/b8ed4f222bfc86ba147fd1e9295d8b6f90fc94c8/68747470733a2f2f7472617669732d63692e6f72672f766f78707570756c692f7075707065742d63617373616e6472612e706e673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/voxpupuli/puppet-cassandra.png?branch=master\" /></a></p>\n<h2><a id=\"user-content-table-of-contents\" class=\"anchor\" aria-hidden=\"true\" href=\"#table-of-contents\"></a>Table of Contents</h2>\n<ol><li><a href=\"#overview\">Overview</a></li>\n<li><a href=\"#setup\">Setup - The basics of getting started with Cassandra</a>\n<ul><li><a href=\"#what-cassandra-affects\">What Cassandra affects</a></li>\n<li><a href=\"#upgrading\">Upgrading</a></li>\n<li><a href=\"#beginning-with-cassandra\">Beginning with Cassandra</a></li>\n</ul></li>\n<li><a href=\"#usage\">Usage - Configuration options and additional functionality</a>\n<ul><li><a href=\"#setup-a-keyspace-and-users\">Setup a keyspace and users</a></li>\n<li><a href=\"#create-a-cluster-in-a-single-data-center\">Create a Cluster in a Single Data Center</a></li>\n<li><a href=\"#create-a-cluster-in-multiple-data-centers\">Create a Cluster in Multiple Data Centers</a></li>\n<li><a href=\"#datastax-enterprise\">DataStax Enterprise</a></li>\n<li><a href=\"#apache-cassandra\">Apache Cassandra</a></li>\n</ul></li>\n<li><a href=\"#reference\">Reference</a></li>\n<li><a href=\"#limitations\">Limitations - OS compatibility, etc.</a></li>\n<li><a href=\"#development\">Development</a></li>\n</ol><h2><a id=\"user-content-overview\" class=\"anchor\" aria-hidden=\"true\" href=\"#overview\"></a>Overview</h2>\n<p>A Puppet module to install and manage Cassandra, DataStax Agent &amp; OpsCenter</p>\n<h2><a id=\"user-content-setup\" class=\"anchor\" aria-hidden=\"true\" href=\"#setup\"></a>Setup</h2>\n<h3><a id=\"user-content-what-cassandra-affects\" class=\"anchor\" aria-hidden=\"true\" href=\"#what-cassandra-affects\"></a>What Cassandra affects</h3>\n<h4><a id=\"user-content-what-the-cassandra-class-affects\" class=\"anchor\" aria-hidden=\"true\" href=\"#what-the-cassandra-class-affects\"></a>What the Cassandra class affects</h4>\n<ul><li>Installs the Cassandra package (default <strong>cassandra22</strong> on Red Hat and\n<strong>cassandra</strong> on Debian).</li>\n<li>Configures settings in <code>${config_path}/cassandra.yaml</code>.</li>\n<li>On CentOS 7 if the <code>init</code> service provider is used, then cassandra\nis added as a system service.</li>\n<li>Optionally ensures that the Cassandra service is enabled and running.</li>\n<li>On Debian systems:\n<ul><li>Optionally replace <code>/etc/init.d/cassandra</code> with a workaround for\n<a href=\"https://issues.apache.org/jira/browse/CASSANDRA-9822\" rel=\"nofollow\">CASSANDRA-9822</a>.</li>\n</ul></li>\n</ul><h4><a id=\"user-content-what-the-cassandradatastax_agent-class-affects\" class=\"anchor\" aria-hidden=\"true\" href=\"#what-the-cassandradatastax_agent-class-affects\"></a>What the cassandra::datastax_agent class affects</h4>\n<ul><li>Optionally installs the DataStax agent.</li>\n<li>Optionally sets JAVA_HOME in <strong>/etc/default/datastax-agent</strong>.</li>\n</ul><h4><a id=\"user-content-what-the-cassandradatastax_repo-class-affects\" class=\"anchor\" aria-hidden=\"true\" href=\"#what-the-cassandradatastax_repo-class-affects\"></a>What the cassandra::datastax_repo class affects</h4>\n<ul><li>Optionally configures a Yum repository to install the Cassandra packages\nfrom (on Red Hat).</li>\n<li>Optionally configures an Apt repository to install the Cassandra packages\nfrom (on Debian).</li>\n</ul><h4><a id=\"user-content-what-the-cassandradse-class-affects\" class=\"anchor\" aria-hidden=\"true\" href=\"#what-the-cassandradse-class-affects\"></a>What the cassandra::dse class affects</h4>\n<ul><li>Optionally configures files in the <code>/etc/dse</code> directory if one is using\nDataStax Enterprise.</li>\n</ul><h4><a id=\"user-content-what-the-cassandrafirewall_ports-class-affects\" class=\"anchor\" aria-hidden=\"true\" href=\"#what-the-cassandrafirewall_ports-class-affects\"></a>What the cassandra::firewall_ports class affects</h4>\n<ul><li>Optionally configures the firewall for the Cassandra related network\nports.</li>\n</ul><h4><a id=\"user-content-what-the-cassandrajava-class-affects\" class=\"anchor\" aria-hidden=\"true\" href=\"#what-the-cassandrajava-class-affects\"></a>What the cassandra::java class affects</h4>\n<ul><li>Optionally installs a JRE/JDK package (e.g. java-1.7.0-openjdk) and the\nJava Native Access (JNA).</li>\n</ul><h4><a id=\"user-content-what-the-cassandraoptutils-class-affects\" class=\"anchor\" aria-hidden=\"true\" href=\"#what-the-cassandraoptutils-class-affects\"></a>What the cassandra::optutils class affects</h4>\n<ul><li>Optionally installs the Cassandra support tools (e.g. cassandra22-tools).</li>\n</ul><h3><a id=\"user-content-upgrading\" class=\"anchor\" aria-hidden=\"true\" href=\"#upgrading\"></a>Upgrading</h3>\n<p>We follow <a href=\"http://semver.org/\" rel=\"nofollow\">SemVer Versioning</a> and an update of the major\nrelease (i.e. from 1.<em>Y</em>.<em>Z</em> to 2.<em>Y</em>.<em>Z</em>) will indicate a significant change\nto the API which will most probably require a change to your manifest.</p>\n<h4><a id=\"user-content-changes-in-200\" class=\"anchor\" aria-hidden=\"true\" href=\"#changes-in-200\"></a>Changes in 2.0.0</h4>\n<p>This is a major change to the API and you will more than likely need to\nchange your manifest to accomodate these changes.</p>\n<p>The <code>service_ensure</code> attribute of the cassandra class now defaults to\n<em>undef</em>, users who do want to manage service status in Puppet can still set\nit to true.  If leaving the value at the default and setting\n<code>service_refresh</code> and <code>service_enable</code> to false will mean that the\nuser and not Puppet running will control the running state of\nCassandra.  This currently works OK on the Red Hat family, but\nhas issues on Debian due to\n<a href=\"https://issues.apache.org/jira/browse/CASSANDRA-2356\" rel=\"nofollow\">CASSANDRA-2356</a>\nduring an initial install or package upgrade.</p>\n<p>All the functionality relating to OpsCenter has been divested to the\n<a href=\"https://forge.puppet.com/locp/opscenter\" rel=\"nofollow\">locp/opscenter</a> module on\nPuppet Forge.</p>\n<p>It should also be noted that the module no longer creates directories for\nthe <code>data</code>, <code>commitlog</code>, <code>saved_caches</code> and for Cassandra 3 the <code>hints</code>\ndirectory.  These resources will now need to be defined in your\nmanifest/profile.</p>\n<p>For a list of features that have been deprecated in this release, please see\n<a href=\"https://github.com/voxpupuli/puppet-cassandra/wiki/Deprecations\">https://github.com/voxpupuli/puppet-cassandra/wiki/Deprecations</a></p>\n<p>For details on migrating from the version 1.X.X attributes to the <code>settings</code>\nhash, see\n(<a href=\"https://github.com/voxpupuli/puppet-cassandra/wiki/Suggested-Baseline-Settings\">https://github.com/voxpupuli/puppet-cassandra/wiki/Suggested-Baseline-Settings</a>)</p>\n<p>Please also see the notes for 2.0.0 in the\n<a href=\"https://github.com/voxpupuli/puppet-cassandra/blob/master/CHANGELOG.md\">CHANGELOG</a>.</p>\n<h4><a id=\"user-content-changes-in-1190\" class=\"anchor\" aria-hidden=\"true\" href=\"#changes-in-1190\"></a>Changes in 1.19.0</h4>\n<p>The hints_directory documentation will cause a change in the cassandra.yaml\nfile regardless of the value you set it to.  If you do not wish this to\nresult in a refesh of the Cassandra service, please set service_refresh to\nfalse.</p>\n<h4><a id=\"user-content-changes-in-192\" class=\"anchor\" aria-hidden=\"true\" href=\"#changes-in-192\"></a>Changes in 1.9.2</h4>\n<p>Now that Cassandra 3 is available from the DataStax repositories, there is\na problem (especially on Debian) with the operating system package manager\nattempting to install Cassandra 3.  This can be mitigated against using\nsomething similar to the code in this modules acceptance test.  Please note\nthat the default Cassandra package name has now been changed from 'dsc'.  See\nthe documentation for cassandra::package_name below for details.</p>\n<div class=\"highlight highlight-source-puppet\"><pre> if $::osfamily == 'RedHat' {\n   $version = '2.2.4-1'\n } else {\n   $version = '2.2.4'\n }\n class { 'cassandra':\n   package_ensure =&gt; $version,\n }</pre></div>\n<h4><a id=\"user-content-changes-in-180\" class=\"anchor\" aria-hidden=\"true\" href=\"#changes-in-180\"></a>Changes in 1.8.0</h4>\n<p>A somewhat embarrassing correction to the spelling of the\ncassandra::fail_on_non_suppoted_os to cassandra::fail_on_non_supported_os.</p>\n<h4><a id=\"user-content-issues-when-upgrading-to-140\" class=\"anchor\" aria-hidden=\"true\" href=\"#issues-when-upgrading-to-140\"></a>Issues when Upgrading to 1.4.0</h4>\n<p>Unfortunately both releases 1.3.7 and 1.4.0 have subsequently been found to\ncall a refresh service even when no changes had been made to the underlying\nconfiguration.  In release 1.8.0 (somewhat belatedly) the service_refresh\nflag has been introduced to mitigate against similar problems.</p>\n<h4><a id=\"user-content-issues-when-upgrading-to-137\" class=\"anchor\" aria-hidden=\"true\" href=\"#issues-when-upgrading-to-137\"></a>Issues When Upgrading to 1.3.7</h4>\n<ul><li>Please see the notes for 1.4.0.</li>\n</ul><h4><a id=\"user-content-changes-in-100\" class=\"anchor\" aria-hidden=\"true\" href=\"#changes-in-100\"></a>Changes in 1.0.0</h4>\n<ul><li>cassandra::cassandra_package_ensure has been renamed to\ncassandra::package_ensure.</li>\n<li>cassandra::cassandra_package_name has been renamed to\ncassandra::package_name.</li>\n</ul><h4><a id=\"user-content-changes-in-040\" class=\"anchor\" aria-hidden=\"true\" href=\"#changes-in-040\"></a>Changes in 0.4.0</h4>\n<p>There is now a cassandra::datastax_agent class, therefore:</p>\n<ul><li>cassandra::datastax_agent_package_ensure has now been replaced with\ncassandra::datastax_agent::package_ensure.</li>\n<li>cassandra::datastax_agent_service_enable has now been replaced with\ncassandra::datastax_agent::service_enable.</li>\n<li>cassandra::datastax_agent_service_ensure has now been replaced with\ncassandra::datastax_agent::service_ensure.</li>\n<li>cassandra::datastax_agent_package_name has now been replaced with\ncassandra::datastax_agent::package_name.</li>\n<li>cassandra::datastax_agent_service_name has now been replaced with\ncassandra::datastax_agent::service_name.</li>\n</ul><p>Likewise now there is a new class for handling the installation of Java:</p>\n<ul><li>cassandra::java_package_ensure has now been replaced with\ncassandra::java::ensure.</li>\n<li>cassandra::java_package_name has now been replaced with\ncassandra::java::package_name.</li>\n</ul><p>Also there is now a class for installing the optional utilities:</p>\n<ul><li>cassandra::cassandra_opt_package_ensure has now been replaced with\ncassandra::optutils:ensure.</li>\n<li>cassandra::cassandra_opt_package_name has now been replaced with\ncassandra::optutils:package_name.</li>\n</ul><h4><a id=\"user-content-changes-in-030\" class=\"anchor\" aria-hidden=\"true\" href=\"#changes-in-030\"></a>Changes in 0.3.0</h4>\n<ul><li>\n<p>cassandra_opt_package_ensure changed from 'present' to undef.</p>\n</li>\n<li>\n<p>The manage_service option has been replaced with service_enable and\nservice_ensure.</p>\n</li>\n</ul><h3><a id=\"user-content-beginning-with-cassandra\" class=\"anchor\" aria-hidden=\"true\" href=\"#beginning-with-cassandra\"></a>Beginning with Cassandra</h3>\n<p>Create a Cassandra 2.X cluster called MyCassandraCluster which uses the\nGossipingPropertyFileSnitch and password authentication.  In this very\nbasic example the node itself becomes a seed for the cluster and the\ncredentials will default to a user called cassandra with a password\ncalled of cassandra.</p>\n<div class=\"highlight highlight-source-puppet\"><pre># Cassandra pre-requisites\ninclude cassandra::datastax_repo\ninclude cassandra::java\nclass { 'cassandra':\n  settings =&gt; {\n    'authenticator'               =&gt; 'PasswordAuthenticator',\n    'cluster_name'                =&gt; 'MyCassandraCluster',\n    'commitlog_directory'         =&gt; '/var/lib/cassandra/commitlog',\n    'commitlog_sync'              =&gt; 'periodic',\n    'commitlog_sync_period_in_ms' =&gt; 10000,\n    'data_file_directories'       =&gt; ['/var/lib/cassandra/data'],\n    'endpoint_snitch'             =&gt; 'GossipingPropertyFileSnitch',\n    'listen_address'              =&gt; $::ipaddress,\n    'partitioner'                 =&gt; 'org.apache.cassandra.dht.Murmur3Partitioner',\n    'saved_caches_directory'      =&gt; '/var/lib/cassandra/saved_caches',\n    'seed_provider'               =&gt; [\n      {\n        'class_name' =&gt; 'org.apache.cassandra.locator.SimpleSeedProvider',\n        'parameters' =&gt; [\n          {\n            'seeds' =&gt; $::ipaddress,\n          },\n        ],\n      },\n    ],\n    'start_native_transport'      =&gt; true,\n  },\n  require  =&gt; Class['cassandra::datastax_repo', 'cassandra::java'],\n}</pre></div>\n<p>However, <strong>PLEASE</strong> note that this is the <strong>ABSOLUTE MINIMUM</strong> configuration\nto get Cassandra up and running but will probably give you a rather badly\nconfigured node.  Please see\n<a href=\"https://github.com/voxpupuli/puppet-cassandra/wiki/Suggested-Baseline-Settings\">Suggested Baseline Settings</a>\nfor details on making your configuration a lot more robust.</p>\n<p>For this code to run with version 3.X of Cassandra, the <code>hints_directory</code> will\nalso need to be specified:</p>\n<div class=\"highlight highlight-source-puppet\"><pre>...\nclass { 'cassandra':\n  settings =&gt; {\n    ...\n    'hints_directory'             =&gt; '/var/lib/cassandra/hints',\n    ...\n  },\n  require  =&gt; Class['cassandra::datastax_repo', 'cassandra::java'],\n}</pre></div>\n<h3><a id=\"user-content-hiera\" class=\"anchor\" aria-hidden=\"true\" href=\"#hiera\"></a>Hiera</h3>\n<p>In your top level node classification (usually <code>common.yaml</code>), add the\nsettings hash and all the tweaks you want all the clusters to use:</p>\n<div class=\"highlight highlight-source-yaml\"><pre>cassandra::baseline_settings:\n  authenticator: AllowAllAuthenticator\n  authorizer: AllowAllAuthorizer\n  auto_bootstrap: true\n  auto_snapshot: true\n  ...</pre></div>\n<p>Then, in the individual node classification add the parts which define\nthe cluster:</p>\n<div class=\"highlight highlight-source-yaml\"><pre>cassandra::settings:\n  cluster_name: developer playground cassandra cluster\ncassandra::dc: Onsite1\ncassandra::rack: RAC1\ncassandra::package_ensure: 3.0.5-1\ncassandra::package_name: cassandra30</pre></div>\n<h2><a id=\"user-content-usage\" class=\"anchor\" aria-hidden=\"true\" href=\"#usage\"></a>Usage</h2>\n<h3><a id=\"user-content-setup-a-keyspace-and-users\" class=\"anchor\" aria-hidden=\"true\" href=\"#setup-a-keyspace-and-users\"></a>Setup a keyspace and users</h3>\n<p>We assume that authentication has been enabled for the cassandra\ncluster and we are connecting with the default user name and password\n('cassandra/cassandra').</p>\n<p>In this example, we create a keyspace (mykeyspace) with a table called\n'users' and an index called 'users_lname_idx'.</p>\n<p>We also add three users (to Cassandra, not the mykeyspace.users\ntable) called spillman, akers and boone while ensuring that a user\ncalled lucan is absent.</p>\n<div class=\"highlight highlight-source-puppet\"><pre>class { 'cassandra':\n  ...\n}\nclass { 'cassandra::schema':\n  cqlsh_password =&gt; 'cassandra',\n  cqlsh_user     =&gt; 'cassandra',\n  cqlsh_host     =&gt; $::ipaddress,\n  indexes        =&gt; {\n    'users_lname_idx' =&gt; {\n      table    =&gt; 'users',\n      keys     =&gt; 'lname',\n      keyspace =&gt; 'mykeyspace',\n    },\n  },\n  keyspaces      =&gt; {\n    'mykeyspace' =&gt; {\n      durable_writes  =&gt; false,\n      replication_map =&gt; {\n        keyspace_class     =&gt; 'SimpleStrategy',\n        replication_factor =&gt; 1,\n      },\n    }\n  },\n  permissions    =&gt; {\n    'Grant select permissions to spillman to all keyspaces' =&gt; {\n      permission_name =&gt; 'SELECT',\n      user_name       =&gt; 'spillman',\n    },\n    'Grant modify to to keyspace mykeyspace to akers'       =&gt; {\n      keyspace_name   =&gt; 'mykeyspace',\n      permission_name =&gt; 'MODIFY',\n      user_name       =&gt; 'akers',\n    },\n    'Grant alter permissions to mykeyspace to boone'        =&gt; {\n      keyspace_name   =&gt; 'mykeyspace',\n      permission_name =&gt; 'ALTER',\n      user_name       =&gt; 'boone',\n    },\n    'Grant ALL permissions to mykeyspace.users to gbennet'  =&gt; {\n      keyspace_name   =&gt; 'mykeyspace',\n      permission_name =&gt; 'ALTER',\n      table_name      =&gt; 'users',\n      user_name       =&gt; 'gbennet',\n    },\n  },\n  tables         =&gt; {\n    'users' =&gt; {\n      columns  =&gt; {\n        user_id       =&gt; 'int',\n        fname         =&gt; 'text',\n        lname         =&gt; 'text',\n        'PRIMARY KEY' =&gt; '(user_id)',\n      },\n      keyspace =&gt; 'mykeyspace',\n    },\n  },\n  users          =&gt; {\n    'spillman' =&gt; {\n      password =&gt; 'Niner27',\n    },\n    'akers'    =&gt; {\n      password  =&gt; 'Niner2',\n      superuser =&gt; true,\n    },\n    'boone'    =&gt; {\n      password =&gt; 'Niner75',\n    },\n    'gbennet'  =&gt; {\n      'password' =&gt; 'foobar',\n    },\n    'lucan'    =&gt; {\n      'ensure' =&gt; absent\n    },\n  },\n}</pre></div>\n<h3><a id=\"user-content-create-a-cluster-in-a-single-data-center\" class=\"anchor\" aria-hidden=\"true\" href=\"#create-a-cluster-in-a-single-data-center\"></a>Create a Cluster in a Single Data Center</h3>\n<p>In the DataStax documentation <em>Initializing a multiple node cluster (single\ndata center)</em>\n<a href=\"http://docs.datastax.com/en/cassandra/2.2/cassandra/initialize/initSingleDS.html\" rel=\"nofollow\">http://docs.datastax.com/en/cassandra/2.2/cassandra/initialize/initSingleDS.html</a>\nthere is a basic example of a six node cluster with two seeds to be created in\na single data center spanning two racks.  The nodes in the cluster are:</p>\n<table><thead><tr><th><strong>Node Name</strong></th>\n<th><strong>IP Address</strong></th>\n</tr></thead><tbody><tr><td>node0 (seed 1)</td>\n<td>110.82.155.0</td>\n</tr><tr><td>node1</td>\n<td>110.82.155.1</td>\n</tr><tr><td>node2</td>\n<td>110.82.155.2</td>\n</tr><tr><td>node3 (seed 2)</td>\n<td>110.82.156.3</td>\n</tr><tr><td>node4</td>\n<td>110.82.156.4</td>\n</tr><tr><td>node5</td>\n<td>110.82.156.5</td>\n</tr></tbody></table><p>Each node is configured to use the GossipingPropertyFileSnitch and 256 virtual\nnodes (vnodes).  The name of the cluster is <em>MyCassandraCluster</em>.  Also,\nwhile building the initial cluster, we are setting the auto_bootstrap\nto false.</p>\n<p>In this initial example, we are going to expand the example by:</p>\n<ul><li>Ensuring that the software is installed via the DataStax Community\nrepository by including <code>cassandra::datastax_repo</code>.  This needs to be\nexecuted before the Cassandra package is installed.</li>\n<li>That a suitable Java Runtime environment (JRE) is installed with Java Native\nAccess (JNA) by including <code>cassandra::java</code>.  This need to be executed\nbefore the Cassandra service is started.</li>\n</ul><div class=\"highlight highlight-source-puppet\"><pre>node /^node\\d+$/ {\n  class { 'cassandra::datastax_repo':\n    before =&gt; Class['cassandra']\n  }\n  class { 'cassandra::java':\n    before =&gt; Class['cassandra']\n  }\n  class { 'cassandra':\n    settings       =&gt; {\n      'authenticator'               =&gt; 'AllowAllAuthenticator',\n      'auto_bootstrap'              =&gt; false,\n      'cluster_name'                =&gt; 'MyCassandraCluster',\n      'commitlog_directory'         =&gt; '/var/lib/cassandra/commitlog',\n      'commitlog_sync'              =&gt; 'periodic',\n      'commitlog_sync_period_in_ms' =&gt; 10000,\n      'data_file_directories'       =&gt; ['/var/lib/cassandra/data'],\n      'endpoint_snitch'             =&gt; 'GossipingPropertyFileSnitch',\n      'hints_directory'             =&gt; '/var/lib/cassandra/hints',\n      'listen_interface'            =&gt; 'eth1',\n      'num_tokens'                  =&gt; 256,\n      'partitioner'                 =&gt; 'org.apache.cassandra.dht.Murmur3Partitioner',\n      'saved_caches_directory'      =&gt; '/var/lib/cassandra/saved_caches',\n      'seed_provider'               =&gt; [\n        {\n          'class_name' =&gt; 'org.apache.cassandra.locator.SimpleSeedProvider',\n          'parameters' =&gt; [\n            {\n              'seeds' =&gt; '110.82.155.0,110.82.156.3',\n            },\n          ],\n        },\n      ],\n      'start_native_transport'      =&gt; true,\n    },\n  }\n}</pre></div>\n<p>The default value for the num_tokens is already 256, but it is\nincluded in the example for clarity.  Do not forget to either\nset auto_bootstrap to true or not set the attribute at all\nafter initializing the cluster.</p>\n<h3><a id=\"user-content-create-a-cluster-in-multiple-data-centers\" class=\"anchor\" aria-hidden=\"true\" href=\"#create-a-cluster-in-multiple-data-centers\"></a>Create a Cluster in Multiple Data Centers</h3>\n<p>To continue with the examples provided by DataStax, we look at the example\nfor a cluster across multiple data centers\n<a href=\"http://docs.datastax.com/en/cassandra/2.2/cassandra/initialize/initMultipleDS.html\" rel=\"nofollow\">http://docs.datastax.com/en/cassandra/2.2/cassandra/initialize/initMultipleDS.html</a>.</p>\n<table><thead><tr><th><strong>Node Name</strong></th>\n<th><strong>IP Address</strong></th>\n<th><strong>Data Center</strong></th>\n<th><strong>Rack</strong></th>\n</tr></thead><tbody><tr><td>node0 (seed 1)</td>\n<td>10.168.66.41</td>\n<td>DC1</td>\n<td>RAC1</td>\n</tr><tr><td>node1</td>\n<td>10.176.43.66</td>\n<td>DC1</td>\n<td>RAC1</td>\n</tr><tr><td>node2</td>\n<td>10.168.247.41</td>\n<td>DC1</td>\n<td>RAC1</td>\n</tr><tr><td>node3 (seed 2)</td>\n<td>10.176.170.59</td>\n<td>DC2</td>\n<td>RAC1</td>\n</tr><tr><td>node4</td>\n<td>10.169.61.170</td>\n<td>DC2</td>\n<td>RAC1</td>\n</tr><tr><td>node5</td>\n<td>10.169.30.138</td>\n<td>DC2</td>\n<td>RAC1</td>\n</tr></tbody></table><p>For the sake of simplicity, we will confine this example to the nodes:</p>\n<div class=\"highlight highlight-source-puppet\"><pre>node /^node[012]$/ {\n  class { 'cassandra':\n    dc             =&gt; 'DC1',\n    settings       =&gt; {\n      'authenticator'               =&gt; 'AllowAllAuthenticator',\n      'auto_bootstrap'              =&gt; false,\n      'cluster_name'                =&gt; 'MyCassandraCluster',\n      'commitlog_directory'         =&gt; '/var/lib/cassandra/commitlog',\n      'commitlog_sync'              =&gt; 'periodic',\n      'commitlog_sync_period_in_ms' =&gt; 10000,\n      'data_file_directories'       =&gt; ['/var/lib/cassandra/data'],\n      'endpoint_snitch'             =&gt; 'GossipingPropertyFileSnitch',\n      'hints_directory'             =&gt; '/var/lib/cassandra/hints',\n      'listen_interface'            =&gt; 'eth1',\n      'num_tokens'                  =&gt; 256,\n      'partitioner'                 =&gt; 'org.apache.cassandra.dht.Murmur3Partitioner',\n      'saved_caches_directory'      =&gt; '/var/lib/cassandra/saved_caches',\n      'seed_provider'               =&gt; [\n        {\n          'class_name' =&gt; 'org.apache.cassandra.locator.SimpleSeedProvider',\n          'parameters' =&gt; [\n            {\n              'seeds' =&gt; '110.82.155.0,110.82.156.3',\n            },\n          ],\n        },\n      ],\n      'start_native_transport'      =&gt; true,\n    },\n  }\n}\nnode /^node[345]$/ {\n  class { 'cassandra':\n    dc             =&gt; 'DC2',\n    settings       =&gt; {\n      'authenticator'               =&gt; 'AllowAllAuthenticator',\n      'auto_bootstrap'              =&gt; false,\n      'cluster_name'                =&gt; 'MyCassandraCluster',\n      'commitlog_directory'         =&gt; '/var/lib/cassandra/commitlog',\n      'commitlog_sync'              =&gt; 'periodic',\n      'commitlog_sync_period_in_ms' =&gt; 10000,\n      'data_file_directories'       =&gt; ['/var/lib/cassandra/data'],\n      'endpoint_snitch'             =&gt; 'GossipingPropertyFileSnitch',\n      'hints_directory'             =&gt; '/var/lib/cassandra/hints',\n      'listen_interface'            =&gt; 'eth1',\n      'num_tokens'                  =&gt; 256,\n      'partitioner'                 =&gt; 'org.apache.cassandra.dht.Murmur3Partitioner',\n      'saved_caches_directory'      =&gt; '/var/lib/cassandra/saved_caches',\n      'seed_provider'               =&gt; [\n        {\n          'class_name' =&gt; 'org.apache.cassandra.locator.SimpleSeedProvider',\n          'parameters' =&gt; [\n            {\n              'seeds' =&gt; '110.82.155.0,110.82.156.3',\n            },\n          ],\n        },\n      ],\n      'start_native_transport'      =&gt; true,\n    },\n  }\n}</pre></div>\n<p>We don't need to specify the rack name (with the rack attribute) as RAC1 is\nthe default value.  Again, do not forget to either set auto_bootstrap to\ntrue or not set the attribute at all after initializing the cluster.</p>\n<h3><a id=\"user-content-datastax-enterprise\" class=\"anchor\" aria-hidden=\"true\" href=\"#datastax-enterprise\"></a>DataStax Enterprise</h3>\n<p>After configuring the relevant repository, the following snippet works on\nCentOS 7 to install DSE Cassandra 4.7.0, set the HADOOP_LOG_DIR, set the\nDSE_HOME and configure DataStax Enterprise to use LDAP for authentication:</p>\n<div class=\"highlight highlight-source-puppet\"><pre>class { 'cassandra::datastax_repo':\n  descr   =&gt; 'DataStax Repo for DataStax Enterprise',\n  pkg_url =&gt; 'https://username:password@rpm.datastax.com/enterprise',\n  before  =&gt; Class['cassandra'],\n}\nclass { 'cassandra':\n  cluster_name    =&gt; 'MyCassandraCluster',\n  config_path     =&gt; '/etc/dse/cassandra',\n  package_ensure  =&gt; '4.7.0-1',\n  package_name    =&gt; 'dse-full',\n  service_name    =&gt; 'dse',\n  ...\n}\nclass { 'cassandra::dse':\n  file_lines =&gt; {\n    'Set HADOOP_LOG_DIR directory' =&gt; {\n      ensure =&gt; present,\n      path   =&gt; '/etc/dse/dse-env.sh',\n      line   =&gt; 'export HADOOP_LOG_DIR=/var/log/hadoop',\n      match  =&gt; '^# export HADOOP_LOG_DIR=&lt;log_dir&gt;',\n    },\n    'Set DSE_HOME'                 =&gt; {\n      ensure =&gt; present,\n      path   =&gt; '/etc/dse/dse-env.sh',\n      line   =&gt; 'export DSE_HOME=/usr/share/dse',\n      match  =&gt; '^#export DSE_HOME',\n    },\n  },\n  settings   =&gt; {\n    ldap_options =&gt; {\n      server_host                =&gt; localhost,\n      server_port                =&gt; 389,\n      search_dn                  =&gt; 'cn=Admin',\n      search_password            =&gt; secret,\n      use_ssl                    =&gt; false,\n      use_tls                    =&gt; false,\n      truststore_type            =&gt; jks,\n      user_search_base           =&gt; 'ou=users,dc=example,dc=com',\n      user_search_filter         =&gt; '(uid={0})',\n      credentials_validity_in_ms =&gt; 0,\n      connection_pool            =&gt; {\n        max_active =&gt; 8,\n        max_idle   =&gt; 8,\n      }\n    }\n  }\n}</pre></div>\n<h3><a id=\"user-content-apache-cassandra\" class=\"anchor\" aria-hidden=\"true\" href=\"#apache-cassandra\"></a>Apache Cassandra</h3>\n<p>DataStax announced in late October 2016 that it was no longer supporting\nthe community edition of Cassandra or DSC as it was known (see\n<em>[Take a bow Planet\nCassandra]</em>(<a href=\"http://www.datastax.com/2016/10/take-a-bow-planet-cassandra\" rel=\"nofollow\">http://www.datastax.com/2016/10/take-a-bow-planet-cassandra</a>)\nfor details).  However, the following snippet of code running on Ubuntu\n14.04 worked fine without having to change any of the <code>::cassandra</code> class\nsettings:</p>\n<div class=\"highlight highlight-source-puppet\"><pre>require cassandra::java\ninclude cassandra::optutils\nclass { 'cassandra::apache_repo':\n  release =&gt; '310x',\n  before  =&gt; Class['cassandra', 'cassandra::optutils'],\n}\nclass { 'cassandra':\n  ...\n}</pre></div>\n<h2><a id=\"user-content-reference\" class=\"anchor\" aria-hidden=\"true\" href=\"#reference\"></a>Reference</h2>\n<p>The reference documentation is generated using the\n<a href=\"https://github.com/puppetlabs/puppet-strings\">puppet-strings</a> tool.  To see\nall of it, please go to\n<a href=\"http://voxpupuli.github.io/puppet-cassandra/_index.html\" rel=\"nofollow\">http://voxpupuli.github.io/puppet-cassandra</a>.</p>\n<h2><a id=\"user-content-limitations\" class=\"anchor\" aria-hidden=\"true\" href=\"#limitations\"></a>Limitations</h2>\n<ul><li>When using a Ruby version before 1.9.0, the contents of the Cassandra\nconfiguration file may change order of elements due to a problem with\nto_yaml in earlier versions of Ruby.</li>\n<li>When creating key spaces, indexes, cql_types and users the settings will only\nbe used to create a new resource if it does not currently exist.  If a change\nis made to the Puppet manifest but the resource already exits, this change\nwill not be reflected.</li>\n<li>At the moment the <code>cassandra::system::transparent_hugepage</code> does not\npersist between reboots.</li>\n<li>Acceptance for Debian 7 are confined to Cassandra 2.1 and 2.2.  There is\na conflict between the GLIBC on Debian 7 and the newer releases of Cassandra\n3.X.</li>\n</ul><h2><a id=\"user-content-development\" class=\"anchor\" aria-hidden=\"true\" href=\"#development\"></a>Development</h2>\n<p>Contributions will be gratefully accepted.  Please go to the project page,\nfork the project, make your changes locally and then raise a pull request.\nDetails on how to do this are available at\n<a href=\"https://guides.github.com/activities/contributing-to-open-source\">https://guides.github.com/activities/contributing-to-open-source</a>.</p>\n<p>Please also see the\n<a href=\"https://github.com/voxpupuli/puppet-cassandra/blob/master/CONTRIBUTING.md\">CONTRIBUTING.md</a>\npage for project specific requirements.</p>\n<h3><a id=\"user-content-additional-contributers\" class=\"anchor\" aria-hidden=\"true\" href=\"#additional-contributers\"></a>Additional Contributers</h3>\n<p>For a list of contributers see\n<a href=\"https://github.com/voxpupuli/puppet-cassandra/blob/master/CONTRIBUTING.md\">CONTRIBUTING.md</a>\nand <a href=\"https://github.com/voxpupuli/puppet-cassandra/graphs/contributors\">https://github.com/voxpupuli/puppet-cassandra/graphs/contributors</a></p>\n</article>",
        "created_at": "2018-10-24T12:35:59+0000",
        "updated_at": "2018-10-24T12:36:04+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 14,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/8693967?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12452"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 44,
            "label": "json",
            "slug": "json"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 998,
            "label": "etl",
            "slug": "etl"
          }
        ],
        "is_public": false,
        "id": 12424,
        "uid": null,
        "title": "masumsoft/cassandra-exporter",
        "url": "https://github.com/masumsoft/cassandra-exporter",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>Cassandra exporter is a data export / import tool for cassandra that is simple to use and works for unicode and complex data types. It is developed in Javascript and the exported data is stored in JSON formatted files.</p>\n<p>Cassandra has some great tools for exporting and importing data:</p>\n<ul><li>snapshots</li>\n<li>sstable2json</li>\n<li>CQL's COPY FROM/TO</li>\n</ul><p>But the problem is snapshots and sstable2json are not that straight forward to use. They are intended for moving large data sets and to me unnecessarily complicated to use for day to day development.</p>\n<p>The COPY command was intended for development or moving small datasets, but is not reliable. Because it uses csv exports which breaks for complex data types and non ascii encodings if you try to import that data. So for development purposes and for moving small datasets (&lt; few million rows per table) I needed something that works robustly and is simple to use.</p>\n<p>You can either download the compiled binary for your operating system from the <a href=\"https://github.com/masumsoft/cassandra-exporter/releases\">releases</a> section or if you have nodejs installed, you can use the source code directly to execute the export / import scripts.</p>\n<h2><a id=\"user-content-to-export-all-table-data-from-a-keyspace\" class=\"anchor\" aria-hidden=\"true\" href=\"#to-export-all-table-data-from-a-keyspace\"></a>To export all table data from a keyspace</h2>\n<pre>HOST=127.0.0.1 KEYSPACE=from_keyspace_name ./export\n</pre>\n<p>It will create exported json files in the data directory for each table in the keyspace.</p>\n<h2><a id=\"user-content-to-import-all-table-data-into-a-keyspace\" class=\"anchor\" aria-hidden=\"true\" href=\"#to-import-all-table-data-into-a-keyspace\"></a>To import all table data into a keyspace</h2>\n<pre>HOST=127.0.0.1 KEYSPACE=to_keyspace_name ./import\n</pre>\n<p>It will process all json files in the data directory and import them to corresponding tables in the keyspace.</p>\n<h2><a id=\"user-content-to-exportimport-a-single-table-in-a-keyspace\" class=\"anchor\" aria-hidden=\"true\" href=\"#to-exportimport-a-single-table-in-a-keyspace\"></a>To export/import a single table in a keyspace</h2>\n<pre>HOST=127.0.0.1 KEYSPACE=from_keyspace_name TABLE=my_table_name ./export\nHOST=127.0.0.1 KEYSPACE=to_keyspace_name TABLE=my_table_name ./import\n</pre>\n<h2><a id=\"user-content-to-exportimport-using-authentication\" class=\"anchor\" aria-hidden=\"true\" href=\"#to-exportimport-using-authentication\"></a>To export/import using authentication</h2>\n<pre>KEYSPACE=from_keyspace_name USER=user1 PASSWORD=pa$$word ./export\nKEYSPACE=to_keyspace_name USER=user1 PASSWORD=pa$$word ./import\n</pre>\n<p>Please note that the user requires access to the system tables in order to work properly.</p>\n<p>If you already have nodejs installed in your system, then you can execute using the source directly like this:</p>\n<pre>HOST=127.0.0.1 KEYSPACE=from_keyspace_name TABLE=my_table_name node export.js\nHOST=127.0.0.1 KEYSPACE=from_keyspace_name TABLE=my_table_name node import.js\n</pre>\n<p>The Dockerfiles provide a volume mounted at /data and expect the environment variables <code>HOST</code> and <code>KEYSPACE</code>. <code>Dockerfile.import</code> provides <code>import.js</code> functionality. <code>Dockerfile.export</code> provides <code>export.js</code> functionality. By using the -v option of <code>docker run</code> this provides the facility to store the output/input directory in an arbitrary location. It also allows running cassandra-export from any location. This requires <a href=\"https://www.docker.com/\" rel=\"nofollow\">Docker</a> to be installed.</p>\n<p>To run a test in the tests folder, for example <code>numbers.js</code>, run the command <code>node tests/numbers.js</code> at the root of the repo. A localhost cassandra must be running.</p>\n<p>Tests use recent node.js features and requires Node.js 8.</p>\n<p>Cassandra exporter only export / import data. It expects the tables to be present beforehand. If you need to also export schema and the indexes, then you could easily use cqlsh and the source command to export / import the schema before moving the data.</p>\n<pre>// To export keyspace schema, use cqlsh like this\ncqlsh -e \"DESC KEYSPACE mykeyspace\" &gt; my_keyspace_schema.cql\n// To import keyspace schema open the cqlsh shell\n// in the same directory of `my_keyspace_schema.cql`, then\nsource 'my_keyspace_schema.cql'\n</pre>\n</article>",
        "created_at": "2018-10-22T22:12:15+0000",
        "updated_at": "2018-10-22T22:12:21+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/1887201?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12424"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 24,
            "label": "node",
            "slug": "node-js"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 867,
            "label": "express",
            "slug": "express"
          }
        ],
        "is_public": false,
        "id": 12421,
        "uid": null,
        "title": "masumsoft/express-cassandra",
        "url": "https://github.com/masumsoft/express-cassandra",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\"><p><a href=\"https://travis-ci.org/masumsoft/express-cassandra\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/4228a3d9ffe9a23c07250b7223d97b453945004d/68747470733a2f2f7472617669732d63692e6f72672f6d6173756d736f66742f657870726573732d63617373616e6472612e737667\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/masumsoft/express-cassandra.svg\" /></a>\n<a href=\"https://www.npmjs.com/package/express-cassandra\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/cd0f3a27efabd7686d2b5f9a51347f8ce2821168/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f646d2f657870726573732d63617373616e6472612e737667\" alt=\"Download Stats\" data-canonical-src=\"https://img.shields.io/npm/dm/express-cassandra.svg\" /></a>\n<a href=\"https://www.npmjs.com/package/express-cassandra\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/e0280d4d2cd3949e9183755f9d1b6bf0186302a3/68747470733a2f2f62616467652e667572792e696f2f6a732f657870726573732d63617373616e6472612e737667\" alt=\"Npm Version\" data-canonical-src=\"https://badge.fury.io/js/express-cassandra.svg\" /></a>\n<a href=\"http://express-cassandra.readthedocs.io/en/latest/?badge=latest\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/3154c44be2ea931683cc0c5a20a0a41656eed475/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f657870726573732d63617373616e6472612f62616467652f3f76657273696f6e3d6c6174657374\" alt=\"Documentation Status\" data-canonical-src=\"https://readthedocs.org/projects/express-cassandra/badge/?version=latest\" /></a>\n<a href=\"https://david-dm.org/masumsoft/express-cassandra\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/0decb5c72cf401125ec1b4c513cfee9084da8118/68747470733a2f2f64617669642d646d2e6f72672f6d6173756d736f66742f657870726573732d63617373616e6472612f7374617475732e737667\" alt=\"Dependencies Status\" data-canonical-src=\"https://david-dm.org/masumsoft/express-cassandra/status.svg\" /></a></p>\n<p>Express-Cassandra is a Cassandra ORM/ODM/OGM for NodeJS with Elassandra &amp; JanusGraph Support.</p>\n<p>No more hassling with raw cql queries from your nodejs web frameworks. Express-Cassandra automatically loads your models and provides you with object oriented mapping to your cassandra tables like a standard ORM/ODM. Built in support for <a href=\"http://www.elassandra.io/\" rel=\"nofollow\">Elassandra</a> and <a href=\"http://janusgraph.org/\" rel=\"nofollow\">JanusGraph</a> allows you to automatically manage synced elasticsearch and janusgraph indexes stored in cassandra.</p>\n<p>Express-cassandra enables your nodejs app to manage a highly available distributed data store capable of handling large dataset with powerful query features powered by cassandra, elasticsearch and janusgraph combined. Express-cassandra empowers you to manage and query this truely distributed datastore with search, analytics and graph computing functionality from nodejs like you are just dealing with javascript objects and methods. Models are written as javascript modules and they automatically create the underlying db tables, indexes, materialized views, graphs etc. Afterwards you can save, update, delete and query your data using supported model methods. It's decoupled nature allows you to use it with many popular node frameworks without much hassle.</p>\n<p>If you are using elassandra, then saved data in cassandra automatically syncs with elasticsearch indexes defined in your schema. You can then do any query <a href=\"https://www.elastic.co/products/elasticsearch\" rel=\"nofollow\">elasticsearch</a> indexes support.</p>\n<p>If you are using janusgraph, then you can easily manage your graphs and graph indexes. Creating vertices and edges become simple function calls. You can then do any graph query the tinkerpop3 <a href=\"http://docs.janusgraph.org/latest/gremlin.html\" rel=\"nofollow\">gremlin query language</a> supports.</p>\n<ul><li>supports the latest cassandra 3.x releases</li>\n<li>support for elassandra index management and search queries</li>\n<li>support for janusgraph graph management and tinkerpop3 gremlin queries</li>\n<li>compatible with datastax enterprise solr search and dse graph</li>\n<li>full featured CRUD operations and data type validations</li>\n<li>full support for collections and other advanced data types</li>\n<li>support for materialized views, secondary/custom/sasi indexes</li>\n<li>support for complex queries, streaming and token based pagination</li>\n<li>support for user defined types/functions/aggregates</li>\n<li>support for batching ORM operations for atomic updates</li>\n<li>support for before and after hook functions for save/update/delete</li>\n<li>promise support with async suffixed functions</li>\n<li>built-in experimental support for automatic migrations</li>\n<li>built-in experimental support for fixture data import/export</li>\n<li>optional support for query debug and progress logs</li>\n<li>most of the bleeding edge cassandra features are supported</li>\n</ul><p>This module uses datastax <a href=\"https://github.com/datastax/nodejs-driver\">cassandra-driver</a> by default or uses the <a href=\"https://github.com/datastax/nodejs-driver-dse\">dse-driver</a> if already installed.</p>\n<p>If you are using apache cassandra, then the module should work just fine. Datastax enterprise users can use this module, but needs to install the dse-driver separately in your app.</p>\n<p>For apache cassandra version 3.x</p>\n<pre>npm install express-cassandra\n</pre>\n<p>For datastax enterprise version 5.x</p>\n<pre>npm install express-cassandra dse-driver@1.2.0\n</pre>\n<p>For elassandra version 5.5 or above</p>\n<pre>npm install express-cassandra elasticsearch\n</pre>\n<p>For janusgraph version 0.2.0 or above</p>\n<pre>npm install express-cassandra gremlin\n</pre>\n<p>For older cassandra 2.x (no longer supported)</p>\n<pre>npm install express-cassandra@0.5.4\n</pre>\n<p>Please note that if you use the legacy cassandra 2.x compliant version then please use the corresponding README.md file for that version. The following documentation is for version 3.x and datastax enterprise 5.x only. The materialized view support and several other part of the documentation is strictly applicable for cassandra 3.x / dse 5.x and will not work in earlier versions of cassandra.</p>\n<p>Read the full usage documentation in the <a href=\"http://express-cassandra.readthedocs.io\" rel=\"nofollow\">express-cassandra documentation</a> site.</p>\n<p>A detailed changelog for released versions are available in the <a href=\"https://github.com/masumsoft/express-cassandra/blob/master/CHANGELOG.md\">changelog</a> section.</p>\n<p>All contributions, bug reports, bug fixes, documentation improvements, enhancements and ideas are welcome.</p>\n<p>A detailed overview on how to contribute can be found in the <a href=\"https://github.com/masumsoft/express-cassandra/blob/master/CONTRIBUTING.md\">contributing guide</a>.</p>\n<ul><li>\n<p>Express-cassandra started off from a highly modified version of <a href=\"https://github.com/3logic/apollo-cassandra\">apollo-cassandra</a> module. Afterwards major refactoring and new development went on to support missing features of cassandra 3.x and additional functionalities.</p>\n</li>\n<li>\n<p>Apache Cassandra, Apache Lucene, Apache, Lucene, Solr, TinkerPop, and Cassandra are trademarks of the Apache Software Foundation or its subsidiaries in Canada, the United States and/or other countries.</p>\n</li>\n<li>\n<p>DataStax, Titan, and TitanDB are registered trademark of DataStax, Inc. and its subsidiaries in the United States and/or other countries.</p>\n</li>\n<li>\n<p>Elasticsearch and Kibana are trademarks of Elasticsearch BV, registered in the U.S. and in other countries.</p>\n</li>\n<li>\n<p>Elassandra is a trademark of Strapdata SAS.</p>\n</li>\n<li>\n<p>JanusGraph is a trademark of The Linux Foundation.</p>\n</li>\n</ul></article>",
        "created_at": "2018-10-22T22:02:48+0000",
        "updated_at": "2018-10-22T22:02:52+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/1887201?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12421"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 901,
            "label": "security",
            "slug": "security"
          }
        ],
        "is_public": false,
        "id": 12420,
        "uid": null,
        "title": "Cassandra - Database - Secrets Engines - Vault by HashiCorp",
        "url": "https://www.vaultproject.io/docs/secrets/databases/cassandra.html",
        "content": "<p>Cassandra is one of the supported plugins for the database secrets engine. This\nplugin generates database credentials dynamically based on configured roles for\nthe Cassandra database.</p><p>See the <a href=\"https://www.vaultproject.io/docs/secrets/databases/index.html\">database secrets engine</a> docs for\nmore information about setting up the database secrets engine.</p><h2 id=\"setup\">\n  <a name=\"setup\" class=\"anchor\" href=\"#setup\">»</a>\n  Setup\n</h2><ol><li><p>Enable the database secrets engine if it is not already enabled:</p>\n<div class=\"highlight\"><pre>$ vault secrets enable database\nSuccess! Enabled the database secrets engine at: database/\n</pre></div><p>By default, the secrets engine will enable at the name of the engine. To\nenable the secrets engine at a different path, use the <code>-path</code> argument.</p>\n</li>\n<li><p>Configure Vault with the proper plugin and connection information:</p>\n<div class=\"highlight\"><pre>$ vault write database/config/my-cassandra-database \\\n    plugin_name=\"cassandra-database-plugin\" \\\n    hosts=127.0.0.1 \\\n    protocol_version=4 \\\n    username=cassandra \\\n    password=cassandra \\\n    allowed_roles=my-role\n</pre></div></li>\n<li><p>Configure a role that maps a name in Vault to an SQL statement to execute to\ncreate the database credential:</p>\n<div class=\"highlight\"><pre>$ vault write database/roles/my-role \\\n    db_name=my-cassandra-database \\\n    creation_statements=\"CREATE USER '{{username}}' WITH PASSWORD '{{password}}' NOSUPERUSER; \\\n         GRANT SELECT ON ALL KEYSPACES TO {{username}};\" \\\n    default_ttl=\"1h\" \\\n    max_ttl=\"24h\"\nSuccess! Data written to: database/roles/my-role\n</pre></div></li>\n</ol><h2 id=\"usage\">\n  <a name=\"usage\" class=\"anchor\" href=\"#usage\">»</a>\n  Usage\n</h2><p>After the secrets engine is configured and a user/machine has a Vault token with\nthe proper permission, it can generate credentials.</p><ol><li><p>Generate a new credential by reading from the <code>/creds</code> endpoint with the name\nof the role:</p>\n<div class=\"highlight\"><pre>$ vault read database/creds/my-role\nKey                Value\n---                -----\nlease_id           database/creds/my-role/2f6a614c-4aa2-7b19-24b9-ad944a8d4de6\nlease_duration     1h\nlease_renewable    true\npassword           8cab931c-d62e-a73d-60d3-5ee85139cd66\nusername           v-root-e2978cd0-\n</pre></div></li>\n</ol><h2 id=\"api\">\n  <a name=\"api\" class=\"anchor\" href=\"#api\">»</a>\n  API\n</h2><p>The full list of configurable options can be seen in the <a href=\"https://www.vaultproject.io/api/secret/databases/cassandra.html\">Cassandra database\nplugin API</a> page.</p><p>For more information on the database secrets engine's HTTP API please see the <a href=\"https://www.vaultproject.io/api/secret/databases/index.html\">Database secret\nsecrets engine API</a> page.</p>",
        "created_at": "2018-10-22T17:56:23+0000",
        "updated_at": "2018-10-22T17:56:29+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "www.vaultproject.io",
        "preview_picture": "https://www.vaultproject.io/img/og-image.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12420"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 85,
            "label": "interface",
            "slug": "interface"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 217,
            "label": "tool",
            "slug": "tool"
          }
        ],
        "is_public": false,
        "id": 12415,
        "uid": null,
        "title": "Kindrat/cassandra-client",
        "url": "https://github.com/Kindrat/cassandra-client",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>This client is intended to be a simple GUI solution to work with cassandra 3.</p>\n<p><a href=\"https://www.codefactor.io/repository/github/kindrat/cassandra-client\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/d47fe124f51c557b99de616ee63956dfd6a32724/68747470733a2f2f7777772e636f6465666163746f722e696f2f7265706f7369746f72792f6769746875622f6b696e647261742f63617373616e6472612d636c69656e742f6261646765\" alt=\"CodeFactor\" data-canonical-src=\"https://www.codefactor.io/repository/github/kindrat/cassandra-client/badge\" /></a>\n<a href=\"https://travis-ci.com/Kindrat/cassandra-client\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/76c63b736954930a6b063c40e5504e49b24e60c8/68747470733a2f2f7472617669732d63692e636f6d2f4b696e647261742f63617373616e6472612d636c69656e742e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.com/Kindrat/cassandra-client.svg?branch=master\" /></a>\n<a href=\"https://codecov.io/gh/Kindrat/cassandra-client\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/e0f10f05423f85b03df7808c1a655a7a675e7638/68747470733a2f2f636f6465636f762e696f2f67682f4b696e647261742f63617373616e6472612d636c69656e742f6272616e63682f6d61737465722f67726170682f62616467652e737667\" alt=\"codecov\" data-canonical-src=\"https://codecov.io/gh/Kindrat/cassandra-client/branch/master/graph/badge.svg\" /></a>\n<a href=\"https://snyk.io/test/github/Kindrat/cassandra-client\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a04da00457da90fbe9c36a27a18465c21fb2ce0a/68747470733a2f2f736e796b2e696f2f746573742f6769746875622f4b696e647261742f63617373616e6472612d636c69656e742f62616467652e737667\" alt=\"Known Vulnerabilities\" data-canonical-src=\"https://snyk.io/test/github/Kindrat/cassandra-client/badge.svg\" /></a></p>\n<p>What it already can:</p>\n<ul class=\"contains-task-list\"><li class=\"task-list-item\"> connect to cassandra</li>\n<li class=\"task-list-item\"> load and show tables</li>\n<li class=\"task-list-item\"> show table DDL</li>\n<li class=\"task-list-item\"> show table data (simple editable table view with header)</li>\n<li class=\"task-list-item\"> apply composite filters to loaded data</li>\n<li class=\"task-list-item\"> execute query</li>\n<li class=\"task-list-item\"> lazy data load/pagination</li>\n<li class=\"task-list-item\"> add/delete tables</li>\n<li class=\"task-list-item\"> validation in filter values</li>\n<li class=\"task-list-item\"> safe mode with manual commit-reset</li>\n<li class=\"task-list-item\"> add/save connections</li>\n<li class=\"task-list-item\"> select driver</li>\n<li class=\"task-list-item\"> load driver files</li>\n<li class=\"task-list-item\"> packaging</li>\n</ul><h2><a id=\"user-content-requirements\" class=\"anchor\" aria-hidden=\"true\" href=\"#requirements\"></a>Requirements</h2>\n<ul><li>Install JDK8 (<a href=\"http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html\" rel=\"nofollow\">Oracle</a>, <a href=\"http://openjdk.java.net/\" rel=\"nofollow\">OpenJDK</a>)</li>\n<li>Set <code>JAVA_HOME</code> env variable <a href=\"https://docs.oracle.com/cd/E19182-01/820-7851/inst_cli_jdk_javahome_t/\" rel=\"nofollow\">doc</a></li>\n<li>If you are using OpenJDK, make sure to have installed openjfx.</li>\n</ul><h2><a id=\"user-content-build-and-run\" class=\"anchor\" aria-hidden=\"true\" href=\"#build-and-run\"></a>Build and run</h2>\n<ol><li>\n<p>Clone the source if you haven't done so. <code>git clone https://github.com/Kindrat/cassandra-client.git</code></p>\n</li>\n<li>\n<p>Go to the directory: cd <code>cassandra-client</code></p>\n</li>\n<li>\n<p>Build sources using gradle (add <code>-x test</code> to provided cli command to skip tests):</p>\n<p>3.1 For Windows <code>./gradlew.bat build</code></p>\n<p>3.2 For Unix <code>./gradlew build</code></p>\n</li>\n<li>\n<p>Run client <code>java -jar build/libs/cassadra-client-1.0.2.jar</code> or from gradle <code>./gradlew bootRun</code></p>\n</li>\n</ol><h3><a id=\"user-content-editor-window\" class=\"anchor\" aria-hidden=\"true\" href=\"#editor-window\"></a>Editor window</h3>\n<p>On selecting table data from context menu in table list all rows are loaded from cassandra\nthat is quite dangerous when having millions of entries in single table. Lazy loading and\npagination is planned but not implemented yet. <br />On cell edit updated row is immediately sent to cassandra - I'm planning to add <em>safe mode</em>\nby executing DB queries only on <em>commit button</em> click with ability to reset all local uncommited\nchanges.</p>\n<h3><a id=\"user-content-available-filters\" class=\"anchor\" aria-hidden=\"true\" href=\"#available-filters\"></a>Available filters</h3>\n<p>Type of cassandra column is respected. String value from filter is converted to same\ntype using cassandra driver codecs and column metadata. Filters are combined with\n<code>AND</code> <code>OR</code> keywords and parentheses brackets.</p>\n<div class=\"highlight highlight-source-sql\"><pre>var1 = val1 AND var2 &lt;= val2 OR (var1 != val2 AND var5 LIKE .*test_value{1,2}.*)</pre></div>\n<ul><li>\n<p><strong>equal check</strong><br /><em>field = value</em><br /></p>\n</li>\n<li>\n<p><strong>not equal</strong><br /><em>field != value</em><br /></p>\n</li>\n<li>\n<p><strong>less or equal</strong><br /><em>field &lt;= value</em><br /></p>\n</li>\n<li>\n<p><strong>less than</strong><br /><em>field &lt; value</em><br /></p>\n</li>\n<li>\n<p><strong>greater or equal</strong><br /><em>field &gt;= value</em><br /></p>\n</li>\n<li>\n<p><strong>greater than</strong><br /><em>field &gt; value</em><br /></p>\n</li>\n<li>\n<p><strong>string REGEX check</strong><br /><em>field LIKE value</em><br />value should represent Java Pattern-style regex</p>\n</li>\n</ul><h3><a id=\"user-content-gui\" class=\"anchor\" aria-hidden=\"true\" href=\"#gui\"></a>GUI</h3>\n<h4><a id=\"user-content-main-window\" class=\"anchor\" aria-hidden=\"true\" href=\"#main-window\"></a>Main window</h4>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://raw.githubusercontent.com/Kindrat/cassandra-client/master/doc/window.png\"><img src=\"https://raw.githubusercontent.com/Kindrat/cassandra-client/master/doc/window.png\" alt=\"main window\" /></a></p>\n<h4><a id=\"user-content-connect\" class=\"anchor\" aria-hidden=\"true\" href=\"#connect\"></a>Connect</h4>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://raw.githubusercontent.com/Kindrat/cassandra-client/master/doc/connect_popup.png\"><img src=\"https://raw.githubusercontent.com/Kindrat/cassandra-client/master/doc/connect_popup.png\" alt=\"connect popup\" /></a></p>\n<h4><a id=\"user-content-list-tables\" class=\"anchor\" aria-hidden=\"true\" href=\"#list-tables\"></a>List tables</h4>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://raw.githubusercontent.com/Kindrat/cassandra-client/master/doc/list_tables.png\"><img src=\"https://raw.githubusercontent.com/Kindrat/cassandra-client/master/doc/list_tables.png\" alt=\"list tables\" /></a></p>\n<h4><a id=\"user-content-show-ddl\" class=\"anchor\" aria-hidden=\"true\" href=\"#show-ddl\"></a>Show DDL</h4>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://raw.githubusercontent.com/Kindrat/cassandra-client/master/doc/show_ddl.png\"><img src=\"https://raw.githubusercontent.com/Kindrat/cassandra-client/master/doc/show_ddl.png\" alt=\"show DDL\" /></a></p>\n<h4><a id=\"user-content-show-data\" class=\"anchor\" aria-hidden=\"true\" href=\"#show-data\"></a>Show data</h4>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://raw.githubusercontent.com/Kindrat/cassandra-client/master/doc/show_data.png\"><img src=\"https://raw.githubusercontent.com/Kindrat/cassandra-client/master/doc/show_data.png\" alt=\"show DDL\" /></a></p>\n</article>",
        "created_at": "2018-10-21T18:49:21+0000",
        "updated_at": "2018-10-21T18:49:27+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/2321291?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12415"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 901,
            "label": "security",
            "slug": "security"
          }
        ],
        "is_public": false,
        "id": 12403,
        "uid": null,
        "title": "securing-cassandra",
        "url": "https://8kmiles.com/blog/securing-cassandra/",
        "content": null,
        "created_at": "2018-10-18T16:13:08+0000",
        "updated_at": "2018-10-18T16:13:13+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": null,
        "language": null,
        "reading_time": 0,
        "domain_name": "8kmiles.com",
        "preview_picture": null,
        "http_status": null,
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12403"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 901,
            "label": "security",
            "slug": "security"
          }
        ],
        "is_public": false,
        "id": 12402,
        "uid": null,
        "title": "Hardening Cassandra Step by Step - Part 1 Inter-Node Encryption (And a Gentle Intro to Certificates)",
        "url": "http://thelastpickle.com/blog/2015/09/30/hardening-cassandra-step-by-step-part-1-server-to-server.html",
        "content": "<h2 id=\"tldr\">TL;DR:</h2><p>This is a tutorial extracted from part of a presentation I gave at Cassandra Summit 2015 titled <a href=\"http://cassandrasummit-datastax.com/agenda/hardening-apache-cassandra-for-compliance-or-paranoia/\">Hardening Cassandra for Compliance (or Paranoia)</a>. The <a href=\"http://www.slideshare.net/zznate/hardening-cassandra-for-compliance-or-paranoia\">slides</a> are available and the “SSL Certificates: a brief interlude” section is probably the most expedient route if you are impatient. We build on that process here by actually installing everything on a local three node cluster. I’ll provide a link to the video of the presentation as soon as it is posted.</p>\n<h2 id=\"overview\">Overview</h2>\n<p>This is the first of a five part tutorial covering the following aspects of securing Apache Cassandra (including some handy features of DataStax Enterprise where relevant). As with the presentation mentioned above, we will break this down into the following:</p>\n<ul><li><strong>Inter-node Communication</strong></li>\n  <li>Encryption at Rest</li>\n  <li>Client-Server Communication</li>\n  <li>Authentication and Authorization</li>\n  <li>Management and Tooling</li>\n</ul><p>Being further down the stack, most developers are not exposed to encryption in their day to day work. Therefor this first post will focus on Inter-node encryption. It is going to be a bit longer than the others as this post walks through the steps to <em>correctly</em> create SSL certificates. We’ll put that knowledge to immediate use by configuring inter-node encryption on a local CCM cluster. Hopefully this tutorial will give you enough information to make the correct choices.</p>\n<p>The <a href=\"http://docs.datastax.com/en/cassandra/2.1/cassandra/security/secureSSLCertificates_t.html\">current documentation</a> describes a  basic approach that is useful for development and experimentation. However, were these steps used in building a production deployment, they would create a substantial maintenance burden and be quite difficult to automate. These concerns would be amplified by the need to scale out.  Most importantly, if followed directly, you will have secured traffic, but have done nothing to thwart a bad actor with network access from attacking your cluster directly.</p>\n<p>Why is this? Because like most examples of generating certificates and configuring SSL, the documentation still holds to the model of a client, like a web browser, talking to a server. In this model, the main concern is that the client is validating the identity of the web server and that the data is secured in transit. The client simply sends a public key to the server identifying itself so they can negotiate a secured connection. The server has no trust relationship with that client, it’s just using the certificate to encrypt the communication.</p>\n<p>Grafting this onto our cluster model, it means that a Cassandra node will open secure sockets with other nodes, but not attempt to identify the actor requesting the connection. Think about that for a second. Without authenticating that we are indeed talking to another Cassandra node, we can write a program to attach to a cluster and execute arbitrary commands, listen to writes on arbitrary token ranges, even inject an administrator account into the <code class=\"highlighter-rouge\">system_auth</code> table with specially crafted message packets. In short, we can do pretty much anything. The best part: the communication between our program and the cluster will be via SSL, so no one will know it’s happening.</p>\n<p>Instead, the model we want to follow is called Client Certificate Authentication. Using this approach, the server takes the extra step of verifying the client against a local trust store (see <a href=\"http://javarevisited.blogspot.com/2012/09/difference-between-truststore-vs-keyStore-Java-SSL.html\">this page</a> for an excellent description of both trust stores and key stores in the context of the JDK). If it does not recognize the client’s certificate either directly or through a chain of trust, it will not accept the connection. The extra verification can be enabled with a single flag in Cassandra’s configuration : <code class=\"highlighter-rouge\">require_client_auth: true</code>.</p>\n<p>Setting this option (as we’ll see in the steps below) enables Client Certificate Authentication as previously discussed. For encrypting inter-node traffic for our cluster, it means that each node has a trust relationship with the rest of the cluster which can be verified against a local Trust Store.</p>\n<p>To demonstrate how to create and install all the components for this, we are going to walk through all the steps necessary to set up inter node encryption in a way that will make it both easy to manage and production deployable.</p>\n<h2 id=\"creating-a-three-node-test-cluster\">Creating a Three Node Test Cluster</h2>\n<p>We’ll start by using <code class=\"highlighter-rouge\">ccm</code> to create a three node cluster which we’ll use to walk through SSL setup. If you are not familiar with <code class=\"highlighter-rouge\">ccm</code>, you can find information and installation instructions <a href=\"https://github.com/pcmanus/ccm\">here</a>.</p>\n<p>Provided you have <code class=\"highlighter-rouge\">ccm</code> setup and configured correctly, the following commands will create and start an Apache Cassandra cluster named <code class=\"highlighter-rouge\">sslverify</code> using Apache Cassandra version <code class=\"highlighter-rouge\">2.1.9</code>:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>ccm create -n 3 -v 2.1.9 sslverify\nccm start\n</pre></div></div>\n<p>Configuration and data for each of the three nodes in the cluster will be placed in sub directories under <code class=\"highlighter-rouge\">~/.ccm/sslverify/</code> following the convention of a normal Cassandra distribution from there on.</p>\n<p>You can verify the cluster’s status from both the <code class=\"highlighter-rouge\">ccm</code> and Cassandra perspectives with:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>ccm status\nccm node1 nodetool status\n</pre></div></div>\n<p>Note: to use a Cassandra command directly, move into the directory for a node and execute the command as you would on any local installation. For example, the following commands would have the same effect as the <code class=\"highlighter-rouge\">ccm</code> wrapped version above:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>cd ~/.ccm/sslverify/node1\n./bin/nodetool status\n</pre></div></div>\n<p>With our cluster in place, it’s time to move on to certificate management.</p>\n<p>When dealing with even a small cluster, creating our own Certificate Authority (CA) becomes essential to minimizing trust chain complexity. This allows us to create a Root Certificate that can be used to sign all of our server-specific certificates. Once signed, this creates a trust chain that will make managing the certificates significantly easier. We’ll go into further detail on this below.</p>\n<p>We will be using OpenSSL for create the Certificate Authority (CA) and sign certificate requests with such. The <code class=\"highlighter-rouge\">openssl</code> tool should be available on most UNIX-derived systems. For this tutorial, I was using <code class=\"highlighter-rouge\">OpenSSL 1.0.1j 15 Oct 2014</code> on OS X <code class=\"highlighter-rouge\">10.10.5</code>.</p>\n<p>For the first step, create the CA using the following configuration file titled <code class=\"highlighter-rouge\">gen_ca_cert.conf</code> in the current directory:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>[ req ]\ndistinguished_name     = req_distinguished_name\nprompt                 = no\noutput_password        = mypass\ndefault_bits           = 2048\n[ req_distinguished_name ]\nC                      = US\nST                     = TX\nL                      = Austin\nO                      = TLP\nOU                     = TestCluster\nCN                     = TestClusterMasterCA\nemailAddress           = info@thelastpickle.com\n</pre></div></div>\n<p>Obviously, you’ll want to put your specific information in there. When I create a CA for Cassandra, I like to use the the name of the cluster for the Organizational Unit (OU) and specify that it is our cluster CA via the Common Name (CN) attribute. This is of course a matter of personal preference. The take away is to pick something that will make managing these easy.</p>\n<p>Now, run the following OpenSSL command to create the CA:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>openssl req -config gen_ca_cert.conf -new -x509 -keyout ca-key -out ca-cert -days 365\n</pre></div></div>\n<p>There is a lot going on here, so let’s break that one down:</p>\n<ul><li><code class=\"highlighter-rouge\">req</code>: An OpenSSL sub-command saying that we are (in this case) creating a <a href=\"https://www.openssl.org/docs/manmaster/apps/req.html\">PKCS#10 X.509 Certificate</a>. Note: the following parameters are all options of the <code class=\"highlighter-rouge\">req</code> command</li>\n  <li><code class=\"highlighter-rouge\">-config</code>: The path to the config file above to avoid having to provide information on STDIN</li>\n  <li><code class=\"highlighter-rouge\">-new</code>: This is a new signing request we are making</li>\n  <li><code class=\"highlighter-rouge\">-x509</code>: The output will be an <a href=\"https://en.wikipedia.org/wiki/X.509\">X.509</a> compatible self-signed certificate we can use as a root CA</li>\n  <li><code class=\"highlighter-rouge\">-keyout</code>: The filename to which we will write our key</li>\n  <li><code class=\"highlighter-rouge\">-out</code>: The filename to which we will write our certificate</li>\n  <li><code class=\"highlighter-rouge\">-days</code>: The number of days for which the generated certificate will be valid</li>\n</ul><p>You can verify the contents of the certificate you just created with the following command:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>openssl x509 -in ca-cert -text -noout\n</pre></div></div>\n<h3 id=\"per-server-certificate-creation\">Per-Server Certificate Creation</h3>\n<p>Now we will create a public/private key pair for each server using the built-in <a href=\"https://docs.oracle.com/javase/8/docs/technotes/tools/unix/keytool.html\">keytool utility</a> (<em>note</em>: I used JDK 8 for this tutorial, in which <code class=\"highlighter-rouge\">keytool</code> has had a bit of a revamp - see the previous link for details).</p>\n<p>In the process of doing this step, we are creating the node-specific key stores which will be distributed directly to those nodes in a later step. We’ve used the node names from the <code class=\"highlighter-rouge\">ccm</code> cluster we created previously as part of our naming scheme:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>keytool -genkeypair -keyalg RSA -alias node1 -keystore node1-server-keystore.jks -storepass awesomekeypass -keypass awesomekeypass -validity 365 -keysize 2048 -dname \"CN=node1, OU=SSL-verification-cluster, O=TheLastPickle, C=US\"\nkeytool -genkeypair -keyalg RSA -alias node2 -keystore node2-server-keystore.jks -storepass awesomekeypass -keypass awesomekeypass -validity 365 -keysize 2048 -dname \"CN=node2, OU=SSL-verification-cluster, O=TheLastPickle, C=US\"\nkeytool -genkeypair -keyalg RSA -alias node3 -keystore node3-server-keystore.jks -storepass awesomekeypass -keypass awesomekeypass -validity 365 -keysize 2048 -dname \"CN=node3, OU=SSL-verification-cluster, O=TheLastPickle, C=US\"\n</pre></div></div>\n<p>As with the OpenSSL incantation above, let’s summarize what we are doing for each node:</p>\n<ul><li><code class=\"highlighter-rouge\">-genkeypair</code>: the keytool command to generate a public/private key pair combination</li>\n  <li><code class=\"highlighter-rouge\">-keyalg</code>: The algorithm to use, RSA in this case</li>\n  <li><code class=\"highlighter-rouge\">-alias</code>: An alias to use for this public/private key pair, <code class=\"highlighter-rouge\">alias</code> is used to identify the public key when imported into a key store. I usually use some form of $hostname-cassandra</li>\n  <li><code class=\"highlighter-rouge\">-keystore</code>: The location of our key store (created if it does not already exist)</li>\n  <li><code class=\"highlighter-rouge\">-storepass</code>: The password for the key store</li>\n  <li><code class=\"highlighter-rouge\">-keypass</code>: The password for the key</li>\n  <li><code class=\"highlighter-rouge\">-validity</code>: The number of days for which this key pair will be valid</li>\n  <li><code class=\"highlighter-rouge\">-keysize</code>: The size of the key to generate</li>\n  <li><code class=\"highlighter-rouge\">-dname</code>: See below</li>\n</ul><p>The arguments to <code class=\"highlighter-rouge\">-dname</code> can be summarized as follows: the subject’s common name (CN), organizational unit (OU), organization (O), and country (C). In this context, it’s a good idea to make the CN the hostname. For OU, I like to use the same string as the <code class=\"highlighter-rouge\">cluster_name</code> attribute in <code class=\"highlighter-rouge\">cassandra.yaml</code> as a personal preference. The other attributes are straight forward, but can be whatever given this is all self-signed. Just be consistent with them.</p>\n<p>As with the CA we created earlier on, we should verify that the key store is accessible and contains the key pair with the correct information:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>keytool -list -v -keystore node1-server-keystore.jks -storepass awesomekeypass\n</pre></div></div>\n<h3 id=\"some-things-to-note\">Some Things to Note</h3>\n<p>The <code class=\"highlighter-rouge\">-genkeypair</code> sub-command can also take a <code class=\"highlighter-rouge\">-startdate</code> option which is handy when you know you are making a configuration change at some point in the future. If used, the <code class=\"highlighter-rouge\">-validity</code> is then calculated as being from that point in time onwards.</p>\n<p>There is currently a limitation in Cassandra which forces us to use the same password for the key store as for the key. In all fairness, <code class=\"highlighter-rouge\">javax.security</code> is a very obtuse API with which to work. Specifically, loading individual certificates with different passwords from a key store is shockingly cumbersome, particularly if one or more of those entries is based on a trust chain.</p>\n<p>Using the <code class=\"highlighter-rouge\">CN</code> attribute for the hostname is considered deprecated in the context of <a href=\"https://en.wikipedia.org/wiki/Public_key_infrastructure\">PKI</a>. You can enable DNS hostname verification (referred to as “Subject Alternative Name” in PKI parlance), but since we building our own CA for private consumption, I consider it overkill for this case. Nevertheless, if you want to do this, simply append <code class=\"highlighter-rouge\">-ext SAN=DNS:thelastpickle.com</code> to the <code class=\"highlighter-rouge\">keytool</code> invocation above. You’ll have to do the same when generating the CA as well (see the <a href=\"http://thelastpickle.com/blog/2015/09/30/hardening-cassandra-step-by-step-part-1-server-to-server.html\">OpenSSL documentation</a> for details).</p>\n<h3 id=\"export-certificates-as-signing-requests\">Export Certificates as Signing Requests</h3>\n<p>With our key stores created and populated, we now need to export a certificates from each node’s key store as a “Signing Request” for our CA:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>keytool -keystore node1-server-keystore.jks -alias node1 -certreq -file node1_cert_sr -keypass awesomekeypass -storepass awesomekeypass\nkeytool -keystore node2-server-keystore.jks -alias node2 -certreq -file node2_cert_sr -keypass awesomekeypass -storepass awesomekeypass\nkeytool -keystore node3-server-keystore.jks -alias node3 -certreq -file node3_cert_sr -keypass awesomekeypass -storepass awesomekeypass\n</pre></div></div>\n<p>We’ve seen two new <code class=\"highlighter-rouge\">keytool</code> options here which we’ll briefly describe:</p>\n<ul><li><code class=\"highlighter-rouge\">-certreq</code> another <code class=\"highlighter-rouge\">keytool</code> sub-command, this says to export the certificate specifically for signing by a CA</li>\n  <li><code class=\"highlighter-rouge\">-file</code> specifies the file to which the signing request will be written</li>\n</ul><h3 id=\"sign-each-certificate-with-the-cas-public-key\">Sign Each Certificate with the CA’s Public Key</h3>\n<p>With the certificate signing requests ready to go, it’s now time to sign each with our CA’s public key via OpenSSL:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>openssl x509 -req -CA ca-cert -CAkey ca-key -in node1_cert_sr -out node1_cert_signed -days 365 -CAcreateserial -passin pass:mypass\nopenssl x509 -req -CA ca-cert -CAkey ca-key -in node2_cert_sr -out node2_cert_signed -days 365 -CAcreateserial -passin pass:mypass\nopenssl x509 -req -CA ca-cert -CAkey ca-key -in node3_cert_sr -out node3_cert_signed -days 365 -CAcreateserial -passin pass:mypass\n</pre></div></div>\n<p>This OpenSSL incantation is quite a bit different than our the one for creating a CA above, so we’ll again summarize:</p>\n<ul><li><code class=\"highlighter-rouge\">x509</code> Use the display and <a href=\"https://www.openssl.org/docs/manmaster/apps/x509.html\">signing subcommand</a></li>\n  <li><code class=\"highlighter-rouge\">-req</code> We are signing a certificate request as opposed to a certificate</li>\n  <li><code class=\"highlighter-rouge\">-CA</code> The certificate file we specified via the <code class=\"highlighter-rouge\">-out</code> parameter used when creating our CA</li>\n  <li><code class=\"highlighter-rouge\">-CAKey</code> The key file we specified via the <code class=\"highlighter-rouge\">-keyout</code> parameter used when creating our CA</li>\n  <li><code class=\"highlighter-rouge\">-in</code> The per-node certificate request we are signing; this was the <code class=\"highlighter-rouge\">-file</code> parameter from the Signing Request step above</li>\n  <li><code class=\"highlighter-rouge\">-out</code> The newly-signed certificate file to create (use a clear naming scheme to keep track of files)</li>\n  <li><code class=\"highlighter-rouge\">-days</code> The number of days for which the signed certificate will be valid</li>\n  <li><code class=\"highlighter-rouge\">-CAcreateserial</code> Create a serialnumber for this CSR (see the doc <code class=\"highlighter-rouge\">openssl x509</code> documentation above, it’s complicated)</li>\n  <li><code class=\"highlighter-rouge\">-passin</code> The keypassword source. The arguments to <code class=\"highlighter-rouge\">passin</code> have their own <a href=\"https://www.openssl.org/docs/manmaster/apps/openssl.html\">formatting instructions</a> with which you should become familiar</li>\n</ul><h3 id=\"add-the-ca-to-the-key-store\">Add the CA to the Key Store</h3>\n<p>With the certificates now signed, we will need to re-import them back into each node’s key store via the <code class=\"highlighter-rouge\">-import</code> sub-command of <code class=\"highlighter-rouge\">keytool</code>. However, before we can do that, we have to add the certificate from our CA to each key store. This step is required for the trust chain to function correctly.</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>keytool -keystore node1-server-keystore.jks -alias CARoot -import -file ca-cert -noprompt -keypass awesomekeypass -storepass awesomekeypass\nkeytool -keystore node2-server-keystore.jks -alias CARoot -import -file ca-cert -noprompt -keypass awesomekeypass -storepass awesomekeypass\nkeytool -keystore node3-server-keystore.jks -alias CARoot -import -file ca-cert -noprompt -keypass awesomekeypass -storepass awesomekeypass\n</pre></div></div>\n<p>With each key store now containing our CA, we can import the signed certificate with the same alias back into the key store:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>keytool -keystore node1-server-keystore.jks -alias node1 -import -file node1_cert_signed -keypass awesomekeypass -storepass awesomekeypass\nkeytool -keystore node2-server-keystore.jks -alias node2 -import -file node2_cert_signed -keypass awesomekeypass -storepass awesomekeypass\nkeytool -keystore node3-server-keystore.jks -alias node3 -import -file node3_cert_signed -keypass awesomekeypass -storepass awesomekeypass\n</pre></div></div>\n<h3 id=\"building-the-trust-store\">Building the Trust Store</h3>\n<p>Our key store is now all set. But since we are using Client Certificate Authentication, we need to add a trust store to each node. This is how each node will verify incoming connections from the rest of the cluster.</p>\n<p>All we need to do is create trust store by importing CA root certificate’s public key:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>keytool -keystore generic-server-truststore.jks -alias CARoot -importcert -file ca-cert -keypass mypass -storepass truststorepass -noprompt\n</pre></div></div>\n<p>Now this is where it all comes together. Since all of our instance-specific keys have now been signed by the CA, we can share this trust store instance across the cluster as it effectively just says “I’m going to trust all connections whose client certificates were signed by this CA.” This is the same way that an authority like Verisign works when you get a commercial certificate for your web server. We are in this case just acting as our own authority (which is the safest approach when creating public key infrastructure for your internal services).</p>\n<h3 id=\"configuring-the-cluster\">Configuring the Cluster</h3>\n<p>So now that we have all of our files created, let’s place them where they go so CCM can find them. First, we’ll move the key stores:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>cp node1-server-keystore.jks ~/.ccm/sslverify/node1/conf/server-keystore.jks\ncp node2-server-keystore.jks ~/.ccm/sslverify/node2/conf/server-keystore.jks\ncp node3-server-keystore.jks ~/.ccm/sslverify/node3/conf/server-keystore.jks\n</pre></div></div>\n<p>Note: we make the target name here generic for the sake of convention as this is what you would do with a CM system anyway.</p>\n<p>And the same with generic trust store:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>cp generic-server-truststore.jks ~/.ccm/sslverify/node1/conf/server-truststore.jks\ncp generic-server-truststore.jks ~/.ccm/sslverify/node2/conf/server-truststore.jks\ncp generic-server-truststore.jks ~/.ccm/sslverify/node3/conf/server-truststore.jks\n</pre></div></div>\n<h3 id=\"add-encryption-options-to-the-configuration\">Add Encryption Options to the Configuration:</h3>\n<p>With the files in place, let’s modify the configuration to enable server to server encryption. Replace the <code class=\"highlighter-rouge\">server_encryption_options</code> in each server’s <code class=\"highlighter-rouge\">cassandra.yaml</code> located in <code class=\"highlighter-rouge\">~/.ccm/sslverify/[server]/conf/</code> with the following (adjusting the path to fit your environment and the $NODE variable below for each of our three nodes):</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>server_encryption_options:\n  internode_encryption: all\n  keystore: /Users/zznate/.ccm/sslverify/$NODE/conf/server-keystore.jks\n  keystore_password: awesomekeypass\n  truststore: /Users/zznate/.ccm/sslverify/$NODE/conf/server-truststore.jks\n  truststore_password: truststorepass\n  protocol: TLS\n  algorithm: SunX509\n  store_type: JKS\n  cipher_suites: [TLS_RSA_WITH_AES_256_CBC_SHA]\n  require_client_auth: true\n</pre></div></div>\n<p>In the above snippet, we have specified <a href=\"https://en.wikipedia.org/wiki/Advanced_Encryption_Standard\">256bit AES</a>. This is a fairly strong cypher and is therefore subject to US export control (I’m not providing a link here because this whole idea of cryptographic export controls is outmoded at best - look it up if you want &lt;/rant&gt;). To use this, we need to install the strong encryption policy files into our JDK. Follow the download titled “Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files for JDK/JRE 8” from Oracle’s <a href=\"http://www.oracle.com/technetwork/java/javase/downloads/index.html\">Java download page</a>.</p>\n<p>Those files usually get installed into <code class=\"highlighter-rouge\">${java.home}/jre/lib/security/</code>. The internet has copious amounts of information for specific systems if that is not were your installation has placed them.</p>\n<p>Some additional info on why you need to do this can be found <a href=\"http://docs.oracle.com/javase/8/docs/technotes/guides/security/SunProviders.html#importlimits\">here</a></p>\n<p>Alternatively, you can skip the policy file installation by choosing a weaker strength cipher:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA]\n</pre></div></div>\n<p>Depending on your requirements, your network segmentation, or any industry guidelines to which you may be beholden, using 128-bit keys might be fine. If you are going over any sort of public connection, you should do 256-bit keys.</p>\n<p>With the policy jars in place (or with the 128-bit AES cipher specified), let’s restart <code class=\"highlighter-rouge\">node1</code> via ccm:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>ccm node1 stop\nccm node1 start\n</pre></div></div>\n<p>If everything is working correctly, you should see log output (available in <code class=\"highlighter-rouge\">~/.ccm/sslverify/node1/logs/system.log</code>) containing:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>Starting Encrypted Messaging Service on SSL port 7001\n</pre></div></div>\n<p>The output of <code class=\"highlighter-rouge\">ccm node1 nodetool status</code> should look like:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>Datacenter: datacenter1\n=======================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address    Load       Tokens  Owns   Host ID                               Rack\nUN  127.0.0.1  15.78 MB   1       33.3%  7ac70e8c-507b-4dfc-bed8-ddbd736ae9dc  rack1\nDN  127.0.0.2  ?          1       33.3%  e12e1ac3-12b8-48e8-8f5e-897573026896  rack1\nDN  127.0.0.3  ?          1       33.3%  0e867789-ec7d-4704-b0ed-e0437a28f83d  rack1\n</pre></div></div>\n<p>What we are seeing here is that <code class=\"highlighter-rouge\">node1</code> has toggled over to using SSL and can no longer communicate with the other two cluster members. It therefore assumes they are down. Executing <code class=\"highlighter-rouge\">nodetool status</code> against one of the other nodes will indeed show the opposite picture.</p>\n<p>Let’s move on with the rest of the cluster. Repeating the process above with <code class=\"highlighter-rouge\">node2</code>, the output from <code class=\"highlighter-rouge\">ccm node1 nodetool status</code> should now show:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>Datacenter: datacenter1\n=======================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address    Load       Tokens  Owns   Host ID                               Rack\nUN  127.0.0.1  15.79 MB   1       33.3%  7ac70e8c-507b-4dfc-bed8-ddbd736ae9dc  rack1\nUN  127.0.0.2  37.87 MB   1       33.3%  e12e1ac3-12b8-48e8-8f5e-897573026896  rack1\nDN  127.0.0.3  ?          1       33.3%  0e867789-ec7d-4704-b0ed-e0437a28f83d  rack1\n</pre></div></div>\n<p>And the same again with <code class=\"highlighter-rouge\">node3</code> should show all nodes as up.</p>\n<h3 id=\"debugging\">Debugging</h3>\n<p>There are a few things that can go wrong when setting up SSL. If you have exceptions, or it’s just not working as intended (there are several cases when setting up SSL where Cassandra will happily start even though no connections could be negotiated) you can enable debug logging on the SSL handshake via adding the following option to <code class=\"highlighter-rouge\">~/.ccm/sslverify/$NODE/conf/cassandra-env.sh</code>:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>-Djavax.net.debug=ssl\n</pre></div></div>\n<p>This will print <strong>everything</strong> about the connection setup to STDOUT including how and why an SSL handshake failed. See <a href=\"http://docs.oracle.com/javase/7/docs/technotes/guides/security/jsse/ReadDebug.html\">this page</a> for details on how to interpret what you are seeing.</p>\n<h2 id=\"one-last-thing\">One Last Thing</h2>\n<p>Whatever you have used for various expiration date parameters throughout this tutorial, make sure they sync it up with each other sensibly. Most importantly, put those dates on a calendar <strong>now</strong> with a healthy number of reminders leading up to expiration, inviting everyone even remotely involved with your team. I once saw an expired certificates cause havoc across an otherwise robust architecture because the expiration dates were set and promptly forgotten about.</p>\n<p>And just as with any failure scenario, make sure you test your certificate updating process. This is not something with which you want to experiment at the 11th hour.</p>\n<h2 id=\"up-next\">Up Next</h2>\n<p>We’ll continue this series soon with a post on options available for setting up Encryption at Rest. It will be higher-level than this post, as we will focus more on the options available for and known to work with Cassandra. Keep checking back for updates!</p>\n<p>As always, please let us know if there are any errors factual or command-wise in the steps above.</p>",
        "created_at": "2018-10-18T16:12:34+0000",
        "updated_at": "2018-10-18T16:12:38+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 18,
        "domain_name": "thelastpickle.com",
        "preview_picture": "http://thelastpickle.com/android-chrome-192x192.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12402"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 190,
            "label": "sql",
            "slug": "sql"
          }
        ],
        "is_public": false,
        "id": 12391,
        "uid": null,
        "title": "Spark SQL cassandra delete records",
        "url": "https://stackoverflow.com/questions/36905475/spark-sql-cassandra-delete-records",
        "content": "<div class=\"vote\"><h3>Vote count:&#13;\n        &#13;\n        0&#13;\n        &#13;\n&#13;\n        &#13;\n        &#13;\n&#13;\n&#13;\n&#13;\n</h3></div><div class=\"post-text\" itemprop=\"text\">&#13;\n&#13;\n<p>Is there a way to delete some records based on a select query?</p>\n<p>I have this query,</p>\n<p><code>Select min(id) from ID having count(*)&gt;1</code> which will show the duplicates. I need to get those ids and delete them. How can I do it in spark sql?</p></div><div class=\"user-action-time\">&#13;\n        asked Apr 28 '16 at 4:48&#13;</div><div class=\"user-details\">&#13;\n        <a href=\"https://stackoverflow.com/users/1732516/ashk\">ashK</a>&#13;\n        <div class=\"-flair\">&#13;\n            1881214&#13;\n        </div>&#13;</div><h2 data-answercount=\"1\">&#13;\n                                1 Answer&#13;\n                                1&#13;</h2><div class=\"vote\"><h3>Vote count:&#13;\n        &#13;\n        0&#13;\n        &#13;\n&#13;\n&#13;\n&#13;\n&#13;\n&#13;\n</h3></div><div class=\"post-text\" itemprop=\"text\">&#13;\n<p>Spark SQL does not support DELETE.</p>\n<p>If the number of ids to delete is small, you can do it using the Cassandra driver instead of through Spark:</p>\n<pre>import scala.collection.JavaConverters._\nimport scala.collection.JavaConversions._\nimport com.datastax.driver.core.{Cluster, Session, BatchStatement}\nimport com.datastax.driver.core.querybuilder.QueryBuilder\nval cluster = Cluster.builder().addContactPoint(host_ip).build()\nval session = cluster.connect(keyspace)\nval idsToDelete = ... // perform your query and collect the ids\nval queries = idsToDelete.map({ id =&gt; QueryBuilder.delete().from(keyspace, table).where(QueryBuilder.eq(\"id\", id)) })\nval batch = batchStatement().addAll(queries.asJava)\nsession.execute(batch)\ncluster.close\n</pre></div><div class=\"user-action-time\">&#13;\n        <a href=\"https://stackoverflow.com/posts/40288410/revisions\" title=\"show all edits to this post\">edited Oct 27 '16 at 16:34</a>&#13;</div><div class=\"user-details\">&#13;\n        &#13;\n        &#13;</div><div class=\"user-action-time\">&#13;\n        answered Oct 27 '16 at 15:18&#13;</div><div class=\"user-details\">&#13;\n        <a href=\"https://stackoverflow.com/users/2320087/didier\">Didier</a>&#13;\n        &#13;</div>",
        "created_at": "2018-10-15T16:48:40+0000",
        "updated_at": "2018-10-15T16:49:10+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 1,
        "domain_name": "stackoverflow.com",
        "preview_picture": "https://cdn.sstatic.net/Sites/stackoverflow/img/apple-touch-icon@2.png?v=73d79a89bded",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12391"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 903,
            "label": "orchestration",
            "slug": "orchestration"
          },
          {
            "id": 985,
            "label": "cluster",
            "slug": "cluster"
          }
        ],
        "is_public": false,
        "id": 12375,
        "uid": null,
        "title": "thelastpickle/tlp-cluster",
        "url": "https://github.com/thelastpickle/tlp-cluster",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>This is a tool to aid in Cassandra development.</p>\n<p>This tool is a work in progress and is intended for developers to use to quickly launch clusters based on arbitrary builds.</p>\n<p>If you aren't comfortable digging into code, this tool probably isn't for you, as you're very likely going to need to do some customizations.</p>\n<p>Please refer to the project <a href=\"http://thelastpickle.com/tlp-cluster/\" rel=\"nofollow\">documentation</a> for usage instructions.</p>\n</article>",
        "created_at": "2018-10-12T20:05:01+0000",
        "updated_at": "2018-10-12T20:05:08+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/30403496?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12375"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12364,
        "uid": null,
        "title": "About Data Partitioning in Cassandra — Apache Cassandra 1.1.x documentation",
        "url": "https://docs.datastax.com/en/archived/cassandra/1.1/docs/cluster_architecture/partitioning.html",
        "content": "<p>A partitioner determines how data is distributed across the nodes in the cluster (including replicas). Basically, a partitioner is a hash function for computing the token (it's hash) of a row key. Each row of data is uniquely identified by a row key and distributed across the cluster by the value of the token.</p><div class=\"section\" id=\"data-distribution-in-the-ring\"><h2>Data Distribution in the Ring</h2><p>In Cassandra, the total amount of data managed by the cluster is represented as a ring. The ring is divided into ranges equal to the number of nodes, with each node being responsible for one or more ranges of the data. Before a node can join the ring, it must be assigned a token. The token value determines the node's position in the ring and its range of data. Column family data is partitioned across the nodes based on the row key. To determine the node where the first replica of a row will live, the ring is walked clockwise until it locates the node with a token value greater than that of the row key. Each node is responsible for the region of the ring between itself (inclusive) and its predecessor (exclusive). With the nodes sorted in token order, the last node is considered the predecessor of the first node; hence the ring representation.</p><p>For example, consider a simple four node cluster where all of the row keys managed by the cluster were numbers in the range of 0 to 100. Each node is assigned a token that represents a point in this range. In this simple example, the token values are 0, 25, 50, and 75. The first node, the one with token 0, is responsible for the <em>wrapping range</em> (75-0). The node with the lowest token also accepts row keys less than the lowest token and more than the highest token.</p><div class=\"figure\"><a class=\"reference internal image-reference\" href=\"https://docs.datastax.com/en/archived/cassandra/1.1/_images/ring_partitions.png\"><img alt=\"../../_images/ring_partitions.png\" src=\"https://docs.datastax.com/en/archived/cassandra/1.1/_images/ring_partitions.png\" /></a></div></div><div class=\"section\" id=\"understanding-the-partitioner-types\"><h2>Understanding the Partitioner Types</h2><p>When you deploy a Cassandra cluster, you must assign a partitioner and assign each node an <a class=\"reference internal\" href=\"https://docs.datastax.com/en/archived/cassandra/1.1/docs/configuration/node_configuration.html#initial-token\"><em>initial_token</em></a> value so each node is responsible for roughly an equal amount of data (load balancing). DataStax strongly recommends using the RandomPartitioner (default) for all cluster deployments.</p><p>To calculate the tokens for nodes in a single data center cluster, you divide the range by the total number of nodes in the cluster. In multiple data center deployments, you calculate the tokens such that each data center is individually load balanced. See <a class=\"reference internal\" href=\"https://docs.datastax.com/en/archived/cassandra/1.1/docs/initialize/token_generation.html#token-gen-cassandra\"><em>Generating Tokens</em></a> for the different approaches to generating tokens for nodes in single and multiple data center clusters.</p><p>Unlike almost every other configuration choice in Cassandra, the partitioner may not be changed without reloading all of your data. Therefore, it is important to choose and configure the correct partitioner before initializing your cluster. You set the partitioner in the <a class=\"reference internal\" href=\"https://docs.datastax.com/en/archived/cassandra/1.1/docs/configuration/node_configuration.html#cassandra-yaml\"><em>cassandra.yaml</em></a> file.</p><p>Cassandra offers the following partitioners:</p><ul class=\"simple\"><li><a class=\"reference internal\" href=\"#randompartitioner\"><em>RandomPartitioner</em></a></li>\n<li><a class=\"reference internal\" href=\"#byteorderedpartitioner\"><em>ByteOrderedPartitioner</em></a></li>\n</ul></div>",
        "created_at": "2018-10-10T17:42:28+0000",
        "updated_at": "2018-10-10T17:42:34+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 2,
        "domain_name": "docs.datastax.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12364"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          }
        ],
        "is_public": false,
        "id": 12357,
        "uid": null,
        "title": "How to log in Apache Spark, a functional approach",
        "url": "https://hackernoon.com/how-to-log-in-apache-spark-a-functional-approach-e48ffbbd935b?gi=17d82cca4f1a",
        "content": "<p id=\"305f\" class=\"graf graf--p graf-after--h3\">Logging in Apache Spark comes very easy since Spark offers access to a <em class=\"markup--em markup--p-em\">log </em>object out of the box. Only some configuration setups need to be done. In a <a href=\"https://medium.com/@anicolaspp/how-to-log-in-apache-spark-f4204fad78a#.7asl0yplw\" data-href=\"https://medium.com/@anicolaspp/how-to-log-in-apache-spark-f4204fad78a#.7asl0yplw\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\"><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">previous post</em></strong></a><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\"> </em></strong>we have looked at how to do this while showing some problems that may arise. However, the solution presented might cause some problems at the moment we want to collect the logs since they are distributed across the entire cluster. Even if we utilize <strong class=\"markup--strong markup--p-strong\">Yarn</strong> log aggregation capabilities, there will be some contentions that might affect performance or even worse, in some cases we could end with log interleaves corrupting the nature of logs itself, they time ordered properties they should present.</p><p id=\"2119\" class=\"graf graf--p graf-after--p\">In order to solve these problems, a different approach needs to be taken, a functional one.</p><h4 id=\"6b67\" class=\"graf graf--h4 graf-after--p\">The Monad Writer</h4><p id=\"a193\" class=\"graf graf--p graf-after--h4\">I do not intend to go over the details about monads or in this particular case, the Monad Writer, if you are interested in learning more, take a look at this link (<a href=\"http://adit.io/posts/2013-04-17-functors,_applicatives,_and_monads_in_pictures.html\" data-href=\"http://adit.io/posts/2013-04-17-functors,_applicatives,_and_monads_in_pictures.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\"><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">functor, applicative, and monad</em></strong></a>) which is very informative about this topic.</p><p id=\"294b\" class=\"graf graf--p graf-after--p\">Just to put things in context, let’s say that the monad writer (<strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">writer) </em></strong>is a container that holds the current value of a computation in addition to history (log) of the value (set of transformation on the value).</p><p id=\"d207\" class=\"graf graf--p graf-after--p\">Because the <strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">writer </em></strong>monadic properties, it allows us to do functional transformations and we will soon see how everything sticks together.</p><h4 id=\"c555\" class=\"graf graf--h4 graf-after--p\">A Simplistic Log</h4><p id=\"a2c4\" class=\"graf graf--p graf-after--h4\">The following code demonstrates a simplistic log.</p><figure id=\"a739\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"7b79\" class=\"graf graf--p graf-after--figure\">The only thing to note is that logging is actually happening on the Spark driver, so we don’t have synchronization or contention problems. Everything starts to get complicated once we start distributing our computations.</p><p id=\"b811\" class=\"graf graf--p graf-after--p\">The following code won’t work (read <a href=\"https://medium.com/@anicolaspp/how-to-log-in-apache-spark-f4204fad78a#.7asl0yplw\" data-href=\"https://medium.com/@anicolaspp/how-to-log-in-apache-spark-f4204fad78a#.7asl0yplw\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\"><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">previous post</em></strong></a> to know why)</p><figure id=\"b690\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"dcc8\" class=\"graf graf--p graf-after--figure\">A solution to this was also presented in the <a href=\"https://medium.com/@anicolaspp/how-to-log-in-apache-spark-f4204fad78a#.7asl0yplw\" data-href=\"https://medium.com/@anicolaspp/how-to-log-in-apache-spark-f4204fad78a#.7asl0yplw\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\"><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">previous post</em></strong></a>, but it requires extra work to manage the logs.</p><p id=\"7b80\" class=\"graf graf--p graf-after--p\">Once we start logging on each node of the cluster, we need to go to each node, and collect each log file in order to make sense of whatever is in the logs. Hopefully, you are using some kind of tool to help you with this task, such as Splunk, Datalog, etc… yet you still need to know a lot of stuffs to get those logs into your system.</p><h4 id=\"b17f\" class=\"graf graf--h4 graf-after--p\">Our Data Set</h4><p id=\"b5b9\" class=\"graf graf--p graf-after--h4\">Our data set is a collection of the class Person that is going to be transformed while keeping an unified log of the operations on our data set.</p><p id=\"beb9\" class=\"graf graf--p graf-after--p\">Let’s suppose we want our data set to get loaded, then filter each people whose age is less than 20 years, and finally extract its name. It is a very silly example, but it will demonstrate how the logs are produced. You could replace these computations, but the ideas of building an unified log will remain.</p><h4 id=\"2b96\" class=\"graf graf--h4 graf-after--p\">Getting the Writer</h4><p id=\"4202\" class=\"graf graf--p graf-after--h4\">We are going to use <a href=\"http://typelevel.org/projects/\" data-href=\"http://typelevel.org/projects/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\"><em class=\"markup--em markup--p-em\">Typelevel / Cats</em></a><em class=\"markup--em markup--p-em\"> </em>library to import the monad writer, to do this we add the following line to our <strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">build.sbt</em></strong> file.</p><figure id=\"cb47\" class=\"graf graf--figure graf--iframe graf-after--p\"><h4 id=\"0424\" class=\"graf graf--h4 graf-after--figure\">Playing with our data</h4><p id=\"f54c\" class=\"graf graf--p graf-after--h4\">Now, let’s define the transformations we are going to use.</p><p id=\"472e\" class=\"graf graf--p graf-after--p\">First, let’s load the data.</p><figure id=\"d889\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"7ae9\" class=\"graf graf--p graf-after--figure\">In here the <strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">~&gt; </em></strong>operation is defined via implicit conversions as follows.</p><figure id=\"4697\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"ddf8\" class=\"graf graf--p graf-after--figure\">If you look closely, our loading operation is not returning an RDD, in fact, it returns the monad writer that keeps track of the logs.</p><p id=\"25dc\" class=\"graf graf--p graf-after--p\">Let’s define the filter that we want to apply over the collection of users.</p><figure id=\"6021\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"baac\" class=\"graf graf--p graf-after--figure\">Again, we are applying the same function (<strong class=\"markup--strong markup--p-strong\">~&gt;</strong>) to keep track of this transformation.</p><p id=\"b90b\" class=\"graf graf--p graf-after--p\">Lastly, we define the mapping which follows the same pattern we just saw.</p><figure id=\"cc92\" class=\"graf graf--figure graf--iframe graf-after--p\"><h4 id=\"a1d0\" class=\"graf graf--h4 graf-after--figure\">Putting it together</h4><p id=\"db37\" class=\"graf graf--p graf-after--h4\">So far we have only defined our transformations, but we need to stick them together. Scala <strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">for</em></strong> is a very convenient way to work with monadic structures. Let’s see how.</p><figure id=\"d74c\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"ab50\" class=\"graf graf--p graf-after--figure\">Please note that <strong class=\"markup--strong markup--p-strong\">result</strong> is of type: <strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">Writer[List[String], RDD[String]]</em></strong>.</p><p id=\"34a3\" class=\"graf graf--p graf-after--p\">Calling <strong class=\"markup--strong markup--p-strong\">result.run </strong>will give us the <strong class=\"markup--strong markup--p-strong\">log: List[String] </strong>and the final computation expressed by <strong class=\"markup--strong markup--p-strong\">rdd</strong>: <strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">RDD[String]</em></strong>.</p><p id=\"7ed8\" class=\"graf graf--p graf-after--p\">At this point we could use <em class=\"markup--em markup--p-em\">Spark logger </em>to write down the log generated by the chain of transformations. Note that this operation will be executed on Spark master which implies that one log file will be created with all the log information. Also, we are removing potential contention problems during the log writes. In addition, we are not locking the log file, which avoid performance issues by creating and writing to the file in a serial way.</p><h4 id=\"034d\" class=\"graf graf--h4 graf-after--p\">Conclusions</h4><p id=\"c5cb\" class=\"graf graf--p graf-after--h4\">We have improved how we log on Apache Spark by using the <em class=\"markup--em markup--p-em\">Monad Writer</em>. This functional approach allows us to distribute the creation of logs along with our computations, something Spark knows well how to do. However, instead of writing the logs on each worker node, we are collecting them back to the master to write them down. This mechanism has certain advantages over our previous implementation. We now control exactly how and when our logs are going to be written down, we boost performance by removing IO operations on the worker nodes, we also removed synchronization issues by writing the logs in a serial way, and we avoid the hazard of fishing logs across our entire cluster.</p><blockquote id=\"4cff\" class=\"graf graf--blockquote graf-after--p\"><a href=\"http://bit.ly/Hackernoon\" data-href=\"http://bit.ly/Hackernoon\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"noopener nofollow noopener\" target=\"_blank\">Hacker Noon</a><div> is how hackers start their afternoons. We’re a part of the </div><a href=\"http://bit.ly/atAMIatAMI\" data-href=\"http://bit.ly/atAMIatAMI\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener nofollow noopener\" target=\"_blank\">@AMI</a><div>family. We are now </div><a href=\"http://bit.ly/hackernoonsubmission\" data-href=\"http://bit.ly/hackernoonsubmission\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener nofollow noopener\" target=\"_blank\">accepting submissions</a><div> and happy to </div><a href=\"mailto:partners@amipublications.com\" data-href=\"mailto:partners@amipublications.com\" class=\"markup--anchor markup--blockquote-anchor\" target=\"_blank\">discuss advertising &amp;sponsorship</a><div> opportunities.</div></blockquote><blockquote id=\"dca4\" class=\"graf graf--blockquote graf-after--blockquote\"><div>To learn more, <a href=\"https://goo.gl/4ofytp\" data-href=\"https://goo.gl/4ofytp\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">read our about page</a>, <a href=\"http://bit.ly/HackernoonFB\" data-href=\"http://bit.ly/HackernoonFB\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">like/message us on Facebook</a>, or simply, <a href=\"https://goo.gl/k7XYbx\" data-href=\"https://goo.gl/k7XYbx\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">tweet/DM @HackerNoon.</a></div></blockquote><blockquote id=\"708a\" class=\"graf graf--blockquote graf-after--blockquote\"><div>If you enjoyed this story, we recommend reading our <a href=\"http://bit.ly/hackernoonlatestt\" data-href=\"http://bit.ly/hackernoonlatestt\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener nofollow noopener\" target=\"_blank\">latest tech stories</a> and <a href=\"https://hackernoon.com/trending\" data-href=\"https://hackernoon.com/trending\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener nofollow\" target=\"_blank\">trending tech stories</a>. Until next time, don’t take the realities of the world for granted!</div></blockquote><figure id=\"0f71\" class=\"graf graf--figure graf-after--blockquote graf--trailing\"><div class=\"aspectRatioPlaceholder is-locked\"><a href=\"https://bit.ly/2O1yNyY\" data-href=\"https://bit.ly/2O1yNyY\" class=\"graf-imageAnchor\" data-action=\"image-link\" data-action-observe-only=\"true\"><img class=\"graf-image\" data-image-id=\"1*QCV7h713dLgy5COZTyBLdQ@2x.png\" data-width=\"1400\" data-height=\"701\" src=\"https://cdn-images-1.medium.com/max/1600/1*QCV7h713dLgy5COZTyBLdQ@2x.png\" alt=\"image\" /></a></div></figure></figure></figure></figure></figure></figure></figure></figure></figure>",
        "created_at": "2018-10-10T16:15:44+0000",
        "updated_at": "2018-10-10T16:15:45+0000",
        "published_at": "2016-07-23T04:24:01+0000",
        "published_by": [
          "Nicolas A Perez"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 4,
        "domain_name": "hackernoon.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12357"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          }
        ],
        "is_public": false,
        "id": 12356,
        "uid": null,
        "title": "How to log in Apache Spark, a functional approach",
        "url": "https://hackernoon.com/how-to-log-in-apache-spark-a-functional-approach-e48ffbbd935b?gi=4cfa794d8878",
        "content": "<p id=\"305f\" class=\"graf graf--p graf-after--h3\">Logging in Apache Spark comes very easy since Spark offers access to a <em class=\"markup--em markup--p-em\">log </em>object out of the box. Only some configuration setups need to be done. In a <a href=\"https://medium.com/@anicolaspp/how-to-log-in-apache-spark-f4204fad78a#.7asl0yplw\" data-href=\"https://medium.com/@anicolaspp/how-to-log-in-apache-spark-f4204fad78a#.7asl0yplw\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\"><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">previous post</em></strong></a><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\"> </em></strong>we have looked at how to do this while showing some problems that may arise. However, the solution presented might cause some problems at the moment we want to collect the logs since they are distributed across the entire cluster. Even if we utilize <strong class=\"markup--strong markup--p-strong\">Yarn</strong> log aggregation capabilities, there will be some contentions that might affect performance or even worse, in some cases we could end with log interleaves corrupting the nature of logs itself, they time ordered properties they should present.</p><p id=\"2119\" class=\"graf graf--p graf-after--p\">In order to solve these problems, a different approach needs to be taken, a functional one.</p><h4 id=\"6b67\" class=\"graf graf--h4 graf-after--p\">The Monad Writer</h4><p id=\"a193\" class=\"graf graf--p graf-after--h4\">I do not intend to go over the details about monads or in this particular case, the Monad Writer, if you are interested in learning more, take a look at this link (<a href=\"http://adit.io/posts/2013-04-17-functors,_applicatives,_and_monads_in_pictures.html\" data-href=\"http://adit.io/posts/2013-04-17-functors,_applicatives,_and_monads_in_pictures.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\"><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">functor, applicative, and monad</em></strong></a>) which is very informative about this topic.</p><p id=\"294b\" class=\"graf graf--p graf-after--p\">Just to put things in context, let’s say that the monad writer (<strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">writer) </em></strong>is a container that holds the current value of a computation in addition to history (log) of the value (set of transformation on the value).</p><p id=\"d207\" class=\"graf graf--p graf-after--p\">Because the <strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">writer </em></strong>monadic properties, it allows us to do functional transformations and we will soon see how everything sticks together.</p><h4 id=\"c555\" class=\"graf graf--h4 graf-after--p\">A Simplistic Log</h4><p id=\"a2c4\" class=\"graf graf--p graf-after--h4\">The following code demonstrates a simplistic log.</p><figure id=\"a739\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"7b79\" class=\"graf graf--p graf-after--figure\">The only thing to note is that logging is actually happening on the Spark driver, so we don’t have synchronization or contention problems. Everything starts to get complicated once we start distributing our computations.</p><p id=\"b811\" class=\"graf graf--p graf-after--p\">The following code won’t work (read <a href=\"https://medium.com/@anicolaspp/how-to-log-in-apache-spark-f4204fad78a#.7asl0yplw\" data-href=\"https://medium.com/@anicolaspp/how-to-log-in-apache-spark-f4204fad78a#.7asl0yplw\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\"><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">previous post</em></strong></a> to know why)</p><figure id=\"b690\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"dcc8\" class=\"graf graf--p graf-after--figure\">A solution to this was also presented in the <a href=\"https://medium.com/@anicolaspp/how-to-log-in-apache-spark-f4204fad78a#.7asl0yplw\" data-href=\"https://medium.com/@anicolaspp/how-to-log-in-apache-spark-f4204fad78a#.7asl0yplw\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\"><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">previous post</em></strong></a>, but it requires extra work to manage the logs.</p><p id=\"7b80\" class=\"graf graf--p graf-after--p\">Once we start logging on each node of the cluster, we need to go to each node, and collect each log file in order to make sense of whatever is in the logs. Hopefully, you are using some kind of tool to help you with this task, such as Splunk, Datalog, etc… yet you still need to know a lot of stuffs to get those logs into your system.</p><h4 id=\"b17f\" class=\"graf graf--h4 graf-after--p\">Our Data Set</h4><p id=\"b5b9\" class=\"graf graf--p graf-after--h4\">Our data set is a collection of the class Person that is going to be transformed while keeping an unified log of the operations on our data set.</p><p id=\"beb9\" class=\"graf graf--p graf-after--p\">Let’s suppose we want our data set to get loaded, then filter each people whose age is less than 20 years, and finally extract its name. It is a very silly example, but it will demonstrate how the logs are produced. You could replace these computations, but the ideas of building an unified log will remain.</p><h4 id=\"2b96\" class=\"graf graf--h4 graf-after--p\">Getting the Writer</h4><p id=\"4202\" class=\"graf graf--p graf-after--h4\">We are going to use <a href=\"http://typelevel.org/projects/\" data-href=\"http://typelevel.org/projects/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\"><em class=\"markup--em markup--p-em\">Typelevel / Cats</em></a><em class=\"markup--em markup--p-em\"> </em>library to import the monad writer, to do this we add the following line to our <strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">build.sbt</em></strong> file.</p><figure id=\"cb47\" class=\"graf graf--figure graf--iframe graf-after--p\"><h4 id=\"0424\" class=\"graf graf--h4 graf-after--figure\">Playing with our data</h4><p id=\"f54c\" class=\"graf graf--p graf-after--h4\">Now, let’s define the transformations we are going to use.</p><p id=\"472e\" class=\"graf graf--p graf-after--p\">First, let’s load the data.</p><figure id=\"d889\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"7ae9\" class=\"graf graf--p graf-after--figure\">In here the <strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">~&gt; </em></strong>operation is defined via implicit conversions as follows.</p><figure id=\"4697\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"ddf8\" class=\"graf graf--p graf-after--figure\">If you look closely, our loading operation is not returning an RDD, in fact, it returns the monad writer that keeps track of the logs.</p><p id=\"25dc\" class=\"graf graf--p graf-after--p\">Let’s define the filter that we want to apply over the collection of users.</p><figure id=\"6021\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"baac\" class=\"graf graf--p graf-after--figure\">Again, we are applying the same function (<strong class=\"markup--strong markup--p-strong\">~&gt;</strong>) to keep track of this transformation.</p><p id=\"b90b\" class=\"graf graf--p graf-after--p\">Lastly, we define the mapping which follows the same pattern we just saw.</p><figure id=\"cc92\" class=\"graf graf--figure graf--iframe graf-after--p\"><h4 id=\"a1d0\" class=\"graf graf--h4 graf-after--figure\">Putting it together</h4><p id=\"db37\" class=\"graf graf--p graf-after--h4\">So far we have only defined our transformations, but we need to stick them together. Scala <strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">for</em></strong> is a very convenient way to work with monadic structures. Let’s see how.</p><figure id=\"d74c\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"ab50\" class=\"graf graf--p graf-after--figure\">Please note that <strong class=\"markup--strong markup--p-strong\">result</strong> is of type: <strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">Writer[List[String], RDD[String]]</em></strong>.</p><p id=\"34a3\" class=\"graf graf--p graf-after--p\">Calling <strong class=\"markup--strong markup--p-strong\">result.run </strong>will give us the <strong class=\"markup--strong markup--p-strong\">log: List[String] </strong>and the final computation expressed by <strong class=\"markup--strong markup--p-strong\">rdd</strong>: <strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">RDD[String]</em></strong>.</p><p id=\"7ed8\" class=\"graf graf--p graf-after--p\">At this point we could use <em class=\"markup--em markup--p-em\">Spark logger </em>to write down the log generated by the chain of transformations. Note that this operation will be executed on Spark master which implies that one log file will be created with all the log information. Also, we are removing potential contention problems during the log writes. In addition, we are not locking the log file, which avoid performance issues by creating and writing to the file in a serial way.</p><h4 id=\"034d\" class=\"graf graf--h4 graf-after--p\">Conclusions</h4><p id=\"c5cb\" class=\"graf graf--p graf-after--h4\">We have improved how we log on Apache Spark by using the <em class=\"markup--em markup--p-em\">Monad Writer</em>. This functional approach allows us to distribute the creation of logs along with our computations, something Spark knows well how to do. However, instead of writing the logs on each worker node, we are collecting them back to the master to write them down. This mechanism has certain advantages over our previous implementation. We now control exactly how and when our logs are going to be written down, we boost performance by removing IO operations on the worker nodes, we also removed synchronization issues by writing the logs in a serial way, and we avoid the hazard of fishing logs across our entire cluster.</p><blockquote id=\"4cff\" class=\"graf graf--blockquote graf-after--p\"><a href=\"http://bit.ly/Hackernoon\" data-href=\"http://bit.ly/Hackernoon\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"noopener nofollow noopener\" target=\"_blank\">Hacker Noon</a><div> is how hackers start their afternoons. We’re a part of the </div><a href=\"http://bit.ly/atAMIatAMI\" data-href=\"http://bit.ly/atAMIatAMI\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener nofollow noopener\" target=\"_blank\">@AMI</a><div>family. We are now </div><a href=\"http://bit.ly/hackernoonsubmission\" data-href=\"http://bit.ly/hackernoonsubmission\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener nofollow noopener\" target=\"_blank\">accepting submissions</a><div> and happy to </div><a href=\"mailto:partners@amipublications.com\" data-href=\"mailto:partners@amipublications.com\" class=\"markup--anchor markup--blockquote-anchor\" target=\"_blank\">discuss advertising &amp;sponsorship</a><div> opportunities.</div></blockquote><blockquote id=\"dca4\" class=\"graf graf--blockquote graf-after--blockquote\"><div>To learn more, <a href=\"https://goo.gl/4ofytp\" data-href=\"https://goo.gl/4ofytp\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">read our about page</a>, <a href=\"http://bit.ly/HackernoonFB\" data-href=\"http://bit.ly/HackernoonFB\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">like/message us on Facebook</a>, or simply, <a href=\"https://goo.gl/k7XYbx\" data-href=\"https://goo.gl/k7XYbx\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">tweet/DM @HackerNoon.</a></div></blockquote><blockquote id=\"708a\" class=\"graf graf--blockquote graf-after--blockquote\"><div>If you enjoyed this story, we recommend reading our <a href=\"http://bit.ly/hackernoonlatestt\" data-href=\"http://bit.ly/hackernoonlatestt\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener nofollow noopener\" target=\"_blank\">latest tech stories</a> and <a href=\"https://hackernoon.com/trending\" data-href=\"https://hackernoon.com/trending\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener nofollow\" target=\"_blank\">trending tech stories</a>. Until next time, don’t take the realities of the world for granted!</div></blockquote><figure id=\"c5b7\" class=\"graf graf--figure graf-after--blockquote graf--trailing\"><div class=\"aspectRatioPlaceholder is-locked\"><a href=\"https://bit.ly/2O1yNyY\" data-href=\"https://bit.ly/2O1yNyY\" class=\"graf-imageAnchor\" data-action=\"image-link\" data-action-observe-only=\"true\"><img class=\"graf-image\" data-image-id=\"1*QCV7h713dLgy5COZTyBLdQ@2x.png\" data-width=\"1400\" data-height=\"701\" src=\"https://cdn-images-1.medium.com/max/1600/1*QCV7h713dLgy5COZTyBLdQ@2x.png\" alt=\"image\" /></a></div></figure></figure></figure></figure></figure></figure></figure></figure></figure>",
        "created_at": "2018-10-10T15:17:41+0000",
        "updated_at": "2018-10-10T15:17:43+0000",
        "published_at": "2016-07-23T04:24:01+0000",
        "published_by": [
          "Nicolas A Perez"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 4,
        "domain_name": "hackernoon.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12356"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 995,
            "label": "testing",
            "slug": "testing"
          }
        ],
        "is_public": false,
        "id": 12345,
        "uid": null,
        "title": "Stubbed Cassandra",
        "url": "http://www.scassandra.org/",
        "content": "<p>Developing software is fun. Developing software that is well tested, where those tests run quickly is even more fun.</p><p>Stubbed Cassandra is an open source tool that enables you to test applications that use Cassandra in a quick, deterministic way.</p><p>It is especially aimed edge case testing such as read and write timeouts.</p><p>It acts as a real Cassandra instance and can be primed to respond with results or with exceptions like read timeouts. It does this by implementing the server side of the CQL binary protocol.</p><p>It is separated into two components:</p><ul><li><a href=\"http://scassandra-docs.readthedocs.org/en/latest/java/overview/\">Java Client</a>: Java client for Scassandra. A thin Java wrapper around Scassandra that allows Java projects to depend on Scassandra via maven dependency and have a programmatic interface for starting/stopping and priming.</li>\n  <li><a href=\"http://scassandra-docs.readthedocs.org/en/latest/standalone/overview/\">Scassandra Server</a>: Stubbed Scassandra server. Only go here if you’re insterested in running Stubbed Cassandra without a build tool such as maven or gradle. Implemented in Scala, can be run as a standalone server or depended on via the Java client. Doesn’t have an embedded Cassandra, is a standalone implementation of the server side of the Cassandra native protocol. You can prime the server to return rows, read timeout and write timeout via a REST API.</li>\n</ul><p>Scassandra, currently v1.1.0, is aimed at Java developers so most of the information is on the Java Client section of the website.  It also may be used as a standalone jar.</p><h3 id=\"release-v111\">Release v1.1.1</h3><ul><li><a href=\"https://github.com/scassandra/scassandra-server/issues/181\">#181</a> - Record ‘USE ' queries in activity log</li>\n</ul><h3 id=\"release-v110\">Release v1.1.0</h3><ul><li><a href=\"https://github.com/scassandra/scassandra-server/pull/171\">#171</a> - Protocol 3 and 4 support.  Migrate cql-antlr into tree as submodule.</li>\n</ul><h3 id=\"release-v1010\">Release v1.0.10</h3><ul><li><a href=\"https://github.com/scassandra/scassandra-server/pull/167\">#167</a> - Provide CORS support to Scassandra endpoints</li>\n</ul><h3 id=\"release-v109\">Release v1.0.9</h3><ul><li><a href=\"https://github.com/scassandra/scassandra-server/pull/157\">#157</a> - Share ActorSystem between Scassandra instances</li>\n</ul><h3 id=\"release-v108\">Release v1.0.8</h3><ul><li><a href=\"https://github.com/scassandra/scassandra-server/pull/155\">#155</a> - Add getters for thens in multi-prime requests</li>\n</ul><h3 id=\"release-v107\">Release v1.0.7</h3><ul><li><a href=\"https://github.com/scassandra/scassandra-server/pull/151\">#151</a> - Allow priming of config using config object</li>\n  <li><a href=\"https://github.com/scassandra/scassandra-server/pull/154\">#154</a> - Provide custom configuration for batch result</li>\n</ul><h3 id=\"release-v106\">Release v1.0.6</h3><ul><li><a href=\"https://github.com/scassandra/scassandra-server/pull/149\">#149</a> - Introduce alias for Then to avoid reserved word issues</li>\n  <li><a href=\"https://github.com/scassandra/scassandra-server/pull/150\">#150</a> - Handle invalid types with an informative error message</li>\n</ul><h3 id=\"release-v105\">Release v1.0.5</h3><ul><li><a href=\"https://github.com/scassandra/scassandra-server/pull/148\">#148</a> - Use shapeless 2</li>\n</ul><h3 id=\"release-v104\">Release v1.0.4</h3><ul><li><a href=\"https://github.com/scassandra/scassandra-server/pull/147\">#147</a> - Akka configuration fix</li>\n</ul><h3 id=\"release-v103\">Release v1.0.3</h3><ul><li><a href=\"https://github.com/scassandra/scassandra-server/pull/141\">#141</a> - Add ability to retrieve prepared multi primes</li>\n</ul><h3 id=\"release-v102\">Release v1.0.2</h3><ul><li><a href=\"https://github.com/scassandra/scassandra-server/pull/140\">#140</a> - Support for clearing prepared multi primes</li>\n</ul><h3 id=\"release-v101\">Release v1.0.1</h3><ul><li><a href=\"https://github.com/scassandra/scassandra-server/pull/139\">#139</a> - Delay support for batch statements</li>\n</ul><h3 id=\"release-v100\">Release v1.0.0</h3><ul><li>Multi prime API.</li>\n</ul><h3 id=\"release-v0110\">Release v0.11.0</h3><ul><li>Log better error message for timeout binding to ports</li>\n  <li><a href=\"https://github.com/scassandra/scassandra-server/issues/112\">#112</a> - Prime Result that closes connection</li>\n  <li><a href=\"https://github.com/scassandra/scassandra-server/pull/115\">#115</a> - Archive standalone jar</li>\n</ul><h3 id=\"release-v0100\">Release v0.10.0</h3><ul><li>Batch support</li>\n  <li>Ability to see currently connected clients and kill connections</li>\n</ul><h3 id=\"release-v091\">Release v0.9.1</h3><ul><li>Java 6 suppport for those still sadly using it</li>\n  <li>Verification for when a prepared statement is prepared</li>\n  <li>Fat jar support</li>\n  <li>Capturing of query parameters for non-prepared statements</li>\n</ul><h3 id=\"release-v080\">Release v0.8.0</h3><ul><li>Noving to a single build for server, client and tests.</li>\n</ul><h3 id=\"release-v070\">Release v0.7.0</h3><ul><li>Support for priming specific errors e.g the number of replicas responded rather than just that it wsa a read/write timeout</li>\n  <li>Removed support for version 1.* of the Java DataStax driver due to classpath issues (you can still use 2.* with Cassandra 1.2 and it will work)</li>\n</ul><h3 id=\"release-v060\">Release v0.6.0</h3><ul><li>Support for maps of any type</li>\n  <li>New method for priming types: CqlType (the ColumnTypes enum has been deprecated)</li>\n  <li>Prepared statement matcher now handles maps</li>\n</ul><h3 id=\"release-v050\">Release v0.5.0</h3><ul><li>Support for lists and sets of any type</li>\n  <li>Prepared statement matcher that handles matching the varialbe list correctly</li>\n</ul><h3 id=\"release-v041\">Release v0.4.1</h3><ul><li>[Feature #50] Support adding a fixed delay to both queries and prepared statements</li>\n  <li>[Feature #52] Cassandra 2.1 support</li>\n  <li>[Feature #53] JUnit matchers for queries and prepared statements</li>\n</ul><h3 id=\"release-v030\">Release v0.3.0</h3><ul><li>Text map maps support: varchar, ascii and text</li>\n  <li>Can use a queryPattern rather than a query for priming, making knowing the exact query the application will execute no longer necessary</li>\n  <li>[Bug #49] QueryPattern for prepared statements for error scenarios do not work</li>\n</ul><h3 id=\"release-v020\">Release v0.2.0</h3><ul><li>Lists and sets of the character types: varchar, ascii and text</li>\n  <li>JUnit rule for Java Client</li>\n</ul><h3 id=\"release-v01\">Release v0.1</h3><ul><li>Priming of queries with columns of all the primitive types (no suport for collections/custom tyes).</li>\n  <li>Priming of prepared statements. The variable (?s) types and response types can be any of the primitive types.</li>\n  <li>Retrieval of a list of all recorded queries.</li>\n  <li>Retrieval of a list of all the recorded executed prepared statements. If the prepared statement has been primed then the variable values are also visible.</li>\n</ul><h3 id=\"feature-backlog\">Feature backlog</h3><ul><li>User defined types</li>\n  <li>Batches</li>\n  <li>Retrieval of a list of all prepared statements even if they haven’t been executed.</li>\n  <li>Priming of tables rather than queries. Currently Scassandra does not parse the query and compares an executed query with all the primes query field. This would be very useful for priming the system keyspace as certain drivers expect the same thing to be in system.local but do slightly different queries to retireve it.</li>\n</ul><p>For feature requests and bug reports raise an issue at the <a href=\"https://github.com/scassandra/scassandra-server/issues\">Github issues page</a></p><p>Any questions ping me on twitter: @chbatey</p><ul><li>Christopher Batey (@chbatey)</li>\n  <li>Andrew Tolbert</li>\n  <li>Dogan Narinc</li>\n</ul>",
        "created_at": "2018-10-08T20:04:55+0000",
        "updated_at": "2018-10-08T20:05:01+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 4,
        "domain_name": "www.scassandra.org",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12345"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          },
          {
            "id": 1239,
            "label": "data.pipeline",
            "slug": "data-pipeline"
          }
        ],
        "is_public": false,
        "id": 12336,
        "uid": null,
        "title": "Data Pipelines with Spark & DataStax Enterprise",
        "url": "https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise",
        "content": "Data Pipelines with Spark &amp; DataStax Enterprise\n      \n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            </a><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div>\n    <div class=\"wrapper\"><p>Successfully reported this slideshow.</p><div id=\"slideview-container\" class=\"\"><div class=\"row\"><div id=\"main-panel\" class=\"small-12 large-8 columns\"><div class=\"sectionElements\"><div class=\"playerWrapper\"><div><div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\">Data Pipelines with Spark &amp; DataStax Enterprise<div class=\"stage valign-first-slide\"><div class=\"slide_container\"><section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-1-638.jpg?cb=1461863541\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-1-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-1-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-1-1024.jpg?cb=1461863541\" alt=\"Simon Ambridge&#10;Data Pipelines With Spark &amp; DSE&#10;An Introduction To Building Agile, Flexible and Scalable Big Data and Data ...\" /></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-2-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-2-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-2-1024.jpg?cb=1461863541\" alt=\"Certified Apache Cassandra and DataStax enthusiast who enjoys&#10;explaining that the traditional approaches to data managemen...\" /></i></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-3-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-3-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-3-1024.jpg?cb=1461863541\" alt=\"Introduction To Big Data Pipelines&#10;\" /></i></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-4-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-4-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-4-1024.jpg?cb=1461863541\" alt=\"Big, Static Data&#10;Fast, Streaming Data&#10;Big Data Pipelining: Classification&#10;Big Data Pipelines can mean different things to ...\" /></i></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-5-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-5-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-5-1024.jpg?cb=1461863541\" alt=\"Static Datasets&#10;All You Can Eat?&#10;Really.&#10;\" /></i></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-6-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-6-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-6-1024.jpg?cb=1461863541\" alt=\"Static Data Analytics : Traditional Tools&#10;Repeated iterations, at each stage&#10;Run/debug cycle can be slow&#10;Sampling Modeling...\" /></i></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-7-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-7-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-7-1024.jpg?cb=1461863541\" alt=\"Static Data Analytics : Scale Up Challenges&#10;Sampling and analysis often run on a single machine&#10;• CPU and memory limitatio...\" /></i></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-8-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-8-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-8-1024.jpg?cb=1461863541\" alt=\"Static Data Analytics : Traditional Scaling&#10;DATA (GB)&#10;DATA (MB)&#10;DATA (TB)&#10;Small datasets, small servers&#10;Large datasets, la...\" /></i></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-9-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-9-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-9-1024.jpg?cb=1461863541\" alt=\"Static Data Analytics: Big Data Problems&#10;Data is getting Really Big!&#10;• Data volumes are getting larger!&#10;• The number of da...\" /></i></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-10-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-10-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-10-1024.jpg?cb=1461863541\" alt=\"Static Data Analytics : Big Data Needs&#10;We need scalable infrastructure + distributed technologies&#10;• Data volumes can be sc...\" /></i></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-11-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-11-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-11-1024.jpg?cb=1461863541\" alt=\"Static Data Analytics : DSE Delivers&#10;Building a distributed data processing framework can be a complex task!&#10;It needs to b...\" /></i></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-12-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-12-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-12-1024.jpg?cb=1461863541\" alt=\"Cassandra: THE Web, IoT &amp; Cloud Database&#10;What is Apache Cassandra?&#10;• Very fast&#10;• Extremely resilient&#10;• Across multiple dat...\" /></i></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-13-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-13-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-13-1024.jpg?cb=1461863541\" alt=\"DataStax&#10;Enterprise&#10;DataStax Enterprise: Editions&#10;DataStax Enterprise Standard&#10;• DSE Standard is DataStax’s entry level&#10;co...\" /></i></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-14-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-14-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-14-1024.jpg?cb=1461863541\" alt=\"Spark: THE Analytics Engine&#10;What is Apache Spark?&#10;• Distributed in-memory analytic processing&#10;• Batch and streaming analyt...\" /></i></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-15-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-15-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-15-1024.jpg?cb=1461863541\" alt=\"Spark: Dayton Gray Sort Contest&#10;Dayton Gray benchmark - tests how fast a system can sort 100 TB&#10;of data (1 trillion record...\" /></i></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-16-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-16-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-16-1024.jpg?cb=1461863541\" alt=\"DataStax Enterprise: Analytics Integration&#10;Cassandra Cluster&#10;Spark Cluster&#10;ETL&#10;Spark Cluster&#10;• Tight integration&#10;• Data lo...\" /></i></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-17-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-17-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-17-1024.jpg?cb=1461863541\" alt=\"Static Data Analytics : Requirements&#10;Valid data pipeline analysis methods must be:&#10;Auditable&#10;• Reproducible&#10;• Documented&#10;C...\" /></i></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-18-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-18-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-18-1024.jpg?cb=1461863541\" alt=\"Notebooks: Features&#10;What are Notebooks?&#10;• Drive your data analysis from the browser&#10;• Highly interactive&#10;• Tight integrati...\" /></i></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-19-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-19-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-19-1024.jpg?cb=1461863541\" alt=\"Example: Spark Notebook&#10;Cells&#10;Markdown&#10;Output&#10;Controls&#10;\" /></i></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-20-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-20-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-20-1024.jpg?cb=1461863541\" alt=\"Static Data Analytics : Approach&#10;Example architecture &amp; requirements&#10;1. Optimised source data format&#10;2. Distributed in-mem...\" /></i></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-21-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-21-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-21-1024.jpg?cb=1461863541\" alt=\"Static Data Analytics : Example&#10;ADAM&#10;Notebook Persistent Storage&#10;OLTP Database Visualisation&#10;Genome research platform - AD...\" /></i></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-22-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-22-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-22-1024.jpg?cb=1461863541\" alt=\"Static Data Analytics : Pipeline Process Flow&#10;3. Persistent data storage&#10;2. Interactive, flexible and reproducible analysi...\" /></i></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-23-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-23-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-23-1024.jpg?cb=1461863541\" alt=\"Static Data Analytics : Pipeline Scalability&#10;• Add more (physical or virtual) nodes as&#10;required to add capacity&#10;• Containe...\" /></i></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-24-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-24-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-24-1024.jpg?cb=1461863541\" alt=\"Static Data Analytics : Now&#10;• No longer an iterative process constrained by hardware limitations&#10;• Now a more scalable, re...\" /></i></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-25-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-25-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-25-1024.jpg?cb=1461863541\" alt=\"Real-Time Datasets&#10;If it’s Not “Now”, Then It’s Probably Already Too Late&#10;\" /></i></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-26-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-26-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-26-1024.jpg?cb=1461863541\" alt=\"Big Data Pipelining: Why Real-Time?&#10;• React to customers faster and with more accuracy&#10;• Reduce risk through more accurate...\" /></i></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-27-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-27-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-27-1024.jpg?cb=1461863541\" alt=\"Big Data Pipelining: Real-Time Analytics&#10;• Capture, prepare, and process fast streaming data&#10;• Different approach from tra...\" /></i></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-28-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-28-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-28-1024.jpg?cb=1461863541\" alt=\"Big Data Pipelining: Real-Time Use Cases&#10;Sensor data (IoT)&#10;Transactional data&#10;User Experience&#10;Social media&#10;Use cases for s...\" /></i></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-29-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-29-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-29-1024.jpg?cb=1461863541\" alt=\"Big Data Analytics: Streams&#10;Data tidal waves!Netflix&#10;• Ingests Petabytes of data per day&#10;• Over 1 TRILLION transactions pe...\" /></i></section><section data-index=\"30\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-30-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-30-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-30-1024.jpg?cb=1461863541\" alt=\"Big Data Pipelining: Real-Time architecture&#10;Analytics in real-time, at scale&#10;Fast processing, distributed, in-memory&#10;Incre...\" /></i></section><section data-index=\"31\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-31-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-31-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-31-1024.jpg?cb=1461863541\" alt=\"Kafka: Architecture&#10;How Does Kafka Work?&#10;Kafka “De-couples” producers and consumers in data pipelines&#10;’Producers’ send mes...\" /></i></section><section data-index=\"32\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-32-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-32-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-32-1024.jpg?cb=1461863541\" alt=\"Kafka: Streaming With Spark&#10;Kafka writes, Spark reads&#10;• Topics can have multiple partitions&#10;• Each topic partition stored ...\" /></i></section><section data-index=\"33\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-33-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-33-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-33-1024.jpg?cb=1461863541\" alt=\"DataStax Enterprise: Streaming Schematic&#10;Sensor&#10;Network&#10;Signal&#10;Aggregation&#10;Services&#10;Messaging Queue&#10;Sensor Data Queue&#10;Mana...\" /></i></section><section data-index=\"34\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-34-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-34-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-34-1024.jpg?cb=1461863541\" alt=\"DataStax Enterprise: Streaming Analytics&#10;Real-time&#10;Analytics&#10;Persistent Storage&#10;OLTP Database&#10;!$£€!&#10;Personalisation&#10;Action...\" /></i></section><section data-index=\"35\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-35-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-35-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-35-1024.jpg?cb=1461863541\" alt=\"DataStax Enterprise: Multi-DC Uses&#10;DC: EUROPEDC: USA&#10;Real-time active-active geo-replication&#10;across physical datacentres&#10;4...\" /></i></section><section data-index=\"36\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-36-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-36-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-36-1024.jpg?cb=1461863541\" alt=\"Real-Time Analytics: DSE Multi-DC&#10;Workload Management and Separation With DSE&#10;Analytics / BI&#10;Analytics&#10;Datacentre&#10;OLTP&#10;Dat...\" /></i></section><section data-index=\"37\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-37-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-37-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-37-1024.jpg?cb=1461863541\" alt=\"DSE &amp; Analytics : Summary&#10;Static, Massive Data&#10;Scalable Data Pipelines&#10;1. Optimised data storage formats&#10;2. Scalable, dist...\" /></i></section><section data-index=\"38\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-38-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-38-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-38-1024.jpg?cb=1461863541\" alt=\"Thank you!&#10;\" /></i></section><section data-index=\"39\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/data-pipelines-with-spark-datastax-enterprise\" data-small=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/85/data-pipelines-with-spark-datastax-enterprise-39-320.jpg?cb=1461863541\" data-normal=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-39-638.jpg?cb=1461863541\" data-full=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-39-1024.jpg?cb=1461863541\" alt=\"Data Pipelines with Spark &amp; DataStax Enterprise\" /></i></section><div class=\"j-next-container next-container\"><div class=\"content-container\"><div class=\"next-slideshow-wrapper\"><div class=\"j-next-slideshow next-slideshow\"><p>Upcoming SlideShare</p></div><p>Loading in …5</p><p>×</p></div></div></div></div></div></div></div></div></div><div class=\"slideshow-info-container\" itemscope=\"itemscope\" itemtype=\"https://schema.org/MediaObject\"><div class=\"slideshow-tabs-container show-for-medium-up\"><ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    0 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\">\n                \n                  2 Likes\n                \n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\">\n                Statistics\n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\">\n                Notes\n              </i></a>\n            </li>\n          </ul><div class=\"tabs-content\"><div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\"><ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"itemscope\"><li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"ClaudioSantoLongo\" rel=\"nofollow\" href=\"https://www.slideshare.net/ClaudioSantoLongo?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Claudio Santo Longo\n                            \n                              \n                                , \n                                Engineering Researcher\n                              \n                              \n                                 at \n                                Università degli Studi di Modena e Reggio Emilia\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"sujeetv\" rel=\"nofollow\" href=\"https://www.slideshare.net/sujeetv?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Sujeet Varakhedi\n                            \n                              \n                                , \n                                Engineering eBay- Hiring senior distributed systems engineers in data, real-time analytics and deep learning pipelines\n                              \n                              \n                                 at \n                                eBay\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n              </ul></div><div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No Downloads</p></div><div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No notes for slide</p></div></div></div><div class=\"notranslate transcript add-padding-right j-transcript\"><ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    Simon Ambridge\nData Pipelines With Spark &amp; DSE\nAn Introduction To Building Agile, Flexible and Scalable Big Data and Data Science Pipelines\nVersion 0.8\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-2-638.jpg?cb=1461863541\" title=\"Certified Apache Cassandra and DataStax enthusiast who enjo...\" target=\"_blank\">\n        2.\n      </a>\n    Certified Apache Cassandra and DataStax enthusiast who enjoys\nexplaining that the traditional approaches to data management just\ndon’t cut it anymore in the new always on, no single point of failure,\nhigh volume, high velocity, real time distributed data management\nworld.\nPreviously 25 years implementing Oracle relational data management\nsolutions. Certified in Exadata, Oracle Cloud, Oracle Essbase, Oracle\nLinux and OBIEE\nsimon.ambridge@datastax.com\n@stratman1958\nSimon Ambridge\nPre-Sales Solution Engineer, Datastax UK\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-3-638.jpg?cb=1461863541\" title=\"Introduction To Big Data Pipelines&#10;\" target=\"_blank\">\n        3.\n      </a>\n    Introduction To Big Data Pipelines\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-4-638.jpg?cb=1461863541\" title=\"Big, Static Data&#10;Fast, Streaming Data&#10;Big Data Pipelining: ...\" target=\"_blank\">\n        4.\n      </a>\n    Big, Static Data\nFast, Streaming Data\nBig Data Pipelining: Classification\nBig Data Pipelines can mean different things to different people\nRepeated analysis on a static but massive dataset\n• An element of research – e.g. genomics, clinical trial,\ndemographic data\n• Typically repetitive, iterative, shared amongst data\nscientists for analysis\nReal-time analytics on streaming data\n• Industrialised or commercial processes – sensors, tick\ndata, bioinformatics, transactional data, real-time\npersonalisation\n• Happening in real-time, data cannot be dropped or lost\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-5-638.jpg?cb=1461863541\" title=\"Static Datasets&#10;All You Can Eat?&#10;Really.&#10;\" target=\"_blank\">\n        5.\n      </a>\n    Static Datasets\nAll You Can Eat?\nReally.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-6-638.jpg?cb=1461863541\" title=\"Static Data Analytics : Traditional Tools&#10;Repeated iteratio...\" target=\"_blank\">\n        6.\n      </a>\n    Static Data Analytics : Traditional Tools\nRepeated iterations, at each stage\nRun/debug cycle can be slow\nSampling Modeling InterpretTuning Reporting\nRe-sample\nTypical traditional ‘static’ data analysis model\nData\nResults\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-7-638.jpg?cb=1461863541\" title=\"Static Data Analytics : Scale Up Challenges&#10;Sampling and an...\" target=\"_blank\">\n        7.\n      </a>\n    Static Data Analytics : Scale Up Challenges\nSampling and analysis often run on a single machine\n• CPU and memory limitations\nLimited sampling of a large dataset because of data size limitations\n• Multiple iterations over large datasets is frequently not an ideal\napproach\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-8-638.jpg?cb=1461863541\" title=\"Static Data Analytics : Traditional Scaling&#10;DATA (GB)&#10;DATA ...\" target=\"_blank\">\n        8.\n      </a>\n    Static Data Analytics : Traditional Scaling\nDATA (GB)\nDATA (MB)\nDATA (TB)\nSmall datasets, small servers\nLarge datasets, large servers\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-9-638.jpg?cb=1461863541\" title=\"Static Data Analytics: Big Data Problems&#10;Data is getting Re...\" target=\"_blank\">\n        9.\n      </a>\n    Static Data Analytics: Big Data Problems\nData is getting Really Big!\n• Data volumes are getting larger!\n• The number of data sources is exploding!\n• More data is arriving faster!\nScaling up is becoming impractical\n• Physical limits\n• Datalimits\n• The validity of the analysis becomes obsolete, faster\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-10-638.jpg?cb=1461863541\" title=\"Static Data Analytics : Big Data Needs&#10;We need scalable inf...\" target=\"_blank\">\n        10.\n      </a>\n    Static Data Analytics : Big Data Needs\nWe need scalable infrastructure + distributed technologies\n• Data volumes can be scaled\n• Distribute the data across multiple low-cost machines\n• Faster processing\n• More complex processing\n• No single point of failure\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-11-638.jpg?cb=1461863541\" title=\"Static Data Analytics : DSE Delivers&#10;Building a distributed...\" target=\"_blank\">\n        11.\n      </a>\n    Static Data Analytics : DSE Delivers\nBuilding a distributed data processing framework can be a complex task!\nIt needs to be:\n• Scalable\n• Fast in-memory processing\n• Replicated for resiliency\n• Batch and real-time data feeds\n• Ad-hoc queries\nDataStax delivers an integrated analytics platform\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-12-638.jpg?cb=1461863541\" title=\"Cassandra: THE Web, IoT &amp; Cloud Database&#10;What is Apache Cas...\" target=\"_blank\">\n        12.\n      </a>\n    Cassandra: THE Web, IoT &amp; Cloud Database\nWhat is Apache Cassandra?\n• Very fast\n• Extremely resilient\n• Across multiple data centres\n• No single point of failure\n• Continuous Availability, Disaster Avoidance\n• Linear scale\n• Easy to operate\nEnterprise Cassandra platform from Datastax\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-13-638.jpg?cb=1461863541\" title=\"DataStax&#10;Enterprise&#10;DataStax Enterprise: Editions&#10;DataStax ...\" target=\"_blank\">\n        13.\n      </a>\n    DataStax\nEnterprise\nDataStax Enterprise: Editions\nDataStax Enterprise Standard\n• DSE Standard is DataStax’s entry level\ncommercial database offering\n• Represents the minimum recommended to\ndeploy Cassandra in a production environment\nDataStax Enterprise Max\n• DSE Max is DataStax’s advanced commercial\ndatabase offering\n• Designed for production Cassandra\nenvironments that have mixed workload\nrequirements\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-14-638.jpg?cb=1461863541\" title=\"Spark: THE Analytics Engine&#10;What is Apache Spark?&#10;• Distrib...\" target=\"_blank\">\n        14.\n      </a>\n    Spark: THE Analytics Engine\nWhat is Apache Spark?\n• Distributed in-memory analytic processing\n• Batch and streaming analytics\n• Fast - 10x-100x faster than Hadoop MapReduce\n• Rich Scala, Java and Python APIs\nTightly integrated with DSE\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-15-638.jpg?cb=1461863541\" title=\"Spark: Dayton Gray Sort Contest&#10;Dayton Gray benchmark - tes...\" target=\"_blank\">\n        15.\n      </a>\n    Spark: Dayton Gray Sort Contest\nDayton Gray benchmark - tests how fast a system can sort 100 TB\nof data (1 trillion records)\n• Previous world record held by Hadoop MapReduce cluster of 2100\nnodes, in 72 minutes\n• 2014: Spark completed the benchmark in 23 minutes on just 206 EC2\nnodes = 3X faster using 10X fewer machines\n• Spark sorted 1 PB (10 trillion records) on 190 machines in &lt; 4 hours.\nPrevious Hadoop MapReduce time of 16 hours on 3800 machines = 4X\nfaster using 20X fewer machines\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-16-638.jpg?cb=1461863541\" title=\"DataStax Enterprise: Analytics Integration&#10;Cassandra Cluste...\" target=\"_blank\">\n        16.\n      </a>\n    DataStax Enterprise: Analytics Integration\nCassandra Cluster\nSpark Cluster\nETL\nSpark Cluster\n• Tight integration\n• Data locality\n• Microsecond response times\nX\n• Apache Cassandra for Distributed Persistent Storage\n• Integrated Apache Spark for Distributed Real-Time Analytics\n• Analytics nodes close to data - no ETL required\nX\n• Loose integration\n• Data separate from processing\n• Millisecond response times\n“Latency\t\n  when\t\n  transferring\t\n  data\t\n  is\t\n  unavoidable.\t\n  The\t\n  trick\t\n  is\t\n  to\t\n  reduce\t\n  \nthe\t\n  latency\t\n  to\t\n  as\t\n  close\t\n  to\t\n  zero\t\n  as\t\n  possible…”\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-17-638.jpg?cb=1461863541\" title=\"Static Data Analytics : Requirements&#10;Valid data pipeline an...\" target=\"_blank\">\n        17.\n      </a>\n    Static Data Analytics : Requirements\nValid data pipeline analysis methods must be:\nAuditable\n• Reproducible\n• Documented\nControlled\n• Version control\nCollaborative\n• Accessible\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-18-638.jpg?cb=1461863541\" title=\"Notebooks: Features&#10;What are Notebooks?&#10;• Drive your data a...\" target=\"_blank\">\n        18.\n      </a>\n    Notebooks: Features\nWhat are Notebooks?\n• Drive your data analysis from the browser\n• Highly interactive\n• Tight integration with Apache Spark\n• Handy tools for analysts:\n• Reproducible visual analysis\n• Code in Scala, CQL, SparkSQL, Python\n• Charting – pie, bar, line etc\n• Extensible with custom libraries\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-19-638.jpg?cb=1461863541\" title=\"Example: Spark Notebook&#10;Cells&#10;Markdown&#10;Output&#10;Controls&#10;\" target=\"_blank\">\n        19.\n      </a>\n    Example: Spark Notebook\nCells\nMarkdown\nOutput\nControls\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-20-638.jpg?cb=1461863541\" title=\"Static Data Analytics : Approach&#10;Example architecture &amp; req...\" target=\"_blank\">\n        20.\n      </a>\n    Static Data Analytics : Approach\nExample architecture &amp; requirements\n1. Optimised source data format\n2. Distributed in-memory analytics\n3. Interactive and flexible data analysis tool\n4. Persistent data store\n5. Visualisation tools\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-21-638.jpg?cb=1461863541\" title=\"Static Data Analytics : Example&#10;ADAM&#10;Notebook Persistent St...\" target=\"_blank\">\n        21.\n      </a>\n    Static Data Analytics : Example\nADAM\nNotebook Persistent Storage\nOLTP Database Visualisation\nGenome research platform - ADST (Agile Data Science Toolkit)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-22-638.jpg?cb=1461863541\" title=\"Static Data Analytics : Pipeline Process Flow&#10;3. Persistent...\" target=\"_blank\">\n        22.\n      </a>\n    Static Data Analytics : Pipeline Process Flow\n3. Persistent data storage\n2. Interactive, flexible and reproducible analysis\n1. Source data\n4. Visualise and analyse\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-23-638.jpg?cb=1461863541\" title=\"Static Data Analytics : Pipeline Scalability&#10;• Add more (ph...\" target=\"_blank\">\n        23.\n      </a>\n    Static Data Analytics : Pipeline Scalability\n• Add more (physical or virtual) nodes as\nrequired to add capacity\n• Container tools ease configuration\nmanagement and deployment\n• Scale out quickly\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-24-638.jpg?cb=1461863541\" title=\"Static Data Analytics : Now&#10;• No longer an iterative proces...\" target=\"_blank\">\n        24.\n      </a>\n    Static Data Analytics : Now\n• No longer an iterative process constrained by hardware limitations\n• Now a more scalable, resilient, dynamic, interactive process, easily shareable\nAnalyse\nThe new model for large-scale static data analytics\nShare\nX\nLoad\nSCALE &amp; DISTRIBUTE PROCESSING\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-25-638.jpg?cb=1461863541\" title=\"Real-Time Datasets&#10;If it’s Not “Now”, Then It’s Probably Al...\" target=\"_blank\">\n        25.\n      </a>\n    Real-Time Datasets\nIf it’s Not “Now”, Then It’s Probably Already Too Late\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-26-638.jpg?cb=1461863541\" title=\"Big Data Pipelining: Why Real-Time?&#10;• React to customers fa...\" target=\"_blank\">\n        26.\n      </a>\n    Big Data Pipelining: Why Real-Time?\n• React to customers faster and with more accuracy\n• Reduce risk through more accurate understanding of the market\n• Optimise return on marketing investment\n• Faster time to market\n• Improve efficiency\nIn a highly connected world\nIn most cases ‘real-time’ data changing at &lt;1s intervals\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-27-638.jpg?cb=1461863541\" title=\"Big Data Pipelining: Real-Time Analytics&#10;• Capture, prepare...\" target=\"_blank\">\n        27.\n      </a>\n    Big Data Pipelining: Real-Time Analytics\n• Capture, prepare, and process fast streaming data\n• Different approach from traditional batch processing\n• The speed of now – cannot wait\n• Immediate insight, instant decisions\nWhat problem are we trying to solve?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-28-638.jpg?cb=1461863541\" title=\"Big Data Pipelining: Real-Time Use Cases&#10;Sensor data (IoT)&#10;...\" target=\"_blank\">\n        28.\n      </a>\n    Big Data Pipelining: Real-Time Use Cases\nSensor data (IoT)\nTransactional data\nUser Experience\nSocial media\nUse cases for streaming analytics\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-29-638.jpg?cb=1461863541\" title=\"Big Data Analytics: Streams&#10;Data tidal waves!Netflix&#10;• Inge...\" target=\"_blank\">\n        29.\n      </a>\n    Big Data Analytics: Streams\nData tidal waves!Netflix\n• Ingests Petabytes of data per day\n• Over 1 TRILLION transactions per day (&gt;10 m per second) into DSE\nData streams?\nData torrent?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-30-638.jpg?cb=1461863541\" title=\"Big Data Pipelining: Real-Time architecture&#10;Analytics in re...\" target=\"_blank\">\n        30.\n      </a>\n    Big Data Pipelining: Real-Time architecture\nAnalytics in real-time, at scale\nFast processing, distributed, in-memory\nIncreasingly using a technology stack comprising Kafka, Spark and Cassandra\n• Scalable\n• Distributed\n• Resilient\nStreaming analytics architecture - what do we need?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-31-638.jpg?cb=1461863541\" title=\"Kafka: Architecture&#10;How Does Kafka Work?&#10;Kafka “De-couples”...\" target=\"_blank\">\n        31.\n      </a>\n    Kafka: Architecture\nHow Does Kafka Work?\nKafka “De-couples” producers and consumers in data pipelines\n’Producers’ send messages to the Kafka cluster, which in turn serves them up to\n’Consumers’\n• Kafka maintains feeds of messages in categories called topics\n• A Kafka cluster is comprised of one or more servers called a broker\nProducer\nProducer\nProducer\nConsumer\nConsumer\nConsumer\nKafka\nCluster\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-32-638.jpg?cb=1461863541\" title=\"Kafka: Streaming With Spark&#10;Kafka writes, Spark reads&#10;• Top...\" target=\"_blank\">\n        32.\n      </a>\n    Kafka: Streaming With Spark\nKafka writes, Spark reads\n• Topics can have multiple partitions\n• Each topic partition stored as a log (an ordered set of messages)\n• Messages are simply byte arrays, so can store any object in any format\n• Each message in a partition is assigned a unique offset\nSpark consumes messages as a stream, in micro batches, saved as RDD’s\n1 2 3 4 5 6 7 8\nPartition 0\n1 2 3 4 5 6 7 8\nPartition 1\n1 2 3 4 5 6\nPartition 0\nTemperature Topic\nRainfall Topic\nTemperature Consumer\nRainfall Consumer\nTemperature Consumer\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-33-638.jpg?cb=1461863541\" title=\"DataStax Enterprise: Streaming Schematic&#10;Sensor&#10;Network&#10;Sig...\" target=\"_blank\">\n        33.\n      </a>\n    DataStax Enterprise: Streaming Schematic\nSensor\nNetwork\nSignal\nAggregation\nServices\nMessaging Queue\nSensor Data Queue\nManagement\nBroker\nBroker\nCollection\nService\nData Storage\nOLTP PersistenceLayer\nStreaming Data\nIngest\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-34-638.jpg?cb=1461863541\" title=\"DataStax Enterprise: Streaming Analytics&#10;Real-time&#10;Analytic...\" target=\"_blank\">\n        34.\n      </a>\n    DataStax Enterprise: Streaming Analytics\nReal-time\nAnalytics\nPersistent Storage\nOLTP Database\n!$£€!\nPersonalisation\nActionable insight Monitoring\nWeb / Analytics / BI\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-35-638.jpg?cb=1461863541\" title=\"DataStax Enterprise: Multi-DC Uses&#10;DC: EUROPEDC: USA&#10;Real-t...\" target=\"_blank\">\n        35.\n      </a>\n    DataStax Enterprise: Multi-DC Uses\nDC: EUROPEDC: USA\nReal-time active-active geo-replication\nacross physical datacentres\n4 3\n25\n1\n4 3\n25\n1\n8\n1\n2\n3\n4\n5\n6\n7\n1\n2\n3\nOLTP:\nCassandra\n5\n4\nAnalytics:\nCassandra + Spark\nReplication\nReplication\nWorkload separation via virtual datacentres\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-36-638.jpg?cb=1461863541\" title=\"Real-Time Analytics: DSE Multi-DC&#10;Workload Management and S...\" target=\"_blank\">\n        36.\n      </a>\n    Real-Time Analytics: DSE Multi-DC\nWorkload Management and Separation With DSE\nAnalytics / BI\nAnalytics\nDatacentre\nOLTP\nDatacentre\n100% Uptime, Global Scale\nOLTP\nReal-Time Analytics\nMixed Load OLTP and Analytics Platform\nReplication\nReplication\nJDBC\nODBC\nSeparation of OLTP\nfrom Analytics\nSocial Media\nIoT\nPersonalisation &amp; Persistence\nPersonalisation\n!$£€!\nActionable insight\nMonitoring\nApp, Web\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-37-638.jpg?cb=1461863541\" title=\"DSE &amp; Analytics : Summary&#10;Static, Massive Data&#10;Scalable Dat...\" target=\"_blank\">\n        37.\n      </a>\n    DSE &amp; Analytics : Summary\nStatic, Massive Data\nScalable Data Pipelines\n1. Optimised data storage formats\n2. Scalable, distributed technologies\n3. Flexible and interactive analysis tools\n4. Resilient, persistent Storage\nReal-Time Streaming Data\nScalable Data Pipelines\n1. Scalable, distributed technologies\n2. De-coupled Producers and Consumers\n3. Real-Time analytics\n4. Resilient, persistent Storage\nSpark\nMesos\nAkka\nCassandra\nKafka\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347/95/data-pipelines-with-spark-datastax-enterprise-38-638.jpg?cb=1461863541\" title=\"Thank you!&#10;\" target=\"_blank\">\n        38.\n      </a>\n    Thank you!\n \n  </li>\n              </ol></div></div></div><aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\"><dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\"><ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"61471495\" title=\"PowerPoint: Designing Better Slides\" href=\"https://www.linkedin.com/learning/powerpoint-designing-better-slides?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_PowerPoint: Designing Better Slides\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"PowerPoint: Designing Better Slides\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=Qzyjo5pakOdYhauvchS%2F2whtPj0%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-kUyWs-dWfZX_pf8TfZLSiol4feCwDkwc2feivRTXiEY69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>PowerPoint: Designing Better Slides</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"61471495\" title=\"Teaching Future-Ready Students\" href=\"https://www.linkedin.com/learning/teaching-future-ready-students?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Teaching Future-Ready Students\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Teaching Future-Ready Students\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=l5Oo%2BC8xrExl1LjsAzoFgo7%2B9iA%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lWiKq89OfZHbuec7ZZLSiol8eeywAlgEzfemtRDTpEo69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Teaching Future-Ready Students</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"61471495\" title=\"Learning Schoology\" href=\"https://www.linkedin.com/learning/learning-schoology?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Learning Schoology\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Learning Schoology\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=Xw1azvTOagp%2FzMWd7i2GB7dHme8%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lWCWq_dOfZHTpecDZZLSioVUSfCUHkwI7fOyqQTHiEo69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Learning Schoology</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"43475359\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Streaming Big Data with Spark, Kafka, Cassandra, Akka &amp; Scala (from webinar)\" href=\"https://www.slideshare.net/helenaedelson/streaming-bigdata-helenawebinarv3\">\n    \n    <div class=\"related-content\"><p>Streaming Big Data with Spark, Kafka, Cassandra, Akka &amp; Scala (from webinar)</p><p>Helena Edelson</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"66171045\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Graph Data Modeling in DataStax Enterprise\" href=\"https://www.slideshare.net/ArtemChebotko/graph-data-modeling-in-datastax-enterprise\">\n    \n    <div class=\"related-content\"><p>Graph Data Modeling in DataStax Enterprise</p><p>Artem Chebotko</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"45517672\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Performance Pipeline Key Recommendations\" href=\"https://www.slideshare.net/ILworkNet/performance-pipelinekeyrecommendations\">\n    \n    <div class=\"related-content\"><p>Performance Pipeline Key Recommendations</p><p>Illinois workNet</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"45520069\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Pipeline Performance Analysis: Cohort Model\" href=\"https://www.slideshare.net/ILworkNet/pipeline-performanceanalysiscohortmodelmarch2015\">\n    \n    <div class=\"related-content\"><p>Pipeline Performance Analysis: Cohort Model</p><p>Illinois workNet</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"44256284\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Kafka aws\" href=\"https://www.slideshare.net/relmos/kafka-aws\">\n    \n    <div class=\"related-content\"><p>Kafka aws</p><p>Ariel Moskovich</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"28421166\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Big Data Integration &amp; Analytics Data Flows with AWS Data Pipeline (BDT207) | AWS re:Invent 2013\" href=\"https://www.slideshare.net/AmazonWebServices/big-data-integration-analytics-data-flows-with-aws-data-pipeline-bdt207-aws-reinvent-2013\">\n    \n    <div class=\"related-content\"><p>Big Data Integration &amp; Analytics Data Flows with AWS Data Pipeline (BDT207) |...</p><p>Amazon Web Services</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"62573289\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Big Data-Driven Applications  with Cassandra and Spark\" href=\"https://www.slideshare.net/ArtemChebotko/artem-chebotko-jeeconf-final\">\n    \n    <div class=\"related-content\"><p>Big Data-Driven Applications  with Cassandra and Spark</p><p>Artem Chebotko</p></div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div></div><footer>\n          <div class=\"row\"><div class=\"columns\"><ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div></div>\n          \n          <div class=\"row\"><div class=\"columns\"><p class=\"copyright text-center\">LinkedIn Corporation © 2018</p></div></div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div>\n    <div class=\"row\"><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></div>\n        \n    </div>\n    \n    \n      <noscript>\n    </noscript>",
        "created_at": "2018-10-05T23:26:14+0000",
        "updated_at": "2018-10-05T23:26:22+0000",
        "published_at": null,
        "published_by": [
          "DataStax"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 8,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/kikfvtxzqxwiueyte1go-signature-f7a57773a44c251d51a052d04c11ad12b41ccd5d5a146a6de773df449ed8bfe5-poli-160428162347-thumbnail-4.jpg?cb=1461863541",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12336"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1233,
            "label": "data.modeling",
            "slug": "data-modeling"
          },
          {
            "id": 1262,
            "label": "time.series",
            "slug": "time-series"
          }
        ],
        "is_public": false,
        "id": 12323,
        "uid": null,
        "title": "Advanced Time Series with Cassandra",
        "url": "https://www.datastax.com/dev/blog/advanced-time-series-with-cassandra",
        "content": "<section class=\"row\"><article id=\"post-10089\" class=\"post-10089 dev-post type-dev-post status-publish hentry category-blog-post\"><div class=\"DXDvBlgCtaAD_wrp1  DXDvBlgCtaAD_img5\"><p>Learn more about Apache Cassandra</p></div><p>Cassandra is an excellent fit for time series data, and it's widely used for storing many types of data that follow the time series pattern: performance metrics, fleet tracking, sensor data, logs, financial data (pricing and ratings histories), user activity, and so on.</p><p>A great introduction to this topic is Kelley Reynolds' <a href=\"http://rubyscale.com/blog/2011/03/06/basic-time-series-with-cassandra/\">Basic Time Series with Cassandra</a>. If you haven't read that yet, I highly recommend starting with it. This post builds on that material, covering a few more details, corner cases, and<br />advanced techniques.</p><h2>Indexes vs Materialized Views</h2><p>When working with time series data, one of two strategies is typically employed: either the column values contain row keys pointing to a separate column family which contains the actual data for events, or the complete set of data for each event is stored in the timeline itself.  The latter strategy can be implemented by serializing the entire event into a single column value or by using composite column names of the form <code>&lt;timestamp&gt;:&lt;event_field&gt;</code>.</p><p>With the first strategy, which is similar to building an index, you first fetch a set of row keys from a timeline and then multiget the matching data rows from a separate column family. This approach is appealing to many at first because it is more normalized; it allows for easy updates of events, doesn't require you to repeat the same data in multiple timelines, and lets you easily add built-in secondary indexes to your main data column family. However, the second step of the data fetching process, the multiget, is fairly expensive and slow. It requires querying many nodes where each node will need to perform many disk seeks to fetch the rows if they aren't well cached. This approach will not scale well with large data sets.</p><div id=\"attachment_10119\" class=\"wp-caption aligncenter\"><a rel=\"attachment wp-att-10119\" href=\"https://www.datastax.com/wp-content/uploads/2012/03/index_pattern2.png\"><img class=\"size-full wp-image-10119\" title=\"cassandra_timeline_index\" src=\"https://www.datastax.com/wp-content/uploads/2012/03/index_pattern2.png\" alt=\"Column family diagram for the index pattern\" width=\"718\" height=\"632\" srcset=\"https://www.datastax.com/wp-content/uploads/2012/03/index_pattern2.png 718w, https://www.datastax.com/wp-content/uploads/2012/03/index_pattern2-300x264.png 300w\" /></a><p class=\"wp-caption-text\">The top column family contains only a timeline index; the bottom, the actual data for the events.</p></div><p>The second strategy, which resembles maintaining a materialized view, provides much more efficient reads. Fetching a time slice of events only requires reading a contiguous portion of a row on one set of replicas. If the same event is tracked in multiple timelines, it's okay to denormalize and store all of the event data in each of those timelines. One of the main principles that Cassandra was built on is that disk space is very cheap resource; minimizing disk seeks at the cost of higher space consumption is a good tradeoff. Unless the data for each event is very large, I always prefer this strategy over the index strategy.</p><div id=\"attachment_10122\" class=\"wp-caption aligncenter\"><a rel=\"attachment wp-att-10122\" href=\"https://www.datastax.com/wp-content/uploads/2012/03/materialized_view_pattern.png\"><img class=\"size-full wp-image-10122\" title=\"cassandra_materialized_view_timeline\" src=\"https://www.datastax.com/wp-content/uploads/2012/03/materialized_view_pattern.png\" alt=\"Diagram of the &quot;materialized view&quot; pattern\" width=\"718\" height=\"313\" srcset=\"https://www.datastax.com/wp-content/uploads/2012/03/materialized_view_pattern.png 718w, https://www.datastax.com/wp-content/uploads/2012/03/materialized_view_pattern-300x130.png 300w\" /></a><p class=\"wp-caption-text\">All event data is serialized as JSON in the column values.</p></div><h2>Reversed Column Comparators</h2><p>Since <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-2355\">Cassandra 0.8</a>, column comparators can easily be reversed.  This means that if you're using timestamps or TimeUUIDs as column names, you can choose to have them sorted in reverse chronological order.</p><p>If the majority of your queries ask for the N most recent events in a timeline or N events immediately before a point in time, using a reversed comparator will give you a small performance boost over always setting <code>reversed=True</code> when fetching row slices from the timeline.</p><h2>Timeline Starting Points</h2><p>To support queries that ask for all events before a given time, your application usually needs to know when the timeline was first started. Otherwise, if you aren't guarenteed to have events in every bucket, you cannot just fetch buckets further and further back in time until you get back an empty row; there's no way to distinguish between a bucket that just happens to contain no events and one that falls before the timeline even began.</p><p>To prevent uneccessary searching through empty rows, we can keep track of when the earliest event was inserted for a given timeline using a metadata row. When an application writes to a timeline for the first time after starting up, it can read the metadata row, find out the current earliest timestamp, and write a new timestamp if it ever inserts an earlier event. To avoid race conditions, add a new column to the metadata row each time a new earliest event is inserted. I suggest using TimeUUIDs with a timestamp matching the event's timestamp for the column name so that the earliest timestamp will always be at the beginning of the metadata row.</p><p>After reading only the first column from the metadata row (either on startup or the first time it's required, refreshing periodically), the application can know exactly how far in the past it should look for events in a given timeline.</p><h2>High Throughput Timelines</h2><p>Each row in a timeline will be handled by a single set of replicas, so they may become hotspots while the row holding the current time bucket falls in their range. It's not very common, but occasionally a single timeline may grow at such a rate that a single node cannot easily handle it. This may happen if tens of thousands of events are being inserted per second or at a lower rate if the column values are large. Sometimes, by reducing the size of the time bucket enough, a single set of replicas will only have to ingest writes for a short enough period of time that the throughput is sustainable, but this isn't always a feasible option.</p><p>In order to spread the write load among more nodes in the cluster, we can split each time bucket into multiple rows. We can use row keys of the form <code>&lt;timeline&gt;:&lt;bucket&gt;:&lt;partition&gt;</code>, where <code>partition</code> is a number between 1 and the number of rows we want to split the bucket across. When writing, clients should append new events to each of the partitions in round robin fashion so that all partitions grow at a similar rate. When reading, clients should fetch slices from all of the partition rows for the time bucket they are interested in and merge the results client-side, similar to the merge step of merge-sort.</p><p>If some timelines require splitting while others do not, or if you need to be able to adjust the number of rows a timeline is split across periodically, I suggest storing info about the splits in a metadata row for the timeline in a separate column family (see the notes at the end of this post). The metadata row might have one column for each time the splitting factor is adjusted, something like <code>{&lt;timestamp&gt;: &lt;splitting_factor&gt;}</code>, where <code>timestamp</code> should align with the beginning of a time bucket after which clients should use the new splitting factor. When reading a time slice, clients can know how many partition rows to ask for during a given range of time based on this metadata.</p><div id=\"attachment_10150\" class=\"wp-caption aligncenter\"><a rel=\"attachment wp-att-10150\" href=\"https://www.datastax.com/wp-content/uploads/2012/03/high_throughput2.png\"><img class=\"size-full wp-image-10150\" title=\"cassandra_high_throughput_timeline\" src=\"https://www.datastax.com/wp-content/uploads/2012/03/high_throughput2.png\" alt=\"A diagram of the high throughput timeline layout\" width=\"718\" height=\"801\" srcset=\"https://www.datastax.com/wp-content/uploads/2012/03/high_throughput2.png 718w, https://www.datastax.com/wp-content/uploads/2012/03/high_throughput2-268x300.png 268w\" /></a><p class=\"wp-caption-text\">    The \"jbellis\" timeline has increased its splitting factor over time; it currently spans three rows for each time bucket.</p></div><h2>Variable Time Bucket Sizes</h2><p>For some applications, the rate of events for different timelines may differ drastically. If some timelines have an incoming event rate that is 100x or 1000x higher than other timelines, you may want to use a different time bucket size for different timelines to prevent extremely wide rows for the busy timelines or a very sparse set of rows for the slow timelines. In other cases, a single timeline may increase or decrease its rate of events over time; eventually, this timeline may need to change its bucket size to keep rows from growing too wide or too sparse.</p><p>Similar to the timeline metadata suggestion for high throughput timelines (above), we can track time bucket sizes and their changes for individual timelines with a metadata row. Use a column of the form <code>{&lt;timestamp&gt;: &lt;bucket_size&gt;}</code>, where <code>timestamp</code> aligns with the start of a time bucket, and <code>bucket_size</code> is the bucket size to use after that point in time, measured in a number of seconds. When reading a time slice of events, calculate the appropriate set of row keys based on the bucket size during that time period.</p><div id=\"attachment_10143\" class=\"wp-caption aligncenter\"><a rel=\"attachment wp-att-10143\" href=\"https://www.datastax.com/wp-content/uploads/2012/03/variable_buckets.png\"><img class=\"size-full wp-image-10143\" title=\"cassandra_variable_bucket_size_timeline\" src=\"https://www.datastax.com/wp-content/uploads/2012/03/variable_buckets.png\" alt=\"A diagram of a timeline with changing time bucket sizes\" width=\"718\" height=\"801\" srcset=\"https://www.datastax.com/wp-content/uploads/2012/03/variable_buckets.png 718w, https://www.datastax.com/wp-content/uploads/2012/03/variable_buckets-268x300.png 268w\" /></a><p class=\"wp-caption-text\">At time 1332959000, the \"jbellis\" timeline switched from using 1000 second time buckets to 10 second buckets.</p></div><h2>Notes on Timeline Metadata</h2><p>When using timeline metadata for high throughput timelines or variable bucket size timelines, the metadata rows should typically be stored in a separate column family to allow for cache tuning. I suggest using a fair amount of key cache on the metadata column family if it will be queried frequently.</p><p>The timeline metadata should generally be written by a process external to the application to avoid race conditions, unless the application operates in such a fashion that this isn't a concern. The application can read the metadata row on startup or on demand for a particular timeline; if the application is long lived, it should periodically poll the metadata row for updates. If this is done, a new splitting factor or bucket size can safely be set to start with a new time bucket that begins shortly in the future; the application processes should see the updated metadata in advance, before the new bucket begins, allowing them to change their behavior right on time.</p><h2>Further Data Modeling Advaice</h2><p>If you want to learn more about data modeling, I recommend taking <a href=\"https://academy.datastax.com/courses/ds220-data-modeling?dxt=blogposting\">Datastax's free, self-paced online data modeling course (DS220)</a>.</p><hr /><p><a href=\"https://www.datastax.com/\">DataStax</a> has many ways for you to advance in your career and knowledge. \n</p><p>You can take <a href=\"https://academy.datastax.com/user/register?destination=home&amp;utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_register\" target=\"_self\" title=\"academy.datastax.com\">free classes</a>, <a href=\"https://academy.datastax.com/certifications?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_certifications\" target=\"_self\" title=\"academy.datastax.com/certifications\">get certified</a>, or read <a href=\"https://www.datastax.com/dbas-guide-to-nosql\" target=\"_self\" title=\"dbas-guide-to-nosql\">one of our many white papers</a>.\n</p><p><a href=\"https://academy.datastax.com/user/register?destination=home&amp;utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_register\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"academy.datastax.com\">register for classes</a>\n</p><p><a href=\"https://academy.datastax.com/certifications?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_certifications\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"academy.datastax.com/certifications\">get certified</a>\n</p><p><a href=\"http://www.datastax.com/dbas-guide-to-nosql?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_dbasguidetonosql\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"dbas-guide-to-nosql\">DBA's Guide to NoSQL</a>\n</p><br class=\"clear\" /><div id=\"mto_newsletter_121316_Css\"><p>Subscribe for newsletter:</p><br /></div></article></section>",
        "created_at": "2018-10-05T11:13:35+0000",
        "updated_at": "2018-10-05T11:13:43+0000",
        "published_at": "2012-03-28T15:32:19+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 7,
        "domain_name": "www.datastax.com",
        "preview_picture": "https://www.datastax.com/wp-content/uploads/2012/03/index_pattern.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12323"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1233,
            "label": "data.modeling",
            "slug": "data-modeling"
          }
        ],
        "is_public": false,
        "id": 12322,
        "uid": null,
        "title": "Understanding the Cassandra Data Model from a SQL Perspective",
        "url": "http://www.rubyscale.com/post/143067472270/understanding-the-cassandra-data-model-from-a-sql?is_related_post=1",
        "content": "RubyScale — Understanding the Cassandra Data Model from a SQL...<div class=\"app-nag\"><p>&#13;\n                &#13;\n                <a class=\"app-nag-large-button app-nag-app-store-link button blue\">Sounds perfect</a>&#13;\n                <a class=\"app-nag-large-button app-nag-app-store-deny\">Wahhhh, I don’t wanna</a>&#13;</p></div>&#13;\n&#13;\n        <section id=\"page\"><section id=\"posts\" class=\"content clearfix   avatar-hidden show-nav\"><div class=\"container\"><div class=\"main\"><article class=\"text not-page post-143067472270  active exposed\" data-post-id=\"143067472270\"><div class=\"post-wrapper clearfix\"><section class=\"post\"><div class=\"post-content\"><div class=\"body-text\"><p>If you come from the SQL world, sometimes it can be difficult to understand the <a href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Fwiki.apache.org%2Fcassandra%2FDataModel&amp;t=ZDgyZTBmNzNjMDQzZDJmNmFlOGY2YmJhMzkxOTBiNzJlNjQyMTJjMyxPUDZGYU03eA%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067472270%2Funderstanding-the-cassandra-data-model-from-a-sql&amp;m=1\" target=\"_blank\">Cassandra Data Model</a> and all that it implies in terms of management and scalability. For this post, we’re going to go backwards. Instead of writing an example application using Cassandra to understand it, I’ll describe implementing Cassandra on a traditional SQL database and what that would look like.</p><p><strong>1. Mapping a Column Family to SQL Tables</strong></p><p>In Cassandra, a Column Family has any number of rows, and each row has N column names and values. For this example, let’s assume that in Cassandra we have a Users Column Family with uuids as the row key and column name/value pairs as attributes such as username, password, email, etc. If we had 10000 such users, then in SQL we’d have 10000 tables that looked like the following where 91b56770-bf40-11df-969c-b2b36bca998e was the actual uuid of a user:</p><p><code>CREATE TABLE users-91b56770-bf40-11df-969c-b2b36bca998e (column_name varchar, column_value varchar) UNIQUE column_name PRIMARY KEY column_name;</code></p><p><code>INSERT INTO </code><code>users-91b56770-bf40-11df-969c-b2b36bca998e VALUES ('username', 'joe'), ('password', 'weak'), ('email', 'joe@example.com');</code></p><ul><li><strong>10000 tables!</strong> That’s right, 10000 tables. In SQL, that many tables would be absurd but Cassandra is designed to scale that way and lots of rows are encouraged as it helps load balancing and scalability. In fact, millions and billions of rows are the norm. You can think of Cassandra as a system that distributes these tables over many nodes with some consistency and replication guarantees.</li><li><strong>You said N rows, but then said a table is on a single node. You don’t really mean N do you?</strong> No, not really. A row has to fit on a single node just like a table has to fit on a single machine for most SQL implementations. For older versions of Cassandra, a row also had to fit in RAM.</li><li><strong>It looks like I can only query one user at a time and I already have to know his UUID. Is that true?</strong> In SQL terms, Cassandra can perform table ranges and row slices on table ranges. In Cassandra terms, you can get a slice of keys and a slice of columns within those keys. This is what is meant by ‘column-oriented key-value store’. One thing you don’t get to do is join. Cassandra does not join. <em>Cassandra does not join.</em> If you want to join, you do it in your application.</li><li><strong>What is a Column Family for then? Just a table prefix?</strong> A Column Family has a number of settings that go with it that alter it’s behavior. There are cache settings for the keys (the UUIDs in this example), cache settings for the entire rows (the entire table in this example), and most importantly, sorting. In Cassandra there is no OFFSET, only LIMIT and the equivalent of BETWEEN. In this example, the column names are just strings but they could also be integers or timestamps and they are always stored in sort order. One Column Family might have timestamp-sorted data where you query things by time slice and another might be address book data where you query things in alphabetical order. The only sorting you get to do after the fact is reversing a particular slice. </li><li><strong>What if I want to sort by column value and by column name?</strong> Then you have two Column Families, one sorted by whatever datatype your column names are, and one sorted by whatever your column values are.</li><li><strong>So if I wanted to find the user who had a particular email address, how would I do that? I don’t know what table to query!</strong> Exactly! If you want to answer that question, you’ll have to make another Column Family that keys on e-mail address and points to the UUID of the user that has it, then you can query the relevant users table. Think about it this way .. in SQL, you can CREATE INDEX for things you want to query and in Cassandra, you have to manually maintain that index. (Secondary indexes are present in 0.7+, but I’ll cover that another time.)</li><li><strong>In SQL, I can still do a full table scan to pull out the information I’m looking for. Why can’t Cassandra do that?</strong> You can iterate with Cassandra and functionally get the same result, but I wouldn’t if I were you.</li><li><strong>But if I have two Column Families with the same data sorted differently, that’s denormalized. I heard that was bad.</strong> Denormalization is the norm with Cassandra. This is a complicated topic but in short, denormalization is a requirement for linear horizontal scaling and many people were already doing this in their SQL databases to avoid joins anyway.</li><li><strong>Wait, there is no OFFSET? How do I paginate with cassandra? </strong>Awkwardly. You have to start at the beginning, get N items, then on the next page you’d have to start at the last of the previous items and get N+1, throwing away the first one, etc. Alternately, you can create another Column Family which caches pages but in general, pagination is not what Cassandra was designed for.</li></ul><p><strong>2. Mapping a Super Column Family to SQL tables</strong></p><p>A Super Column Family is exactly like a Column Family, except you get one more column in your table. For this example, we’ll make an address book.</p><p><code>CREATE TABLE addressbooks-91b56770-bf40-11df-969c-b2b36bca998e (super_column_name, column_name varchar, column_value varchar) UNIQUE (super_column_name, column_name) PRIMARY KEY super_column_name;</code></p><p><code>INSERT INTO </code><code>addressbooks-91b56770-bf40-11df-969c-b2b36bca998e VALUES ('bob', 'street', '1313 Mockingbird Lane'), ('bob', 'city', 'Chicago'), ('bob', 'state', 'IL');</code></p><p><code>INSERT INTO </code><code>addressbooks-91b56770-bf40-11df-969c-b2b36bca998e VALUES ('alice', 'street', '123 Foo St.'), ('alice', 'city', 'Kona'), ('alice', 'state', 'HI');</code></p><ul><li><strong>Do the column_names have to be the same for each Super Column?</strong> No.</li><li><strong>Can the super_column_name and column_name be sorted differently?</strong> Yes.</li><li><strong>Does the entire table still  have to fit on one node or does it split out the Super Columns?</strong> The entire row still has to fit on one node.</li><li><strong>Is there a limit to how many Columns can be in a Super Column?</strong> Other than the space limitation already mentioned, theoretically, no. There is one gigantic caveat: <em>Accessing a single Column in a Column Family is efficient and fast. Accessing a single Column in a Super Column ('city’ or 'state’ in this case) requires deserialization of the entire Super Column.</em> For this example, this means that to get alice -&gt; state, Cassandra has to load everything in the alice super column. For an addressbook, this doesn’t really matter but if you had 1000 columns and a read-heavy load, it could heavily impact performance since you’d have to deserialize all 1000 columns just to get one of them.</li><li><strong>It sounds like Super Column Families can be dangerous if I’m not careful.</strong> Exactly! They were designed specifically for inverted indexing. If you aren’t doing that or don’t know what that is, don’t use them.</li></ul><p><strong>3. Transactions and/or Rollbacks</strong></p><p>Cassandra does neither transactions nor rollbacks. I you want those, some other thing such as <a href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Fcode.google.com%2Fp%2Fcages%2F&amp;t=NmNhNjg1ZTYyNGQxOWZhNjhmY2VlNWFjYzQxMzJjNWFmYTliNWFiYSxPUDZGYU03eA%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067472270%2Funderstanding-the-cassandra-data-model-from-a-sql&amp;m=1\" target=\"_blank\">Cages/ZooKeeper</a> has to do that.</p><p><strong>Next Post</strong></p><p>To learn more about how these tables would be replicated and queried, continue on to the next post, <a href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Fblog.insidesystems.net%2Fcassandra-data-model-from-an-sql-perspective&amp;t=MDIzNDU5YjQ1MGQxZTYxZTBiNjA5ODM3MjQwOThjYjdiYjZmY2Y5NCxPUDZGYU03eA%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067472270%2Funderstanding-the-cassandra-data-model-from-a-sql&amp;m=1\" target=\"_self\">Replication Factor and Consistency Levels</a>.</p></div></div></section><section class=\"panel\"></section></div></article><section class=\"related-posts-wrapper\"><div class=\"related-posts\"><article class=\"text not-page post-143067472490  active exposed\" data-post-id=\"143067472490\"><div class=\"post-wrapper clearfix\"><section class=\"post\"><div class=\"post-content\"><div class=\"body-text\"><p>If you read the last post, <a href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Fblog.insidesystems.net%2Funderstanding-the-cassandra-data-model-from-a&amp;t=YzIwN2MxNjEzZTQ1YzE4YzU5NGUxNjcxMDk3Yjk0NjBkZTlmZDY4YyxKVWVkQW91VQ%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067472490%2Fcassandra-data-model-from-an-sql-perspective&amp;m=1\" target=\"_blank\">Understanding the Cassandra Data Model from a SQL Perspective</a>, then you already have a decent handle on what a column-oriented key/value store is. The next thing that tends to confuse new users of Cassandra is the notion of <strong>Consistency Level</strong> and <strong>Replication Factor</strong>. This post doesn’t really have a lot to do with SQL in particular, it’s more of a discussion about how tables would be replicated across multiple servers and how you read/write from those tables.</p><p><strong>1. Replication Factor</strong></p><p>In the previous post, we described how every single Cassandra Row would be a separate table in SQL with an arbitrary number of rows. The <strong>Replication Factor</strong> (aka ‘RF’) determines <em>how many nodes have a copy of that table</em>. If you have 10 nodes and your RF=3, then every single table will exist on 3 nodes. Table users-91b56770-bf40-11df-969c-b2b36bca998e might exist on nodes 2 through 4 and users-28b56770-b410-15ef-968c-b2c36d511e78 might exist on nodes 6 through 8.</p><ul><li><strong>Can the RF be any number?</strong> Anything one or higher works but make sure it’s not higher than the actual number of nodes you have. This may be fixed by now but it used to be handled ungracefully. In general it should also be an odd number &gt;=3 so that quorum is a useful consistency level (for more details on this, keep reading).</li><li><strong>How do I know which node to query to get my data if it could be anywhere? Do I have to know where to write it?</strong> Each node acts as a storage proxy for every other node. If you query node 5 for data that is on nodes 8-10, it knows to ask one or more of those nodes for the answer. This is also true of writes.</li><li><strong>What if a node goes down? How do I synchronize them? What if they have different values?</strong> This will be covered in the next section, <strong>Consistency Levels</strong>.</li></ul><p><strong>2. Consistency Level</strong></p><p>Consistency in Cassandra is Eventual, which is to say that the N nodes responsible for a particular table could have a slightly different opinion of what it’s contents are. That’s ok! When reading from or writing to a Cassandra cluster, you can tune the tolerance for that sort of tomfoolery by reading or writing at a varying <strong>Consistency Level</strong> (aka 'CL’). Remember that each node acts as a proxy for every other node so you don’t have to worry about which node you are interacting with (unless you are really trying to optimize network activity but in general, this is unnecessary).</p><p>CL::ALL will not return success unless it has successfully written the same value to every node responsible for holding whatever you are writing. Using CL::ALL, you can guarantee that whatever you are writing is the same on all nodes when success is returned. There is of course a performance penalty for this, but in return you get very strong Consistency. Reading at CL::ALL functions in a similar way .. if one of the nodes does not respond, it will return an error since it cannot guarantee that all of the nodes are consistent for the value in question.</p><p>CL::QUORUM only requires a majority of the nodes responsible for a table to respond instead of all of them. This allows for one or more nodes to be down or otherwise unavailable and still have your read/write return. If your RF=3, then only 2 nodes responsible for a table need to be online in order to reliably manipulate the data in that table. Note that if RF=1 or RF=2 then there is no meaningful difference between CL::ALL and CL::QUORUM so if you want the benefits of CL::QUORUM, make sure your RF&gt;=3.</p><p>CL::ONE requires that only one of the nodes responsible for a table respond. This makes reads and writes fast, but it also means that depending on what else is reading and writing, it’s possible that they could briefly give conflicting answers. This is a fine tradeoff for speed in many applications, but not all. For example, recording votes or website hits where the outside possibility of a few going missing on machine failure is probably fine. Recording a financial transaction of some sort, probably not so much.</p><p>CL::ANY is only used for writing, not reading. CL::ANY means that as soon as a write is received by <em>any</em> node, the call returns success. This occurs when your client might be connecting to node 5 but the nodes responsible for it are 6-8. The difference between CL::ONE and CL::ANY is that with CL::ANY, as soon as node 5 receives the write, it returns success (but nodes 6-8 could be down or whatever). CL::ONE means that if you write to node 5, either 6, 7, or 8 have to return success before node 5 returns success.</p><ul><li><strong>OK .. so if I use CL::ALL and it fails, does that mean that my write failed?</strong> Not necessarily! It may have succeeded on two nodes and failed on the third which means that <em>eventually</em> it will be propagated to the third, but the <em>required guarantee</em> was not met.</li><li><strong>Uh .. ok. So if that happens, how do I know that any of my writes succeeded?</strong> You don’t! At least not yet. The error doesn’t indicate percentage of success, just failure. Not great, I know.</li><li><strong>I have multiple datacenters. How do I tell Cassandra to put some data in each?</strong> There are various strategies and snitches that tell Cassandra where to put things and they are constantly in flux. You can also write your own if you have specific requirements. For more information, see the <a href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Fwiki.apache.org%2Fcassandra%2F&amp;t=N2QyNjI0YzA3Nzk3ZmExMDZkZTM1MTU0MGNmZjVkYjZmYzgwNjNhNSxKVWVkQW91VQ%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067472490%2Fcassandra-data-model-from-an-sql-perspective&amp;m=1\" target=\"_blank\">Cassandra Wiki</a>.</li><li><strong>What happens if I write at CL::ANY and that node explodes before it can send data to where it belongs?</strong> Congratulations, you just lost data. If you can’t tolerate that, don’t use CL::ANY.</li><li><strong>What happens if all 3 nodes are up, but have different values for something for some reason and I read at CL::ALL. Does it give me the finger or what?</strong> Cassandra performs 'read repair’. That is, upon reading, if you are using a CL that requires more than one node and they disagree on the value, Cassandra will compare the values and use whichever one has the latest timestamp. It will also write that value back to the node which had an outdated value hence 'repairing it’.</li><li><strong>What if a node goes offline for a whole day, then comes back and has all sorts of wonky outdated data. Is there a way to fix it all at once instead of on read? That sounds expensive and slow to do all ad-hoc.</strong> Indeed it is! You can initiate a complete 'repair’ of a node after a failure like that using nodetool.</li></ul></div></div></section><section class=\"panel\"></section></div></article><article class=\"text not-page post-143067473000  active exposed\" data-post-id=\"143067473000\"><div class=\"post-wrapper clearfix\"><section class=\"post\"><div class=\"post-content\"><div class=\"body-text\"><p>Cassandra-cql has been updated for the latest version of cassandra. It supports all of the datatypes and has proper encoding support for ruby 1.9. It is also now published as a gem so a simple ‘gem install cassandra-cql’ should get you ready to roll with Ruby and Cassandra.</p><p>The official location for the code is now <a href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Fcode.google.com%2Fa%2Fapache-extras.org%2Fp%2Fcassandra-ruby%2F&amp;t=NjcyMGI3NDQ3OTI2NGM2OTlkM2U2MTIzNjA4MzA2NTUwNzI5ZDYyMyx6cDFhZkhoUQ%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067473000%2Fcassandra-cql-ruby-gem-for-cassandra-10&amp;m=1\" title=\"Google Code\" target=\"_blank\">Google Code</a> but I also use the <a href=\"https://t.umblr.com/redirect?z=https%3A%2F%2Fgithub.com%2Fkreynolds%2Fcassandra-cql&amp;t=YjdiMzVhZWE1NzZiNWU0YTkyYWE0YjFlYTljYWE5ZjVjZmY3YWY5Nix6cDFhZkhoUQ%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067473000%2Fcassandra-cql-ruby-gem-for-cassandra-10&amp;m=1\" title=\"Cassandra-CQL github repo\" target=\"_blank\">Cassandra-CQL github repo</a> if you want to participate that way. For a quick introduction on usage, check out the <a href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Fcode.google.com%2Fa%2Fapache-extras.org%2Fp%2Fcassandra-ruby%2Fwiki%2FGettingStarted&amp;t=NTFlYzgwNWEwNWY3YzkwMGExYjFiMjgyOGMzYjVjZmQ4MmRkOTliMCx6cDFhZkhoUQ%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067473000%2Fcassandra-cql-ruby-gem-for-cassandra-10&amp;m=1\" title=\"Google Code Wiki\" target=\"_blank\">Google Code Wiki</a>.</p><p>Enjoy!</p></div></div></section><section class=\"panel\"></section></div></article><article class=\"text not-page post-143067472090  active exposed\" data-post-id=\"143067472090\"><div class=\"post-wrapper clearfix\"><section class=\"post\"><div class=\"post-content\"><div class=\"body-text\"><p>Our recently-released <a href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Fblog.insidesystems.net%2Fusing-cassandra-with-paperclip-on-rails&amp;t=NGM3YjQxYzdjYzkyZGM4NWIyMDExMTczMzFmYTc1ZTY5ZmY4MWJiYiwxd3h1dUV5Mg%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067472090%2Fpaperclip-cassandra-gets-consistent&amp;m=1\" target=\"_self\">Cassandra storage plugin for paperclip</a> was lacking one thing: the ability to specify read/write consistency levels. Well, now you can just by adding the following options to your has_attachment specification (these are the defaults, btw).</p><div class=\"CodeRay\"><div class=\"code\"><pre>:read_consistency =&gt; Cassandra::Consistency::ONE\n:write_consistency =&gt; Cassandra::Consistency::QUORUM</pre></div></div><p>Enjoy!</p></div></div></section><section class=\"panel\"></section></div></article><article class=\"text not-page post-143067472805  active exposed\" data-post-id=\"143067472805\"><div class=\"post-wrapper clearfix\"><section class=\"post\"><div class=\"post-content\"><div class=\"body-text\"><h3>Cassandra Query Language (CQL)</h3><a href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Fcassandra.apache.org&amp;t=OWIxNjZhMjUzYTc5OWMzNmJlOTE0ZTc3ZDVhN2E5NjM2NmExNmQ5MixtUFpnbXlCTA%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067472805%2Fcassandra-cql-a-ruby-cql-client-for-cassandra&amp;m=1\" target=\"_blank\"><img class=\"alignright\" title=\"Apache Cassandra Project\" src=\"http://cassandra.apache.org/media/img/cassandra_logo.png\" alt=\"\" width=\"350\" height=\"70\" /></a>Cassandra originally went with a Thrift RPC-based API as a way to provide a common denominator that more idiomatic clients could build upon independently. However, this worked poorly in practice: raw Thrift is too low-level to use productively, and keeping pace with new API methods to support (for example) indexes in 0.7 or distributed counters in 0.8 is too much for many maintainers.\nCQL, the Cassandra Query Language, addresses this by pushing all implementation details to the server; all the client has to know for any operation is how to interpret “resultset” objects. So adding a feature like counters just requires teaching the CQL parser to understand “column + N” notation; no client-side changes are necessary.\n(CQL Specification: <a title=\"CQL Reference\" href=\"https://t.umblr.com/redirect?z=https%3A%2F%2Fgithub.com%2Fapache%2Fcassandra%2Fblob%2Ftrunk%2Fdoc%2Fcql%2FCQL.textile&amp;t=ZmI4YTRhNWZmZTVkZGJmNDZmZWM5MDM0Y2Q4Y2Q3MTk4ZDg5NWE0ZixtUFpnbXlCTA%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067472805%2Fcassandra-cql-a-ruby-cql-client-for-cassandra&amp;m=1\" target=\"_blank\">github.com/apache/cassandra/blob/trunk/doc/cql/CQL.textile</a>)<h3>Cassandra-CQL For Ruby</h3>Cassandra-CQL implements a DBI-like interface on top of CQL in Ruby that should be familiar to anybody who has worked with a traditional RDBMS before. It is not yet released as a gem as it will shortly be included in Apache Extras and distributed from there. To get started developing with it now, just clone it from Github and build/install the gem locally with ‘rake install’.\n(Github:<a title=\"Cassandra-CQL\" href=\"https://t.umblr.com/redirect?z=https%3A%2F%2Fgithub.com%2Fkreynolds%2Fcassandra-cql&amp;t=OTk0ZjI0NDU4NTlmZmIyODc0MTlkOTM5ZmU4NzlhNTMzZGJjN2RhMixtUFpnbXlCTA%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067472805%2Fcassandra-cql-a-ruby-cql-client-for-cassandra&amp;m=1\" target=\"_blank\">http://github.com/kreynolds/cassandra-cql\n</a></div></div></section><section class=\"panel\"></section></div></article><article class=\"text not-page post-143067474645  active exposed\" data-post-id=\"143067474645\"><div class=\"post-wrapper clearfix\"><section class=\"post\"><div class=\"post-content\"><div class=\"body-text\"><p>It’s been a while since the last release of the <a title=\"Cassandra-CQL\" href=\"https://t.umblr.com/redirect?z=https%3A%2F%2Fgithub.com%2Fkreynolds%2Fcassandra-cql&amp;t=YWM2YTJkYzBhN2JjNWRkZThkMDRiNDMwN2ViMDk4MWRmYjg2NTU1MCxLWkV3MVdoNg%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067474645%2Fcassandra-cql-110&amp;m=1\" target=\"_blank\">cassandra-cql gem</a>, and this version fixes a few long-standing encoding and data-access bugs as well as support for <a title=\"Datastax CQL Reference\" href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Fwww.datastax.com%2Fdocs%2F1.1%2Freferences%2Fcql%2Findex&amp;t=NDc5NTczOGFhMGQ3YjBmNDAwYjNhOGYxYjJjNjE1NGNmYjIxYjUxZSxLWkV3MVdoNg%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067474645%2Fcassandra-cql-110&amp;m=1\" target=\"_blank\">CQL3</a>. The driver now also uses <a title=\"Cassandra-CQL in Travis\" href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Ftravis-ci.org%2F%23%21%2Fkreynolds%2Fcassandra-cql&amp;t=OTgzY2M0MzVlYzRlMGMxYjE4MjgwZTdhYTM5MTdkMTAxNDBjMWRmYyxLWkV3MVdoNg%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067474645%2Fcassandra-cql-110&amp;m=1\" target=\"_blank\">Travis</a> for continuous integration.</p><p>The largest improvement is the ability to cast single columns at a time. One of the major pain points previously was that if a column in Cassandra had existing data and it’s validation changed to make that data invalid (empty string in an Integer-validated column for instance), none of the data in that row would be accessible as the entire row was casted at once. Each column is now individually casted and cached and if there is invalid data in a validated column, it will raise a CastException that contains a useful description of the problem as well as the bytes that raised the exception.</p><p>Another important bug fix is for character encoding in Ruby 1.9. The previous version incorrectly used ASCII-8BIT on data retrieved from ASCII-validated columns which when stored back into Cassandra, would encode it as binary. A simple change to US-ASCII in the casting has fixed that.</p><p><code>gem install cassandra-cql</code></p><p>Enjoy!</p></div></div></section><section class=\"panel\"></section></div></article><article class=\"text not-page post-143067470585  active exposed\" data-post-id=\"143067470585\"><div class=\"post-wrapper clearfix\"><section class=\"post\"><div class=\"post-content\"><div class=\"body-text\"><pre><em>editors note: as of 7 Mar 2012, this info is still current and correct.</em></pre><br />One of the most common use cases for Cassandra is tracking time-series data. Server log files, usage, sensor data, SIP packets, stuff that changes over time. For the most part this is a straight forward process but given that Cassandra has real-world limitations on how much data can or should be in a row, there are a few details to consider.<strong>Basic Inserts and Queries</strong>The most basic and intuitive way to go about storing time-series data is to use a column family that has TimeUUID columns (or Long if you know that no two entries willhappen at the same timestamp), use the name of the thing you are monitoring as the row_key (server1-load for example), column_name as the timestamp, and the column_value would be the actual value of the thing (0.75 for example):<ul><li>Inserting data – {:key =&gt; ‘server1-load’, :column_name =&gt; TimeUUID(now), :column_value =&gt; 0.75}</li></ul>Using this method, one uses a column_slice to get the data in question:<ul><li>Load at Time X –  {:key =&gt; 'server1-load’, :start =&gt; TimeUUID(X), :count =&gt; 1}</li><li>Load between X and Y – {:key =&gt; 'server1-load’, :start =&gt; TimeUUID(X), :end =&gt; TimeUUID(Y)}</li></ul>This works well enough for a while, but over time, this row will get very large. If you are storing sensor data that updates hundreds of times per second, that row will quickly become gigantic and unusable. The answer to that is to shard the data up in some way. To accomplish this, the application has to have a little more intelligence about how to store and query the information.\nFor this example, we’ll pick a day as our shard interval (details on picking the right shard interval later). The only change we make when we insert our data is to add a day to the row-key:<ul><li>Inserting data – {:key =&gt; 'server1-load-20110306’, :column_name =&gt; TimeUUID(now), :column_value =&gt; 0.75}</li></ul>Using this method, one still uses a column slice, but you have to then also specify a different row_key depending on what you are querying:<ul><li>Load at Time X – {:key =&gt; 'server1-load-&lt;X.strftime&gt;’, :start =&gt; TimeUUID(X), :count =&gt; 1}</li><li>Load between Time X and Y (if X and Y are on the same day) – {:key =&gt; 'server1-load-&lt;X.strftime&gt;’, :start =&gt; TimeUUID(X), :end =&gt; TimeUUID(Y)}</li></ul>Now what to do if X and Y are not on the same day? No worries! You can use a multi-get to fetch more than one key at a time (or issue parallel gets for maximum performance). If your X and Y span two days, you just need to generate keys for those two days and issue them in a multiget:<ul><li>Load between Time X and Y – {:key =&gt; ['server1-load-&lt;X.strftime&gt;’, 'server1-load-&lt;Y.strftime&gt;’], :start =&gt; TimeUUID(X), :end =&gt; TimeUUID(Y)}</li></ul>Then in your application, you will need to aggregate/concatenate/iterate those two rows however you see fit. If your data spans 3 or more days, you’ll need to also generate every key in between. Don’t be tempted to use the Order-Preserving Partitioner here, it won’t save you that much typing and it’ll will make managing your cluster much more difficult.<strong>Calculating Shard Size</strong>Now on the topic of determining your shard interval .. that’s a complicated topic that is often application dependent but the single biggest issue is to make sure your rows don’t get too big. The better you are at the ops side of Cassandra, the larger you can let your rows get but if I have to tell you that, you should keep them small. A quick ballpark formula for determining shard size is as follows (yes rcoli, it ignores overhead):<ul><li>shard_size_in_seconds / update_frequency * avg_data_size_in_bytes == row_size_in_bytes</li></ul>Set your shard size so that the row_size doesn’t get much larger than 10MB (this number can move around for many reasons but I’d consider it safe). For example, if you are storing hits on a website that gets 10 hits/sec and each entry is about 200B, then we have:<ul><li>Daily – 86400 / (1 / 10) * 200 = 172800000 (165MB)</li><li>Hourly – 3600 / (1 / 10) * 200 = 7200000 (6.9MB)</li></ul>Looks like sharding this up on hours hits our target row size. Of course you can use any shard size you want, 6 hours, 2 hours, seconds, months, whatever. If you code up your application properly, it should be easy to adjust. Even if you decide to change your shard partway through the life if your application, you just have to know that before a certain point, use keys with one format, and after a certain point, use another, it’s that simple.<strong>Indexing and Aggregation</strong>Indexing and aggregation of time-series data is a more complicated topic as they are highly application dependent. Various new and upcoming features of Cassandra also change the best practices for how things like aggregation are done so I won’t go into that. For more details, hit #cassandra on irc.freenode and ask around. There is usually somebody there to help.</div></div></section><section class=\"panel\"></section></div></article><article class=\"text not-page post-143067471930  active exposed\" data-post-id=\"143067471930\"><div class=\"post-wrapper clearfix\"><section class=\"post\"><div class=\"post-content\"><div class=\"body-text\"><p>These have already been sent to the port maintainers so they should find their way into the ports collection soon enough but if you can’t wait, here are some scripts for pfstat (pfstatd in particular) and cassandra.</p><p>/usr/local/etc/rc.d/pfstatd</p><div class=\"CodeRay\"><div class=\"code\"><pre>#!/bin/sh\n# PROVIDE: pfstatd\n# REQUIRE: LOGIN\n# BEFORE:  securelevel\n# KEYWORD: shutdown\n# Add the following lines to /etc/rc.conf to enable `pfstatd':\n#\n# pfstatd_enable=\"YES\"\n# pfstatd_flags=\"\"\n#\n# See pfstat(1) for pfstatd_flags\n#\n. /etc/rc.subr\nname=\"pfstatd\"\nrcvar=`set_rcvar`\ncommand=\"/usr/local/bin/pfstatd\"\n# read configuration and set defaults\nload_rc_config \"$name\"\n: ${pfstatd_enable=\"NO\"}\nrun_rc_command \"$1\"</pre></div></div><p>/usr/local/etc/rc.d/cassandra</p><div class=\"CodeRay\"><div class=\"code\"><pre>#!/bin/sh\n# PROVIDE: cassandra\n# REQUIRE: LOGIN\n# BEFORE:  securelevel\n# KEYWORD: shutdown\n# Add the following lines to /etc/rc.conf to enable `cassandra':\n#\n# cassandra_enable=\"YES\"\n# cassandra_flags=\"\"\n#\n# See cassandra(1) for cassandra_flags\n#\n. /etc/rc.subr\nname=\"cassandra\"\nprocname=\"java\"\nrcvar=`set_rcvar`\ncommand=\"/usr/local/share/cassandra/bin/cassandra\"\npidfile=\"/var/run/$name.pid\"\ncommand_args=\"-p ${pidfile}\"\n# read configuration and set defaults\nload_rc_config \"$name\"\n: ${cassandra_enable=\"NO\"}\nrun_rc_command \"$1\"</pre></div></div></div></div></section><section class=\"panel\"></section></div></article><article class=\"text not-page post-143067472625  active exposed\" data-post-id=\"143067472625\"><div class=\"post-wrapper clearfix\"><section class=\"post\"><div class=\"post-content\"><div class=\"body-text\"><p>I’m sure you were expecting another post with a brief topical analysis of why Facebook chose HBase instead of Cassandra with some similarly topical and mostly incorrect summary of how those two systems work and why one is better than the other one.</p><p>Sorry to disappoint, but rather than pour more fuel on what is already a pointless fire, I’m going to suggest instead that you look at what your <strong>application</strong> and your <strong>business</strong> requires for success and can operationally support. That might mean that you need transactional rollback so Cassandra won’t work straight up, or that you already have HDFS for other jobs so HBase is easier to support than adding on an entirely new technology.</p><p>Regardless, please take the time to <strong>understand the systems</strong> and realize that the choices one company makes aren’t necessarily the same choices you should make for many reasons, technical and otherwise.</p></div></div></section><section class=\"panel\"></section></div></article><article class=\"text not-page post-143067473965  active exposed\" data-post-id=\"143067473965\"><div class=\"post-wrapper clearfix\"><section class=\"post\"><div class=\"post-content\"><div class=\"body-text\"><p>I recently had a conversation in #cassandra about the Data Model that I thought might be useful to try to distill into a few lines. These few lines ignore all of the implementation details to make it work in practice but it gives you the starting point. It looks a little something like this …\n</p><ol><li><strong><em>Keyspace</em></strong> - What method should I use to turn my application key into a <em>Row Key</em>?</li><li><strong><em>Row Key</em></strong> - Which <em>Node</em> is my value on?</li><li><strong><em>Column Family</em></strong> - Which file on the <em>node</em> is my value in?</li><li><strong><em>Column Name</em></strong> - Which piece of the file on the <em>node</em> contains my value?</li><li><strong><em>Column Value</em></strong> - My value!</li></ol><p>Since the columns are already sorted in the file, getting a slice of them is very efficient, and this is what makes Cassandra a column-oriented database. It’s worth noting that with most Cassandra clients, the <em>Row Key</em> and the <em>Column Family</em> are almost always conceptually swapped because it’s more common to access multiple keys in the same column family than to access multiple column families with the same key. Given the above, let’s revisit some of the other concepts that are required to make it work in practice.</p><p><strong>Partitioner</strong></p><p>The partitioner determines how your application keys get turned into <em>Row Keys</em>. <em>RandomPartitioner</em> (which should be called HashedPartitioner in my opinion) takes an MD5 of the key and uses that as the <em>Row Key </em>and <em>ByteOrderedPartitioner</em> uses the key unmodified.</p><p><strong>Replication Factor and Replica Placement Strategy</strong></p><p><em>ReplicationFactor</em> determines how many nodes get a copy of a particular key. There is no master/slave thing going on here, it’s just how many nodes get a copy of a key. The <em>Strategy</em> determines which exact nodes get a copy of your key based on network topology or ring placement.</p><p><strong>Consistency Levels</strong></p><p>Most relevant when the <em>ReplicationFactor</em> &gt; 1, the <em>ConsistencyLevel</em> determines how many nodes have to successfully record a write or agree on the value of a read for the operation to be considered successful. There are tables of consistency levels and what exactly they mean in multiple places so I won’t repeat them here.</p><p><strong>Memtables, Commitlog, SSTables, and Compaction</strong></p><p>As writes come in to a <em>ColumnFamily</em>, they are simultaneously (don’t argue) stored in memory in a <em>memtable</em> and written out to disk in a <em>commitlog</em>. The <em>memtable</em> is periodically written out to disk in in column-order as an <em>sstable</em> for efficient slicing later. Over time, a column could exist in a <em>memtable</em> and/or multiple <em>sstables</em> at once so a timestamp is used to determine which one to use, highest one wins. The timestamp is supplied by the client and can be any integer. Over time, you get a number of <em>sstables</em> and a process called <em>compaction</em> combines them all and throws away the out of date stuff to save space. Back to the <em>commitlog</em>, that’s there so that if the node explodes while some things are only in a <em>memtable</em>, they get replayed when the node starts back up so no data is lost (durability). Since the <em>commitlog</em> is serial write-only, if you put it on a separate disk, you can accept writes really fast.</p><p><strong>Miscellaneous</strong></p><div><ul><li>You can ask any node anything, they all act as proxies for one another.</li><li>If your <em>ReplicationFactor</em> &gt; 1, then <em>ReadRepair</em> will propagate the most recent column value to the nodes responsible or it.</li><li>There are various operations in <em>nodetool</em> that let you move nodes, add nodes, clean out old data/keys, manually compact things, repair everything at once, get statistics about each node, etc.</li><li><em>Gossip</em> is used to propagate node status and information to other nodes.</li></ul><p>Cassandra works in a complicated problem space and there are many subtle operational and technical details that aren’t covered her, but this is the gist of it.</p></div></div></div></section><section class=\"panel\"></section></div></article><article class=\"text not-page post-143067475055  active exposed\" data-post-id=\"143067475055\"><div class=\"post-wrapper clearfix\"><section class=\"post\"><div class=\"post-content\"><div class=\"body-text\"><a href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Frubyscale.com%2F%3Fattachment_id%3D446&amp;t=Mjg2ODQwNzc2MDE2ZjVmY2RlNzE0YjNmZjk0MjZlM2RmM2JjOGE3NywwMzlLTURCTg%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067475055%2Fusing-ruby-with-mios-from-mi-casa-verde&amp;m=1\" rel=\"attachment wp-att-446\"><img class=\"alignright size-medium wp-image-446\" alt=\"Vera Lite\" src=\"http://rubyscale.com/wp-content/uploads/2013/01/Vera-Lite1-300x236.png\" width=\"300\" height=\"236\" /></a>Mi Casa Verde. You can read the specifications of the devices for yourself but I like them because they are inexpensive, have low-power requirements, are capable of interfacing with a number of different kind of home automation systems, and most importantly, have a documented JSON/XML API. Coincidentally, they also have a<a title=\"Developer Special Program\" href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Fwww.micasaverde.com%2Fdevelop-an-app.php&amp;t=YTE3NDg5NzRlZmNlYWUzMDNiN2Y3ZmMyNTdhZGQ0ZTQ5MWZiZGFlNiwwMzlLTURCTg%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067475055%2Fusing-ruby-with-mios-from-mi-casa-verde&amp;m=1\" target=\"_blank\">developer special program</a>and lack a ruby gem, so another rainy day project is born!\nThe operating system these devices run is called MiOS which is essentially Linux/ARM and lots of glue. Mi Casa Verde operates a free VPN service that allows you to operate them remotely, and there are a number of free/paid smartphone apps available (though none struck me as particularly special). All of the interactions with MiOS work on a job queue basis. When a job is submitted, it’s status must be polled for success/failure. The Ruby library embraces this and allows jobs to be run synchronously or asynchronously.\nSome snippets from the github page:<pre>mios = MiOS::Interface.new('http://192.168.15.1:3480')\nswitch = mios.devices[0]\nswitch.off! { |obj|\n  puts \"The #{obj.name} is now off\"\n}\nputs \"This will get printed once the switch is off and the block has been executed\"\nswitch.on!(true) { |obj|\n  puts \"The #{obj.name} is now on\"\n}\nputs \"This will output immediately\"\nsleep(5) # Sleep to wait for the thread to finish\n</pre>Every device is supported by manually issuing commands as listed<a title=\"Mi Casa Verde Luup variables and actions\" href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Fwiki.micasaverde.com%2Findex.php%2FLuup_UPnP_Variables_and_Actions&amp;t=YzlhZGZiNTJlY2ZkYTkzN2U2ZDZkN2E2NWE3ODZlODZhMmQ2YTE5NiwwMzlLTURCTg%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067475055%2Fusing-ruby-with-mios-from-mi-casa-verde&amp;m=1\" target=\"_blank\">in the wiki</a> and the devices that I currently use have nice wrappers around those for idiomatic usage (<a title=\"Example of Door Lock API\" href=\"https://t.umblr.com/redirect?z=https%3A%2F%2Fgithub.com%2Fkreynolds%2Fruby-mios%2Fblob%2Fmaster%2Flib%2Fmios%2Fservices%2Fdoor_lock1.rb&amp;t=OGFmZmQ4OWI0MjUyZGE2MDZjOTY0NjAwZGE5MmNhMGU2YzcxNjNjNywwMzlLTURCTg%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067475055%2Fusing-ruby-with-mios-from-mi-casa-verde&amp;m=1\" target=\"_blank\">example</a>).\nInstallation instructions and additional usage examples can be found on github under the<a title=\"Ruby MiOS\" href=\"https://t.umblr.com/redirect?z=https%3A%2F%2Fgithub.com%2Fkreynolds%2Fruby-mios&amp;t=YjQ0NTU4MjEzOGVhYTdjYWIwNTcyZDY0NTZmMTUzYTFhYjg3YjI5NSwwMzlLTURCTg%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067475055%2Fusing-ruby-with-mios-from-mi-casa-verde&amp;m=1\" target=\"_blank\">ruby-mios project</a>.\nEnjoy!</div></div></section><section class=\"panel\"></section></div></article></div><p>&#13;\n                                        &#13;\n                                        <a class=\"related-post-cta button bordered open-in-app\">See this in the app</a>&#13;\n                                        <a class=\"related-post-cta see-more button bordered\" href=\"http://www.rubyscale.com/\">Show more</a>&#13;</p></section></div></div></section><footer id=\"footer\" class=\"content clearfix\"></footer></section><noscript><noscript><noscript id=\"bootloader\" data-bootstrap=\"{&quot;Components&quot;:{&quot;PostActivity&quot;:[],&quot;NotificationPoller&quot;:{&quot;messaging_keys&quot;:[],&quot;token&quot;:&quot;c86f8a500596474a7ef5034081f694bd&quot;,&quot;inbox_unread&quot;:0},&quot;DesktopDashboardLogo&quot;:{&quot;animations&quot;:[[&quot;http:\\/\\/assets.tumblr.com\\/images\\/logo\\/hover-animations\\/1.png?_v=161861acded461bb6e995593a3bae835&quot;,&quot;http:\\/\\/assets.tumblr.com\\/images\\/logo\\/hover-animations\\/1@2x.png?_v=496a774637302a598c851381d00009b0&quot;]]},&quot;TumblelogIframe&quot;:{&quot;unified&quot;:true,&quot;variant&quot;:null,&quot;isCompact&quot;:false,&quot;tumblelogBundleSrc&quot;:&quot;http:\\/\\/assets.tumblr.com\\/client\\/prod\\/standalone\\/tumblelog\\/index.build.js?_v=d31f5c6ecc27fedacb700eee8f83161f&quot;,&quot;tumblelogName&quot;:&quot;rubyscalecom&quot;,&quot;isLoggedIn&quot;:false,&quot;isFriend&quot;:false,&quot;formKey&quot;:&quot;&quot;,&quot;canSubscribe&quot;:false,&quot;isSubscribed&quot;:false,&quot;tumblelogTitle&quot;:&quot;RubyScale&quot;,&quot;tumblelogAvatar&quot;:&quot;https:\\/\\/assets.tumblr.com\\/images\\/default_avatar\\/octahedron_closed_40.png&quot;,&quot;tumblelogHost&quot;:&quot;http:\\/\\/www.rubyscale.com&quot;,&quot;isOptica&quot;:true,&quot;isCustomTheme&quot;:false,&quot;themeHeaderImage&quot;:&quot;https:\\/\\/assets.tumblr.com\\/images\\/default_header\\/optica_pattern_07.png?_v=c52eb3e7fd4be5c98b8970b4c330d172&quot;,&quot;themeBackgroundColor&quot;:&quot;#FAFAFA&quot;,&quot;themeTitleColor&quot;:&quot;#444444&quot;,&quot;themeAccentColor&quot;:&quot;#529ECC&quot;,&quot;brag&quot;:false,&quot;adsEnabled&quot;:true,&quot;canShowAd&quot;:false,&quot;isPremium&quot;:false,&quot;showLrecAds&quot;:false,&quot;showStickyLrecBackfill&quot;:false,&quot;showGeminiAds&quot;:false,&quot;geminiSectionCodeDesktop&quot;:&quot;a10bca9c-0c5d-4a02-ab13-14ab8513d81d&quot;,&quot;geminiSectionCodeMobile&quot;:&quot;ced63809-b609-4aca-96a0-abc099acba6b&quot;,&quot;currentPageType&quot;:&quot;single_post&quot;,&quot;currentPage&quot;:&quot;1&quot;,&quot;searchQuery&quot;:&quot;&quot;,&quot;tag&quot;:&quot;&quot;,&quot;query&quot;:&quot;&quot;,&quot;chrono&quot;:false,&quot;postId&quot;:&quot;143067472270&quot;,&quot;src&quot;:&quot;http:\\/\\/www.rubyscale.com\\/post\\/143067472270\\/understanding-the-cassandra-data-model-from-a-sql?is_related_post=1&quot;,&quot;postUrl&quot;:&quot;&quot;,&quot;isBlocked&quot;:null,&quot;isAdmin&quot;:false,&quot;lookupButtonUrl&quot;:&quot;&quot;,&quot;showSpamButton&quot;:false,&quot;showRootPostButton&quot;:false,&quot;rootPostUrl&quot;:&quot;&quot;,&quot;showRadarPostButton&quot;:false,&quot;radarKeys&quot;:&quot;&quot;,&quot;isUniblocked&quot;:false,&quot;isNsfw&quot;:false,&quot;isAdult&quot;:false,&quot;isSpam&quot;:false,&quot;isPrimaryBlog&quot;:false,&quot;canEdit&quot;:false,&quot;canReblogSelf&quot;:false,&quot;showLikeButton&quot;:false,&quot;showReblogButton&quot;:false,&quot;reblogUrl&quot;:&quot;&quot;,&quot;showFanMailButton&quot;:false,&quot;showMessagingButton&quot;:false,&quot;loginCheckIframeSrc&quot;:&quot;http:\\/\\/assets.tumblr.com\\/assets\\/html\\/iframe\\/login_check.html?_v=3de94a184d600617102ddd5b48fb36e9&quot;,&quot;appInstallUrls&quot;:{&quot;android&quot;:&quot;https:\\/\\/play.google.com\\/store\\/apps\\/details?id=com.tumblr\\u0026referrer=utm_source%3Dtumblr%26utm_medium%3Diframe%26utm_campaign%3Dblog_network_floating_cta&quot;,&quot;ios&quot;:&quot;https:\\/\\/itunes.apple.com\\/app\\/apple-store\\/id305343404?pt=9029\\u0026ct=blog_network_floating_cta\\u0026mt=8&quot;},&quot;appOpenReferrer&quot;:&quot;tumblr_new_iframe&quot;,&quot;isShowSearch&quot;:true,&quot;supplyLogging&quot;:[],&quot;secondsSinceLastActivity&quot;:-1}},&quot;Flags&quot;:{&quot;doods&quot;:&quot;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&quot;},&quot;Context&quot;:{&quot;name&quot;:&quot;default&quot;,&quot;time&quot;:1538737980000,&quot;userinfo&quot;:{&quot;primary&quot;:&quot;&quot;,&quot;name&quot;:&quot;&quot;,&quot;channels&quot;:[]},&quot;hosts&quot;:{&quot;assets_host&quot;:&quot;http:\\/\\/assets.tumblr.com&quot;,&quot;secure_assets_host&quot;:&quot;https:\\/\\/assets.tumblr.com&quot;,&quot;www_host&quot;:&quot;http:\\/\\/www.tumblr.com&quot;,&quot;secure_www_host&quot;:&quot;https:\\/\\/www.tumblr.com&quot;,&quot;embed_host&quot;:&quot;https:\\/\\/embed.tumblr.com&quot;,&quot;safe_host&quot;:&quot;http:\\/\\/safe.txmblr.com&quot;,&quot;platform_host&quot;:&quot;http:\\/\\/platform.tumblr.com&quot;},&quot;language&quot;:&quot;en_US&quot;,&quot;language_simple&quot;:&quot;en&quot;,&quot;assets&quot;:&quot;http:\\/\\/assets.tumblr.com\\/client\\/prod\\/&quot;},&quot;Translations&quot;:{&quot;%1$sReport %2$s's post?%3$sIf it violates our community guidelines, we'll remove it.%4$s&quot;:&quot;%1$sReport %2$s's reblog?%3$sIf it violates our community guidelines, we'll remove it.%4$s&quot;,&quot;%1$sReport %2$s's reply?%3$sIf it violates our community guidelines, we'll remove it.%4$s&quot;:&quot;%1$sReport %2$s's reblog?%3$sIf it violates our community guidelines, we'll remove it.%4$s&quot;}}\"></noscript></noscript></noscript>",
        "created_at": "2018-10-05T11:13:01+0000",
        "updated_at": "2018-10-05T11:13:09+0000",
        "published_at": "2010-09-13T13:49:00+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 25,
        "domain_name": "www.rubyscale.com",
        "preview_picture": "http://assets.tumblr.com/images/og/fb_landscape_share.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12322"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1262,
            "label": "time.series",
            "slug": "time-series"
          }
        ],
        "is_public": false,
        "id": 12321,
        "uid": null,
        "title": "Basic Time Series with Cassandra",
        "url": "http://www.rubyscale.com/post/143067470585/basic-time-series-with-cassandra",
        "content": "RubyScale — Basic Time Series with Cassandra<div class=\"app-nag\"><p>&#13;\n                &#13;\n                <a class=\"app-nag-large-button app-nag-app-store-link button blue\">Sounds perfect</a>&#13;\n                <a class=\"app-nag-large-button app-nag-app-store-deny\">Wahhhh, I don’t wanna</a>&#13;</p></div>&#13;\n&#13;\n        <section id=\"page\"><section id=\"posts\" class=\"content clearfix   avatar-hidden show-nav\"><div class=\"container\"><div class=\"main\"><article class=\"text not-page post-143067470585  active exposed\" data-post-id=\"143067470585\"><div class=\"post-wrapper clearfix\"><section class=\"post\"><div class=\"post-content\"><div class=\"body-text\"><pre><em>editors note: as of 7 Mar 2012, this info is still current and correct.</em></pre><br />One of the most common use cases for Cassandra is tracking time-series data. Server log files, usage, sensor data, SIP packets, stuff that changes over time. For the most part this is a straight forward process but given that Cassandra has real-world limitations on how much data can or should be in a row, there are a few details to consider.<strong>Basic Inserts and Queries</strong>The most basic and intuitive way to go about storing time-series data is to use a column family that has TimeUUID columns (or Long if you know that no two entries willhappen at the same timestamp), use the name of the thing you are monitoring as the row_key (server1-load for example), column_name as the timestamp, and the column_value would be the actual value of the thing (0.75 for example):<ul><li>Inserting data – {:key =&gt; ‘server1-load’, :column_name =&gt; TimeUUID(now), :column_value =&gt; 0.75}</li></ul>Using this method, one uses a column_slice to get the data in question:<ul><li>Load at Time X –  {:key =&gt; 'server1-load’, :start =&gt; TimeUUID(X), :count =&gt; 1}</li><li>Load between X and Y – {:key =&gt; 'server1-load’, :start =&gt; TimeUUID(X), :end =&gt; TimeUUID(Y)}</li></ul>This works well enough for a while, but over time, this row will get very large. If you are storing sensor data that updates hundreds of times per second, that row will quickly become gigantic and unusable. The answer to that is to shard the data up in some way. To accomplish this, the application has to have a little more intelligence about how to store and query the information.\nFor this example, we’ll pick a day as our shard interval (details on picking the right shard interval later). The only change we make when we insert our data is to add a day to the row-key:<ul><li>Inserting data – {:key =&gt; 'server1-load-20110306’, :column_name =&gt; TimeUUID(now), :column_value =&gt; 0.75}</li></ul>Using this method, one still uses a column slice, but you have to then also specify a different row_key depending on what you are querying:<ul><li>Load at Time X – {:key =&gt; 'server1-load-&lt;X.strftime&gt;’, :start =&gt; TimeUUID(X), :count =&gt; 1}</li><li>Load between Time X and Y (if X and Y are on the same day) – {:key =&gt; 'server1-load-&lt;X.strftime&gt;’, :start =&gt; TimeUUID(X), :end =&gt; TimeUUID(Y)}</li></ul>Now what to do if X and Y are not on the same day? No worries! You can use a multi-get to fetch more than one key at a time (or issue parallel gets for maximum performance). If your X and Y span two days, you just need to generate keys for those two days and issue them in a multiget:<ul><li>Load between Time X and Y – {:key =&gt; ['server1-load-&lt;X.strftime&gt;’, 'server1-load-&lt;Y.strftime&gt;’], :start =&gt; TimeUUID(X), :end =&gt; TimeUUID(Y)}</li></ul>Then in your application, you will need to aggregate/concatenate/iterate those two rows however you see fit. If your data spans 3 or more days, you’ll need to also generate every key in between. Don’t be tempted to use the Order-Preserving Partitioner here, it won’t save you that much typing and it’ll will make managing your cluster much more difficult.<strong>Calculating Shard Size</strong>Now on the topic of determining your shard interval .. that’s a complicated topic that is often application dependent but the single biggest issue is to make sure your rows don’t get too big. The better you are at the ops side of Cassandra, the larger you can let your rows get but if I have to tell you that, you should keep them small. A quick ballpark formula for determining shard size is as follows (yes rcoli, it ignores overhead):<ul><li>shard_size_in_seconds / update_frequency * avg_data_size_in_bytes == row_size_in_bytes</li></ul>Set your shard size so that the row_size doesn’t get much larger than 10MB (this number can move around for many reasons but I’d consider it safe). For example, if you are storing hits on a website that gets 10 hits/sec and each entry is about 200B, then we have:<ul><li>Daily – 86400 / (1 / 10) * 200 = 172800000 (165MB)</li><li>Hourly – 3600 / (1 / 10) * 200 = 7200000 (6.9MB)</li></ul>Looks like sharding this up on hours hits our target row size. Of course you can use any shard size you want, 6 hours, 2 hours, seconds, months, whatever. If you code up your application properly, it should be easy to adjust. Even if you decide to change your shard partway through the life if your application, you just have to know that before a certain point, use keys with one format, and after a certain point, use another, it’s that simple.<strong>Indexing and Aggregation</strong>Indexing and aggregation of time-series data is a more complicated topic as they are highly application dependent. Various new and upcoming features of Cassandra also change the best practices for how things like aggregation are done so I won’t go into that. For more details, hit #cassandra on irc.freenode and ask around. There is usually somebody there to help.</div></div></section><section class=\"panel\"></section></div></article><section class=\"related-posts-wrapper\"><div class=\"related-posts\"><article class=\"text not-page post-143067473000  active exposed\" data-post-id=\"143067473000\"><div class=\"post-wrapper clearfix\"><section class=\"post\"><div class=\"post-content\"><div class=\"body-text\"><p>Cassandra-cql has been updated for the latest version of cassandra. It supports all of the datatypes and has proper encoding support for ruby 1.9. It is also now published as a gem so a simple ‘gem install cassandra-cql’ should get you ready to roll with Ruby and Cassandra.</p><p>The official location for the code is now <a href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Fcode.google.com%2Fa%2Fapache-extras.org%2Fp%2Fcassandra-ruby%2F&amp;t=NjcyMGI3NDQ3OTI2NGM2OTlkM2U2MTIzNjA4MzA2NTUwNzI5ZDYyMyx6cDFhZkhoUQ%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067473000%2Fcassandra-cql-ruby-gem-for-cassandra-10&amp;m=1\" title=\"Google Code\" target=\"_blank\">Google Code</a> but I also use the <a href=\"https://t.umblr.com/redirect?z=https%3A%2F%2Fgithub.com%2Fkreynolds%2Fcassandra-cql&amp;t=YjdiMzVhZWE1NzZiNWU0YTkyYWE0YjFlYTljYWE5ZjVjZmY3YWY5Nix6cDFhZkhoUQ%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067473000%2Fcassandra-cql-ruby-gem-for-cassandra-10&amp;m=1\" title=\"Cassandra-CQL github repo\" target=\"_blank\">Cassandra-CQL github repo</a> if you want to participate that way. For a quick introduction on usage, check out the <a href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Fcode.google.com%2Fa%2Fapache-extras.org%2Fp%2Fcassandra-ruby%2Fwiki%2FGettingStarted&amp;t=NTFlYzgwNWEwNWY3YzkwMGExYjFiMjgyOGMzYjVjZmQ4MmRkOTliMCx6cDFhZkhoUQ%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067473000%2Fcassandra-cql-ruby-gem-for-cassandra-10&amp;m=1\" title=\"Google Code Wiki\" target=\"_blank\">Google Code Wiki</a>.</p><p>Enjoy!</p></div></div></section><section class=\"panel\"></section></div></article><article class=\"text not-page post-143067472090  active exposed\" data-post-id=\"143067472090\"><div class=\"post-wrapper clearfix\"><section class=\"post\"><div class=\"post-content\"><div class=\"body-text\"><p>Our recently-released <a href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Fblog.insidesystems.net%2Fusing-cassandra-with-paperclip-on-rails&amp;t=NGM3YjQxYzdjYzkyZGM4NWIyMDExMTczMzFmYTc1ZTY5ZmY4MWJiYiwxd3h1dUV5Mg%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067472090%2Fpaperclip-cassandra-gets-consistent&amp;m=1\" target=\"_self\">Cassandra storage plugin for paperclip</a> was lacking one thing: the ability to specify read/write consistency levels. Well, now you can just by adding the following options to your has_attachment specification (these are the defaults, btw).</p><div class=\"CodeRay\"><div class=\"code\"><pre>:read_consistency =&gt; Cassandra::Consistency::ONE\n:write_consistency =&gt; Cassandra::Consistency::QUORUM</pre></div></div><p>Enjoy!</p></div></div></section><section class=\"panel\"></section></div></article><article class=\"text not-page post-143067472270  active exposed\" data-post-id=\"143067472270\"><div class=\"post-wrapper clearfix\"><section class=\"post\"><div class=\"post-content\"><div class=\"body-text\"><p>If you come from the SQL world, sometimes it can be difficult to understand the <a href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Fwiki.apache.org%2Fcassandra%2FDataModel&amp;t=ZDgyZTBmNzNjMDQzZDJmNmFlOGY2YmJhMzkxOTBiNzJlNjQyMTJjMyxPUDZGYU03eA%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067472270%2Funderstanding-the-cassandra-data-model-from-a-sql&amp;m=1\" target=\"_blank\">Cassandra Data Model</a> and all that it implies in terms of management and scalability. For this post, we’re going to go backwards. Instead of writing an example application using Cassandra to understand it, I’ll describe implementing Cassandra on a traditional SQL database and what that would look like.</p><p><strong>1. Mapping a Column Family to SQL Tables</strong></p><p>In Cassandra, a Column Family has any number of rows, and each row has N column names and values. For this example, let’s assume that in Cassandra we have a Users Column Family with uuids as the row key and column name/value pairs as attributes such as username, password, email, etc. If we had 10000 such users, then in SQL we’d have 10000 tables that looked like the following where 91b56770-bf40-11df-969c-b2b36bca998e was the actual uuid of a user:</p><p><code>CREATE TABLE users-91b56770-bf40-11df-969c-b2b36bca998e (column_name varchar, column_value varchar) UNIQUE column_name PRIMARY KEY column_name;</code></p><p><code>INSERT INTO </code><code>users-91b56770-bf40-11df-969c-b2b36bca998e VALUES ('username', 'joe'), ('password', 'weak'), ('email', 'joe@example.com');</code></p><ul><li><strong>10000 tables!</strong> That’s right, 10000 tables. In SQL, that many tables would be absurd but Cassandra is designed to scale that way and lots of rows are encouraged as it helps load balancing and scalability. In fact, millions and billions of rows are the norm. You can think of Cassandra as a system that distributes these tables over many nodes with some consistency and replication guarantees.</li><li><strong>You said N rows, but then said a table is on a single node. You don’t really mean N do you?</strong> No, not really. A row has to fit on a single node just like a table has to fit on a single machine for most SQL implementations. For older versions of Cassandra, a row also had to fit in RAM.</li><li><strong>It looks like I can only query one user at a time and I already have to know his UUID. Is that true?</strong> In SQL terms, Cassandra can perform table ranges and row slices on table ranges. In Cassandra terms, you can get a slice of keys and a slice of columns within those keys. This is what is meant by ‘column-oriented key-value store’. One thing you don’t get to do is join. Cassandra does not join. <em>Cassandra does not join.</em> If you want to join, you do it in your application.</li><li><strong>What is a Column Family for then? Just a table prefix?</strong> A Column Family has a number of settings that go with it that alter it’s behavior. There are cache settings for the keys (the UUIDs in this example), cache settings for the entire rows (the entire table in this example), and most importantly, sorting. In Cassandra there is no OFFSET, only LIMIT and the equivalent of BETWEEN. In this example, the column names are just strings but they could also be integers or timestamps and they are always stored in sort order. One Column Family might have timestamp-sorted data where you query things by time slice and another might be address book data where you query things in alphabetical order. The only sorting you get to do after the fact is reversing a particular slice. </li><li><strong>What if I want to sort by column value and by column name?</strong> Then you have two Column Families, one sorted by whatever datatype your column names are, and one sorted by whatever your column values are.</li><li><strong>So if I wanted to find the user who had a particular email address, how would I do that? I don’t know what table to query!</strong> Exactly! If you want to answer that question, you’ll have to make another Column Family that keys on e-mail address and points to the UUID of the user that has it, then you can query the relevant users table. Think about it this way .. in SQL, you can CREATE INDEX for things you want to query and in Cassandra, you have to manually maintain that index. (Secondary indexes are present in 0.7+, but I’ll cover that another time.)</li><li><strong>In SQL, I can still do a full table scan to pull out the information I’m looking for. Why can’t Cassandra do that?</strong> You can iterate with Cassandra and functionally get the same result, but I wouldn’t if I were you.</li><li><strong>But if I have two Column Families with the same data sorted differently, that’s denormalized. I heard that was bad.</strong> Denormalization is the norm with Cassandra. This is a complicated topic but in short, denormalization is a requirement for linear horizontal scaling and many people were already doing this in their SQL databases to avoid joins anyway.</li><li><strong>Wait, there is no OFFSET? How do I paginate with cassandra? </strong>Awkwardly. You have to start at the beginning, get N items, then on the next page you’d have to start at the last of the previous items and get N+1, throwing away the first one, etc. Alternately, you can create another Column Family which caches pages but in general, pagination is not what Cassandra was designed for.</li></ul><p><strong>2. Mapping a Super Column Family to SQL tables</strong></p><p>A Super Column Family is exactly like a Column Family, except you get one more column in your table. For this example, we’ll make an address book.</p><p><code>CREATE TABLE addressbooks-91b56770-bf40-11df-969c-b2b36bca998e (super_column_name, column_name varchar, column_value varchar) UNIQUE (super_column_name, column_name) PRIMARY KEY super_column_name;</code></p><p><code>INSERT INTO </code><code>addressbooks-91b56770-bf40-11df-969c-b2b36bca998e VALUES ('bob', 'street', '1313 Mockingbird Lane'), ('bob', 'city', 'Chicago'), ('bob', 'state', 'IL');</code></p><p><code>INSERT INTO </code><code>addressbooks-91b56770-bf40-11df-969c-b2b36bca998e VALUES ('alice', 'street', '123 Foo St.'), ('alice', 'city', 'Kona'), ('alice', 'state', 'HI');</code></p><ul><li><strong>Do the column_names have to be the same for each Super Column?</strong> No.</li><li><strong>Can the super_column_name and column_name be sorted differently?</strong> Yes.</li><li><strong>Does the entire table still  have to fit on one node or does it split out the Super Columns?</strong> The entire row still has to fit on one node.</li><li><strong>Is there a limit to how many Columns can be in a Super Column?</strong> Other than the space limitation already mentioned, theoretically, no. There is one gigantic caveat: <em>Accessing a single Column in a Column Family is efficient and fast. Accessing a single Column in a Super Column ('city’ or 'state’ in this case) requires deserialization of the entire Super Column.</em> For this example, this means that to get alice -&gt; state, Cassandra has to load everything in the alice super column. For an addressbook, this doesn’t really matter but if you had 1000 columns and a read-heavy load, it could heavily impact performance since you’d have to deserialize all 1000 columns just to get one of them.</li><li><strong>It sounds like Super Column Families can be dangerous if I’m not careful.</strong> Exactly! They were designed specifically for inverted indexing. If you aren’t doing that or don’t know what that is, don’t use them.</li></ul><p><strong>3. Transactions and/or Rollbacks</strong></p><p>Cassandra does neither transactions nor rollbacks. I you want those, some other thing such as <a href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Fcode.google.com%2Fp%2Fcages%2F&amp;t=NmNhNjg1ZTYyNGQxOWZhNjhmY2VlNWFjYzQxMzJjNWFmYTliNWFiYSxPUDZGYU03eA%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067472270%2Funderstanding-the-cassandra-data-model-from-a-sql&amp;m=1\" target=\"_blank\">Cages/ZooKeeper</a> has to do that.</p><p><strong>Next Post</strong></p><p>To learn more about how these tables would be replicated and queried, continue on to the next post, <a href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Fblog.insidesystems.net%2Fcassandra-data-model-from-an-sql-perspective&amp;t=MDIzNDU5YjQ1MGQxZTYxZTBiNjA5ODM3MjQwOThjYjdiYjZmY2Y5NCxPUDZGYU03eA%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067472270%2Funderstanding-the-cassandra-data-model-from-a-sql&amp;m=1\" target=\"_self\">Replication Factor and Consistency Levels</a>.</p></div></div></section><section class=\"panel\"></section></div></article><article class=\"text not-page post-143067472805  active exposed\" data-post-id=\"143067472805\"><div class=\"post-wrapper clearfix\"><section class=\"post\"><div class=\"post-content\"><div class=\"body-text\"><h3>Cassandra Query Language (CQL)</h3><a href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Fcassandra.apache.org&amp;t=OWIxNjZhMjUzYTc5OWMzNmJlOTE0ZTc3ZDVhN2E5NjM2NmExNmQ5MixtUFpnbXlCTA%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067472805%2Fcassandra-cql-a-ruby-cql-client-for-cassandra&amp;m=1\" target=\"_blank\"><img class=\"alignright\" title=\"Apache Cassandra Project\" src=\"http://cassandra.apache.org/media/img/cassandra_logo.png\" alt=\"\" width=\"350\" height=\"70\" /></a>Cassandra originally went with a Thrift RPC-based API as a way to provide a common denominator that more idiomatic clients could build upon independently. However, this worked poorly in practice: raw Thrift is too low-level to use productively, and keeping pace with new API methods to support (for example) indexes in 0.7 or distributed counters in 0.8 is too much for many maintainers.\nCQL, the Cassandra Query Language, addresses this by pushing all implementation details to the server; all the client has to know for any operation is how to interpret “resultset” objects. So adding a feature like counters just requires teaching the CQL parser to understand “column + N” notation; no client-side changes are necessary.\n(CQL Specification: <a title=\"CQL Reference\" href=\"https://t.umblr.com/redirect?z=https%3A%2F%2Fgithub.com%2Fapache%2Fcassandra%2Fblob%2Ftrunk%2Fdoc%2Fcql%2FCQL.textile&amp;t=ZmI4YTRhNWZmZTVkZGJmNDZmZWM5MDM0Y2Q4Y2Q3MTk4ZDg5NWE0ZixtUFpnbXlCTA%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067472805%2Fcassandra-cql-a-ruby-cql-client-for-cassandra&amp;m=1\" target=\"_blank\">github.com/apache/cassandra/blob/trunk/doc/cql/CQL.textile</a>)<h3>Cassandra-CQL For Ruby</h3>Cassandra-CQL implements a DBI-like interface on top of CQL in Ruby that should be familiar to anybody who has worked with a traditional RDBMS before. It is not yet released as a gem as it will shortly be included in Apache Extras and distributed from there. To get started developing with it now, just clone it from Github and build/install the gem locally with ‘rake install’.\n(Github:<a title=\"Cassandra-CQL\" href=\"https://t.umblr.com/redirect?z=https%3A%2F%2Fgithub.com%2Fkreynolds%2Fcassandra-cql&amp;t=OTk0ZjI0NDU4NTlmZmIyODc0MTlkOTM5ZmU4NzlhNTMzZGJjN2RhMixtUFpnbXlCTA%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067472805%2Fcassandra-cql-a-ruby-cql-client-for-cassandra&amp;m=1\" target=\"_blank\">http://github.com/kreynolds/cassandra-cql\n</a></div></div></section><section class=\"panel\"></section></div></article><article class=\"text not-page post-143067474645  active exposed\" data-post-id=\"143067474645\"><div class=\"post-wrapper clearfix\"><section class=\"post\"><div class=\"post-content\"><div class=\"body-text\"><p>It’s been a while since the last release of the <a title=\"Cassandra-CQL\" href=\"https://t.umblr.com/redirect?z=https%3A%2F%2Fgithub.com%2Fkreynolds%2Fcassandra-cql&amp;t=YWM2YTJkYzBhN2JjNWRkZThkMDRiNDMwN2ViMDk4MWRmYjg2NTU1MCxLWkV3MVdoNg%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067474645%2Fcassandra-cql-110&amp;m=1\" target=\"_blank\">cassandra-cql gem</a>, and this version fixes a few long-standing encoding and data-access bugs as well as support for <a title=\"Datastax CQL Reference\" href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Fwww.datastax.com%2Fdocs%2F1.1%2Freferences%2Fcql%2Findex&amp;t=NDc5NTczOGFhMGQ3YjBmNDAwYjNhOGYxYjJjNjE1NGNmYjIxYjUxZSxLWkV3MVdoNg%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067474645%2Fcassandra-cql-110&amp;m=1\" target=\"_blank\">CQL3</a>. The driver now also uses <a title=\"Cassandra-CQL in Travis\" href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Ftravis-ci.org%2F%23%21%2Fkreynolds%2Fcassandra-cql&amp;t=OTgzY2M0MzVlYzRlMGMxYjE4MjgwZTdhYTM5MTdkMTAxNDBjMWRmYyxLWkV3MVdoNg%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067474645%2Fcassandra-cql-110&amp;m=1\" target=\"_blank\">Travis</a> for continuous integration.</p><p>The largest improvement is the ability to cast single columns at a time. One of the major pain points previously was that if a column in Cassandra had existing data and it’s validation changed to make that data invalid (empty string in an Integer-validated column for instance), none of the data in that row would be accessible as the entire row was casted at once. Each column is now individually casted and cached and if there is invalid data in a validated column, it will raise a CastException that contains a useful description of the problem as well as the bytes that raised the exception.</p><p>Another important bug fix is for character encoding in Ruby 1.9. The previous version incorrectly used ASCII-8BIT on data retrieved from ASCII-validated columns which when stored back into Cassandra, would encode it as binary. A simple change to US-ASCII in the casting has fixed that.</p><p><code>gem install cassandra-cql</code></p><p>Enjoy!</p></div></div></section><section class=\"panel\"></section></div></article><article class=\"text not-page post-143067472490  active exposed\" data-post-id=\"143067472490\"><div class=\"post-wrapper clearfix\"><section class=\"post\"><div class=\"post-content\"><div class=\"body-text\"><p>If you read the last post, <a href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Fblog.insidesystems.net%2Funderstanding-the-cassandra-data-model-from-a&amp;t=YzIwN2MxNjEzZTQ1YzE4YzU5NGUxNjcxMDk3Yjk0NjBkZTlmZDY4YyxKVWVkQW91VQ%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067472490%2Fcassandra-data-model-from-an-sql-perspective&amp;m=1\" target=\"_blank\">Understanding the Cassandra Data Model from a SQL Perspective</a>, then you already have a decent handle on what a column-oriented key/value store is. The next thing that tends to confuse new users of Cassandra is the notion of <strong>Consistency Level</strong> and <strong>Replication Factor</strong>. This post doesn’t really have a lot to do with SQL in particular, it’s more of a discussion about how tables would be replicated across multiple servers and how you read/write from those tables.</p><p><strong>1. Replication Factor</strong></p><p>In the previous post, we described how every single Cassandra Row would be a separate table in SQL with an arbitrary number of rows. The <strong>Replication Factor</strong> (aka ‘RF’) determines <em>how many nodes have a copy of that table</em>. If you have 10 nodes and your RF=3, then every single table will exist on 3 nodes. Table users-91b56770-bf40-11df-969c-b2b36bca998e might exist on nodes 2 through 4 and users-28b56770-b410-15ef-968c-b2c36d511e78 might exist on nodes 6 through 8.</p><ul><li><strong>Can the RF be any number?</strong> Anything one or higher works but make sure it’s not higher than the actual number of nodes you have. This may be fixed by now but it used to be handled ungracefully. In general it should also be an odd number &gt;=3 so that quorum is a useful consistency level (for more details on this, keep reading).</li><li><strong>How do I know which node to query to get my data if it could be anywhere? Do I have to know where to write it?</strong> Each node acts as a storage proxy for every other node. If you query node 5 for data that is on nodes 8-10, it knows to ask one or more of those nodes for the answer. This is also true of writes.</li><li><strong>What if a node goes down? How do I synchronize them? What if they have different values?</strong> This will be covered in the next section, <strong>Consistency Levels</strong>.</li></ul><p><strong>2. Consistency Level</strong></p><p>Consistency in Cassandra is Eventual, which is to say that the N nodes responsible for a particular table could have a slightly different opinion of what it’s contents are. That’s ok! When reading from or writing to a Cassandra cluster, you can tune the tolerance for that sort of tomfoolery by reading or writing at a varying <strong>Consistency Level</strong> (aka 'CL’). Remember that each node acts as a proxy for every other node so you don’t have to worry about which node you are interacting with (unless you are really trying to optimize network activity but in general, this is unnecessary).</p><p>CL::ALL will not return success unless it has successfully written the same value to every node responsible for holding whatever you are writing. Using CL::ALL, you can guarantee that whatever you are writing is the same on all nodes when success is returned. There is of course a performance penalty for this, but in return you get very strong Consistency. Reading at CL::ALL functions in a similar way .. if one of the nodes does not respond, it will return an error since it cannot guarantee that all of the nodes are consistent for the value in question.</p><p>CL::QUORUM only requires a majority of the nodes responsible for a table to respond instead of all of them. This allows for one or more nodes to be down or otherwise unavailable and still have your read/write return. If your RF=3, then only 2 nodes responsible for a table need to be online in order to reliably manipulate the data in that table. Note that if RF=1 or RF=2 then there is no meaningful difference between CL::ALL and CL::QUORUM so if you want the benefits of CL::QUORUM, make sure your RF&gt;=3.</p><p>CL::ONE requires that only one of the nodes responsible for a table respond. This makes reads and writes fast, but it also means that depending on what else is reading and writing, it’s possible that they could briefly give conflicting answers. This is a fine tradeoff for speed in many applications, but not all. For example, recording votes or website hits where the outside possibility of a few going missing on machine failure is probably fine. Recording a financial transaction of some sort, probably not so much.</p><p>CL::ANY is only used for writing, not reading. CL::ANY means that as soon as a write is received by <em>any</em> node, the call returns success. This occurs when your client might be connecting to node 5 but the nodes responsible for it are 6-8. The difference between CL::ONE and CL::ANY is that with CL::ANY, as soon as node 5 receives the write, it returns success (but nodes 6-8 could be down or whatever). CL::ONE means that if you write to node 5, either 6, 7, or 8 have to return success before node 5 returns success.</p><ul><li><strong>OK .. so if I use CL::ALL and it fails, does that mean that my write failed?</strong> Not necessarily! It may have succeeded on two nodes and failed on the third which means that <em>eventually</em> it will be propagated to the third, but the <em>required guarantee</em> was not met.</li><li><strong>Uh .. ok. So if that happens, how do I know that any of my writes succeeded?</strong> You don’t! At least not yet. The error doesn’t indicate percentage of success, just failure. Not great, I know.</li><li><strong>I have multiple datacenters. How do I tell Cassandra to put some data in each?</strong> There are various strategies and snitches that tell Cassandra where to put things and they are constantly in flux. You can also write your own if you have specific requirements. For more information, see the <a href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Fwiki.apache.org%2Fcassandra%2F&amp;t=N2QyNjI0YzA3Nzk3ZmExMDZkZTM1MTU0MGNmZjVkYjZmYzgwNjNhNSxKVWVkQW91VQ%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067472490%2Fcassandra-data-model-from-an-sql-perspective&amp;m=1\" target=\"_blank\">Cassandra Wiki</a>.</li><li><strong>What happens if I write at CL::ANY and that node explodes before it can send data to where it belongs?</strong> Congratulations, you just lost data. If you can’t tolerate that, don’t use CL::ANY.</li><li><strong>What happens if all 3 nodes are up, but have different values for something for some reason and I read at CL::ALL. Does it give me the finger or what?</strong> Cassandra performs 'read repair’. That is, upon reading, if you are using a CL that requires more than one node and they disagree on the value, Cassandra will compare the values and use whichever one has the latest timestamp. It will also write that value back to the node which had an outdated value hence 'repairing it’.</li><li><strong>What if a node goes offline for a whole day, then comes back and has all sorts of wonky outdated data. Is there a way to fix it all at once instead of on read? That sounds expensive and slow to do all ad-hoc.</strong> Indeed it is! You can initiate a complete 'repair’ of a node after a failure like that using nodetool.</li></ul></div></div></section><section class=\"panel\"></section></div></article><article class=\"text not-page post-143067471930  active exposed\" data-post-id=\"143067471930\"><div class=\"post-wrapper clearfix\"><section class=\"post\"><div class=\"post-content\"><div class=\"body-text\"><p>These have already been sent to the port maintainers so they should find their way into the ports collection soon enough but if you can’t wait, here are some scripts for pfstat (pfstatd in particular) and cassandra.</p><p>/usr/local/etc/rc.d/pfstatd</p><div class=\"CodeRay\"><div class=\"code\"><pre>#!/bin/sh\n# PROVIDE: pfstatd\n# REQUIRE: LOGIN\n# BEFORE:  securelevel\n# KEYWORD: shutdown\n# Add the following lines to /etc/rc.conf to enable `pfstatd':\n#\n# pfstatd_enable=\"YES\"\n# pfstatd_flags=\"\"\n#\n# See pfstat(1) for pfstatd_flags\n#\n. /etc/rc.subr\nname=\"pfstatd\"\nrcvar=`set_rcvar`\ncommand=\"/usr/local/bin/pfstatd\"\n# read configuration and set defaults\nload_rc_config \"$name\"\n: ${pfstatd_enable=\"NO\"}\nrun_rc_command \"$1\"</pre></div></div><p>/usr/local/etc/rc.d/cassandra</p><div class=\"CodeRay\"><div class=\"code\"><pre>#!/bin/sh\n# PROVIDE: cassandra\n# REQUIRE: LOGIN\n# BEFORE:  securelevel\n# KEYWORD: shutdown\n# Add the following lines to /etc/rc.conf to enable `cassandra':\n#\n# cassandra_enable=\"YES\"\n# cassandra_flags=\"\"\n#\n# See cassandra(1) for cassandra_flags\n#\n. /etc/rc.subr\nname=\"cassandra\"\nprocname=\"java\"\nrcvar=`set_rcvar`\ncommand=\"/usr/local/share/cassandra/bin/cassandra\"\npidfile=\"/var/run/$name.pid\"\ncommand_args=\"-p ${pidfile}\"\n# read configuration and set defaults\nload_rc_config \"$name\"\n: ${cassandra_enable=\"NO\"}\nrun_rc_command \"$1\"</pre></div></div></div></div></section><section class=\"panel\"></section></div></article><article class=\"text not-page post-143067472625  active exposed\" data-post-id=\"143067472625\"><div class=\"post-wrapper clearfix\"><section class=\"post\"><div class=\"post-content\"><div class=\"body-text\"><p>I’m sure you were expecting another post with a brief topical analysis of why Facebook chose HBase instead of Cassandra with some similarly topical and mostly incorrect summary of how those two systems work and why one is better than the other one.</p><p>Sorry to disappoint, but rather than pour more fuel on what is already a pointless fire, I’m going to suggest instead that you look at what your <strong>application</strong> and your <strong>business</strong> requires for success and can operationally support. That might mean that you need transactional rollback so Cassandra won’t work straight up, or that you already have HDFS for other jobs so HBase is easier to support than adding on an entirely new technology.</p><p>Regardless, please take the time to <strong>understand the systems</strong> and realize that the choices one company makes aren’t necessarily the same choices you should make for many reasons, technical and otherwise.</p></div></div></section><section class=\"panel\"></section></div></article><article class=\"text not-page post-143067473965  active exposed\" data-post-id=\"143067473965\"><div class=\"post-wrapper clearfix\"><section class=\"post\"><div class=\"post-content\"><div class=\"body-text\"><p>I recently had a conversation in #cassandra about the Data Model that I thought might be useful to try to distill into a few lines. These few lines ignore all of the implementation details to make it work in practice but it gives you the starting point. It looks a little something like this …\n</p><ol><li><strong><em>Keyspace</em></strong> - What method should I use to turn my application key into a <em>Row Key</em>?</li><li><strong><em>Row Key</em></strong> - Which <em>Node</em> is my value on?</li><li><strong><em>Column Family</em></strong> - Which file on the <em>node</em> is my value in?</li><li><strong><em>Column Name</em></strong> - Which piece of the file on the <em>node</em> contains my value?</li><li><strong><em>Column Value</em></strong> - My value!</li></ol><p>Since the columns are already sorted in the file, getting a slice of them is very efficient, and this is what makes Cassandra a column-oriented database. It’s worth noting that with most Cassandra clients, the <em>Row Key</em> and the <em>Column Family</em> are almost always conceptually swapped because it’s more common to access multiple keys in the same column family than to access multiple column families with the same key. Given the above, let’s revisit some of the other concepts that are required to make it work in practice.</p><p><strong>Partitioner</strong></p><p>The partitioner determines how your application keys get turned into <em>Row Keys</em>. <em>RandomPartitioner</em> (which should be called HashedPartitioner in my opinion) takes an MD5 of the key and uses that as the <em>Row Key </em>and <em>ByteOrderedPartitioner</em> uses the key unmodified.</p><p><strong>Replication Factor and Replica Placement Strategy</strong></p><p><em>ReplicationFactor</em> determines how many nodes get a copy of a particular key. There is no master/slave thing going on here, it’s just how many nodes get a copy of a key. The <em>Strategy</em> determines which exact nodes get a copy of your key based on network topology or ring placement.</p><p><strong>Consistency Levels</strong></p><p>Most relevant when the <em>ReplicationFactor</em> &gt; 1, the <em>ConsistencyLevel</em> determines how many nodes have to successfully record a write or agree on the value of a read for the operation to be considered successful. There are tables of consistency levels and what exactly they mean in multiple places so I won’t repeat them here.</p><p><strong>Memtables, Commitlog, SSTables, and Compaction</strong></p><p>As writes come in to a <em>ColumnFamily</em>, they are simultaneously (don’t argue) stored in memory in a <em>memtable</em> and written out to disk in a <em>commitlog</em>. The <em>memtable</em> is periodically written out to disk in in column-order as an <em>sstable</em> for efficient slicing later. Over time, a column could exist in a <em>memtable</em> and/or multiple <em>sstables</em> at once so a timestamp is used to determine which one to use, highest one wins. The timestamp is supplied by the client and can be any integer. Over time, you get a number of <em>sstables</em> and a process called <em>compaction</em> combines them all and throws away the out of date stuff to save space. Back to the <em>commitlog</em>, that’s there so that if the node explodes while some things are only in a <em>memtable</em>, they get replayed when the node starts back up so no data is lost (durability). Since the <em>commitlog</em> is serial write-only, if you put it on a separate disk, you can accept writes really fast.</p><p><strong>Miscellaneous</strong></p><div><ul><li>You can ask any node anything, they all act as proxies for one another.</li><li>If your <em>ReplicationFactor</em> &gt; 1, then <em>ReadRepair</em> will propagate the most recent column value to the nodes responsible or it.</li><li>There are various operations in <em>nodetool</em> that let you move nodes, add nodes, clean out old data/keys, manually compact things, repair everything at once, get statistics about each node, etc.</li><li><em>Gossip</em> is used to propagate node status and information to other nodes.</li></ul><p>Cassandra works in a complicated problem space and there are many subtle operational and technical details that aren’t covered her, but this is the gist of it.</p></div></div></div></section><section class=\"panel\"></section></div></article><article class=\"text not-page post-143067475055  active exposed\" data-post-id=\"143067475055\"><div class=\"post-wrapper clearfix\"><section class=\"post\"><div class=\"post-content\"><div class=\"body-text\"><a href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Frubyscale.com%2F%3Fattachment_id%3D446&amp;t=Mjg2ODQwNzc2MDE2ZjVmY2RlNzE0YjNmZjk0MjZlM2RmM2JjOGE3NywwMzlLTURCTg%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067475055%2Fusing-ruby-with-mios-from-mi-casa-verde&amp;m=1\" rel=\"attachment wp-att-446\"><img class=\"alignright size-medium wp-image-446\" alt=\"Vera Lite\" src=\"http://rubyscale.com/wp-content/uploads/2013/01/Vera-Lite1-300x236.png\" width=\"300\" height=\"236\" /></a>Mi Casa Verde. You can read the specifications of the devices for yourself but I like them because they are inexpensive, have low-power requirements, are capable of interfacing with a number of different kind of home automation systems, and most importantly, have a documented JSON/XML API. Coincidentally, they also have a<a title=\"Developer Special Program\" href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Fwww.micasaverde.com%2Fdevelop-an-app.php&amp;t=YTE3NDg5NzRlZmNlYWUzMDNiN2Y3ZmMyNTdhZGQ0ZTQ5MWZiZGFlNiwwMzlLTURCTg%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067475055%2Fusing-ruby-with-mios-from-mi-casa-verde&amp;m=1\" target=\"_blank\">developer special program</a>and lack a ruby gem, so another rainy day project is born!\nThe operating system these devices run is called MiOS which is essentially Linux/ARM and lots of glue. Mi Casa Verde operates a free VPN service that allows you to operate them remotely, and there are a number of free/paid smartphone apps available (though none struck me as particularly special). All of the interactions with MiOS work on a job queue basis. When a job is submitted, it’s status must be polled for success/failure. The Ruby library embraces this and allows jobs to be run synchronously or asynchronously.\nSome snippets from the github page:<pre>mios = MiOS::Interface.new('http://192.168.15.1:3480')\nswitch = mios.devices[0]\nswitch.off! { |obj|\n  puts \"The #{obj.name} is now off\"\n}\nputs \"This will get printed once the switch is off and the block has been executed\"\nswitch.on!(true) { |obj|\n  puts \"The #{obj.name} is now on\"\n}\nputs \"This will output immediately\"\nsleep(5) # Sleep to wait for the thread to finish\n</pre>Every device is supported by manually issuing commands as listed<a title=\"Mi Casa Verde Luup variables and actions\" href=\"https://t.umblr.com/redirect?z=http%3A%2F%2Fwiki.micasaverde.com%2Findex.php%2FLuup_UPnP_Variables_and_Actions&amp;t=YzlhZGZiNTJlY2ZkYTkzN2U2ZDZkN2E2NWE3ODZlODZhMmQ2YTE5NiwwMzlLTURCTg%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067475055%2Fusing-ruby-with-mios-from-mi-casa-verde&amp;m=1\" target=\"_blank\">in the wiki</a> and the devices that I currently use have nice wrappers around those for idiomatic usage (<a title=\"Example of Door Lock API\" href=\"https://t.umblr.com/redirect?z=https%3A%2F%2Fgithub.com%2Fkreynolds%2Fruby-mios%2Fblob%2Fmaster%2Flib%2Fmios%2Fservices%2Fdoor_lock1.rb&amp;t=OGFmZmQ4OWI0MjUyZGE2MDZjOTY0NjAwZGE5MmNhMGU2YzcxNjNjNywwMzlLTURCTg%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067475055%2Fusing-ruby-with-mios-from-mi-casa-verde&amp;m=1\" target=\"_blank\">example</a>).\nInstallation instructions and additional usage examples can be found on github under the<a title=\"Ruby MiOS\" href=\"https://t.umblr.com/redirect?z=https%3A%2F%2Fgithub.com%2Fkreynolds%2Fruby-mios&amp;t=YjQ0NTU4MjEzOGVhYTdjYWIwNTcyZDY0NTZmMTUzYTFhYjg3YjI5NSwwMzlLTURCTg%3D%3D&amp;b=t%3A2q2t0Z-Cm5Q6MebxUd5EmA&amp;p=http%3A%2F%2Fwww.rubyscale.com%2Fpost%2F143067475055%2Fusing-ruby-with-mios-from-mi-casa-verde&amp;m=1\" target=\"_blank\">ruby-mios project</a>.\nEnjoy!</div></div></section><section class=\"panel\"></section></div></article></div><p>&#13;\n                                        &#13;\n                                        <a class=\"related-post-cta button bordered open-in-app\">See this in the app</a>&#13;\n                                        <a class=\"related-post-cta see-more button bordered\" href=\"http://www.rubyscale.com/\">Show more</a>&#13;</p></section></div></div></section><footer id=\"footer\" class=\"content clearfix\"></footer></section><noscript><noscript><noscript id=\"bootloader\" data-bootstrap=\"{&quot;Components&quot;:{&quot;PostActivity&quot;:[],&quot;NotificationPoller&quot;:{&quot;messaging_keys&quot;:[],&quot;token&quot;:&quot;c86f8a500596474a7ef5034081f694bd&quot;,&quot;inbox_unread&quot;:0},&quot;DesktopDashboardLogo&quot;:{&quot;animations&quot;:[[&quot;http:\\/\\/assets.tumblr.com\\/images\\/logo\\/hover-animations\\/1.png?_v=161861acded461bb6e995593a3bae835&quot;,&quot;http:\\/\\/assets.tumblr.com\\/images\\/logo\\/hover-animations\\/1@2x.png?_v=496a774637302a598c851381d00009b0&quot;]]},&quot;TumblelogIframe&quot;:{&quot;unified&quot;:true,&quot;variant&quot;:null,&quot;isCompact&quot;:false,&quot;tumblelogBundleSrc&quot;:&quot;http:\\/\\/assets.tumblr.com\\/client\\/prod\\/standalone\\/tumblelog\\/index.build.js?_v=d31f5c6ecc27fedacb700eee8f83161f&quot;,&quot;tumblelogName&quot;:&quot;rubyscalecom&quot;,&quot;isLoggedIn&quot;:false,&quot;isFriend&quot;:false,&quot;formKey&quot;:&quot;&quot;,&quot;canSubscribe&quot;:false,&quot;isSubscribed&quot;:false,&quot;tumblelogTitle&quot;:&quot;RubyScale&quot;,&quot;tumblelogAvatar&quot;:&quot;https:\\/\\/assets.tumblr.com\\/images\\/default_avatar\\/octahedron_closed_40.png&quot;,&quot;tumblelogHost&quot;:&quot;http:\\/\\/www.rubyscale.com&quot;,&quot;isOptica&quot;:true,&quot;isCustomTheme&quot;:false,&quot;themeHeaderImage&quot;:&quot;https:\\/\\/assets.tumblr.com\\/images\\/default_header\\/optica_pattern_07.png?_v=c52eb3e7fd4be5c98b8970b4c330d172&quot;,&quot;themeBackgroundColor&quot;:&quot;#FAFAFA&quot;,&quot;themeTitleColor&quot;:&quot;#444444&quot;,&quot;themeAccentColor&quot;:&quot;#529ECC&quot;,&quot;brag&quot;:false,&quot;adsEnabled&quot;:true,&quot;canShowAd&quot;:false,&quot;isPremium&quot;:false,&quot;showLrecAds&quot;:false,&quot;showStickyLrecBackfill&quot;:false,&quot;showGeminiAds&quot;:false,&quot;geminiSectionCodeDesktop&quot;:&quot;a10bca9c-0c5d-4a02-ab13-14ab8513d81d&quot;,&quot;geminiSectionCodeMobile&quot;:&quot;ced63809-b609-4aca-96a0-abc099acba6b&quot;,&quot;currentPageType&quot;:&quot;single_post&quot;,&quot;currentPage&quot;:&quot;1&quot;,&quot;searchQuery&quot;:&quot;&quot;,&quot;tag&quot;:&quot;&quot;,&quot;query&quot;:&quot;&quot;,&quot;chrono&quot;:false,&quot;postId&quot;:&quot;143067470585&quot;,&quot;src&quot;:&quot;http:\\/\\/www.rubyscale.com\\/post\\/143067470585\\/basic-time-series-with-cassandra&quot;,&quot;postUrl&quot;:&quot;&quot;,&quot;isBlocked&quot;:null,&quot;isAdmin&quot;:false,&quot;lookupButtonUrl&quot;:&quot;&quot;,&quot;showSpamButton&quot;:false,&quot;showRootPostButton&quot;:false,&quot;rootPostUrl&quot;:&quot;&quot;,&quot;showRadarPostButton&quot;:false,&quot;radarKeys&quot;:&quot;&quot;,&quot;isUniblocked&quot;:false,&quot;isNsfw&quot;:false,&quot;isAdult&quot;:false,&quot;isSpam&quot;:false,&quot;isPrimaryBlog&quot;:false,&quot;canEdit&quot;:false,&quot;canReblogSelf&quot;:false,&quot;showLikeButton&quot;:false,&quot;showReblogButton&quot;:false,&quot;reblogUrl&quot;:&quot;&quot;,&quot;showFanMailButton&quot;:false,&quot;showMessagingButton&quot;:false,&quot;loginCheckIframeSrc&quot;:&quot;http:\\/\\/assets.tumblr.com\\/assets\\/html\\/iframe\\/login_check.html?_v=3de94a184d600617102ddd5b48fb36e9&quot;,&quot;appInstallUrls&quot;:{&quot;android&quot;:&quot;https:\\/\\/play.google.com\\/store\\/apps\\/details?id=com.tumblr\\u0026referrer=utm_source%3Dtumblr%26utm_medium%3Diframe%26utm_campaign%3Dblog_network_floating_cta&quot;,&quot;ios&quot;:&quot;https:\\/\\/itunes.apple.com\\/app\\/apple-store\\/id305343404?pt=9029\\u0026ct=blog_network_floating_cta\\u0026mt=8&quot;},&quot;appOpenReferrer&quot;:&quot;tumblr_new_iframe&quot;,&quot;isShowSearch&quot;:true,&quot;supplyLogging&quot;:[],&quot;secondsSinceLastActivity&quot;:-1}},&quot;Flags&quot;:{&quot;doods&quot;:&quot;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&quot;},&quot;Context&quot;:{&quot;name&quot;:&quot;default&quot;,&quot;time&quot;:1538737946000,&quot;userinfo&quot;:{&quot;primary&quot;:&quot;&quot;,&quot;name&quot;:&quot;&quot;,&quot;channels&quot;:[]},&quot;hosts&quot;:{&quot;assets_host&quot;:&quot;http:\\/\\/assets.tumblr.com&quot;,&quot;secure_assets_host&quot;:&quot;https:\\/\\/assets.tumblr.com&quot;,&quot;www_host&quot;:&quot;http:\\/\\/www.tumblr.com&quot;,&quot;secure_www_host&quot;:&quot;https:\\/\\/www.tumblr.com&quot;,&quot;embed_host&quot;:&quot;https:\\/\\/embed.tumblr.com&quot;,&quot;safe_host&quot;:&quot;http:\\/\\/safe.txmblr.com&quot;,&quot;platform_host&quot;:&quot;http:\\/\\/platform.tumblr.com&quot;},&quot;language&quot;:&quot;en_US&quot;,&quot;language_simple&quot;:&quot;en&quot;,&quot;assets&quot;:&quot;http:\\/\\/assets.tumblr.com\\/client\\/prod\\/&quot;},&quot;Translations&quot;:{&quot;%1$sReport %2$s's post?%3$sIf it violates our community guidelines, we'll remove it.%4$s&quot;:&quot;%1$sReport %2$s's reblog?%3$sIf it violates our community guidelines, we'll remove it.%4$s&quot;,&quot;%1$sReport %2$s's reply?%3$sIf it violates our community guidelines, we'll remove it.%4$s&quot;:&quot;%1$sReport %2$s's reblog?%3$sIf it violates our community guidelines, we'll remove it.%4$s&quot;}}\"></noscript></noscript></noscript>",
        "created_at": "2018-10-05T11:12:26+0000",
        "updated_at": "2018-10-05T11:12:28+0000",
        "published_at": "2011-03-06T14:00:00+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 25,
        "domain_name": "www.rubyscale.com",
        "preview_picture": "http://assets.tumblr.com/images/og/fb_landscape_share.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12321"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 15,
            "label": "tutorial",
            "slug": "tutorial"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12320,
        "uid": null,
        "title": "Intro to Cassandra",
        "url": "https://www.slideshare.net/tylerhobbs/intro-to-cassandra",
        "content": "Intro to Cassandra\n      \n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            </a><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div>\n    <div class=\"wrapper\"><p>Successfully reported this slideshow.</p><div id=\"slideview-container\" class=\"\"><div class=\"row\"><div id=\"main-panel\" class=\"small-12 large-8 columns\"><div class=\"sectionElements\"><div class=\"playerWrapper\"><div><div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\">Intro to Cassandra<div class=\"stage valign-first-slide\"><div class=\"slide_container\"><section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-1-728.jpg?cb=1317900768\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-1-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-1-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-1-1024.jpg?cb=1317900768\" alt=\"Intro toCassandra  Tyler Hobbs\" /></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-2-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-2-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-2-1024.jpg?cb=1317900768\" alt=\"HistoryDynamo                     BigTable(clustering)               (data model)               Cassandra\" /></i></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-3-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-3-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-3-1024.jpg?cb=1317900768\" alt=\"Users\" /></i></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-4-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-4-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-4-1024.jpg?cb=1317900768\" alt=\"Clustering    Every node plays the same role    – No masters, slaves, or special nodes    – No single point of failure\" /></i></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-5-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-5-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-5-1024.jpg?cb=1317900768\" alt=\"Consistent Hashing           0     50          10     40          20           30\" /></i></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-6-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-6-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-6-1024.jpg?cb=1317900768\" alt=\"Consistent Hashing                      Key: “www.google.com”           0     50          10     40          20           30\" /></i></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-7-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-7-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-7-1024.jpg?cb=1317900768\" alt=\"Consistent Hashing                      Key: “www.google.com”           0                      md5(“www.google.com”)     5...\" /></i></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-8-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-8-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-8-1024.jpg?cb=1317900768\" alt=\"Consistent Hashing                      Key: “www.google.com”           0                      md5(“www.google.com”)     5...\" /></i></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-9-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-9-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-9-1024.jpg?cb=1317900768\" alt=\"Consistent Hashing                      Key: “www.google.com”           0                      md5(“www.google.com”)     5...\" /></i></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-10-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-10-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-10-1024.jpg?cb=1317900768\" alt=\"Consistent Hashing                        Key: “www.google.com”           0                        md5(“www.google.com”)  ...\" /></i></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-11-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-11-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-11-1024.jpg?cb=1317900768\" alt=\"Clustering    Client can talk to any node\" /></i></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-12-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-12-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-12-1024.jpg?cb=1317900768\" alt=\"ScalingRF = 2             0              50        10The node at50 owns thered portion             20                   30\" /></i></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-13-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-13-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-13-1024.jpg?cb=1317900768\" alt=\"ScalingRF = 2               0                50        10   Add a new    40        20   node at 40                     30\" /></i></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-14-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-14-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-14-1024.jpg?cb=1317900768\" alt=\"ScalingRF = 2               0                50        10   Add a new    40        20   node at 40                     30\" /></i></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-15-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-15-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-15-1024.jpg?cb=1317900768\" alt=\"Node FailuresRF = 2               0                50        10   Replicas                40        20                    ...\" /></i></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-16-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-16-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-16-1024.jpg?cb=1317900768\" alt=\"Node FailuresRF = 2               0                50        10   Replicas                40        20                    ...\" /></i></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-17-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-17-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-17-1024.jpg?cb=1317900768\" alt=\"Node FailuresRF = 2               0                50        10                40        20                     30\" /></i></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-18-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-18-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-18-1024.jpg?cb=1317900768\" alt=\"Consistency, Availability    Consistency    – Can I read stale data?    Availability    – Can I write/read at all?    T...\" /></i></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-19-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-19-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-19-1024.jpg?cb=1317900768\" alt=\"Consistency    N = Total number of replicas    R = Number of replicas read from    – (before the response is returned) ...\" /></i></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-20-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-20-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-20-1024.jpg?cb=1317900768\" alt=\"Consistency    N = Total number of replicas    R = Number of replicas read from    – (before the response is returned) ...\" /></i></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-21-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-21-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-21-1024.jpg?cb=1317900768\" alt=\"Consistency W + R &gt; N gives strong consistency N=3 W=2 R=2 2 + 2 &gt; 3 ==&gt; strongly consistent\" /></i></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-22-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-22-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-22-1024.jpg?cb=1317900768\" alt=\"Consistency W + R &gt; N gives strong consistency N=3 W=2 R=2 2 + 2 &gt; 3 ==&gt; strongly consistent Only 2 of the 3 replicas must...\" /></i></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-23-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-23-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-23-1024.jpg?cb=1317900768\" alt=\"Consistency    Tunable Consistency    – Specify N (Replication Factor) per data set    – Specify R, W per operation\" /></i></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-24-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-24-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-24-1024.jpg?cb=1317900768\" alt=\"Consistency    Tunable Consistency    – Specify N (Replication Factor) per data set    – Specify R, W per operation    – ...\" /></i></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-25-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-25-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-25-1024.jpg?cb=1317900768\" alt=\"Availability    Can tolerate the loss of:    – N – R replicas for reads    – N – W replicas for writes\" /></i></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-26-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-26-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-26-1024.jpg?cb=1317900768\" alt=\"CAP TheoremDuring node or network failure:          100%                                          Not                     ...\" /></i></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-27-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-27-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-27-1024.jpg?cb=1317900768\" alt=\"CAP TheoremDuring node or network failure:          100%                                                 Not              ...\" /></i></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-28-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-28-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-28-1024.jpg?cb=1317900768\" alt=\"Clustering    No single point of failure    Replication that works    Scales linearly    – 2x nodes = 2x performance   ...\" /></i></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-29-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-29-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-29-1024.jpg?cb=1317900768\" alt=\"Data Model    Comes from Google BigTable    Goals    – Minimize disk seeks    – High throughput    – Low latency    – Du...\" /></i></section><section data-index=\"30\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-30-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-30-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-30-1024.jpg?cb=1317900768\" alt=\"Data Model    Keyspace    – A collection of Column Families    – Controls replication settings    Column Family    – Kin...\" /></i></section><section data-index=\"31\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-31-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-31-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-31-1024.jpg?cb=1317900768\" alt=\"Column Families    Static    – Object data    – Similar to a table in a relational database    Dynamic    – Pre-calculat...\" /></i></section><section data-index=\"32\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-32-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-32-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-32-1024.jpg?cb=1317900768\" alt=\"Static Column Families                   Users   zznate    password: *    name: Nate   driftx    password: *   name: Brand...\" /></i></section><section data-index=\"33\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-33-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-33-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-33-1024.jpg?cb=1317900768\" alt=\"Dynamic Column Families    Rows    – Each row has a unique primary key    – Sorted list of (name, value) tuples       • L...\" /></i></section><section data-index=\"34\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-34-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-34-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-34-1024.jpg?cb=1317900768\" alt=\"Dynamic Column Families                     Followingzznate    driftx:   thobbs:driftxthobbs    zznate:jbellis   driftx:  ...\" /></i></section><section data-index=\"35\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-35-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-35-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-35-1024.jpg?cb=1317900768\" alt=\"Dynamic Column Families    Column Timestamps    – Each column (tuple) has a timestamp    – In the case of a collision, th...\" /></i></section><section data-index=\"36\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-36-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-36-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-36-1024.jpg?cb=1317900768\" alt=\"Dynamic Column Families    Other Examples:    – Timeline of tweets by a user    – Timeline of tweets by all of the people...\" /></i></section><section data-index=\"37\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-37-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-37-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-37-1024.jpg?cb=1317900768\" alt=\"The Data API    Two choices    – RPC-based API    – CQL       • Cassandra Query Language\" /></i></section><section data-index=\"38\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-38-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-38-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-38-1024.jpg?cb=1317900768\" alt=\"Inserting Data INSERT INTO users (KEY, “name”, “age”)     VALUES (“thobbs”, “Tyler”, 24);\" /></i></section><section data-index=\"39\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-39-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-39-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-39-1024.jpg?cb=1317900768\" alt=\"Updating Data Updates are the same as inserts: INSERT INTO users (KEY, “age”)     VALUES (“thobbs”, 34); Or UPDATE users S...\" /></i></section><section data-index=\"40\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-40-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-40-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-40-1024.jpg?cb=1317900768\" alt=\"Fetching Data Whole row select: SELECT * FROM users WHERE KEY = “thobbs”;\" /></i></section><section data-index=\"41\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-41-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-41-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-41-1024.jpg?cb=1317900768\" alt=\"Fetching Data Explicit column select: SELECT “name”, “age” FROM users     WHERE KEY = “thobbs”;\" /></i></section><section data-index=\"42\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-42-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-42-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-42-1024.jpg?cb=1317900768\" alt=\"Fetching Data Get a slice of columns UPDATE letters SET 1=a, 2=b, 3=c, 4=d, 5=e     WHERE KEY = “key”; SELECT 1..3 FROM le...\" /></i></section><section data-index=\"43\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-43-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-43-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-43-1024.jpg?cb=1317900768\" alt=\"Fetching Data Get a slice of columns SELECT FIRST 2 FROM letters WHERE KEY = “key”; Returns [(1, a), (2, b)] SELECT FIRST ...\" /></i></section><section data-index=\"44\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-44-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-44-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-44-1024.jpg?cb=1317900768\" alt=\"Fetching Data Get a slice of columns SELECT 3.. FROM letters WHERE KEY = “key”; Returns [(3, c), (4, d), (5, e)] SELECT FI...\" /></i></section><section data-index=\"45\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-45-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-45-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-45-1024.jpg?cb=1317900768\" alt=\"Deleting Data Delete a whole row: DELETE FROM users WHERE KEY = “thobbs”; Delete specific columns: DELETE “age” FROM users...\" /></i></section><section data-index=\"46\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-46-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-46-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-46-1024.jpg?cb=1317900768\" alt=\"Secondary Indexes Builtin basic indexes CREATE INDEX ageIndex ON users (age); SELECT name FROM USERS     WHERE age = 24 AN...\" /></i></section><section data-index=\"47\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-47-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-47-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-47-1024.jpg?cb=1317900768\" alt=\"Performance    Writes    – 10k – 30k per second per node    – Sub-millisecond latency    Reads    – 1k – 10k per second ...\" /></i></section><section data-index=\"48\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-48-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-48-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-48-1024.jpg?cb=1317900768\" alt=\"Other Features    Distributed Counters    – Can support millions of high-volume counters    Excellent Multi-datacenter S...\" /></i></section><section data-index=\"49\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-49-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-49-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-49-1024.jpg?cb=1317900768\" alt=\"What Cassandra Cant Do    Transactions    – Unless you use a distributed lock    – Atomicity, Isolation    – These arent ...\" /></i></section><section data-index=\"50\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-50-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-50-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-50-1024.jpg?cb=1317900768\" alt=\"Not One-size-fits-all    Use alongside an RDBMS    – Use the RDBMS for highly-transactional or highly-      relational da...\" /></i></section><section data-index=\"51\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-51-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-51-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-51-1024.jpg?cb=1317900768\" alt=\"Language Support    Good:    – Java    – Python    – Ruby    – PHP    – C#    Coming Soon:    – Everything else, now tha...\" /></i></section><section data-index=\"52\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/tylerhobbs/intro-to-cassandra\" data-small=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/85/intro-to-cassandra-52-320.jpg?cb=1317900768\" data-normal=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-52-728.jpg?cb=1317900768\" data-full=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-52-1024.jpg?cb=1317900768\" alt=\"Questions?          Tyler Hobbs               @tylhobbs       tyler@datastax.com\" /></i></section><div class=\"j-next-container next-container\"><div class=\"content-container\"><div class=\"next-slideshow-wrapper\"><div class=\"j-next-slideshow next-slideshow\"><p>Upcoming SlideShare</p></div><p>Loading in …5</p><p>×</p></div></div></div></div></div></div></div></div></div><div class=\"slideshow-info-container\" itemscope=\"itemscope\" itemtype=\"https://schema.org/MediaObject\"><div class=\"slideshow-tabs-container show-for-medium-up\"><ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    0 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\">\n                \n                  6 Likes\n                \n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\">\n                Statistics\n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\">\n                Notes\n              </i></a>\n            </li>\n          </ul><div class=\"tabs-content\"><div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\"><ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"itemscope\"><li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"RadhaKrishnaProddatu\" rel=\"nofollow\" href=\"https://www.slideshare.net/RadhaKrishnaProddatu?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Radha Krishna Proddaturi\n                            \n                              \n                                , \n                                Principal Software Engineer at Coupons.com\n                              \n                              \n                                 at \n                                Coupons.com\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"willhayworth\" rel=\"nofollow\" href=\"https://www.slideshare.net/willhayworth?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Will Hayworth\n                            \n                              \n                                \n                                \n                              \n                              \n                                \n                                \n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"uvaraj6\" rel=\"nofollow\" href=\"https://www.slideshare.net/uvaraj6?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Yuvaraj Loganathan\n                            \n                              \n                                , \n                                Technical Architect at Owler\n                              \n                              \n                                 at \n                                Owler\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"colinkuo\" rel=\"nofollow\" href=\"https://www.slideshare.net/colinkuo?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Colin Kuo\n                            \n                              \n                                , \n                                Staff Engineer at Brocade Communications Systems\n                              \n                              \n                                 at \n                                Brocade\n                              \n                            \n                            <p>Tags <strong></strong></p></a><a href=\"https://www.slideshare.net/tag/cassandra\" rel=\"nofollow\">cassandra</a>\n                            </div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"davekincaid\" rel=\"nofollow\" href=\"https://www.slideshare.net/davekincaid?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Dave Kincaid\n                            \n                              \n                                , \n                                Managing Architect - Data Services/Data Science at IDEXX Laboratories\n                              \n                              \n                                 at \n                                IDEXX Laboratories\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n              </ul><div class=\"more-container text-center\"><a href=\"#\" class=\"j-more-favs\">\n                    Show More\n                    \n                  </a></div></div><div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No Downloads</p></div><div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No notes for slide</p></div></div></div><div class=\"notranslate transcript add-padding-right j-transcript\"><ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    Intro toCassandra  Tyler Hobbs \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-2-728.jpg?cb=1317900768\" title=\"HistoryDynamo                     BigTable(clustering)     ...\" target=\"_blank\">\n        2.\n      </a>\n    HistoryDynamo                     BigTable(clustering)               (data model)               Cassandra \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-3-728.jpg?cb=1317900768\" title=\"Users\" target=\"_blank\">\n        3.\n      </a>\n    Users \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-4-728.jpg?cb=1317900768\" title=\"Clustering    Every node plays the same role    – No maste...\" target=\"_blank\">\n        4.\n      </a>\n    Clustering    Every node plays the same role    – No masters, slaves, or special nodes    – No single point of failure \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-5-728.jpg?cb=1317900768\" title=\"Consistent Hashing           0     50          10     40   ...\" target=\"_blank\">\n        5.\n      </a>\n    Consistent Hashing           0     50          10     40          20           30 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-6-728.jpg?cb=1317900768\" title=\"Consistent Hashing                      Key: “www.google.co...\" target=\"_blank\">\n        6.\n      </a>\n    Consistent Hashing                      Key: “www.google.com”           0     50          10     40          20           30 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-7-728.jpg?cb=1317900768\" title=\"Consistent Hashing                      Key: “www.google.co...\" target=\"_blank\">\n        7.\n      </a>\n    Consistent Hashing                      Key: “www.google.com”           0                      md5(“www.google.com”)     50          10                               14     40          20           30 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-8-728.jpg?cb=1317900768\" title=\"Consistent Hashing                      Key: “www.google.co...\" target=\"_blank\">\n        8.\n      </a>\n    Consistent Hashing                      Key: “www.google.com”           0                      md5(“www.google.com”)     50          10                               14     40          20           30 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-9-728.jpg?cb=1317900768\" title=\"Consistent Hashing                      Key: “www.google.co...\" target=\"_blank\">\n        9.\n      </a>\n    Consistent Hashing                      Key: “www.google.com”           0                      md5(“www.google.com”)     50          10                               14     40          20           30 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-10-728.jpg?cb=1317900768\" title=\"Consistent Hashing                        Key: “www.google....\" target=\"_blank\">\n        10.\n      </a>\n    Consistent Hashing                        Key: “www.google.com”           0                        md5(“www.google.com”)     50          10                                   14     40          20           30                Replication Factor = 3 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-11-728.jpg?cb=1317900768\" title=\"Clustering    Client can talk to any node\" target=\"_blank\">\n        11.\n      </a>\n    Clustering    Client can talk to any node \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-12-728.jpg?cb=1317900768\" title=\"ScalingRF = 2             0              50        10The no...\" target=\"_blank\">\n        12.\n      </a>\n    ScalingRF = 2             0              50        10The node at50 owns thered portion             20                   30 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-13-728.jpg?cb=1317900768\" title=\"ScalingRF = 2               0                50        10  ...\" target=\"_blank\">\n        13.\n      </a>\n    ScalingRF = 2               0                50        10   Add a new    40        20   node at 40                     30 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-14-728.jpg?cb=1317900768\" title=\"ScalingRF = 2               0                50        10  ...\" target=\"_blank\">\n        14.\n      </a>\n    ScalingRF = 2               0                50        10   Add a new    40        20   node at 40                     30 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-15-728.jpg?cb=1317900768\" title=\"Node FailuresRF = 2               0                50      ...\" target=\"_blank\">\n        15.\n      </a>\n    Node FailuresRF = 2               0                50        10   Replicas                40        20                     30 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-16-728.jpg?cb=1317900768\" title=\"Node FailuresRF = 2               0                50      ...\" target=\"_blank\">\n        16.\n      </a>\n    Node FailuresRF = 2               0                50        10   Replicas                40        20                     30 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-17-728.jpg?cb=1317900768\" title=\"Node FailuresRF = 2               0                50      ...\" target=\"_blank\">\n        17.\n      </a>\n    Node FailuresRF = 2               0                50        10                40        20                     30 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-18-728.jpg?cb=1317900768\" title=\"Consistency, Availability    Consistency    – Can I read s...\" target=\"_blank\">\n        18.\n      </a>\n    Consistency, Availability    Consistency    – Can I read stale data?    Availability    – Can I write/read at all?    Tunable Consistency \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-19-728.jpg?cb=1317900768\" title=\"Consistency    N = Total number of replicas    R = Number...\" target=\"_blank\">\n        19.\n      </a>\n    Consistency    N = Total number of replicas    R = Number of replicas read from    – (before the response is returned)    W = Number of replicas written to    – (before the write is considered a success) \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-20-728.jpg?cb=1317900768\" title=\"Consistency    N = Total number of replicas    R = Number...\" target=\"_blank\">\n        20.\n      </a>\n    Consistency    N = Total number of replicas    R = Number of replicas read from    – (before the response is returned)    W = Number of replicas written to    – (before the write is considered a success)    W + R &gt; N gives strong consistency \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-21-728.jpg?cb=1317900768\" title=\"Consistency W + R &gt; N gives strong consistency N=3 W=2 R=2 ...\" target=\"_blank\">\n        21.\n      </a>\n    Consistency W + R &gt; N gives strong consistency N=3 W=2 R=2 2 + 2 &gt; 3 ==&gt; strongly consistent \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-22-728.jpg?cb=1317900768\" title=\"Consistency W + R &gt; N gives strong consistency N=3 W=2 R=2 ...\" target=\"_blank\">\n        22.\n      </a>\n    Consistency W + R &gt; N gives strong consistency N=3 W=2 R=2 2 + 2 &gt; 3 ==&gt; strongly consistent Only 2 of the 3 replicas must be available. \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-23-728.jpg?cb=1317900768\" title=\"Consistency    Tunable Consistency    – Specify N (Replica...\" target=\"_blank\">\n        23.\n      </a>\n    Consistency    Tunable Consistency    – Specify N (Replication Factor) per data set    – Specify R, W per operation \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-24-728.jpg?cb=1317900768\" title=\"Consistency    Tunable Consistency    – Specify N (Replica...\" target=\"_blank\">\n        24.\n      </a>\n    Consistency    Tunable Consistency    – Specify N (Replication Factor) per data set    – Specify R, W per operation    – Quorum: N/2 + 1       • R = W = Quorum       • Strong consistency       • Tolerate the loss of N – Quorum replicas    – R, W can also be 1 or N \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-25-728.jpg?cb=1317900768\" title=\"Availability    Can tolerate the loss of:    – N – R repli...\" target=\"_blank\">\n        25.\n      </a>\n    Availability    Can tolerate the loss of:    – N – R replicas for reads    – N – W replicas for writes \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-26-728.jpg?cb=1317900768\" title=\"CAP TheoremDuring node or network failure:          100%   ...\" target=\"_blank\">\n        26.\n      </a>\n    CAP TheoremDuring node or network failure:          100%                                          Not                                          Possible   Availability                     Possible                     Consistency   100% \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-27-728.jpg?cb=1317900768\" title=\"CAP TheoremDuring node or network failure:          100%   ...\" target=\"_blank\">\n        27.\n      </a>\n    CAP TheoremDuring node or network failure:          100%                                                 Not                            Ca                   Possible                              ss                                an                                   dr   Availability                       a                     Possible                     Consistency          100% \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-28-728.jpg?cb=1317900768\" title=\"Clustering    No single point of failure    Replication t...\" target=\"_blank\">\n        28.\n      </a>\n    Clustering    No single point of failure    Replication that works    Scales linearly    – 2x nodes = 2x performance       • For both writes and reads    – Up to 100s of nodes    Operationally simple    Multi-Datacenter Replication \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-29-728.jpg?cb=1317900768\" title=\"Data Model    Comes from Google BigTable    Goals    – Mi...\" target=\"_blank\">\n        29.\n      </a>\n    Data Model    Comes from Google BigTable    Goals    – Minimize disk seeks    – High throughput    – Low latency    – Durable \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-30-728.jpg?cb=1317900768\" title=\"Data Model    Keyspace    – A collection of Column Familie...\" target=\"_blank\">\n        30.\n      </a>\n    Data Model    Keyspace    – A collection of Column Families    – Controls replication settings    Column Family    – Kinda resembles a table \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-31-728.jpg?cb=1317900768\" title=\"Column Families    Static    – Object data    – Similar to...\" target=\"_blank\">\n        31.\n      </a>\n    Column Families    Static    – Object data    – Similar to a table in a relational database    Dynamic    – Pre-calculated query results    – Materialized views \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-32-728.jpg?cb=1317900768\" title=\"Static Column Families                   Users   zznate    ...\" target=\"_blank\">\n        32.\n      </a>\n    Static Column Families                   Users   zznate    password: *    name: Nate   driftx    password: *   name: Brandon   thobbs    password: *    name: Tyler   jbellis   password: *   name: Jonathan   site: riptano.com \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-33-728.jpg?cb=1317900768\" title=\"Dynamic Column Families    Rows    – Each row has a unique...\" target=\"_blank\">\n        33.\n      </a>\n    Dynamic Column Families    Rows    – Each row has a unique primary key    – Sorted list of (name, value) tuples       • Like a sorted map or dictionary    – The (name, value) tuple is called a “column” \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-34-728.jpg?cb=1317900768\" title=\"Dynamic Column Families                     Followingzznate...\" target=\"_blank\">\n        34.\n      </a>\n    Dynamic Column Families                     Followingzznate    driftx:   thobbs:driftxthobbs    zznate:jbellis   driftx:   mdennis:   pcmanus   thobbs:   xedin:   zznate \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-35-728.jpg?cb=1317900768\" title=\"Dynamic Column Families    Column Timestamps    – Each col...\" target=\"_blank\">\n        35.\n      </a>\n    Dynamic Column Families    Column Timestamps    – Each column (tuple) has a timestamp    – In the case of a collision, the latest timestamp wins    – Client specifies timestamp with write    – Writes are idempotent       • Infinite retries allowed \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-36-728.jpg?cb=1317900768\" title=\"Dynamic Column Families    Other Examples:    – Timeline o...\" target=\"_blank\">\n        36.\n      </a>\n    Dynamic Column Families    Other Examples:    – Timeline of tweets by a user    – Timeline of tweets by all of the people a user is      following    – List of comments sorted by score    – List of friends grouped by state \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-37-728.jpg?cb=1317900768\" title=\"The Data API    Two choices    – RPC-based API    – CQL   ...\" target=\"_blank\">\n        37.\n      </a>\n    The Data API    Two choices    – RPC-based API    – CQL       • Cassandra Query Language \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-38-728.jpg?cb=1317900768\" title=\"Inserting Data INSERT INTO users (KEY, “name”, “age”)     V...\" target=\"_blank\">\n        38.\n      </a>\n    Inserting Data INSERT INTO users (KEY, “name”, “age”)     VALUES (“thobbs”, “Tyler”, 24); \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-39-728.jpg?cb=1317900768\" title=\"Updating Data Updates are the same as inserts: INSERT INTO ...\" target=\"_blank\">\n        39.\n      </a>\n    Updating Data Updates are the same as inserts: INSERT INTO users (KEY, “age”)     VALUES (“thobbs”, 34); Or UPDATE users SET “age” = 34     WHERE KEY = “thobbs”; \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-40-728.jpg?cb=1317900768\" title=\"Fetching Data Whole row select: SELECT * FROM users WHERE K...\" target=\"_blank\">\n        40.\n      </a>\n    Fetching Data Whole row select: SELECT * FROM users WHERE KEY = “thobbs”; \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-41-728.jpg?cb=1317900768\" title=\"Fetching Data Explicit column select: SELECT “name”, “age” ...\" target=\"_blank\">\n        41.\n      </a>\n    Fetching Data Explicit column select: SELECT “name”, “age” FROM users     WHERE KEY = “thobbs”; \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-42-728.jpg?cb=1317900768\" title=\"Fetching Data Get a slice of columns UPDATE letters SET 1=a...\" target=\"_blank\">\n        42.\n      </a>\n    Fetching Data Get a slice of columns UPDATE letters SET 1=a, 2=b, 3=c, 4=d, 5=e     WHERE KEY = “key”; SELECT 1..3 FROM letters WHERE KEY = “key”; Returns [(1, a), (2, b), (3, c)] \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-43-728.jpg?cb=1317900768\" title=\"Fetching Data Get a slice of columns SELECT FIRST 2 FROM le...\" target=\"_blank\">\n        43.\n      </a>\n    Fetching Data Get a slice of columns SELECT FIRST 2 FROM letters WHERE KEY = “key”; Returns [(1, a), (2, b)] SELECT FIRST 2 REVERSED FROM letters     WHERE KEY = “key”; Returns [(5, e), (4, d)] \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-44-728.jpg?cb=1317900768\" title=\"Fetching Data Get a slice of columns SELECT 3.. FROM letter...\" target=\"_blank\">\n        44.\n      </a>\n    Fetching Data Get a slice of columns SELECT 3.. FROM letters WHERE KEY = “key”; Returns [(3, c), (4, d), (5, e)] SELECT FIRST 2 REVERSED 4.. FROM letters     WHERE KEY = “key”; Returns [(4, d), (3, c)] \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-45-728.jpg?cb=1317900768\" title=\"Deleting Data Delete a whole row: DELETE FROM users WHERE K...\" target=\"_blank\">\n        45.\n      </a>\n    Deleting Data Delete a whole row: DELETE FROM users WHERE KEY = “thobbs”; Delete specific columns: DELETE “age” FROM users     WHERE KEY = “thobbs”; \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-46-728.jpg?cb=1317900768\" title=\"Secondary Indexes Builtin basic indexes CREATE INDEX ageInd...\" target=\"_blank\">\n        46.\n      </a>\n    Secondary Indexes Builtin basic indexes CREATE INDEX ageIndex ON users (age); SELECT name FROM USERS     WHERE age = 24 AND state = “TX”; \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-47-728.jpg?cb=1317900768\" title=\"Performance    Writes    – 10k – 30k per second per node  ...\" target=\"_blank\">\n        47.\n      </a>\n    Performance    Writes    – 10k – 30k per second per node    – Sub-millisecond latency    Reads    – 1k – 10k per second per node    – Depends on data set, caching    – Usually 0.1 to 10ms latency \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-48-728.jpg?cb=1317900768\" title=\"Other Features    Distributed Counters    – Can support mi...\" target=\"_blank\">\n        48.\n      </a>\n    Other Features    Distributed Counters    – Can support millions of high-volume counters    Excellent Multi-datacenter Support    – Disaster recovery    – Locality    Hadoop Integration    – Isolation of resources    – Hive and Pig drivers    Compression \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-49-728.jpg?cb=1317900768\" title=\"What Cassandra Cant Do    Transactions    – Unless you use...\" target=\"_blank\">\n        49.\n      </a>\n    What Cassandra Cant Do    Transactions    – Unless you use a distributed lock    – Atomicity, Isolation    – These arent needed as often as youd think    Limited support for ad-hoc queries    – Know what you want to do with the data \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-50-728.jpg?cb=1317900768\" title=\"Not One-size-fits-all    Use alongside an RDBMS    – Use t...\" target=\"_blank\">\n        50.\n      </a>\n    Not One-size-fits-all    Use alongside an RDBMS    – Use the RDBMS for highly-transactional or highly-      relational data       • Usually a small set of data    – Let Cassandra scale to handle the rest \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-51-728.jpg?cb=1317900768\" title=\"Language Support    Good:    – Java    – Python    – Ruby ...\" target=\"_blank\">\n        51.\n      </a>\n    Language Support    Good:    – Java    – Python    – Ruby    – PHP    – C#    Coming Soon:    – Everything else, now that we have CQL \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/introtocassandra-111006113105-phpapp02/95/intro-to-cassandra-52-728.jpg?cb=1317900768\" title=\"Questions?          Tyler Hobbs               @tylhobbs    ...\" target=\"_blank\">\n        52.\n      </a>\n    Questions?          Tyler Hobbs               @tylhobbs       tyler@datastax.com \n  </li>\n              </ol></div></div></div><aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\">\n<dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\"><ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"9577764\" title=\"Visual Thinking Strategies\" href=\"https://www.linkedin.com/learning/visual-thinking-strategies?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Visual Thinking Strategies\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Visual Thinking Strategies\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=wkPTuURgO3KQy5upy7KCUCeYyHI%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-gUySu_NCfYX_ofcHaZLSioVkTcSkHkAA1euqpQTnjFI69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Visual Thinking Strategies</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"9577764\" title=\"Creative Inspirations: Duarte Design, Presentation Design Studio\" href=\"https://www.linkedin.com/learning/creative-inspirations-duarte-design-presentation-design-studio?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Creative Inspirations: Duarte Design, Presentation Design Studio\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Creative Inspirations: Duarte Design, Presentation Design Studio\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=gN0XHJkdhuJ3Fk75H6mdK0VF9ag%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-kWiGr-MqFYXPoe9ref_qougdWLw\" /></div>\n    <div class=\"lynda-content\"><p>Creative Inspirations: Duarte Design, Presentation Design Studio</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"9577764\" title=\"Gaining Skills with LinkedIn Learning\" href=\"https://www.linkedin.com/learning/gaining-skills-with-linkedin-learning?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Gaining Skills with LinkedIn Learning\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Gaining Skills with LinkedIn Learning\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=dTAVDscYes1J4Ec%2F4ONSWvNwSnk%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-kWyai-9SfZXfqccbeZLSiolwWfy8JlQEyfuisRznmEY69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Gaining Skills with LinkedIn Learning</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"52666082\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"SC 2015: Thinking Fast and Slow with Software Development\" href=\"https://www.slideshare.net/dbryant_uk/sc-2015-thinking-fast-and-slow-with-software-development\">\n    \n    <div class=\"related-content\"><p>SC 2015: Thinking Fast and Slow with Software Development</p><p>Daniel Bryant</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"9227788\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Cassandra for Python Developers\" href=\"https://www.slideshare.net/tylerhobbs/cassandra-for-python-developers\">\n    \n    <div class=\"related-content\"><p>Cassandra for Python Developers</p><p>Tyler Hobbs</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"52148495\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Detect all memory leaks with LeakCanary!\" href=\"https://www.slideshare.net/pyricau/detect-all-memory-leaks-with-leakcanary-52148495\">\n    \n    <div class=\"related-content\"><p>Detect all memory leaks with LeakCanary!</p><p>Pierre-Yves Ricau</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"53477115\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"How Yelp Uses Sensu to Monitor Services in a SOA World\" href=\"https://www.slideshare.net/solarkennedy/how-yelp-uses-sensu-to-monitor-services-in-a-soa-world\">\n    \n    <div class=\"related-content\"><p>How Yelp Uses Sensu to Monitor Services in a SOA World</p><p>Kyle Anderson</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"54912236\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Evolving the Netflix API\" href=\"https://www.slideshare.net/KatharinaProbst/evolving-the-netflix-api\">\n    \n    <div class=\"related-content\"><p>Evolving the Netflix API</p><p>Katharina Probst</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"36153305\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Datomic – A Modern Database - StampedeCon 2014\" href=\"https://www.slideshare.net/StampedeCon/datomic-a-modern-database-stampedecon-2014\">\n    \n    <div class=\"related-content\"><p>Datomic – A Modern Database - StampedeCon 2014</p><p>StampedeCon</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"48640688\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"7 Common Mistakes in Go (2015)\" href=\"https://www.slideshare.net/spf13/7-common-mistakes-in-go-2015\">\n    \n    <div class=\"related-content\"><p>7 Common Mistakes in Go (2015)</p><p>Steven Francia</p></div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div></div><footer>\n          <div class=\"row\"><div class=\"columns\"><ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div></div>\n          \n          <div class=\"row\"><div class=\"columns\"><p class=\"copyright text-center\">LinkedIn Corporation © 2018</p></div></div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div>\n    <div class=\"row\"><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></div>\n        \n    </div>\n    \n    \n      <noscript>\n    </noscript>",
        "created_at": "2018-10-05T11:11:54+0000",
        "updated_at": "2018-10-05T11:12:01+0000",
        "published_at": null,
        "published_by": [
          "Tyler Hobbs"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 6,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/introtocassandra-111006113105-phpapp02-thumbnail-4.jpg?cb=1317900768",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12320"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12319,
        "uid": null,
        "title": "Optimizing Cassandra Performance: Sometimes Two Writes Are Better Than One | SignalFx",
        "url": "https://www.signalfx.com/blog/optimizing-cassandra-performance-sometimes-two-writes-better-one/",
        "content": "<p>SignalFx is a <a href=\"https://www.signalfx.com/infrastructure-monitoring/\" target=\"_blank\">modern monitoring</a> service that ingests, stores and performs <a href=\"https://www.signalfx.com/why-signalfx/analytics/\" target=\"_blank\">real-time streaming analytics</a> on high-volume, high-resolution metric data from companies all over the world.</p><p>Providing real-time streaming analytics means that we ingest tens of billions of points of time series data per day, and we give our <a href=\"https://www.signalfx.com/customers/\" target=\"_blank\">customers</a> the capability to send data at one second resolution. All of this data ends up in <a href=\"https://www.signalfx.com/cassandra-monitoring\" target=\"_blank\">Cassandra</a>, which we use as the backend of our time series database (or TSDB).</p><p>We chose Cassandra for scalability and read and write performance at extremely high load. For operational efficiency, we’ve gone through multiple stages of performance optimization. And we came to the counterintuitive conclusion: sometimes two writes perform better than one.</p><h3>Measuring Overall Performance</h3><p>We built a test environment with a load simulator to measure the difference in Cassandra performance as we moved through each optimization stage. For a constant simulated load measured in data points per second, we monitored and measured:</p><ul><li>Write volume per second</li>\n<li>Write latency in milliseconds</li>\n<li>Host CPU load utilization</li>\n<li>Host disk writes in bytes per second</li>\n</ul><p>We compared Cassandra performance across these metrics as we transitioned from one stage to the next and show these comparisons in a handful of key charts.</p><h3>Stage 1: Vertical Writes</h3><p>Our Cassandra schema is what you would expect. Each data point consists of a key name (or a time series ID key), a timestamp and a value for that timestamp. Each time series has its own row in the table, and we create tables representing distinct time ranges.</p><div style=\"text-align: center;\">\n<table><tbody><tr><td>\n<h6>TSDB Schema</h6>\n</td>\n</tr><tr><td>\n<pre>CREATE TABLE table_0 (&#13;\n  timeseries text&#13;\n  time timestamp,&#13;\n  value blob,&#13;\n  PRIMARY KEY (timeseries, time)&#13;\n) WITH COMPACT STORAGE;&#13;\n</pre>\n</td>\n</tr></tbody></table></div><p>As each datapoint comes in, we write it to the appropriate row and column for that time series.</p><p><img class=\"aligncenter wp-image-6618 size-full\" src=\"https://www.signalfx.com/wp-content/uploads/Cassandra-1.gif\" alt=\"Cassandra 1\" width=\"480\" height=\"268\" /></p><p>It turns out that writing each data point individually is very expensive. In other words, touching every row, every second is a very expensive load pattern. So we decided to buffer data in memory and write multiple points for each time series in a single batch statement.</p><h3>Stage 2: Buffered Writes</h3><p>In this version of the ingest system, we will write new data into a memory-tier. A migrator process will periodically read data from the memory-tier, write it to Cassandra, and, once it’s safely in Cassandra, remove it from the memory-tier. In other words, the migrator picks up a time range of data and moves it as a batch into Cassandra.</p><p>The TSDB is now effectively two-tiered. The memory-tier is essentially a higher performance backend for the most recent data. It knows whether data for a specific time range belongs in the memory-tier or on Cassandra, and therefore routes reads and writes appropriately.</p><p><img class=\"aligncenter wp-image-6620 size-full\" src=\"https://www.signalfx.com/wp-content/uploads/Cassandra-2.gif\" alt=\"Cassandra 2\" width=\"480\" height=\"266\" /></p><p>There are two independent operations here:</p><ol><li>New points are being ingested on the right side (same as the non-buffered Cassandra case)</li>\n<li>Batches of points are being written to Cassandra on the left side</li>\n</ol><p>The buffered writes performance shows improvements in efficiency for Cassandra compared to vertical writes. While the write pattern is choppier, Cassandra is doing many fewer writes as the previous stage. These writes are larger, but the host CPU utilization decreases significantly.</p><p><img class=\"aligncenter wp-image-6621\" src=\"https://www.signalfx.com/wp-content/uploads/Cassandra-stage-1-to-2.png\" alt=\"Cassandra stage 1 to 2\" width=\"750\" height=\"354\" srcset=\"https://www.signalfx.com/wp-content/uploads/Cassandra-stage-1-to-2.png 975w, https://www.signalfx.com/wp-content/uploads/Cassandra-stage-1-to-2-300x142.png 300w, https://www.signalfx.com/wp-content/uploads/Cassandra-stage-1-to-2-500x236.png 500w, https://www.signalfx.com/wp-content/uploads/Cassandra-stage-1-to-2-552x260.png 552w\" /></p><p>So buffering was an improvement to performance, although there was more that we can do. In this stage, writing data point-by-point means that a column is created for each data point and this has implications for storage overhead in Cassandra.</p><h3>Stage 3: Packed Writes</h3><p>The next optimization stage is to have the migrator pack the contents of each batch of points into a single block which it writes to Cassandra. This reduces the number of columns and write operations in each row, which has a larger benefit for storage than CPU.</p><p><img class=\"aligncenter wp-image-6622 size-full\" src=\"https://www.signalfx.com/wp-content/uploads/Cassandra-3.gif\" alt=\"Cassandra 3\" width=\"480\" height=\"266\" /></p><p>This packed write operation is essentially the same as the previous, buffered case. However, we are writing fewer, bigger objects to Cassandra and the write rate has dropped tremendously. Latency also improves with fewer writes while data writes per disk also drops as blocks are more compact.</p><p><img class=\"aligncenter wp-image-6680\" src=\"https://www.signalfx.com/wp-content/uploads/Cassandra-2-to-3-1-300x142.png\" alt=\"\" width=\"750\" height=\"354\" srcset=\"https://www.signalfx.com/wp-content/uploads/Cassandra-2-to-3-1-300x142.png 300w, https://www.signalfx.com/wp-content/uploads/Cassandra-2-to-3-1-500x236.png 500w, https://www.signalfx.com/wp-content/uploads/Cassandra-2-to-3-1-552x260.png 552w, https://www.signalfx.com/wp-content/uploads/Cassandra-2-to-3-1.png 975w\" /></p><h3>Stage 4: Persistent Logs</h3><p>While the above changes represent significant performance improvements, they introduce a big problem: if a memory-tier server crashes, we lose all of the buffered data. Obviously that is not acceptable. We’ll solve that by writing data to Cassandra as we receive it. Why is that a good idea now when it performed so poorly before?</p><p>The answer is that we use a more favorable write pattern for Cassandra. The schema has row for each timestamp. Data points arriving at the same time for different time series typically have similar timestamps. Therefore, we can write these data points across a small number of rows in Cassandra instead of a row per time series as we did originally.</p><div style=\"text-align: center;\">\n<table><tbody><tr><td>\n<h6>TSDB Schema</h6>\n</td>\n<td>                         </td>\n<td>\n<h6>Log Schema</h6>\n</td>\n</tr><tr><td>\n<pre>CREATE TABLE table_0 (&#13;\n  timeseries text&#13;\n  time timestamp,&#13;\n  value blob,&#13;\n  PRIMARY KEY (timeseries, time)&#13;\n) WITH COMPACT STORAGE;&#13;\n</pre>\n</td>\n<td> </td>\n<td>\n<pre>CREATE TABLE table_0 (&#13;\n  stamp text,&#13;\n  sequence bigint,&#13;\n  value blob,&#13;\n  PRIMARY KEY (stamp, sequence)&#13;\n) WITH COMPACT STORAGE;&#13;\n</pre>\n</td>\n</tr></tbody></table></div><p>We write data points for different time series in the order they arrive so that we can get them onto persistent storage as quickly as possible. Because this order of arrival is effectively non-deterministic, there’s no efficient way to retrieve a datapoint for a specific time series.</p><p>This is not a problem as we only read this data when we need to construct the memory-tier of the ingest server; we do random reads from the memory-tier.</p><p><img class=\"aligncenter wp-image-6629 size-full\" src=\"https://www.signalfx.com/wp-content/uploads/Cassandra-4.gif\" alt=\"Cassandra 4\" width=\"480\" height=\"266\" /></p><p>The data is migrated from the memory-tier just as before. Once it’s been migrated, we can also remove the log data from Cassandra simply by truncating the table in which we store it.</p><p><img class=\"aligncenter wp-image-6630\" src=\"https://www.signalfx.com/wp-content/uploads/Cassandra-3-to-4.png\" alt=\"Cassandra 3 to 4\" width=\"750\" height=\"354\" srcset=\"https://www.signalfx.com/wp-content/uploads/Cassandra-3-to-4.png 975w, https://www.signalfx.com/wp-content/uploads/Cassandra-3-to-4-300x142.png 300w, https://www.signalfx.com/wp-content/uploads/Cassandra-3-to-4-500x236.png 500w, https://www.signalfx.com/wp-content/uploads/Cassandra-3-to-4-552x260.png 552w\" /></p><p>With this process, there are clearly more write operations and an increase in disk I/O. However there are no adverse effects on write latency or on CPU load and, of course, our data is protected from a crash.</p><h3>Ongoing Optimizations</h3><p>We’ve learned a lot about how to incrementally improve Cassandra performance based on these optimization stages. Our analysis shows that CPU load utilization is much more dependent on the rate of writes than on the volume of data being written. For our very write-heavy workload, we saw a very large efficiency improvement by doing fewer, larger writes. This lets us get much better utilization from our Cassandra cluster.</p><hr /><hr />",
        "created_at": "2018-10-05T11:07:28+0000",
        "updated_at": "2018-10-05T11:07:37+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 5,
        "domain_name": "www.signalfx.com",
        "preview_picture": "https://www.signalfx.com/wp-content/uploads/integrations_cassandra@4x.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12319"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12318,
        "uid": null,
        "title": "New Blog Series: Cassandra - What You May Learn the Hard Way - OpenCredo",
        "url": "https://opencredo.com/new-blog-cassandra-what-you-may-learn-the-hard-way/",
        "content": "<p><img class=\"wp-image-28753 size-medium aligncenter\" src=\"https://opencredo.com/wp-content/uploads/2016/08/cassandra-400x92.png\" alt=\"Cassandra\" width=\"400\" height=\"92\" srcset=\"https://opencredo.com/wp-content/uploads/2016/08/cassandra-400x92.png 400w, https://opencredo.com/wp-content/uploads/2016/08/cassandra-200x46.png 200w, https://opencredo.com/wp-content/uploads/2016/08/cassandra.png 499w\" /></p><p>At OpenCredo we have been working with Cassandra since 2012 and we are big fans of both open source Apache Cassandra and the capabilities of DataStax Enterprise. Over the years we have collected a great deal of experience throughout the company on how to deliver the benefits of Cassandra in real world projects and have also seen some common pitfalls that businesses have fallen into. </p><p>In order to help more people enjoy success with Cassandra we have decided to put together a series of articles to help people who are adopting Cassandra get the kind of results they expect whilst navigating around some of the common issues. <strong>The conclusion of the series will be a webinar exploring some of these issues with the opportunity at the end to put questions to our consultants in a Question and Answer session. <a href=\"https://opencredo.com/cassandra-good-bad-ugly-webinar-recording/\">(View the recording here) </a></strong></p><p>In our first article we will explore at a high level the strengths and limitations of Cassandra before moving on to more detailed technical ideas as well as looking deeper into issues that can have negative consequences if they aren’t well understood or accounted for at the outset of an initiative. The nature of some of the issues that we have seen are such that often the problems they cause only surface late on in projects where corrective measures are more costly to resolve. So we hope sharing this series of articles and hence our experience, will help many people avoid potential issues in the future. </p><p>Our first article by <a href=\"https://opencredo.com/people/guy-richardson/\">Guy Richardson</a> “<a href=\"https://opencredo.com/fulfilling-promise-apache-cassandra/\"><b>Fulfilling the promise of Apache Cassandra</b></a>” discusses the promises and limitations of Cassandra before then exploring when and in what environments one should consider using Cassandra.</p><p><a href=\"https://opencredo.com/?p=28541\">Alla Babkina</a> then explores data modelling in a two part series “<a href=\"https://opencredo.com/cassandra-data-modelling-patterns/\"><strong>Patterns of Successful Cassandra Data Modelling</strong></a>”. Alla provides some patterns which can avoid, as she puts it, a “whole world of hurt as your application starts to scale”. By reference to the way Cassandra stores its data she explains patterns that are sympathetic to the underlying storage.</p><p>The third article in the series will explore a common problem which is that the SQL like nature of CQL can lead developers to treat Cassandra like a relational database. In his article “<a href=\"https://opencredo.com/how-not-to-use-cassandra-like-an-rdbms-and-what-will-happen-if-you-do/\"><b>How Not To Use Cassandra like an RDBMS (and what will happen if you do)</b></a>” <a href=\"https://opencredo.com/?p=50\">Dominic Fox</a> provides detailed examples of usage that is more RDBMS and explains “where a beginner might be unpleasantly surprised by the differences”. He also shares his advice that “if Cassandra doesn’t immediately allow you to do something, it’s worth making sure you understand why”.</p><p>In the final article of the series, “<b><a href=\"https://opencredo.com/cassandra-tombstones-common-issues/\">Common Problems with Cassandra Tombstones</a></b>” by <a href=\"https://opencredo.com/?p=28541\">Alla Babkina</a> will explore how tombstones are created, answering questions such as “We are not deleting anything, how are we getting tombstones?”. Before then exploring how to get a better understanding of tombstones in your Cassandra cluster by looking at SSTable content where necessary. </p><h4>The webinar took place on October 6th, 2016. To view a recording click <a href=\"https://opencredo.com/cassandra-good-bad-ugly-webinar-recording/\">here.</a></h4><h4><a class=\"button\" href=\"#text_icl-6\">Sign up to receive updates via email</a></h4>",
        "created_at": "2018-10-05T11:07:05+0000",
        "updated_at": "2018-10-05T11:07:12+0000",
        "published_at": "2016-08-26T14:45:22+0000",
        "published_by": [
          "Jonas Partner"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 2,
        "domain_name": "opencredo.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12318"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12317,
        "uid": null,
        "title": "twissandra/twissandra",
        "url": "https://github.com/twissandra/twissandra",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>Twissandra is an example project, created to learn and demonstrate how to use\nCassandra.  Running the project will present a website that has similar\nfunctionality to Twitter.</p>\n<p>Most of the magic happens in twissandra/cass.py, so check that out.</p>\n<h2><a id=\"user-content-installation\" class=\"anchor\" aria-hidden=\"true\" href=\"#installation\"></a>Installation</h2>\n<p>Installing Twissandra is fairly straightforward.  Really it just involves\nchecking out Cassandra and Twissandra, doing a little configuration, and\nthen starting it up.  Here's a roadmap of the steps we're going to take to\ninstall the project:</p>\n<ol><li>Check out the Twissandra source code</li>\n<li>Install and configure Cassandra</li>\n<li>Create a virtual Python environment with Twissandra's dependencies</li>\n<li>Start up the webserver</li>\n</ol><h3><a id=\"user-content-check-out-the-twissandra-source-code\" class=\"anchor\" aria-hidden=\"true\" href=\"#check-out-the-twissandra-source-code\"></a>Check out the Twissandra source code</h3>\n<pre>git clone git://github.com/twissandra/twissandra.git\n</pre>\n<h3><a id=\"user-content-install-and-configure-cassandra\" class=\"anchor\" aria-hidden=\"true\" href=\"#install-and-configure-cassandra\"></a>Install and configure Cassandra</h3>\n<p>Follow the instructions for <a href=\"http://wiki.apache.org/cassandra/GettingStarted\" rel=\"nofollow\">installing and setting up Cassandra</a>.\nNote, Twissandra requires <strong>at-least Cassandra 2.0</strong> or later.</p>\n<p>And then make sure Cassandra is running:</p>\n<pre>bin/cassandra -f\n</pre>\n<h3><a id=\"user-content-create-a-virtual-python-environment-with-twissandras-dependencies\" class=\"anchor\" aria-hidden=\"true\" href=\"#create-a-virtual-python-environment-with-twissandras-dependencies\"></a>Create a virtual Python environment with Twissandra's dependencies</h3>\n<p>First, make sure to have virtualenv installed.  If it isn't installed already,\nthis should do the trick:</p>\n<pre>sudo easy_install -U virtualenv\n</pre>\n<p>Now let's create a new virtual environment, and begin using it:</p>\n<pre>virtualenv twiss\nsource twiss/bin/activate\n</pre>\n<p>We should install pip, so that we can more easily install Twissandra's\ndependencies into our new virtual environment:</p>\n<pre>easy_install -U pip\n</pre>\n<p>Now let's install all of the dependencies:</p>\n<pre>pip install -U -r twissandra/requirements.txt\n</pre>\n<p>Now that we've got all of our dependencies installed, we're ready to start up\nthe server.</p>\n<h3><a id=\"user-content-create-the-schema\" class=\"anchor\" aria-hidden=\"true\" href=\"#create-the-schema\"></a>Create the schema</h3>\n<p>Make sure you're in the Twissandra checkout, and then run the sync_cassandra\ncommand to create the proper keyspace in Cassandra:</p>\n<pre>python manage.py sync_cassandra\n</pre>\n<h3><a id=\"user-content-start-up-the-webserver\" class=\"anchor\" aria-hidden=\"true\" href=\"#start-up-the-webserver\"></a>Start up the webserver</h3>\n<p>This is the fun part! We're done setting everything up, we just need to run it:</p>\n<pre>python manage.py runserver\n</pre>\n<p>Now go to <a href=\"http://127.0.0.1:8000/\" rel=\"nofollow\">http://127.0.0.1:8000/</a> and you can play with Twissandra!</p>\n<h2><a id=\"user-content-schema-layout\" class=\"anchor\" aria-hidden=\"true\" href=\"#schema-layout\"></a>Schema Layout</h2>\n<p>In Cassandra, the way that your data is structured is very closely tied to how\nhow it will be retrieved.  Let's start with the 'users' table. The key is\nthe username, and the remaining columns are properties on the user:</p>\n<pre>CREATE TABLE users (\n    username text PRIMARY KEY,\n    password text\n)\n</pre>\n<p>The 'friends' and 'followers' tables have a compound primary key. The first\ncomponent, the \"partition key\", controls how the data is spread around the\ncluster.  The second component, the \"clustering key\", controls how the data\nis sorted on disk.  In this case, the sort order isn't very interesting,\nbut what's important is that all friends and all followers of a user will be\nstored contiguously on disk, making a query to lookup all friends or followers\nof a user very efficient.</p>\n<pre>CREATE TABLE friends (\n    username text,\n    friend text,\n    since timestamp,\n    PRIMARY KEY (username, friend)\n)\nCREATE TABLE followers (\n    username text,\n    follower text,\n    since timestamp,\n    PRIMARY KEY (username, follower)\n)\n</pre>\n<p>Tweets are stored with a UUID for the key.</p>\n<pre>CREATE TABLE tweets (\n    tweet_id uuid PRIMARY KEY,\n    username text,\n    body text\n)\n</pre>\n<p>The 'timeline' and 'userline' tables keep track of what tweets were\nmade and in what order.  To acheive this, we use a TimeUUID for the\nclustering key, resulting in tweets being stored in chronological\norder.  The \"WITH CLUSERING ORDER\" option just means that the\ntweets will be stored in reverse chronological order (newest first),\nwhich is slightly more efficient for the queries we'll be performing.</p>\n<pre>CREATE TABLE userline (\n    username text,\n    time timeuuid,\n    tweet_id uuid,\n    PRIMARY KEY (username, time)\n) WITH CLUSTERING ORDER BY (time DESC)\nCREATE TABLE timeline (\n    username text,\n    time timeuuid,\n    tweet_id uuid,\n    PRIMARY KEY (username, time)\n) WITH CLUSTERING ORDER BY (time DESC)\n</pre>\n<h2><a id=\"user-content-fake-data-generation\" class=\"anchor\" aria-hidden=\"true\" href=\"#fake-data-generation\"></a>Fake data generation</h2>\n<p>For testing purposes, you can populate the database with some fake tweets.</p>\n<pre>python manage.py fake_data &lt;num_users&gt; &lt;max_tweets&gt;\n</pre>\n<p><code>num_users</code> is the total number of users to generate and <code>max_tweets</code> is the\nmaximum number of tweets per user. The number of tweets per user is determined\nby the Pareto distribution so the number of tweets actually generated will vary\nbetween runs.</p>\n</article>",
        "created_at": "2018-10-05T11:01:42+0000",
        "updated_at": "2018-10-05T11:01:47+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "github.com",
        "preview_picture": "https://avatars3.githubusercontent.com/u/587189?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12317"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1367,
            "label": "scaling",
            "slug": "scaling"
          }
        ],
        "is_public": false,
        "id": 12316,
        "uid": null,
        "title": "How Discord Stores Billions of Messages",
        "url": "https://blog.discordapp.com/how-discord-stores-billions-of-messages-7fa6ec7ee4c7?gi=bd1823f7e603",
        "content": "<div class=\"section-inner sectionLayout--fullWidth\"><figure id=\"7660\" class=\"graf graf--figure graf--layoutFillWidth graf-after--h3\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*8eA5bgAG9NmCJshhccMhZQ.png\" data-width=\"2500\" data-height=\"900\" src=\"https://cdn-images-1.medium.com/max/2000/1*8eA5bgAG9NmCJshhccMhZQ.png\" alt=\"image\" /></div></figure></div><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"d1b5\" class=\"graf graf--p graf-after--figure\">Discord continues to grow faster than we expected and so does our user-generated content. With more users comes more chat messages. In July, <a href=\"https://blog.discordapp.com/11-million-players-in-one-year/\" data-href=\"https://blog.discordapp.com/11-million-players-in-one-year/\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">we announced 40 million messages a day</a>, in December <a href=\"http://venturebeat.com/2016/12/08/discord-hits-25-million-users-and-releases-gamebridge-sdk-for-its-voice-chat/\" data-href=\"http://venturebeat.com/2016/12/08/discord-hits-25-million-users-and-releases-gamebridge-sdk-for-its-voice-chat/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">we announced 100 million</a>, and as of this blog post we are well past 120 million. We decided early on to store all chat history forever so users can come back at any time and have their data available on any device. This is a lot of data that is ever increasing in velocity, size, and must remain available. <em class=\"markup--em markup--p-em\">How do we do it? Cassandra!</em></p><h3 id=\"8d4c\" class=\"graf graf--h3 graf-after--p\">What we were doing</h3><p id=\"37d6\" class=\"graf graf--p graf-after--h3\">The original version of Discord was built in just under two months in early 2015. Arguably, one of the best databases for iterating quickly is MongoDB. Everything on Discord was stored in a single MongoDB replica set and this was intentional, but we also planned everything for easy migration to a new database (we knew we were not going to use MongoDB sharding because it is complicated to use and not known for stability). This is actually part of our company culture: build quickly to prove out a product feature, but always with a path to a more robust solution.</p><p id=\"05e8\" class=\"graf graf--p graf-after--p\">The messages were stored in a MongoDB collection with a single compound index on <code class=\"markup--code markup--p-code\">channel_id</code> and <code class=\"markup--code markup--p-code\">created_at</code>. Around November 2015, we reached 100 million stored messages and at this time we started to see the expected issues appearing: the data and the index could no longer fit in RAM and latencies started to become unpredictable. It was time to migrate to a database more suited to the task.</p><h3 id=\"d9b8\" class=\"graf graf--h3 graf-after--p\">Choosing the Right Database</h3><p id=\"6a39\" class=\"graf graf--p graf-after--h3\">Before choosing a new database, we had to understand our read/write patterns and why we were having problems with our current solution.</p><ul class=\"postList\"><li id=\"e2a4\" class=\"graf graf--li graf-after--p\">It quickly became clear that our reads were extremely random and our read/write ratio was about 50/50.</li><li id=\"4e27\" class=\"graf graf--li graf-after--li\">Voice chat heavy Discord servers send almost no messages. This means they send a message or two every few days. In a year, this kind of server is unlikely to reach 1,000 messages. The problem is that even though this is a small amount of messages it makes it harder to serve this data to the users. Just returning 50 messages to a user can result in many random seeks on disk causing disk cache evictions.</li><li id=\"0c8c\" class=\"graf graf--li graf-after--li\">Private text chat heavy Discord servers send a decent number of messages, easily reaching between 100 thousand to 1 million messages a year. The data they are requesting is usually very recent only. The problem is since these servers usually have under 100 members the rate at which this data is requested is low and unlikely to be in disk cache.</li><li id=\"415e\" class=\"graf graf--li graf-after--li\">Large public Discord servers send a lot of messages. They have thousands of members sending thousands of messages a day and easily rack up millions of messages a year. They almost always are requesting messages sent in the last hour and they are requesting them often. Because of that the data is usually in the disk cache.</li><li id=\"e87c\" class=\"graf graf--li graf-after--li\">We knew that in the coming year we would add even more ways for users to issue random reads: the ability to view your mentions for the last 30 days then jump to that point in history, viewing plus jumping to pinned messages, and full-text search. <em class=\"markup--em markup--li-em\">All of this spells more random reads!!</em></li></ul><p id=\"2d85\" class=\"graf graf--p graf-after--li\">Next we defined our requirements:</p><ul class=\"postList\"><li id=\"c0e2\" class=\"graf graf--li graf-after--p\"><strong class=\"markup--strong markup--li-strong\">Linear scalability — </strong>We do not want to reconsider the solution later or manually re-shard the data.</li><li id=\"9a9a\" class=\"graf graf--li graf-after--li\"><strong class=\"markup--strong markup--li-strong\">Automatic failover — </strong>We love sleeping at night and build Discord to self heal as much as possible.</li><li id=\"a6ca\" class=\"graf graf--li graf-after--li\"><strong class=\"markup--strong markup--li-strong\">Low maintenance — </strong>It should just work once we set it up. We should only have to add more nodes as data grows.</li><li id=\"b5ea\" class=\"graf graf--li graf-after--li\"><strong class=\"markup--strong markup--li-strong\">Proven to work — </strong>We love trying out new technology, but not too new.</li><li id=\"8a1c\" class=\"graf graf--li graf-after--li\"><strong class=\"markup--strong markup--li-strong\">Predictable performance</strong> <strong class=\"markup--strong markup--li-strong\">— </strong>We have alerts go off when our API’s response time 95th percentile goes above 80ms. We also do not want to have to cache messages in Redis or Memcached.</li><li id=\"10c1\" class=\"graf graf--li graf-after--li\"><strong class=\"markup--strong markup--li-strong\">Not a blob store — </strong>Writing thousands of messages per second would not work great if we had to constantly deserialize blobs and append to them.</li><li id=\"cfd9\" class=\"graf graf--li graf-after--li\"><strong class=\"markup--strong markup--li-strong\">Open source — </strong>We believe in controlling our own destiny and don’t want to depend on a third party company.</li></ul><p id=\"87e5\" class=\"graf graf--p graf-after--li\">Cassandra was the only database that fulfilled all of our requirements. We can just add nodes to scale it and it can tolerate a loss of nodes without any impact on the application. Large companies such as Netflix and Apple have thousands of Cassandra nodes. Related data is stored contiguously on disk providing minimum seeks and easy distribution around the cluster. It’s backed by DataStax, but still open source and community driven.</p><p id=\"024a\" class=\"graf graf--p graf-after--p\">Having made the choice, we needed to prove that it would actually work.</p><h3 id=\"7d98\" class=\"graf graf--h3 graf-after--p\">Data Modeling</h3><p id=\"aea7\" class=\"graf graf--p graf-after--h3\">The best way to describe Cassandra to a newcomer is that it is a KKV store. The two Ks comprise the primary key. The first K is the partition key and is used to determine which node the data lives on and where it is found on disk. The partition contains multiple rows within it and a row within a partition is identified by the second K, which is the clustering key. The clustering key acts as both a primary key within the partition and how the rows are sorted. You can think of a partition as an ordered dictionary. These properties combined allow for very powerful data modeling.</p><p id=\"1f84\" class=\"graf graf--p graf-after--p\">Remember that messages were indexed in MongoDB using <code class=\"markup--code markup--p-code\">channel_id</code> and <code class=\"markup--code markup--p-code\">created_at</code>? <code class=\"markup--code markup--p-code\">channel_id</code> became the partition key since all queries operate on a channel, but <code class=\"markup--code markup--p-code\">created_at</code> didn’t make a great clustering key because two messages can have the same creation time. Luckily every ID on Discord is actually a <a href=\"https://blog.twitter.com/2010/announcing-snowflake\" data-href=\"https://blog.twitter.com/2010/announcing-snowflake\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">Snowflake</a> (chronologically sortable), so we were able to use them instead. The primary key became <code class=\"markup--code markup--p-code\">(channel_id, message_id)</code>, where the <code class=\"markup--code markup--p-code\">message_id</code> is a Snowflake. This meant that when loading a channel we could tell Cassandra exactly where to range scan for messages.</p><p id=\"c72a\" class=\"graf graf--p graf-after--p\">Here is a simplified schema for our messages table (this omits about 10 columns).</p><figure id=\"a957\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"93f6\" class=\"graf graf--p graf-after--figure\">While Cassandra has schemas not unlike a relational database, they are cheap to alter and do not impose any temporary performance impact. We get the best of a blob store and a relational store.</p><p id=\"8f6b\" class=\"graf graf--p graf-after--p\">When we started importing existing messages into Cassandra we immediately began to see warnings in the logs telling us that partitions were found over 100MB in size. <em class=\"markup--em markup--p-em\">What gives?!</em> <em class=\"markup--em markup--p-em\">Cassandra advertises that it can support 2GB partitions!</em> Apparently, just because it can be done, it doesn’t mean it should. Large partitions put a lot of GC pressure on Cassandra during compaction, cluster expansion, and more. Having a large partition also means the data in it cannot be distributed around the cluster. It became clear we had to somehow bound the size of partitions because a single Discord channel can exist for years and perpetually grow in size.</p><p id=\"c176\" class=\"graf graf--p graf-after--p\">We decided to bucket our messages by time. We looked at the largest channels on Discord and determined if we stored about 10 days of messages within a bucket that we could comfortably stay under 100MB. Buckets had to be derivable from the <code class=\"markup--code markup--p-code\">message_id</code> or a timestamp.</p><figure id=\"c830\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"0952\" class=\"graf graf--p graf-after--figure\">Cassandra partition keys can be compounded, so our new primary key became <code class=\"markup--code markup--p-code\">((channel_id, bucket), message_id)</code>.</p><figure id=\"defd\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"1d29\" class=\"graf graf--p graf-after--figure\">To query for recent messages in the channel we generate a bucket range from current time to <code class=\"markup--code markup--p-code\">channel_id</code> (it is also a Snowflake and has to be older than the first message). We then sequentially query partitions until enough messages are collected. The downside of this method is that rarely active Discords will have to query multiple buckets to collect enough messages over time. In practice this has proved to be fine because for active Discords enough messages are usually found in the first partition and they are the majority.</p><p id=\"383e\" class=\"graf graf--p graf-after--p\">Importing messages into Cassandra went without a hitch and we were ready to try in production.</p><h3 id=\"7a52\" class=\"graf graf--h3 graf-after--p\">Dark Launch</h3><p id=\"724b\" class=\"graf graf--p graf-after--h3\">Introducing a new system into production is always scary so it’s a good idea to try to test it without impacting users. We setup our code to double read/write to MongoDB and Cassandra.</p><p id=\"f0e5\" class=\"graf graf--p graf-after--p\">Immediately after launching we started getting errors in our bug tracker telling us that <code class=\"markup--code markup--p-code\">author_id</code> was null. <em class=\"markup--em markup--p-em\">How can it be null? It is a required field!</em></p><h3 id=\"08bd\" class=\"graf graf--h3 graf-after--p\">Eventual Consistency</h3><p id=\"7964\" class=\"graf graf--p graf-after--h3\">Cassandra is an <a href=\"https://en.wikipedia.org/wiki/CAP_theorem\" data-href=\"https://en.wikipedia.org/wiki/CAP_theorem\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">AP</a> database which means it trades strong consistency for availability which is something we wanted. It is an anti-pattern to read-before-write (reads are more expensive) in Cassandra and therefore everything that Cassandra does is essentially an upsert even if you provide only certain columns. You can also write to any node and it will resolve conflicts automatically using “last write wins” semantics on a per column basis. <em class=\"markup--em markup--p-em\">So how did this bite us?</em></p></figure></figure></figure></div><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"13d2\" class=\"graf graf--p graf-after--figure\">In the scenario that a user edits a message at the same time as another user deletes the same message, we ended up with a row that was missing all the data except the primary key and the text since all Cassandra writes are upserts. There were two possible solutions for handling this problem:</p><ol class=\"postList\"><li id=\"930e\" class=\"graf graf--li graf-after--p\">Write the whole message back when editing the message. This had the possibility of resurrecting messages that were deleted and adding more chances for conflict for concurrent writes to other columns.</li><li id=\"67a9\" class=\"graf graf--li graf-after--li\">Figuring out that the message is corrupt and deleting it from database.</li></ol><p id=\"8441\" class=\"graf graf--p graf-after--li\">We went with the second option, which we did by choosing a column that was required (in this case <code class=\"markup--code markup--p-code\">author_id</code>) and deleting the message if it was null.</p><p id=\"7b03\" class=\"graf graf--p graf-after--p\">While solving this problem, we noticed we were being very inefficient with our writes. Since Cassandra is eventually consistent it cannot just delete data immediately. It has to replicate deletes to other nodes and do it even if other nodes are temporarily unavailable. Cassandra does this by treating deletes as a form of write called a “tombstone.” On read, it just skips over tombstones it comes across. Tombstones live for a configurable amount of time (10 days by default) and are permanently deleted during compaction when that time expires.</p><p id=\"8fca\" class=\"graf graf--p graf-after--p\">Deleting a column and writing null to a column are the exact same thing. They both generate a tombstone. Since all writes in Cassandra are upserts, that means you are generating a tombstone even when writing null for the first time. In practice, our entire message schema contains 16 columns, but the average message only has 4 values set. We were writing 12 tombstones into Cassandra most of the time for no reason. The solution to this was simple: only write non-null values to Cassandra.</p><h3 id=\"dbd1\" class=\"graf graf--h3 graf-after--p\">Performance</h3><p id=\"50d0\" class=\"graf graf--p graf-after--h3\">Cassandra is known to have faster writes than reads and we observed exactly that. Writes were sub-millisecond and reads were under 5 milliseconds. We observed this regardless of what data was being accessed, and performance stayed consistent during a week of testing. <em class=\"markup--em markup--p-em\">Nothing was surprising, we got exactly what we expected.</em></p><figure id=\"be55\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*MrdDaSA6ghOQQ7WyzqztcQ.png\" data-width=\"1634\" data-height=\"848\" data-action=\"zoom\" data-action-value=\"1*MrdDaSA6ghOQQ7WyzqztcQ.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*MrdDaSA6ghOQQ7WyzqztcQ.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\">Read/Write Latency via Datadog</figcaption></figure><p id=\"34d2\" class=\"graf graf--p graf-after--figure\">In line with fast, consistent read performance, here’s an example of a jump to a message from over a year ago in a channel with millions of messages:</p></div><div class=\"section-inner sectionLayout--insetColumn\"><h3 id=\"aa17\" class=\"graf graf--h3 graf-after--figure\">The Big Surprise</h3><p id=\"fd5c\" class=\"graf graf--p graf-after--h3\">Everything went smoothly, so we rolled it out as our primary database and phased out MongoDB within a week . It continued to work flawlessly…for about 6 months until that one day where Cassandra became unresponsive.</p><p id=\"7083\" class=\"graf graf--p graf-after--p\">We noticed Cassandra was running 10 second <em class=\"markup--em markup--p-em\">“stop-the-world”</em> GC constantly but we had no idea why. We started digging and found a Discord channel that was taking 20 seconds to load. The <a href=\"https://www.reddit.com/r/PuzzleAndDragons/\" data-href=\"https://www.reddit.com/r/PuzzleAndDragons/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">Puzzles &amp; Dragons Subreddit</a> public Discord server was the culprit. Since it was public we joined it to take a look. To our surprise, the channel had only 1 message in it. It was at that moment that it became obvious they deleted millions of messages using our API, leaving only 1 message in the channel.</p><p id=\"b84a\" class=\"graf graf--p graf-after--p\">If you have been paying attention you might remember how Cassandra handles deletes using tombstones (mentioned in <strong class=\"markup--strong markup--p-strong\">Eventual Consistency</strong>). When a user loaded this channel, even though there was only 1 message, Cassandra had to effectively scan millions of message tombstones (generating garbage faster than the JVM could collect it).</p><p id=\"5acc\" class=\"graf graf--p graf-after--p\">We solved this by doing the following:</p><ul class=\"postList\"><li id=\"4366\" class=\"graf graf--li graf-after--p\">We lowered the lifespan of tombstones from 10 days down to 2 days because we run <a href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsRepair.html#toolsRepair__description\" data-href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsRepair.html#toolsRepair__description\" class=\"markup--anchor markup--li-anchor\" rel=\"noopener\" target=\"_blank\">Cassandra repairs</a> (an anti-entropy process) every night on our message cluster.</li><li id=\"e1cd\" class=\"graf graf--li graf-after--li\">We changed our query code to track empty buckets and avoid them in the future for a channel. This meant that if a user caused this query again then at worst Cassandra would be scanning only in the most recent bucket.</li></ul><h3 id=\"805e\" class=\"graf graf--h3 graf-after--li\">The Future</h3><p id=\"dd65\" class=\"graf graf--p graf-after--h3\">We are currently running a 12 node cluster with a replica factor of 3 and will just continue to add new Cassandra nodes as needed. We believe this will continue to work for a long time but as Discord continues to grow there is a distant future where we are storing billions of messages per day. Netflix and Apple run clusters of hundreds of nodes so we know we can punt thinking too much about this for a while. However we like to have some ideas in our pocket for the future.</p><h4 id=\"6c38\" class=\"graf graf--h4 graf-after--p\">Near term</h4><ul class=\"postList\"><li id=\"2de1\" class=\"graf graf--li graf-after--h4\">Upgrade our message cluster from Cassandra 2 to Cassandra 3. Cassandra 3 has a <a href=\"http://www.datastax.com/2015/12/storage-engine-30\" data-href=\"http://www.datastax.com/2015/12/storage-engine-30\" class=\"markup--anchor markup--li-anchor\" rel=\"noopener\" target=\"_blank\">new storage format</a> that can reduce storage size by more than 50%.</li><li id=\"6986\" class=\"graf graf--li graf-after--li\">Newer versions of Cassandra are better at handling more data on a single node. We currently store nearly 1TB of compressed data on each node. We believe we can safely reduce the number of nodes in the cluster by bumping this to 2TB.</li></ul><h4 id=\"e79b\" class=\"graf graf--h4 graf-after--li\">Long term</h4><ul class=\"postList\"><li id=\"f493\" class=\"graf graf--li graf-after--h4\">Explore using <a href=\"http://www.scylladb.com/\" data-href=\"http://www.scylladb.com/\" class=\"markup--anchor markup--li-anchor\" rel=\"noopener\" target=\"_blank\">Scylla</a>, a Cassandra compatible database written in C++. During normal operations our Cassandra nodes are actually not using too much CPU, however at non peak hours when we run repairs (an anti-entropy process) they become fairly CPU bound and the duration increases with the amount of data written since the last repair. Scylla advertises significantly lower repair times.</li><li id=\"3008\" class=\"graf graf--li graf-after--li\">Build a system to archive unused channels to flat files on Google Cloud Storage and load them back on-demand. We want to avoid doing this one and don’t think we will have to do it.</li></ul><h3 id=\"05c6\" class=\"graf graf--h3 graf-after--li\">Conclusion</h3><p id=\"2468\" class=\"graf graf--p graf-after--h3\">It has now been just over a year since we made the switch and, despite <em class=\"markup--em markup--p-em\">“the big surprise,”</em> it has been smooth sailing. We went from over 100 million total messages to more than 120 million messages a day, with performance and stability staying consistent.</p><p id=\"3bc8\" class=\"graf graf--p graf-after--p\">Due to the success of this project we have since moved the rest of our live production data to Cassandra and that has also been a success.</p><p id=\"3f10\" class=\"graf graf--p graf-after--p\">In a follow-up to this post we will explore how we make billions of messages searchable.</p><p id=\"34ed\" class=\"graf graf--p graf-after--p graf--trailing\">We don’t have dedicated DevOps engineers yet (only 4 backend engineers), so having a system we don’t have to worry about has been great. <em class=\"markup--em markup--p-em\">We are hiring, so </em><a href=\"https://discordapp.com/jobs\" data-href=\"https://discordapp.com/jobs\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\"><em class=\"markup--em markup--p-em\">come join us</em></a><em class=\"markup--em markup--p-em\"> if this type of stuff tickles your fancy.</em></p></div>",
        "created_at": "2018-10-05T10:58:10+0000",
        "updated_at": "2018-11-06T00:01:25+0000",
        "published_at": "2017-01-14T01:26:51+0000",
        "published_by": [
          "Stanislav Vishnevskiy"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 12,
        "domain_name": "blog.discordapp.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*8eA5bgAG9NmCJshhccMhZQ.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12316"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12312,
        "uid": null,
        "title": "Introduction to cstar",
        "url": "http://thelastpickle.com/blog/2018/10/01/introduction-to-cstar.html",
        "content": "<p>Spotify is a long time user of Apache Cassandra at very large scale. It is also a creative company which tries to open source most of the tools they build for internal needs. They released <a href=\"http://cassandra-reaper.io\">Cassandra Reaper</a> a few years ago to give the community a reliable way of repairing clusters, which we now love and actively maintain. Their latest open sourced tool for Cassandra is <a href=\"https://github.com/spotify/cstar\">cstar</a>, a <a href=\"https://github.com/ParallelSSH/parallel-ssh\">parallel-ssh</a> equivalent (distributed shell) that is Cassandra topology aware. At TLP, we love it already and are sure you soon will too.</p><h2 id=\"what-is-cstar\">What is cstar?</h2>\n<p>Running distributed databases requires good automation, especially at scale. But even with small clusters, running the same command or roll restarting a cluster can quickly get tedious.\nSure, you can use tools like dsh and pssh, but they run commands on all servers at the same time (or just a given number) and you need to keep a list of the nodes to connect to locally. Each time your cluster scales out/in or if nodes get replaced you need to update the list. If you forget to update you may run commands that won’t touch the whole cluster without noticing.</p>\n<p>All commands cannot run on all nodes at the same time either. For instance upgrading sstables, running cleanup, major compaction or restarting nodes will have an impact on either latencies or availability and require more granularity of execution.</p>\n<p><a href=\"https://github.com/spotify/cstar\">Cstar</a> doesn’t suffer any of the above problems. It will discover the topology of the cluster dynamically and tune concurrency based on replication settings. In addition, cstar will run from a single machine (not necessarily within the cluster) that has SSH access to all nodes in the cluster, and perform operations through SSH and SFTP.\nIt requires no dependency, other than nodetool, to be installed on the Cassandra nodes.</p>\n<h2 id=\"installing-cstar\">Installing cstar</h2>\n<p>You’ll need to have Python 3 and pip3 installed on your server/laptop and then follow the README instructions which will, in the simplest case, boil down to:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>pip3 install cstar\n</pre></div></div>\n<h3 id=\"running-cstar\">Running cstar</h3>\n<p>Cstar is built with Python 3 and offers a straightforward way to run simple commands or complex scripts on an Apache Cassandra cluster using a single contact point.</p>\n<p>The following command, for example, will perform a rolling restart of Cassandra in the cluster, one node at a time using the <code class=\"highlighter-rouge\">one</code> strategy:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>cstar run --command=\"sudo service cassandra restart\" --seed-host=&lt;contact_point_ip&gt; --strategy=one\n</pre></div></div>\n<p>During the execution, cstar will update progress with a clear and pleasant output:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre> +  Done, up      * Executing, up      !  Failed, up      . Waiting, up\n -  Done, down    / Executing, down    X  Failed, down    : Waiting, down\nCluster: Test Cluster\nDC: dc1\n+....\n....\n....\nDC: dc2\n+....\n....\n....\nDC: dc3\n*....\n....\n....\n2 done, 0 failed, 1 executing\n</pre></div></div>\n<p>If we want to perform cleanup with topology awareness and have only one replica at a time, running the command for each token range (leaving a quorum of unaffected replicas at RF=3), we can use the default <code class=\"highlighter-rouge\">topology</code> strategy:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>cstar run --command=\"nodetool cleanup\" --seed-host=&lt;contact_point_ip&gt; --strategy=topology\n</pre></div></div>\n<p>This way, we’ll have several nodes processing the command to minimize the overall time spent on the operation and still ensure low impact on latencies:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre> +  Done, up      * Executing, up      !  Failed, up      . Waiting, up\n -  Done, down    / Executing, down    X  Failed, down    : Waiting, down\nCluster: Test Cluster\nDC: dc1\n****.\n....\n....\nDC: dc2\n++++.\n****\n....\nDC: dc3\n+****\n....\n....\n5 done, 0 failed, 12 executing\n</pre></div></div>\n<p>Finally, if we want to run a command that doesn’t involve pressure on latencies and display the outputs locally, we can use strategy all and add the <code class=\"highlighter-rouge\">-v</code> flag to display the command outputs:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>cstar run --command=\"nodetool getcompactionthroughput\" --seed-host=&lt;contact_point_ip&gt; --strategy=all -v\n</pre></div></div>\n<p>Which will give us the following output:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre> +  Done, up      * Executing, up      !  Failed, up      . Waiting, up\n -  Done, down    / Executing, down    X  Failed, down    : Waiting, down\nCluster: Test Cluster\nDC: dc1\n*****\n****\n****\nDC: dc2\n*****\n****\n****\nDC: dc3\n*****\n****\n****\n0 done, 0 failed, 39 executing\nHost node1.mycompany.com finished successfully\nstdout:\nCurrent compaction throughput: 0 MB/s\nHost node21.mycompany.com finished successfully\nstdout:\nCurrent compaction throughput: 0 MB/s\nHost node10.mycompany.com finished successfully\nstdout:\nCurrent compaction throughput: 0 MB/s\n...\n...\nHost node7.mycompany.com finished successfully\nstdout:\nCurrent compaction throughput: 0 MB/s\nHost node18.mycompany.com finished successfully\nstdout:\nCurrent compaction throughput: 0 MB/s\n +  Done, up      * Executing, up      !  Failed, up      . Waiting, up\n -  Done, down    / Executing, down    X  Failed, down    : Waiting, down\nCluster: Test Cluster\nDC: dc1\n+++++\n++++\n++++\nDC: dc2\n+++++\n++++\n++++\nDC: dc3\n+++++\n++++\n++++\n39 done, 0 failed, 0 executing\nJob cff7f435-1b9a-416f-99e4-7185662b88b2 finished successfully\n</pre></div></div>\n<h2 id=\"how-cstar-does-its-magic\">How cstar does its magic</h2>\n<p>When you run a cstar command it will first connect to the seed node you provided and run a set of nodetool commands through SSH.</p>\n<p>First, nodetool ring will give it the cluster topology with the state of each node. By default, cstar will stop the execution if one node in the cluster is down or unresponsive.\nIf you’re aware that nodes are down and want to run a command nonetheless, you can add the <code class=\"highlighter-rouge\">--ignore-down-nodes</code> flag to bypass the check.</p>\n<p>Then cstar will list the keyspaces using nodetool cfstats and build a map of the replicas for all token ranges for each of them. This will allow it to identify which nodes contain the same token ranges, using nodetool describering, and apply the topology strategy accordingly. As shown before, the topology strategy will not allow two nodes that are replicas for the same token to run the command at the same time. If the cluster does not use vnodes, the topology strategy will run the command every RF node. If the cluster uses vnodes but is not using NetworkTopologyStrategy (NTS) for all keyspaces nor spreading across racks, chances are only one node will be able to run the command at once, even with the topology strategy.If both NTS and racks are in use, the topology strategy will run the command on a whole rack at a time.</p>\n<p>By default, cstar will process the datacenters in parallel, so 2 nodes being replicas for the same tokens but residing in different datacenters can be processed at the same time.</p>\n<p>Once the cluster has been fully mapped execution will start in token order.\nCstar is very resilient because it uploads a script on each remote node through SFTP and runs it using nohup. Each execution will write output (std and err) files along with the exit code for cstar to check on regularly. If the command is interrupted on the server that runs cstar, it can be resumed safely as cstar will first check if the script is still running or has finished already on each node that hasn’t gone through yet.<br />Note that interrupting the command on the cstar host will not stop it on the remote nodes that are already running it.<br />Resuming an interrupted command is done simply by executing : <code class=\"highlighter-rouge\">cstar continue &lt;job_id&gt;</code></p>\n<p>Each time a node finishes running the command cstar will check if the cluster health is still good and if the node is up. This way, if you perform a rolling restart and one of the nodes doesn’t come back up properly, although the exit code of the restart command is <code class=\"highlighter-rouge\">0</code>, cstar will wait indefinitely to protect the availability of the cluster. That is unless you specified a timeout on the job. In such a case, the job will fail.\nOnce the node is up after the command has run, cstar will look for the next candidate node in the ring to run the command.</p>\n<h2 id=\"a-few-handy-flags\">A few handy flags</h2>\n<h3 id=\"two-steps-execution\">Two steps execution</h3>\n<p>Some commands may be scary to run on the whole cluster and you may want to run them on a subset of the nodes first, check that they are in the expected state manually, and then continue the execution on the rest of the cluster.\nThe <code class=\"highlighter-rouge\">--stop-after=&lt;number-of-nodes&gt;</code> flag will do just that. Setting it to <code class=\"highlighter-rouge\">--stop-after=1</code> will run the command on a single node and exit. Once you’ve verified that you’re happy with the execution on that one node you can process the rest of the cluster using <code class=\"highlighter-rouge\">cstar continue &lt;job_id&gt;</code>.</p>\n<h3 id=\"retry-failed-nodes\">Retry failed nodes</h3>\n<p>Some commands might fail mid-course due to transient problems. By default, <code class=\"highlighter-rouge\">cstar continue &lt;job_id&gt;</code> will halt if there is any failed execution in the history of the job. In order to resume the job and retry the execution on the failed nodes, add the <code class=\"highlighter-rouge\">--retry-failed</code> flag.</p>\n<h3 id=\"run-the-command-on-a-specific-datacenter\">Run the command on a specific datacenter</h3>\n<p>To process only a specific datacenter add the <code class=\"highlighter-rouge\">--dc-filter=&lt;datacenter-name&gt;</code> flag. All other datacenters will be ignored by cstar.</p>\n<h3 id=\"datacenter-parallelism\">Datacenter parallelism</h3>\n<p>By default, cstar will process the datacenters in parallel. If you only want only one datacenter to process the command at a time, add the <code class=\"highlighter-rouge\">--dc-serial</code> flag.</p>\n<h3 id=\"specifying-a-maximum-concurrency\">Specifying a maximum concurrency</h3>\n<p>You can forcefully limit the number of nodes running the command at the same time, regardless of topology, by adding the <code class=\"highlighter-rouge\">--max-concurrency=&lt;number-of-nodes&gt;</code> flag.</p>\n<h3 id=\"wait-between-each-node\">Wait between each node</h3>\n<p>You may want to delay executions between nodes in order to give some room for the cluster to recover from the command. The <code class=\"highlighter-rouge\">--node-done-pause-time=&lt;time-in-seconds&gt;</code> flag will allow to specify a pause time that cstar will apply before looking for the next node to run the command on.</p>\n<h3 id=\"run-the-command-regardless-down-nodes\">Run the command regardless down nodes</h3>\n<p>If you want to run a command while nodes are down in the cluster add the <code class=\"highlighter-rouge\">--ignore-down-nodes</code> flag to cstar.</p>\n<h3 id=\"run-on-specific-nodes-only\">Run on specific nodes only</h3>\n<p>If the command is meant to run on some specific nodes only you can use either the <code class=\"highlighter-rouge\">--host</code> or the <code class=\"highlighter-rouge\">--host-file</code> flags.</p>\n<h3 id=\"control-the-verbosity-of-the-output\">Control the verbosity of the output</h3>\n<p>By default, cstar will only display the progress of the execution as shown above in this post. To get the output of the remote commands, add the <code class=\"highlighter-rouge\">-v</code> flag.\nIf you want to get more verbosity on the executions and get debug loggings use either <code class=\"highlighter-rouge\">-vv</code> (very verbose) or <code class=\"highlighter-rouge\">-vvv</code> (extra verbose).</p>\n<h2 id=\"you-havent-installed-it-already\">You haven’t installed it already?</h2>\n<p><a href=\"https://github.com/spotify/cstar\">Cstar</a> is the tool that all Apache Cassandra operators have been waiting for to manage clusters of all sizes. We were happy to collaborate closely with Spotify to help them open source it.\nIt has been built and matured at one of the most smart and successful start-ups in the world and was developed to manage hundreds of clusters of all sizes. It requires no dependency to be installed on the cluster and uses SSH exclusively. Thus, it will comply nicely with any security policy and you should be able to run it within minutes on any cluster of any size.</p>\n<p>We love cstar so much we are already working on integrating it with Reaper as you can see in the following video :</p>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-rYQxGw2Cnk\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>\n<p>We’ve seen in this blog post how to run simple one line commands with cstar, but there is much more than meets the eye. In an upcoming blog post we will introduce complex command scripts that perform operations like upgrading a Cassandra cluster, selectively clearing snapshots, or safely switching compaction strategies in a single cstar invocation.</p>",
        "created_at": "2018-10-05T10:51:38+0000",
        "updated_at": "2018-10-05T10:51:43+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 9,
        "domain_name": "thelastpickle.com",
        "preview_picture": "http://thelastpickle.com/android-chrome-192x192.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12312"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          },
          {
            "id": 1267,
            "label": "microservices",
            "slug": "microservices"
          }
        ],
        "is_public": false,
        "id": 12293,
        "uid": null,
        "title": "Customer Event Hub – a modern Customer 360° view with DataStax Enterp…",
        "url": "https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse",
        "content": "Customer Event Hub – a modern Customer 360° view with DataStax Enterp…\n      \n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            </a><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div>\n    <div class=\"wrapper\"><p>Successfully reported this slideshow.</p><div id=\"slideview-container\" class=\"\"><div class=\"row\"><div id=\"main-panel\" class=\"small-12 large-8 columns\"><div class=\"sectionElements\"><div class=\"playerWrapper\"><div><div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\">Customer Event Hub – a modern Customer 360° view with DataStax Enterprise (DSE)<div class=\"stage valign-first-slide\"><div class=\"slide_container\"><section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-1-638.jpg?cb=1498118851\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-1-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-1-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-1-1024.jpg?cb=1498118851\" alt=\"BASEL BERN BRUGG DÜSSELDORF FRANKFURT A.M. FREIBURG I.BR. GENF&#10;HAMBURG KOPENHAGEN LAUSANNE MÜNCHEN STUTTGART WIEN ZÜRICH&#10;C...\" /></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-2-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-2-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-2-1024.jpg?cb=1498118851\" alt=\"Guido Schmutz&#10;Working at Trivadis for more than 20 years&#10;Oracle ACE Director for Fusion Middleware and SOA&#10;Consultant, Tra...\" /></i></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-3-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-3-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-3-1024.jpg?cb=1498118851\" alt=\"COPENHAGEN&#10;MUNICH&#10;LAUSANNE&#10;BERN&#10;ZURICH&#10;BRUGG&#10;GENEVA&#10;HAMBURG&#10;DÜSSELDORF&#10;FRANKFURT&#10;STUTTGART&#10;FREIBURG&#10;BASEL&#10;VIENNA&#10;With over...\" /></i></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-4-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-4-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-4-1024.jpg?cb=1498118851\" alt=\"Agenda&#10;4&#10;1. Customer 360° View – Introduction&#10;2. Customer 360° View – Challenges&#10;3. Customer 360° View – DataStax Enterpri...\" /></i></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-5-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-5-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-5-1024.jpg?cb=1498118851\" alt=\"Customer 360° View - Introduction&#10;5&#10;\" /></i></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-6-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-6-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-6-1024.jpg?cb=1498118851\" alt=\"Why Customer 360° View?&#10;“Get closer than ever to your&#10;customers. So close that you tell&#10;them what they need well before&#10;th...\" /></i></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-7-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-7-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-7-1024.jpg?cb=1498118851\" alt=\"Customer 360°: Experience Expectations&#10;7&#10;\" /></i></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-8-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-8-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-8-1024.jpg?cb=1498118851\" alt=\"“The Amazon effect” – why can’t I do .... as easy as&#10;buying a product on Amazon&#10;8&#10;Each time a customer is exposed to an im...\" /></i></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-9-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-9-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-9-1024.jpg?cb=1498118851\" alt=\"Customer 360°: Experience Expectations&#10;Consistent&#10;Across&#9;all&#9;channels,&#9;brands&#9;&#10;and&#9;devices&#10;Personalized&#10;To&#9;reflect&#9;prefere...\" /></i></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-10-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-10-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-10-1024.jpg?cb=1498118851\" alt=\"Customer 360 – Key Use Cases&#10;• Customer micro&#10;segmentation&#10;• Next Best Offer&#10;• Campaign Analytics&#10;• Geo-Location&#10;Analytics...\" /></i></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-11-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-11-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-11-1024.jpg?cb=1498118851\" alt=\"From Static to Dynamic, Real-Time Micro-Segmentation&#10;Age&#10;Gender&#10;Average Spend&#10;Price Plans&#10;Usage History&#10;Data, Voice, Text&#10;...\" /></i></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-12-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-12-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-12-1024.jpg?cb=1498118851\" alt=\"From Static to Dynamic, Real-Time Micro-Segmentation&#10;Age&#10;Gender&#10;Average Spend&#10;Price Plans&#10;Usage History&#10;Data, Voice, Text&#10;...\" /></i></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-13-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-13-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-13-1024.jpg?cb=1498118851\" alt=\"A Sample Customer 360° Profile&#10;Who are you?&#10;Where are you?&#10;What have you&#10;purchased?&#10;What content do you&#10;prefer?&#10;Who do you...\" /></i></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-14-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-14-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-14-1024.jpg?cb=1498118851\" alt=\"Customer 360° View - Challenges&#10;16&#10;\" /></i></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-15-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-15-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-15-1024.jpg?cb=1498118851\" alt=\"Key Challenges in Driving a Customer 360° View&#10;Data Silos&#10;New Data Sources Costs of Data Processing&#10;Data Volumes&#10;• Multipl...\" /></i></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-16-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-16-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-16-1024.jpg?cb=1498118851\" alt=\"Customer 360° View - Traditional Flow Diagram&#10;Enterprise Data&#10;Warehouse&#10;ETL / Stored&#10;Procedures&#10;Data Marts /&#10;Aggregations&#10;...\" /></i></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-17-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-17-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-17-1024.jpg?cb=1498118851\" alt=\"Customer 360° View - Traditional Flow Diagram&#10;Enterprise Data&#10;Warehouse&#10;ETL / Stored&#10;Procedures&#10;Data Marts /&#10;Aggregations&#10;...\" /></i></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-18-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-18-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-18-1024.jpg?cb=1498118851\" alt=\"Customer 360° View: Why status quo won’t work?&#10;• Most organizations have a static&#10;version of the customer profile in&#10;their...\" /></i></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-19-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-19-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-19-1024.jpg?cb=1498118851\" alt=\"Journey of Customer through multiple Siloed Systems&#10;21&#10;Social&#10;Media&#10;Call&#10;Center&#10;Complaint&#10;Management&#10;Marketing&#10;Coupons&#10;War...\" /></i></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-20-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-20-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-20-1024.jpg?cb=1498118851\" alt=\"Journey of Customer through multiple Siloed Systems&#10;22&#10;Social&#10;Media&#10;Call&#10;Center&#10;Complaint&#10;Management&#10;Marketing&#10;Coupons&#10;War...\" /></i></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-21-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-21-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-21-1024.jpg?cb=1498118851\" alt=\"Customer 360° View – DataStax&#10;Enterprise (Graph) to the rescue&#10;23&#10;\" /></i></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-22-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-22-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-22-1024.jpg?cb=1498118851\" alt=\"Why using Graph for Customer 360° View&#10;24&#10;Traditional RDBMS&#10;• Multiple Data Locations =&gt; siloes&#10;• Not all information rela...\" /></i></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-23-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-23-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-23-1024.jpg?cb=1498118851\" alt=\"Customer 360° View - Example&#10;25&#10;\" /></i></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-24-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-24-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-24-1024.jpg?cb=1498118851\" alt=\"Geo Point&#10;Customer&#10;Address&#10;Product&#10;ownerOf&#10;(since)&#10;interestedIn&#10;(since, degree)&#10;lives (since)&#10;ActivityparticipatesIn Termu...\" /></i></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-25-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-25-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-25-1024.jpg?cb=1498118851\" alt=\"Hadoop ClusterdHadoop Cluster&#10;DSE Cluster&#10;Batch Data Ingestion into Customer Hub&#10;Billing &amp;&#10;Ordering&#10;CRM /&#10;Profile&#10;Marketin...\" /></i></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-26-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-26-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-26-1024.jpg?cb=1498118851\" alt=\"Batch Data Ingestion into Customer Hub&#10;DSE&#9;Graph&#10;DSE&#9;&#10;GraphLoaderRDBMS&#10;Groovy&#9;&#10;Script&#10;Click&#9;&#10;Stream&#10;28&#10;Geo Point&#10;Customer&#10;...\" /></i></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-27-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-27-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-27-1024.jpg?cb=1498118851\" alt=\"Streaming Ingestion into Customer Event Hub&#10;Microservice Cluster&#10;Microservice State&#10;{&#9;&#9;}&#10;API&#10;Stream Processing Cluster&#10;Str...\" /></i></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-28-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-28-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-28-1024.jpg?cb=1498118851\" alt=\"Process native Event Streams&#10;Twitter&#10;Tweet-to-&#10;Cassandra Cassandra&#10;Tweet&#10;Graph&#10;Tweet-to-Graph&#10;Customer&#9;&#10;Reference&#10;Click&#10;St...\" /></i></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-29-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-29-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-29-1024.jpg?cb=1498118851\" alt=\"Benchmark Single vs. Scripted Insert&#10;34&#10;• One Event ends up in many&#10;modifications of vertex and edges&#10;• many round-trips n...\" /></i></section><section data-index=\"30\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-30-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-30-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-30-1024.jpg?cb=1498118851\" alt=\"Process Change Data Capture Events&#10;CDC&#10;Customer-to-&#10;Cassandra Cassandra&#10;Customer&#10;DSE&#9;Graph&#10;Customer-to-&#10;Graph&#10;CustomerCDC&#10;...\" /></i></section><section data-index=\"31\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-31-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-31-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-31-1024.jpg?cb=1498118851\" alt=\"Streaming Oriented Ingestion into Customer Hub&#10;Microservice Cluster&#10;Microservice State&#10;{&#9;&#9;}&#10;API&#10;Stream Processing Cluster&#10;...\" /></i></section><section data-index=\"32\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-32-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-32-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-32-1024.jpg?cb=1498118851\" alt=\"How to implement an Event Hub?&#10;Apache Kafka to the rescue&#10;• publish-subscribe messaging system&#10;• Designed for processing h...\" /></i></section><section data-index=\"33\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-33-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-33-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-33-1024.jpg?cb=1498118851\" alt=\"Apache Kafka Connect&#10;• Scalably and reliably streaming data between&#10;Apache Kafka and other data systems&#10;• not an ETL frame...\" /></i></section><section data-index=\"34\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-34-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-34-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-34-1024.jpg?cb=1498118851\" alt=\"Declarative Dataflow Definition &amp; Execution&#10;43&#10;Apache NiFi StreamSets&#10;\" /></i></section><section data-index=\"35\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-35-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-35-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-35-1024.jpg?cb=1498118851\" alt=\"Hadoop ClusterdHadoop Cluster&#10;DSE&#10;Cluster&#10;Streaming Ingestion into Customer Hub&#10;OLAP&#10;Microservice Cluster&#10;Microservice Sta...\" /></i></section><section data-index=\"36\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-36-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-36-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-36-1024.jpg?cb=1498118851\" alt=\"Hadoop ClusterdHadoop Cluster&#10;DSE&#10;Cluster&#10;Streaming Ingestion into Customer Hub&#10;OLAP&#10;Microservice Cluster&#10;Microservice Sta...\" /></i></section><section data-index=\"37\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-37-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-37-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-37-1024.jpg?cb=1498118851\" alt=\"Questions which can only be answered by Graph&#10;46&#10;Dependencies&#10;•&#9;Failure&#9;chains&#10;•&#9;Order&#9;of&#9;operation&#10;Matching&#9;/&#9;&#10;Categorizi...\" /></i></section><section data-index=\"38\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-38-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-38-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-38-1024.jpg?cb=1498118851\" alt=\"Questions which can only be answered by Graph -&#10;Visualize Customer 360&#10;49&#10;Source: Expero&#10;\" /></i></section><section data-index=\"39\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-39-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-39-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-39-1024.jpg?cb=1498118851\" alt=\"Questions which can only be answered by Graph -&#10;Visualize Customer 360&#10;50&#10;KeyLines&#10;Cytoscape&#10;LinkuriousJS&#10;SigmajsD3&#10;Source...\" /></i></section><section data-index=\"40\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/gschmutz/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse\" data-small=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/85/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-40-320.jpg?cb=1498118851\" data-normal=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-40-638.jpg?cb=1498118851\" data-full=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-40-1024.jpg?cb=1498118851\" alt=\"Guido Schmutz&#10;Technology Manager&#10;guido.schmutz@trivadis.com&#10;@gschmutz guidoschmutz.wordpress.com&#10;\" /></i></section><div class=\"j-next-container next-container\"><div class=\"content-container\"><div class=\"next-slideshow-wrapper\"><div class=\"j-next-slideshow next-slideshow\"><p>Upcoming SlideShare</p></div><p>Loading in …5</p><p>×</p></div></div></div></div></div></div></div></div></div><div class=\"slideshow-info-container\" itemscope=\"itemscope\" itemtype=\"https://schema.org/MediaObject\"><div class=\"slideshow-tabs-container show-for-medium-up\"><ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    0 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\">\n                \n                  0 Likes\n                \n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\">\n                Statistics\n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\">\n                Notes\n              </i></a>\n            </li>\n          </ul><div class=\"tabs-content\"><div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\"><ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"itemscope\"><li>\n                    <p class=\"empty-stat-box text-center\">\n                      <em>Be the first to like this</em>\n                    </p>\n                  </li>\n              </ul></div><div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No Downloads</p></div><div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No notes for slide</p></div></div></div><div class=\"notranslate transcript add-padding-right j-transcript\"><ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    BASEL BERN BRUGG DÜSSELDORF FRANKFURT A.M. FREIBURG I.BR. GENF\nHAMBURG KOPENHAGEN LAUSANNE MÜNCHEN STUTTGART WIEN ZÜRICH\nCustomer Event Hub\nCustomer 360° view with DataStax Enterprise (DSE)\nGuido Schmutz – 21.6.2017\n@gschmutz guidoschmutz.wordpress.com\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-2-638.jpg?cb=1498118851\" title=\"Guido Schmutz&#10;Working at Trivadis for more than 20 years&#10;Or...\" target=\"_blank\">\n        2.\n      </a>\n    Guido Schmutz\nWorking at Trivadis for more than 20 years\nOracle ACE Director for Fusion Middleware and SOA\nConsultant, Trainer Software Architect for Java, Oracle, SOA and\nBig Data / Fast Data\nHead of Trivadis Architecture Board\nTechnology Manager @ Trivadis\nMore than 30 years of software development experience\nContact: guido.schmutz@trivadis.com\nBlog: http://guidoschmutz.wordpress.com\nSlideshare: http://www.slideshare.net/gschmutz\nTwitter: gschmutz\n2\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-3-638.jpg?cb=1498118851\" title=\"COPENHAGEN&#10;MUNICH&#10;LAUSANNE&#10;BERN&#10;ZURICH&#10;BRUGG&#10;GENEVA&#10;HAMBURG...\" target=\"_blank\">\n        3.\n      </a>\n    COPENHAGEN\nMUNICH\nLAUSANNE\nBERN\nZURICH\nBRUGG\nGENEVA\nHAMBURG\nDÜSSELDORF\nFRANKFURT\nSTUTTGART\nFREIBURG\nBASEL\nVIENNA\nWith over 600 specialists and IT experts in your region.\n14 Trivadis branches and more than\n600 employees\n200 Service Level Agreements\nOver 4,000 training participants\nResearch and development budget:\nCHF 5.0 million\nFinancially self-supporting and\nsustainably profitable\nExperience from more than 1,900\nprojects per year at over 800\ncustomers\n3\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-4-638.jpg?cb=1498118851\" title=\"Agenda&#10;4&#10;1. Customer 360° View – Introduction&#10;2. Customer 3...\" target=\"_blank\">\n        4.\n      </a>\n    Agenda\n4\n1. Customer 360° View – Introduction\n2. Customer 360° View – Challenges\n3. Customer 360° View – DataStax Enterprise (Graph) to the rescue\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-5-638.jpg?cb=1498118851\" title=\"Customer 360° View - Introduction&#10;5&#10;\" target=\"_blank\">\n        5.\n      </a>\n    Customer 360° View - Introduction\n5\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-6-638.jpg?cb=1498118851\" title=\"Why Customer 360° View?&#10;“Get closer than ever to your&#10;custo...\" target=\"_blank\">\n        6.\n      </a>\n    Why Customer 360° View?\n“Get closer than ever to your\ncustomers. So close that you tell\nthem what they need well before\nthey realize it themselves.”\nSteve Jobs, Apple\nq Enhance customer service\nq Provides real-time personalization\nq Opens doors to new applications\nq Increases security\nq Lowers operational costs\nq Increases operational efficiency\n6\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-7-638.jpg?cb=1498118851\" title=\"Customer 360°: Experience Expectations&#10;7&#10;\" target=\"_blank\">\n        7.\n      </a>\n    Customer 360°: Experience Expectations\n7\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-8-638.jpg?cb=1498118851\" title=\"“The Amazon effect” – why can’t I do .... as easy as&#10;buying...\" target=\"_blank\">\n        8.\n      </a>\n    “The Amazon effect” – why can’t I do .... as easy as\nbuying a product on Amazon\n8\nEach time a customer is exposed to an improved digital experience,\ntheir engagement expectations are reset to a new higher level.\nSource: Forrester\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-9-638.jpg?cb=1498118851\" title=\"Customer 360°: Experience Expectations&#10;Consistent&#10;Across&#9;al...\" target=\"_blank\">\n        9.\n      </a>\n    Customer 360°: Experience Expectations\nConsistent\nAcross\tall\tchannels,\tbrands\t\nand\tdevices\nPersonalized\nTo\treflect\tpreferences\tand\t\naspiration\nRelevant\nIn\tthe\tmoment\tto\tcustomer’s\t\nneeds\tand\texpectations\nContextualized\nTo\tpresent\tlocation\tand\t\ncircumstances\n9\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-10-638.jpg?cb=1498118851\" title=\"Customer 360 – Key Use Cases&#10;• Customer micro&#10;segmentation&#10;...\" target=\"_blank\">\n        10.\n      </a>\n    Customer 360 – Key Use Cases\n• Customer micro\nsegmentation\n• Next Best Offer\n• Campaign Analytics\n• Geo-Location\nAnalytics\n• Recommendation\nModels\n• Churn Modeling &amp;\nPrediction\n• Rotational / Social\nChurn\n• Customer Lifetime\nValue\n• Sentiment Analytics\n• Price Elasticity\nModeling\n• Proactive Care\nDashboard\n• Customer Lifetime\nValue\n• Subscriber Analytics\n• QoS Analytics\n• Real-Time Alerts\nTarget Marketing &amp;\nPersonalization\nChurn Prevention &amp;\nCustomer Retention\nProactive Care\n10\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-11-638.jpg?cb=1498118851\" title=\"From Static to Dynamic, Real-Time Micro-Segmentation&#10;Age&#10;Ge...\" target=\"_blank\">\n        11.\n      </a>\n    From Static to Dynamic, Real-Time Micro-Segmentation\nAge\nGender\nAverage Spend\nPrice Plans\nUsage History\nData, Voice, Text\nBilling History\nDevice Upgrade\nTraditional\nSegmentation\nAge\nGender\nAverage Spend\nPrice Plans\nUsage History\nData, Voice, Text\nBilling History\nDevice Upgrade\nDevice History\nOther products / services\nBundling preferences\nOffer History\nCampaign Adoption\nHistory\nCall Center Tickets\nLocation\nSocial Influence\nApplications Used\nContent Preferences\nUsage Details\nRoaming Analysis\nTravel Patterns\nQoS History\nHousehold Analysis\nLifetime Value\nChurn Score\nClickstream Info\nChannel Preferences\nSurvey\nReal-Time Micro-\nSegmentation\n11\nIndividualization\nEngaging customers as a segment of one in real-time by listening,\ncapturing, measuring, assessing, and addressing intent across every\nenterprise touchpoint.\nSource: Forrester\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-12-638.jpg?cb=1498118851\" title=\"From Static to Dynamic, Real-Time Micro-Segmentation&#10;Age&#10;Ge...\" target=\"_blank\">\n        12.\n      </a>\n    From Static to Dynamic, Real-Time Micro-Segmentation\nAge\nGender\nAverage Spend\nPrice Plans\nUsage History\nData, Voice, Text\nBilling History\nDevice Upgrade\nTraditional\nSegmentation\nAge\nGender\nAverage Spend\nPrice Plans\nUsage History\nData, Voice, Text\nBilling History\nDevice Upgrade\nDevice History\nOther products / services\nBundling preferences\nOffer History\nCampaign Adoption\nHistory\nCall Center Tickets\nLocation\nSocial Influence\nApplications Used\nContent Preferences\nUsage Details\nRoaming Analysis\nTravel Patterns\nQoS History\nHousehold Analysis\nLifetime Value\nChurn Score\nClickstream Info\nChannel Preferences\nSurvey\nReal-Time Micro-\nSegmentation\n12\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-13-638.jpg?cb=1498118851\" title=\"A Sample Customer 360° Profile&#10;Who are you?&#10;Where are you?&#10;...\" target=\"_blank\">\n        13.\n      </a>\n    A Sample Customer 360° Profile\nWho are you?\nWhere are you?\nWhat have you\npurchased?\nWhat content do you\nprefer?\nWho do you know?\nWhat can you afford?\nWhat is your value to the\nbusiness?\nHow / why have you\ncontacted us?\n13\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-14-638.jpg?cb=1498118851\" title=\"Customer 360° View - Challenges&#10;16&#10;\" target=\"_blank\">\n        14.\n      </a>\n    Customer 360° View - Challenges\n16\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-15-638.jpg?cb=1498118851\" title=\"Key Challenges in Driving a Customer 360° View&#10;Data Silos&#10;N...\" target=\"_blank\">\n        15.\n      </a>\n    Key Challenges in Driving a Customer 360° View\nData Silos\nNew Data Sources Costs of Data Processing\nData Volumes\n• Multiple Data Silos\n• Often store overlapping and\nconflicting information\n• Data growing rapidly\n• Internet of Things will add to\nthat substantially\n• Semi/Un-Structured Data\nSources\n• Streaming / Real-time data\n• Critical for building a true\n360° view\n• Cost prohibitive\n• Cost of storing data in\nrelational database systems\nper year\nClickstream Location/GPS\nCall center\nRecords\nSocial Media\n17\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-16-638.jpg?cb=1498118851\" title=\"Customer 360° View - Traditional Flow Diagram&#10;Enterprise Da...\" target=\"_blank\">\n        16.\n      </a>\n    Customer 360° View - Traditional Flow Diagram\nEnterprise Data\nWarehouse\nETL / Stored\nProcedures\nData Marts /\nAggregations\nLocation\nSocial\nClickstream\nSegmentation &amp; Churn\nAnalysis\nBI Tools\nMarketing Offers\nBilling &amp;\nOrdering\nCRM / Profile\nMarketing\nCampaigns\n18\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-17-638.jpg?cb=1498118851\" title=\"Customer 360° View - Traditional Flow Diagram&#10;Enterprise Da...\" target=\"_blank\">\n        17.\n      </a>\n    Customer 360° View - Traditional Flow Diagram\nEnterprise Data\nWarehouse\nETL / Stored\nProcedures\nData Marts /\nAggregations\nLocation\nSocial\nClickstream\nSegmentation &amp; Churn\nAnalysis\nBI Tools\nMarketing Offers\nBilling &amp;\nOrdering\nCRM / Profile\nMarketing\nCampaigns\nLimited\nProcessing\nPower\nDoes not\nmodel easily\nto traditional\ndatabase\nschema\nLimited\nProcessing\nPower\nStorage\nScaling\nvery\nexpensive\nBased on\nsample /\nlimited data\nLoss in\nFidelity\nOther /\nNew Data\nSources\nHigh\nVoume\nand\nVelocity\n19\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-18-638.jpg?cb=1498118851\" title=\"Customer 360° View: Why status quo won’t work?&#10;• Most organ...\" target=\"_blank\">\n        18.\n      </a>\n    Customer 360° View: Why status quo won’t work?\n• Most organizations have a static\nversion of the customer profile in\ntheir data warehouse\n• Mainly structured data\n• Only internal data\n• Only “important” data\n• Only limited history\n• Activity data – clickstream data,\ncontent preferences, customer care\nlogs are kept in siloes or not kept at\nall\nData Analyst\nData Analyst\nData Analyst\nData Analyst\nData Analyst\nDetailed\tCustomer\tActivity\tData\tsits\tin\tsilos!\n20\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-19-638.jpg?cb=1498118851\" title=\"Journey of Customer through multiple Siloed Systems&#10;21&#10;Soci...\" target=\"_blank\">\n        19.\n      </a>\n    Journey of Customer through multiple Siloed Systems\n21\nSocial\nMedia\nCall\nCenter\nComplaint\nManagement\nMarketing\nCoupons\nWarehouse CRM\nShipping\nBilling\nOrder\nProcessing\nWeb\nApplication\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-20-638.jpg?cb=1498118851\" title=\"Journey of Customer through multiple Siloed Systems&#10;22&#10;Soci...\" target=\"_blank\">\n        20.\n      </a>\n    Journey of Customer through multiple Siloed Systems\n22\nSocial\nMedia\nCall\nCenter\nComplaint\nManagement\nMarketing\nCoupons\nWarehouse CRM\nShipping\nBilling\nOrder\nProcessing\nWeb\nApplication\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-21-638.jpg?cb=1498118851\" title=\"Customer 360° View – DataStax&#10;Enterprise (Graph) to the res...\" target=\"_blank\">\n        21.\n      </a>\n    Customer 360° View – DataStax\nEnterprise (Graph) to the rescue\n23\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-22-638.jpg?cb=1498118851\" title=\"Why using Graph for Customer 360° View&#10;24&#10;Traditional RDBMS...\" target=\"_blank\">\n        22.\n      </a>\n    Why using Graph for Customer 360° View\n24\nTraditional RDBMS\n• Multiple Data Locations =&gt; siloes\n• Not all information related\n• difficult to access all the different\ninformation and to relate to each other\nGraph Database\n• Connect all customer-related information\n• Model multi-connected customer relationships\n• Special questions graphs can answer\n• Performance &amp; Scalability\nPartner\nId\nFirstName\nLastName\nBirthDate\nProﬁleImageURL\nId\nCustomerId\nWhen\nWhere\nType\nDescription\nContacts\nAddress\nId\nPartnerId\nStreet\nStreetNr\nZipCode\nCity\nCountry\nId\nName\nPrice\nDescription\nImageURL\nProducts\nCustomerId\nProductId\nOrder\nId\nCustomerId\nWhen\nWhere\nInteractionType\nDescription\nCustomer Service\nId\nProductId\nComment\nUserId\nProduct Reviews\nId\nPartnerId\nTwitterId\nFacebookId\nCustomer\nGeo Point\nCustomer\nAddress\nProduct\nownerOf\n(since)\ninterestedIn\n(since, degree)\nlives (since)\nActivityparticipatesIn Termuses\nuses\nEmployee\ninteractsWith\nreference\nid\nname\nid\nwhen\nwhere\ntext\nid\nﬁrstName\nlastName\nbirthDate\nproﬁleImageURL\nuses\nid\nname\nimageURL\nid\nstreet\nstreetNr\nzipCode\ncity\nid\nname\nalternateNames\nlocation\npopulation\nelevation\nCountry\nneighbour\nbelongsTo\nid\nname\niso2\niso3\nisoNumeric\ncapital\narea\npopulation\ncontinent\nuses\nActivityType\nid\nname\nbelongsTo\nid\nname\nat\nknows(since)\nmentions\nuses\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-23-638.jpg?cb=1498118851\" title=\"Customer 360° View - Example&#10;25&#10;\" target=\"_blank\">\n        23.\n      </a>\n    Customer 360° View - Example\n25\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-24-638.jpg?cb=1498118851\" title=\"Geo Point&#10;Customer&#10;Address&#10;Product&#10;ownerOf&#10;(since)&#10;interest...\" target=\"_blank\">\n        24.\n      </a>\n    Geo Point\nCustomer\nAddress\nProduct\nownerOf\n(since)\ninterestedIn\n(since, degree)\nlives (since)\nActivityparticipatesIn Termuses\nuses\nEmployee\ninteractsWith\nreference\nid\nname\nid\nwhen\nwhere\ntext\nid\nﬁrstName\nlastName\nbirthDate\nproﬁleImageURL\nuses\nid\nname\nimageURL\nid\nstreet\nstreetNr\nzipCode\ncity\nid\nname\nalternateNames\nlocation\npopulation\nelevation\nCountry\nneighbour\nbelongsTo\nid\nname\niso2\niso3\nisoNumeric\ncapital\narea\npopulation\ncontinent\nuses\nActivityType\nid\nname\nbelongsTo\nid\nname\nat\nknows(since)\nmentions\nuses\nCustomer 360° View - Example\n26\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-25-638.jpg?cb=1498118851\" title=\"Hadoop ClusterdHadoop Cluster&#10;DSE Cluster&#10;Batch Data Ingest...\" target=\"_blank\">\n        25.\n      </a>\n    Hadoop ClusterdHadoop Cluster\nDSE Cluster\nBatch Data Ingestion into Customer Hub\nBilling &amp;\nOrdering\nCRM /\nProfile\nMarketing\nCampaigns\nLocation\nSocial\nClick\nstream\nSensor\nData\nWeather\nData\nMobile\nApps\nEmails\nv\nFile Import /\nSQL Import\nCassandra DSE Graph\n27\nhigh\tlatency\nEvent Stream\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-26-638.jpg?cb=1498118851\" title=\"Batch Data Ingestion into Customer Hub&#10;DSE&#9;Graph&#10;DSE&#9;&#10;Graph...\" target=\"_blank\">\n        26.\n      </a>\n    Batch Data Ingestion into Customer Hub\nDSE\tGraph\nDSE\t\nGraphLoaderRDBMS\nGroovy\t\nScript\nClick\t\nStream\n28\nGeo Point\nCustomer\nAddress\nProduct\nownerOf (since)\ninterestedIn (since, degree)\nlives (since)\nActivityparticipatesIn Termuses\nuses\nEmployee\ninteractsWith\nreference\nid\nname\nid\nwhen\nwhere\ntext\nid\nﬁrstName\nlastName\nbirthDate\nproﬁleImageURL\nuses\nid\nname\nimageURL\nid\nstreet\nstreetNr\nzipCode\ncity\nid\nname\nalternateNames\nlocation\npopulation\nelevation\nCountry\nneighbour\nbelongsTo\nid\nname\niso2\niso3\nisoNumeric\ncapital\narea\npopulation\ncontinent\nuses\nActivityType\nid\nname\nbelongsTo\nid\nname\nat\nknows(since)\nTransformation CSV\t/\tJSON\nPartner\nId\nFirstName\nLastName\nBirthDate\nProﬁleImageURL\nId\nCustomerId\nWhen\nWhere\nType\nDescription\nContacts\nAddress\nId\nPartnerId\nStreet\nStreetNr\nZipCode\nCity\nCountry\nId\nName\nPrice\nDescription\nImageURL\nProducts\nCustomerId\nProductId\nOrder\nId\nCustomerId\nWhen\nWhere\nInteractionType\nDescription\nCustomer Service\nId\nProductId\nComment\nUserId\nProduct Reviews\nId\nPartnerId\nTwitterId\nFacebookId\nCustomer\nReal-Time Insights?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-27-638.jpg?cb=1498118851\" title=\"Streaming Ingestion into Customer Event Hub&#10;Microservice Cl...\" target=\"_blank\">\n        27.\n      </a>\n    Streaming Ingestion into Customer Event Hub\nMicroservice Cluster\nMicroservice State\n{\t\t}\nAPI\nStream Processing Cluster\nStream\nProcessor\nState\n{\t\t}\nAPI\nBilling &amp;\nOrdering\nCRM /\nProfile\nMarketing\nCampaigns\nLocation\nSocial\nClick\nstream\nSensor\nData\nWeather\nData\nMobile\nApps\nEmail\nFile Import /\nSQL Import\nEvent\nStream\nEvent\nHub\nEvent\nHub\nEvent\nHub\nEvent\nStream\nEvent\nStream\nHadoop ClusterdHadoop Cluster\nDSE Cluster\nCassandra DSE Graph\n32\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-28-638.jpg?cb=1498118851\" title=\"Process native Event Streams&#10;Twitter&#10;Tweet-to-&#10;Cassandra Ca...\" target=\"_blank\">\n        28.\n      </a>\n    Process native Event Streams\nTwitter\nTweet-to-\nCassandra Cassandra\nTweet\nGraph\nTweet-to-Graph\nCustomer\t\nReference\nClick\nStream\nCllck Stream\nActivity-to-\nGraph\nDSE\tCluster\n33\nGeo Point\nCustomer\nAddress\nProduct\nownerOf (since)\ninterestedIn (since, degree)\nlives (since)\nActivityparticipatesIn Termuses\nuses\nEmployee\ninteractsWith\nreference\nid\nname\nid\nwhen\nwhere\ntext\nid\nﬁrstName\nlastName\nbirthDate\nproﬁleImageURL\nuses\nid\nname\nimageURL\nid\nstreet\nstreetNr\nzipCode\ncity\nid\nname\nalternateNames\nlocation\npopulation\nelevation\nCountry\nneighbour\nbelongsTo\nid\nname\niso2\niso3\nisoNumeric\ncapital\narea\npopulation\ncontinent\nuses\nActivityType\nid\nname\nbelongsTo\nid\nname\nat\nknows(since)\nEvent Hub Stream ProcessingSensor\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-29-638.jpg?cb=1498118851\" title=\"Benchmark Single vs. Scripted Insert&#10;34&#10;• One Event ends up...\" target=\"_blank\">\n        29.\n      </a>\n    Benchmark Single vs. Scripted Insert\n34\n• One Event ends up in many\nmodifications of vertex and edges\n• many round-trips need if done with\nsingle API calls\n• batch API calls into a Groovy script\nprovides 3 – 5x performance gains\nDSE\tGraph\nTweet-to-Graph\nTweet\nUser\npublishes\nTerm\t*\nuses*\nDSE\tGraphTweet-to-Graph\n…\n...\nGeo Point\nCustomer\nAddress\nProduct\nownerOf (since)\ninterestedIn (since, degree)\nlives (since)\nActivityparticipatesIn Termuses\nuses\nEmployee\ninteractsWith\nreference\nid\nname\nid\nwhen\nwhere\ntext\nid\nﬁrstName\nlastName\nbirthDate\nproﬁleImageURL\nuses\nid\nname\nimageURL\nid\nstreet\nstreetNr\nzipCode\ncity\nid\nname\nalternateNames\nlocation\npopulation\nelevation\nCountry\nneighbour\nbelongsTo\nid\nname\niso2\niso3\nisoNumeric\ncapital\narea\npopulation\ncontinent\nuses\nActivityType\nid\nname\nbelongsTo\nid\nname\nat\nknows(since)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-30-638.jpg?cb=1498118851\" title=\"Process Change Data Capture Events&#10;CDC&#10;Customer-to-&#10;Cassand...\" target=\"_blank\">\n        30.\n      </a>\n    Process Change Data Capture Events\nCDC\nCustomer-to-\nCassandra Cassandra\nCustomer\nDSE\tGraph\nCustomer-to-\nGraph\nCustomerCDC\nAddressCDC\nContactCDC\nAggregate-to-\nCustomer\nDSE\tCluster\n39\nGeo Point\nCustomer\nAddress\nProduct\nownerOf (since)\ninterestedIn (since, degree)\nlives (since)\nActivityparticipatesIn Termuses\nuses\nEmployee\ninteractsWith\nreference\nid\nname\nid\nwhen\nwhere\ntext\nid\nﬁrstName\nlastName\nbirthDate\nproﬁleImageURL\nuses\nid\nname\nimageURL\nid\nstreet\nstreetNr\nzipCode\ncity\nid\nname\nalternateNames\nlocation\npopulation\nelevation\nCountry\nneighbour\nbelongsTo\nid\nname\niso2\niso3\nisoNumeric\ncapital\narea\npopulation\ncontinent\nuses\nActivityType\nid\nname\nbelongsTo\nid\nname\nat\nknows(since)\nPartner\nId\nFirstName\nLastName\nBirthDate\nProﬁleImageURL\nId\nCustomerId\nWhen\nWhere\nType\nDescription\nContacts\nAddress\nId\nPartnerId\nStreet\nStreetNr\nZipCode\nCity\nCountry\nId\nName\nPrice\nDescription\nImageURL\nProducts\nCustomerId\nProductId\nOrder\nId\nCustomerId\nWhen\nWhere\nInteractionType\nDescription\nCustomer Service\nId\nProductId\nComment\nUserId\nProduct Reviews\nId\nPartnerId\nTwitterId\nFacebookId\nCustomer\nEvent Hub Stream ProcessingSensor\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-31-638.jpg?cb=1498118851\" title=\"Streaming Oriented Ingestion into Customer Hub&#10;Microservice...\" target=\"_blank\">\n        31.\n      </a>\n    Streaming Oriented Ingestion into Customer Hub\nMicroservice Cluster\nMicroservice State\n{\t\t}\nAPI\nStream Processing Cluster\nStream\nProcessor\nState\n{\t\t}\nAPI\nBilling &amp;\nOrdering\nCRM /\nProfile\nMarketing\nCampaigns\nLocation\nSocial\nClick\nstream\nSensor\nData\nWeather\nData\nMobile\nApps\nEmail\nFile Import /\nSQL Import\nEvent\nStream\nEvent\nHub\nEvent\nHub\nEvent\nHub\nEvent\nStream\nEvent\nStream\nHadoop ClusterdHadoop Cluster\nDSE Cluster\nCassandra DSE Graph\n40\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-32-638.jpg?cb=1498118851\" title=\"How to implement an Event Hub?&#10;Apache Kafka to the rescue&#10;•...\" target=\"_blank\">\n        32.\n      </a>\n    How to implement an Event Hub?\nApache Kafka to the rescue\n• publish-subscribe messaging system\n• Designed for processing high-volume,\nreal time activity stream data (logs,\nmetrics, social media, …)\n• Stateless (passive) architecture,\noffset-based consumption\n• Initially developed at LinkedIn, now\npart of Apache\n• Peak Load on single cluster: 2 million\nmessages/sec, 4.7 Gigabits/sec\ninbound, 15 Gigabits/sec outbound\nReliable Data Ingestion in Big Data/IoT\nKafka Cluster\nConsumer Consumer Consumer\nProducer Producer Producer\nBroker 1 Broker 2 Broker 3\nZookeeper\nEnsemble\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-33-638.jpg?cb=1498118851\" title=\"Apache Kafka Connect&#10;• Scalably and reliably streaming data...\" target=\"_blank\">\n        33.\n      </a>\n    Apache Kafka Connect\n• Scalably and reliably streaming data between\nApache Kafka and other data systems\n• not an ETL framework\n• Pre-build connectors available for Data\nSource and Data Sinks\n• JDBC (Source)\n• Cassandra (Source &amp; Sink)\n• Oracle GoldenGate (Source)\n• MQTT (Source)\n• HDFS (Sink)\n• Elasticsearch (Sink)\n• MongoDB (Sink)\nReliable Data Ingestion in Big Data/IoT\nSource: Confluent\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-34-638.jpg?cb=1498118851\" title=\"Declarative Dataflow Definition &amp; Execution&#10;43&#10;Apache NiFi ...\" target=\"_blank\">\n        34.\n      </a>\n    Declarative Dataflow Definition &amp; Execution\n43\nApache NiFi StreamSets\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-35-638.jpg?cb=1498118851\" title=\"Hadoop ClusterdHadoop Cluster&#10;DSE&#10;Cluster&#10;Streaming Ingesti...\" target=\"_blank\">\n        35.\n      </a>\n    Hadoop ClusterdHadoop Cluster\nDSE\nCluster\nStreaming Ingestion into Customer Hub\nOLAP\nMicroservice Cluster\nMicroservice State\n{\t\t}\nAPI\nFile Import /\nSQL Import\nOLTP\nParallel\nProcessing\nCassandra\nCassandra\nStream\nProcessing\nDSEFS\nBilling &amp;\nOrdering\nCRM /\nProfile\nMarketing\nCampaigns\nLocation\nSocial\nClick\nstream\nSensor\nData\nWeather\nData\nMobile\nApps\nEmail\nEvent\nStream\nEvent\nHub\nEvent\nHub\nEvent\nHub\nEvent\nStream\nCassandra\nReplication\n44\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-36-638.jpg?cb=1498118851\" title=\"Hadoop ClusterdHadoop Cluster&#10;DSE&#10;Cluster&#10;Streaming Ingesti...\" target=\"_blank\">\n        36.\n      </a>\n    Hadoop ClusterdHadoop Cluster\nDSE\nCluster\nStreaming Ingestion into Customer Hub\nOLAP\nMicroservice Cluster\nMicroservice State\n{\t\t}\nAPI\nFile Import /\nSQL Import\nOLTP\nParallel\nProcessing\nCassandra\nCassandra\nStream\nProcessing\nDSEFS\nBilling &amp;\nOrdering\nCRM /\nProfile\nMarketing\nCampaigns\nLocation\nSocial\nClick\nstream\nSensor\nData\nWeather\nData\nMobile\nApps\nEmail\nEvent\nStream\nEvent\nHub\nEvent\nHub\nEvent\nHub\nEvent\nStream\nCassandra\nReplication\n45\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-37-638.jpg?cb=1498118851\" title=\"Questions which can only be answered by Graph&#10;46&#10;Dependenci...\" target=\"_blank\">\n        37.\n      </a>\n    Questions which can only be answered by Graph\n46\nDependencies\n•\tFailure\tchains\n•\tOrder\tof\toperation\nMatching\t/\t\nCategorizing\nHighlight\tvariant\tof\t\ndependencies\nClustering\nFinding\tthings\tclosely\t\nrelated\tto\teach\tother\t\n(friends,\tfraud)\nFlow\t/\tCost\nFind\tdistribution\nproblems,\tefficiencies\nSimilarity\nSimilar\tpaths\tor\tpatterns\nCentrality,\tSearch\nWhich\tnodes\tare\tthe\tmost\t\nconnected\tor\trelevant\nSource: Expero\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-38-638.jpg?cb=1498118851\" title=\"Questions which can only be answered by Graph -&#10;Visualize C...\" target=\"_blank\">\n        38.\n      </a>\n    Questions which can only be answered by Graph -\nVisualize Customer 360\n49\nSource: Expero\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-39-638.jpg?cb=1498118851\" title=\"Questions which can only be answered by Graph -&#10;Visualize C...\" target=\"_blank\">\n        39.\n      </a>\n    Questions which can only be answered by Graph -\nVisualize Customer 360\n50\nKeyLines\nCytoscape\nLinkuriousJS\nSigmajsD3\nSource: Expero\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/customer-event-hub-170621095244/95/customer-event-hub-a-modern-customer-360-view-with-datastax-enterprise-dse-40-638.jpg?cb=1498118851\" title=\"Guido Schmutz&#10;Technology Manager&#10;guido.schmutz@trivadis.com...\" target=\"_blank\">\n        40.\n      </a>\n    Guido Schmutz\nTechnology Manager\nguido.schmutz@trivadis.com\n@gschmutz guidoschmutz.wordpress.com\n \n  </li>\n              </ol></div></div></div><aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\"><dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\"><ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"77136685\" title=\"Creative Inspirations: Duarte Design, Presentation Design Studio\" href=\"https://www.linkedin.com/learning/creative-inspirations-duarte-design-presentation-design-studio?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Creative Inspirations: Duarte Design, Presentation Design Studio\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Creative Inspirations: Duarte Design, Presentation Design Studio\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=gN0XHJkdhuJ3Fk75H6mdK0VF9ag%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-kWiGr-MqFYXPoe9ref_qougdWLw\" /></div>\n    <div class=\"lynda-content\"><p>Creative Inspirations: Duarte Design, Presentation Design Studio</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"77136685\" title=\"Common Core: Exploring K-12 Standards\" href=\"https://www.linkedin.com/learning/common-core-exploring-k-12-standards?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Common Core: Exploring K-12 Standards\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Common Core: Exploring K-12 Standards\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=Pz0kwAk3Z6GH2VDJPWLbFcTEQZY%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-gXyGr89KfYXPteM7YZLSioVwefSoBkwAwf-6tRTfmE469LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Common Core: Exploring K-12 Standards</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"77136685\" title=\"Betsy Corcoran on Choosing the Right Technology for Your School\" href=\"https://www.linkedin.com/learning/betsy-corcoran-on-choosing-the-right-technology-for-your-school?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Betsy Corcoran on Choosing the Right Technology for Your School\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Betsy Corcoran on Choosing the Right Technology for Your School\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=Vv%2BjDznrmOCFyVXKUUk43xgDD9A%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-kWy2j_9afZXfhcMLcZLSiolwXeygCkAE2e--gSTDkEI69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Betsy Corcoran on Choosing the Right Technology for Your School</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"56913068\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"How to build an effective omni-channel CRM &amp; Marketing Strategy &amp; 360 customer profile\" href=\"https://www.slideshare.net/Comarch_Benelux/how-to-build-an-effective-omnichannel-crm-marketing-strategy-360-customer-profile\">\n    \n    <div class=\"related-content\"><p>How to build an effective omni-channel CRM &amp; Marketing Strategy &amp; 360 custome...</p><p>Comarch</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"1011944\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Connected Banking Framework\" href=\"https://www.slideshare.net/kashif.akram/connected-banking-framework\">\n    \n    <div class=\"related-content\"><p>Connected Banking Framework</p><p>Kashif Akram</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"77212830\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"A Customer-Centric Banking Platform Powered by MongoDB\" href=\"https://www.slideshare.net/mongodb/a-customercentric-banking-platform-powered-by-mongodb\">\n    \n    <div class=\"related-content\"><p>A Customer-Centric Banking Platform Powered by MongoDB</p><p>MongoDB</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"65447688\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"ANTS - 360 view of your customer - bigdata innovation summit 2016\" href=\"https://www.slideshare.net/dinhledat/ants-360-view-of-your-customer-bigdata-innovation-summit-2016\">\n    \n    <div class=\"related-content\"><p>ANTS - 360 view of your customer - bigdata innovation summit 2016</p><p>Dinh Le Dat (Kevin D.)</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"37801463\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"360° View of Your Customers\" href=\"https://www.slideshare.net/OSF_Global_Services/360-37801463\">\n    \n    <div class=\"related-content\"><p>360° View of Your Customers</p><p>OSF Commerce</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"79365293\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Graph in Customer 360 - StampedeCon Big Data Conference 2017\" href=\"https://www.slideshare.net/StampedeCon/graph-in-customer-360-stampedecon-big-data-conference-2017\">\n    \n    <div class=\"related-content\"><p>Graph in Customer 360 - StampedeCon Big Data Conference 2017</p><p>StampedeCon</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"13032651\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"CMA  Summit 2012\" href=\"https://www.slideshare.net/delviniainteractive/cma-summit-2012\">\n    \n    <div class=\"related-content\"><p>CMA  Summit 2012</p><p>Delvinia</p></div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div></div><footer>\n          <div class=\"row\"><div class=\"columns\"><ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div></div>\n          \n          <div class=\"row\"><div class=\"columns\"><p class=\"copyright text-center\">LinkedIn Corporation © 2018</p></div></div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div>\n    <div class=\"row\"><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></div>\n        \n    </div>\n    \n    \n      <noscript>\n    </noscript>",
        "created_at": "2018-10-04T20:46:37+0000",
        "updated_at": "2018-10-04T20:46:49+0000",
        "published_at": null,
        "published_by": [
          "Guido Schmutz"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 12,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/customer-event-hub-170621095244-thumbnail-4.jpg?cb=1498118851",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12293"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 35,
            "label": "docker",
            "slug": "docker"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 321,
            "label": "zookeeper",
            "slug": "zookeeper"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 1287,
            "label": "spring",
            "slug": "spring"
          }
        ],
        "is_public": false,
        "id": 12292,
        "uid": null,
        "title": "tsarenkotxt/microservices",
        "url": "https://github.com/tsarenkotxt/microservices",
        "content": "<h3>Introduction</h3><p><code>Pet</code> microservices project.</p><ul><li>Docker</li>\n<li>Spring Boot</li>\n<li>Cassandra</li>\n<li>ZooKeeper</li>\n<li>Kafka</li>\n<li>REST API</li>\n</ul><h3>Developer Environment</h3><p>Build docker images</p><pre>chmod +x ./docker-build.sh &amp;&amp; ./docker-build.sh\n</pre><p>Run</p><pre>docker-compose up\n</pre><h3>Architecture</h3><p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/tsarenkotxt/microservices/blob/master/readme/diagram.png\"><img src=\"https://github.com/tsarenkotxt/microservices/raw/master/readme/diagram.png\" alt=\"\" /></a></p>",
        "created_at": "2018-10-04T20:38:02+0000",
        "updated_at": "2018-11-06T00:01:08+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/19788417?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12292"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1267,
            "label": "microservices",
            "slug": "microservices"
          }
        ],
        "is_public": false,
        "id": 12291,
        "uid": null,
        "title": "Apache Cassandra in a Microservices Enterprise Platform",
        "url": "https://tech.finn.no/2015/04/28/Apache-Cassandra-in-a-Microservices-Enterprise-Platform/",
        "content": "<p>In this article we’ll explore how Apache Cassandra, the world’s most popular wide column store and 8th most <a href=\"http://db-engines.com/en/ranking\">popular</a> database overall, will only grow as a cornerstone technology in a microservices platform. With a little theory to microservices, to some examples of microservices and underlying required infrastructure, we’ll show that any solution both capable of scaling and dealing with time-series data-models is going to need to depend upon Apache Cassandra as a persistence layer, despite having a polyglot persistence model at large.</p><h2 id=\"microservices\">microservices</h2><p>Microservices is a term that’s come out of <a href=\"http://martinfowler.com/articles/microservices.html\">ThoughtWorks’</a> Martin Fowler and James Lewis. It’s a bit of a buzzword, basically a fresh revival of the parts of service orientated architecture that you should be focusing on and getting right. A lot of it hopefully is obvious to you already. If you’ve been doing service orientated architecture or even generally just unix programming properly over the years it might well be frustrating just how buzz “microservices” has become. But it’s worth keeping in mind how much garbage we’ve collected and how many aspects of service orientated architecture that we’ve gotten badly wrong over the years. Younger programmers certainly deserve the clarity that ThoughtWorks is giving us here.</p><p>Microservices, following the tips and guidelines from Sam Newman, can basically be broken down into four groups.</p><h3 id=\"interfaces\">interfaces</h3><p>Ensure that you standardise the systems architecture at large and especially the gaps or what we know as the APIs between services. Standardise upon practices and protocols that minimise coupling. Move from tightly coupled systems with many compile time dependencies and distributed published client libraries, to clearly defined and isolated runtime APIs. Take advantage of REST, especially level 3 in richardson’s maturity model, for the synchronous domain driven designed parts of your system. When it comes to event driven design use producer defined schemas, like that offered by Apache Thrift’s IDL which gives you embedded schemas for good forward and backward compatibility along with isolated APIs that prevent transitive dependencies creeping through your platform. Getting this right also means that within services teams get a lot more freedom and autonomy to implement as they like, which in turns diminishes the effects of Brook’s law.</p><h3 id=\"deployment\">deployment</h3><p>Simplify deployment down to having just one way of deploying artifacts, of any type, to any environment. The process of deployment needs to be so easy that deploying continuously each and every change into production becomes standard practice. This often also requires some organisational and practical changes like moving to stable master codebases and getting developers comfortable with working with branches and <a href=\"http://tech.finn.no/2013/06/20/dark-launching-and-feature-toggles/\">dark launching</a>.</p><h3 id=\"monitoring\">monitoring</h3><p>It isn’t just about the motto of “monitor everything” and to have all metrics and logs accessible in one central place, but to include synthetic requests to provide monitoring alerts that catch critical errors immediately, and to use correlation IDs to be able to easily put all the moving parts together for any one specific request.</p><h3 id=\"architectural-safety\">architectural safety</h3><p>Addressing the fallacies of distributed computing, ensure that services are as available as possible and consumers handle failures gracefully by using such mechanisms as circuit breakers, load balancing, and bulkheads.</p><p>If you want more than I can only highly recommend Sam Newman’s just published book on <a href=\"http://shop.oreilly.com/product/0636920033158.do\">Building Microservices</a>.</p><p><img class=\"center-block\" width=\"150\" alt=\"building microservices\" src=\"https://tech.finn.no/images/2015-04-28-Apache-Cassandra-in-a-Microservices-Enterprise-Platform/building-microservices.jpg\" /></p><p>In this article, and when talking about microservices, i’m most interested in how Cassandra, one of the most popular databases in our industry and the database most realistic to the practical realities of distributed computing, comes into its own.\nHere Cassandra is relevant to the monitoring and architectural safety aspects of microservices, from looking at how monitoring is typically time series data, a known strength for Cassandra, and looking into how modern distributed systems should be put together.</p><p>Having worked in the enterprise for over a decade it’s clear that the relational database is at juxtaposition to the rest of our industry. The way we code against the RDMS, layer domain logic upon it, and at various layers up through the stack add additional complexity and cyclic dependencies with caches that require invalidation, makes it all too obvious we’ve been doing things wrong. You can see it in plain sight when watching presentations where people show their wonderful service orientated architectures or microservices platforms, and despite talking about how their services scale, are fault-tolerant and resilent, maybe even throwing in some fancy messaging system, there is still in the corner of their diagrams the magic unicorn – the relational database. Amidst all the promotion and praise for BASE architectures, the acceptance that our own services and those infrastructural like Solr, Elastic Search, or Kafka, need to work with eventual consistency so to achieve performance and availability, the relational database somehow gets a free ride, an exception, to all this common sense.</p><p>Martin Kleppman presented at Strange Loop last year and afterwards wrote an article <a href=\"http://blog.confluent.io/2015/03/04/turning-the-database-inside-out-with-apache-samza/\">“Turning the database inside out with Apache Sanza”</a> that properly hits the nail on the head, perfectly describing my own woes around why the relational database has ruined back-end programming for us. And he sums it up rather elegantly to that the replication mechanism to databases needs to come out and become its own integral and accepted component to our systems designs. And this externalised replication is what we call streams and event driven design, and it leads us to de-normalised datasets and more time-series data models.</p><p><img class=\"center-block\" alt=\"Data flow\" src=\"https://confluentinc.files.wordpress.com/2015/03/slide-40.png?w=400\" /></p><h2 id=\"product-examples\">product examples</h2><p>Let’s look at a few examples from FINN.no and see how these things work in practice.</p><p>First of all we know that Cassandra has a number of known strengths over other databases, from dealing with large volumes of data and providing superior performance on both write and read performance, to time series data and time-to-live data.</p><p>But one of the areas that’s not highlighted enough is that Cassandra’s CQL schema often provides a simpler schema over the SQL equivalent, something that’s easier to work with and for programmers today something that uses more natural types and fluent APIs.</p><p>After all the whole point with your microservices platform is that you’re writing smaller and smaller services, and those smaller services each come with their own private data models. As services and their schemas get smaller and simpler we find that we don’t need relationships and constraints and all the other complexities that the RDMS has to offer. Rather the constructs available to us in CQL are superior, and faster.</p><p>The first example is how we store the users search history. This shouldn’t be a product that needs to be explained to anyone. The CQL schema to this is incredibly simple and it takes advantage of the combination between partition and clustering keys. And it operates fast, just make sure to apply the “CLUSTERING ORDER BY” or you’ll be falling into a Cassandra anti-pattern where you’ll be left reading tons of tombstones each read.</p><figure class=\"highlight\"><pre class=\"language-sql\" data-lang=\"sql\">CREATE TABLE users_search_history (\n  user_id      text,\n  search_id    timeuuid,\n  search_url   text,\n  description  text,\n  PRIMARY KEY  (user_id, search_id)\n)\nWITH CLUSTERING ORDER BY (search_id desc);</pre></figure><p>Another example is fraud detection, and while fraud detection is typically a complicated bounded context at large, breaking it down you may find individual components using small simple isolated schemas. Here we have a CQL schema, much simpler than its relational SQL schema counterpart not only because is it time-series using the clustering key, but using Cassandra’s collection type to store the scores of each of the rules calculated during the fraud detection’s expert rules system.</p><figure class=\"highlight\"><pre class=\"language-sql\" data-lang=\"sql\">CREATE TABLE ad_scores (\n  adid        bigint,\n  updated     timeuuid,\n  rules       map&lt;text, int&gt;,\n  PRIMARY KEY (adid, updated)\n)</pre></figure><p>So it shouldn’t be of any surprise that Cassandra is going to hit the sweet spot for particular services in a number of different ways in any polyglot persistence platform. But bring it back to the bigger picture and we can look at how we can remove that magic unicorn we keep seeing in systems designs’ overviews.</p><h2 id=\"brewers-theorem\">brewer’s theorem</h2><p>Looking at the <a href=\"http://en.wikipedia.org/wiki/CAP_theorem\">CAP theorem</a> you recognise that to build a BASE microservices platform it means building AP systems. When you look at Martin Kleppman’s message that the replication is its own concern in your BASE platform, when you look at domain driven design and how to focus keeping your services within clear bounded contexts and then taking it further to use event driven design to further break those bounded contexts apart, you see that it ties back to the CAP theorem and it is for the sake of scalability and performance and even just simplicity in design, a preference for availability over consistency. Looking into it deeper in how streaming solutions often still need to write to raw event stores, and similar to a event sourcing model when a service needs to bootstrap its de-normalised dataset from scratch from data beyond that to which is stored in the stream’s history, you can see there is a parallel to partition tolerance and how it, just like partition tolerance within the CAP theorem, is a hard fast requirement to any distributed architecture.</p><p><img class=\"center-block\" alt=\"CAP theorem\" src=\"https://tech.finn.no/images/2015-04-28-Apache-Cassandra-in-a-Microservices-Enterprise-Platform/cap.png\" /></p><p>Here’s a simple example of a web application (named “xxx”) making three synchronous requests to underlying services in our platform when the user logs in. One service call to do the authentication, and the other two to fetch user data due to that user data being stored/available in different back-end systems.</p><p><img class=\"center-block\" width=\"400\" alt=\"Finn platform\" src=\"https://tech.finn.no/images/2015-04-28-Apache-Cassandra-in-a-Microservices-Enterprise-Platform/finn-platform-1.png\" /></p><p>It’s not difficult to see this isn’t a great design. First of all it’s keeping all the logic on how these services calls are initiated and how the data joined together high up in the presentation layer. It’s also not a great performer unless you’re willing to introduce concurrency code up in your presentation layer.</p><p>The obvious thing to do is introduce an aggregate service so that the web app only needs to make two inner requests and much of the logic, including any concurrency code, is pushed down into the platform and into the bounded context where it belongs. Another thing that typically happens here is that a cache, one that requires invalidation, is added into the aggregate service to address performance and availability.</p><p><img class=\"center-block\" width=\"400\" alt=\"Finn platform\" src=\"https://tech.finn.no/images/2015-04-28-Apache-Cassandra-in-a-Microservices-Enterprise-Platform/finn-platform-2.png\" /></p><p>But it’s a hack. Now you have more network traffic than before and more overall complexity, and just a poor and possibly very slow system of eventual consistency.</p><p>There is a better way, imagine there was but one service and all the data in a shared schema and then bring the replication mechanism of the database out into a stream to realise this. That is de-normalise the data from the auxiliary user-profile service back into the original user service. What you end up with is faster, more available, better scaling solution. Once you’re in the swing of event driven design and de-normalised datasets this is simpler solution too.</p><p><img class=\"center-block\" width=\"400\" alt=\"Finn platform\" src=\"https://tech.finn.no/images/2015-04-28-Apache-Cassandra-in-a-Microservices-Enterprise-Platform/finn-platform-3.png\" /></p><h2 id=\"infrastructure-examples\">infrastructure examples</h2><p>With some ideas of how Cassandra can become important for a successful microservices platform within product development let’s look into how Cassandra fits into the infastructure and operations side of things. A trap i suspect a lot of people are getting themselves into when starting off with microservices is that they haven’t got the infrastructure required in place first. Even if James Lewis and Sam Newman puts extra emphasis on the needs for deployment and monitoring tools it still can be all too easily overlooked just how demanding this really is. It’s not just about monitoring and logging everything and then making it available in a centralised place. It’s not just about having reproducible containers on an elastic platform, but about all the infrastructure tools and services being rock stable and equally elastic. You don’t want to be running a microservices platform and have crucial monitoring and logging tools fail on you, particularly in any crisis or in the middle of any critical operation.</p><p>When it comes to correlation IDs a brilliant tool out there is Zipkin from Twitter. Zipkin provides for you in all your applications and services this correlation ID, a unique ID for each user request, which you can for example put into your log4j thread context or MDC and then via a tool like Kibana be able to put together all the logs from across your whole platform for one specific user request. But Zipkin goes a lot further than this, based off Google’s Dapper paper, it provides for you distributed tracing or profiling of these individual requests.</p><p><img class=\"center-block\" alt=\"Zipkin\" src=\"https://tech.finn.no/images/2015-04-28-Apache-Cassandra-in-a-Microservices-Enterprise-Platform/zipkin.jpg\" /></p><p>Naturally Zipkin can be put together with Cassandra, the best fit as it’s perfect for large volumes of time series data. We also use scribe for the sending of the trace messages from all the jvms throughout our platform over to the zipkin collector which then stores them into Cassandra.</p><p>Below is the typical page in Zipkin. Under the list of services the first row is the user’s request, here we can see that it took 195ms. Then under that we can see when and how long all the individual service calls took place. We can see which back-end services are running properly in parallel and which service calls are sequential. Services like Solr, Elastic Search, the Kafka producers, and of course Cassandra, are all listed as well. Not only is this fantastic for keeping your platform tuned for performance but it’s a great tool for helping to figure out what’s going on with those slow requests you’ve got, for example in the top 5th percentile.</p><p><img class=\"center-block\" width=\"400\" alt=\"Zipkin request page\" src=\"https://tech.finn.no/images/2015-04-28-Apache-Cassandra-in-a-Microservices-Enterprise-Platform/zipkin-request-page.png\" /></p><p>It’s also a great tool to help keep teams up to date with all the constantly evolving moving parts that exist in a microservices platform, something that’ll no doubt be outdated a week after any manual catalog documentation was written. This is especially useful for front end developers that usually haven’t the faintest idea what’s going on behind the scenes.</p><p>This visualisation can also be offered from within the browser, both Firefox and Chrome have plugins, so that developers can see what’s happening near real-time as they make requests.</p><p><img width=\"400\" class=\"center-block\" alt=\"Zipkin graph\" src=\"https://tech.finn.no/images/2015-04-28-Apache-Cassandra-in-a-Microservices-Enterprise-Platform/zipkin-graph.png\" /></p><p>Something that we’re added to Zipkin is a cascasding job that runs nightly in our hadoop yarn cluster, that aggregates all the different traces made during the day and builds up a graph of the platform showing which services are calling services. In this graph on the left hand side you will see our web and batch applications, then to the right of that the microservices moving down the stack the further to the right you go. Legacy databases with shared schemas end up as big honey pots on the very right while databases with properly isolated schemas appear as satellites to the services that own them. If you’re undertaking a move towards event driven design then you’ll see the connections between services and especially across bounded contexts break apart, and you should see those bounded contexts become more grouped neighbourhoods for themselves.</p><p>This cascading job that aggregates this data should now be available in the <a href=\"https://github.com/twitter/zipkin\">original Twitter Github repository</a>, otherwise you’ll find it in FINN’s fork of it.</p><p>Another crucial infrastructure tool is Grafana, and the Graphite and StatsD stack underneath it. Grafana and Graphite is one of those must-have tools in your infrastructure, but the problem is it just doesn’t scale. Indeed the carbon and whisper components to graphite are dead in the water. We’ve hit this problem and looked into the alternatives, and there’s one interesting alternative out there based on Cassandra, which only makes sense as it’s another perfect match for time series data.</p><p>The plugin to Graphite is called Cyanite and very simply replaces all the carbon and whisper components. In an earlier version it was quite limited and you couldn’t for example get wildcarded paths in graphite working, but it now bundles with Elastic Search to give you a fully functional Graphite.</p><div class=\"line\"><img class=\"unit\" width=\"250\" alt=\"Graphite Carbon\" src=\"https://tech.finn.no/images/2015-04-28-Apache-Cassandra-in-a-Microservices-Enterprise-Platform/graphite-carbon.png\" /><img class=\"unit\" width=\"450\" alt=\"Graphite Cyanite\" src=\"https://tech.finn.no/images/2015-04-28-Apache-Cassandra-in-a-Microservices-Enterprise-Platform/graphite-cyanite.png\" /></div><p>If you want to take a go at setting this up and see for yourself just how easy it is to get running, and how easily Grafana, Graphite, Cyanite, Elastic Search, and Cassandra, are configured together take a look at the GitHub repository <a href=\"https://github.com/mbrannigan/docker-cyanite-grafana\">docker-cyanite-grafana</a>. It’s a docker image – just run <code class=\"highlighter-rouge\">build.sh</code> and once everything has started up run <code class=\"highlighter-rouge\">test.sh</code> to start feeding in dummy metrics and test away all the grafana features you’re used to working with.</p><h2 id=\"a-compliment-to-the-modern-enterprise-platform\">a compliment to the modern enterprise platform</h2><p>With this run through of just a few product and infrastructure examples it’s quickly obvious how useful Cassandra is to have established in any polyglot persistence model. And with an understanding on distributed computing it becomes even harder to deny Cassandra its natural home in the modern enterprise platform.</p>Tags:<a href=\"https://tech.finn.no/2015/04/28/Apache-Cassandra-in-a-Microservices-Enterprise-Platform/tags/\">Microservices</a><a href=\"https://tech.finn.no/2015/04/28/Apache-Cassandra-in-a-Microservices-Enterprise-Platform/tags/\">Cassandra</a><a href=\"https://tech.finn.no/2015/04/28/Apache-Cassandra-in-a-Microservices-Enterprise-Platform/tags/\">REST</a><a href=\"https://tech.finn.no/2015/04/28/Apache-Cassandra-in-a-Microservices-Enterprise-Platform/tags/\">Thrift</a><a href=\"https://tech.finn.no/2015/04/28/Apache-Cassandra-in-a-Microservices-Enterprise-Platform/tags/\">Kafka</a>",
        "created_at": "2018-10-04T20:25:07+0000",
        "updated_at": "2018-10-04T20:25:13+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 14,
        "domain_name": "tech.finn.no",
        "preview_picture": "https://tech.finn.no/images/tech_logo2.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12291"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 901,
            "label": "security",
            "slug": "security"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          },
          {
            "id": 1021,
            "label": "gdpr",
            "slug": "gdpr"
          }
        ],
        "is_public": false,
        "id": 12286,
        "uid": null,
        "title": "DataStax and the New GDPR",
        "url": "https://www.datastax.com/2018/04/datastax-and-the-new-gdpr",
        "content": "DataStax and the New GDPR |  DataStax\n\n<noscript>\n\n\n\n<div class=\"DS17\"><div class=\"connect-us\"><a href=\"https://www.datastax.com/contactus\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Mail.svg\" alt=\"email icon\" />email</a><a href=\"https://www.datastax.com/company#offices\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Phone.svg\" alt=\"phone icon\" />call</a></div></div> \n\t\t \n\t\n\n    \n      <div class=\"DS17\"><div class=\"use-case\"><div class=\"wrapper\"><div class=\"two-col text-light-blue\"><h6>Customer Experience</h6><ul><li><a href=\"https://www.datastax.com/use-cases/customer-360\">Customer 360</a></li>\n          <li><a href=\"https://www.datastax.com/personalization\">Personalization &amp; Recommendations</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/loyalty-programs\">Loyalty Programs</a></li>\n          <li><a href=\"https://www.datastax.com/fraud-detection\">Consumer Fraud Detection</a></li>\n        </ul></div><div class=\"two-col text-light-green\"><h6><a href=\"#\">Enterprise Optimization</a></h6><ul><li><a href=\"https://www.datastax.com/use-cases/ecommerce\">eCommerce</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/identity-management\">Identity Management</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/security\">Security and Compliance</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/supply-chain\">Supply Chain</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/inventory-management\">Inventory Management</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/asset-monitoring\">Asset Monitoring</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/logistics\">Logistics</a></li>\n        </ul></div></div></div></div>\n  \n    \n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\n</noscript>",
        "created_at": "2018-10-03T13:28:04+0000",
        "updated_at": "2018-10-03T13:28:11+0000",
        "published_at": "2018-04-06T20:35:27+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 0,
        "domain_name": "www.datastax.com",
        "preview_picture": "https://www.datastax.com/wp-content/uploads/2018/04/GDPR_Blog_Twitter_Banner.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12286"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 901,
            "label": "security",
            "slug": "security"
          },
          {
            "id": 1021,
            "label": "gdpr",
            "slug": "gdpr"
          }
        ],
        "is_public": false,
        "id": 12285,
        "uid": null,
        "title": "GDPR and Cassandra - DZone Security",
        "url": "https://dzone.com/articles/gdpr-and-cassandra",
        "content": "<div class=\"content-html\" itemprop=\"text\"><p>delete /dɪˈliːt/ — verb: remove or obliterate (written or printed matter), especially by drawing a line through it.</p> \n<h3 id=\"01b5\">Schrödinger’s Data</h3> \n<p>As we all know, GDPR will be <a data-href=\"https://www.eugdpr.org/gdpr-faqs.html\" href=\"https://www.eugdpr.org/gdpr-faqs.html\" rel=\"nofollow\" target=\"_blank\" title=\"GDPR faqs\">in force May 2018</a>. After that, users of software products and services will have the right to be forgotten (cool, right? Finally I can rest assured that my browsing history will not be read aloud at my funeral). In other words, if a user from the EU asks a service provider to delete their data, the provider will have to delete all the user’s data or face severe consequences.</p> \n<p>But, it is unclear what it means to delete a user’s data. I guess the only way to find out is when the audit occurs.</p> \n<figure id=\"a0c8\"><figure><img src=\"https://cdn-images-1.medium.com/max/1600/1*UgssftJJmE2zL5ClenYe2w.png\" class=\"fr-fin fr-dib\" alt=\"The data is both deleted and not deleted until observed.\" title=\"The data is both deleted and not deleted until observed.\" /><figcaption>\n   A user’s data is simultaneously both deleted and not deleted until observed at the time of the audit. \n </figcaption></figure><p>This post is the introduction to a series of blog posts about GDPR and Cassandra databases.</p> \n<h3 id=\"daa2\">Cassandra and Data Deletion</h3> \n<p>As Cassandra consultants, our primary concern is: what does it mean to delete the data from Cassandra points of view? And what we can do to be as sure as possible that a user’s data will stay deleted. As we know, when Cassandra deletes the data, it just <a data-href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlAboutDeletes.html\" href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlAboutDeletes.html\" rel=\"nofollow\" target=\"_blank\">marks it as deleted</a>. The actual “deletion” occurs during the <a data-href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlHowDataMaintain.html#dmlHowDataMaintain__dml-compaction)\" href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlHowDataMaintain.html#dmlHowDataMaintain__dml-compaction%29\" rel=\"nofollow\" target=\"_blank\" title=\"Compaction docs\">compaction process</a>.</p> \n<p>When Cassandra marks data as deleted:</p> \n<ul><li id=\"5f85\">It can’t be fetched anymore using Cassandra’s query language (cql).</li> \n <li id=\"0fa5\">The data still exists in Cassandra’s files on the disk (SSTables) but is flagged as deleted.</li> \n <li id=\"8000\">The data is removed (for real) from SSTables when compaction occurs before the compaction evicts deleted data, the deleted data can still be accessed with specialized (forensic?) tools.</li> \n</ul><p>Once again: Cassandra, like many other systems, does not actually delete data when it <em>deletes the data</em>. But this is in line with the definition of the verb delete from the Oxford dictionary:</p> \n<blockquote id=\"df76\"><div>\n  “remove or obliterate (written or printed matter), especially by drawing a line through it.” \n</div></blockquote> \n<p>On the other hand, a similar thing happens in the underlying OS (Linux). When the OS deletes a file, it just marks it as deleted. And you can <a data-href=\"https://unix.stackexchange.com/a/80285\" href=\"https://unix.stackexchange.com/a/80285\" rel=\"nofollow\" target=\"_blank\" title=\"recover deleted files\">recover the deleted files</a> with specialized forensic tools.</p> \n<p>Okay, so the actual, irreversible deleting of the data does not usually happen in the software engineering. But we would love to do as much as we can to make sure that the data is not accessible from Cassandra and any Cassandra tooling (like <em>sstabledump</em>, <em>sstable2json</em>). OS and file system engineers should do their part of work by doing the same for the OS level (if they think that’s necessary).</p> \n<figure id=\"df70\"><figure><img src=\"https://cdn-images-1.medium.com/max/1600/1*PnwAkR8_J22gyGY9swXPHA.png\" class=\"fr-fin fr-dib\" alt=\"image\" /><figcaption>\n   The only way to make sure that the data stays deleted. \n </figcaption></figure><p>Another problem in Cassandra is that it is hard to filter on fields that are not part of the primary key. So, if some of the user’s data is held in the table where the primary key is something like <em>deviceId</em>, that would mean that we would have to search all the records for all the <em>deviceIds</em> and remove the corresponding user’s data. That does not scale.</p> \n<h4 id=\"abef\">Data Deletion and Compactions</h4> \n<p>As already said, even after a delete statement is issued, it is not guaranteed that the data is deleted. Furthermore, if the data model is not well designed, the deleted data might never get evicted. In Cassandra 3.10, this behavior is improved, and compaction is triggered when there is a certain percent of expired tombstones (read more about it <a data-href=\"https://issues.apache.org/jira/browse/CASSANDRA-7019\" href=\"https://issues.apache.org/jira/browse/CASSANDRA-7019\" rel=\"nofollow\" target=\"_blank\" title=\"tombstone compaction\">here</a>), and <a data-href=\"https://github.com/protectwise/cassandra-util/tree/master/deleting-compaction-strategy\" href=\"https://github.com/protectwise/cassandra-util/tree/master/deleting-compaction-strategy\" rel=\"nofollow\" target=\"_blank\" title=\"deleting compaction strategy\">deleting compaction strategy</a> looks like it could solve this problem (note that the strategy is not an official part of Apache Cassandra). Also, I’m quite sure that I saw a Jira issue on an Apache Cassandra project about some other kind of Deleting compaction strategy, which should guarantee to actually delete the data, not only mark it as deleted, but I can’t find it now. That would be cool.</p> \n<p>Speaking of compaction strategies, <a data-href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlHowDataMaintain.html?hl=stcs#dmlHowDataMaintain__stcs-compaction\" href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlHowDataMaintain.html?hl=stcs#dmlHowDataMaintain__stcs-compaction\" rel=\"nofollow\" target=\"_blank\" title=\"STCS\">SizeTieredCompactionStrategy</a> can be tricky, because if you end up with one huge SSTable file, you need SSTables of a similar size to compact them. Which means that the tombstones will stay in a huge SSTable for a very long time; maybe forever. A situation similar to the one occurring in the 2048 game:</p> \n<p><img alt=\"2048\" class=\"fr-fin fr-dib\" src=\"https://cdn-images-1.medium.com/max/1600/1*BqjYx2m8F9u4_6lYyQj_2g.png\" title=\"2048\" width=\"452\" /></p> \n<p>Tile 2048 will not be merged anytime soon.</p> \n<p>The main takeaway is: be aware of how different compaction strategies work and know your system behavior. If you have a problem with tombstone eviction, it might be a good idea to change your compaction strategy and/or to redesign your tables</p> \n<h4 id=\"1794\">Delete User Data That Is Not Part of the Primary Key</h4> \n<p>Unlike in relational databases, in Cassandra data is stored in denormalized form. Thus, it is not possible to (easily) filter on fields that are not part of the partition key. So, if we have the following table:</p> \n<pre lang=\"text/x-cassandra\">CREATE TABLE device_measurements (\n  device_id uuid,   \n  measurement_type text,\n  measurement_value text,   \n  user_id uuid,   \n  PRIMARY KEY (device_id, measurement_type));</pre> \n<p>This means that we cannot just:</p> \n<pre lang=\"text/plain\">DELETE FROM device_measurement WHERE user_id = bf884b98–0a72–10e8-ba89–0ed5f89f718b</pre> \n<p>It is, however, possible to issue:</p> \n<pre lang=\"text/plain\">DELETE FROM device_measurement WHERE user_id = bf884b98–0a72–10e8-ba89–0ed5f89f718b ALLOW FILTERING</pre> \n<p>But this might ruin the performance of the entire cluster.</p> \n<p>Therefore, we should think about the user’s data in advance when designing the tables.</p> \n<h4 id=\"0b3f\">Embracing Privacy by Design</h4> \n<p>Solution 1: design tables in a way that the user’s data can be easily deleted (user_id part of the primary key) from all the tables. This solution will obviously have an impact on the design process in both greenfield projects and when redesigning existing databases.</p> \n<p>Solution 2: embrace encryption. Okay, this is not a production-ready solution, it’s more of an idea we’re currently playing with at <a data-href=\"https://www.smartcat.io/\" href=\"https://www.smartcat.io/\" rel=\"nofollow\" target=\"_blank\">SmartCat</a>. Encrypting the stored user’s data with <a data-href=\"https://github.com/shaih/HElib\" href=\"https://github.com/shaih/HElib\" rel=\"nofollow\" target=\"_blank\" title=\"homomorphic encryption\">homomorphic encryption</a> to preserve the ordering of clustering columns, and when the data needs to be deleted, just delete the key. If you have any thoughts on this or experience to share, we would love to hear from you.</p> \n<h3 id=\"5b69\">Conclusion</h3> \n<p>Embrace Privacy by design. The idea of GDPR is a good thing from a consumer perspective. A user’s data will be seen as a liability for the companies, not as an asset, which means that companies will, hopefully, be cautious when storing a user’s data. GDPR is also an excellent opportunity for new players on a database as a service market (DaaS) or some derivative of the concept; it seems that it is easier to build new systems with privacy in mind from scratch than to refactor the existing ones. What I would like to see is a database (as a service) that would allow me to issue a delete for the userId, and for me (as a programmer/user of the database) to stop worrying about it. The DaaS provider would be responsible for the rest.</p> \n<p>What are your thoughts on this?</p></figure></figure></div><div class=\"content-html\" itemprop=\"text\"><a>\n                        <img class=\"pub-image\" width=\"420\" itemprop=\"image\" src=\"src\" alt=\"image\" /></a></div>",
        "created_at": "2018-10-03T13:26:32+0000",
        "updated_at": "2018-10-03T13:26:41+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 5,
        "domain_name": "dzone.com",
        "preview_picture": "https://dz2cdn3.dzone.com/storage/article-thumb/8331038-thumb.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12285"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1104,
            "label": "terraform",
            "slug": "terraform"
          },
          {
            "id": 1208,
            "label": "chef",
            "slug": "chef"
          }
        ],
        "is_public": false,
        "id": 12282,
        "uid": null,
        "title": "Easy Cassandra scaling",
        "url": "https://medium.com/teads-engineering/easy-cassandra-scaling-with-terraform-chef-rundeck-9443e0375aa7",
        "content": "<section class=\"section section--body section--first\"><div class=\"section-divider\"><hr class=\"section-divider\" /></div><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><h1 id=\"3a2a\" class=\"graf graf--h3 graf--leading graf--title\">Easy Cassandra scaling</h1><h2 id=\"74a3\" class=\"graf graf--h4 graf-after--h3 graf--subtitle\">With Terraform, Chef and Rundeck</h2><figure id=\"2439\" class=\"graf graf--figure graf-after--h4\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*1gsGOvBiRupn3TwGVFDZOw.png\" data-width=\"1600\" data-height=\"727\" data-action=\"zoom\" data-action-value=\"1*1gsGOvBiRupn3TwGVFDZOw.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*1gsGOvBiRupn3TwGVFDZOw.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\">Painting of Cassandra by <a href=\"https://commons.wikimedia.org/wiki/Evelyn_de_Morgan\" data-href=\"https://commons.wikimedia.org/wiki/Evelyn_de_Morgan\" class=\"markup--anchor markup--figure-anchor\" title=\"Evelyn de Morgan\" rel=\"nofollow noopener\" target=\"_blank\">Evelyn de Morgan</a> — Wikimedia</figcaption></div></figure><p id=\"8660\" class=\"graf graf--p graf-after--figure\"><em class=\"markup--em markup--p-em\">When dealing with many Cassandra clusters, day-to-day operations can get complicated. At Teads, we like to </em><a href=\"https://landing.google.com/sre/book/chapters/eliminating-toil.html\" data-href=\"https://landing.google.com/sre/book/chapters/eliminating-toil.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\"><em class=\"markup--em markup--p-em\">eliminate toil</em></a><em class=\"markup--em markup--p-em\"> as much as possible. In this article, we describe how we automate scaling operations on our AWS infrastructure using </em><a href=\"https://www.terraform.io/\" data-href=\"https://www.terraform.io/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\"><em class=\"markup--em markup--p-em\">Terraform</em></a><em class=\"markup--em markup--p-em\"> coupled with </em><a href=\"https://docs.chef.io/\" data-href=\"https://docs.chef.io/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\"><em class=\"markup--em markup--p-em\">Chef</em></a><em class=\"markup--em markup--p-em\"> and </em><a href=\"http://rundeck.org/\" data-href=\"http://rundeck.org/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\"><em class=\"markup--em markup--p-em\">Rundeck</em></a><em class=\"markup--em markup--p-em\">.</em></p><h3 id=\"d7ee\" class=\"graf graf--h3 graf-after--p\"><strong class=\"markup--strong markup--h3-strong\">Starting point</strong></h3><p id=\"1521\" class=\"graf graf--p graf-after--h3\">We are heavy <a href=\"https://fr.slideshare.net/pygnosis/cassandra-at-teads\" data-href=\"https://fr.slideshare.net/pygnosis/cassandra-at-teads\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Cassandra users</a>, given our current infrastructure which includes 145 production nodes and performs up to one million queries per second.</p><figure id=\"9ce9\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*WaaerBKULWlA-WtQlXTGag.png\" data-width=\"750\" data-height=\"143\" data-action=\"zoom\" data-action-value=\"1*WaaerBKULWlA-WtQlXTGag.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*WaaerBKULWlA-WtQlXTGag.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><em class=\"markup--em markup--figure-em\">Writes (blue) vs Reads (green)</em></figcaption></div></figure><p id=\"9e5c\" class=\"graf graf--p graf-after--figure\">C* is a central and critical piece of this infrastructure. We use it for a lot of things from storing tracking events to targeting data used to display the right ad to the right user. We use both C* 3.0 and C* 2.1.</p><p id=\"2922\" class=\"graf graf--p graf-after--p\">One of the challenges we had operating our C* clusters was streamlining and simplifying day-to-day operations. In fact, even the most<strong class=\"markup--strong markup--p-strong\"> basic tasks required in-depth knowledge</strong> of the technology.</p><p id=\"903c\" class=\"graf graf--p graf-after--p\">In early 2016, our processes relied a lot on manual operations with little automation (<a href=\"http://capistranorb.com/\" data-href=\"http://capistranorb.com/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener nofollow\" target=\"_blank\">Capistrano</a>, pssh, sed) so we had several needs in order to:</p><ul class=\"postList\"><li id=\"8338\" class=\"graf graf--li graf-after--p\">Launch and operate a new region</li><li id=\"9ae9\" class=\"graf graf--li graf-after--li\">Reverse engineer existing regions</li><li id=\"58a3\" class=\"graf graf--li graf-after--li\">Build a staging environment</li><li id=\"cf87\" class=\"graf graf--li graf-after--li\">Improve staff turnover support</li><li id=\"0f9d\" class=\"graf graf--li graf-after--li\">Track infrastructure changes and revert them easily</li></ul><h3 id=\"df78\" class=\"graf graf--h3 graf-after--li\">Step 1: Cassandra configuration automation</h3><p id=\"f5d5\" class=\"graf graf--p graf-after--h3\">The first step we took at that time was moving from Capistrano to Chef with adapted<a href=\"https://github.com/michaelklishin/cassandra-chef-cookbook\" data-href=\"https://github.com/michaelklishin/cassandra-chef-cookbook\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener nofollow\" target=\"_blank\"> cookbooks</a> and a custom wrapper.</p><p id=\"1b50\" class=\"graf graf--p graf-after--p\">It was a big win because we gained confidence in our infrastructure state. That said, when we needed to bootstrap or replace a node we still had to manually spawn an instance and then run a Rundeck job to install Chef client, etc. There was an obvious need for more automation.</p><h3 id=\"dae3\" class=\"graf graf--h3 graf-after--p\">Step 2: AMI creation</h3><p id=\"ac3f\" class=\"graf graf--p graf-after--h3\">At this point we were still using a DataStax AMI based on Ubuntu 12.04 and wanted better control over the whole process. As we are extensive Debian users we decided to build our own AMIs with this distribution.</p><p id=\"df76\" class=\"graf graf--p graf-after--p\">To do that, we chose<a href=\"https://www.packer.io/intro/index.html\" data-href=\"https://www.packer.io/intro/index.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener nofollow\" target=\"_blank\"> Packer</a>, a great tool from Hashicorp. Packer spawns a VM and then calls Chef to: install common software, create an AMI out of the instance and finally kill the VM.</p><p id=\"52a8\" class=\"graf graf--p graf-after--p\">A Jenkins job is used to build production or staging AMIs in different AWS regions.</p><figure id=\"6360\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*vd4dxbRQnngsr6PsgpYAAA.png\" data-width=\"350\" data-height=\"245\" src=\"https://cdn-images-1.medium.com/max/1600/1*vd4dxbRQnngsr6PsgpYAAA.png\" alt=\"image\" /></div></div></figure><p id=\"e1a2\" class=\"graf graf--p graf-after--figure\">Packer works well and even let us <strong class=\"markup--strong markup--p-strong\">debug provisioning</strong> (we already had to): in this case each step has to be confirmed manually and then a SSH key is generated to log onto the spawned instance.</p><h3 id=\"96ce\" class=\"graf graf--h3 graf-after--p\">Step 3: Cluster creation automation (first attempt)</h3><p id=\"1504\" class=\"graf graf--p graf-after--h3\">As we were already using Chef, we tried<a href=\"https://docs.chef.io/provisioning.html\" data-href=\"https://docs.chef.io/provisioning.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener nofollow\" target=\"_blank\"> Chef provisioning</a> to spawn one or several VMs based on our custom AMI.</p><p id=\"dbd9\" class=\"graf graf--p graf-after--p\">At first, it was being used on a local workstation which was really handy. However, it strongly relied upon the laptop configuration launching it. Eventually, it became difficult to maintain and rather fragile. Another drawback to Chef provisioning is that it cannot spawn<a href=\"https://github.com/chef/knife-ec2/pull/284\" data-href=\"https://github.com/chef/knife-ec2/pull/284\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\"> Spot instances</a> that we wanted to use for testing purposes.</p><p id=\"de59\" class=\"graf graf--p graf-after--p\">In the end, we realised we were still relying on a lot of manual operations and the process wasn’t ideal for teamwork.</p><h3 id=\"924f\" class=\"graf graf--h3 graf-after--p\">Step 4: Cluster creation automation (rinse and repeat)</h3><p id=\"4cda\" class=\"graf graf--p graf-after--h3\">At that time we were evaluating <em class=\"markup--em markup--p-em\">Infrastructure-as-Code</em>. We had mainly identified two solutions: <a href=\"https://aws.amazon.com/fr/cloudformation/\" data-href=\"https://aws.amazon.com/fr/cloudformation/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Cloud Formation</a> (via <a href=\"https://github.com/cloudtools/troposphere\" data-href=\"https://github.com/cloudtools/troposphere\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Troposphere</a>) and <a href=\"https://www.terraform.io/\" data-href=\"https://www.terraform.io/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Terraform</a>.</p><p id=\"7822\" class=\"graf graf--p graf-after--p\">Choosing the latter wasn’t totally rational at the time but <strong class=\"markup--strong markup--p-strong\">Terraform looked sexier</strong> and seemed to have greater potential (open source and a fairly active community). Despite the fact that most of our infrastructure is on AWS, it was a plus to be able to <strong class=\"markup--strong markup--p-strong\">describe cross vendor infrastructure</strong> into one unique repository.</p><p id=\"e248\" class=\"graf graf--p graf-after--p\">Also, Terraform is about to <a href=\"https://stackshare.io/terraform\" data-href=\"https://stackshare.io/terraform\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">become the standard</a> for managing infrastructure. It enables high level of abstraction for describing resources and uses a highly declarative and easily readable language (HCL, Hashicorp Language).</p><p id=\"4c8a\" class=\"graf graf--p graf-after--p\">Moving forward, we started by using it to launch a new AWS Region and quickly Terraformed most of our clusters.</p><p id=\"cd0f\" class=\"graf graf--p graf-after--p\">Terraform lets us go much further than before and manage everything from VMs, identity and access management (IAM), network, DNS, managed RDBMS (RDS), alerting (Cloudwatch) to object storage (S3) and more. One of the great things about it is its ability to simulate the impacts of a modification (dry run using a <em class=\"markup--em markup--p-em\">plan</em> command) before executing it for real.</p><p id=\"5708\" class=\"graf graf--p graf-after--p\">Essentially, what we did with Terraform was industrializing around it. We defined pull requests processes, team reviews and <strong class=\"markup--strong markup--p-strong\">ensured that Jenkins and only Jenkins can <em class=\"markup--em markup--p-em\">apply</em>.</strong></p><figure id=\"e72f\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*vKJcvT-9Tpf4e1-X_-cZww.png\" data-width=\"680\" data-height=\"280\" src=\"https://cdn-images-1.medium.com/max/1600/1*vKJcvT-9Tpf4e1-X_-cZww.png\" alt=\"image\" /></div></div></figure><h3 id=\"5f37\" class=\"graf graf--h3 graf-after--figure\">Step 5: Cluster operations automation</h3><h4 id=\"be92\" class=\"graf graf--h4 graf-after--h3\">Scale Out</h4><p id=\"2440\" class=\"graf graf--p graf-after--h4\">To bootstrap a new C* node we use Terraform and<a href=\"https://www.terraform.io/docs/provisioners/chef.html\" data-href=\"https://www.terraform.io/docs/provisioners/chef.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener nofollow\" target=\"_blank\"> Chef provisioner</a> (not to be mistaken for Chef provisioning mentioned in <em class=\"markup--em markup--p-em\">step 3</em>).</p><figure id=\"c887\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*_nqzVxe-N9LJwciTnxJdYw.png\" data-width=\"625\" data-height=\"340\" src=\"https://cdn-images-1.medium.com/max/1600/1*_nqzVxe-N9LJwciTnxJdYw.png\" alt=\"image\" /></div></div></figure><p id=\"77ba\" class=\"graf graf--p graf-after--figure\">Here is a Terraform rack definition example:</p><figure id=\"fe35\" class=\"graf graf--figure graf--iframe graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><div class=\"iframeContainer\"><iframe width=\"700\" height=\"250\" src=\"https://medium.com/media/10613c72e4cb7cd0a935d2e4be6bb70d?postId=9443e0375aa7\" data-media-id=\"10613c72e4cb7cd0a935d2e4be6bb70d\" data-thumbnail=\"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F20056%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\">[embedded content]</iframe></div></div></div></figure><p id=\"5909\" class=\"graf graf--p graf-after--figure\">If we want to scale out, <strong class=\"markup--strong markup--p-strong\">all we have to do is a pull request</strong> that modifies the number of nodes in a rack (+1 on node count):</p><figure id=\"87c3\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*_kcXUDLQDk_23JPyR3CQTw.png\" data-width=\"800\" data-height=\"58\" data-action=\"zoom\" data-action-value=\"1*_kcXUDLQDk_23JPyR3CQTw.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*_kcXUDLQDk_23JPyR3CQTw.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\">Github pull request diff</figcaption></div></figure><p id=\"56a9\" class=\"graf graf--p graf-after--figure\">Then we run a Terraform <em class=\"markup--em markup--p-em\">plan</em> with Jenkins and paste the link of the console output in a pull request comment.</p><figure id=\"4c65\" class=\"graf graf--figure graf--iframe graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><div class=\"iframeContainer\"><iframe width=\"700\" height=\"250\" src=\"https://medium.com/media/f1496489378b0132f86596e2f4f22ae1?postId=9443e0375aa7\" data-media-id=\"f1496489378b0132f86596e2f4f22ae1\" data-thumbnail=\"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F20056%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\">[embedded content]</iframe></div></div><figcaption class=\"imageCaption\">&lt;computed&gt; values will be computed by Terraform/AWS during apply</figcaption></div></figure><p id=\"406a\" class=\"graf graf--p graf-after--figure\">From an organisational standpoint, a <strong class=\"markup--strong markup--p-strong\">mandatory review has to be done</strong> by a team member before merging. Then, we Terraform <em class=\"markup--em markup--p-em\">apply</em> via Jenkins, it will still ask us if we want to proceed:</p><figure id=\"fe4f\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*KgFg-fou1CRsbbpQlcl0yw.png\" data-width=\"400\" data-height=\"102\" src=\"https://cdn-images-1.medium.com/max/1600/1*KgFg-fou1CRsbbpQlcl0yw.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\">Jenkins confirmation</figcaption></div></figure><p id=\"fc92\" class=\"graf graf--p graf-after--figure\">Once the VM is up, Terraform calls Chef using the dedicated <a href=\"https://www.terraform.io/docs/provisioners/chef.html\" data-href=\"https://www.terraform.io/docs/provisioners/chef.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">plugin</a> to provision the VM. It will install Cassandra, Datadog’s agent, some C* tools, etc.</p><p id=\"0663\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">We do not start Cassandra automatically</strong> since we want each node to be added sequentially.</p><p id=\"cf26\" class=\"graf graf--p graf-after--p\">However, we automatically verify that everything went fine using a <em class=\"markup--em markup--p-em\">bootstrap</em> Rundeck job, checking that:</p><ul class=\"postList\"><li id=\"db7d\" class=\"graf graf--li graf-after--p\">RAID0 is correctly mounted</li><li id=\"071e\" class=\"graf graf--li graf-after--li\">the new node is located in the right datacenter</li><li id=\"16ce\" class=\"graf graf--li graf-after--li\">the new node will be able to contact seed nodes</li><li id=\"bbc1\" class=\"graf graf--li graf-after--li\">ensure there are no existing Cassandra data</li></ul><p id=\"6f76\" class=\"graf graf--p graf-after--li\">If all checks are OK, actual C* bootstrap is triggered. When bootstrapping starts we receive an alert on Slack, same goes once it’s finished so we know we can safely carry on with the next node.</p><p id=\"3316\" class=\"graf graf--p graf-after--p\">Provisioning is now possible <strong class=\"markup--strong markup--p-strong\">without mastering Cassandra</strong>. Tuning and troubleshooting still require our expertise, but in normal situations, <strong class=\"markup--strong markup--p-strong\">these operations can be done by everybody</strong> in the team.</p><h4 id=\"76e5\" class=\"graf graf--h4 graf-after--p\">Scale In</h4><p id=\"750c\" class=\"graf graf--p graf-after--h4\">In case of downscale, usually because of a bad node (noisy neighbor, hardware failure), basic alerting helps us identifying a suspicious instance. If proven faulty, we face two situations: A VM can be faulty but still accessible or otherwise totally inaccessible, usually when it’s due for <a href=\"http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-retirement.html\" data-href=\"http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-retirement.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">retirement</a>.</p><p id=\"19aa\" class=\"graf graf--p graf-after--p\">In the first scenario we use a <em class=\"markup--em markup--p-em\">decommissioning</em> Rundeck job that will:</p><ul class=\"postList\"><li id=\"d9ac\" class=\"graf graf--li graf-after--p\">make sure the node is located in the right datacenter</li><li id=\"ebfd\" class=\"graf graf--li graf-after--li\">pause after displaying its information so that we can kill it if we spot a mistake</li></ul><p id=\"9a71\" class=\"graf graf--p graf-after--li\">The job also triggers an alert on Slack when starting and then when it’s done (whether it’s a success or not).</p><p id=\"30e7\" class=\"graf graf--p graf-after--p\">Once decommissioned, we manually kill the VM on AWS and then launch a Terraform <em class=\"markup--em markup--p-em\">apply</em>. Terraform will identify the missing node and respawn a new one.</p><h4 id=\"4915\" class=\"graf graf--h4 graf-after--p\">Replace</h4><p id=\"3c00\" class=\"graf graf--p graf-after--h4\">When totally inaccessible, we manually terminate the AWS instance from the console. <strong class=\"markup--strong markup--p-strong\">We do not want to automate termination</strong>, the main reason being we have seen VMs pop back on after a few minutes.</p><p id=\"0d50\" class=\"graf graf--p graf-after--p\">Once terminated:</p><ul class=\"postList\"><li id=\"8185\" class=\"graf graf--li graf-after--p\">we launch a Terraform <em class=\"markup--em markup--li-em\">apply</em> that will provision a new node</li><li id=\"290f\" class=\"graf graf--li graf-after--li\">then we use a <em class=\"markup--em markup--li-em\">replace</em> Rundeck job that will modify the <em class=\"markup--em markup--li-em\">cassandra-env.sh </em>file accordingly and bootstrap the new node</li></ul><h4 id=\"0022\" class=\"graf graf--h4 graf-after--li\">Other Ops</h4><p id=\"e044\" class=\"graf graf--p graf-after--h4\">We also leverage Rundeck features for other day-to-day operations like:</p><ul class=\"postList\"><li id=\"fb22\" class=\"graf graf--li graf-after--p\">Monitoring rolling restarts,</li><li id=\"3f5d\" class=\"graf graf--li graf-after--li\">Running nodetool commands,</li><li id=\"a26d\" class=\"graf graf--li graf-after--li\">Applying schema migrations,</li><li id=\"e5a8\" class=\"graf graf--li graf-after--li\">Scheduling jobs for backups, major compactions on some specific tables, etc.</li></ul><h3 id=\"3f62\" class=\"graf graf--h3 graf-after--li\"><strong class=\"markup--strong markup--h3-strong\">Main progress and feedback</strong></h3><p id=\"ac38\" class=\"graf graf--p graf-after--h3\">Using Terraform, Chef provisioner, Packer and Rundeck we have greatly simplified and automated scaling operations. It also <strong class=\"markup--strong markup--p-strong\">helped us build a complex yet trusted infrastructure</strong>. Having to precisely know which files to change before starting a new node is now a thing of the past and potential mistakes are checked.</p><p id=\"cfd8\" class=\"graf graf--p graf-after--p\">But using Terraform to handle Kafka and Cassandra datastores in production wasn’t an easy decision. <strong class=\"markup--strong markup--p-strong\">We carefully tested it in staging environment</strong> to make sure that it respawned the right instances (i.e. hostname and Terraform index) and <strong class=\"markup--strong markup--p-strong\">we encourage everybody to do so</strong>.</p><p id=\"0ebd\" class=\"graf graf--p graf-after--p graf--trailing\">We have always used the most recent Terraform version and<strong class=\"markup--strong markup--p-strong\"> have had surprises</strong> after some updates. For example, due to a regression of Chef provisioner we once couldn’t spawn Cassandra nodes anymore. To prevent this from happening again, we implemented a testing process for several use cases on staging environment with a Jenkins slave.</p></div></div></section><section class=\"section section--body\"><div class=\"section-divider\"><hr class=\"section-divider\" /></div><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"1f6c\" class=\"graf graf--p graf--leading\">To wrap up, here are the <strong class=\"markup--strong markup--p-strong\">key improvements</strong> we wouldn’t give up now:</p><ul class=\"postList\"><li id=\"c9a9\" class=\"graf graf--li graf-after--p\">A <strong class=\"markup--strong markup--li-strong\">unified Infrastructure as code</strong> in one repository, describing both our AWS and GCP resources.</li><li id=\"e27d\" class=\"graf graf--li graf-after--li\"><strong class=\"markup--strong markup--li-strong\">Explicit documentation and history</strong> that we couldn’t have using cloud consoles. Each commit contains actionable information, newcomers aren’t lost anymore when discovering our infrastructure (e.g. until<a href=\"https://aws.amazon.com/fr/blogs/aws/new-descriptions-for-security-group-rules/\" data-href=\"https://aws.amazon.com/fr/blogs/aws/new-descriptions-for-security-group-rules/\" class=\"markup--anchor markup--li-anchor\" rel=\"nofollow noopener\" target=\"_blank\"> recently</a> security group IPs couldn’t be annotated).</li><li id=\"e30e\" class=\"graf graf--li graf-after--li\">The ability to <strong class=\"markup--strong markup--li-strong\">spawn entire clusters promptly and safely.</strong></li><li id=\"9dec\" class=\"graf graf--li graf-after--li graf--trailing\">An history of previous actions’ execution time, really useful to give ETAs.</li></ul></div></div></section><section class=\"section section--body section--last\"><div class=\"section-divider\"><hr class=\"section-divider\" /></div><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"2308\" class=\"graf graf--p graf--leading\">Of course, we are continually looking for new enhancements to <strong class=\"markup--strong markup--p-strong\">automate our way out of repeatable tasks</strong>.</p><p id=\"178d\" class=\"graf graf--p graf-after--p\">As <strong class=\"markup--strong markup--p-strong\">Cloud instances are </strong><a href=\"https://www.datadoghq.com/pdf/top_5_aws_ec2_performance_problems_ebook.pdf\" data-href=\"https://www.datadoghq.com/pdf/top_5_aws_ec2_performance_problems_ebook.pdf\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener nofollow\" target=\"_blank\"><strong class=\"markup--strong markup--p-strong\">not all equal</strong></a><strong class=\"markup--strong markup--p-strong\">, </strong>we would like to automatically select a good one when we spawn a new node. By benchmarking several of them we should be able to identify instances that are soon to be retired or prone to performance issues and only pick the best one.</p><p id=\"6a9b\" class=\"graf graf--p graf-after--p\">If you find these kind of challenges exiting, please reach out, we would be happy to talk about it.</p><p id=\"9656\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">In the meantime </strong>here is a couple of other articles from Teads’ Infrastructure team:</p><div id=\"e7f3\" class=\"graf graf--mixtapeEmbed graf-after--p\"><a href=\"https://medium.com/teads-engineering/real-life-aws-cost-optimization-strategy-at-teads-135268b0860f\" data-href=\"https://medium.com/teads-engineering/real-life-aws-cost-optimization-strategy-at-teads-135268b0860f\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"https://medium.com/teads-engineering/real-life-aws-cost-optimization-strategy-at-teads-135268b0860f\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">Real-life AWS cost optimization strategy</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">How we keep a good Amazon Web Services billing hygiene at Teads</em>medium.com</a></div><div id=\"a20b\" class=\"graf graf--mixtapeEmbed graf-after--mixtapeEmbed graf--trailing\"><a href=\"https://medium.com/teads-engineering/scaling-your-on-duty-team-bc467c480747\" data-href=\"https://medium.com/teads-engineering/scaling-your-on-duty-team-bc467c480747\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"https://medium.com/teads-engineering/scaling-your-on-duty-team-bc467c480747\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">Scaling your on-duty team</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">The internet never sleeps, and even with the best design for resilience, one day, your system will go down.</em>medium.com</a></div></div></div></section>",
        "created_at": "2018-10-03T11:45:53+0000",
        "updated_at": "2018-10-03T11:45:59+0000",
        "published_at": "2018-02-13T07:01:01+0000",
        "published_by": [
          "Romain Hardouin"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 8,
        "domain_name": "medium.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*1gsGOvBiRupn3TwGVFDZOw.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12282"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 901,
            "label": "security",
            "slug": "security"
          }
        ],
        "is_public": false,
        "id": 12281,
        "uid": null,
        "title": "Cassandra 4.0 Data Center Security Enhancements",
        "url": "http://thelastpickle.com/blog/2018/05/08/cassandra-4.0-datacentre-security-improvements.html",
        "content": "<p>Apache Cassandra versions 3.x and below have an all or nothing approach when it comes the datacenter user authorization security model. That is, a user has access to all datacenters in the cluster or no datacenters in the cluster. This has changed to something a little more fine grained for versions 4.0 and above, all thanks to  <a href=\"https://github.com/bdeggleston\">Blake Eggleston</a> and the work he has done on <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-13985\">CASSANDRA-13985</a>.</p><p>The Cassandra 4.0 feature added via <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-13985\">CASSANDRA-13985</a> allows an operator to restrict the access of a Cassandra role to specific datacenters. This new shiny feature is effectively datacenter authorization for roles and will help provide better security and protection for multi-datacenter clusters.</p>\n<p>Consider the example scenario where a cluster has two datacenters; <code class=\"highlighter-rouge\">dc1</code> and <code class=\"highlighter-rouge\">dc2</code>.  In this scenario datacenter <code class=\"highlighter-rouge\">dc1</code> backs a web server application that performs Online Transaction Processing, and datacenter <code class=\"highlighter-rouge\">dc2</code> backs an analytics application. The web server application could be restricted via a role to access <code class=\"highlighter-rouge\">dc1</code> only and similarly, the analytics application could be restricted via a role to access <code class=\"highlighter-rouge\">dc2</code> only. The advantage here is that it minimises the reach that each application has to the cluster. If the analytics application was configured incorrectly to connect to <code class=\"highlighter-rouge\">dc1</code> it would fail, rather than quietly running and increasing the load on the <code class=\"highlighter-rouge\">dc1</code> nodes.</p>\n<p>The behaviour of the new datacenter authorization feature can be controlled via the <em>cassandra.yaml</em> file using the new setting named <code class=\"highlighter-rouge\">network_authorizer</code>. Out of the box it can be set to one of two values:</p>\n<ul><li><strong>AllowAllNetworkAuthorizer</strong> - allows any role to access any datacenter effectively disabling datacenter authorization; which is the current behaviour.</li>\n  <li><strong>CassandraNetworkAuthorizer</strong> - allows the ability to store permissions which restrict role access to specific datacenters.</li>\n</ul>\n<ul><li>For the <code class=\"highlighter-rouge\">network_authorizer</code> setting work when set to <strong>CassandraNetworkAuthorizer</strong>, the <code class=\"highlighter-rouge\">authenticator</code> setting must be set to <strong>PasswordAuthenticator</strong>. Otherwise, the node will fail to start.</li>\n  <li>When enabling any authorization feature in Cassandra <strong>including this one</strong>, always increase the <code class=\"highlighter-rouge\">system_auth</code> keyspace replication factor. Failure to do this may result in being locked out of the cluster!</li>\n  <li>Further values can be added for custom behaviour by implementing the <code class=\"highlighter-rouge\">INetworkAuthorizer</code> interface.</li>\n  <li>Apache Cassandra 4.0 will ship with <code class=\"highlighter-rouge\">network_authorizer</code> set to a value of <strong>AllowAllNetworkAuthorizer</strong> in the <em>cassandra.yaml</em> file. This is similar to the existing <code class=\"highlighter-rouge\">authorizer</code> setting in Cassandra where no authorization restrictions are applied by default.</li>\n</ul>\n<p>When <code class=\"highlighter-rouge\">network_authorizer</code> is set to <strong>CassandraNetworkAuthorizer</strong>, the CQL syntax can be used to set the datacenter access for a role in a cluster. To help with the setting of permissions in CQL, its keyword vocabulary has been extended to include the clauses <code class=\"highlighter-rouge\">ACCESS TO ALL DATACENTERS</code> and <code class=\"highlighter-rouge\">ACCESS TO DATACENTERS</code>. Both clause can be added to CQL <code class=\"highlighter-rouge\">ROLE</code> statements when either creating or altering a role.</p>\n<p>To create a role that has access to all datacenters in a cluster use the <code class=\"highlighter-rouge\">ACCESS TO ALL DATACENTERS</code> clause. For example:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>CREATE ROLE foo WITH PASSWORD = '...' AND LOGIN = true AND ACCESS TO ALL DATACENTERS;\n</pre></div></div>\n<p>Similarly a role can be altered to have access to all datacenters. For example:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>ALTER ROLE foo WITH ACCESS TO ALL DATACENTERS;\n</pre></div></div>\n<p>To create a role that is restricted to specific datacenters use the clause <code class=\"highlighter-rouge\">ACCESS TO DATACENTERS</code> followed by a set containing the datacenters the role is authorized to access. The datacenter names are literal values i.e. quoted and comma separated. For example, use the following CQL to restrict the access of a role to datacenters <code class=\"highlighter-rouge\">dc1</code> and <code class=\"highlighter-rouge\">dc3</code> only:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>CREATE ROLE foo WITH PASSWORD = '...' AND LOGIN = true\n    AND ACCESS TO DATACENTERS {'dc1', 'dc3'};\n</pre></div></div>\n<p>Similarly a role can be altered to have restricted access. For example:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>ALTER ROLE foo WITH ACCESS TO DATACENTERS {'dc1', 'dc3'};\n</pre></div></div>\n<p>If the <code class=\"highlighter-rouge\">ACCESS TO DATACENTERS {...}</code> clause is omitted from a <code class=\"highlighter-rouge\">CREATE ROLE</code> command, then the <em>new</em> role will have access to all data centers in the cluster. In this specific case, it is equivalent to adding the <code class=\"highlighter-rouge\">ACCESS TO ALL DATACENTERS</code> clause on the <code class=\"highlighter-rouge\">CREATE ROLE</code> command.</p>\n<p>Here is a quick demo of the feature in action. The following demo uses <code class=\"highlighter-rouge\">ccm</code> to launch a cluster running the trunk version of Apache Cassandra commit Id <a href=\"https://github.com/apache/cassandra/commit/2fe4b9dc69a919cadd6f78e9a4e259e6740b127f\"><code class=\"highlighter-rouge\">2fe4b9d</code></a>. The cluster will have two datacenters with a single node in each, and the <code class=\"highlighter-rouge\">network_authorizer</code> feature will be enabled on each node. The scripts to set up <code class=\"highlighter-rouge\">ccm</code> and cluster are included inline as well.</p>\n<p>Set up <code class=\"highlighter-rouge\">ccm</code> to use the local build of commit <a href=\"https://github.com/apache/cassandra/commit/2fe4b9dc69a919cadd6f78e9a4e259e6740b127f\"><code class=\"highlighter-rouge\">2fe4b9d</code></a> for the Cassandra libraries, by running the following script.</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>#!/bin/bash\nset -e\nif [ -z \"${1}\" ]\nthen\n  echo \"Apache Cassandra repository path required.\"\n  exit 1\nfi\nCCM_CASSANDRA_VERSION=\"4.0.0\"\nCCM_CASSANDRA_REPOSITORY_PATH=\".ccm/repository/${CCM_CASSANDRA_VERSION}\"\nCASSANDRA_DIR_PATH=${1}\nCASSANDRA_SUB_DIR_LIST=\"bin build conf lib pylib tools\"\necho \"Building CCM ${CCM_CASSANDRA_VERSION} repository\"\nmkdir -p ~/${CCM_CASSANDRA_REPOSITORY_PATH}\necho ${CCM_CASSANDRA_VERSION} &gt; ~/${CCM_CASSANDRA_REPOSITORY_PATH}/0.version.txt\nfor dir_name in ${CASSANDRA_SUB_DIR_LIST}\ndo\n  echo \"Copying directory ${CASSANDRA_DIR_PATH}/${dir_name} to CCM ${CCM_CASSANDRA_VERSION} repository\"\n  mkdir -p ~/${CCM_CASSANDRA_REPOSITORY_PATH}/${dir_name}\n  cp -r ${CASSANDRA_DIR_PATH}/${dir_name}/* ~/${CCM_CASSANDRA_REPOSITORY_PATH}/${dir_name}\ndone\n</pre></div></div>\n<p>Create the <code class=\"highlighter-rouge\">ccm</code> cluster which uses the libraries from commit <a href=\"https://github.com/apache/cassandra/commit/2fe4b9dc69a919cadd6f78e9a4e259e6740b127f\"><code class=\"highlighter-rouge\">2fe4b9d</code></a> by running the following script.</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>#!/bin/bash\nset -e\nCLUSTER_NAME=\"${1:-dc-security-demo}\"\nccm remove ${CLUSTER_NAME}\necho \"Creating cluster '${CLUSTER_NAME}'\"\nccm create ${CLUSTER_NAME} -v 4.0.0\n# Modifies the configuration of a node in the CCM cluster.\nfunction update_node_config {\n  CASSANDRA_YAML_SETTINGS=\"authenticator:PasswordAuthenticator \\\n                          endpoint_snitch:GossipingPropertyFileSnitch \\\n                          network_authorizer:CassandraNetworkAuthorizer \\\n                          num_tokens:32 \\\n                          seeds:127.0.0.1,127.0.0.2\"\n  for key_value_setting in ${CASSANDRA_YAML_SETTINGS}\n  do\n    setting_key=$(echo ${key_value_setting} | cut -d':' -f1)\n    setting_val=$(echo ${key_value_setting} | cut -d':' -f2)\n    sed -ie \"s/${setting_key}\\:\\ .*/${setting_key}:\\ ${setting_val}/g\" \\\n      ~/.ccm/${CLUSTER_NAME}/node${1}/conf/cassandra.yaml\n  done\n  sed -ie \"s/dc=.*/dc=dc${1}/g\" \\\n    ~/.ccm/${CLUSTER_NAME}/node${1}/conf/cassandra-rackdc.properties\n  sed -ie 's/\\#MAX_HEAP_SIZE=\\\"4G\\\"/MAX_HEAP_SIZE=\\\"1G\\\"/g' \\\n    ~/.ccm/${CLUSTER_NAME}/node${1}/conf/cassandra-env.sh\n  sed -ie 's/\\#HEAP_NEWSIZE=\\\"800M\\\"/HEAP_NEWSIZE=\\\"250M\\\"/g' \\\n    ~/.ccm/${CLUSTER_NAME}/node${1}/conf/cassandra-env.sh\n}\nNUMBER_NODES=2\nfor node_num in $(seq ${NUMBER_NODES})\ndo\n  echo \"Adding 'node${node_num}'\"\n  ccm add node${node_num} \\\n    -i 127.0.0.${node_num} \\\n    -j 7${node_num}00 \\\n    -r 0 \\\n    -b \\\n    -s\n  update_node_config ${node_num}\n  # Localhost aliases\n  echo \"ifconfig lo0 alias 127.0.0.${node_num} up\"\n  sudo ifconfig lo0 alias 127.0.0.${node_num} up\ndone\nsed -ie 's/use_vnodes\\:\\ false/use_vnodes:\\ true/g' \\\n  ~/.ccm/${CLUSTER_NAME}/cluster.conf\n</pre></div></div>\n<p>Check the cluster nodes were created.</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>anthony@Anthonys-MacBook-Pro ~/ &gt; ccm status\nCluster: 'dc-security-demo'\n---------------------------\nnode1: DOWN (Not initialized)\nnode2: DOWN (Not initialized)\n</pre></div></div>\n<p>Start the nodes in the cluster.</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>anthony@Anthonys-MacBook-Pro ~/ &gt; ccm node1 start\nanthony@Anthonys-MacBook-Pro ~/ &gt; ccm node2 start\n</pre></div></div>\n<p>Check that the cluster is up and running as expected.</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>anthony@Anthonys-MacBook-Pro ~/ &gt; ccm node1 nodetool status\nDatacenter: dc1\n===============\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address    Load        Tokens  Owns (effective)  Host ID                              Rack\nUN  127.0.0.1  115.46 KiB  32      100.0%            7dafff97-e2c5-4e70-a6a9-523f5594671b rack1\nDatacenter: dc2\n===============\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address    Load       Tokens  Owns (effective)  Host ID                               Rack\nUN  127.0.0.2  67.05 KiB  32      100.0%            437e3bca-d0b7-4102-bc56-201b96856f01  rack1\n</pre></div></div>\n<p>Start a CQL session with the cluster and increase the <code class=\"highlighter-rouge\">system_auth</code> replication.</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>anthony@Anthonys-MacBook-Pro ~/ &gt; ccm node1 cqlsh -u cassandra -p cassandra\nConnected to dc-security-demo at 127.0.0.1:9042.\n[cqlsh 5.0.1 | Cassandra 4.0-SNAPSHOT | CQL spec 3.4.5 | Native protocol v4]\nUse HELP for help.\ncassandra@cqlsh&gt;\ncassandra@cqlsh&gt; ALTER KEYSPACE system_auth\n    WITH REPLICATION = {'class' : 'NetworkTopologyStrategy', 'dc1' : 1, 'dc2' : 1};\nWarnings :\nWhen increasing replication factor you need to run a full (-full) repair to distribute the data.\ncassandra@cqlsh&gt; exit;\n</pre></div></div>\n<p>Repair the <code class=\"highlighter-rouge\">system_auth</code> keyspace on both nodes.</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>anthony@Anthonys-MacBook-Pro ~/ &gt; ccm node1 nodetool repair system_auth\n...\nanthony@Anthonys-MacBook-Pro ~/ &gt; ccm node2 nodetool repair system_auth\n...\n</pre></div></div>\n<p>Start another CQL session and create a few roles with different datacenter restrictions.</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>anthony@Anthonys-MacBook-Pro ~/ &gt; ccm node1 cqlsh -u cassandra -p cassandra\nConnected to dc-security-demo at 127.0.0.1:9042.\n[cqlsh 5.0.1 | Cassandra 4.0-SNAPSHOT | CQL spec 3.4.5 | Native protocol v4]\nUse HELP for help.\ncassandra@cqlsh&gt;\ncassandra@cqlsh&gt; CREATE ROLE foo WITH PASSWORD = 'foo' AND LOGIN = true\n    AND ACCESS TO DATACENTERS {'dc1'};\ncassandra@cqlsh&gt; CREATE ROLE bar WITH PASSWORD = 'bar' AND LOGIN = true\n    AND ACCESS TO DATACENTERS {'dc2'};\ncassandra@cqlsh&gt; SELECT * FROM system_auth.network_permissions;\n role      | dcs\n-----------+---------\n roles/foo | {'dc1'}\n roles/bar | {'dc2'}\n(2 rows)\ncassandra@cqlsh&gt; exit;\n</pre></div></div>\n<p>Test the datacenter access for the newly created roles.</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>anthony@Anthonys-MacBook-Pro ~/ &gt; ccm node1 cqlsh -u foo -p foo\nConnected to dc-security-demo at 127.0.0.1:9042.\n[cqlsh 5.0.1 | Cassandra 4.0-SNAPSHOT | CQL spec 3.4.5 | Native protocol v4]\nUse HELP for help.\nfoo@cqlsh&gt; exit;\nanthony@Anthonys-MacBook-Pro ~/ &gt; ccm node2 cqlsh -u foo -p foo\nConnection error: ('Unable to connect to any servers', {'127.0.0.2': Unauthorized('Error from server: code=2100 [Unauthorized] message=\"You do not have access to this datacenter\"',)})\nanthony@Anthonys-MacBook-Pro ~/ &gt; ccm node1 cqlsh -u bar -p bar\nConnection error: ('Unable to connect to any servers', {'127.0.0.1': Unauthorized('Error from server: code=2100 [Unauthorized] message=\"You do not have access to this datacenter\"',)})\nanthony@Anthonys-MacBook-Pro ~/ &gt; ccm node2 cqlsh -u bar -p bar\nConnected to dc-security-demo at 127.0.0.2:9042.\n[cqlsh 5.0.1 | Cassandra 4.0-SNAPSHOT | CQL spec 3.4.5 | Native protocol v4]\nUse HELP for help.\nbar@cqlsh&gt;\n</pre></div></div>\n<p>As can be seen from the output above, a role is unable to establish a CQL session on a node in a particular datacenter unless it has been granted permissions to do so.</p>\n<p>Apache Cassandra 4.0 is definitely shaping up to be an exciting new release of the database! The datacenter authorization feature is a useful for hardening the security of a cluster by limiting the reach of roles and applications talking to the cluster. It is designed to be used in conjunction with other authorization features to create roles that have specific purposes in a cluster. Stay tuned as we post more new features and updates that will be part of Cassandra 4.0.</p>",
        "created_at": "2018-10-03T11:33:39+0000",
        "updated_at": "2018-10-03T11:33:44+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 9,
        "domain_name": "thelastpickle.com",
        "preview_picture": "http://thelastpickle.com/android-chrome-192x192.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12281"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 985,
            "label": "cluster",
            "slug": "cluster"
          }
        ],
        "is_public": false,
        "id": 12280,
        "uid": null,
        "title": "Multi-Region Cassandra Clusters",
        "url": "https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014",
        "content": "Multi-Region Cassandra Clusters\n      \n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            </a><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div>\n    <div class=\"wrapper\"><p>Successfully reported this slideshow.</p><div id=\"slideview-container\" class=\"\"><div class=\"row\"><div id=\"main-panel\" class=\"small-12 large-8 columns\"><div class=\"sectionElements\"><div class=\"playerWrapper\"><div><div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\">Multi-Region Cassandra Clusters<div class=\"stage valign-first-slide\"><div class=\"slide_container\"><section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-1-638.jpg?cb=1444714425\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-1-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-1-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-1-1024.jpg?cb=1444714425\" alt=\"Novel Multi-region Clusters&#10;Cassandra Deployments Split Between Heterogeneous Data Centres&#10;with NAT &amp; DNS-SD&#10;#CassandraSum...\" /></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-2-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-2-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-2-1024.jpg?cb=1444714425\" alt=\"Adam Zegelin&#10;Co-founder &amp; VP of Engineering&#10;www.instaclustr.com&#10;adam@instaclustr.com &#10;@adamzegelin&#10;\" /></i></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-3-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-3-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-3-1024.jpg?cb=1444714425\" alt=\"Instaclustr&#10;• Instaclustr provides Cassandra-as-a-service in the cloud &#10;(Currently only on AWS — Google Cloud in private b...\" /></i></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-4-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-4-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-4-1024.jpg?cb=1444714425\" alt=\"Multi-DC @ Instaclustr&#10;• Cloud ⇄ cloud, “classic” internet-facing data centre ⇄ cloud&#10;• Works out-of-the-box today.&#10;• Requ...\" /></i></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-5-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-5-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-5-1024.jpg?cb=1444714425\" alt=\"• Overview of multi- region/data centre clusters&#10;• What is supported out-of-the-box&#10;• Alternative solutions&#10;• Supporting t...\" /></i></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-6-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-6-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-6-1024.jpg?cb=1444714425\" alt=\"Single Node&#10;• What you get from running&#10;apt-get install&#10;cassandra and /usr/bin/&#10;cassandra&#10;• Fragile (no redundancy)&#10;• Dev/...\" /></i></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-7-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-7-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-7-1024.jpg?cb=1444714425\" alt=\"Multi-node, Single Data Centre&#10;• Two or more servers running&#10;Cassandra within one DC&#10;• Replication of data&#10;(redundancy)&#10;• ...\" /></i></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-8-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-8-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-8-1024.jpg?cb=1444714425\" alt=\"Multi-node, Multi-DC&#10;• Cassandra running in two or&#10;more data centres&#10;• Global deployments&#10;• Data near your customers&#10;(redu...\" /></i></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-9-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-9-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-9-1024.jpg?cb=1444714425\" alt=\"Snitches&#10;• Understands data centres and racks&#10;• Implementation may automatically determine node DC and rack &#10;(EC2MultiRegi...\" /></i></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-10-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-10-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-10-1024.jpg?cb=1444714425\" alt=\"Data Centres&#10;• Collection of Racks&#10;• Complete replications&#10;• Geographically separate&#10;• Possibly high-latency interconnects...\" /></i></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-11-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-11-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-11-1024.jpg?cb=1444714425\" alt=\"Racks&#10;• Collection of nodes&#10;• May fail as a single unit&#10;• Modelled on the traditional DC rack/cage &#10;(n-servers running of ...\" /></i></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-12-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-12-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-12-1024.jpg?cb=1444714425\" alt=\"☁&#10;• Amazon Web Services &#10;(use EC2MultiRegionSnitch)&#10;• Data Centre ≡ AWS Region &#10;(e.g. US_East_1, AP_SOUTHEAST_2)&#10;• Rack ≡ ...\" /></i></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-13-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-13-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-13-1024.jpg?cb=1444714425\" alt=\"Data Centre Aware&#10;• Cassandra is data centre aware&#10;• Only fetch data from a remote DC if absolutely required &#10;(remote data...\" /></i></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-14-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-14-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-14-1024.jpg?cb=1444714425\" alt=\"Cluster cluster = Cluster.builder()&#10;.addContactPoint(…)&#10;.withLoadBalancingPolicy(new DCAwareRoundRobinPolicy(“US_EAST_1&quot;))...\" /></i></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-15-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-15-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-15-1024.jpg?cb=1444714425\" alt=\"Multi DC Support&#10;• Per-node public (internet-facing) IP address&#10;• Optionally, per-node private IP address&#10;• Per-node publi...\" /></i></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-16-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-16-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-16-1024.jpg?cb=1444714425\" alt=\"Multi DC Support&#10;• Cloud ⇄ cloud, traditional ⇄ cloud, traditional ⇄ traditional&#10;• Easy to setup per-node public and priva...\" /></i></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-17-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-17-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-17-1024.jpg?cb=1444714425\" alt=\"IPv4 Address Space Exhaustion&#10;Source: http://www.potaroo.net/tools/ipv4/&#10;\" /></i></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-18-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-18-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-18-1024.jpg?cb=1444714425\" alt=\"Multi-DC Support&#10;• IPv4&#10;• Address exhaustion&#10;• Over time, will become more expensive to purchase addresses&#10;• Wasteful &#10;(be...\" /></i></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-19-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-19-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-19-1024.jpg?cb=1444714425\" alt=\"Alternatives&#10;• IPv6&#10;• Java supports it ∴ Cassandra probably supports it &#10;(untested by us)&#10;• Global IPv6 adoption is ~4% &#10;(...\" /></i></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-20-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-20-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-20-1024.jpg?cb=1444714425\" alt=\"Alternatives&#10;• VPNs&#10;• tinc, OpenVPN, etc.&#10;• All private address space — no dual addressing&#10;• Requires multiple links — bet...\" /></i></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-21-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-21-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-21-1024.jpg?cb=1444714425\" alt=\"Data Centres Links&#10;3 3&#10;5 10&#10;7 21&#10;\" /></i></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-22-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-22-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-22-1024.jpg?cb=1444714425\" alt=\"Alternatives&#10;• Network Address Translation (NAT) &#10;(aka IP Masquerading or Port Address Translation (PAT))&#10;• Deployed on mo...\" /></i></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-23-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-23-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-23-1024.jpg?cb=1444714425\" alt=\"NAT Basics&#10;• Re-maps IP address spaces &#10;(e.g. Public 96.31.81.80 ↔ Private 192.168.*.*)&#10;• 𝑛 public addresses, shared by 𝑥 ...\" /></i></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-24-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-24-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-24-1024.jpg?cb=1444714425\" alt=\"NAT with Inbound Connections&#10;• Static port forwarding &#10;(conﬁgured on the gateway)&#10;• Automatic port forwarding — UPnP, NAT-...\" /></i></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-25-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-25-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-25-1024.jpg?cb=1444714425\" alt=\"NAT + C∗&#10;Situation: 𝑛 Cassandra nodes, 1 public address per data centre&#10;• Port forward different public ports for each nod...\" /></i></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-26-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-26-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-26-1024.jpg?cb=1444714425\" alt=\"Advertising Port Mappings&#10;• Extend Cassandra Gossip&#10;• Include port numbers in node address announcements&#10;• Allow seed node...\" /></i></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-27-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-27-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-27-1024.jpg?cb=1444714425\" alt=\"Advertising Port Mappings&#10;• DNS-SD — dns-sd.org &#10;(aka Bonjour/Zeroconf)&#10;• Reads — works with existing DNS implementations ...\" /></i></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-28-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-28-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-28-1024.jpg?cb=1444714425\" alt=\"Advertising Port Mappings&#10;• DNS-SD cont’d.&#10;• SRV records contain hostname and port &#10;(i.e., hostname of the NAT gateway and...\" /></i></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-29-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-29-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-29-1024.jpg?cb=1444714425\" alt=\"Advertised Details&#10;• Each cluster is it’s own browse domain&#10;• Each NAT gateway device has an A record in the browse domain...\" /></i></section><section data-index=\"30\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-30-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-30-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-30-1024.jpg?cb=1444714425\" alt=\"Conﬁguration&#10;• Cassandra is conﬁgured to only use private addresses&#10;• On cluster creation&#10;• Establish a new DNS-SD browse ...\" /></i></section><section data-index=\"31\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-31-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-31-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-31-1024.jpg?cb=1444714425\" alt=\"$ dns-sd -B _cassandra._tcp 1da53f83-e635-11e3-96eb-2ec9d09504f5.clusters.instaclustr.com.au.&#10;Browsing for _cassandra._tcp...\" /></i></section><section data-index=\"32\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-32-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-32-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-32-1024.jpg?cb=1444714425\" alt=\"Java Driver Modiﬁcations&#10;• This is usually a no-op &#10;(the default is IdentityTranslater)&#10;• Modify translate() to perform a ...\" /></i></section><section data-index=\"33\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-33-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-33-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-33-1024.jpg?cb=1444714425\" alt=\"Modifying Cassandra&#10;• Responsible for managing Socket connections.&#10;• Modify newSocket() to perform a DNS-SD lookup.&#10;• The ...\" /></i></section><section data-index=\"34\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-34-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-34-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-34-1024.jpg?cb=1444714425\" alt=\"C* C*&#10;C*&#10;C* C*&#10;C*&#10;NAT Gateway NAT Gateway&#10;DNS (+ DNS-SD) Server &#10;(Route 53, Self-hosted, etc)Client&#10;Application&#10;\" /></i></section><section data-index=\"35\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\" data-small=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/85/multiregion-cassandra-clusters-35-320.jpg?cb=1444714425\" data-normal=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-35-638.jpg?cb=1444714425\" data-full=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-35-1024.jpg?cb=1444714425\" alt=\"Thanks!&#10;Questions?&#10;adam@instaclustr.com&#10;\" /></i></section><div class=\"j-next-container next-container\"><div class=\"content-container\"><div class=\"next-slideshow-wrapper\"><div class=\"j-next-slideshow next-slideshow\"><p>Upcoming SlideShare</p></div><p>Loading in …5</p><p>×</p></div></div></div></div></div></div></div></div></div><div class=\"slideshow-info-container\" itemscope=\"itemscope\" itemtype=\"https://schema.org/MediaObject\"><div class=\"slideshow-tabs-container show-for-medium-up\"><ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    0 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\">\n                \n                  0 Likes\n                \n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\">\n                Statistics\n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\">\n                Notes\n              </i></a>\n            </li>\n          </ul><div class=\"tabs-content\"><div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\"><ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"itemscope\"><li>\n                    <p class=\"empty-stat-box text-center\">\n                      <em>Be the first to like this</em>\n                    </p>\n                  </li>\n              </ul></div><div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No Downloads</p></div><div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No notes for slide</p></div></div></div><div class=\"notranslate transcript add-padding-right j-transcript\"><ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    Novel Multi-region Clusters\nCassandra Deployments Split Between Heterogeneous Data Centres\nwith NAT &amp; DNS-SD\n#CassandraSummit\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-2-638.jpg?cb=1444714425\" title=\"Adam Zegelin&#10;Co-founder &amp; VP of Engineering&#10;www.instaclustr...\" target=\"_blank\">\n        2.\n      </a>\n    Adam Zegelin\nCo-founder &amp; VP of Engineering\nwww.instaclustr.com\nadam@instaclustr.com \n@adamzegelin\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-3-638.jpg?cb=1444714425\" title=\"Instaclustr&#10;• Instaclustr provides Cassandra-as-a-service i...\" target=\"_blank\">\n        3.\n      </a>\n    Instaclustr\n• Instaclustr provides Cassandra-as-a-service in the cloud \n(Currently only on AWS — Google Cloud in private beta)\n• We currently manage 50+ Cassandra nodes for various customers\n• We often get requests to do cool things — and try and make it\nhappen!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-4-638.jpg?cb=1444714425\" title=\"Multi-DC @ Instaclustr&#10;• Cloud ⇄ cloud, “classic” internet-...\" target=\"_blank\">\n        4.\n      </a>\n    Multi-DC @ Instaclustr\n• Cloud ⇄ cloud, “classic” internet-facing data centre ⇄ cloud\n• Works out-of-the-box today.\n• Requires per-node public IP\n• Private network clusters ⇄ Cloud clusters\n• Easy if your private network allocates per-node public IP addresses\n• VPNs\n• Something else?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-5-638.jpg?cb=1444714425\" title=\"• Overview of multi- region/data centre clusters&#10;• What is ...\" target=\"_blank\">\n        5.\n      </a>\n    • Overview of multi- region/data centre clusters\n• What is supported out-of-the-box\n• Alternative solutions\n• Supporting technology overview (NAT/PAT and DNS-SD)\n• Implementation\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-6-638.jpg?cb=1444714425\" title=\"Single Node&#10;• What you get from running&#10;apt-get install&#10;cas...\" target=\"_blank\">\n        6.\n      </a>\n    Single Node\n• What you get from running\napt-get install\ncassandra and /usr/bin/\ncassandra\n• Fragile (no redundancy)\n• Dev/test/sandbox only\nC*\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-7-638.jpg?cb=1444714425\" title=\"Multi-node, Single Data Centre&#10;• Two or more servers runnin...\" target=\"_blank\">\n        7.\n      </a>\n    Multi-node, Single Data Centre\n• Two or more servers running\nCassandra within one DC\n• Replication of data\n(redundancy)\n• Increased capacity (storage +\nthroughput)\n• Baseline for production\nclusters\nC* C*\nC*\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-8-638.jpg?cb=1444714425\" title=\"Multi-node, Multi-DC&#10;• Cassandra running in two or&#10;more dat...\" target=\"_blank\">\n        8.\n      </a>\n    Multi-node, Multi-DC\n• Cassandra running in two or\nmore data centres\n• Global deployments\n• Data near your customers\n(reduced latency)\n• Supported out-of-the-box\nC* C*\nC*\nC* C*\nC*\nC* C*\nC*\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-9-638.jpg?cb=1444714425\" title=\"Snitches&#10;• Understands data centres and racks&#10;• Implementat...\" target=\"_blank\">\n        9.\n      </a>\n    Snitches\n• Understands data centres and racks\n• Implementation may automatically determine node DC and rack \n(EC2MultiRegionSnitch uses AWS internal metadata service, GossipingPropertiesFileSnitch loads\na .properties ﬁle)\n• Node DC and rack is advertised via Gossip\n• Determine node proximity (estimated link latency)\n• Cluster may use a combination of Snitch implementations\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-10-638.jpg?cb=1444714425\" title=\"Data Centres&#10;• Collection of Racks&#10;• Complete replications&#10;...\" target=\"_blank\">\n        10.\n      </a>\n    Data Centres\n• Collection of Racks\n• Complete replications\n• Geographically separate\n• Possibly high-latency interconnects \n(e.g. East Coast US → Sydney, ~300ms round-trip)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-11-638.jpg?cb=1444714425\" title=\"Racks&#10;• Collection of nodes&#10;• May fail as a single unit&#10;• M...\" target=\"_blank\">\n        11.\n      </a>\n    Racks\n• Collection of nodes\n• May fail as a single unit\n• Modelled on the traditional DC rack/cage \n(n-servers running of a UPS)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-12-638.jpg?cb=1444714425\" title=\"☁&#10;• Amazon Web Services &#10;(use EC2MultiRegionSnitch)&#10;• Data ...\" target=\"_blank\">\n        12.\n      </a>\n    ☁\n• Amazon Web Services \n(use EC2MultiRegionSnitch)\n• Data Centre ≡ AWS Region \n(e.g. US_East_1, AP_SOUTHEAST_2)\n• Rack ≡ Availability Zone \n(e.g. us-east-1a, ap-southeast-2b)\n• Google Cloud Platform \n(no out-of-the-box auto-conﬁguring snitch — use GossipingPropertiesFileSnitch, or roll your own!)\n• Data Centre ≡ GCP Region \n(e.g. US, Europe)\n• Rack ≡ Zone \n(e.g. us-central1-a, europe-west1-a)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-13-638.jpg?cb=1444714425\" title=\"Data Centre Aware&#10;• Cassandra is data centre aware&#10;• Only f...\" target=\"_blank\">\n        13.\n      </a>\n    Data Centre Aware\n• Cassandra is data centre aware\n• Only fetch data from a remote DC if absolutely required \n(remote data is more “expensive”)\n• Clients can be made data centre aware\n• If your app knows its DC, client will talk to the closest DC\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-14-638.jpg?cb=1444714425\" title=\"Cluster cluster = Cluster.builder()&#10;.addContactPoint(…)&#10;.wi...\" target=\"_blank\">\n        14.\n      </a>\n    Cluster cluster = Cluster.builder()\n.addContactPoint(…)\n.withLoadBalancingPolicy(new DCAwareRoundRobinPolicy(“US_EAST_1\"))\n.build();\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-15-638.jpg?cb=1444714425\" title=\"Multi DC Support&#10;• Per-node public (internet-facing) IP add...\" target=\"_blank\">\n        15.\n      </a>\n    Multi DC Support\n• Per-node public (internet-facing) IP address\n• Optionally, per-node private IP address\n• Per-node public address is used for inter-data centre connectivity\n• Per node private address is used for intra-data centre connectivity\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-16-638.jpg?cb=1444714425\" title=\"Multi DC Support&#10;• Cloud ⇄ cloud, traditional ⇄ cloud, trad...\" target=\"_blank\">\n        16.\n      </a>\n    Multi DC Support\n• Cloud ⇄ cloud, traditional ⇄ cloud, traditional ⇄ traditional\n• Easy to setup per-node public and private addresses\n• Private network clusters ⇄ Cloud clusters\n• Private networks: 𝑛 public addresses, shared by 𝑥 private\naddresses. Not 1 ↔ 1 \n(where often 𝑥 &gt; 𝑛)\n• done via Network Address Translation\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-17-638.jpg?cb=1444714425\" title=\"IPv4 Address Space Exhaustion&#10;Source: http://www.potaroo.ne...\" target=\"_blank\">\n        17.\n      </a>\n    IPv4 Address Space Exhaustion\nSource: http://www.potaroo.net/tools/ipv4/\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-18-638.jpg?cb=1444714425\" title=\"Multi-DC Support&#10;• IPv4&#10;• Address exhaustion&#10;• Over time, w...\" target=\"_blank\">\n        18.\n      </a>\n    Multi-DC Support\n• IPv4\n• Address exhaustion\n• Over time, will become more expensive to purchase addresses\n• Wasteful \n(being a good internet citizen)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-19-638.jpg?cb=1444714425\" title=\"Alternatives&#10;• IPv6&#10;• Java supports it ∴ Cassandra probably...\" target=\"_blank\">\n        19.\n      </a>\n    Alternatives\n• IPv6\n• Java supports it ∴ Cassandra probably supports it \n(untested by us)\n• Global IPv6 adoption is ~4% \n(according to Google — google.com/intl/en/ipv6/statistics.html)\n• IPv6/IPv4 hybrid \n(Teredo, 6over4, et. al.)\n• AWS EC2 does not support IPv6. End of story. \n(Elastic Load Balancer does support IPv6)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-20-638.jpg?cb=1444714425\" title=\"Alternatives&#10;• VPNs&#10;• tinc, OpenVPN, etc.&#10;• All private add...\" target=\"_blank\">\n        20.\n      </a>\n    Alternatives\n• VPNs\n• tinc, OpenVPN, etc.\n• All private address space — no dual addressing\n• Requires multiple links — between every DC and per client\n• Address space overlaps between multiple VPNs\n• Connectivity to multiple clusters an issue \n(for multi-cluster apps, centralised monitoring, etc)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-21-638.jpg?cb=1444714425\" title=\"Data Centres Links&#10;3 3&#10;5 10&#10;7 21&#10;\" target=\"_blank\">\n        21.\n      </a>\n    Data Centres Links\n3 3\n5 10\n7 21\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-22-638.jpg?cb=1444714425\" title=\"Alternatives&#10;• Network Address Translation (NAT) &#10;(aka IP M...\" target=\"_blank\">\n        22.\n      </a>\n    Alternatives\n• Network Address Translation (NAT) \n(aka IP Masquerading or Port Address Translation (PAT))\n• Deployed on most private networks\n• Connectivity between private network clusters ⇄ Cloud clusters\n• Supports client connectivity to multiple clusters\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-23-638.jpg?cb=1444714425\" title=\"NAT Basics&#10;• Re-maps IP address spaces &#10;(e.g. Public 96.31....\" target=\"_blank\">\n        23.\n      </a>\n    NAT Basics\n• Re-maps IP address spaces \n(e.g. Public 96.31.81.80 ↔ Private 192.168.*.*)\n• 𝑛 public addresses, shared by 𝑥 private addresses. Not 1 ↔ 1 \n(where often n = 1, 𝑥 ＞ 𝑛)\n• Port Address Translation\n• Private port ↔ Public port\n• Outbound connections only without port forwarding or NAT traversal\n• Per DC gateway device — performs NAT and port forwarding\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-24-638.jpg?cb=1444714425\" title=\"NAT with Inbound Connections&#10;• Static port forwarding &#10;(con...\" target=\"_blank\">\n        24.\n      </a>\n    NAT with Inbound Connections\n• Static port forwarding \n(conﬁgured on the gateway)\n• Automatic port forwarding — UPnP, NAT-PMP/PCP \n(conﬁgured by the application, e.g. Cassandra)\n• NAT Traversal — STUN, ICE, etc.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-25-638.jpg?cb=1444714425\" title=\"NAT + C∗&#10;Situation: 𝑛 Cassandra nodes, 1 public address per...\" target=\"_blank\">\n        25.\n      </a>\n    NAT + C∗\nSituation: 𝑛 Cassandra nodes, 1 public address per data centre\n• Port forward different public ports for each node\n• Advertise assigned ports\n• Modify Cassandra and client applications to connect to\nadvertised ports\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-26-638.jpg?cb=1444714425\" title=\"Advertising Port Mappings&#10;• Extend Cassandra Gossip&#10;• Inclu...\" target=\"_blank\">\n        26.\n      </a>\n    Advertising Port Mappings\n• Extend Cassandra Gossip\n• Include port numbers in node address announcements\n• Allow seed node addresses to include port numbers\n• Allow multiple nodes to have identical public &amp; private addresses \n(only port numbers differ per DC)\n• How to bootstrap? SIP?\n• Cassandra must be aware of the allocated ports in order to advertise\n• Hard if C* is not directly responsible for the port mapping \n(e.g. static port forwarding)\n• Too many modiﬁcations to internals\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-27-638.jpg?cb=1444714425\" title=\"Advertising Port Mappings&#10;• DNS-SD — dns-sd.org &#10;(aka Bonjo...\" target=\"_blank\">\n        27.\n      </a>\n    Advertising Port Mappings\n• DNS-SD — dns-sd.org \n(aka Bonjour/Zeroconf)\n• Reads — works with existing DNS implementations \n(it’s just a DNS query)\n• Even inside restrictive networks, DNS usually works\n• Combination of DNS TXT, SRV and PTR records.\n• Updates\n• via DNS Update &amp; TSIG — supported by bind\n• via API — e.g. for AWS Route 53\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-28-638.jpg?cb=1444714425\" title=\"Advertising Port Mappings&#10;• DNS-SD cont’d.&#10;• SRV records co...\" target=\"_blank\">\n        28.\n      </a>\n    Advertising Port Mappings\n• DNS-SD cont’d.\n• SRV records contain hostname and port \n(i.e., hostname of the NAT gateway and public C* port)\n• TXT records contain key=value pairs \n(useful for additional connection &amp; conﬁg details)\n• Modify C* connection code to lookup foreign node port from DNS\n• Modify client driver connection code to lookup ports from DNS\n• Can be queried &amp; updated out-of-band \n(updated by the NAT device or central management server which knows which ports were mapped)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-29-638.jpg?cb=1444714425\" title=\"Advertised Details&#10;• Each cluster is it’s own browse domain...\" target=\"_blank\">\n        29.\n      </a>\n    Advertised Details\n• Each cluster is it’s own browse domain\n• Each NAT gateway device has an A record in the browse domain\n• Each DNS-SD service is named based on the private IP address\n• Requires unique private IP addresses across data centres\n• SRV port is the C* thrift port\n• Additional ports are advertise via TXT\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-30-638.jpg?cb=1444714425\" title=\"Conﬁguration&#10;• Cassandra is conﬁgured to only use private a...\" target=\"_blank\">\n        30.\n      </a>\n    Conﬁguration\n• Cassandra is conﬁgured to only use private addresses\n• On cluster creation\n• Establish a new DNS-SD browse domain\n• Create A records for each gateway device\n• NAT gateway device is notiﬁed when a new C* node is started\n• Allocates random public ports for C* and conﬁgures Port Forwarding\n• Updates DNS-SD\n• New SRV and TXT record\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-31-638.jpg?cb=1444714425\" title=\"$ dns-sd -B _cassandra._tcp 1da53f83-e635-11e3-96eb-2ec9d09...\" target=\"_blank\">\n        31.\n      </a>\n    $ dns-sd -B _cassandra._tcp 1da53f83-e635-11e3-96eb-2ec9d09504f5.clusters.instaclustr.com.au.\nBrowsing for _cassandra._tcp\nA/R Flags if Domain Service Type Instance Name\nAdd 3 0 1da53f83-e635-11e3-96eb-2ec9d09504f5.clusters.instaclustr.com.au. _cassandra._tcp. 192-168-2-4\nAdd 3 0 1da53f83-e635-11e3-96eb-2ec9d09504f5.clusters.instaclustr.com.au. _cassandra._tcp. 192-168-1-2\nAdd 3 0 1da53f83-e635-11e3-96eb-2ec9d09504f5.clusters.instaclustr.com.au. _cassandra._tcp. 192-168-2-3\nAdd 3 0 1da53f83-e635-11e3-96eb-2ec9d09504f5.clusters.instaclustr.com.au. _cassandra._tcp. 192-168-2-2\nAdd 3 0 1da53f83-e635-11e3-96eb-2ec9d09504f5.clusters.instaclustr.com.au. _cassandra._tcp. 192-168-1-4\nAdd 2 0 1da53f83-e635-11e3-96eb-2ec9d09504f5.clusters.instaclustr.com.au. _cassandra._tcp. 192-168-1-3\n$ dns-sd -L 192-168-1-4 _cassandra._tcp 1da53f83-e635-11e3-96eb-2ec9d09504f5.clusters.instaclustr.com.au.\nLookup 192-168-1-4._cassandra._tcp.1da53f83-e635-11e3-96eb-2ec9d09504f5.clusters.instaclustr.com.au.\n192-168-1-4._cassandra._tcp.1da53f83-e635-11e3-96eb-2ec9d09504f5.clusters.instaclustr.com.au. can be reached at aws-\nus-east1-gateway.1da53f83-e635-11e3-96eb-2ec9d09504f5.clusters.instaclustr.com.au.:1236 (interface 0)\nversion=2.0.7\ncqlport=1237\n$ nslookup aws-us-east1-gateway.1da53f83-e635-11e3-96eb-2ec9d09504f5.clusters.instaclustr.com.au.\nNon-authoritative answer:\nName: aws-us-east1-gateway.1da53f83-e635-11e3-96eb-2ec9d09504f5.clusters.instaclustr.com.au\nAddress: 54.209.123.195\nOutput of dns-sd \n(Can also use avahi-browse, dig, or any other DNS query tool)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-32-638.jpg?cb=1444714425\" title=\"Java Driver Modiﬁcations&#10;• This is usually a no-op &#10;(the de...\" target=\"_blank\">\n        32.\n      </a>\n    Java Driver Modiﬁcations\n• This is usually a no-op \n(the default is IdentityTranslater)\n• Modify translate() to perform a DNS-SD lookup.\n• The address parameter is a node private IP address.\n• Locate a service with a name = private IP address to determine\npublic IP/port.\npublic interface AddressTranslater {\npublic InetSocketAddress translate(InetSocketAddress address);\n}\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-33-638.jpg?cb=1444714425\" title=\"Modifying Cassandra&#10;• Responsible for managing Socket conne...\" target=\"_blank\">\n        33.\n      </a>\n    Modifying Cassandra\n• Responsible for managing Socket connections.\n• Modify newSocket() to perform a DNS-SD lookup.\n• The endpoint parameter is a node private IP address.\n• Locate a service with a name = private IP address to determine\npublic IP/port\npublic class OutboundTcpConnectionPool\n{\n⋮\npublic static Socket newSocket(InetAddress endpoint) throws IOException {…} \n⋮\n}\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-34-638.jpg?cb=1444714425\" title=\"C* C*&#10;C*&#10;C* C*&#10;C*&#10;NAT Gateway NAT Gateway&#10;DNS (+ DNS-SD) Se...\" target=\"_blank\">\n        34.\n      </a>\n    C* C*\nC*\nC* C*\nC*\nNAT Gateway NAT Gateway\nDNS (+ DNS-SD) Server \n(Route 53, Self-hosted, etc)Client\nApplication\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891/95/multiregion-cassandra-clusters-35-638.jpg?cb=1444714425\" title=\"Thanks!&#10;Questions?&#10;adam@instaclustr.com&#10;\" target=\"_blank\">\n        35.\n      </a>\n    Thanks!\nQuestions?\nadam@instaclustr.com\n \n  </li>\n              </ol></div></div></div><aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\"><dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\"><ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"53858704\" title=\"Teaching Complex Topics\" href=\"https://www.linkedin.com/learning/teaching-complex-topics?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Teaching Complex Topics\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Teaching Complex Topics\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=cDbzwXgIw4qUMFNwQVRpzzJqvW8%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lXiet_9WfZHLrfsLfZLSiol8eeywAmQw1fuahSTfkF469LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Teaching Complex Topics</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"53858704\" title=\"Office 365: PowerPoint Essential Training\" href=\"https://www.linkedin.com/learning/office-365-powerpoint-essential-training?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Office 365: PowerPoint Essential Training\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Office 365: PowerPoint Essential Training\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=Z%2Bf7UhZxTfFiRRjPnJ65JFvZVMs%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-iXCCj-NKfY3DscMXYZLSiol4Rfy0Hlgc2feavSTniEo69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Office 365: PowerPoint Essential Training</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"53858704\" title=\"Brain-Based Elearning Design\" href=\"https://www.linkedin.com/learning/brain-based-elearning-design?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Brain-Based Elearning Design\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Brain-Based Elearning Design\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=oeh1GtlKyLzCey9HnQ0%2B%2BXfGgy4%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-kXSWo-dSfZXHpe8TeZLSiol4ffysBlQ02e-utSTHkE469LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Brain-Based Elearning Design</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"53500779\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"GumGum: Multi-Region Cassandra in AWS\" href=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\">\n    \n    <div class=\"related-content\"><p>GumGum: Multi-Region Cassandra in AWS</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"37514256\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Managing 50K+ Redis Databases Over 4 Public Clouds ... with a Tiny Devops Team\" href=\"https://www.slideshare.net/RedisLabs/redis-labs-igt-0714\">\n    \n    <div class=\"related-content\"><p>Managing 50K+ Redis Databases Over 4 Public Clouds ... with a Tiny Devops Team</p><p>Redis Labs</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"80965555\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Apache Cassandra Community Health\" href=\"https://www.slideshare.net/Instaclustr/apache-cassandra-community-health\">\n    \n    <div class=\"related-content\"><p>Apache Cassandra Community Health</p><p>Instaclustr</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"75486973\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Instaclustr introduction to managing cassandra\" href=\"https://www.slideshare.net/Instaclustr/instaclustr-introduction-to-managing-cassandra\">\n    \n    <div class=\"related-content\"><p>Instaclustr introduction to managing cassandra</p><p>Instaclustr</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"73692909\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Instaclustr webinar 50,000 transactions per second with Apache Spark on Apache Cassandra\" href=\"https://www.slideshare.net/Instaclustr/instaclustr-webinar-50000-transactions-per-second-with-apache-spark-on-apache-cassandra-73692909\">\n    \n    <div class=\"related-content\"><p>Instaclustr webinar 50,000 transactions per second with Apache Spark on Apach...</p><p>Instaclustr</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"73566869\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Instaclustr Apache Cassandra Best Practices &amp; Toubleshooting\" href=\"https://www.slideshare.net/Instaclustr/instaclustr-apache-cassandra-best-practices-toubleshooting\">\n    \n    <div class=\"related-content\"><p>Instaclustr Apache Cassandra Best Practices &amp; Toubleshooting</p><p>Instaclustr</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"72845663\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Instaclustr Webinar 50,000 Transactions Per Second with Apache Spark on Apache Cassandra\" href=\"https://www.slideshare.net/Instaclustr/instaclustr-webinar-50000-transactions-per-second-with-apache-spark-on-apache-cassandra\">\n    \n    <div class=\"related-content\"><p>Instaclustr Webinar 50,000 Transactions Per Second with Apache Spark on Apach...</p><p>Instaclustr</p></div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div></div><footer>\n          <div class=\"row\"><div class=\"columns\"><ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div></div>\n          \n          <div class=\"row\"><div class=\"columns\"><p class=\"copyright text-center\">LinkedIn Corporation © 2018</p></div></div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div>\n    <div class=\"row\"><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></div>\n        \n    </div>\n    \n    \n      <noscript>\n    </noscript>",
        "created_at": "2018-10-03T11:28:36+0000",
        "updated_at": "2018-10-03T11:28:40+0000",
        "published_at": null,
        "published_by": [
          "Instaclustr"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 8,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/instaclustrcassandramulti-regionclusterscs2014-151013050427-lva1-app6891-thumbnail-4.jpg?cb=1444714425",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12280"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 33,
            "label": "internet.architecture",
            "slug": "internet-architecture"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          },
          {
            "id": 978,
            "label": "cloud",
            "slug": "cloud"
          }
        ],
        "is_public": false,
        "id": 12279,
        "uid": null,
        "title": "Global Cloud — Active-Active and Beyond",
        "url": "https://medium.com/netflix-techblog/global-cloud-active-active-and-beyond-a0fdfa2c3a45",
        "content": "<section class=\"section section--body section--first\"><div class=\"section-divider\"><hr class=\"section-divider\" /></div><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><h1 id=\"ee40\" class=\"graf graf--h3 graf--leading graf--title\">Global Cloud — Active-Active and Beyond</h1><p id=\"3fbc\" class=\"graf graf--p graf-after--h3\">This is a continuing post on the Netflix architecture for Global Availability. In the past we talked about efforts like <a href=\"https://medium.com/@Netflix_Techblog/isthmus-resiliency-against-elb-outages-d9e0623484f3\" data-href=\"https://medium.com/@Netflix_Techblog/isthmus-resiliency-against-elb-outages-d9e0623484f3\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">Isthmus</a> and <a href=\"https://medium.com/@Netflix_Techblog/active-active-for-multi-regional-resiliency-c47719f6685b\" data-href=\"https://medium.com/@Netflix_Techblog/active-active-for-multi-regional-resiliency-c47719f6685b\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">Active-Active</a>:</p><div id=\"044d\" class=\"graf graf--mixtapeEmbed graf-after--p\"><a href=\"https://medium.com/@Netflix_Techblog/isthmus-resiliency-against-elb-outages-d9e0623484f3\" data-href=\"https://medium.com/@Netflix_Techblog/isthmus-resiliency-against-elb-outages-d9e0623484f3\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"https://medium.com/@Netflix_Techblog/isthmus-resiliency-against-elb-outages-d9e0623484f3\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">Isthmus — Resiliency against ELB outages</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">achieving multi-regional ELB resiliency</em>medium.com</a></div><div id=\"019e\" class=\"graf graf--mixtapeEmbed graf-after--mixtapeEmbed\"><a href=\"https://medium.com/@Netflix_Techblog/active-active-for-multi-regional-resiliency-c47719f6685b\" data-href=\"https://medium.com/@Netflix_Techblog/active-active-for-multi-regional-resiliency-c47719f6685b\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"https://medium.com/@Netflix_Techblog/active-active-for-multi-regional-resiliency-c47719f6685b\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">Active-Active for Multi-Regional Resiliency</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">With large scale and velocity there is increased chance of failure.</em>medium.com</a></div><p id=\"f34a\" class=\"graf graf--p graf-after--mixtapeEmbed\">We continue the story from where we left off at the end of the Active-Active project in 2013. We had achieved multi-regional resiliency for our members in the Americas, where the vast majority of Netflix members were located at the time. Our European members, however, were still at risk from a single point of failure.</p><figure id=\"d9b7\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*AJFbTcWbGFHqSJvV.\" data-width=\"499\" data-height=\"224\" src=\"https://cdn-images-1.medium.com/max/1600/0*AJFbTcWbGFHqSJvV.\" alt=\"image\" /></div></div></figure><p id=\"212c\" class=\"graf graf--p graf-after--figure\">Our expansion around the world since then, has resulted in a growing percentage of international members who were exposed to this single point of failure, so we set out to make our cloud deployment even more resilient.</p><h3 id=\"47dd\" class=\"graf graf--h3 graf-after--p\">Creating a Global Cloud</h3><p id=\"a10e\" class=\"graf graf--p graf-after--h3\">We decided to create a global cloud where we would be able to serve requests from any member in any AWS region where we are deployed. The diagram below shows the logical structure of our multi-region deployment and the default routing of member traffic to AWS region.</p><figure id=\"0193\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*gkGFwvOCDtZu3cp2c3wjvg.png\" data-width=\"389\" data-height=\"330\" src=\"https://cdn-images-1.medium.com/max/1600/1*gkGFwvOCDtZu3cp2c3wjvg.png\" alt=\"image\" /></div></div></figure><h3 id=\"0d85\" class=\"graf graf--h3 graf-after--figure\">Getting There</h3><p id=\"366a\" class=\"graf graf--p graf-after--h3\">Getting to the end state, while not disrupting our ongoing operations and the development of new features, required breaking the project down into a number of stages. From an availability perspective, removing AWS EU-West-1 as a single point of failure was the most important goal, so we started in the Summer of 2014 by identifying the tasks that we needed to execute in order to be able to serve our European members from US-East-1.</p><h4 id=\"6fa1\" class=\"graf graf--h4 graf-after--p\">Data Replication</h4><p id=\"b8d1\" class=\"graf graf--p graf-after--h4\">When we initially launched service in Europe in 2012, we made an explicit decision to build regional data islands for most, but not all, of the member related data. In particular, while a member’s subscription allowed them to stream anywhere that we offered service, information about what they watched while in Europe would not be merged with the information about what they watched while in the Americas. Since we figured we would have relatively few members travelling across the Atlantic, we felt that the isolation that these data islands created was a win as it would mitigate the impact of a region specific outage.</p><p id=\"5bb4\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">Cassandra</strong></p><p id=\"410b\" class=\"graf graf--p graf-after--p\">In order to serve our EU members a normal experience from US-East-1, we needed to replicate the data in the EU Cassandra island data sets to the Cassandra clusters in US-East-1 and US-West-2. We considered replicating this data into separate keyspaces in US clusters or merging the data with our Americas data. While using separate keyspaces would have been more cost efficient, merging the datasets was more in line with our longer term goal of being able to serve any member from any region as the Americas data would be replicated to the Cassandra clusters in EU-West-1.</p><p id=\"46a0\" class=\"graf graf--p graf-after--p\">Merging the EU and Americas data was more complicated than the replication work that was part of the 2013 Active-Active project as we needed to examine each component data set to understand how to merge the data. Some data sets were appropriately keyed such that the result was the union of the two island data sets. To simplify the migration of such data sets, the Netflix Cloud Database Engineering (CDE) team enhanced the Astyanax Cassandra client to support writing to two keyspaces in parallel. This dual write functionality was sometimes used in combination with another tool built by the CDE that could be used to forklift data from one cluster or keyspace to another. For other data sets, such as member viewing history, custom tools were needed to handle combining the data associated with each key. We also discovered one or two data sets in which there were unexpected inconsistencies in the data that required deeper analysis to determine which particular values to keep.</p><p id=\"f1cb\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">EVCache</strong></p><p id=\"00a8\" class=\"graf graf--p graf-after--p\">As described in the blog post on the Active-Active project, we built a mechanism to allow updates to EVCache clusters in one region to invalidate the entry in the corresponding cluster in the other US region using an SQS message. EVCache now supports both full replication and invalidation of data in other regions, which allows application teams to select the strategy that is most appropriate to their particular data set. Additional details about the current EVCache architecture are available in <a href=\"https://medium.com/@Netflix_Techblog/caching-for-a-global-netflix-7bcc457012f1\" data-href=\"https://medium.com/@Netflix_Techblog/caching-for-a-global-netflix-7bcc457012f1\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">a recent Tech Blog post</a>:</p><div id=\"04f0\" class=\"graf graf--mixtapeEmbed graf-after--p\"><a href=\"https://medium.com/@Netflix_Techblog/caching-for-a-global-netflix-7bcc457012f1\" data-href=\"https://medium.com/@Netflix_Techblog/caching-for-a-global-netflix-7bcc457012f1\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"https://medium.com/@Netflix_Techblog/caching-for-a-global-netflix-7bcc457012f1\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">Caching for a Global Netflix</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">#CachesEverywhere</em>medium.com</a></div><h4 id=\"342e\" class=\"graf graf--h4 graf-after--mixtapeEmbed\">Personalization Data</h4><p id=\"8bfc\" class=\"graf graf--p graf-after--h4\">Historically the personalization data for any given member has been pre-computed in only one of our AWS regions and then replicated to whatever other regions might service requests for that member. When a member interacted with the Netflix service in a way that was supposed to trigger an update of the recommendations, this would only happen if the interaction was serviced in the member’s “home” region, or its active-active replica, if any.</p><p id=\"b7e2\" class=\"graf graf--p graf-after--p\">This meant that when a member was serviced from a different region during a traffic migration, their personalized information would not be updated. Since there are regular, clock driven, updates to the precomputed data sets, this was considered acceptable for the first phase of the Global Cloud project. In the longer term, however, the precomputation system was enhanced to allow the events that triggered recomputation to be delivered across all three regions. This change also allowed us to redistribute the precomputation workload based on resource availability.</p><h4 id=\"5b40\" class=\"graf graf--h4 graf-after--p\">Handling Misrouted Traffic</h4><p id=\"b1eb\" class=\"graf graf--p graf-after--h4\">In the past, Netflix has used a variety of application level mechanisms to redirect device traffic that has landed in the “wrong” AWS region, due to DNS anomalies, back to the member’s “home” region. While these mechanisms generally worked, they were often a source of confusion due the differences in their implementations. As we started moving towards the Global Cloud, we decided that, rather than redirecting the misrouted traffic, we would use the same Zuul-to-Zuul routing mechanism that we use when failing over traffic to another region to transparently proxy traffic from the “wrong” region to the “home” region.</p><p id=\"9fcf\" class=\"graf graf--p graf-after--p\">As each region became capable of serving all members, we could then update the Zuul configuration to stop proxying the “misrouted” traffic to the member’s home region and simply serve it locally. While this potentially added some latency versus sticky redirects, it allowed several teams to simplify their applications by removing the often crufty redirect code. Application teams were given the guidance that they should no longer worry about whether a member was in the “correct” region and instead serve them the best response that they could give the locally available information.</p><h4 id=\"d6a2\" class=\"graf graf--h4 graf-after--p\">Evolving Chaos Kong</h4><p id=\"b9d9\" class=\"graf graf--p graf-after--h4\">With the Active-Active deployment model, our Chaos Kong exercises involved failing over a single region into another region. This is also the way we did our first few Global Cloud failovers. The following graph shows our traffic steering during a production issue in US-East-1. We steered traffic first from US-East-1 to US-West-2 and then later in the day to EU-West-1. The upper graph shows that the aggregate, global, stream starts tracked closely to the previous week’s pattern, despite the shifts in the amount of traffic being served by each region. The thin light blue line shows SPS traffic for each region the previous week and allows you to see the amount of traffic we are shifting.</p></div><div class=\"section-inner sectionLayout--outsetColumn\"><figure id=\"fc70\" class=\"graf graf--figure graf--layoutOutsetCenter graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*nRlYKs7dRoWcvYE0.\" data-width=\"1200\" data-height=\"288\" data-action=\"zoom\" data-action-value=\"0*nRlYKs7dRoWcvYE0.\" src=\"https://cdn-images-1.medium.com/max/2000/0*nRlYKs7dRoWcvYE0.\" alt=\"image\" /></div></div></figure></div><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"5b7c\" class=\"graf graf--p graf-after--figure\">By enhancing our traffic steering tools, we are now able to steer traffic from one region to both remaining regions to make use of available capacity. The graphs below show a situation where we evacuated all traffic from US-East-1, sending most of the traffic to EU-West-1 and a smaller portion to US-West-2.</p></div><div class=\"section-inner sectionLayout--outsetColumn\"><figure id=\"31e0\" class=\"graf graf--figure graf--layoutOutsetCenter graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*DOqQH3D_Xq-6o2jl.\" data-width=\"1200\" data-height=\"273\" data-is-featured=\"true\" data-action=\"zoom\" data-action-value=\"0*DOqQH3D_Xq-6o2jl.\" src=\"https://cdn-images-1.medium.com/max/2000/0*DOqQH3D_Xq-6o2jl.\" alt=\"image\" /></div></div></figure></div><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"d08a\" class=\"graf graf--p graf-after--figure\">We have done similar evacuations for the other two regions, each of them involving rerouted traffic being split between both remaining regions based on available capacity and minimizing member impact. For more details on the evolution of the Kong exercises and our Chaos philosophy behind them, <a href=\"https://medium.com/@Netflix_Techblog/chaos-engineering-upgraded-878d341f15fa\" data-href=\"https://medium.com/@Netflix_Techblog/chaos-engineering-upgraded-878d341f15fa\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">see our earlier post</a>:</p><div id=\"5e64\" class=\"graf graf--mixtapeEmbed graf-after--p\"><a href=\"https://medium.com/@Netflix_Techblog/chaos-engineering-upgraded-878d341f15fa\" data-href=\"https://medium.com/@Netflix_Techblog/chaos-engineering-upgraded-878d341f15fa\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"https://medium.com/@Netflix_Techblog/chaos-engineering-upgraded-878d341f15fa\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">Chaos Engineering Upgraded</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">Chaos Kong is the most destructive Chaos Monkey yet</em>medium.com</a></div><h3 id=\"bb44\" class=\"graf graf--h3 graf-after--mixtapeEmbed\">Are We Done?</h3><p id=\"e13e\" class=\"graf graf--p graf-after--h3\">Not even close. We will continue to explore new ways in which to efficiently and reliably deliver service to our millions of global members. We will report on those experiments in future updates here.</p><p id=\"bc23\" class=\"graf graf--p graf-after--p graf--trailing\"><em class=\"markup--em markup--p-em\">— Peter Stout on behalf of all the teams that contributed to the Global Cloud Project</em></p></div></div></section><section class=\"section section--body section--last\"><div class=\"section-divider\"><hr class=\"section-divider\" /></div><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"9cee\" class=\"graf graf--p graf--leading graf--trailing\"><em class=\"markup--em markup--p-em\">Originally published at </em><a href=\"http://techblog.netflix.com/2016/03/global-cloud-active-active-and-beyond.html\" data-href=\"http://techblog.netflix.com/2016/03/global-cloud-active-active-and-beyond.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\"><em class=\"markup--em markup--p-em\">techblog.netflix.com</em></a><em class=\"markup--em markup--p-em\"> on March 30, 2016.</em></p></div></div></section>",
        "created_at": "2018-10-03T11:25:23+0000",
        "updated_at": "2018-10-03T11:25:36+0000",
        "published_at": "2016-03-30T07:00:00+0000",
        "published_by": [
          "Netflix Technology Blog"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 7,
        "domain_name": "medium.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/0*DOqQH3D_Xq-6o2jl.",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12279"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 10,
            "label": "api",
            "slug": "api"
          },
          {
            "id": 23,
            "label": "elasticsearch",
            "slug": "elasticsearch"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 883,
            "label": "java",
            "slug": "java"
          }
        ],
        "is_public": false,
        "id": 12274,
        "uid": null,
        "title": "apache/usergrid",
        "url": "https://github.com/apache/usergrid",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<h2><a id=\"user-content-overview\" class=\"anchor\" aria-hidden=\"true\" href=\"#overview\"></a>Overview</h2>\n<p><strong>Usergrid is a multi-tenant Backend-as-a-Service stack for web &amp; mobile apps, based on RESTful APIs.</strong></p>\n<h2><a id=\"user-content-contributing\" class=\"anchor\" aria-hidden=\"true\" href=\"#contributing\"></a>Contributing</h2>\n<p>We accept all contributions via our GitHub, so you can fork our repo (apache/usergrid) and then submit a PR back to us for approval. For larger PRs you'll need to have an ICLA form on file with Apache. For more information see <a href=\"http://usergrid.apache.org/docs/reference/contribute-code.html\" rel=\"nofollow\">How to Contribute Code &amp; Docs</a>.</p>\n<h2><a id=\"user-content-build-awesome-apps-with-usergrid\" class=\"anchor\" aria-hidden=\"true\" href=\"#build-awesome-apps-with-usergrid\"></a>Build awesome apps with Usergrid!</h2>\n<p>Apache Usergrid provides all code necessary to build and power modern mobile applications.  This includes the server stack, administrative portal website, SDKs in most popular languages, as well as command line tools.</p>\n<ul><li>\n<p>The server-side stack, a Java 8 + Cassandra + ElasticSearch codebase that powers all of the features, is located under <a href=\"https://github.com/apache/usergrid/blob/master/stack\"><code>/stack</code></a>. You can install dependencies and compile it with maven. See <a href=\"https://github.com/apache/usergrid/blob/master/stack#requirements\">stack/README.md</a> for instructions.</p>\n</li>\n<li>\n<p>The admin portal is a pure HTML5+JavaScript app allowing you to register developers and let them manage their apps in a multi-tenant cluster. Located under <a href=\"https://github.com/apache/usergrid/blob/master/portal\"><code>/portal</code></a></p>\n</li>\n<li>\n<p>SDKs for <a href=\"https://github.com/apache/usergrid-swift\">Swift</a>, <a href=\"https://github.com/apache/usergrid-android\">Android</a>, <a href=\"https://github.com/apache/usergrid-javascript\">HTML5/JavaScript</a>, <a href=\"https://github.com/apache/usergrid-nodejs\">node.js</a>, <a href=\"https://github.com/apache/usergrid-java\">Java</a>, <a href=\"https://github.com/apache/usergrid-dotnet\">.Net / Windows</a>, and <a href=\"https://github.com/apache/usergrid-python\">Python</a>.</p>\n</li>\n<li>\n<p>a command-line client “ugc” allowing you to complete most maintenance tasks, as well as queries in a manner similar to the mysql or the mongo shell, located under <a href=\"https://github.com/apache/usergrid/blob/master/ugc\"><code>/ugc</code></a>. You can install it on your machine with a simple <code>sudo gem install ugc</code></p>\n</li>\n</ul><h2><a id=\"user-content-for-more-information\" class=\"anchor\" aria-hidden=\"true\" href=\"#for-more-information\"></a>For more information</h2>\n<p>See the Apache Usergrid <a href=\"http://usergrid.apache.org\" rel=\"nofollow\">web site</a> and <a href=\"http://usergrid.apache.org\" rel=\"nofollow\">documentation</a>.</p>\n</article>",
        "created_at": "2018-10-03T00:47:27+0000",
        "updated_at": "2018-10-03T00:47:39+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://avatars3.githubusercontent.com/u/47359?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12274"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 903,
            "label": "orchestration",
            "slug": "orchestration"
          }
        ],
        "is_public": false,
        "id": 12264,
        "uid": null,
        "title": "spotify/cstar",
        "url": "https://github.com/spotify/cstar",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p><a href=\"https://circleci.com/gh/spotify/cstar\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/261a421941d35a414a19dac873482ed1b7e8b7ca/68747470733a2f2f636972636c6563692e636f6d2f67682f73706f746966792f63737461722f747265652f6d61737465722e7376673f7374796c653d736869656c64\" alt=\"CircleCI\" data-canonical-src=\"https://circleci.com/gh/spotify/cstar/tree/master.svg?style=shield\" /></a>\n<a href=\"https://github.com/spotify/cstar/blob/master/LICENSE\"><img src=\"https://camo.githubusercontent.com/38ba649f1d42cc25c982fe6511d30c6dc8a6a2a3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f73706f746966792f63737461722e737667\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/spotify/cstar.svg\" /></a></p>\n<p><code>cstar</code> is an Apache Cassandra cluster orchestration tool for the command line.</p>\n<p><a href=\"https://asciinema.org/a/BJkHpAGCdkSXTAhYf7bPVmerz?autoplay=1\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/abfd4b51f4c1180fcf22ef4c8f36c83110766359/68747470733a2f2f61736369696e656d612e6f72672f612f424a6b4870414743646b53585441685966376250566d65727a2e706e67\" alt=\"asciicast\" data-canonical-src=\"https://asciinema.org/a/BJkHpAGCdkSXTAhYf7bPVmerz.png\" /></a></p>\n<h2><a id=\"user-content-why-not-simply-use-ansible-or-fabric\" class=\"anchor\" aria-hidden=\"true\" href=\"#why-not-simply-use-ansible-or-fabric\"></a>Why not simply use Ansible or Fabric?</h2>\n<p>Ansible does not have the primitives required to run things in a topology aware fashion. One could\nsplit the C* cluster into groups that can be safely executed in parallel and run one group at a time.\nBut unless the job takes almost exactly the same amount of time to run on every host, such a solution\nwould run with a significantly lower rate of parallelism, not to mention it would be kludgy enough to\nbe unpleasant to work with.</p>\n<p>Unfortunately, Fabric is not thread safe, so the same type of limitations apply. Fabric allows one to\nrun a job in parallel on many machines, but with similar restrictions as those of Ansible groups.\nIt’s possibly to use fabric and celery together to do what is needed, but it’s a very complicated\nsolution.</p>\n<h2><a id=\"user-content-requirements\" class=\"anchor\" aria-hidden=\"true\" href=\"#requirements\"></a>Requirements</h2>\n<p>All involved machines are assumed to be some sort of UNIX-like system like OS X or Linux. The machine\nrunning cstar must have python3, the Cassandra hosts must have a Bourne style shell.</p>\n<h2><a id=\"user-content-installing\" class=\"anchor\" aria-hidden=\"true\" href=\"#installing\"></a>Installing</h2>\n<p>You need to have Python3 and run an updated version of pip (9.0.1).</p>\n<pre># pip3 install cstar\n</pre>\n<p>It's also possible to install straight from repo. This installs the latest version that may not be pushed to pypi:</p>\n<pre># pip install git+https://github.com/spotify/cstar.git\n</pre>\n<p>Some systems (like Ubuntu 14.04) might trigger ssh2-python related errors when installing because the locally available libssh2 is too old (&lt;1.6.0).\nIn such case, please apply the following procedure :</p>\n<pre>sudo apt-get install cmake libssl-dev libffi-dev python3-pip -y\ngit clone --recurse-submodules https://github.com/ParallelSSH/ssh2-python.git\ncd ssh2-python; sudo ./ci/install-ssh2.sh\nsudo pip3 install setuptools bcrypt --upgrade\nsudo pip3 install cstar --upgrade\n# or: \n# sudo pip3 install git+https://github.com/spotify/cstar.git --upgrade\n</pre>\n<p>This will build libssh2 from source using the one that ships with ssh2-python and install some required dependencies.</p>\n<h2><a id=\"user-content-code-of-conduct\" class=\"anchor\" aria-hidden=\"true\" href=\"#code-of-conduct\"></a>Code of conduct</h2>\n<p>This project adheres to the\n<a href=\"https://github.com/spotify/code-of-conduct/blob/master/code-of-conduct.md\">Open Code of Conduct</a>.\nBy participating, you are expected to honor this code.</p>\n<h2><a id=\"user-content-cli\" class=\"anchor\" aria-hidden=\"true\" href=\"#cli\"></a>CLI</h2>\n<p>CStar is run through the cstar command, like so</p>\n<pre># cstar COMMAND [HOST-SPEC] [PARAMETERS]\n</pre>\n<p>The HOST-SPEC specifies what nodes to run the script on. There are three ways to specify a the spec:</p>\n<ol><li>The <code>--seed-host</code> switch tells cstar to connect to a specific host and fetch the full ring topology\nfrom there, and then run the script on all nodes in the cluster. <code>--seed-host</code> can be specified\nmultiple times, and multiple hosts can be specified as a comma-separated list in order to run a\nscript across multiple clusters.</li>\n<li>The <code>--host</code> switch specifies an exact list of hosts to use. <code>--host</code> can be specified multiple\ntimes, and multiple hosts can be specified as a comma-separated list.</li>\n<li>The <code>--host-file</code> switch points to a file name containing a newline separated list of hosts. This\ncan be used together with process substitution, e.g. <code>--host-file &lt;(dig -t srv ...)</code></li>\n</ol><p>The command is the name of a script located in either <code>/usr/lib/cstar/commands</code> or in\n<code>~/.cstar/commands</code>. This script will be uploaded to all nodes in the cluster and executed. File suffixes\nare stripped. The requirements of the script are described below. Cstar comes pre-packaged with one script file\ncalled <code>run</code> which takes a single parameter <code>--command</code> - see examples below.</p>\n<p>Some additional switches to control cstar:</p>\n<ul><li>One can override the parallelism specified in a script by setting the switches\n<code>--cluster-parallelism</code>, <code>--dc-parallelism</code> and <code>--strategy</code>.</li>\n</ul><p>There are two special case invocations:</p>\n<ul><li>\n<p>One can skip the script name and instead use the <code>continue</code> command to specify a previously halted job\nto resume.</p>\n</li>\n<li>\n<p>One can skip the script name and instead use the <code>cleanup-jobs</code>. See <a href=\"#Cleaning-up-old-jobs\">Cleaning up old jobs</a>.</p>\n</li>\n<li>\n<p>Two python ssh modules can be used : <code>paramiko</code> (default) and <code>ssh2-python</code>. To use the faster (but experimental) ssh2-python module add the following flag : <code>--ssh-lib=ssh2</code></p>\n</li>\n<li>\n<p>If you need to access the remote cluster with a specific username, add <code>--ssh-username=remote_username</code> to your cstar command line. A private key file can also be specified using <code>--ssh-identity-file=my_key_file.pem</code>.</p>\n</li>\n<li>\n<p>To use plain text authentication, please add <code>--ssh-password=my_password</code> to the command line.</p>\n</li>\n<li>\n<p>In order to run the command first on a single node and then stop execution to verify everything worked as expected, add the following flag to your command line : <code>--stop-after=1</code>. cstar will stop after the first node executed the command and print out the appropriate resume command to continue the execution when ready : <code>cstar continue &lt;JOB_ID&gt;</code></p>\n</li>\n</ul><p>A script file can specify additional parameters.</p>\n<h2><a id=\"user-content-command-syntax\" class=\"anchor\" aria-hidden=\"true\" href=\"#command-syntax\"></a>Command syntax</h2>\n<p>In order to run a command, it is first uploaded to the relevant host, and then executed from there.</p>\n<p>Commands can be written in any scripting language in which the hash symbol starts a line comment, e.g.\nshell-script, python, perl or ruby.</p>\n<p>The first line must be a valid shebang. After that, commented lines containing key value pairs may\nbe used to override how the script is parallelised as well as providing additional parameters for\nthe script, e.g. <code># C* dc-parallel: true</code></p>\n<p>The possible keys are:</p>\n<p><code>cluster-parallelism</code>, can the script be run on multiple clusters in parallel. Default value is <code>true</code>.</p>\n<p><code>dc-parallelism</code>, can the script be run on multiple data centers in the same cluster in parallel. Default value is <code>false</code>.</p>\n<p><code>strategy</code>, how many nodes within one data center can the script be run on. Default is <code>topology</code>.\nCan be one of:</p>\n<ul><li><code>one</code>, only one node per data center</li>\n<li><code>topology</code>, inspect topology and run on as many nodes as the topology allows</li>\n<li><code>all</code>, can be run on all nodes at once</li>\n</ul><p><code>description</code>, specifies a description for the script used in the help message.</p>\n<p><code>argument</code>, specifies an additional input parameter for the script, as well as a help text and an\noptional default value.</p>\n<h2><a id=\"user-content-job-output\" class=\"anchor\" aria-hidden=\"true\" href=\"#job-output\"></a>Job output</h2>\n<p>Cstar automatically saves the job status to file during operation.</p>\n<p>Standard output, standard error and exit status of each command run against a Cassandra host is\nsaved locally on machine where cstar is running. They are available under the users home directory in\n<code>.cstar/jobs/JOB_ID/HOSTNAME</code></p>\n<h2><a id=\"user-content-how-jobs-are-run\" class=\"anchor\" aria-hidden=\"true\" href=\"#how-jobs-are-run\"></a>How jobs are run</h2>\n<p>When a new cstar job is created, it is assigned an id. (It's a UUID)</p>\n<p>Cstar stores intermediate job output in the directory\n<code>~/.cstar/remote_jobs/&lt;JOB_ID&gt;</code>. This directory contains files with the stdout, stderr and PID of the\nscript, and once it finishes, it will also contain a file with the exit status of the script.</p>\n<p>Once the job finishes, these files will be moved over to the original host and put in the directory <code>~/.cstar/jobs/&lt;JOB_ID&gt;/&lt;REMOTE_HOST_NAME&gt;</code>.</p>\n<p>Cstar jobs are run nohuped, this means that even if the ssh connection is severed, the job will proceed.\nIn order to kill a cstar script invocation on a specific host, you will need ssh to the host and kill\nthe proccess.</p>\n<p>If a job is halted half-way, either by pressing <code>^C</code> or by using the <code>--stop-after</code> parameter, it can be\nrestarted using <code>cstar continue &lt;JOB_ID&gt;</code>. If the script was finished or already running when cstar\nshut down, it will not be rerun.</p>\n<h2><a id=\"user-content-cleaning-up-old-jobs\" class=\"anchor\" aria-hidden=\"true\" href=\"#cleaning-up-old-jobs\"></a>Cleaning up old jobs</h2>\n<p>Even on successful completion, the output of a cstar job is not deleted. This means it's easy to check\nwhat the output of a script was after it completed. The downside of this is that you can get a lot of\ndata lying around in <code>~/.cstar/jobs</code>. In order to clean things up, you can use\n<code>cstar cleanup-jobs</code>. By default it will remove all jobs older than one week. You can override the\nmaximum age of a job before it's deleted by using the <code>--max-job-age</code> parameter.</p>\n<h2><a id=\"user-content-examples\" class=\"anchor\" aria-hidden=\"true\" href=\"#examples\"></a>Examples</h2>\n<pre># cstar run --command='service cassandra restart' --seed-host some-host\n</pre>\n<p>Explanation: Run the local cli command <code>service cassandra restart</code> on a cluster. If necessary, add <code>sudo</code> to the\ncommand.</p>\n<pre># cstar puppet-upgrade-cassandra --seed-host some-host --puppet-branch=cass-2.2-upgrade\n</pre>\n<p>Explanation: Run the command puppet-upgrade-cassandra on a cluster. The puppet-upgrade-cassandra\ncommand expects a parameter, the puppet branch to run in order to perform the Cassandra upgrade. See the\npuppet-upgrade-cassandra example <a href=\"#Example-script-file\">below</a>.</p>\n<pre># cstar puppet-upgrade-cassandra --help\n</pre>\n<p>Explanation: Show help for the puppet-upgrade-cassandra command. This includes documentation for any\nadditional command-specific switches for the puppet-upgrade-cassandra command.</p>\n<pre># cstar continue 90642c11-4714-44c4-a13a-94b86f09e3bb\n</pre>\n<p>Explanation: Resume previously created job with job id 90642c11-4714-44c4-a13a-94b86f09e3bb.\nThe job id is the first line written on any executed job.</p>\n<h2><a id=\"user-content-example-script-file\" class=\"anchor\" aria-hidden=\"true\" href=\"#example-script-file\"></a>Example script file</h2>\n<p>This is an example script file that would saved to <code>~/.cstar/commands/puppet-upgrade-cassandra.sh</code>. It upgrades a\nCassandra cluster by running puppet on a different branch, then restarting the node, then upgrading the sstables.</p>\n<pre># !/usr/bin/env bash\n# C* cluster-parallel: true                                                                                                                                                                                    \n# C* dc-parallel: true                                                                                                                                                                                         \n# C* strategy: topology                                                                                                                                                                                        \n# C* description: Upgrade one or more clusters by switching to a different puppet branch                                                                                                                       \n# C* argument: {\"option\":\"--snapshot-name\", \"name\":\"SNAPSHOT_NAME\", \"description\":\"Name of pre-upgrade snapshot\", \"default\":\"preupgrade\"}                                                                      \n# C* argument: {\"option\":\"--puppet-branch\", \"name\":\"PUPPET_BRANCH\", \"description\":\"Name of puppet branch to switch to\", \"required\":true}                                                                       \nnodetool snapshot -t $SNAPSHOT_NAME\nsudo puppet --branch $PUPPET_BRANCH\nsudo service cassandra restart\nnodetool upgradesstables\n</pre>\n</article>",
        "created_at": "2018-09-29T19:36:29+0000",
        "updated_at": "2018-09-29T19:36:33+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 7,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/251374?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12264"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 11,
            "label": "database",
            "slug": "database"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 93,
            "label": "data",
            "slug": "data"
          }
        ],
        "is_public": false,
        "id": 12218,
        "uid": null,
        "title": "The Curious Case Of Tombstones",
        "url": "https://medium.com/cassandra-tombstones-clearing-use-case/the-curios-case-of-tombstones-d897f681a378",
        "content": "<div class=\"section-inner sectionLayout--insetColumn\"><h1 id=\"d5ef\" class=\"graf graf--h3 graf--leading graf--title\">The Curious Case Of Tombstones</h1><p id=\"752c\" class=\"graf graf--p graf--hasDropCapModel graf--hasDropCap graf-after--h3\">The dead are harmless, one would think but not quite so incase of noSQL database Cassandra !</p><p id=\"d626\" class=\"graf graf--p graf-after--p\">One of the biggest puzzles so far in my two years of working with Cassandra, has been tombstones. Unlike relational databases like Oracle, MySQL, etc, delete operation in Cassandra does not really snuff the data away rather it creates dead records called “tombstones” which stay along with the data till they get evicted by compaction depending on a few settings defined at the column family level and the yaml file. If not cleared, they can impact the health of the cluster in many ways. Some of the most obvious ones being</p><ul class=\"postList\"><li id=\"8de7\" class=\"graf graf--li graf-after--p\">Increase in read latency as a read operation will have to read all the live and the dead rows and then filter out the dead ones.</li><li id=\"3739\" class=\"graf graf--li graf-after--li\">Occupy space, which can be an issue depending on the infra that C* is set up on.</li><li id=\"4715\" class=\"graf graf--li graf-after--li\">Danger of data resurrection</li></ul><p id=\"821b\" class=\"graf graf--p graf-after--li\">In this blog post, I intend to share how we dealt with tombstone issues and reclaimed space in our cluster.</p><p id=\"1788\" class=\"graf graf--p graf-after--p\">We started our Cassandra journey three years ago, as part of a big project to move our Enterprise platform to cloud Native architecture. Cassandra was selected as the preferred no-SQL database and it completely replaced Oracle. It was a big change and the migration journey was quite challenging.Three years down the line, the platform is nearing full adoption and supports high revenue generating applications which are mission critical to the company.</p><p id=\"3938\" class=\"graf graf--p graf-after--p\">Due to the evolving and iterative development of the system, there had been no purge done on the transactions data. Also the complex business rules made it difficult to make use of Cassandra’s TTL (Time –To-Live) functionality which allows to write transactions with an auto expiry time. Every type of transaction has different purge rules, so we ended up with a situation of dealing with a huge number of deletes in the first pass of the purge run. Almost 60% of data in column families met the purge rules.</p><p id=\"eaec\" class=\"graf graf--p graf-after--p\">Since Cassandra delete is actually an upsert statement in disguise, it essentially translates to a momentary increase in the db storage space until the <strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">gc_grace_seconds</em></strong> value is reached and compaction removes the tombstones. However, this was not the case. I carried out the purge in various phases, purging a certain number of records each day and monitoring the space before and after every purge run. The records were getting deleted and tombstones were also created, but there was neither an increase nor a decrease in the space.</p><p id=\"7ea5\" class=\"graf graf--p graf-after--p\">We have analytics jobs which read all data from Cassandra and push it out to other systems like Kafka, Tableau and Elastic Search for analysis and reporting (<em class=\"markup--em markup--p-em\">more on that use case in a future blog</em>) , so it was very essential for us to remove the unwanted tombstones.</p><p id=\"9e69\" class=\"graf graf--p graf-after--p\">For faster tombstone eviction, the gc_grace_seconds on the column families was altered to 4 days from the default value of 10 days. Repair jobs were also scheduled every 3 days but even after all the records were purged and the gc_grace_seconds had elapsed, there was no change in space.</p><p id=\"7acb\" class=\"graf graf--p graf-after--p\">An examination of the sstablesmetadata revealed that the droppable tombstone value was 0.02, and the <em class=\"markup--em markup--p-em\">tombstone_threshold</em> was the default value of 0.2.</p><blockquote id=\"5cab\" class=\"graf graf--blockquote graf-after--p\"><div>Estimated droppable tombstones: 0.02039067027235851</div></blockquote><p id=\"6f45\" class=\"graf graf--p graf-after--blockquote\">This meant that we were pretty far from the threshold value of the tombstones being removed by compaction.</p><p id=\"9186\" class=\"graf graf--p graf-after--p\">All the Column families that were being purged had LCS (Leveled Compaction Strategy).We also played around with column family settings to see if space would be reclaimed. But lowering the tombstone threshold to a value of 0.01 (lower than Estimated droppable tombstones), or modifying the <em class=\"markup--em markup--p-em\">unchecked_tombstone</em> property made no difference.</p><p id=\"80b0\" class=\"graf graf--p graf-after--p\">We then decided to try out a newer garbage collect command that was introduced by Datastax in version 5.1 and above. Steps followed for this were :</p><ul class=\"postList\"><li id=\"70f5\" class=\"graf graf--li graf-after--p\">Lower gc_grace_seconds on column families to half a day.</li><li id=\"f47e\" class=\"graf graf--li graf-after--li\">Run garbage collect command</li></ul><blockquote id=\"da64\" class=\"graf graf--blockquote graf-after--li\"><div><strong class=\"markup--strong markup--blockquote-strong\">nodetool garbagecollect keyspace cf -j 0</strong></div></blockquote><ul class=\"postList\"><li id=\"1494\" class=\"graf graf--li graf-after--blockquote\">Monitor in ops center. The above command triggers the below compaction jobs</li></ul><blockquote id=\"8a33\" class=\"graf graf--blockquote graf--startsWithDoubleQuote graf-after--li\"><div><strong class=\"markup--strong markup--blockquote-strong\">“Remove deleted data of keyspace.cf”</strong></div></blockquote><ul class=\"postList\"><li id=\"f7f5\" class=\"graf graf--li graf-after--blockquote\">Run a repair after garbage collect job is run. Repair can be run just on the specific column families which were purged.</li></ul><blockquote id=\"83ab\" class=\"graf graf--blockquote graf--startsWithDoubleQuote graf-after--li\"><div><strong class=\"markup--strong markup--blockquote-strong\">“nodetool repair -pr -full keyspace cf1 cf2 cf3”</strong></div></blockquote><ul class=\"postList\"><li id=\"324e\" class=\"graf graf--li graf-after--blockquote\">Revert the gc_grace_seconds value to the original one.</li></ul><p id=\"f51d\" class=\"graf graf--p graf-after--li\">And ta-da, we were able to reclaim the space!! Like I mentioned above, 60% of our transactions’ data got purged so the amount of space reclaimed was significant. Given below is the actual stats (from one of our transactional nodes) of this whole exercise. The column family names have been altered.</p><figure id=\"d621\" class=\"graf graf--figure graf-after--p graf--trailing\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*GDe4BcPBsIYwyGTQd9ilFg.png\" data-width=\"1729\" data-height=\"502\" data-action=\"zoom\" data-action-value=\"1*GDe4BcPBsIYwyGTQd9ilFg.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*GDe4BcPBsIYwyGTQd9ilFg.png\" alt=\"image\" /></div></div></figure></div><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"9017\" class=\"graf graf--p graf--leading\">One important thing to note here is, the garbage collect command is per node so if there are multiple nodes of C* in a cluster, the above steps need to be performed on every node. We have a multi node and multi DC cluster, so the process was definitely time consuming but totally worth it.</p><p id=\"0571\" class=\"graf graf--p graf-after--p\">For the long term, once this massive purge is completed on all key spaces of the cluster we plan to move some of our Column Families to STCS (Size tiered compaction Strategy) as STCS is more suited to our workload which is 50–50 read vs write and the fact that there is going to be a regular purge going forward.</p><p id=\"3815\" class=\"graf graf--p graf-after--p graf--trailing\">I would love to hear about similar problems/experiences that users of Cassandra have had and how they dealt with it.</p></div>",
        "created_at": "2018-09-26T23:13:04+0000",
        "updated_at": "2018-09-26T23:13:40+0000",
        "published_at": "2018-08-24T04:34:18+0000",
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 4,
        "domain_name": "medium.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*GDe4BcPBsIYwyGTQd9ilFg.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12218"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 93,
            "label": "data",
            "slug": "data"
          }
        ],
        "is_public": false,
        "id": 12217,
        "uid": null,
        "title": "Understanding How Cassandra Stores Data",
        "url": "https://www.hakkalabs.co/articles/how-cassandra-stores-data",
        "content": "<p>So far, I've explained why you <a href=\"http://www.hakkalabs.co/articles/shouldnt-use-c\" target=\"_blank\">shouldn't migrate to C*</a> and the <a href=\"http://www.hakkalabs.co/articles/c-cheat-sheet\" target=\"_blank\">origins and key terms</a>. Now, I'm going to turn my attention to how Cassandra stores data.\n<br /></p><p><a href=\"http://g33ktalk.com/wp-content/uploads/2014/04/Screen-Shot-2014-04-30-at-10.41.53.png\"><img class=\"aligncenter  wp-image-1354\" alt=\"Cassandra nodes, clusters, rings\" src=\"http://g33ktalk.com/wp-content/uploads/2014/04/Screen-Shot-2014-04-30-at-10.41.53.png\" width=\"530\" height=\"386\" /></a></p><br />At a very high level, Cassandra operates by dividing all data evenly around a cluster of<strong>nodes</strong>, which can be visualized as a<strong>ring</strong>. Nodes generally run on commodity hardware. Each C* node in the cluster is responsible for and assigned a<strong>token range</strong>(which is essentially a range of hashes defined by a partitioner, which defaults to Murmur3Partitioner in C* v1.2+). By default this hash range is defined with a maximum number of possible hash values ranging from 0 to 2^127-1.<p><a href=\"http://g33ktalk.com/wp-content/uploads/2014/04/Screen-Shot-2014-04-30-at-10.42.07.png\"><img class=\"aligncenter size-full wp-image-1355\" alt=\"Figure 2 The output from the command &#x2018;nodetool status&#x2019;, which is showing the ranges and state information for 3-node cluster using vnodes and Murmur3Partitioner\" src=\"http://g33ktalk.com/wp-content/uploads/2014/04/Screen-Shot-2014-04-30-at-10.42.07.png\" width=\"663\" height=\"148\" /></a></p><p>Each update or addition of data contains a unique <strong>row key</strong> (also known as a <strong>primary key</strong>). The primary key is hashed to determine a replica (or node) responsible for a token range inclusive of a given row key. The data is then stored in the cluster n times (where n is defined by the keyspace’s <strong>replication factor</strong>), or once on each replica responsible a given query’s row key. All nodes in Cassandra are peers and a client’s read or write request can be sent to any node in the cluster, regardless of whether or not that node actually contains and is responsible for the requested data. There is no concept of a master or slave, and nodes dynamically learn about each other and the state and health of other nodes thru the <strong>gossip</strong> protocol. A node that receives a client query is referred to as the <strong>coordinator</strong> for the client operation; it facilitates communication between all replica nodes responsible for the query (contacting at least n replica nodes to satisfy the query’s <strong>consistency level)</strong> and prepares and returns a result to the client.</p><p><a href=\"http://www.dataengconf.com/tickets?utm_source=hakka&amp;utm_medium=content\" target=\"_blank\">\n<img src=\"https://www.hakkalabs.co/images/ads/dataenconfnyc2016-logos3.jpg\" title=\"DataEngConf\" alt=\"image\" /></a>\n<br /></p><p><strong>Reads and Writes</strong>\n<br />Clients may interface with Cassandra for reads and writes via either the <strong>native binary protocol</strong> or <strong>Thrift</strong>. CQL queries can be made over both transports. As a general recommendation, if you are just getting started with Cassandra you should stick to the native binary protocol and CQL and ignore Thrift.</p><p>When a client performs a read or write request, the coordinator node contacts the number of required replicas to satisfy the consistency level included with each request. For example, if a read request is processed using QUORUM consistency, and the keyspace was created with a “replication factor” of 3, 2 of the 3 replicas for the requested data would be contacted, their results merged, and a single result returned to the client. With write requests, the coordinator node will send a write requests with all mutated columns to all replica nodes for a given row key.</p><p><strong>Processing a Local Update</strong>\n<br />When an update is processed – also known as a mutation -- an entry is first added to the <strong>commit log</strong>, which ensures durability of the transaction. Next, it is also added to the memtable. A memtable is a bounded in memory write-back cache that contains recent writes which have not yet been flushed to an SSTable (a permanent, immutable, and serialized on disk copy of the tables data). When updates cause a <strong>memtable</strong> to reach it’s configured maximum in-memory size, the memtable is <strong>flushed</strong> to an immutable<strong> SSTable</strong>, persisting the data from the memtable permanently on disk while making room for future updates. In the event of a crash or node failure, events are replayed from the commit log, which prevents the loss of any data from memtables that had not been flushed to disk prior to an unexpected event such as a power outage or crash.</p><p><strong>CAP Theorem: Why you should know it</strong>\n<br />Finally, before we discuss some Cassandra specific capabilities, features, and limitations, I’d like to introduce an important theorem known as the <strong>CAP theorem</strong>. This is an important theorem to be aware of when first starting with Cassandra and distributed systems in general. Authored by Eric Brewer, the theorem states that it is impossible for a distributed system to provide all three guarantees: 1) Consistency 2) Availability and 3) Partition tolerance simultaneously. So at best, a distributed system will only ever be able to provide 2 of the 3 guarantees. Keep this theorem in mind, as it will help you understand some of Cassandra’s (a distributed database) limitations.</p><p>Stay tuned this week for more posts from Michael Kjellman. This post is an <a title=\"Tired of MySQL?\" href=\"http://planetcassandra.org/mysql-to-cassandra-migration/\">excerpt</a> from '<em>Frustrated with MySQL? Improving the scalability, reliability and performance of your application by migrating from MySQL to Cassandra.</em>' In the meantime, check out our other <a title=\"Cassandra Week\" href=\"http://www.hakkalabs.co/cassandra\" target=\"_blank\">Cassandra Week</a> posts.</p>",
        "created_at": "2018-09-26T23:11:32+0000",
        "updated_at": "2018-10-10T17:44:12+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 3,
        "domain_name": "www.hakkalabs.co",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12217"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 93,
            "label": "data",
            "slug": "data"
          }
        ],
        "is_public": false,
        "id": 12205,
        "uid": null,
        "title": "Apache Drill Contribution Ideas - Apache Drill",
        "url": "https://drill.apache.org/docs/apache-drill-contribution-ideas/",
        "content": "<ul><li>Fixing JIRAs</li>\n<li>SQL functions </li>\n<li>Support for new file format readers/writers</li>\n<li>Support for new data sources</li>\n<li>New query language parsers</li>\n<li>Application interfaces\n<ul><li>BI Tool testing</li>\n</ul></li>\n<li>General CLI improvements </li>\n<li>Eco system integrations\n<ul><li>MapReduce</li>\n<li>Hive views</li>\n<li>YARN</li>\n<li>Spark</li>\n<li>Hue</li>\n<li>Phoenix</li>\n</ul></li>\n</ul><h2 id=\"fixing-jiras\">Fixing JIRAs</h2><p>This is a good place to begin if you are new to Drill. Feel free to pick\nissues from the Drill JIRA list. When you pick an issue, assign it to\nyourself, inform the team, and start fixing it.</p><p>For any questions, seek help from the team through the <a href=\"http://drill.apache.org/community/#mailinglists\">mailing list</a>.</p><p><a href=\"https://issues.apache.org/jira/browse/DRILL/?selectedTab=com.atlassian.jira%0A.jira-projects-plugin:summary-panel\">https://issues.apache.org/jira/browse/DRILL/?selectedTab=com.atlassian.jira\n.jira-projects-plugin:summary-panel</a></p><p>One of the next simple places to start is to implement a DrillFunc. DrillFuncs\nis way that Drill express all scalar functions (UDF or system).  First you can\nput together a JIRA for one of the DrillFunc's we don't yet have but should\n(referencing the capabilities of something like Postgres or SQL Server or your\nown use case). Then try to implement one.</p><p>One example DrillFunc:<br /><a href=\"https://github.com/apache/drill/blob/3f93454f014196a4da198ce012b605b70081fde0/exec/java-exec/src/main/codegen/templates/ComparisonFunctions.java\">ComparisonFunctions.java</a></p><hr /><p><strong>Additional ideas on functions that can be added to SQL support</strong></p><ul><li>Madlib integration</li>\n<li>Machine learning functions</li>\n<li>Approximate aggregate functions (such as what is available in BlinkDB)</li>\n</ul><h2 id=\"support-for-new-file-format-readers/writers\">Support for new file format readers/writers</h2><p>Currently Drill supports text, JSON and Parquet file formats natively when\ninteracting with file system. More readers/writers can be introduced by\nimplementing custom storage plugins. Example formats are.</p><ul><li>Sequence</li>\n<li>RC</li>\n<li>ORC</li>\n<li>Protobuf</li>\n<li>XML</li>\n<li>Thrift</li>\n</ul><h2 id=\"support-for-new-data-sources\">Support for new data sources</h2><p>Writing a new file-based storage plugin, such as a JSON or text-based storage plugin, simply involves implementing a couple of interfaces. The JSON storage plugin is a good example. </p><p>You can refer to the github commits to the mongo db and hbase storage plugin for implementation details: </p><ul><li><a href=\"https://github.com/apache/drill/commit/2ca9c907bff639e08a561eac32e0acab3a0b3304\">mongodb_storage_plugin</a></li>\n<li><a href=\"https://github.com/apache/drill/commit/3651182141b963e24ee48db0530ec3d3b8b6841a\">hbase_storage_plugin</a></li>\n</ul><p>Focus on implementing/extending this list of classes and the corresponding implementations done by Mongo and Hbase. Ignore the mongo db plugin optimizer rules for pushing predicates into the scan.</p><p>Initially, concentrate on basics:</p><ul><li>AbstractGroupScan (MongoGroupScan, HbaseGroupScan)<br /></li>\n<li>SubScan (MongoSubScan, HbaseSubScan)<br /></li>\n<li>RecordReader (MongoRecordReader, HbaseRecordReader)<br /></li>\n<li>BatchCreator (MongoScanBatchCreator, HbaseScanBatchCreator)<br /></li>\n<li>AbstractStoragePlugin (MongoStoragePlugin, HbaseStoragePlugin)<br /></li>\n<li>StoragePluginConfig (MongoStoragePluginConfig, HbaseStoragePluginConfig)</li>\n</ul><p>Implement custom storage plugins for the following non-Hadoop data sources:</p><ul><li>NoSQL databases (such as Mongo, Cassandra, Couch etc)</li>\n<li>Search engines (such as Solr, Lucidworks, Elastic Search etc)</li>\n<li>SQL databases (MySQL&lt; PostGres etc)</li>\n<li>Generic JDBC/ODBC data sources</li>\n<li>HTTP URL</li>\n<li>----</li>\n</ul><h2 id=\"new-query-language-parsers\">New query language parsers</h2><p>Drill exposes strongly typed JSON APIs for logical and physical plans. Drill provides a\nSQL language parser today, but any language parser that can generate\nlogical/physical plans can use Drill's power on the backend as the distributed\nlow latency query execution engine along with its support for self-describing\ndata and complex/multi-structured data.</p><ul><li>Pig parser : Use Pig as the language to query data from Drill. Great for existing Pig users.</li>\n<li>Hive parser : Use HiveQL as the language to query data from Drill. Great for existing Hive users.</li>\n</ul><h2 id=\"application-interfaces\">Application interfaces</h2><p>Drill currently provides JDBC/ODBC drivers for the applications to interact\nalong with a basic version of REST API and a C++ API. The following list\nprovides a few possible application interface opportunities:</p><ul><li>Enhancements to REST APIs (<a href=\"https://issues.apache.org/jira/browse/DRILL-77\">https://issues.apache.org/jira/browse/DRILL-77</a>)</li>\n<li>Expose Drill tables/views as REST APIs</li>\n<li>Language drivers for Drill (python etc)</li>\n<li>Thrift support</li>\n<li>....</li>\n</ul><p>Drill provides JDBC/ODBC drivers to connect to BI tools. We need to make sure\nDrill works with all major BI tools. Doing a quick sanity testing with your\nfavorite BI tool is a good place to learn Drill and also uncover issues in\nbeing able to do so.</p><h2 id=\"general-cli-improvements\">General CLI improvements</h2><p>Currently Drill uses SQLLine as the CLI. The goal of this effort is to improve\nthe CLI experience by adding functionality such as execute statements from a\nfile, output results to a file, display version information, and so on.</p><h2 id=\"eco-system-integrations\">Eco system integrations</h2><h3 id=\"mapreduce\">MapReduce</h3><p>Allow using result set from Drill queries as input to the Hadoop/MapReduce\njobs.</p><h3 id=\"hive-views\">Hive views</h3><p>Query data from existing Hive views using Drill queries. Drill needs to parse\nthe HiveQL and translate them appropriately (into Drill's SQL or\nlogical/physical plans) to execute the requests.</p><h3 id=\"yarn\">YARN</h3><p><a href=\"https://issues.apache.org%0A/jira/browse/DRILL-1170\">https://issues.apache.org/jira/browse/<em>DRILL</em>-1170</a></p><h2 id=\"spark\">Spark</h2><p>Provide ability to invoke Drill queries as part of Apache Spark programs. This\ngives ability for Spark developers/users to leverage Drill richness of the\nquery layer , for data source access and as low latency execution engine.</p><h3 id=\"hue\">Hue</h3><p>Hue is a GUI for users to interact with various Hadoop eco system components\n(such as Hive, Oozie, Pig, HBase, Impala ...). The goal of this project is to\nexpose Drill as an application inside Hue so users can explore Drill metadata\nand do SQL queries.</p><h3 id=\"phoenix\">Phoenix</h3><p>Phoenix provides a low latency query layer on HBase for operational\napplications. The goal of this effort is to explore opportunities for\nintegrating Phoenix with Drill.</p><p><a href=\"https://drill.apache.org/docs/apache-drill-contribution-guidelines/\">← Apache Drill Contribution Guidelines</a><a href=\"https://drill.apache.org/docs/design-docs/\">Design Docs →</a></p>",
        "created_at": "2018-09-26T23:01:14+0000",
        "updated_at": "2018-09-26T23:01:24+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 4,
        "domain_name": "drill.apache.org",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12205"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12204,
        "uid": null,
        "title": "Apache Cassandra Multi-Datacenter Essentials (Julien Anguenot, iLand …",
        "url": "https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016",
        "content": "Apache Cassandra Multi-Datacenter Essentials (Julien Anguenot, iLand …\n      \n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            </a><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div>\n    <div class=\"wrapper\"><p>Successfully reported this slideshow.</p><div id=\"slideview-container\" class=\"\"><div class=\"row\"><div id=\"main-panel\" class=\"small-12 large-8 columns\"><div class=\"sectionElements\"><div class=\"playerWrapper\"><div><div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\">Apache Cassandra Multi-Datacenter Essentials (Julien Anguenot, iLand Internet Solutions) | C* Summit 2016<div class=\"stage valign-first-slide\"><div class=\"slide_container\"><section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-1-638.jpg?cb=1474336642\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-1-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-1-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-1-1024.jpg?cb=1474336642\" alt=\"Apache Cassandra multi-dc essentials&#10;Julien Anguenot (@anguenot)&#10;VP Software Engineering, iland cloud&#10;\" /></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-2-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-2-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-2-1024.jpg?cb=1474336642\" alt=\"1 key notions &amp; configuration&#10;2 bootstrapping &amp; decommissioning new DCs / nodes&#10;3 operations pain points&#10;4 q&amp;a&#10;2&#10;\" /></i></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-3-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-3-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-3-1024.jpg?cb=1474336642\" alt=\"iland cloud?&#10;• public / private cloud provider&#10;• footprint in U.S., EU and Asia&#10;• compliance, advanced security&#10;• custom S...\" /></i></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-4-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-4-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-4-1024.jpg?cb=1474336642\" alt=\"key notions &amp; configuration&#10;\" /></i></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-5-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-5-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-5-1024.jpg?cb=1474336642\" alt=\"What is Apache Cassandra?&#10;• distributed partitioned row store&#10;• physical multi-datacenter native support&#10;• tailored (featu...\" /></i></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-6-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-6-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-6-1024.jpg?cb=1474336642\" alt=\"Why multi-dc deployments?&#10;• multi-datacenter distributed application&#10;• performances&#10;read / write isolation or geographical...\" /></i></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-7-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-7-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-7-1024.jpg?cb=1474336642\" alt=\"Essentially…&#10;• sequential writes in commit log (flat files)&#10;• indexed and written in memtables (in-memory: write-back&#10;cach...\" /></i></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-8-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-8-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-8-1024.jpg?cb=1474336642\" alt=\"Cassandra hierarchy of elements&#10;8&#10;cluster&#10;datacenter(s)&#10;rack(s)&#10;server(s)&#10;Vnode(s)&#10;\" /></i></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-9-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-9-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-9-1024.jpg?cb=1474336642\" alt=\"Cassandra cluster&#10;• the sum total of all the servers in your database&#10;throughout all datacenters&#10;• span physical locations...\" /></i></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-10-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-10-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-10-1024.jpg?cb=1474336642\" alt=\"cassandra.yaml: `cluster_name`&#10;# The name of the cluster. This is mainly used to prevent machines in&#10;# one logical cluster...\" /></i></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-11-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-11-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-11-1024.jpg?cb=1474336642\" alt=\"Cassandra datacenter&#10;• grouping of nodes&#10;• synonymous with replication group&#10;• a grouping of nodes configured together for...\" /></i></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-12-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-12-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-12-1024.jpg?cb=1474336642\" alt=\"Cassandra rack&#10;• collection of servers&#10;• at least one (1) rack per datacenter&#10;• one (1) rack is the most simple and common...\" /></i></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-13-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-13-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-13-1024.jpg?cb=1474336642\" alt=\"Cassandra server&#10;• Cassandra (the software) instance installed on a&#10;machine&#10;• AKA node&#10;• contains virtual nodes (or Vnodes...\" /></i></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-14-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-14-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-14-1024.jpg?cb=1474336642\" alt=\"Virtual nodes (Vnodes)&#10;• C* &gt;= 1.2&#10;• data storage layer within a server&#10;• tokens automatically calculated and assigned ran...\" /></i></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-15-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-15-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-15-1024.jpg?cb=1474336642\" alt=\"cassandra.yaml: `num_tokens`&#10;# This defines the number of tokens randomly assigned to this node on the ring&#10;# The more tok...\" /></i></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-16-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-16-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-16-1024.jpg?cb=1474336642\" alt=\"Ring with Vnodes&#10;16&#10;\" /></i></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-17-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-17-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-17-1024.jpg?cb=1474336642\" alt=\"Partition&#10;• individual unit of data&#10;• partitions are replicated across multiple Vnodes&#10;• each copy of the partition is cal...\" /></i></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-18-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-18-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-18-1024.jpg?cb=1474336642\" alt=\"Vnodes and consistent hashing&#10;• allows distribution of data across a cluster&#10;• Cassandra assigns a hash value to each part...\" /></i></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-19-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-19-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-19-1024.jpg?cb=1474336642\" alt=\"Partitioner (1/2)&#10;• partitions the data across the cluster&#10;• function for deriving a token representing a row&#10;from its par...\" /></i></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-20-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-20-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-20-1024.jpg?cb=1474336642\" alt=\"Partitioner (2/2)&#10;• Murmur3Partitioner (default C* &gt;= 1.2) &#10;uniformly distributes data across the cluster based on&#10;MurmurH...\" /></i></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-21-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-21-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-21-1024.jpg?cb=1474336642\" alt=\"cassandra.yaml: `partitioner`&#10;# The partitioner is responsible for distributing groups of rows (by&#10;# partition key) across...\" /></i></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-22-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-22-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-22-1024.jpg?cb=1474336642\" alt=\"Partitioner example (1/4)&#10;22&#10;Credits to Datastax. Extract from documentation.&#10;\" /></i></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-23-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-23-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-23-1024.jpg?cb=1474336642\" alt=\"Partitioner example (2/4)&#10;23&#10;Credits to Datastax. Extract from documentation.&#10;\" /></i></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-24-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-24-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-24-1024.jpg?cb=1474336642\" alt=\"Partitioner example (3/4)&#10;24&#10;Credits to Datastax. Extract from documentation.&#10;\" /></i></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-25-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-25-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-25-1024.jpg?cb=1474336642\" alt=\"Partitioner example (4/4)&#10;25&#10;Credits to Datastax. Extract from documentation.&#10;\" /></i></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-26-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-26-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-26-1024.jpg?cb=1474336642\" alt=\"Cassandra hierarchy of elements (recap)&#10;26&#10;cluster&#10;datacenter(s)&#10;rack(s)&#10;server(s)&#10;Vnode(s)&#10;\" /></i></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-27-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-27-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-27-1024.jpg?cb=1474336642\" alt=\"Cassandra Keyspace (KS)&#10;• namespace container that defines how data is&#10;replicated on nodes&#10;• cluster defines KS&#10;• contains...\" /></i></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-28-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-28-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-28-1024.jpg?cb=1474336642\" alt=\"Data replication&#10;• process of storing copies (replicas) on multiple nodes&#10;• KS has a replication factor (RF) and replica p...\" /></i></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-29-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-29-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-29-1024.jpg?cb=1474336642\" alt=\"Replica placement strategy&#10;there are two (2) available replication strategies:&#10;1. SimpleStrategy (single DC)&#10;2. NetworkTop...\" /></i></section><section data-index=\"30\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-30-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-30-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-30-1024.jpg?cb=1474336642\" alt=\"Consistency Level (CL)&#10;• how many nodes must ACK operation at client level?&#10;• tunable consistency at client level&#10;• ANY&#10;• ...\" /></i></section><section data-index=\"31\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-31-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-31-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-31-1024.jpg?cb=1474336642\" alt=\"local_quorum examples&#10;• nodes=3, RF=3 - can tolerate 1 replica being down&#10;• nodes=5, RF=3 - can tolerate 2 replica being d...\" /></i></section><section data-index=\"32\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-32-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-32-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-32-1024.jpg?cb=1474336642\" alt=\"snitch (1/3)&#10;• determines which data centers &amp; racks nodes belong&#10;to&#10;• informs Cassandra about the network topology&#10;• effe...\" /></i></section><section data-index=\"33\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-33-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-33-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-33-1024.jpg?cb=1474336642\" alt=\"snitch (2/3)&#10;• SimpleSnitch &#10;single DC only&#10;• GossipingPropertySnitch &#10;cassandra-rackdc.properties&#10;• PropertyFileSnitch &#10;c...\" /></i></section><section data-index=\"34\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-34-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-34-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-34-1024.jpg?cb=1474336642\" alt=\"Snitch files (examples)&#10;These properties are used with&#10;GossipingPropertyFileSnitch and will&#10;indicate the rack and dc for t...\" /></i></section><section data-index=\"35\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-35-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-35-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-35-1024.jpg?cb=1474336642\" alt=\"snitch (3/3)&#10;• more deployment specific snitches for EC2, Google,&#10;Cloudstack etc.&#10;35&#10;\" /></i></section><section data-index=\"36\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-36-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-36-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-36-1024.jpg?cb=1474336642\" alt=\"cassandra.yaml: `endpoint_snitch`&#10;# You can use a custom Snitch by setting this to the full class name&#10;# of the snitch, wh...\" /></i></section><section data-index=\"37\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-37-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-37-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-37-1024.jpg?cb=1474336642\" alt=\"seed node&#10;• bootstrapping the gossip process for new nodes&#10;joining the cluster&#10;• use the same list of seed nodes for all n...\" /></i></section><section data-index=\"38\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-38-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-38-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-38-1024.jpg?cb=1474336642\" alt=\"cassandra.yaml: `seed_provider`&#10;# any class that implements the SeedProvider interface and has a&#10;# constructor that takes ...\" /></i></section><section data-index=\"39\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-39-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-39-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-39-1024.jpg?cb=1474336642\" alt=\"Gossip&#10;• peer-to-peer communication protocol&#10;• discover and share location and state information&#10;about the other nodes in ...\" /></i></section><section data-index=\"40\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-40-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-40-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-40-1024.jpg?cb=1474336642\" alt=\"operations pain points&#10;\" /></i></section><section data-index=\"41\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-41-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-41-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-41-1024.jpg?cb=1474336642\" alt=\"operations pain points&#10;• bootstrapping new nodes / new DCs&#10;• repairs&#10;• hints&#10;• tombstones&#10;• compactions&#10;• indexes&#10;• sick n...\" /></i></section><section data-index=\"42\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-42-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-42-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-42-1024.jpg?cb=1474336642\" alt=\"bootstrapping new nodes / new DCs&#10;• slow if dense nodes&#10;• don’t expect to “just add a new node” to accommodate&#10;load as it ...\" /></i></section><section data-index=\"43\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-43-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-43-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-43-1024.jpg?cb=1474336642\" alt=\"43&#10;# Throttles all outbound streaming ﬁle transfers on this node to the&#10;# given total throughput in Mbps. This is necessar...\" /></i></section><section data-index=\"44\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-44-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-44-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-44-1024.jpg?cb=1474336642\" alt=\"44&#10;# Set socket timeout for streaming operation.&#10;# The stream session is failed if no data/ack is received by any of the p...\" /></i></section><section data-index=\"45\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-45-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-45-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-45-1024.jpg?cb=1474336642\" alt=\"repairs&#10;• Anti-Entropy: QUORUM &amp; ALL replicas compared for&#10;CF and discrepancies fixed.&#10;• must run before `gc_grace_period`...\" /></i></section><section data-index=\"46\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-46-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-46-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-46-1024.jpg?cb=1474336642\" alt=\"repairs: what to do then?&#10;• you must define a repair strategy from the beginning&#10;• custom tooling&#10;• DSE and OpsCenter&#10;• Sp...\" /></i></section><section data-index=\"47\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-47-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-47-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-47-1024.jpg?cb=1474336642\" alt=\"hints&#10;• if node down: spool and redelivery&#10;• slow and broken until 3.0: must truncate manually as some&#10;are left off&#10;• &lt; 3....\" /></i></section><section data-index=\"48\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-48-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-48-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-48-1024.jpg?cb=1474336642\" alt=\"cassandra.yaml: `hints`&#10;max_hints_delivery_threads: 2&#10;# Directory where Cassandra should store hints.&#10;# If not set, the de...\" /></i></section><section data-index=\"49\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-49-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-49-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-49-1024.jpg?cb=1474336642\" alt=\"compactions&#10;• process of merging SSTables to single files&#10;• IO heavy: GC / CPU / eat disk space&#10;• removes tombstones&#10;• hig...\" /></i></section><section data-index=\"50\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-50-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-50-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-50-1024.jpg?cb=1474336642\" alt=\"cassandra.yaml: `concurrent_compactors`&#10;[…]&#10;concurrent_compactors: 2&#10;[…]&#10;50&#10;\" /></i></section><section data-index=\"51\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-51-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-51-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-51-1024.jpg?cb=1474336642\" alt=\"tombstones&#10;• monitor for tombstones warnings&#10;• maintenance ops issue or application level issue?&#10;51&#10;\" /></i></section><section data-index=\"52\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-52-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-52-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-52-1024.jpg?cb=1474336642\" alt=\"cassandra.yaml: `tombstone_warn_threshold`&#10;# When executing a scan, within or across a partition, we need to keep the&#10;# to...\" /></i></section><section data-index=\"53\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-53-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-53-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-53-1024.jpg?cb=1474336642\" alt=\"indexes&#10;• secondary indexes? SASI?&#10;• if looking for search:&#10;• use DSE and its integrated search&#10;• check https://github.com...\" /></i></section><section data-index=\"54\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-54-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-54-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-54-1024.jpg?cb=1474336642\" alt=\"sick nodes&#10;• what is the issue?&#10;• remove “sick nodes” from the ring when it happens&#10;54&#10;\" /></i></section><section data-index=\"55\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-55-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-55-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-55-1024.jpg?cb=1474336642\" alt=\"must reads&#10;• Datastax Apache Cassandra documentation&#10;• http://docs.datastax.com/en//cassandra/3.0/cassandra/&#10;cassandraAbou...\" /></i></section><section data-index=\"56\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/85/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-56-320.jpg?cb=1474336642\" data-normal=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-56-638.jpg?cb=1474336642\" data-full=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-56-1024.jpg?cb=1474336642\" alt=\"Q&amp;A&#10;@anguenot&#10;http://www.slideshare.net/anguenot/&#10;\" /></i></section><div class=\"j-next-container next-container\"><div class=\"content-container\"><div class=\"next-slideshow-wrapper\"><div class=\"j-next-slideshow next-slideshow\"><p>Upcoming SlideShare</p></div><p>Loading in …5</p><p>×</p></div></div></div></div></div></div></div></div></div><div class=\"slideshow-info-container\" itemscope=\"itemscope\" itemtype=\"https://schema.org/MediaObject\"><div class=\"slideshow-tabs-container show-for-medium-up\"><ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    0 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\">\n                \n                  4 Likes\n                \n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\">\n                Statistics\n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\">\n                Notes\n              </i></a>\n            </li>\n          </ul><div class=\"tabs-content\"><div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\"><ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"itemscope\"><li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"JieYao11\" rel=\"nofollow\" href=\"https://www.slideshare.net/JieYao11?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Jie Yao\n                            \n                              \n                                , \n                                高级架构师 at 唯品会\n                              \n                              \n                                 at \n                                唯品会\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"tashish786\" rel=\"nofollow\" href=\"https://www.slideshare.net/tashish786?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Ashish Tiwari\n                            \n                              \n                                , \n                                Sr. Architect (Big Data,IoT,AI)\n                              \n                              \n                                 at \n                                Manthan\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"timmersthomas\" rel=\"nofollow\" href=\"https://www.slideshare.net/timmersthomas?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Thomas Timmers\n                            \n                              \n                                \n                                \n                              \n                              \n                                \n                                \n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"justinfreitag\" rel=\"nofollow\" href=\"https://www.slideshare.net/justinfreitag?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Justin Freitag\n                            \n                              \n                                , \n                                Technical Consultant at Arthrolife, Melbourne Stem Cell Centre\n                              \n                              \n                                 at \n                                Arthrolife, Melbourne Stem Cell Centre\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n              </ul></div><div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No Downloads</p></div><div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No notes for slide</p></div></div></div><div class=\"notranslate transcript add-padding-right j-transcript\"><ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    Apache Cassandra multi-dc essentials\nJulien Anguenot (@anguenot)\nVP Software Engineering, iland cloud\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-2-638.jpg?cb=1474336642\" title=\"1 key notions &amp; configuration&#10;2 bootstrapping &amp; decommissio...\" target=\"_blank\">\n        2.\n      </a>\n    1 key notions &amp; configuration\n2 bootstrapping &amp; decommissioning new DCs / nodes\n3 operations pain points\n4 q&amp;a\n2\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-3-638.jpg?cb=1474336642\" title=\"iland cloud?&#10;• public / private cloud provider&#10;• footprint ...\" target=\"_blank\">\n        3.\n      </a>\n    iland cloud?\n• public / private cloud provider\n• footprint in U.S., EU and Asia\n• compliance, advanced security\n• custom SLA\n• Apache Cassandra users since 1.2\nC* summit 2015 presentation: http://\nwww.slideshare.net/anguenot/leveraging-\ncassandra-for-realtime-multidatacenter-\npublic-cloud-analytics\nApache Cassandra spanning 6\ndatacenters.\n• http://www.iland.com\n3\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-4-638.jpg?cb=1474336642\" title=\"key notions &amp; configuration&#10;\" target=\"_blank\">\n        4.\n      </a>\n    key notions &amp; configuration\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-5-638.jpg?cb=1474336642\" title=\"What is Apache Cassandra?&#10;• distributed partitioned row sto...\" target=\"_blank\">\n        5.\n      </a>\n    What is Apache Cassandra?\n• distributed partitioned row store\n• physical multi-datacenter native support\n• tailored (features) for multi-datacenter deployment\n5\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-6-638.jpg?cb=1474336642\" title=\"Why multi-dc deployments?&#10;• multi-datacenter distributed ap...\" target=\"_blank\">\n        6.\n      </a>\n    Why multi-dc deployments?\n• multi-datacenter distributed application\n• performances\nread / write isolation or geographical distribution\n• disaster recovery (DR)\nfailover and redundancy\n• analytics\n6\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-7-638.jpg?cb=1474336642\" title=\"Essentially…&#10;• sequential writes in commit log (flat files)...\" target=\"_blank\">\n        7.\n      </a>\n    Essentially…\n• sequential writes in commit log (flat files)\n• indexed and written in memtables (in-memory: write-back\ncache)\n• serialized to disk in a SSTable data file\n• writes are partitioned and replicated automatically in cluster\n• SSTables consolidated though compaction to clean tombstones\n• repairs to ensure consistency cluster wide\n7\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-8-638.jpg?cb=1474336642\" title=\"Cassandra hierarchy of elements&#10;8&#10;cluster&#10;datacenter(s)&#10;rac...\" target=\"_blank\">\n        8.\n      </a>\n    Cassandra hierarchy of elements\n8\ncluster\ndatacenter(s)\nrack(s)\nserver(s)\nVnode(s)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-9-638.jpg?cb=1474336642\" title=\"Cassandra cluster&#10;• the sum total of all the servers in you...\" target=\"_blank\">\n        9.\n      </a>\n    Cassandra cluster\n• the sum total of all the servers in your database\nthroughout all datacenters\n• span physical locations\n• defines one or more keyspaces\n• no cross-cluster replication\n9\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-10-638.jpg?cb=1474336642\" title=\"cassandra.yaml: `cluster_name`&#10;# The name of the cluster. T...\" target=\"_blank\">\n        10.\n      </a>\n    cassandra.yaml: `cluster_name`\n# The name of the cluster. This is mainly used to prevent machines in\n# one logical cluster from joining another.\ncluster_name: ‘my little cluster'\n10\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-11-638.jpg?cb=1474336642\" title=\"Cassandra datacenter&#10;• grouping of nodes&#10;• synonymous with ...\" target=\"_blank\">\n        11.\n      </a>\n    Cassandra datacenter\n• grouping of nodes\n• synonymous with replication group\n• a grouping of nodes configured together for\nreplication purposes\n• each datacenter contains a complete token ring\n• collection of Cassandra racks\n11\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-12-638.jpg?cb=1474336642\" title=\"Cassandra rack&#10;• collection of servers&#10;• at least one (1) r...\" target=\"_blank\">\n        12.\n      </a>\n    Cassandra rack\n• collection of servers\n• at least one (1) rack per datacenter\n• one (1) rack is the most simple and common setup\n12\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-13-638.jpg?cb=1474336642\" title=\"Cassandra server&#10;• Cassandra (the software) instance instal...\" target=\"_blank\">\n        13.\n      </a>\n    Cassandra server\n• Cassandra (the software) instance installed on a\nmachine\n• AKA node\n• contains virtual nodes (or Vnodes). 256 by default\n13\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-14-638.jpg?cb=1474336642\" title=\"Virtual nodes (Vnodes)&#10;• C* &gt;= 1.2&#10;• data storage layer wit...\" target=\"_blank\">\n        14.\n      </a>\n    Virtual nodes (Vnodes)\n• C* &gt;= 1.2\n• data storage layer within a server\n• tokens automatically calculated and assigned randomly\nfor all Vnodes\n• automatic rebalancing\n• no manual token generation and assignment\n• default to 256 (num_tokens in cassandra.yaml)\n14\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-15-638.jpg?cb=1474336642\" title=\"cassandra.yaml: `num_tokens`&#10;# This defines the number of t...\" target=\"_blank\">\n        15.\n      </a>\n    cassandra.yaml: `num_tokens`\n# This defines the number of tokens randomly assigned to this node on the ring\n# The more tokens, relative to other nodes, the larger the proportion of data\n# that this node will store. You probably want all nodes to have the same number\n# of tokens assuming they have equal hardware capability.\n#\n# If you leave this unspecified, Cassandra will use the default of 1 token for\nlegacy compatibility,\n# and will use the initial_token as described below.\n#\n# Specifying initial_token will override this setting on the node's initial start,\n# on subsequent starts, this setting will apply even if initial token is set.\n#\n# If you already have a cluster with 1 token per node, and wish to migrate to\n# multiple tokens per node, see http://wiki.apache.org/cassandra/Operations\nnum_tokens: 256\n15\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-16-638.jpg?cb=1474336642\" title=\"Ring with Vnodes&#10;16&#10;\" target=\"_blank\">\n        16.\n      </a>\n    Ring with Vnodes\n16\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-17-638.jpg?cb=1474336642\" title=\"Partition&#10;• individual unit of data&#10;• partitions are replic...\" target=\"_blank\">\n        17.\n      </a>\n    Partition\n• individual unit of data\n• partitions are replicated across multiple Vnodes\n• each copy of the partition is called a replica\n17\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-18-638.jpg?cb=1474336642\" title=\"Vnodes and consistent hashing&#10;• allows distribution of data...\" target=\"_blank\">\n        18.\n      </a>\n    Vnodes and consistent hashing\n• allows distribution of data across a cluster\n• Cassandra assigns a hash value to each partition key\n• each Vnode in the cluster is responsible for a range of\ndata based on the hash value\n• Cassandra places the data on each node according to\nthe value of the partition key and the range that the\nnode is responsible for\n18\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-19-638.jpg?cb=1474336642\" title=\"Partitioner (1/2)&#10;• partitions the data across the cluster&#10;...\" target=\"_blank\">\n        19.\n      </a>\n    Partitioner (1/2)\n• partitions the data across the cluster\n• function for deriving a token representing a row\nfrom its partition key\n• hashing function\n• each row of data is then distributed across the cluster\nby the value of the token\n19\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-20-638.jpg?cb=1474336642\" title=\"Partitioner (2/2)&#10;• Murmur3Partitioner (default C* &gt;= 1.2) ...\" target=\"_blank\">\n        20.\n      </a>\n    Partitioner (2/2)\n• Murmur3Partitioner (default C* &gt;= 1.2) \nuniformly distributes data across the cluster based on\nMurmurHash hash values\n• RandomPartitioner (default C* &lt; 1.2) \nuniformly distributes data across the cluster based on MD5\nhash values\n• ByteOrderedPartitioner (BBB) \nkeeps an ordered distribution of data lexically by key bytes\n20\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-21-638.jpg?cb=1474336642\" title=\"cassandra.yaml: `partitioner`&#10;# The partitioner is responsi...\" target=\"_blank\">\n        21.\n      </a>\n    cassandra.yaml: `partitioner`\n# The partitioner is responsible for distributing groups of rows (by\n# partition key) across nodes in the cluster. You should leave this\n# alone for new clusters. The partitioner can NOT be changed without\n# reloading all data, so when upgrading you should set this to the\n# same partitioner you were already using.\n#\n# Besides Murmur3Partitioner, partitioners included for backwards\n# compatibility include RandomPartitioner, ByteOrderedPartitioner, and\n# OrderPreservingPartitioner.\n#\npartitioner: org.apache.cassandra.dht.Murmur3Partitioner\n21\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-22-638.jpg?cb=1474336642\" title=\"Partitioner example (1/4)&#10;22&#10;Credits to Datastax. Extract f...\" target=\"_blank\">\n        22.\n      </a>\n    Partitioner example (1/4)\n22\nCredits to Datastax. Extract from documentation.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-23-638.jpg?cb=1474336642\" title=\"Partitioner example (2/4)&#10;23&#10;Credits to Datastax. Extract f...\" target=\"_blank\">\n        23.\n      </a>\n    Partitioner example (2/4)\n23\nCredits to Datastax. Extract from documentation.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-24-638.jpg?cb=1474336642\" title=\"Partitioner example (3/4)&#10;24&#10;Credits to Datastax. Extract f...\" target=\"_blank\">\n        24.\n      </a>\n    Partitioner example (3/4)\n24\nCredits to Datastax. Extract from documentation.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-25-638.jpg?cb=1474336642\" title=\"Partitioner example (4/4)&#10;25&#10;Credits to Datastax. Extract f...\" target=\"_blank\">\n        25.\n      </a>\n    Partitioner example (4/4)\n25\nCredits to Datastax. Extract from documentation.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-26-638.jpg?cb=1474336642\" title=\"Cassandra hierarchy of elements (recap)&#10;26&#10;cluster&#10;datacent...\" target=\"_blank\">\n        26.\n      </a>\n    Cassandra hierarchy of elements (recap)\n26\ncluster\ndatacenter(s)\nrack(s)\nserver(s)\nVnode(s)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-27-638.jpg?cb=1474336642\" title=\"Cassandra Keyspace (KS)&#10;• namespace container that defines ...\" target=\"_blank\">\n        27.\n      </a>\n    Cassandra Keyspace (KS)\n• namespace container that defines how data is\nreplicated on nodes\n• cluster defines KS\n• contains tables\n• defines the replica placement strategy and the\nnumber of replicas\n27\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-28-638.jpg?cb=1474336642\" title=\"Data replication&#10;• process of storing copies (replicas) on ...\" target=\"_blank\">\n        28.\n      </a>\n    Data replication\n• process of storing copies (replicas) on multiple nodes\n• KS has a replication factor (RF) and replica placement\nstrategy\n• max (RF) = max(number of nodes) in one (1) data\ncenter\n• data replication is defined per datacenter\n28\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-29-638.jpg?cb=1474336642\" title=\"Replica placement strategy&#10;there are two (2) available repl...\" target=\"_blank\">\n        29.\n      </a>\n    Replica placement strategy\nthere are two (2) available replication strategies:\n1. SimpleStrategy (single DC)\n2. NetworkTopologyStrategy (recommended cause easier to\nexpand) \nchoose strategy depending on failure scenarios and application\nneeds for consistency level\n29\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-30-638.jpg?cb=1474336642\" title=\"Consistency Level (CL)&#10;• how many nodes must ACK operation ...\" target=\"_blank\">\n        30.\n      </a>\n    Consistency Level (CL)\n• how many nodes must ACK operation at client level?\n• tunable consistency at client level\n• ANY\n• ONE\n• ALL\n• QUORUM / LOCAL_QUORUM (DC only)\n• SERIAL and conditional updates (IF DOES NOT EXIST)\n30\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-31-638.jpg?cb=1474336642\" title=\"local_quorum examples&#10;• nodes=3, RF=3 - can tolerate 1 repl...\" target=\"_blank\">\n        31.\n      </a>\n    local_quorum examples\n• nodes=3, RF=3 - can tolerate 1 replica being down\n• nodes=5, RF=3 - can tolerate 2 replica being down\n• etc.\n31\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-32-638.jpg?cb=1474336642\" title=\"snitch (1/3)&#10;• determines which data centers &amp; racks nodes ...\" target=\"_blank\">\n        32.\n      </a>\n    snitch (1/3)\n• determines which data centers &amp; racks nodes belong\nto\n• informs Cassandra about the network topology\n• effective routing\n• replication strategy places the replicas based on\nsnitch\n32\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-33-638.jpg?cb=1474336642\" title=\"snitch (2/3)&#10;• SimpleSnitch &#10;single DC only&#10;• GossipingProp...\" target=\"_blank\">\n        33.\n      </a>\n    snitch (2/3)\n• SimpleSnitch \nsingle DC only\n• GossipingPropertySnitch \ncassandra-rackdc.properties\n• PropertyFileSnitch \ncassandra-topology.properties\n• RackInferringSnitch \ndetermined by rack and data center, which are 3rd and 2nd octet of\neach node’s IP respectively\n33\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-34-638.jpg?cb=1474336642\" title=\"Snitch files (examples)&#10;These properties are used with&#10;Goss...\" target=\"_blank\">\n        34.\n      </a>\n    Snitch files (examples)\nThese properties are used with\nGossipingPropertyFileSnitch and will\nindicate the rack and dc for this\nindividual node only\ndc=west-dc\nrack=rack1\n34\n# These properties are used with PropertyFileSnitch\nand will be identical on every nodes.\n# Cassandra Node IP=Data Center:Rack\n192.168.1.100=east-dc:rack1\n192.168.1.101=east-dc:rack1\n192.168.1.102=east-dc:rack1\n192.168.2.100=west-dc:rack1\n192.168.2.101=west-dc:rack1\n192.168.2.102=west-dc:rack1\ncassandra-rackdc.properties cassandra-topology.properties\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-35-638.jpg?cb=1474336642\" title=\"snitch (3/3)&#10;• more deployment specific snitches for EC2, G...\" target=\"_blank\">\n        35.\n      </a>\n    snitch (3/3)\n• more deployment specific snitches for EC2, Google,\nCloudstack etc.\n35\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-36-638.jpg?cb=1474336642\" title=\"cassandra.yaml: `endpoint_snitch`&#10;# You can use a custom Sn...\" target=\"_blank\">\n        36.\n      </a>\n    cassandra.yaml: `endpoint_snitch`\n# You can use a custom Snitch by setting this to the full class name\n# of the snitch, which will be assumed to be on your classpath.\nendpoint_snitch: RackInferringSnitch\n36\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-37-638.jpg?cb=1474336642\" title=\"seed node&#10;• bootstrapping the gossip process for new nodes&#10;...\" target=\"_blank\">\n        37.\n      </a>\n    seed node\n• bootstrapping the gossip process for new nodes\njoining the cluster\n• use the same list of seed nodes for all nodes in a\ncluster\n• include at least one (1) node of each datacenter in\nseeds list\n37\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-38-638.jpg?cb=1474336642\" title=\"cassandra.yaml: `seed_provider`&#10;# any class that implements...\" target=\"_blank\">\n        38.\n      </a>\n    cassandra.yaml: `seed_provider`\n# any class that implements the SeedProvider interface and has a\n# constructor that takes a Map&lt;String, String&gt; of parameters will do.\nseed_provider:\n# Addresses of hosts that are deemed contact points.\n# Cassandra nodes use this list of hosts to find each other and learn\n# the topology of the ring. You must change this if you are running\n# multiple nodes!\n- class_name: org.apache.cassandra.locator.SimpleSeedProvider\nparameters:\n# seeds is actually a comma-delimited list of addresses.\n# Ex: \"&lt;ip1&gt;,&lt;ip2&gt;,&lt;ip3&gt;\"\n- seeds: “10.239.206.80,10.239.206.81,10.244.206.80,10.244.206.81\"\n38\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-39-638.jpg?cb=1474336642\" title=\"Gossip&#10;• peer-to-peer communication protocol&#10;• discover and...\" target=\"_blank\">\n        39.\n      </a>\n    Gossip\n• peer-to-peer communication protocol\n• discover and share location and state information\nabout the other nodes in a Cassandra cluster\n• persisted by each node\n• nodes exchange state messages on regular basis\n39\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-40-638.jpg?cb=1474336642\" title=\"operations pain points&#10;\" target=\"_blank\">\n        40.\n      </a>\n    operations pain points\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-41-638.jpg?cb=1474336642\" title=\"operations pain points&#10;• bootstrapping new nodes / new DCs&#10;...\" target=\"_blank\">\n        41.\n      </a>\n    operations pain points\n• bootstrapping new nodes / new DCs\n• repairs\n• hints\n• tombstones\n• compactions\n• indexes\n• sick nodes\n41\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-42-638.jpg?cb=1474336642\" title=\"bootstrapping new nodes / new DCs&#10;• slow if dense nodes&#10;• d...\" target=\"_blank\">\n        42.\n      </a>\n    bootstrapping new nodes / new DCs\n• slow if dense nodes\n• don’t expect to “just add a new node” to accommodate\nload as it comes.\n• use C* &gt;= 2.2 (nodetool bootstrap resume)\n• pressure on network\n• first seed node: nodetool rebuild -- &lt;dc&gt; \nstream timeouts / throughput params\n42\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-43-638.jpg?cb=1474336642\" title=\"43&#10;# Throttles all outbound streaming ﬁle transfers on this...\" target=\"_blank\">\n        43.\n      </a>\n    43\n# Throttles all outbound streaming ﬁle transfers on this node to the\n# given total throughput in Mbps. This is necessary because Cassandra does\n# mostly sequential IO when streaming data during bootstrap or repair, which\n# can lead to saturating the network connection and degrading rpc performance.\n# When unset, the default is 200 Mbps or 25 MB/s.\nstream_throughput_outbound_megabits_per_sec: 200\n# Throttles all streaming ﬁle transfer between the datacenters,\n# this setting allows users to throttle inter dc stream throughput in addition\n# to throttling all network stream trafﬁc as conﬁgured with\n# stream_throughput_outbound_megabits_per_sec\n# When unset, the default is 200 Mbps or 25 MB/s\ninter_dc_stream_throughput_outbound_megabits_per_sec: 200\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-44-638.jpg?cb=1474336642\" title=\"44&#10;# Set socket timeout for streaming operation.&#10;# The stre...\" target=\"_blank\">\n        44.\n      </a>\n    44\n# Set socket timeout for streaming operation.\n# The stream session is failed if no data/ack is received by any of the participants\n# within that period, which means this should also be sufﬁcient to stream a large\n# sstable or rebuild table indexes.\n# Default value is 86400000ms, which means stale streams timeout after 24 hours.\n# A value of zero means stream sockets should never time out.\n# streaming_socket_timeout_in_ms: 86400000\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-45-638.jpg?cb=1474336642\" title=\"repairs&#10;• Anti-Entropy: QUORUM &amp; ALL replicas compared for&#10;...\" target=\"_blank\">\n        45.\n      </a>\n    repairs\n• Anti-Entropy: QUORUM &amp; ALL replicas compared for\nCF and discrepancies fixed.\n• must run before `gc_grace_period` (10 days by\ndefault)\n• cluster pressure\n• network pressure (same as bootstrapping)\n• GC fun…\n• plan extra cluster capability for repairs\n45\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-46-638.jpg?cb=1474336642\" title=\"repairs: what to do then?&#10;• you must define a repair strate...\" target=\"_blank\">\n        46.\n      </a>\n    repairs: what to do then?\n• you must define a repair strategy from the beginning\n• custom tooling\n• DSE and OpsCenter\n• Spotify Cassandra Reaper: \nhttps://github.com/spotify/cassandra-reaper  \nhttps://github.com/adejanovski/cassandra-reaper\n• do not necessary repair everything all the time (know\nyour data)\n46\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-47-638.jpg?cb=1474336642\" title=\"hints&#10;• if node down: spool and redelivery&#10;• slow and broke...\" target=\"_blank\">\n        47.\n      </a>\n    hints\n• if node down: spool and redelivery\n• slow and broken until 3.0: must truncate manually as some\nare left off\n• &lt; 3.0: SSTables (which means compactions)\n• &gt;= 3.0 flat files with compression\n• &gt;= 3.0 disablehintsfordc / enablehintsfordc to selectively\ndisable or enable hinted handoffs for a data center.\n• increase hint delivery threads along with # of DCs\n47\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-48-638.jpg?cb=1474336642\" title=\"cassandra.yaml: `hints`&#10;max_hints_delivery_threads: 2&#10;# Dir...\" target=\"_blank\">\n        48.\n      </a>\n    cassandra.yaml: `hints`\nmax_hints_delivery_threads: 2\n# Directory where Cassandra should store hints.\n# If not set, the default directory is $CASSANDRA_HOME/data/hints.\n# hints_directory: /var/lib/cassandra/hints\n# Compression to apply to the hint files. If omitted, hints files\n# will be written uncompressed. LZ4, Snappy, and Deflate compressors\n# are supported.\n#hints_compression:\n# - class_name: LZ4Compressor\n# parameters:\n# -\n48\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-49-638.jpg?cb=1474336642\" title=\"compactions&#10;• process of merging SSTables to single files&#10;•...\" target=\"_blank\">\n        49.\n      </a>\n    compactions\n• process of merging SSTables to single files\n• IO heavy: GC / CPU / eat disk space\n• removes tombstones\n• high write throughout on a single table from every\nnodes of every DC might eat your CPU w/ compactions:\nchoose compaction strategy wisely!\n• increment # of compactors\n49\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-50-638.jpg?cb=1474336642\" title=\"cassandra.yaml: `concurrent_compactors`&#10;[…]&#10;concurrent_comp...\" target=\"_blank\">\n        50.\n      </a>\n    cassandra.yaml: `concurrent_compactors`\n[…]\nconcurrent_compactors: 2\n[…]\n50\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-51-638.jpg?cb=1474336642\" title=\"tombstones&#10;• monitor for tombstones warnings&#10;• maintenance ...\" target=\"_blank\">\n        51.\n      </a>\n    tombstones\n• monitor for tombstones warnings\n• maintenance ops issue or application level issue?\n51\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-52-638.jpg?cb=1474336642\" title=\"cassandra.yaml: `tombstone_warn_threshold`&#10;# When executing...\" target=\"_blank\">\n        52.\n      </a>\n    cassandra.yaml: `tombstone_warn_threshold`\n# When executing a scan, within or across a partition, we need to keep the\n# tombstones seen in memory so we can return them to the coordinator, which\n# will use them to make sure other replicas also know about the deleted rows.\n# With workloads that generate a lot of tombstones, this can cause performance\n# problems and even exaust the server heap.\n# (http://www.datastax.com/dev/blog/cassandra-anti-patterns-queues-and-queue-like-datasets)\n# Adjust the thresholds here if you understand the dangers and want to\n# scan more tombstones anyway. These thresholds may also be adjusted at runtime\n# using the StorageService mbean.\ntombstone_warn_threshold: 1000\ntombstone_failure_threshold: 100000\n52\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-53-638.jpg?cb=1474336642\" title=\"indexes&#10;• secondary indexes? SASI?&#10;• if looking for search:...\" target=\"_blank\">\n        53.\n      </a>\n    indexes\n• secondary indexes? SASI?\n• if looking for search:\n• use DSE and its integrated search\n• check https://github.com/vroyer/elassandra\n• use another service to do the job outside C* and\nmove / sync data\n53\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-54-638.jpg?cb=1474336642\" title=\"sick nodes&#10;• what is the issue?&#10;• remove “sick nodes” from ...\" target=\"_blank\">\n        54.\n      </a>\n    sick nodes\n• what is the issue?\n• remove “sick nodes” from the ring when it happens\n54\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-55-638.jpg?cb=1474336642\" title=\"must reads&#10;• Datastax Apache Cassandra documentation&#10;• http...\" target=\"_blank\">\n        55.\n      </a>\n    must reads\n• Datastax Apache Cassandra documentation\n• http://docs.datastax.com/en//cassandra/3.0/cassandra/\ncassandraAbout.html\n• Al's Cassandra 2.1 tuning guide\n• https://tobert.github.io/pages/als-cassandra-21-tuning-\nguide.html\n• cassandra-user mailing list\n• http://www.planetcassandra.org/apache-cassandra-mailing-lists/\n55\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/anguenotcassandrasummmit2016-160919235344/95/apache-cassandra-multidatacenter-essentials-julien-anguenot-iland-internet-solutions-c-summit-2016-56-638.jpg?cb=1474336642\" title=\"Q&amp;A&#10;@anguenot&#10;http://www.slideshare.net/anguenot/&#10;\" target=\"_blank\">\n        56.\n      </a>\n    Q&amp;A\n@anguenot\nhttp://www.slideshare.net/anguenot/\n \n  </li>\n              </ol></div></div></div><aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\"><dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\"><ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"66190687\" title=\"PowerPoint 2016: Shortcuts\" href=\"https://www.linkedin.com/learning/powerpoint-2016-shortcuts?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_PowerPoint 2016: Shortcuts\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"PowerPoint 2016: Shortcuts\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=Q8CAKihtghsRRL8hygXfdU0PiFk%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lXiyu8t2fZHLgfc_XZLSioVQTcSsBmAQ2d-2rRzbpFY69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>PowerPoint 2016: Shortcuts</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"66190687\" title=\"Teaching Future-Ready Students\" href=\"https://www.linkedin.com/learning/teaching-future-ready-students?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Teaching Future-Ready Students\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Teaching Future-Ready Students\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=l5Oo%2BC8xrExl1LjsAzoFgo7%2B9iA%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lWiKq89OfZHbuec7ZZLSiol8eeywAlgEzfemtRDTpEo69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Teaching Future-Ready Students</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"66190687\" title=\"Learning to Teach Online\" href=\"https://www.linkedin.com/learning/learning-to-teach-online?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Learning to Teach Online\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Learning to Teach Online\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=oaL29uUYS0ceLaNm0nQ7NB0R3wQ%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lXyGj_tyfZHPtcMPWZLSiol8eeywAmQIyfumvQjPgEo69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Learning to Teach Online</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"66106401\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Building a Distributed Reservation System with Cassandra (Andrew Baker &amp; Jeffrey Carpenter, Choice Hotels) | C* Summit 2016\" href=\"https://www.slideshare.net/DataStax/building-a-distributed-reservation-system-with-cassandra-andrew-baker-jeffrey-carpenter-choice-hotels-c-summit-2016\">\n    \n    <div class=\"related-content\"><p>Building a Distributed Reservation System with Cassandra (Andrew Baker &amp; Jeff...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"66741444\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Terror &amp; Hysteria: Cost Effective Scaling of Time Series Data with Cassandra (Sam Bisbee, Threat Stack) | C* Summit 2016\" href=\"https://www.slideshare.net/DataStax/terror-hysteria-cost-effective-scaling-of-time-series-data-with-cassandra-sam-bisbee-threat-stack-c-summit-2016\">\n    \n    <div class=\"related-content\"><p>Terror &amp; Hysteria: Cost Effective Scaling of Time Series Data with Cassandra ...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"66741594\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"The Promise and Perils of Encrypting Cassandra Data (Ameesh Divatia, Baffle, Inc.) | C* Summit 2016\" href=\"https://www.slideshare.net/DataStax/the-promise-and-perils-of-encrypting-cassandra-data-ameesh-divatia-baffle-inc-c-summit-2016\">\n    \n    <div class=\"related-content\"><p>The Promise and Perils of Encrypting Cassandra Data (Ameesh Divatia, Baffle, ...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"66406067\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Tales From the Field: The Wrong Way of Using Cassandra (Carlos Rolo, Pythian) | C* Summit 2016\" href=\"https://www.slideshare.net/DataStax/tales-from-the-field-the-wrong-way-of-using-cassandra-carlos-rolo-pythian-c-summit-2016\">\n    \n    <div class=\"related-content\"><p>Tales From the Field: The Wrong Way of Using Cassandra (Carlos Rolo, Pythian)...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"66407148\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Clock Skew and Other Annoying Realities in Distributed Systems (Donny Nadolny, PagerDuty) | Cassandra Summit 2016\" href=\"https://www.slideshare.net/DataStax/clock-skew-and-other-annoying-realities-in-distributed-systems-donny-nadolny-pagerduty-cassandra-summit-2016\">\n    \n    <div class=\"related-content\"><p>Clock Skew and Other Annoying Realities in Distributed Systems (Donny Nadolny...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"53480945\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"PagerDuty: One Year of Cassandra Failures\" href=\"https://www.slideshare.net/planetcassandra/pagerduty-one-year-of-cassandra-failures\">\n    \n    <div class=\"related-content\"><p>PagerDuty: One Year of Cassandra Failures</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"66646161\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Lessons Learned on Java Tuning for Our Cassandra Clusters (Carlos Monroy, Knewton) | C* Summit 2016\" href=\"https://www.slideshare.net/DataStax/lessons-learned-on-java-tuning-for-our-cassandra-clusters-carlos-monroy-knewton-c-summit-2016\">\n    \n    <div class=\"related-content\"><p>Lessons Learned on Java Tuning for Our Cassandra Clusters (Carlos Monroy, Kne...</p><p>DataStax</p></div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div></div><footer>\n          <div class=\"row\"><div class=\"columns\"><ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div></div>\n          \n          <div class=\"row\"><div class=\"columns\"><p class=\"copyright text-center\">LinkedIn Corporation © 2018</p></div></div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div>\n    <div class=\"row\"><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></div>\n        \n    </div>\n    \n    \n    \n  \n    \n    \n  \n  \n  <noscript>\n    </noscript>",
        "created_at": "2018-09-26T22:41:41+0000",
        "updated_at": "2018-09-26T22:41:54+0000",
        "published_at": null,
        "published_by": [
          "DataStax"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 12,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/anguenotcassandrasummmit2016-160919235344-thumbnail-4.jpg?cb=1474336642",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12204"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 36,
            "label": "solr",
            "slug": "solr"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          },
          {
            "id": 1011,
            "label": "pdf",
            "slug": "pdf"
          },
          {
            "id": 1068,
            "label": "graph",
            "slug": "graph"
          }
        ],
        "is_public": false,
        "id": 12193,
        "uid": null,
        "title": "DataStax Enterprise (DSE) on the AWS Cloud",
        "url": "https://s3.amazonaws.com/quickstart-reference/datastax/latest/doc/datastax-enterprise-on-the-aws-cloud.pdf",
        "content": "DataStax Enterprise\t (DSE)\t\t<br />\n on the AWS Cloud\t\t<br />\nQuick Start Reference Deployment<br />\n<br />\n–\t\t\t\t\t<br />\n\t\t\t\t\t<br />\n\t\t\t\t\t\t\t<br />\n\t\t\t\t\t\t\t<br />\n<br />\n\t\t\t\t<br />\n\t\t\t<br />\nOverview\t\t<br />\n\t\t\t<br />\n\t\t\t\t\t\t\t<br />\n\t\t\t<br />\n\t\t<br />\n\t\t\t<br />\n\t\t<br />\n<br />\n<br />\n\t\t\t\t\t<br />\n<br />\n\t\t<br />\n\t\t\t\t<br />\n\t\t<br />\n\t\t<br />\n\t\t\t\t<br />\nCosts and Licenses\t\t<br />\n<br />\n\t\t<br />\n\t\t<br />\n<br />\n\t\t\t<br />\n\t\t<br />\n\t\t\t<br />\n\t\t\t\t\t<br />\nuse.  To use the Quick Start to build a production environment, you’ll need to acquire a<br />\n<br />\n–\t\t\t\t\t<br />\n\t\t\t\t\t<br />\n\t\t\t<br />\n\t\t\t\t<br />\nArchitecture\t\t<br />\n\t\t\t\t\t\t<br />\n\t\t\t\t\t\t\t\t<br />\n\t<br />\n\t\t\t\t\t<br />\n\t\t<br />\n\t\t<br />\n<br />\n<br />\n–\t\t\t\t\t<br />\n\t\t\t\t\t<br />\n\t\t\t\t\t<br />\n\t\t<br />\n\t\t\t<br />\n\t\t\t\t\t\t<br />\n\t\t\t\t<br />\n\t\t<br />\n\t\t\t\t\t\t<br />\n\t\t<br />\n\t\t\t\t\t\t\t<br />\n\t\t\t<br />\n\t\t\t<br />\n\t\t\t\t\t\t\t\t<br />\n\t\t\t<br />\n\t\t<br />\n\t\t\t\t\t<br />\n\t\t<br />\nDSE Data Centers and Nodes\t\t<br />\n<br />\n\t\t\t\t<br />\n\t\t\t<br />\n\t\t<br />\n<br />\n<br />\n\t\t<br />\n\t\t\t<br />\n\t\t\t<br />\n\t\t<br />\nPrerequisites\t\t<br />\nSpecialized Knowledge\t\t<br />\n\t\t\t\t\t\t<br />\n\t\t\t\t<br />\n\t\t\t<br />\n<br />\n<br />\n–\t\t\t\t\t<br />\n\t\t\t\t\t<br />\n\t\t\t<br />\n\t\t<br />\n\t\t\t\t<br />\nTechnical Requirements\t\t<br />\n<br />\n\t\t<br />\n\t\t<br />\nYou’ll \t\t\t\t\t<br />\n\t\t\t<br />\nDeployment Options\t\t<br />\n\t\t\t<br />\n\t\t\t\t\t\t\t\t\t\t<br />\n\t\t\t<br />\n\t\t\t\t\t\t\t<br />\n\t\t\t\t\t\t\t\t<br />\n\t\t\t<br />\n<br />\n\t\t\t\t<br />\nDeployment\t Steps\t\t<br />\nStep 1\t. Prepare \tYour\t AWS Account\t\t<br />\nIf you don’t already have<br />\n<br />\n–\t\t\t\t\t<br />\n\t\t\t\t\t<br />\nStep 2. C\treate a DataSt\tax \tAcademy Account\t\t<br />\n\t\t\t\t<br />\n\trt. You’ll be prompted for your\t\t<br />\n\t\t<br />\nStep \t3. Launch the\t Quick Start\t\t<br />\n\t\t\t\t<br />\n\t\t\t<br />\n\t\t\t<br />\n\t\t\t\t<br />\n\t\t\t<br />\n\t\t\t\t<br />\n\t\t<br />\n\t\t<br />\n\t\t\t\t<br />\n\t\t<br />\n\t\t<br />\n\t\t\t<br />\n\t\t\t<br />\n<br />\n\t\tIf you’re deploying \t\t\t<br />\n\t\t\t<br />\n\t\t\t\t\t<br />\n\t\t\t<br />\n\tnternet. You’ll also need the domain name option configured \t<br />\n\t\t\t. You’ll be \t<br />\n\t\t<br />\n\t\t<br />\n\t\t<br />\nCheck the region that’s di\t\t\t\t<br />\n\t\t\t\t<br />\n\t\t\t\t\t\t<br />\n\t\t\t\t\t\t<br />\n\t\t\t<br />\nLaunch\tLaunch<br />\n<br />\n–\t\t\t\t\t<br />\n\t\t\t\t\t<br />\n\t\t\t\t\t<br />\n\t\t<br />\n\t\t<br />\n\t\t\t<br />\n\t\t\t<br />\n\t\t\t<br />\n–\t\t\t\t\t<br />\n–\t\t\t\t\t<br />\n<br />\n<br />\n–\t\t\t\t\t<br />\n\t\t\t\t\t<br />\n<br />\n\t\t<br />\n\t\t\t\t<br />\n\t\t<br />\n\t\t<br />\n\t\t\t\t\t\t<br />\n\t\t<br />\n\t\t<br />\n\t\t\t\t\t\t<br />\n\t\t<br />\n\t\t<br />\n\t\t<br />\n\t\t\t\t\t<br />\n\t\t\t\t<br />\n\t\t<br />\n\t\t\t\t<br />\n<br />\n\t\t<br />\n\t\t<br />\n\t\t<br />\n\t\t\t\t\t\t<br />\n\t\t\t\t\t\t<br />\n\t\t\t<br />\n\t\t\t<br />\n\t\t\t\t<br />\n\t\t<br />\n\t\t\t\t\t\t<br />\n<br />\n\t\t\t\t<br />\n<br />\n\t\t\t\t<br />\n\t\t<br />\n\t\t<br />\n\t\t\t<br />\n\t\t\t<br />\n\t\t\t\t<br />\n\t\t<br />\n<br />\n\t\t\t<br />\n\t\t<br />\n\t\t<br />\n\t\t\t\t\t<br />\n\t\t\t<br />\n\t\t\t\t\t<br />\n<br />\n\t\t\t\t\t<br />\n\t\t\t<br />\n\t\t\t<br />\n\t\t<br />\n\t\t<br />\n\t\t\t\t<br />\n\t\t<br />\n\t\t<br />\n<br />\n\t\t<br />\n\t\t\t\t<br />\n\t\t<br />\n\t\t<br />\n\t\t<br />\n\t\t<br />\n\t\t<br />\n\t\t\t\t<br />\n\t\t<br />\nyou’ve created for your copy of Quick Start assets<br />\n<br />\n–\t\t\t\t\t<br />\n\t\t\t\t\t<br />\n<br />\n\t\t<br />\n\t\t\t\t<br />\n\t<br />\n\t\t<br />\n<br />\n\t\t<br />\n\t\t<br />\n\t<br />\n\t\t\t\t\t\t\t<br />\n\t\t\t<br />\n\t\t\t<br />\n<br />\n\t\t<br />\n<br />\n<br />\n–<br />\n<br />\n–\t\t\t\t\t<br />\n\t\t\t\t\t\t<br />\n\t\t<br />\n<br />\n\t\t<br />\n\t\t\t\t<br />\n<br />\n\t\t<br />\n\t\t<br />\n\t<br />\n\t\t<br />\n\t\t<br />\n\t\t\t\t<br />\n\t\t<br />\nyou’ve created for your copy of Quick Start assets\t\t\t<br />\n\t\t\t<br />\n<br />\n<br />\n\t\t<br />\n<br />\n\t\t<br />\n\t\t<br />\n\t<br />\n\t\t\t\t\t\t\t<br />\n\t\t\t<br />\n\t\t\t<br />\n<br />\n\t\t<br />\n\t\t\t\t\t\t\t\t<br />\n\t\t. When you’re done, choose \t\t\t<br />\n\t\t\t\t\t\t<br />\n\t\t<br />\n\t\t\t\t<br />\n\t\t\t<br />\nThis deployment uses nested stacks. In addition to the root stack, you’ll see a VPC stack <br />\n\t\t\t\t\t<br />\n\t\t\t<br />\n\t\t\t\t<br />\n\t\t\t\t<br />\nOpsCenter’s<br />\n<br />\n–\t\t\t\t\t<br />\n\t\t\t\t\t\t<br />\nIf you don’t see the complete cluster, as illustrated in Figure 2, see the \t\t\t<br />\n\t\t<br />\n\t<br />\n\t\t<br />\nStep 4\t. Test the Deployment<br />\n<br />\n–\t\t\t\t\t<br />\n\t\t\t\t\t\t<br />\n\t\t<br />\n\t\t<br />\n\t\t<br />\n\t\t<br />\n<br />\n<br />\n\t\t\t\t\t\t<br />\n\t\t<br />\n\t\t\t<br />\n\t\t<br />\n\t\t\t<br />\n\t\t\t\t<br />\n\t\t\t<br />\n\t<br />\n\t\t\t\t<br />\n\t\t\t\t<br />\n\t\t\t<br />\n\t\t\t\t\t\t\t<br />\n\t\t\t\t’re\t\t<br />\n\t\t\t\t\t\t\t\t\t<br />\nStep 5. Back up Your Data<br />\n<br />\n–\t\t\t\t\t<br />\n\t\t\t\t\t\t<br />\nTroubleshooting\t\t<br />\n\t\t\t<br />\n\t\t\t<br />\n\t\t\t\t\t\t\t<br />\n\t\tpage.) With this setting, the stack’s state will be \t<br />\n\t\t\t\t<br />\n\t\t\t\t\t\t\t\t\t\t<br />\n\t\t\t\t\t, you’ll continue to \t<br />\n<br />\nyou’ve finished troubleshoo\t\t\t<br />\n\t\t\t<br />\n\t\t<br />\n\tThe stacks all report CREATE_COMPLETE, but I don’t see my cluster in \t\t<br />\n\t\t<br />\n\t\t\t<br />\nOpsCenter’s \t\t\t\t\t\t\t<br />\n\t\t\t\t\t\t<br />\nyou’l\t\t\t\t\t<br />\n\t\t\t\t\t\t<br />\n\t\t\t\t\t\t\t\t<br />\n\t\t\t\t\t\t\t<br />\n\t\t\t\t\t\t\t<br />\n\t\t\t\t\t\t<br />\n\t\t\t\t\t<br />\n\t\t\t\t\t\t\t<br />\n\t\t\t\t\t\t\t\t<br />\n\t\t\t<br />\n\t\t<br />\n\tWe recommend that you launch the Quick Start templates from the location we’ve<br />\n<br />\n–\t\t\t\t\t<br />\n\t\t\t\t\t\t<br />\n\t\t\t<br />\n\t\t\t<br />\nAdditional Resources\t\t<br />\n\t\t<br />\n\t\t\t<br />\n\t\t<br />\n\t\t\t<br />\n\t\t<br />\n\t\t\t<br />\n\t\t<br />\n\t\t<br />\n\t\t\t<br />\n\t\t\t<br />\n\t\t\t<br />\n\t\t<br />\n\t\t\t<br />\n\t\t\t<br />\n\t\t<br />\nG\titHub Repository\t\t<br />\n\t\t\t<br />\n\t\t\t\t\t<br />\nDocument Revisions\t\t<br />\n\t\t\t\t\t\t<br />\n\t\t\t\t\t\t<br />\n\t\t\t\t\t—<br />\n<br />\n–\t\t\t\t\t<br />\n\t\t\t\t\t\t<br />\n<br />\n<br />\n\t\t\t\t\t\t<br />\n\t\t<br />\n\tinformational purposes only. It represents AWS’s current product offerings \t<br />\n<br />\n\t\t<br />\nuse of AWS’s products or services, each of which is provided “as is” without warranty of any kind, whether",
        "created_at": "2018-09-25T11:53:26+0000",
        "updated_at": "2018-09-25T11:53:42+0000",
        "published_at": "2018-05-24T18:37:19+0000",
        "published_by": [
          "Amazon Web Services"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "application/pdf",
        "language": null,
        "reading_time": 1,
        "domain_name": "s3.amazonaws.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12193"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 901,
            "label": "security",
            "slug": "security"
          }
        ],
        "is_public": false,
        "id": 12192,
        "uid": null,
        "title": "Securing Cassandra for Compliance",
        "url": "https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1",
        "content": "Securing Cassandra for Compliance\n      \n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            </a><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div>\n    <div class=\"wrapper\"><p>Successfully reported this slideshow.</p><div id=\"slideview-container\" class=\"\"><div class=\"row\"><div id=\"main-panel\" class=\"small-12 large-8 columns\"><div class=\"sectionElements\"><div class=\"playerWrapper\"><div><div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\">Securing Cassandra for Compliance<div class=\"stage valign-first-slide\"><div class=\"slide_container\"><section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-1-638.jpg?cb=1463070499\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-1-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-1-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-1-1024.jpg?cb=1463070499\" alt=\"Securing Cassandra&#10;for Compliance (or Paranoia)&#10;\" /></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-2-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-2-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-2-1024.jpg?cb=1463070499\" alt=\"Hi, I'm Nate.&#10;@zznate&#10;https://www.linkedin.com/in/zznate&#10;http://www.slideshare.net/zznate/&#10;Co-Founder, CTO&#10;The Last Pickle...\" /></i></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-3-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-3-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-3-1024.jpg?cb=1463070499\" alt=\"Security presentations can be scary.&#10;Here's a cat.&#10;\" /></i></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-4-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-4-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-4-1024.jpg?cb=1463070499\" alt=\"First, how did we get here and why is&#10;securing Cassandra important?&#10;\" /></i></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-5-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-5-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-5-1024.jpg?cb=1463070499\" alt=\"&quot;Target CEO Gregg Steinhafel Resigns In&#10;Data Breach Fallout&quot;&#10;http://www.forbes.com/sites/clareoconnor/2014/05/05/target-ce...\" /></i></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-6-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-6-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-6-1024.jpg?cb=1463070499\" alt=\"I have&#10;your&#10;personal&#10;information&#10;Customers place a lot of trust&#10;in technology companies&#10;\" /></i></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-7-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-7-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-7-1024.jpg?cb=1463070499\" alt=\"LOL! Me too!&#10;Sometimes too much.&#10;\" /></i></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-8-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-8-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-8-1024.jpg?cb=1463070499\" alt=\"Ease of scalability comes with a price&#10;\" /></i></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-9-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-9-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-9-1024.jpg?cb=1463070499\" alt=\"HA! A bin-packed&#10;message format with no source&#10;veriﬁcation!*&#10;Ease of scalability comes with a price&#10;* &lt;currently reading o...\" /></i></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-10-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-10-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-10-1024.jpg?cb=1463070499\" alt=\"nmap -Pn -p7000 &#10;-oG logs/cass.gnmap 54.88.0.0/14&#10;\" /></i></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-11-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-11-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-11-1024.jpg?cb=1463070499\" alt=\"I'm publicly&#10;discussing your&#10;technical&#10;shortcomings&#10;Then you end up in this situation.&#10;\" /></i></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-12-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-12-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-12-1024.jpg?cb=1463070499\" alt=\"Meanwhile, at the FCC...&#10;We have to require two&#10;factor, secure socket transport&#10;encryption, something something...&#10;ZZZzzzz...\" /></i></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-13-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-13-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-13-1024.jpg?cb=1463070499\" alt=\"We did a regulation!&#10;My staffers still print&#10;out my email :)&#10;\" /></i></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-14-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-14-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-14-1024.jpg?cb=1463070499\" alt=\"Why&#10;are we doing&#10;this again?&#10;Sssshhhh.&#10;I'm AES'ing...&#10;...even though the trafﬁc&#10;never leaves a backplane.&#10;Some industries ...\" /></i></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-15-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-15-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-15-1024.jpg?cb=1463070499\" alt=\"1. Encrypting data at rest&#10;2. Encrypting data on the wire&#10;3. Authentication and authorization&#10;4. Management and tooling&#10;Fo...\" /></i></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-16-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-16-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-16-1024.jpg?cb=1463070499\" alt=\"1. Encryption at rest&#10;\" /></i></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-17-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-17-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-17-1024.jpg?cb=1463070499\" alt=\"No matter what:&#10;understand the failure modes&#10;\" /></i></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-18-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-18-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-18-1024.jpg?cb=1463070499\" alt=\"bit rot, entropy, etc.&#10;Horrible things can happen with on disk encryption.&#10;\" /></i></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-19-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-19-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-19-1024.jpg?cb=1463070499\" alt=\"Don't mind me, I'm just&#10;your key server.&#10;\" /></i></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-20-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-20-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-20-1024.jpg?cb=1463070499\" alt=\"Haha! Later!&#10;x&#10;What's on this&#10;disk again?&#10;Shrug.&#10;\" /></i></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-21-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-21-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-21-1024.jpg?cb=1463070499\" alt=\"...but you may not have a choice.&#10;Because we said &quot;at rest&quot;&#10;\" /></i></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-22-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-22-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-22-1024.jpg?cb=1463070499\" alt=\"dmcrypt, eCryptFS&#10;Open source options:&#10;\" /></i></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-23-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-23-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-23-1024.jpg?cb=1463070499\" alt=\"Vormetric, Gazzang&#10;Commercial options:&#10;\" /></i></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-24-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-24-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-24-1024.jpg?cb=1463070499\" alt=\"DSE Encryption&#10;CREATETABLE users&#10;...&#10;WITH compression_parameters:sstable_compression = 'Encryptor'&#10;and compression_paramet...\" /></i></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-25-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-25-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-25-1024.jpg?cb=1463070499\" alt=\"DSE Encryption&#10;CREATETABLE users&#10;...&#10;WITH compression_parameters:sstable_compression = 'Encryptor'&#10;and compression_paramet...\" /></i></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-26-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-26-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-26-1024.jpg?cb=1463070499\" alt=\"EBS Encryption&#10;(a.k.a &quot;not my problem&quot;)&#10;\" /></i></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-27-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-27-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-27-1024.jpg?cb=1463070499\" alt=\"(Looks like this)&#10;EBS Encryption&#10;(a.k.a &quot;not my problem&quot;)&#10;http://www.slideshare.net/AmazonWebServices/bdt323-amazon-ebs-ca...\" /></i></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-28-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-28-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-28-1024.jpg?cb=1463070499\" alt=\"Maybe Client Side?&#10;The Java Driver now has custom codecs&#10;which would make this easy to implement&#10;https://github.com/datast...\" /></i></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-29-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-29-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-29-1024.jpg?cb=1463070499\" alt=\"Maybe Client Side?&#10;The Java Driver now has custom codecs&#10;which would make this easy to implement&#10;https://github.com/datast...\" /></i></section><section data-index=\"30\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-30-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-30-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-30-1024.jpg?cb=1463070499\" alt=\"New in Cassandra 3.4&#10;(DSE 5.1?):&#10;Commitlog Encryption: CASSANDRA-6018&#10;Hint File Encryption: CASSANDRA-11040&#10;https://issues...\" /></i></section><section data-index=\"31\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-31-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-31-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-31-1024.jpg?cb=1463070499\" alt=\"2. Encryption on the wire&#10;\" /></i></section><section data-index=\"32\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-32-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-32-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-32-1024.jpg?cb=1463070499\" alt=\"Because:&#10;It is really easy to attack&#10;an un-protected cluster&#10;\" /></i></section><section data-index=\"33\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-33-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-33-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-33-1024.jpg?cb=1463070499\" alt=\"It takes a single Message&#10;to insert an admin account&#10;into the system table&#10;\" /></i></section><section data-index=\"34\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-34-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-34-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-34-1024.jpg?cb=1463070499\" alt=\"-Dcassandra.write_survey=true&#10;How to steal writes in real time:&#10;\" /></i></section><section data-index=\"35\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-35-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-35-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-35-1024.jpg?cb=1463070499\" alt=\"The ﬁx is straight forward:&#10;node to node encryption and SSL client certiﬁcate&#10;authentication to cluster trafﬁc&#10;\" /></i></section><section data-index=\"36\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-36-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-36-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-36-1024.jpg?cb=1463070499\" alt=\"Awwwwww.&#10;The ﬁx is straight forward:&#10;node to node encryption and SSL client certiﬁcate&#10;authentication to cluster trafﬁc&#10;\" /></i></section><section data-index=\"37\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-37-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-37-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-37-1024.jpg?cb=1463070499\" alt=\"Awwwwww.&#10;The ﬁx is straight forward:&#10;node to node encryption and SSL client certiﬁcate&#10;authentication to cluster trafﬁc&#10;Bo...\" /></i></section><section data-index=\"38\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-38-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-38-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-38-1024.jpg?cb=1463070499\" alt=\"Awwwwww.&#10;The ﬁx is straight forward:&#10;node to node encryption and SSL client certiﬁcate&#10;authentication to cluster trafﬁc&#10;Bo...\" /></i></section><section data-index=\"39\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-39-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-39-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-39-1024.jpg?cb=1463070499\" alt=\"When you are done it should look like:&#10;\" /></i></section><section data-index=\"40\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-40-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-40-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-40-1024.jpg?cb=1463070499\" alt=\"Things to note:&#10;Use &quot;dc&quot; or &quot;rack&quot; to limit encryption to&#10;connections between racks and data centers&#10;\" /></i></section><section data-index=\"41\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-41-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-41-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-41-1024.jpg?cb=1463070499\" alt=\"Thanks for that!!&#10;Huzzah!&#10;(But AES on modern hardware&#10;will not be a bottleneck)&#10;\" /></i></section><section data-index=\"42\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-42-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-42-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-42-1024.jpg?cb=1463070499\" alt=\"Things to note:&#10;Keystore and key password must match&#10;(artifact of JDK X.509 Impl complexity)&#10;\" /></i></section><section data-index=\"43\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-43-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-43-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-43-1024.jpg?cb=1463070499\" alt=\"Things to note:&#10;256 bit means export restrictions&#10;(requires JCE provider JAR)&#10;http://www.oracle.com/technetwork/java/javas...\" /></i></section><section data-index=\"44\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-44-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-44-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-44-1024.jpg?cb=1463070499\" alt=\"Don't forget this part or else...&#10;Things to note:&#10;\" /></i></section><section data-index=\"45\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-45-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-45-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-45-1024.jpg?cb=1463070499\" alt=\"Hahaha!&#10;Now I'm hacking you over SSL.&#10;*Still* vulnerable AND you can't see what the&#10;attacker is doing.&#10;\" /></i></section><section data-index=\"46\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-46-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-46-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-46-1024.jpg?cb=1463070499\" alt=\"Client to Server SSL&#10;\" /></i></section><section data-index=\"47\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-47-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-47-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-47-1024.jpg?cb=1463070499\" alt=\"Client to Server SSL&#10;(see slides 30 to 35)&#10;\" /></i></section><section data-index=\"48\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-48-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-48-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-48-1024.jpg?cb=1463070499\" alt=\"Client to Server SSL&#10;(see slides 30 to 35)&#10;Now with NO downtime!!!&#10;https://issues.apache.org/jira/browse/CASSANDRA-10559&#10;A...\" /></i></section><section data-index=\"49\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-49-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-49-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-49-1024.jpg?cb=1463070499\" alt=\"Need to Debug SSL?&#10;-Djavax.net.debug=ssl&#10;http://docs.oracle.com/javase/7/docs/technotes/guides/security/jsse/ReadDebug.html&#10;\" /></i></section><section data-index=\"50\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-50-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-50-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-50-1024.jpg?cb=1463070499\" alt=\"Certs are hard :(&#10;Netﬂix Lemur:&#10;x.509 Certiﬁcate Orchestration Framework&#10;http://techblog.netﬂix.com/2015/09/introducing-le...\" /></i></section><section data-index=\"51\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-51-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-51-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-51-1024.jpg?cb=1463070499\" alt=\"Certs are hard :(&#10;Hashicorp Vault&#10;&quot;secures, stores, and tightly controls access to&#10;tokens, passwords, certiﬁcates, API key...\" /></i></section><section data-index=\"52\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-52-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-52-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-52-1024.jpg?cb=1463070499\" alt=\"2. Encryption on the wire&#10;But wait! There's more!&#10;\" /></i></section><section data-index=\"53\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-53-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-53-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-53-1024.jpg?cb=1463070499\" alt=\"The internode authentication API:&#10;BYO identity veriﬁcation&#10;\" /></i></section><section data-index=\"54\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-54-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-54-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-54-1024.jpg?cb=1463070499\" alt=\"Looks like this:&#10;\" /></i></section><section data-index=\"55\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-55-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-55-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-55-1024.jpg?cb=1463070499\" alt=\"3. Authentication and Authorization&#10;\" /></i></section><section data-index=\"56\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-56-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-56-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-56-1024.jpg?cb=1463070499\" alt=\"Best practices should not be new to you.&#10;user segmentation&#10;schema access limitation&#10;etc.&#10;\" /></i></section><section data-index=\"57\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-57-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-57-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-57-1024.jpg?cb=1463070499\" alt=\"(Everything we did with an RDBMS)&#10;Best practices should not be new to you.&#10;user segmentation&#10;schema access limitation&#10;etc.&#10;\" /></i></section><section data-index=\"58\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-58-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-58-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-58-1024.jpg?cb=1463070499\" alt=\"Best practices should not be new to you.&#10;user segmentation&#10;schema access limitation&#10;etc.&#10;(Everything we did with an RDBMS)...\" /></i></section><section data-index=\"59\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-59-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-59-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-59-1024.jpg?cb=1463070499\" alt=\"An Example&#10;\" /></i></section><section data-index=\"60\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-60-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-60-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-60-1024.jpg?cb=1463070499\" alt=\"An Example&#10;\" /></i></section><section data-index=\"61\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-61-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-61-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-61-1024.jpg?cb=1463070499\" alt=\"An Example&#10;\" /></i></section><section data-index=\"62\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-62-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-62-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-62-1024.jpg?cb=1463070499\" alt=\"An Example&#10;\" /></i></section><section data-index=\"63\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-63-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-63-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-63-1024.jpg?cb=1463070499\" alt=\"An Example&#10;buzzword compliant!&#10;\" /></i></section><section data-index=\"64\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-64-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-64-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-64-1024.jpg?cb=1463070499\" alt=\"An Example&#10;\" /></i></section><section data-index=\"65\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-65-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-65-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-65-1024.jpg?cb=1463070499\" alt=\"An Example&#10;\" /></i></section><section data-index=\"66\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-66-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-66-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-66-1024.jpg?cb=1463070499\" alt=\"Turning it all on&#10;authenticator: PasswordAuthenticator&#10;Tip: keep your read-only cqlsh credentials in&#10;$HOME/.cassandra/cqls...\" /></i></section><section data-index=\"67\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-67-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-67-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-67-1024.jpg?cb=1463070499\" alt=\"Turning it all on&#10;authorizer: CassandraAuthorizer&#10;\" /></i></section><section data-index=\"68\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-68-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-68-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-68-1024.jpg?cb=1463070499\" alt=\"Turning it all on&#10;role_manager: CassandraRoleManager&#10;\" /></i></section><section data-index=\"69\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-69-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-69-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-69-1024.jpg?cb=1463070499\" alt=\"Turning it all on&#10;authorizer: CassandraAuthorizer&#10;authenticator: PasswordAuthenticator&#10;role_manager: CassandraRoleManager&#10;...\" /></i></section><section data-index=\"70\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-70-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-70-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-70-1024.jpg?cb=1463070499\" alt=\"authorizer: CassandraAuthorizer&#10;authenticator: PasswordAuthenticator&#10;role_manager: CassandraRoleManager&#10;Turning it all on&#10;...\" /></i></section><section data-index=\"71\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-71-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-71-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-71-1024.jpg?cb=1463070499\" alt=\"authorizer: CassandraAuthorizer&#10;authenticator: PasswordAuthenticator&#10;role_manager: CassandraRoleManager&#10;Turning it all on&#10;...\" /></i></section><section data-index=\"72\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-72-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-72-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-72-1024.jpg?cb=1463070499\" alt=\"authorizer: CassandraAuthorizer&#10;authenticator: PasswordAuthenticator&#10;role_manager: CassandraRoleManager&#10;Turning it all on&#10;...\" /></i></section><section data-index=\"73\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-73-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-73-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-73-1024.jpg?cb=1463070499\" alt=\"Turning it all on&#10;authorizer: TransitionalAuthorizer&#10;authenticator: TransitionalAuthenticator&#10;DSE plugins to avoid downtime&#10;\" /></i></section><section data-index=\"74\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-74-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-74-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-74-1024.jpg?cb=1463070499\" alt=\"Turning it all on&#10;system.schema_keyspace&#10;system.schema_columns&#10;system.schema_columnfamilies&#10;system.local&#10;system.peers&#10;Thes...\" /></i></section><section data-index=\"75\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-75-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-75-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-75-1024.jpg?cb=1463070499\" alt=\"Turning it all on&#10;IMPORTANT cassandra.yaml line note:&#10;&quot;Please increase system_auth keyspace&#10;replication factor if you use ...\" /></i></section><section data-index=\"76\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-76-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-76-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-76-1024.jpg?cb=1463070499\" alt=\"Turning it all on&#10;IMPORTANT cassandra.yaml line note:&#10;&quot;Please increase system_auth keyspace&#10;replication factor if you use ...\" /></i></section><section data-index=\"77\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-77-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-77-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-77-1024.jpg?cb=1463070499\" alt=\"4. Management and tooling&#10;\" /></i></section><section data-index=\"78\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-78-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-78-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-78-1024.jpg?cb=1463070499\" alt=\"4. Management and tooling&#10;\" /></i></section><section data-index=\"79\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-79-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-79-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-79-1024.jpg?cb=1463070499\" alt=\"Securing JMX&#10;\" /></i></section><section data-index=\"80\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-80-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-80-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-80-1024.jpg?cb=1463070499\" alt=\"nmap -Pn -p7199 &#10;-oG logs/cass.gnmap 54.88.0.0/14&#10;Always a few suckers that&#10;TL,DR'ed&#10;\" /></i></section><section data-index=\"81\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-81-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-81-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-81-1024.jpg?cb=1463070499\" alt=\"Why do I need to secure JMX?&#10;\" /></i></section><section data-index=\"82\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-82-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-82-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-82-1024.jpg?cb=1463070499\" alt=\"Works as Advertised!&#10;\" /></i></section><section data-index=\"83\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-83-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-83-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-83-1024.jpg?cb=1463070499\" alt=\"also&#10;good for&#10;some&#10;LOLs&#10;\" /></i></section><section data-index=\"84\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-84-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-84-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-84-1024.jpg?cb=1463070499\" alt=\"Securing JMX&#10;SSL setup is like node to node and client to server&#10;http://docs.oracle.com/javase/8/docs/technotes/guides/man...\" /></i></section><section data-index=\"85\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-85-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-85-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-85-1024.jpg?cb=1463070499\" alt=\"Securing JMX&#10;JMX Authentication is straightforward&#10;and well documented&#10;$JAVA_HOME/jre/lib/management/jmxremote.access&#10;$JAV...\" /></i></section><section data-index=\"86\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-86-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-86-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-86-1024.jpg?cb=1463070499\" alt=\"Securing JMX&#10;$JAVA_HOME/jre/lib/management/jmxremote.access&#10;$JAVA_HOME/jre/lib/management/&#10;jmxremote.password.template&#10;Now...\" /></i></section><section data-index=\"87\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-87-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-87-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-87-1024.jpg?cb=1463070499\" alt=\"Securing JMX&#10;$JAVA_HOME/jre/lib/management/jmxremote.access&#10;$JAVA_HOME/jre/lib/management/&#10;jmxremote.password.template&#10;Now...\" /></i></section><section data-index=\"88\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-88-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-88-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-88-1024.jpg?cb=1463070499\" alt=\"Securing JMX&#10;$JAVA_HOME/jre/lib/management/jmxremote.access&#10;$JAVA_HOME/jre/lib/management/&#10;jmxremote.password.template&#10;Now...\" /></i></section><section data-index=\"89\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-89-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-89-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-89-1024.jpg?cb=1463070499\" alt=\"Thanks!@zznate&#10;\" /></i></section><section data-index=\"90\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/securing-cassandra-for-compliance?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/85/securing-cassandra-for-compliance-90-320.jpg?cb=1463070499\" data-normal=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-90-638.jpg?cb=1463070499\" data-full=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-90-1024.jpg?cb=1463070499\" alt=\"Securing Cassandra for Compliance\" /></i></section><div class=\"j-next-container next-container\"><div class=\"content-container\"><div class=\"next-slideshow-wrapper\"><div class=\"j-next-slideshow next-slideshow\"><p>Upcoming SlideShare</p></div><p>Loading in …5</p><p>×</p></div></div></div></div></div></div></div></div></div><div class=\"slideshow-info-container\" itemscope=\"itemscope\" itemtype=\"https://schema.org/MediaObject\"><div class=\"slideshow-tabs-container show-for-medium-up\"><ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    0 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\">\n                \n                  5 Likes\n                \n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\">\n                Statistics\n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\">\n                Notes\n              </i></a>\n            </li>\n          </ul><div class=\"tabs-content\"><div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\"><ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"itemscope\"><li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"Tanyadesigan\" rel=\"nofollow\" href=\"https://www.slideshare.net/Tanyadesigan?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Tanyadesigan\n                            \n                              \n                                \n                                \n                              \n                              \n                                \n                                \n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"nakai.kanako\" rel=\"nofollow\" href=\"https://www.slideshare.net/nakai.kanako?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Kanako Ogorochi\n                            \n                              \n                                , \n                                Rakuten - DBA\n                              \n                              \n                                 at \n                                Rakuten\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"ksambaiah\" rel=\"nofollow\" href=\"https://www.slideshare.net/ksambaiah?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Sambaiah Kilaru\n                            \n                              \n                                \n                                \n                              \n                              \n                                \n                                \n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"mageru\" rel=\"nofollow\" href=\"https://www.slideshare.net/mageru?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Justin Miller\n                            \n                              \n                                , \n                                Senior Systems Engineer at Cotiviti\n                              \n                              \n                                 at \n                                Cotiviti\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"JeremyBae1\" rel=\"nofollow\" href=\"https://www.slideshare.net/JeremyBae1?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Jeremy Bae\n                            \n                              \n                                , \n                                DevSecOps, AppSec specialist\n                              \n                              \n                                \n                                \n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n              </ul></div><div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No Downloads</p></div><div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No notes for slide</p></div></div></div><div class=\"notranslate transcript add-padding-right j-transcript\"><ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    Securing Cassandra\nfor Compliance (or Paranoia)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-2-638.jpg?cb=1463070499\" title=\"Hi, I'm Nate.&#10;@zznate&#10;https://www.linkedin.com/in/zznate&#10;ht...\" target=\"_blank\">\n        2.\n      </a>\n    Hi, I'm Nate.\n@zznate\nhttps://www.linkedin.com/in/zznate\nhttp://www.slideshare.net/zznate/\nCo-Founder, CTO\nThe Last Pickle\nCassandra user since 2009 (v0.4)\nAustin, Texas\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-3-638.jpg?cb=1463070499\" title=\"Security presentations can be scary.&#10;Here's a cat.&#10;\" target=\"_blank\">\n        3.\n      </a>\n    Security presentations can be scary.\nHere's a cat.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-4-638.jpg?cb=1463070499\" title=\"First, how did we get here and why is&#10;securing Cassandra im...\" target=\"_blank\">\n        4.\n      </a>\n    First, how did we get here and why is\nsecuring Cassandra important?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-5-638.jpg?cb=1463070499\" title=\"&quot;Target CEO Gregg Steinhafel Resigns In&#10;Data Breach Fallout...\" target=\"_blank\">\n        5.\n      </a>\n    \"Target CEO Gregg Steinhafel Resigns In\nData Breach Fallout\"\nhttp://www.forbes.com/sites/clareoconnor/2014/05/05/target-ceo-gregg-steinhafel-resigns-in-wake-of-data-breach-fallout/\nFirst, how did we get here and why is\nsecuring Cassandra important?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-6-638.jpg?cb=1463070499\" title=\"I have&#10;your&#10;personal&#10;information&#10;Customers place a lot of t...\" target=\"_blank\">\n        6.\n      </a>\n    I have\nyour\npersonal\ninformation\nCustomers place a lot of trust\nin technology companies\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-7-638.jpg?cb=1463070499\" title=\"LOL! Me too!&#10;Sometimes too much.&#10;\" target=\"_blank\">\n        7.\n      </a>\n    LOL! Me too!\nSometimes too much.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-8-638.jpg?cb=1463070499\" title=\"Ease of scalability comes with a price&#10;\" target=\"_blank\">\n        8.\n      </a>\n    Ease of scalability comes with a price\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-9-638.jpg?cb=1463070499\" title=\"HA! A bin-packed&#10;message format with no source&#10;veriﬁcation!...\" target=\"_blank\">\n        9.\n      </a>\n    HA! A bin-packed\nmessage format with no source\nveriﬁcation!*\nEase of scalability comes with a price\n* &lt;currently reading o.a.c.net.MessageIn#read&gt;\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-10-638.jpg?cb=1463070499\" title=\"nmap -Pn -p7000 &#10;-oG logs/cass.gnmap 54.88.0.0/14&#10;\" target=\"_blank\">\n        10.\n      </a>\n    nmap -Pn -p7000 \n-oG logs/cass.gnmap 54.88.0.0/14\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-11-638.jpg?cb=1463070499\" title=\"I'm publicly&#10;discussing your&#10;technical&#10;shortcomings&#10;Then yo...\" target=\"_blank\">\n        11.\n      </a>\n    I'm publicly\ndiscussing your\ntechnical\nshortcomings\nThen you end up in this situation.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-12-638.jpg?cb=1463070499\" title=\"Meanwhile, at the FCC...&#10;We have to require two&#10;factor, sec...\" target=\"_blank\">\n        12.\n      </a>\n    Meanwhile, at the FCC...\nWe have to require two\nfactor, secure socket transport\nencryption, something something...\nZZZzzzzzzzZZZzz\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-13-638.jpg?cb=1463070499\" title=\"We did a regulation!&#10;My staffers still print&#10;out my email :)&#10;\" target=\"_blank\">\n        13.\n      </a>\n    We did a regulation!\nMy staffers still print\nout my email :)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-14-638.jpg?cb=1463070499\" title=\"Why&#10;are we doing&#10;this again?&#10;Sssshhhh.&#10;I'm AES'ing...&#10;...ev...\" target=\"_blank\">\n        14.\n      </a>\n    Why\nare we doing\nthis again?\nSssshhhh.\nI'm AES'ing...\n...even though the trafﬁc\nnever leaves a backplane.\nSome industries will require node to node SSL\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-15-638.jpg?cb=1463070499\" title=\"1. Encrypting data at rest&#10;2. Encrypting data on the wire&#10;3...\" target=\"_blank\">\n        15.\n      </a>\n    1. Encrypting data at rest\n2. Encrypting data on the wire\n3. Authentication and authorization\n4. Management and tooling\nFocusing our Discussion: Architecture\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-16-638.jpg?cb=1463070499\" title=\"1. Encryption at rest&#10;\" target=\"_blank\">\n        16.\n      </a>\n    1. Encryption at rest\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-17-638.jpg?cb=1463070499\" title=\"No matter what:&#10;understand the failure modes&#10;\" target=\"_blank\">\n        17.\n      </a>\n    No matter what:\nunderstand the failure modes\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-18-638.jpg?cb=1463070499\" title=\"bit rot, entropy, etc.&#10;Horrible things can happen with on d...\" target=\"_blank\">\n        18.\n      </a>\n    bit rot, entropy, etc.\nHorrible things can happen with on disk encryption.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-19-638.jpg?cb=1463070499\" title=\"Don't mind me, I'm just&#10;your key server.&#10;\" target=\"_blank\">\n        19.\n      </a>\n    Don't mind me, I'm just\nyour key server.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-20-638.jpg?cb=1463070499\" title=\"Haha! Later!&#10;x&#10;What's on this&#10;disk again?&#10;Shrug.&#10;\" target=\"_blank\">\n        20.\n      </a>\n    Haha! Later!\nx\nWhat's on this\ndisk again?\nShrug.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-21-638.jpg?cb=1463070499\" title=\"...but you may not have a choice.&#10;Because we said &quot;at rest&quot;&#10;\" target=\"_blank\">\n        21.\n      </a>\n    ...but you may not have a choice.\nBecause we said \"at rest\"\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-22-638.jpg?cb=1463070499\" title=\"dmcrypt, eCryptFS&#10;Open source options:&#10;\" target=\"_blank\">\n        22.\n      </a>\n    dmcrypt, eCryptFS\nOpen source options:\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-23-638.jpg?cb=1463070499\" title=\"Vormetric, Gazzang&#10;Commercial options:&#10;\" target=\"_blank\">\n        23.\n      </a>\n    Vormetric, Gazzang\nCommercial options:\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-24-638.jpg?cb=1463070499\" title=\"DSE Encryption&#10;CREATETABLE users&#10;...&#10;WITH compression_param...\" target=\"_blank\">\n        24.\n      </a>\n    DSE Encryption\nCREATETABLE users\n...\nWITH compression_parameters:sstable_compression = 'Encryptor'\nand compression_parameters:cipher_algorithm = 'AES/ECB/\nPKCS5Padding'\nand compression_parameters:secret_key_strength = 128;\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-25-638.jpg?cb=1463070499\" title=\"DSE Encryption&#10;CREATETABLE users&#10;...&#10;WITH compression_param...\" target=\"_blank\">\n        25.\n      </a>\n    DSE Encryption\nCREATETABLE users\n...\nWITH compression_parameters:sstable_compression = 'Encryptor'\nand compression_parameters:cipher_algorithm = 'AES/ECB/\nPKCS5Padding'\nand compression_parameters:secret_key_strength = 128;\nWARNING:\ncommitlog not included*\n*eCryptFS would work ﬁne for this\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-26-638.jpg?cb=1463070499\" title=\"EBS Encryption&#10;(a.k.a &quot;not my problem&quot;)&#10;\" target=\"_blank\">\n        26.\n      </a>\n    EBS Encryption\n(a.k.a \"not my problem\")\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-27-638.jpg?cb=1463070499\" title=\"(Looks like this)&#10;EBS Encryption&#10;(a.k.a &quot;not my problem&quot;)&#10;h...\" target=\"_blank\">\n        27.\n      </a>\n    (Looks like this)\nEBS Encryption\n(a.k.a \"not my problem\")\nhttp://www.slideshare.net/AmazonWebServices/bdt323-amazon-ebs-cassandra-1-million-writes-per-second\nSee Crowdstrike's presentation on\nCassandra GP2 performance (with encryption):\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-28-638.jpg?cb=1463070499\" title=\"Maybe Client Side?&#10;The Java Driver now has custom codecs&#10;wh...\" target=\"_blank\">\n        28.\n      </a>\n    Maybe Client Side?\nThe Java Driver now has custom codecs\nwhich would make this easy to implement\nhttps://github.com/datastax/java-driver/tree/3.0/manual/custom_codecs\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-29-638.jpg?cb=1463070499\" title=\"Maybe Client Side?&#10;The Java Driver now has custom codecs&#10;wh...\" target=\"_blank\">\n        29.\n      </a>\n    Maybe Client Side?\nThe Java Driver now has custom codecs\nwhich would make this easy to implement\nhttps://github.com/datastax/java-driver/tree/3.0/manual/custom_codecs\nColumn-level encryption!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-30-638.jpg?cb=1463070499\" title=\"New in Cassandra 3.4&#10;(DSE 5.1?):&#10;Commitlog Encryption: CASS...\" target=\"_blank\">\n        30.\n      </a>\n    New in Cassandra 3.4\n(DSE 5.1?):\nCommitlog Encryption: CASSANDRA-6018\nHint File Encryption: CASSANDRA-11040\nhttps://issues.apache.org/jira/browse/CASSANDRA-6018\nhttps://issues.apache.org/jira/browse/CASSANDRA-11040\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-31-638.jpg?cb=1463070499\" title=\"2. Encryption on the wire&#10;\" target=\"_blank\">\n        31.\n      </a>\n    2. Encryption on the wire\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-32-638.jpg?cb=1463070499\" title=\"Because:&#10;It is really easy to attack&#10;an un-protected cluster&#10;\" target=\"_blank\">\n        32.\n      </a>\n    Because:\nIt is really easy to attack\nan un-protected cluster\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-33-638.jpg?cb=1463070499\" title=\"It takes a single Message&#10;to insert an admin account&#10;into t...\" target=\"_blank\">\n        33.\n      </a>\n    It takes a single Message\nto insert an admin account\ninto the system table\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-34-638.jpg?cb=1463070499\" title=\"-Dcassandra.write_survey=true&#10;How to steal writes in real t...\" target=\"_blank\">\n        34.\n      </a>\n    -Dcassandra.write_survey=true\nHow to steal writes in real time:\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-35-638.jpg?cb=1463070499\" title=\"The ﬁx is straight forward:&#10;node to node encryption and SSL...\" target=\"_blank\">\n        35.\n      </a>\n    The ﬁx is straight forward:\nnode to node encryption and SSL client certiﬁcate\nauthentication to cluster trafﬁc\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-36-638.jpg?cb=1463070499\" title=\"Awwwwww.&#10;The ﬁx is straight forward:&#10;node to node encryptio...\" target=\"_blank\">\n        36.\n      </a>\n    Awwwwww.\nThe ﬁx is straight forward:\nnode to node encryption and SSL client certiﬁcate\nauthentication to cluster trafﬁc\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-37-638.jpg?cb=1463070499\" title=\"Awwwwww.&#10;The ﬁx is straight forward:&#10;node to node encryptio...\" target=\"_blank\">\n        37.\n      </a>\n    Awwwwww.\nThe ﬁx is straight forward:\nnode to node encryption and SSL client certiﬁcate\nauthentication to cluster trafﬁc\nBonus: can be done\nwith NO downtime!!!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-38-638.jpg?cb=1463070499\" title=\"Awwwwww.&#10;The ﬁx is straight forward:&#10;node to node encryptio...\" target=\"_blank\">\n        38.\n      </a>\n    Awwwwww.\nThe ﬁx is straight forward:\nnode to node encryption and SSL client certiﬁcate\nauthentication to cluster trafﬁc\nBonus: can be done\nwith NO downtime!!!\nHow-to guide:\nhttp://thelastpickle.com/blog/2015/09/30/hardening-cassandra-step-by-step-part-1-server-to-\nserver.html\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-39-638.jpg?cb=1463070499\" title=\"When you are done it should look like:&#10;\" target=\"_blank\">\n        39.\n      </a>\n    When you are done it should look like:\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-40-638.jpg?cb=1463070499\" title=\"Things to note:&#10;Use &quot;dc&quot; or &quot;rack&quot; to limit encryption to&#10;c...\" target=\"_blank\">\n        40.\n      </a>\n    Things to note:\nUse \"dc\" or \"rack\" to limit encryption to\nconnections between racks and data centers\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-41-638.jpg?cb=1463070499\" title=\"Thanks for that!!&#10;Huzzah!&#10;(But AES on modern hardware&#10;will ...\" target=\"_blank\">\n        41.\n      </a>\n    Thanks for that!!\nHuzzah!\n(But AES on modern hardware\nwill not be a bottleneck)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-42-638.jpg?cb=1463070499\" title=\"Things to note:&#10;Keystore and key password must match&#10;(artif...\" target=\"_blank\">\n        42.\n      </a>\n    Things to note:\nKeystore and key password must match\n(artifact of JDK X.509 Impl complexity)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-43-638.jpg?cb=1463070499\" title=\"Things to note:&#10;256 bit means export restrictions&#10;(requires...\" target=\"_blank\">\n        43.\n      </a>\n    Things to note:\n256 bit means export restrictions\n(requires JCE provider JAR)\nhttp://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html\nhttp://docs.oracle.com/javase/8/docs/technotes/guides/security/SunProviders.html#importlimits\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-44-638.jpg?cb=1463070499\" title=\"Don't forget this part or else...&#10;Things to note:&#10;\" target=\"_blank\">\n        44.\n      </a>\n    Don't forget this part or else...\nThings to note:\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-45-638.jpg?cb=1463070499\" title=\"Hahaha!&#10;Now I'm hacking you over SSL.&#10;*Still* vulnerable AN...\" target=\"_blank\">\n        45.\n      </a>\n    Hahaha!\nNow I'm hacking you over SSL.\n*Still* vulnerable AND you can't see what the\nattacker is doing.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-46-638.jpg?cb=1463070499\" title=\"Client to Server SSL&#10;\" target=\"_blank\">\n        46.\n      </a>\n    Client to Server SSL\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-47-638.jpg?cb=1463070499\" title=\"Client to Server SSL&#10;(see slides 30 to 35)&#10;\" target=\"_blank\">\n        47.\n      </a>\n    Client to Server SSL\n(see slides 30 to 35)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-48-638.jpg?cb=1463070499\" title=\"Client to Server SSL&#10;(see slides 30 to 35)&#10;Now with NO down...\" target=\"_blank\">\n        48.\n      </a>\n    Client to Server SSL\n(see slides 30 to 35)\nNow with NO downtime!!!\nhttps://issues.apache.org/jira/browse/CASSANDRA-10559\nAvailable in: 2.1.12, 2.2.4, 3.0.0\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-49-638.jpg?cb=1463070499\" title=\"Need to Debug SSL?&#10;-Djavax.net.debug=ssl&#10;http://docs.oracle...\" target=\"_blank\">\n        49.\n      </a>\n    Need to Debug SSL?\n-Djavax.net.debug=ssl\nhttp://docs.oracle.com/javase/7/docs/technotes/guides/security/jsse/ReadDebug.html\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-50-638.jpg?cb=1463070499\" title=\"Certs are hard :(&#10;Netﬂix Lemur:&#10;x.509 Certiﬁcate Orchestrat...\" target=\"_blank\">\n        50.\n      </a>\n    Certs are hard :(\nNetﬂix Lemur:\nx.509 Certiﬁcate Orchestration Framework\nhttp://techblog.netﬂix.com/2015/09/introducing-lemur.html\nhttps://github.com/Netﬂix/lemur\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-51-638.jpg?cb=1463070499\" title=\"Certs are hard :(&#10;Hashicorp Vault&#10;&quot;secures, stores, and tig...\" target=\"_blank\">\n        51.\n      </a>\n    Certs are hard :(\nHashicorp Vault\n\"secures, stores, and tightly controls access to\ntokens, passwords, certiﬁcates, API keys, and\nother secrets in modern computing. \"\nhttps://www.vaultproject.io/\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-52-638.jpg?cb=1463070499\" title=\"2. Encryption on the wire&#10;But wait! There's more!&#10;\" target=\"_blank\">\n        52.\n      </a>\n    2. Encryption on the wire\nBut wait! There's more!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-53-638.jpg?cb=1463070499\" title=\"The internode authentication API:&#10;BYO identity veriﬁcation&#10;\" target=\"_blank\">\n        53.\n      </a>\n    The internode authentication API:\nBYO identity veriﬁcation\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-54-638.jpg?cb=1463070499\" title=\"Looks like this:&#10;\" target=\"_blank\">\n        54.\n      </a>\n    Looks like this:\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-55-638.jpg?cb=1463070499\" title=\"3. Authentication and Authorization&#10;\" target=\"_blank\">\n        55.\n      </a>\n    3. Authentication and Authorization\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-56-638.jpg?cb=1463070499\" title=\"Best practices should not be new to you.&#10;user segmentation&#10;...\" target=\"_blank\">\n        56.\n      </a>\n    Best practices should not be new to you.\nuser segmentation\nschema access limitation\netc.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-57-638.jpg?cb=1463070499\" title=\"(Everything we did with an RDBMS)&#10;Best practices should not...\" target=\"_blank\">\n        57.\n      </a>\n    (Everything we did with an RDBMS)\nBest practices should not be new to you.\nuser segmentation\nschema access limitation\netc.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-58-638.jpg?cb=1463070499\" title=\"Best practices should not be new to you.&#10;user segmentation&#10;...\" target=\"_blank\">\n        58.\n      </a>\n    Best practices should not be new to you.\nuser segmentation\nschema access limitation\netc.\n(Everything we did with an RDBMS)\nNew in 2.2:\nRole-based access control!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-59-638.jpg?cb=1463070499\" title=\"An Example&#10;\" target=\"_blank\">\n        59.\n      </a>\n    An Example\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-60-638.jpg?cb=1463070499\" title=\"An Example&#10;\" target=\"_blank\">\n        60.\n      </a>\n    An Example\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-61-638.jpg?cb=1463070499\" title=\"An Example&#10;\" target=\"_blank\">\n        61.\n      </a>\n    An Example\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-62-638.jpg?cb=1463070499\" title=\"An Example&#10;\" target=\"_blank\">\n        62.\n      </a>\n    An Example\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-63-638.jpg?cb=1463070499\" title=\"An Example&#10;buzzword compliant!&#10;\" target=\"_blank\">\n        63.\n      </a>\n    An Example\nbuzzword compliant!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-64-638.jpg?cb=1463070499\" title=\"An Example&#10;\" target=\"_blank\">\n        64.\n      </a>\n    An Example\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-65-638.jpg?cb=1463070499\" title=\"An Example&#10;\" target=\"_blank\">\n        65.\n      </a>\n    An Example\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-66-638.jpg?cb=1463070499\" title=\"Turning it all on&#10;authenticator: PasswordAuthenticator&#10;Tip:...\" target=\"_blank\">\n        66.\n      </a>\n    Turning it all on\nauthenticator: PasswordAuthenticator\nTip: keep your read-only cqlsh credentials in\n$HOME/.cassandra/cqlshrc\nof the system's admin account\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-67-638.jpg?cb=1463070499\" title=\"Turning it all on&#10;authorizer: CassandraAuthorizer&#10;\" target=\"_blank\">\n        67.\n      </a>\n    Turning it all on\nauthorizer: CassandraAuthorizer\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-68-638.jpg?cb=1463070499\" title=\"Turning it all on&#10;role_manager: CassandraRoleManager&#10;\" target=\"_blank\">\n        68.\n      </a>\n    Turning it all on\nrole_manager: CassandraRoleManager\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-69-638.jpg?cb=1463070499\" title=\"Turning it all on&#10;authorizer: CassandraAuthorizer&#10;authentic...\" target=\"_blank\">\n        69.\n      </a>\n    Turning it all on\nauthorizer: CassandraAuthorizer\nauthenticator: PasswordAuthenticator\nrole_manager: CassandraRoleManager\nWARNING:\npotential downtime!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-70-638.jpg?cb=1463070499\" title=\"authorizer: CassandraAuthorizer&#10;authenticator: PasswordAuth...\" target=\"_blank\">\n        70.\n      </a>\n    authorizer: CassandraAuthorizer\nauthenticator: PasswordAuthenticator\nrole_manager: CassandraRoleManager\nTurning it all on\nWARNING:\npotential downtime!\nWARNING:\nstupid defaults\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-71-638.jpg?cb=1463070499\" title=\"authorizer: CassandraAuthorizer&#10;authenticator: PasswordAuth...\" target=\"_blank\">\n        71.\n      </a>\n    authorizer: CassandraAuthorizer\nauthenticator: PasswordAuthenticator\nrole_manager: CassandraRoleManager\nTurning it all on\nWARNING:\npotential downtime!\nWARNING:\nstupid defaults\nTIP: turn these WAY UP:\npermissions_validity_in_ms\nroles_validity_in_ms\nAlso: use permissions_update_interval_in_ms\nfor async refresh if needed\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-72-638.jpg?cb=1463070499\" title=\"authorizer: CassandraAuthorizer&#10;authenticator: PasswordAuth...\" target=\"_blank\">\n        72.\n      </a>\n    authorizer: CassandraAuthorizer\nauthenticator: PasswordAuthenticator\nrole_manager: CassandraRoleManager\nTurning it all on\nWARNING:\npotential downtime!\nWARNING:\nstupid defaults\nNEW in 3.4:credentials_validity_in_ms*\n* https://issues.apache.org/jira/browse/CASSANDRA-7715\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-73-638.jpg?cb=1463070499\" title=\"Turning it all on&#10;authorizer: TransitionalAuthorizer&#10;authen...\" target=\"_blank\">\n        73.\n      </a>\n    Turning it all on\nauthorizer: TransitionalAuthorizer\nauthenticator: TransitionalAuthenticator\nDSE plugins to avoid downtime\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-74-638.jpg?cb=1463070499\" title=\"Turning it all on&#10;system.schema_keyspace&#10;system.schema_colu...\" target=\"_blank\">\n        74.\n      </a>\n    Turning it all on\nsystem.schema_keyspace\nsystem.schema_columns\nsystem.schema_columnfamilies\nsystem.local\nsystem.peers\nThese tables have default read permissions for every\nauthenticated user:\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-75-638.jpg?cb=1463070499\" title=\"Turning it all on&#10;IMPORTANT cassandra.yaml line note:&#10;&quot;Plea...\" target=\"_blank\">\n        75.\n      </a>\n    Turning it all on\nIMPORTANT cassandra.yaml line note:\n\"Please increase system_auth keyspace\nreplication factor if you use this...\"\nTip: replication factor for the system_auth\nkeyspace should be the same as the number\nof nodes in the data center\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-76-638.jpg?cb=1463070499\" title=\"Turning it all on&#10;IMPORTANT cassandra.yaml line note:&#10;&quot;Plea...\" target=\"_blank\">\n        76.\n      </a>\n    Turning it all on\nIMPORTANT cassandra.yaml line note:\n\"Please increase system_auth keyspace\nreplication factor if you use this...\"\nTip: replication factor for the system_auth\nkeyspace should be the same as the number\nof nodes in the data center\nWARNING:\nstupid defaults*\n*https://issues.apache.org/jira/browse/CASSANDRA-11340\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-77-638.jpg?cb=1463070499\" title=\"4. Management and tooling&#10;\" target=\"_blank\">\n        77.\n      </a>\n    4. Management and tooling\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-78-638.jpg?cb=1463070499\" title=\"4. Management and tooling&#10;\" target=\"_blank\">\n        78.\n      </a>\n    4. Management and tooling\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-79-638.jpg?cb=1463070499\" title=\"Securing JMX&#10;\" target=\"_blank\">\n        79.\n      </a>\n    Securing JMX\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-80-638.jpg?cb=1463070499\" title=\"nmap -Pn -p7199 &#10;-oG logs/cass.gnmap 54.88.0.0/14&#10;Always a ...\" target=\"_blank\">\n        80.\n      </a>\n    nmap -Pn -p7199 \n-oG logs/cass.gnmap 54.88.0.0/14\nAlways a few suckers that\nTL,DR'ed\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-81-638.jpg?cb=1463070499\" title=\"Why do I need to secure JMX?&#10;\" target=\"_blank\">\n        81.\n      </a>\n    Why do I need to secure JMX?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-82-638.jpg?cb=1463070499\" title=\"Works as Advertised!&#10;\" target=\"_blank\">\n        82.\n      </a>\n    Works as Advertised!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-83-638.jpg?cb=1463070499\" title=\"also&#10;good for&#10;some&#10;LOLs&#10;\" target=\"_blank\">\n        83.\n      </a>\n    also\ngood for\nsome\nLOLs\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-84-638.jpg?cb=1463070499\" title=\"Securing JMX&#10;SSL setup is like node to node and client to s...\" target=\"_blank\">\n        84.\n      </a>\n    Securing JMX\nSSL setup is like node to node and client to server\nhttp://docs.oracle.com/javase/8/docs/technotes/guides/management/agent.html\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-85-638.jpg?cb=1463070499\" title=\"Securing JMX&#10;JMX Authentication is straightforward&#10;and well...\" target=\"_blank\">\n        85.\n      </a>\n    Securing JMX\nJMX Authentication is straightforward\nand well documented\n$JAVA_HOME/jre/lib/management/jmxremote.access\n$JAVA_HOME/jre/lib/management/\njmxremote.password.template\nhttp://docs.oracle.com/javase/8/docs/technotes/guides/management/agent.html\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-86-638.jpg?cb=1463070499\" title=\"Securing JMX&#10;$JAVA_HOME/jre/lib/management/jmxremote.access...\" target=\"_blank\">\n        86.\n      </a>\n    Securing JMX\n$JAVA_HOME/jre/lib/management/jmxremote.access\n$JAVA_HOME/jre/lib/management/\njmxremote.password.template\nNow you can:\nnodetool -u admin -pw secret compactionstats\nhttp://docs.oracle.com/javase/8/docs/technotes/guides/management/agent.html\nJMX Authentication is straightforward\nand well documented\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-87-638.jpg?cb=1463070499\" title=\"Securing JMX&#10;$JAVA_HOME/jre/lib/management/jmxremote.access...\" target=\"_blank\">\n        87.\n      </a>\n    Securing JMX\n$JAVA_HOME/jre/lib/management/jmxremote.access\n$JAVA_HOME/jre/lib/management/\njmxremote.password.template\nNow you can:\nnodetool -u admin -pw secret compactionstats\nTip: -pwf option will read the password from a ﬁle\nhttp://docs.oracle.com/javase/8/docs/technotes/guides/management/agent.html\nJMX Authentication is straightforward\nand well documented\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-88-638.jpg?cb=1463070499\" title=\"Securing JMX&#10;$JAVA_HOME/jre/lib/management/jmxremote.access...\" target=\"_blank\">\n        88.\n      </a>\n    Securing JMX\n$JAVA_HOME/jre/lib/management/jmxremote.access\n$JAVA_HOME/jre/lib/management/\njmxremote.password.template\nNow you can:\nnodetool -u admin -pw secret compactionstats\nJMX Authentication is straightforward\nand well documented\nTHIS JUST IN!!!\nRBAC for JMX Authentication and Authorization\nhttps://issues.apache.org/jira/browse/CASSANDRA-10091\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756/95/securing-cassandra-for-compliance-89-638.jpg?cb=1463070499\" title=\"Thanks!@zznate&#10;\" target=\"_blank\">\n        89.\n      </a>\n    Thanks!@zznate\n \n  </li>\n              </ol></div></div></div><aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\"><dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\"><ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"61955222\" title=\"Learning PowerPoint 2016\" href=\"https://www.linkedin.com/learning/learning-powerpoint-2016-2?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Learning PowerPoint 2016\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Learning PowerPoint 2016\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=YMWoNmB8XClGw1SV61hXx8O8w1I%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lWCWq_dCfZHTpecDaZLSiol0ecSgClAI0fumvQzXnF469LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Learning PowerPoint 2016</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"61955222\" title=\"Teaching Techniques: Writing Effective Learning Objectives\" href=\"https://www.linkedin.com/learning/teaching-techniques-writing-effective-learning-objectives?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Teaching Techniques: Writing Effective Learning Objectives\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Teaching Techniques: Writing Effective Learning Objectives\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=8g9LchVQPqjokhBzWyb8MRYUQls%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lXySs-tCfZHPof8faZLSiol8QcS4DkAQ7feitRzXjEI69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Teaching Techniques: Writing Effective Learning Objectives</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"61955222\" title=\"How to Use LinkedIn Learning\" href=\"https://www.linkedin.com/learning/how-to-use-linkedin-learning?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_How to Use LinkedIn Learning\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"How to Use LinkedIn Learning\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=ZIeOnCTwwJ5EdO2pPeJkIjCzbPY%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lUiOj_tWfZH7vcMPfZLSiol4QfyoCkwY6feasQDHpGo69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>How to Use LinkedIn Learning</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"49978957\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Securing Cassandra The Right Way\" href=\"https://www.slideshare.net/planetcassandra/securing-cassandra-the-right-way\">\n    \n    <div class=\"related-content\"><p>Securing Cassandra The Right Way</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"116307595\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Webinar  |  Aligning GDPR Requirements with Today's Hybrid Cloud Realities\" href=\"https://www.slideshare.net/DataStax/webinar-aligning-gdpr-requirements-with-todays-hybrid-cloud-realities\">\n    \n    <div class=\"related-content\"><p>Webinar  |  Aligning GDPR Requirements with Today's Hybrid Cloud Realities</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"110183586\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Designing a Distributed Cloud Database for Dummies\" href=\"https://www.slideshare.net/DataStax/designing-a-distributed-cloud-database-for-dummies-110183586\">\n    \n    <div class=\"related-content\"><p>Designing a Distributed Cloud Database for Dummies</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"105785146\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"How to Power Innovation with Geo-Distributed Data Management in Hybrid Cloud\" href=\"https://www.slideshare.net/DataStax/data-staxwebinar-howtopower-innovationwithgeodistributed-datainhybridcloudfor-slideshare\">\n    \n    <div class=\"related-content\"><p>How to Power Innovation with Geo-Distributed Data Management in Hybrid Cloud</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"102824145\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"How to Evaluate Cloud Databases for eCommerce\" href=\"https://www.slideshare.net/DataStax/how-to-evaluate-cloud-databases-for-ecommerce\">\n    \n    <div class=\"related-content\"><p>How to Evaluate Cloud Databases for eCommerce</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"95288307\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Webinar: DataStax Enterprise 6: 10 Ways to Multiply the Power of Apache Cassandra™ Without the Complexity\" href=\"https://www.slideshare.net/DataStax/webinar-datastax-enterprise-6-10-ways-to-multiply-the-power-of-apache-cassandra-without-the-complexity-95288307\">\n    \n    <div class=\"related-content\"><p>Webinar: DataStax Enterprise 6: 10 Ways to Multiply the Power of Apache Cassa...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"93695860\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Webinar: DataStax and Microsoft Azure: Empowering the Right-Now Enterprise with Real-Time Apps at Cloud Scale\" href=\"https://www.slideshare.net/DataStax/webinar-datastax-and-microsoft-azure-empowering-the-rightnow-enterprise-with-realtime-apps-at-cloud-scale-93695860\">\n    \n    <div class=\"related-content\"><p>Webinar: DataStax and Microsoft Azure: Empowering the Right-Now Enterprise wi...</p><p>DataStax</p></div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div></div><footer>\n          <div class=\"row\"><div class=\"columns\"><ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div></div>\n          \n          <div class=\"row\"><div class=\"columns\"><p class=\"copyright text-center\">LinkedIn Corporation © 2018</p></div></div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div>\n    <div class=\"row\"><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></div>\n        \n    </div>\n    \n    \n      <noscript>\n    </noscript>",
        "created_at": "2018-09-24T19:46:58+0000",
        "updated_at": "2018-09-24T19:47:49+0000",
        "published_at": null,
        "published_by": [
          "DataStax"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 9,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/6fp2flfksggdgaw9cns7-signature-6be443b101acb53b501e9ea40050227feb88d3118800110aed35e280f1a44f0b-poli-160512160756-thumbnail-4.jpg?cb=1463070499",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12192"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 901,
            "label": "security",
            "slug": "security"
          }
        ],
        "is_public": false,
        "id": 12191,
        "uid": null,
        "title": "Securing Cassandra The Right Way",
        "url": "https://www.slideshare.net/planetcassandra/securing-cassandra-the-right-way",
        "content": "<div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div><div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><h4 class=\"modal-title\">Select another clipboard</h4><hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><h3>You just clipped your first slide!</h3>Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div><p><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></p></div>",
        "created_at": "2018-09-24T19:46:43+0000",
        "updated_at": "2018-10-18T16:12:18+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/cassandraandsecurity-150629224025-lva1-app6892-thumbnail-4.jpg?cb=1435617797",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12191"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 22,
            "label": "open.source",
            "slug": "open-source"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 995,
            "label": "testing",
            "slug": "testing"
          }
        ],
        "is_public": false,
        "id": 12190,
        "uid": null,
        "title": "intuit/wasabi",
        "url": "https://github.com/intuit/wasabi",
        "content": "<p><strong>Support:</strong> <a href=\"https://gitter.im/intuit/wasabi?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/13115004d1de8c8debb6d9636ed770ceb3cd44ea/68747470733a2f2f6261646765732e6769747465722e696d2f696e747569742f7761736162692e737667\" alt=\"Join the chat at https://gitter.im/intuit/wasabi\" data-canonical-src=\"https://badges.gitter.im/intuit/wasabi.svg\" /></a> <br /><strong>Documentation:</strong> <a href=\"https://intuit.github.io/wasabi/v1/guide/index.html\" rel=\"nofollow\">User Guide</a>, <a href=\"https://intuit.github.io/wasabi/v1/javadocs/latest/\" rel=\"nofollow\">JavaDocs</a>\n<br /><strong>A/B Testing Overview:</strong> <a href=\"https://www.youtube.com/watch?v=_HtvJwBPUqk&amp;feature=youtu.be\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/7d0321a6112c23f202f9852f1db1d4587db17d85/687474703a2f2f696d672e736869656c64732e696f2f62616467652f766964656f2d412532464225323054657374696e672532304f766572766965772d7265642e737667\" alt=\"A/B Testing Overview\" data-canonical-src=\"http://img.shields.io/badge/video-A%2FB%20Testing%20Overview-red.svg\" /></a> <a href=\"https://medium.com/blueprint-by-intuit/open-sourcing-wasabi-the-a-b-testing-platform-by-intuit-a8d5abc958d\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/31955dbe1b8474ed4a4ab59e38a26e05b0ac0622/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f626c6f672d4d6565742532305761736162692d627269676874677265656e2e737667\" alt=\"Blog Meet Wasabi\" data-canonical-src=\"https://img.shields.io/badge/blog-Meet%20Wasabi-brightgreen.svg\" /></a> <a href=\"https://medium.com/blueprint-by-intuit/the-architecture-behind-wasabi-an-open-source-a-b-testing-platform-b52430d3fd80\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/f1f2ee9250c9a93017c1c82c4dc7fa0013d61401/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f626c6f672d417263686974656374757265253230426568696e642532305761736162692d6f72616e67652e737667\" alt=\"Blog Architecture Behind Wasabi\" data-canonical-src=\"https://img.shields.io/badge/blog-Architecture%20Behind%20Wasabi-orange.svg\" /></a> <br /><strong>Continuous Integration:</strong> <a href=\"https://travis-ci.org/intuit/wasabi\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/c02cc709a1c51c0107101c3d1a96e28b7f10dd1c/68747470733a2f2f6170692e7472617669732d63692e6f72672f696e747569742f7761736162692e7376673f6272616e63683d646576656c6f70\" alt=\"Build Status\" data-canonical-src=\"https://api.travis-ci.org/intuit/wasabi.svg?branch=develop\" /></a>\n<a href=\"https://coveralls.io/github/intuit/wasabi?branch=develop\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/21a3f18f699685c7ac4db1f687158a9bcfe1470f/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f696e747569742f7761736162692f62616467652e737667\" alt=\"Coverage Status\" data-canonical-src=\"https://coveralls.io/repos/github/intuit/wasabi/badge.svg\" /></a>\n<a href=\"https://maven-badges.herokuapp.com/maven-central/com.intuit.wasabi/wasabi\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/6da12cf851ba62b357771ca25a0e3561339330c5/68747470733a2f2f6d6176656e2d6261646765732e6865726f6b756170702e636f6d2f6d6176656e2d63656e7472616c2f636f6d2e696e747569742e7761736162692f7761736162692f62616467652e737667\" alt=\"Maven Central\" data-canonical-src=\"https://maven-badges.herokuapp.com/maven-central/com.intuit.wasabi/wasabi/badge.svg\" /></a> <br /><strong>License:</strong> <a href=\"http://www.apache.org/licenses/LICENSE-2.0\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/cb7ed4d28c8df19eb3b269fe0cad477d17621500/687474703a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d417061636865253230322d627269676874677265656e2e737667\" alt=\"Apache 2\" data-canonical-src=\"http://img.shields.io/badge/license-Apache%202-brightgreen.svg\" /></a> <br /></p><h2>Project</h2><p>Wasabi A/B Testing Service is a real-time, enterprise-grade, 100% API driven project. Users are empowered to own their own data, and run experiments across web, mobile, and desktop. It’s fast, easy to use, it’s chock full of features, and instrumentation is minimal.</p><p>Learn more about how Wasabi can empower your team to move from hunches to actionable, data-driven user insights with our simple, flexible, and scalable experimentation platform.</p><h3>Features</h3><ul><li><strong>Own your own data</strong> - Wasabi runs on your servers, in the cloud or on-premise, where you have complete control over your data.</li>\n<li><strong>Proven</strong> - Wasabi is battle-tested in production at Intuit, Inc., a financial technology company. Wasabi is the experimentation platform for TurboTax, QuickBooks, Mint.com, and other Intuit offerings.</li>\n<li><strong>High Performance</strong> - Consistent server-side response times for user assignments within 30ms.</li>\n<li><strong>100% API-Driven</strong> - The Wasabi REST API is compatible with any language and environment.</li>\n<li><strong>Platform Agnostic</strong> - Uniform, consistent testing across Web, mobile, desktop. Also supports front-end, back-end integrations.</li>\n<li><strong>Real-time user assignments</strong> - Assign users into experiments in real time, to preserve traffic for other parallel A/B tests.</li>\n<li><strong>Cloud and on-premise</strong> - Designed to live in the cloud or in your own data center.</li>\n<li><strong>Analytics</strong> - Core experiment analytics and metrics visualization out of the box, as well as the ability to send data to your existing analytics infrastructure.</li>\n<li><strong>Pluggable</strong> - Well-defined interfaces for plugging in your own access control, sending data to data pipelines, and providing fully custom bucket allocations.</li>\n<li><strong>Experiment Management UI</strong> - Setup and manage experiments via a modern Web interface. Management via REST API is also possible.</li>\n<li><strong>Dockerized</strong> - Spin up a Wasabi Docker instance in 5 minutes and be in production with the platform, instrumentation, and experiments within a day.</li>\n</ul><h3>User Interface</h3><ul><li><strong>Create an experiment and its buckets:</strong>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://camo.githubusercontent.com/2e4a1b3b58c231a9b2ee753131d28b8982f751c5/68747470733a2f2f696e747569742e6769746875622e696f2f7761736162692f76312f67756964652f696d616765732f726561646d652f4372656174654275636b65742e706e67\"><img src=\"https://camo.githubusercontent.com/2e4a1b3b58c231a9b2ee753131d28b8982f751c5/68747470733a2f2f696e747569742e6769746875622e696f2f7761736162692f76312f67756964652f696d616765732f726561646d652f4372656174654275636b65742e706e67\" alt=\"\" data-canonical-src=\"https://intuit.github.io/wasabi/v1/guide/images/readme/CreateBucket.png\" /></a></li>\n<li><strong>Filter which customers are considered for your experiment:</strong>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://camo.githubusercontent.com/86693dfd1865dc803f40a0a61786a1f573903968/68747470733a2f2f696e747569742e6769746875622e696f2f7761736162692f76312f67756964652f696d616765732f726561646d652f5365676d656e746174696f6e52756c65732e706e67\"><img src=\"https://camo.githubusercontent.com/86693dfd1865dc803f40a0a61786a1f573903968/68747470733a2f2f696e747569742e6769746875622e696f2f7761736162692f76312f67756964652f696d616765732f726561646d652f5365676d656e746174696f6e52756c65732e706e67\" alt=\"\" data-canonical-src=\"https://intuit.github.io/wasabi/v1/guide/images/readme/SegmentationRules.png\" /></a></li>\n<li><strong>Follow your currently running experiments:</strong>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://camo.githubusercontent.com/f0372ebe376391b70abe344675870d8830e32621/68747470733a2f2f696e747569742e6769746875622e696f2f7761736162692f76312f67756964652f696d616765732f726561646d652f4578706572696d656e744c6973742e706e67\"><img src=\"https://camo.githubusercontent.com/f0372ebe376391b70abe344675870d8830e32621/68747470733a2f2f696e747569742e6769746875622e696f2f7761736162692f76312f67756964652f696d616765732f726561646d652f4578706572696d656e744c6973742e706e67\" alt=\"\" data-canonical-src=\"https://intuit.github.io/wasabi/v1/guide/images/readme/ExperimentList.png\" /></a></li>\n<li><strong>Track your experiment results in real-time:</strong>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://camo.githubusercontent.com/26570e8c8a76366b25111ee208473c5f85b057ee/68747470733a2f2f696e747569742e6769746875622e696f2f7761736162692f76312f67756964652f696d616765732f726561646d652f4578706572696d656e7444657461696c732e706e67\"><img src=\"https://camo.githubusercontent.com/26570e8c8a76366b25111ee208473c5f85b057ee/68747470733a2f2f696e747569742e6769746875622e696f2f7761736162692f76312f67756964652f696d616765732f726561646d652f4578706572696d656e7444657461696c732e706e67\" alt=\"\" data-canonical-src=\"https://intuit.github.io/wasabi/v1/guide/images/readme/ExperimentDetails.png\" /></a></li>\n</ul><h2>Get Started</h2><p>The following steps will help you install the needed tools, then build and run a complete Wasabi stack.</p><h4>Bootstrap Your Environment</h4><h5>Mac OS</h5><div class=\"highlight highlight-source-shell\"><pre>% /usr/bin/ruby \\\n  -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n% brew install git\n% git clone https://github.com/intuit/wasabi.git\n% cd wasabi\n% ./bin/wasabi.sh bootstrap</pre></div><p>Installed tools include: <a href=\"http://brew.sh\" rel=\"nofollow\">homebrew 0.9</a>, <a href=\"https://git-scm.com\" rel=\"nofollow\">git 2</a>,\n<a href=\"https://maven.apache.org\" rel=\"nofollow\">maven 3</a>, <a href=\"http://www.oracle.com/technetwork/java/javase/overview/index.html\" rel=\"nofollow\">java 1.8</a>,\n<a href=\"https://docker.com\" rel=\"nofollow\">docker 1.12</a>, <a href=\"https://nodejs.org/en\" rel=\"nofollow\">node 6</a> and <a href=\"https://www.python.org\" rel=\"nofollow\">python 2.7</a>.</p><h5>Ubuntu</h5><p>Bootstrapping on Ubuntu requires sudo privileges to install all the required dependencies. You will be prompted to enter your password. Currently only 16.04 (x64) is supported.</p><div class=\"highlight highlight-source-shell\"><pre>% sudo apt-get install git\n% git clone https://github.com/intuit/wasabi.git\n% cd wasabi\n% ./bin/wasabi.sh bootstrap\n% sudo reboot</pre></div><p>NOTE: A reboot is required after running the bootstrap command on Ubuntu.</p><p>For all other processes (build, start etc.) the commands are same for Ubuntu and Mac OS.</p><p>Installed tools include: <a href=\"https://git-scm.com\" rel=\"nofollow\">git 2</a>,\n<a href=\"https://maven.apache.org\" rel=\"nofollow\">maven 3</a>, <a href=\"http://openjdk.java.net/projects/jdk8/\" rel=\"nofollow\">OpenJdk 8</a>,\n<a href=\"https://docker.com\" rel=\"nofollow\">docker 1.12</a>, <a href=\"https://nodejs.org/en\" rel=\"nofollow\">node 6</a> and <a href=\"https://www.python.org\" rel=\"nofollow\">python 2.7</a></p><p>Similar tooling will work for Windows. Contribute a patch :)</p><h4>Start Wasabi</h4><p>Now that we have the necessary tools in place, let's move on to build and start Wasabi, followed by issuing a <em>ping</em>\ncommand to verify the build:</p><div class=\"highlight highlight-source-shell\"><pre>% ./bin/wasabi.sh build start\n...\nwasabi is operational:\n  ui: % open http://localhost:8080     note: sign in as admin/admin\n  ping: % curl -i http://localhost:8080/api/v1/ping\n  debug: attach to localhost:8180\n% curl -i http://localhost:8080/api/v1/ping\nHTTP/1.1 200 OK\nDate: Wed, 25 May 2016 00:25:47 GMT\n...\nX-Application-Id: wasabi-api-20151215171929-SNAPSHOT-development\nContent-Type: application/json\nTransfer-Encoding: chunked\nServer: Jetty(9.3.z-SNAPSHOT)\n{\n  \"componentHealths\":[\n    {\n      \"componentName\":\"Experiments Cassandra\",\n      \"healthy\":true\n    },\n    {\n      \"componentName\":\"MySql\",\"healthy\":true\n    }\n  ],\n  \"wasabiVersion\":\"wasabi-api-20151215171929-SNAPSHOT-development\"\n}</pre></div><p>Congratulations! You are the proud owner of a newly minted Wasabi instance. :)</p><h3>Running Wasabi with remote storage</h3><h5>Set Mysql and Cassandra credentials</h5><ul><li>Modify /pom.xml to set the values that apply to your environment</li>\n</ul><h5>Download Cassandra migration tool <a href=\"https://oss.sonatype.org/content/repositories/public/com/builtamont/cassandra-migration/0.9/cassandra-migration-0.9-jar-with-dependencies.jar\" rel=\"nofollow\">https://oss.sonatype.org/content/repositories/public/com/builtamont/cassandra-migration/0.9/cassandra-migration-0.9-jar-with-dependencies.jar</a></h5><h5>Set up your environment variables</h5><ul><li>Set location of the migration tool</li>\n</ul><div class=\"highlight highlight-source-shell\"><pre>export CASSANDRA_MIGRATION=/location/of/cassandra-migration-0.9-jar-with-dependencies.jar</pre></div><ul><li>Set location of migration scripts within your project</li>\n</ul><div class=\"highlight highlight-source-shell\"><pre>export MIGRATION_SCRIPT=/location/of/modules/repository-datastax/src/main/resources/com/intuit/wasabi/repository/impl/cassandra/migration</pre></div><h5>Set up Cassandra tables</h5><div class=\"highlight highlight-source-shell\"><pre>CQLSH_VERSION=&lt;version&gt; CQLSH_USERNAME=&lt;username&gt; CQLSH_PASSWORD=&lt;pwd&gt; CQLSH_HOST=&lt;host&gt; bin/docker/migration.sh</pre></div><h5>Run Wasabi with env variables for remote storage hosts</h5><div class=\"highlight highlight-source-shell\"><pre>MYSQL_HOST=&lt;mysql_host&gt; NODE_HOST=&lt;cassandra_host&gt; ./bin/wasabi.sh start:wasabi</pre></div><h5>Run Wasabi outside of docker with WASABI_CONFIGURATION for remote storage hosts</h5><div class=\"highlight highlight-source-shell\"><pre>WASABI_CONFIGURATION=\"\n  -Ddatabase.url.host=$MYSQL_HOST\\\n  -Ddatabase.url.port=$MYSQL_PORT\\\n  -Ddatabase.url.dbname=$MYSQL_DATABASE\\\n  -Ddatabase.user=$MYSQL_USER\\\n  -Ddatabase.password=$MYSQL_PASSWORD\\\n  -Ddatabase.pool.connections.min=$MYSQL_MIN_CONNECTIONS\\\n  -Ddatabase.pool.connections.max=$MYSQL_MAX_CONNECTIONS\\\n  -Dusername=$CASSANDRA_USER\\\n  -Dpassword=$CASSANDRA_PASSWORD\\\n  -DnodeHosts=$CASSANDRA_HOST\\\n  -DtokenAwareLoadBalancingLocalDC=$CASSANDRA_DATACENTER\\\n  -Dapplication.http.port=$PORT\" bash usr/local/wasabi-main-*/bin/run</pre></div><h4>Troubleshooting</h4><ul><li>\n<p>While starting Wasabi, if you see an error when the docker containers are starting up, you could do the following:</p>\n<ul><li>Look at the current docker containers that have been successfully started.</li>\n</ul></li>\n</ul><div class=\"highlight highlight-source-shell\"><pre>% ./bin/wasabi.sh status\nCONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS              PORTS                                                                     NAMES\n8c12458057ef        wasabi-main              \"entrypoint.sh wasabi\"   25 minutes ago      Up 25 minutes       0.0.0.0:8080-&gt;8080/tcp, 0.0.0.0:8090-&gt;8090/tcp, 0.0.0.0:8180-&gt;8180/tcp    wasabi-main\n979ecc885239        mysql:5.6                \"docker-entrypoint.sh\"   26 minutes ago      Up 26 minutes       0.0.0.0:3306-&gt;3306/tcp                                                    wasabi-mysql\n2d33a96abdcb        cassandra:2.1            \"/docker-entrypoint.s\"   27 minutes ago      Up 27 minutes       7000-7001/tcp, 0.0.0.0:9042-&gt;9042/tcp, 7199/tcp, 0.0.0.0:9160-&gt;9160/tcp   wasabi-cassandra</pre></div><ul><li>The above shell output shows a successful start of 3 docker containers needed by Wasabi: wasabi-main (the Wasabi server),\nwasabi-mysql, and wasabi-cassandra. If any of these are not running, try starting them individually. For example, if the\nMySQL container is running, but Cassandra and Wasabi containers failed to start (perhaps due to a network timeout docker\ncould not download the Cassandra image), do the following:</li>\n</ul><div class=\"highlight highlight-source-shell\"><pre>% ./bin/wasabi.sh start:cassandra\n% ./bin/wasabi.sh start:wasabi</pre></div><h4>Call Wasabi</h4><p>These are the 3 common REST endpoints that you will use to instrument your client application with Wasabi.</p><p>Let's assume that you've created and started an experiment, 'BuyButton,' in the 'Demo_App' application with the following buckets:</p><ul><li>'BucketA': green button (control bucket)</li>\n<li>'BucketB': orange button bucket</li>\n</ul><p>You can assign a user with a unique ID (e.g. 'userID1') to the experiment by making this HTTP request:</p><blockquote>\n<p>Assign a user to experiment and bucket:</p>\n</blockquote><div class=\"highlight highlight-source-shell\"><pre>% curl -H \"Content-Type: application/json\" \\\n    http://localhost:8080/api/v1/assignments/applications/Demo_App/experiments/BuyButton/users/userID1\n{  \n   \"cache\":true,\n   \"payload\":\"green\",\n   \"assignment\":\"BucketA\",\n   \"context\":\"PROD\",\n   \"status\":\"NEW_ASSIGNMENT\"\n}</pre></div><p>Now the 'userID1' user is assigned into the 'BucketA' bucket. Let's further record an impression, meaning the user has seen a given experience:</p><blockquote>\n<p>Record an impression:</p>\n</blockquote><div class=\"highlight highlight-source-shell\"><pre>% curl -H \"Content-Type: application/json\" \\\n    -d \"{\\\"events\\\":[{\\\"name\\\":\\\"IMPRESSION\\\"}]}\" \\\n    http://localhost:8080/api/v1/events/applications/Demo_App/experiments/BuyButton/users/userID1</pre></div><p>If the 'userID1' user performs an action such as clicking the Buy button, you'd record that action with the following request:</p><blockquote>\n<p>Record an action:</p>\n</blockquote><div class=\"highlight highlight-source-shell\"><pre>% curl -H \"Content-Type: application/json\" \\\n    -d \"{\\\"events\\\":[{\\\"name\\\":\\\"BuyClicked\\\"}]}\" \\\n    http://localhost:8080/api/v1/events/applications/Demo_App/experiments/BuyButton/users/userID1</pre></div><h4>Explore Various Resources</h4><p>The following developer resources are available:</p><blockquote>\n<p>API: Swagger API playground</p>\n</blockquote><div class=\"highlight highlight-source-shell\"><pre>% ./bin/wasabi.sh resource:api</pre></div><blockquote>\n<p>Javadoc</p>\n</blockquote><div class=\"highlight highlight-source-shell\"><pre>% ./bin/wasabi.sh resource:doc</pre></div><blockquote>\n<p>Wasabi UI</p>\n</blockquote><div class=\"highlight highlight-source-shell\"><pre>% ./bin/wasabi.sh resource:ui</pre></div><blockquote>\n<p>Cassandra: cqlsh shell</p>\n</blockquote><div class=\"highlight highlight-source-shell\"><pre>% ./bin/wasabi.sh resource:cassandra</pre></div><blockquote>\n<p>MySQL: mysql shell</p>\n</blockquote><div class=\"highlight highlight-source-shell\"><pre>% ./bin/wasabi.sh resource:mysql</pre></div><blockquote>\n<p>Java Debugger: Remote attach configuration</p>\n</blockquote><div class=\"highlight highlight-source-shell\"><pre>-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=8180</pre></div><h4>Stop Wasabi</h4><p>Alas, all good things must come to an end. Let's clean things up a bit stop the newly created Wasabi stack:</p><p>At this point in time, we now have all the requisite tools installed, and subsequent invocations of Wasabi will\nstart up much more quickly.</p><div class=\"highlight highlight-source-shell\"><pre>% ./bin/wasabi.sh stop</pre></div><h4>Get Familiar with wasabi.sh</h4><p>Further, there are a number of additional wasabi.sh options available you should become familiar with:</p><div class=\"highlight highlight-source-shell\"><pre>% ./bin/wasabi.sh --help\n  usage: wasabi.sh [options] [commands]\n  options:\n    -e | --endpoint [ host:port ]          : api endpoint; default: localhost:8080\n    -v | --verify [ true | false ]         : verify installation configuration; default: false\n    -s | --sleep [ sleep-time ]            : sleep/wait time in seconds; default: 30\n    -h | --help                            : help message\n  commands:\n    bootstrap                              : install dependencies\n    build                                  : build project\n    start[:cassandra,mysql,wasabi]         : start all, cassandra, mysql, wasabi\n    test                                   : test wasabi\n    stop[:wasabi,cassandra,mysql]          : stop all, wasabi, cassandra, mysql\n    resource[:ui,api,doc,cassandra,mysql]  : open resource api, javadoc, cassandra, mysql\n    status                                 : display resource status\n    remove[:wasabi,cassandra,mysql]        : remove all, wasabi, cassandra, mysql\n    package                                : build deployable packages\n    release[:start,finish]                 : promote release</pre></div><h2>Develop</h2><h4>Build and Run Wasabi Server</h4><div class=\"highlight highlight-source-shell\"><pre>% mvn package\n% ./bin/wasabi.sh start:cassandra,mysql\n% (cd modules/main/target; \\\n    WASABI_CONFIGURATION=\"-DnodeHosts=localhost -Ddatabase.url.host=localhost\" ./wasabi-main-*-SNAPSHOT-development/bin/run) &amp;\n% curl -i http://localhost:8080/api/v1/ping\n...</pre></div><p>The runtime logs can be accessed executing the following command in a another shell:</p><blockquote>\n<p>Viewing runtime logs:</p>\n</blockquote><div class=\"highlight highlight-source-shell\"><pre>% tail -f modules/main/target/wasabi-main-*-SNAPSHOT-development/logs/wasabi-main-*-SNAPSHOT-development.log</pre></div><h4>Build and Run Wasabi UI</h4><div class=\"highlight highlight-source-shell\"><pre>% cd modules/ui\n% grunt build</pre></div><div class=\"highlight highlight-source-shell\"><pre>% grunt serve</pre></div><h4>Stop Wasabi</h4><div class=\"highlight highlight-source-shell\"><pre>% ./bin/wasabi.sh stop</pre></div><p>Now while that was fun, in all likelihood you will be using an IDE to work on Wasabi. In doing so, you need only\nadd the configuration information above to the JVM commandline prior to startup:</p><blockquote>\n<p>Wasabi runtime configuration:</p>\n</blockquote><div class=\"highlight highlight-source-shell\"><pre>-DnodeHosts=localhost -Ddatabase.url.host=localhost</pre></div><h4>Run Integration Tests</h4><p>Code changes can readily be verified by running the growing collection of included integration tests:</p><div class=\"highlight highlight-source-shell\"><pre>% ./bin/wasabi.sh start test stop</pre></div><h5>Troubleshooting</h5><p>Integration tests might fail intermittently due to a time drift issue in docker containers on Mac OSX.</p><p>When the Mac sleeps and wakes back up, there is a lag created between the clock in the Mac vs the\nrunning docker containers. This is a known issue in Docker for Mac.</p><p>This can be fixed by running the following command:</p><div class=\"highlight highlight-source-shell\"><pre>% docker run --rm --privileged alpine hwclock -s</pre></div><p>The above command will need to be run every time when there is a time drift.</p><p>To automatically run this command and update the time each time the Mac wakes up, you could install\nthe following agent:</p><div class=\"highlight highlight-source-shell\"><pre>% curl https://raw.githubusercontent.com/arunvelsriram/docker-time-sync-agent/master/install.sh | bash</pre></div><p>You can read more about this at: <a href=\"https://blog.shameerc.com/2017/03/quick-tip-fixing-time-drift-issue-on-docker-for-mac\" rel=\"nofollow\">quick-tip-fixing-time-drift-issue-on-docker-for-mac</a></p><h2>Package and Deploy at Scale</h2><p>Wasabi can readily be packaged as installable <em>rpm</em> or <em>deb</em> distributions and deployed at scale as follows:</p><blockquote>\n<p>Package by running integration tests 1st:</p>\n</blockquote><div class=\"highlight highlight-source-shell\"><pre>% ./bin/wasabi.sh start package</pre></div><blockquote>\n<p>Package without integration tests, if needed:</p>\n</blockquote><div class=\"highlight highlight-source-shell\"><pre>% ./bin/wasabi.sh -t false package</pre></div><blockquote>\n<p>Find generated package files:</p>\n</blockquote><div class=\"highlight highlight-source-shell\"><pre>% find . -type f \\( -name \"*.rpm\" -or -name \"*.deb\" \\)</pre></div><p>Note: <a href=\"http://www.oracle.com/technetwork/java/javase/overview/index.html\" rel=\"nofollow\">Java 8</a> is a runtime dependency</p><h2>Integrate</h2><p>Wasabi is readily embeddable via the following <em>maven</em> dependency GAV family:</p><div class=\"highlight highlight-text-xml\"><pre>&lt;dependency&gt;\n    &lt;groupId&gt;com.intuit.wasabi&lt;/groupId&gt;\n    &lt;artifactId&gt;wasabi&lt;/artifactId&gt;\n    &lt;version&gt;1.0.20160627213750&lt;build_timestamp&gt;&lt;/version&gt;\n&lt;/dependency&gt;</pre></div><h2>Contribute</h2><p>We greatly encourage contributions! You can add new features, report and fix existing bugs, write docs and\ntutorials, or any of the above. Feel free to open issues and/or send pull requests.</p><p>The <code>master</code> branch of this repository contains the latest stable release of Wasabi, while snapshots are published to the <code>develop</code> branch. In general, pull requests should be submitted against <code>develop</code> by forking this repo into your account, developing and testing your changes, and creating pull requests to request merges. See the <a href=\"https://guides.github.com/activities/contributing-to-open-source/\">Contributing to a Project</a>\narticle for more details about how to contribute in general and find more specific information on how to write code for Wasabi in our <a href=\"https://intuit.github.io/wasabi/v1/guide/index.html#developing-wasabi\" rel=\"nofollow\">user guide</a>.</p><p>Extension projects such as browser plugins, client integration libraries, and apps can be contributed under the <code>contrib</code> directory.</p><p>Steps to contribute:</p><ol><li>Fork this repository into your account on Github</li>\n<li>Clone <em>your forked repository</em> (not our original one) to your hard drive with <code>git clone https://github.com/YOURUSERNAME/wasabi.git</code></li>\n<li>Design and develop your changes</li>\n<li>Add/update unit tests</li>\n<li>Add/update integration tests</li>\n<li>Add/update documentation on <code>gh-pages</code> branch</li>\n<li>Create a pull request for review to request merge</li>\n<li>Obtain 2 approval <em>squirrels</em> before your changes can be merged</li>\n</ol><p>Thank you for your contribution!</p>",
        "created_at": "2018-09-24T16:18:17+0000",
        "updated_at": "2018-10-29T19:26:22+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 10,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/2495066?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12190"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          }
        ],
        "is_public": false,
        "id": 12187,
        "uid": null,
        "title": "Interpreting Cassandra repair logs and leveraging the OpsCenter repair service",
        "url": "https://www.datastax.com/dev/blog/interpreting-repair-logs",
        "content": "<section class=\"post-content\"><p>Cassandra repairs consist of comparing data from between replica nodes, identifying inconsistencies, and streaming the latest value for mismatched data. We can't compare an entire cassandra database value by value so we create <a href=\"https://en.wikipedia.org/wiki/Merkle_tree\">Merkle trees</a> to identify inconsistencies and then we stream them.</p><p>Repairs are expensive; CPU is needed to generate the Merkle trees and networking / io is needed to stream missing data. Usually repairs also trigger lots of compactions if they have not been run for a while (especially when there's a lot of inconsistent data and leveled / date tiered compaction strategies are being used).</p><p>The OpsCenter repair service splits up the repair job into lots of little slices (256 per table) and runs them around the clock, turning a heavy, manual, weekly operation into an automatic, constant job. Clusters will see higher consistent cpu utilization / load when the repair service is on instead of a big spike once per week.</p><h2 id=\"stepsinarepair\">Steps in a repair</h2><p>Each repair session will be identified by a UUID (for example #0d4544b0-8fc9-11e5-a498-4b9679ec178d). The following are the logs from a healthy repair. Notice all the messages are INFO messages, there are no WARN or ERROR.</p><p>Repair <code>sessions</code> have repair <code>jobs</code> and <code>jobs</code> which occur for each table in the session.</p><p>Repeat for every table:<br />1) <code>RepairJob.java</code> Request merkle trees<br />2) <code>RepairSession.java</code> Receive merkle trees<br />3) <code>Differencer.java</code> Check for inconsistencies<br />4) <em>Optional</em><code>StreamingRepairTask.java</code> Stream differences--if any<br />5) <code>RepairSession.java</code> is fully synced<br />6) <code>StorageService.java</code> Repair session for range (,] finished</p><h2 id=\"summarizingrepairlogscleanrun\">Summarizing repair logs - clean run</h2><p>To group the tasks use the following bash foo:</p><pre>$ cat system.log| grep \"11ff9870-8fc9-11e5-a498-4b9679ec178d\" | sed -E 's/([0-9]{1,3}\\.){3}[0-9]{1,3}/Source/'|sed -E 's/([0-9]{1,3}\\.){3}[0-9]{1,3}/Target/' | awk '{ split($3,a,\":\"); $2=a[0] ; $3=\"\"; $4=\"\"; print }'|uniq -c&#13;\n</pre><p>In this case there was no streaming and all the jobs complete successfully for the range.</p><pre>   1 INFO    RepairSession.java:260 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] new session: will sync /Source, /Target1, /Target2, ...... on range (4393112290973329820,4394202908924102592] for OpsCenter.[rollups86400, events_timeline, rollups7200, events, bestpractice_results, backup_reports, settings, rollups60, rollups300, pdps]&#13;\n   1 INFO    RepairJob.java:163 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] requesting merkle trees for rollups86400 (to [/Source, /Target, ...])&#13;\n   9 INFO    RepairSession.java:171 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] Received merkle tree for rollups86400 from /Source&#13;\n  36 INFO    Differencer.java:67 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] Endpoints /Source and /Target are consistent for rollups86400&#13;\n   1 INFO    RepairSession.java:237 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] rollups86400 is fully synced&#13;\n   1 INFO    RepairJob.java:163 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] requesting merkle trees for events_timeline (to [/Source, /Target, ... ])&#13;\n   9 INFO    RepairSession.java:171 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] Received merkle tree for events_timeline from /Source&#13;\n  36 INFO    Differencer.java:67 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] Endpoints /Source and /Target are consistent for events_timeline&#13;\n   1 INFO    RepairSession.java:237 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] events_timeline is fully synced&#13;\n   1 INFO    RepairJob.java:163 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] requesting merkle trees for rollups7200 (to [/Source, /Target, ... ])&#13;\n   9 INFO    RepairSession.java:171 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] Received merkle tree for rollups7200 from /Source&#13;\n  36 INFO    Differencer.java:67 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] Endpoints /Source and /Target are consistent for rollups7200&#13;\n   1 INFO    RepairSession.java:237 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] rollups7200 is fully synced&#13;\n   1 INFO    RepairJob.java:163 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] requesting merkle trees for events (to [/Source, /Target,  ... ])&#13;\n   9 INFO    RepairSession.java:171 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] Received merkle tree for events from /Source&#13;\n  36 INFO    Differencer.java:67 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] Endpoints /Source and /Target are consistent for events&#13;\n   1 INFO    RepairSession.java:237 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] events is fully synced&#13;\n   1 INFO    RepairJob.java:163 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] requesting merkle trees for bestpractice_results (to [/Source, /Target,  ... ])&#13;\n   9 INFO    RepairSession.java:171 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] Received merkle tree for bestpractice_results from /Source&#13;\n  36 INFO    Differencer.java:67 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] Endpoints /Source and /Target are consistent for bestpractice_results&#13;\n   1 INFO    RepairSession.java:237 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] bestpractice_results is fully synced&#13;\n   1 INFO    RepairJob.java:163 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] requesting merkle trees for backup_reports (to [/Source, /Target,  ... ])&#13;\n   9 INFO    RepairSession.java:171 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] Received merkle tree for backup_reports from /Source&#13;\n  36 INFO    Differencer.java:67 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] Endpoints /Source and /Target are consistent for backup_reports&#13;\n   1 INFO    RepairSession.java:237 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] backup_reports is fully synced&#13;\n   1 INFO    RepairJob.java:163 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] requesting merkle trees for settings (to [/Source, /Target,  ... ])&#13;\n   9 INFO    RepairSession.java:171 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] Received merkle tree for settings from /Source&#13;\n  36 INFO    Differencer.java:67 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] Endpoints /Source and /Target are consistent for settings&#13;\n   1 INFO    RepairSession.java:237 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] settings is fully synced&#13;\n   1 INFO    RepairJob.java:163 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] requesting merkle trees for rollups60 (to [/Source, /Target,  ... ])&#13;\n   9 INFO    RepairSession.java:171 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] Received merkle tree for rollups60 from /Source&#13;\n  36 INFO    Differencer.java:67 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] Endpoints /Source and /Target are consistent for rollups60&#13;\n   1 INFO    RepairSession.java:237 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] rollups60 is fully synced&#13;\n   1 INFO    RepairJob.java:163 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] requesting merkle trees for rollups300 (to [/Source, /Target,  ... ])&#13;\n   9 INFO    RepairSession.java:171 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] Received merkle tree for rollups300 from /Source&#13;\n  36 INFO    Differencer.java:67 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] Endpoints /Source and /Target are consistent for rollups300&#13;\n   1 INFO    RepairSession.java:237 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] rollups300 is fully synced&#13;\n   1 INFO    RepairJob.java:163 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] requesting merkle trees for pdps (to [/Source, /Target,  ... ])&#13;\n   9 INFO    RepairSession.java:171 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] Received merkle tree for pdps from /Source&#13;\n  36 INFO    Differencer.java:67 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] Endpoints /Source and /Target are consistent for pdps&#13;\n   1 INFO    RepairSession.java:237 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] pdps is fully synced&#13;\n   1 INFO    RepairSession.java:299 - [repair #11ff9870-8fc9-11e5-a498-4b9679ec178d] session completed successfully&#13;\n   1 INFO    StorageService.java:3001 - Repair session 11ff9870-8fc9-11e5-a498-4b9679ec178d for range (4393112290973329820,4394202908924102592] finished&#13;\n</pre><h2 id=\"summarizingrepairlogserrors\">Summarizing repair logs - errors</h2><p>Now let's look at a repair session with some errors.</p><pre>$ cat system.log| grep \"0fb1b0d0-8fc9-11e5-a498-4b9679ec178d\" | sed -E 's/([0-9]{1,3}\\.){3}[0-9]{1,3}/Source/'|sed -E 's/([0-9]{1,3}\\.){3}[0-9]{1,3}/Target/' | awk '{ split($3,a,\":\"); $2=a[0] ; $3=\"\"; $4=\"\"; print }'|uniq -c&#13;\n</pre><pre>   1 INFO    RepairSession.java:260 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] new session: will sync /Source, /Target, ... on range (4393112290973329820,4394202908924102592] for keyspace.[table1, table2, ...]&#13;\n   1 INFO    RepairJob.java:163 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] requesting merkle trees for table1 (to [/Source, /Target, ...])&#13;\n   9 INFO    RepairSession.java:171 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Received merkle tree for table1 from /Source&#13;\n  36 INFO    Differencer.java:67 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Endpoints /Source and /Target are consistent for datasources&#13;\n   1 INFO    RepairSession.java:237 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] datasources is fully synced&#13;\n   1 INFO    RepairJob.java:163 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] requesting merkle trees for table1 (to [/Source, /Target, ...])&#13;\n   9 INFO    RepairSession.java:171 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Received merkle tree for table1 from /Source&#13;\n  36 INFO    Differencer.java:67 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Endpoints /Source and /Target are consistent for tablex_error&#13;\n   1 INFO    RepairSession.java:237 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] tablex_error is fully synced&#13;\n   1 INFO    RepairJob.java:163 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] requesting merkle trees for tablex (to [/Source, /Target, ])&#13;\n   9 INFO    RepairSession.java:171 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Received merkle tree for tablex from /Source&#13;\n   9 INFO    Differencer.java:67 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Endpoints /Source and /Target are consistent for tablex&#13;\n   2 INFO    Differencer.java:74 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Endpoints /Source and /Target have 1 range(s) out of sync for tablex&#13;\n   1 INFO    Differencer.java:67 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Endpoints /Source and /Target are consistent for tablex&#13;\n   3 INFO    Differencer.java:74 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Endpoints /Source and /Target have 1 range(s) out of sync for tablex&#13;\n   1 INFO    Differencer.java:67 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Endpoints /Source and /Target are consistent for tablex&#13;\n   3 INFO    Differencer.java:74 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Endpoints /Source and /Target have 1 range(s) out of sync for tablex&#13;\n   1 INFO    Differencer.java:67 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Endpoints /Source and /Target are consistent for tablex&#13;\n   1 INFO    Differencer.java:74 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Endpoints /Source and /Target have 1 range(s) out of sync for tablex&#13;\n   3 INFO    Differencer.java:67 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Endpoints /Source and /Target are consistent for tablex&#13;\n   3 INFO    Differencer.java:74 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Endpoints /Source and /Target have 1 range(s) out of sync for tablex&#13;\n   1 INFO    Differencer.java:67 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Endpoints /Source and /Target are consistent for tablex&#13;\n   6 INFO    Differencer.java:74 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Endpoints /Source and /Target have 1 range(s) out of sync for tablex&#13;\n   2 INFO    Differencer.java:67 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Endpoints /Source and /Target are consistent for tablex&#13;\n  11 INFO    StreamingRepairTask.java:81 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Forwarding streaming repair of 1 ranges to /Source (to be streamed with /Target)&#13;\n   2 INFO    StreamingRepairTask.java:68 - [streaming task #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Performing streaming repair of 1 ranges with /Source&#13;\n   1 INFO    StreamingRepairTask.java:81 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Forwarding streaming repair of 1 ranges to /Source (to be streamed with /Target)&#13;\n   4 INFO    StreamingRepairTask.java:68 - [streaming task #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Performing streaming repair of 1 ranges with /Source&#13;\n   1 INFO    RepairJob.java:163 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] requesting merkle trees for tablex (to [/Source, /Target, ...])&#13;\n   1 INFO    RepairSession.java:171 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Received merkle tree for tablex from /Source&#13;\n   1 INFO    StreamingRepairTask.java:96 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] streaming task succeed, returning response to /Target&#13;\n   1 INFO    RepairSession.java:171 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Received merkle tree for tablex from /Source&#13;\n   2 INFO    StreamingRepairTask.java:96 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] streaming task succeed, returning response to /Target&#13;\n   7 INFO    RepairSession.java:171 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Received merkle tree for tablex from /Source&#13;\n  36 INFO    Differencer.java:67 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Endpoints /Source and /Target are consistent for tablex&#13;\n   1 INFO    RepairSession.java:237 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] tablex is fully synced (1 remaining column family to sync for this session)&#13;\n   1 INFO    StreamingRepairTask.java:96 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] streaming task succeed, returning response to /Target&#13;\n   1 INFO    RepairJob.java:163 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] requesting merkle trees for tablex_processed (to [/Source, /Target, ...])&#13;\n   2 INFO    StreamingRepairTask.java:96 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] streaming task succeed, returning response to /Target&#13;\n   2 INFO    RepairSession.java:171 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Received merkle tree for tablex from /Source&#13;\n   1 ERROR    RepairSession.java:303 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] session completed with the following error&#13;\n   1 org.apache.cassandra.exceptions.RepairException:    keyspace/tablex, (4393112290973329820,4394202908924102592]] Sync failed between /Source and /Target&#13;\n   1 java.lang.RuntimeException:    on keyspace/tablex, (4393112290973329820,4394202908924102592]] Sync failed between /Source and /Target&#13;\n   1 Caused    #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d on keyspace/tablex, (4393112290973329820,4394202908924102592]] Sync failed between /Source and /Target&#13;\n   1 ERROR    StorageService.java:3008 - Repair session 0fb1b0d0-8fc9-11e5-a498-4b9679ec178d for range (4393112290973329820,4394202908924102592] failed with error org.apache.cassandra.exceptions.RepairException: [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d on keyspace/tablex, (4393112290973329820,4394202908924102592]] Sync failed between /Source and /Target&#13;\n   1 java.util.concurrent.ExecutionException:    #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d on keyspace/tablex, (4393112290973329820,4394202908924102592]] Sync failed between /Source and /Target&#13;\n   1 Caused    [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d on keyspace/tablex, (4393112290973329820,4394202908924102592]] Sync failed between /Source and /Target&#13;\n   1 Caused    #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d on keyspace/tablex, (4393112290973329820,4394202908924102592]] Sync failed between /Source and /Target&#13;\n</pre><p>Notice the error message</p><pre>ERROR    StorageService.java:3008 - Repair session 0fb1b0d0-8fc9-11e5-a498-4b9679ec178d for range (4393112290973329820,4394202908924102592] failed with error org.apache.cassandra.exceptions.RepairException: [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d on keyspace/tablex, (4393112290973329820,4394202908924102592]] Sync failed between /Source and /Target&#13;\n</pre><p>Streaming for the range (4393112290973329820, 4394202908924102592] between nodes Source and Target failed and caused the repair session to fail. The OpsCenter repair service will retry this session again (up to a configurable threshold) on failure.</p><h2 id=\"whathappened\">What happened?</h2><p>The failure may have been due to a networking problem, an sstable corruption, etc. To get more information, we can 1) check our logs at the Target repair node for additional errors and 2) run the slice again and see if it works.</p><h3 id=\"checkthetarget\">Check the target</h3><p>Checking the logs at the target, there are no failures associated with this repair session which means there must have been a problem on the stream receiving end.</p><pre>$ cat system.log| grep \"0fb1b0d0-8fc9-11e5-a498-4b9679ec178d\" | sed -E 's/([0-9]{1,3}\\.){3}[0-9]{1,3}/Source/'|sed -E 's/([0-9]{1,3}\\.){3}[0-9]{1,3}/Target/' | awk '{ split($3,a,\":\"); $2=a[0] ; $3=\"\"; $4=\"\"; print }'|uniq -c&#13;\n   1 INFO    Validator.java:257 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Sending completed merkle tree to /Source for keyspace/tablex&#13;\n   1 INFO    Validator.java:257 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Sending completed merkle tree to /Source for keyspace/tablex&#13;\n   1 INFO    Validator.java:257 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Sending completed merkle tree to /Source for keyspace/tablex&#13;\n   2 INFO    StreamingRepairTask.java:68 - [streaming task #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Performing streaming repair of 1 ranges with /Source&#13;\n   1 INFO    Validator.java:257 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Sending completed merkle tree to /Source for keyspace/tablex&#13;\n   1 INFO    Validator.java:257 - [repair #0fb1b0d0-8fc9-11e5-a498-4b9679ec178d] Sending completed merkle tree to /Source for keyspace/tablex&#13;\n</pre><h3 id=\"runtheslice\">Run the slice</h3><p>In this case we can run:</p><pre>nodetool repair -par -st 4393112290973329820 -et 4394202908924102592&#13;\n</pre><p>here is the result (notice the networking problem was temporary and now the repair slice succeeds!):</p><pre>]# nodetool repair -par -st 4393112290973329820 -et 4394202908924102592&#13;\n[2015-11-23 21:36:44,138] Starting repair command #1, repairing 1 ranges for keyspace dse_perf (parallelism=PARALLEL, full=true)&#13;\n[2015-11-23 21:36:46,086] Repair session 4aa7a290-922a-11e5-ae1c-4b5d0d7247d3 for range (4393112290973329820,4394202908924102592] finished&#13;\n[2015-11-23 21:36:46,086] Repair command #1 finished&#13;\n[2015-11-23 21:36:46,095] Starting repair command #2, repairing 1 ranges for keyspace keyspace (parallelism=PARALLEL, full=true)&#13;\n[2015-11-23 21:36:47,967] Repair session 4bc6f540-922a-11e5-ae1c-4b5d0d7247d3 for range (4393112290973329820,4394202908924102592] finished&#13;\n[2015-11-23 21:36:47,968] Repair command #2 finished&#13;\n[2015-11-23 21:36:47,979] Nothing to repair for keyspace 'system'&#13;\n[2015-11-23 21:36:47,985] Starting repair command #3, repairing 1 ranges for keyspace keyspace (parallelism=PARALLEL, full=true)&#13;\n[2015-11-23 21:36:53,100] Repair session 4ce92e20-922a-11e5-ae1c-4b5d0d7247d3 for range (4393112290973329820,4394202908924102592] finished&#13;\n[2015-11-23 21:36:53,102] Repair command #3 finished&#13;\n[2015-11-23 21:36:53,112] Starting repair command #4, repairing 1 ranges for keyspace dse_system (parallelism=PARALLEL, full=true)&#13;\n[2015-11-23 21:36:53,979] Repair session 4fe50900-922a-11e5-ae1c-4b5d0d7247d3 for range (4393112290973329820,4394202908924102592] finished&#13;\n[2015-11-23 21:36:53,979] Repair command #4 finished&#13;\n[2015-11-23 21:36:53,987] Starting repair command #5, repairing 1 ranges for keyspace keyspace (parallelism=PARALLEL, full=true)&#13;\n[2015-11-23 21:36:58,390] Repair session 507477c0-922a-11e5-ae1c-4b5d0d7247d3 for range (4393112290973329820,4394202908924102592] finished&#13;\n[2015-11-23 21:36:58,390] Repair command #5 finished&#13;\n[2015-11-23 21:36:58,399] Starting repair command #6, repairing 1 ranges for keyspace OpsCenter (parallelism=PARALLEL, full=true)&#13;\n[2015-11-23 21:37:11,448] Repair session 531931f0-922a-11e5-ae1c-4b5d0d7247d3 for range (4393112290973329820,4394202908924102592] finished&#13;\n[2015-11-23 21:37:11,448] Repair command #6 finished&#13;\n[2015-11-23 21:37:11,458] Starting repair command #7, repairing 1 ranges for keyspace system_traces (parallelism=PARALLEL, full=true)&#13;\n[2015-11-23 21:37:11,878] Repair session 5ae2e890-922a-11e5-ae1c-4b5d0d7247d3 for range (4393112290973329820,4394202908924102592] finished&#13;\n[2015-11-23 21:37:11,878] Repair command #7 finished&#13;\n</pre><p>There are a couple of known streaming issues to keep an eye on.<br /><a href=\"https://issues.apache.org/jira/browse/CASSANDRA-10791\">CASSANDRA-10791</a> and <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-10012\">CASSANDRA-10012</a> which may cause streaming errors. If you are on an affected version, upgrade. If you encounter a reproducible streaming error and can't find the particular stack trace in an existing jira, <a href=\"https://issues.apache.org/jira/\">open a new one</a>.</p><h3 id=\"acorruption\">A corruption</h3><p>In a different repair session we see a different repair error, this time it refers to a specific sstable.</p><pre>WARN  [STREAM-IN-/x.x.x.x] 2015-11-20 20:55:45,529  StreamSession.java:625 - [Stream #114cea40-8fc9-11e5-ae1c-4b5d0d7247d3] Retrying for following error  &#13;\njava.lang.RuntimeException: Last written key DecoratedKey(4393675392884570836, 000b313032333432333334303000000343504600) &gt;= current key DecoratedKey(918610503192973903, 00102941d767895c11e5b5dfaadf7c0db7b80000087765627369746573ff) writing into /cassandra/data/mykeyspace/table1-bd0990407e6711e5a4cae7fe9b813e81/mykeyspace-table1-tmp-ka-7890-Data.db  &#13;\n    at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:164) ~[cassandra-all-2.1.11.908.jar:2.1.11.908]&#13;\n    at org.apache.cassandra.io.sstable.SSTableWriter.appendFromStream(SSTableWriter.java:261) ~[cassandra-all-2.1.11.908.jar:2.1.11.908]&#13;\n    at org.apache.cassandra.streaming.StreamReader.writeRow(StreamReader.java:168) ~[cassandra-all-2.1.11.908.jar:2.1.11.908]&#13;\n    at org.apache.cassandra.streaming.compress.CompressedStreamReader.read(CompressedStreamReader.java:89) ~[cassandra-all-2.1.11.908.jar:2.1.11.908]&#13;\n    at org.apache.cassandra.streaming.messages.IncomingFileMessage$1.deserialize(IncomingFileMessage.java:48) [cassandra-all-2.1.11.908.jar:2.1.11.908]&#13;\n    at org.apache.cassandra.streaming.messages.IncomingFileMessage$1.deserialize(IncomingFileMessage.java:38) [cassandra-all-2.1.11.908.jar:2.1.11.908]&#13;\n    at org.apache.cassandra.streaming.messages.StreamMessage.deserialize(StreamMessage.java:56) [cassandra-all-2.1.11.908.jar:2.1.11.908]&#13;\n    at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandler.run(ConnectionHandler.java:250) [cassandra-all-2.1.11.908.jar:2.1.11.908]&#13;\n    at java.lang.Thread.run(Unknown Source) [na:1.8.0_60]&#13;\n</pre><p>When there is an sstable corruption (due to disk failures or possibly even a bug), the procedure is to run <code>nodetool scrub</code> on the sstable which will correct the corruption.</p><p>In this case the issue was due to <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-9133\">CASSANDRA-9133</a> which was fixed in 2.0.15 so an upgrade was also in order!</p><h2 id=\"summarystayrepaired\">Summary, stay repaired!</h2><p>Repairs can fail due to networking issues or sstable corruptions. The former are usually short lived and will go away on retry; the latter are more rare and require admin intervention in the form of running <code>nodetool scrub</code>.</p><p>Remember, repairs aren't things you run when your cluster is broken; they are a mandatory anti-entropy administrative task (like an oil change) that keeps your cluster healthy. In many cases, running a repair on an unhealthy cluster will just make things worse.</p><p>Hopefully this post will help you understand how repairs work, how to troubleshoot the repair service, and keep your production cluster happy and your database boring. Enjoy!</p></section><p>Update:</p><p>To learn about the future of repair troubleshooting check out:</p><p>https://issues.apache.org/jira/browse/CASSANDRA-8076 and https://issues.apache.org/jira/browse/CASSANDRA-5839</p><hr /><p><a href=\"https://www.datastax.com/\">DataStax</a> has many ways for you to advance in your career and knowledge. \n</p><p>You can take <a href=\"https://academy.datastax.com/user/register?destination=home&amp;utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_register\" target=\"_self\" title=\"academy.datastax.com\">free classes</a>, <a href=\"https://academy.datastax.com/certifications?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_certifications\" target=\"_self\" title=\"academy.datastax.com/certifications\">get certified</a>, or read <a href=\"https://www.datastax.com/dbas-guide-to-nosql\" target=\"_self\" title=\"dbas-guide-to-nosql\">one of our many white papers</a>.\n</p><p><a href=\"https://academy.datastax.com/user/register?destination=home&amp;utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_register\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"academy.datastax.com\">register for classes</a>\n</p><p><a href=\"https://academy.datastax.com/certifications?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_certifications\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"academy.datastax.com/certifications\">get certified</a>\n</p><p><a href=\"http://www.datastax.com/dbas-guide-to-nosql?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_dbasguidetonosql\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"dbas-guide-to-nosql\">DBA's Guide to NoSQL</a>\n</p><br class=\"clear\" /><div id=\"mto_newsletter_121316_Css\"><p>Subscribe for newsletter:</p><br /></div>",
        "created_at": "2018-09-24T15:19:02+0000",
        "updated_at": "2018-09-24T15:19:08+0000",
        "published_at": "2015-12-01T16:33:47+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 17,
        "domain_name": "www.datastax.com",
        "preview_picture": "https://www.datastax.com/wp-content/themes/datastax-2014-08/images/common/DataStax_Web_Social_DefaultGenericV2_1024x351_wide.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12187"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 11,
            "label": "database",
            "slug": "database"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12171,
        "uid": null,
        "title": "Surveying the Cassandra-compatible database landscape - Instaclustr",
        "url": "https://www.instaclustr.com/surveying-cassandra-compatible-database-landscape/",
        "content": "<p>The popularity of <a href=\"https://www.instaclustr.com/apache-cassandra/\">Apache Cassandra</a> and the applicability of it’s development model has seen it clearly emerge as the leading NoSQL technology for scale, performance and availability. One only needs to survey the ever increasing range of Cassandra-compatible options now available on the market to gain a further proof point to its popularity. </p><p>As we get started with 2018, the range of Cassandra-compatible offerings available on the market include:</p><ul><li>Datastax Enterprise</li> <li>ScyllaDB* (<em><strong>Please note that due to extremely limited customer demand, we no longer support ScyllaDB. Please contact our <a href=\"mailto:sales@instaclustr.com\">Sales Team</a> should have you have any further questions.)</strong></em></li> <li>Yugabyte</li> <li>Azure Cosmos DB</li> </ul><p>We all know that the database is a key foundational technology for any application. You need to ensure you choose a product that meets the functional requirements of your use case, is robust and scalable, makes efficient use of compute resources and will be usable by your dev team and supportable by your ops team now and into the future.  Selection of the database technology for a new application therefore deserves rigorous consideration of your specific requirements.</p><p>This blog post surveys the current state and key considerations for people evaluating these offerings and finishes with an overview of some of in progress development for Apache Cassandra that should ensure it remains the default, and best, choice for the majority of use cases.</p><p>This post provides some high-level considerations that should help you to narrow down contenders for evaluation. For each technology we consider:</p><ul><li>Breadth of production deployment – How widely is the product used in production?</li> <li>Licensing model – Is the product open source? If so, what open source licensing model is used and what are the implications of that?</li> <li>Strength of community – Is the product dependent on a single vendor for ongoing support or is it backed by a range of invested organisations? Does the breadth of user community allow access to required expertise?</li> <li>Functionality – Does the technology have any particular functional advantages or limitations that stand out for the comparable technologies?</li> <li>Scalability and performance – Has the system demonstrated ability to operate at scale? Is it able to deliver low latencies and make efficient use of available compute resources?</li> </ul><h2>Datastax Enterprise</h2><p>Datastax Enterprise (DSE) is a closed source product derived from Apache Cassandra. For core Cassandra features, it is driver level compatible with Apache Cassandra. Online migration from DSE to Apache Cassandra can be achieved with minimal effort where DSE proprietary features have not been used. However, DSE contains a number of extensions that are not included in Apache Cassandra and such as bundling Spark and Solr into the same application and providing customer security and compaction providers.</p><p><strong><i>Breadth of production deployment</i><i>: </i></strong>DSE has been used in production by many organisations over several years.</p><p><strong><i>Licensing model:</i></strong> DSE is a closed-source, proprietary product derived from open source products. Use of DSE requires payment of a licensing fee to Datastax.</p><p><strong><i>Strength of community:</i></strong> As a proprietary product, support and enhancement of DSE is entirely reliant on Datastax. However, DSE does build on contributions from the communities for the underlying open source products.</p><p><strong><i>Functionality:</i></strong> Functional enhancements in DSE vs the open source products are generally enterprise-specific features (such as LDAP authentication integration), relatively simple integration of the other included products (Spark, Solr) and the entirely proprietary DSE Graph graph database functionality.</p><p><strong><i>Scalability and Performance</i></strong><strong>:</strong> In general, DSE performance will be very similar to the underlying open source productions. However, Datastax does claim some proprietary performance improvements.</p><h2>Scylla*</h2><p>Scylla is effectively a re-implementation of Apache Cassandra in C++ with an aim of providing highly optimised performance. From a functional point of view, it provides most, but not all, functions of Cassandra and generally doesn’t aim to provide additional functions to Cassandra. It is driver-level compatible with Apache Cassandra but migration to/from Scylla requires an application-level migration strategy such as dual-writes.</p><p><strong><i>Breadth of production deployment: </i></strong> Scylla 1.0 was released in March 2016. Several organisations are reported as running it in production although level of production deployment would be a small fraction of Apache Cassandra deployment.</p><p><strong><i>Licensing Model</i></strong><strong>:</strong> Scylla is open source but licensed under the AGPL (Gnu Affero General Public Use Licence). This license requires that any organisation making a modified version of the product (even for internal use) must publish those modifications. As a result of this requirement, many organisations (particularly large tech orgs that tend to adopt and drive enhancements to open source projects) will not adopt software using the AGPL.</p><p><strong><i>Strength of Community:</i></strong> Scylla is largely dependent on a single company (ScyllaDB) for all development and support.</p><p><strong><i>Functionality</i></strong><strong>:</strong> Scylla generally aims to be functionally compatible with Cassandra although not all features are currently available (light weight transactions being one notable exception).</p><p><strong><i>Scalability and Performance:</i></strong> Improved performance is Scylla’s main objectives. Scylla has published many benchmarks demonstrating substantial performance improvements. However, the most significant gains are seen when running large machines with high performance IO and performance gains in more typical cloud deployments (for manageability) are often less than these benchmarks.</p><p><em><strong>*Please note due to extremely limited customer demand, we no longer support ScyllaDB. Please contact our <a href=\"mailto:sales@instaclustr.com\">Sales Team</a> should have you have any further questions.</strong></em></p><h2>Yugabyte</h2><p>Yugabyte is a new database aiming to offer both SQL and NoSQL functionality in a single database. It is driver compatible with Cassandra (although there is also a Yugabyte-specific fork of the Cassandra driver) and also Redis with announced plans for PostgresSQL compatibility.</p><p><strong><i>Breadth of production deployment:</i></strong> Yugabyte is currently in Beta with production release planned for 2018.</p><p><strong><i>Licensing Model</i></strong><strong>:</strong> Yugabyte is Apache 2.0 Licensed open source software. A closed source “enterprise” edition is also offered with additional manageability and other features.</p><p><strong><i>Strength of Community: </i></strong>Yugabyte is a new product developed by a single company (Yugabyte) and all development and support of the product is dependant on this company.</p><p><strong><i>Functionality:</i></strong> The core Yugabyte engine supports full ACID transactions and a different replication model to Apache Cassandra. Presumably additional features will also be required for PostgresSQL compatibility. While Yugabyte claims compatibility with core Cassandra features it seems likely that, given the differences in underlying engine models, there will be semantic differences that are not readily apparent (for example, Yugabyte already claims differences in consistency semantics).</p><p><strong><i>Scalability and Performance:</i></strong> Yugabyte have published benchmarks claiming improved performance for some scenarios. However, tuning of the Apache Cassandra configuration for their comparison benchmarks was extremely poor and, in any event, the very different architecture of Yugabyte is likely to lead to quite different performance characteristics versus Apache Cassandra depending on the use case.</p><h2>Azure Cosmos DB</h2><p>Cosmos DB is a Microsoft Azure offering designed to provide a globally distributed database with NoSQL functionality. It supports multiple APIs including SQL, Javascript, Gremlin (graph), MongoDB and Cassandra.</p><p><strong><i>Breadth of production deployment: </i></strong>Cosmos DB was released in May 2017 although it builds on Azure DocumentDB which was released in 2014. The Cassandra API was released into preview in November 2017.</p><p><strong><i>Licensing model:</i></strong> Cosmos DB is a proprietary, closed source, technology offered only as an Azure service.</p><p><strong><i>Strength of Community:</i></strong> Cosmos DB is developed and supported by Microsoft.</p><p><strong><i>Functionality: </i></strong> Cosmos DB claims Cassandra compatibility but without providing a detailed breakdown of supported/not support Cassandra features and it seems unlikely there would be complete feature compatibility (at a minimum, the approach to consistency levels is quite different). The documented strategy to import data from Cassandra in Cosmos DB is via CQLshell COPY FROM / COPY TO commands which export data via CSV and generally aren’t suitable for production-size datasets.</p><p><strong><i>Scalability and Performance:</i></strong> Cosmos provide latency SLAs for the 99th percentile which are comparable to other latency focus offerings such as Scylla. Cost effectiveness at scale is hard to gauge and is dependent on Azure pricing.</p><h2>Apache Cassandra</h2><p>Apache Cassandra is the inspiration and genesis for all of these offerings. From it’s 1.0 release in October 2011, Apache Cassandra is now at version 3.11 with version 4.0 in development. It aims to provide virtually unlimited scalability and the ability to run with highest levels of availability and full global distribution. Many household name internet services (eg Apple, Uber, Spotify, Instagram) rely on Apache Cassandra as a core component of their architecture.</p><p><strong><i>Breadth of production deployment</i></strong><strong>:</strong> Production deployment of Apache Cassandra are likely an order magnitude greater than any of the other products mentioned above.</p><p><strong><i>Licensing model:</i></strong> Apache Cassandra  is Apache 2.0 Licensed open source software.</p><p><strong><i>Strength of Community:</i></strong> Apache Cassandra development is governed by the Apache Foundation, the same organisation and governance rules that some of the most successful open source projects such as Hadoop, Spark, Tomcat and the original Apache web server. Apache Cassandra committers are employed by close to 10 different companies with regular contributions from a wide range of companies. Apple, as a key user, is one of the most active contributors to the project.</p><p><strong><i>Functionality</i></strong><strong>:</strong> While some of the other products above are aiming to extend the functionality of Cassandra, Cassandra define the core feature set that the others are aiming to emulate.</p><p><strong><i>Scalability and Performance</i></strong><strong>:</strong> There can be little question as the the scalability of Cassandra with production clusters in the thousands of nodes holding petabytes of data and service millions of operations per second. The Cassandra community is alway working to improve the performance of Cassandra with several major performance initiatives currently underway in the project (eg <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-13476\">CASSANDRA-13476</a>). </p>",
        "created_at": "2018-09-13T15:11:52+0000",
        "updated_at": "2018-09-13T15:11:57+0000",
        "published_at": "2018-01-17T10:28:52+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 7,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12171"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 1273,
            "label": "kafka.connect",
            "slug": "kafka-connect"
          }
        ],
        "is_public": false,
        "id": 12170,
        "uid": null,
        "title": "Apache Kafka Connect Architecture Overview - Instaclustr",
        "url": "https://www.instaclustr.com/apache-kafka-connect-architecture-overview/",
        "content": "<p><a href=\"https://kafka.apache.org/documentation/#connect\">Kafka Connect</a> is an API and ecosystem of 3rd party connectors that enables <a href=\"https://www.instaclustr.com/solutions/managed-apache-kafka/\">Apache Kafka</a> to be scalable, reliable, and easily integrated with other heterogeneous systems (such as Cassandra, Spark, and Elassandra) without having to write any extra code. This blog is an overview of the main Kafka Connect components and their relationships. We’ll cover Source and Sink Connectors; Connectors, Plugins, Tasks and Workers; Clusters; and Converters.</p><p>For an example of how to use Kafka Connect see <strong><a href=\"https://www.instaclustr.com/apache-kafka/\">Apache Kafka</a> “Kongo” Part <a href=\"https://www.instaclustr.com/apache-kafka-kongo-part-4-1-connecting-kafka-to-cassandra-with-kafka-connect\">4.1</a> and <a href=\"https://www.instaclustr.com/apache-kafka-kongo-part-4-2-connecting-kafka-to-cassandra-with-kafka-connect\">4.2</a>: Connecting Kafka to Cassandra with Kafka Connect.</strong></p><p>At a high level, “Source connectors” pull data from an external system (the Source) and write it to <a href=\"https://www.instaclustr.com/solutions/managed-apache-kafka/\">Kafka</a> topics. “Sink connectors” read data from <a href=\"https://www.instaclustr.com/solutions/managed-apache-kafka/\">Kafka</a> topics and push it to an external system (the Sink). Each connector flavour is unidirectional, you can’t go against the flow. Here’s a simple diagram showing the high level Kafka Connect architecture with Source (green) and Sink (blue) data flows:</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/04/Kafka-Connect-Architecture-diagram-1.png\"><img class=\"aligncenter size-full wp-image-9503\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/04/Kafka-Connect-Architecture-diagram-1.png\" alt=\"\" width=\"2896\" height=\"1725\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/04/Kafka-Connect-Architecture-diagram-1.png 2896w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/04/Kafka-Connect-Architecture-diagram-1-300x179.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/04/Kafka-Connect-Architecture-diagram-1-768x457.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/04/Kafka-Connect-Architecture-diagram-1-1024x610.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/04/Kafka-Connect-Architecture-diagram-1-1200x715.png 1200w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/04/Kafka-Connect-Architecture-diagram-1-966x575.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/04/Kafka-Connect-Architecture-diagram-1-640x381.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/04/Kafka-Connect-Architecture-diagram-1-81x48.png 81w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/04/Kafka-Connect-Architecture-diagram-1-181x108.png 181w\" /></a>High Level Kafka Connect Architecture showing Source and Sink Flows</p><p>There are three main components to the Kafka Connect API, each with a different role: Connectors, Tasks and Workers. </p><p>Connectors are either <a href=\"https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/source/SourceConnector.html\">Source</a> or <a href=\"https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/sink/SinkConnector.html\">Sink</a> Connectors, and are responsible for a some of the Task management, but not the actual data movement.</p><p>Tasks come in two corresponding flavours as well, <a href=\"https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/source/SourceTask.html\">Source</a> and <a href=\"https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/sink/SinkTask.html\">Sink</a> Tasks. A Source Task will contain custom code to get data from the Source system (in the pull() method) and uses a Kafka producer which sends the data to Kafka topics. A Sink Task uses a Kafka consumer to poll Kafka topics and read data, and custom code to push data to the Sink system (in the put() method). Each Sink Task has a thread, and they belong to the same consumer group for <a href=\"https://www.instaclustr.com/exploring-apache-kafka-castle-architecture-semantics/\">load balancing</a>. </p><p>The components work together like this (with inspiration from <a href=\"http://shop.oreilly.com/product/0636920044123.do\">“Kafka: The Definitive Guide”</a>):</p><p><b>Connector “Plugins” (Collections of Connectors and Tasks)</b></p><p>A Connector <i>Plugin</i> is a collection of Connectors and Tasks deployed to each Worker.</p><p><b>Connector</b></p><p>Connectors are responsible for the number of tasks, splitting work between tasks, getting configurations for the tasks from the workers and passing it to the Tasks. E.g. to decide how many tasks to run for a Sink, a Connector could use the minimum of max.tasks set in the configuration and the number of partitions of the Kafka topic it is reading from). The workers actually start the Tasks.</p><p><b>Tasks</b></p><p>Tasks are responsible for getting data into and out of Kafka (but only on the Source or Sink side, the Workers manage data flow to/from Kafka topics). Once started, Source Tasks poll Source systems and get the data that the Workers send to Kafka topics, and Sink Tasks get records from Kafka via the Worker, and write the records to the Sink system.</p><p><b>Workers</b></p><p>Workers are the <i>processes</i> that execute the Connectors and Tasks. They handle the REST requests that define connectors and configurations, start the connectors and tasks and pass configurations to them. If using <i>distributed workers</i>, and a worker process dies, then the connectors and tasks associated with the failed worked will be taken over and load balanced among the remaining workers.</p><p>A Kafka Connect Cluster has one (<i>standalone</i>) or more (<i>distributed</i>) Workers running on one or multiple servers, and the Workers manage Connectors and Tasks, distributing work among the available Worker processes. Note that Kafka Connect does not automatically handle restarting or scaling of Workers, so this must be handled with some other solution.</p><p>The following diagram shows the main relationships and functions of each component in a connect cluster. A Kafka connect cluster can be run on one or more servers (for production these will be separate to the servers that the Kafka Brokers are running on), and one (but potentially more) workers on each server. Data movement is shown with green lines:</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/05/Kafka-Connect-Architecture-diagram-2.png\"><img class=\"aligncenter size-full wp-image-9521\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/05/Kafka-Connect-Architecture-diagram-2.png\" alt=\"\" width=\"3510\" height=\"2185\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Kafka-Connect-Architecture-diagram-2.png 3510w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Kafka-Connect-Architecture-diagram-2-300x187.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Kafka-Connect-Architecture-diagram-2-768x478.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Kafka-Connect-Architecture-diagram-2-1024x637.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Kafka-Connect-Architecture-diagram-2-1200x747.png 1200w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Kafka-Connect-Architecture-diagram-2-966x601.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Kafka-Connect-Architecture-diagram-2-640x398.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Kafka-Connect-Architecture-diagram-2-77x48.png 77w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Kafka-Connect-Architecture-diagram-2-173x108.png 173w\" /></a></p><p>Apache Kafka Connect Architecture UML Diagram</p><p><img class=\"aligncenter size-full wp-image-9523\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/05/pasted-image-0-1.png\" alt=\"\" width=\"548\" height=\"405\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/pasted-image-0-1.png 548w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/pasted-image-0-1-300x222.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/pasted-image-0-1-65x48.png 65w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/pasted-image-0-1-146x108.png 146w\" /></p><p>Just like Catalytic Converters for cars, <i>converters</i> are also a key part of the Kafka connector pipeline! I initially found converters perplexing as Kafka consumers and producers already have (De-)Serializers. Are converters the same or different?  Kafka doesn’t know anything about the data format of topic keys and value, it just treats them as byte arrays. So consumers and producers need to be able to convert objects to and from byte arrays, and that’s exactly what the (De-)Serializers do.</p><p>Doing some more research on Converters I found that the <a href=\"https://kafka.apache.org/0102/javadoc/org/apache/kafka/connect/storage/Converter.html\">converter interface docs say</a>: </p><blockquote> <p>“The Converter interface provides support for translating between Kafka Connect’s runtime data format and byte. Internally, this likely includes an intermediate step to the format used by the serialization layer.”</p> </blockquote><p>I also found that Converter has <code>fromConnectData()</code> and <code>toConnectData()</code> method that must be implemented for converting byte arrays to/from <i>Kafka Connect Data Objects</i>.  Connect “Data Objects” have <a href=\"https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/SchemaAndValue.html\">schemas and values</a>, and a <a href=\"https://kafka.apache.org/0110/javadoc/org/apache/kafka/connect/data/SchemaBuilder.html\">SchemaBuilder</a> which provides a fluent API for constructing Schema objects. Schemas are optional to support cases with schema-free data. <a href=\"https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/connector/ConnectRecord.html\">ConnectRecords</a> (subclasses SinkRecord and SourceRecord) are analogous to Kafka’s ConsumerRecord and ProducerRecord classes, and contain the data read from or written to Kafka. </p><p>In conclusion, here’s how Sources, Tasks, Converters, Topics, (De-)Serializers and Sinks fit together to give a complete end-to-end Kafka data pipeline:</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/05/Kafka-Connect-Architecture-diagram-3e.png\"><img class=\"aligncenter size-full wp-image-9525\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/05/Kafka-Connect-Architecture-diagram-3e.png\" alt=\"\" width=\"2731\" height=\"1876\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Kafka-Connect-Architecture-diagram-3e.png 2731w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Kafka-Connect-Architecture-diagram-3e-300x206.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Kafka-Connect-Architecture-diagram-3e-768x528.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Kafka-Connect-Architecture-diagram-3e-1024x703.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Kafka-Connect-Architecture-diagram-3e-1200x824.png 1200w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Kafka-Connect-Architecture-diagram-3e-897x616.png 897w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Kafka-Connect-Architecture-diagram-3e-640x440.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Kafka-Connect-Architecture-diagram-3e-70x48.png 70w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Kafka-Connect-Architecture-diagram-3e-157x108.png 157w\" /></a></p><p>Complete end-to-end Kafka Data Pipeline</p><p>Finally, one nice feature of the Kafka Connect architecture is that because converters are decoupled from connectors, you can reuse any Kafka Connect Converter with any Kafka Connect Connector.</p><aside class=\"content-cta\"><div class=\"primary\"><p>Related Articles:</p></div></aside>",
        "created_at": "2018-09-13T15:11:36+0000",
        "updated_at": "2018-09-13T15:11:44+0000",
        "published_at": "2018-05-09T09:31:35+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 4,
        "domain_name": "www.instaclustr.com",
        "preview_picture": "https://www.instaclustr.com/wp-content/uploads/2018/04/Kafka-Connect-Architecture-diagram-1.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12170"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1232,
            "label": "stress",
            "slug": "stress"
          },
          {
            "id": 1262,
            "label": "time.series",
            "slug": "time-series"
          }
        ],
        "is_public": false,
        "id": 12169,
        "uid": null,
        "title": "Using Cassandra Stress to model a time series workload - Instaclustr",
        "url": "https://www.instaclustr.com/using-cassandra-stress-to-model-a-time-series-workload/",
        "content": "<p><b>Motivation</b></p><p>When examining whether <a href=\"https://www.instaclustr.com/apache-cassandra/\">Cassandra</a> is a good fit for your needs, it is good practice to stress test Cassandra using a workload that looks similar to the expected workload in Production. </p><p>In the past we have examined the richness of features using YAML profiles in Cassandra’s stress tool – if you haven’t seen the previous post or are unfamiliar with YAML profiles in Cassandra stress, I’d recommend <a href=\"https://www.instaclustr.com/deep-diving-cassandra-stress-part-3-using-yaml-profiles/\">checking it out now</a>. </p><p>YAML profiles are all fine and dandy when it comes to mixed or general workloads using SizeTieredCompactionStrategy (STCS) or LeveledCompactionStrategy (LCS), but sometimes we may want to model a time series workload using TimeWindowCompactionStrategy (TWCS). How would we do that with the current options available to us in stress? Ideally, we would be able to do such a thing without having to schedule cassandra-stress instances every X minutes. </p><p><b>Native functions</b></p><p>As it turns out, Cassandra has a native function <b>now()</b> that returns the current time as a <b>timeuuid</b>, which is a unique representation of time. Cassandra also ships with the function <b>toTimestamp()</b> that accepts a <b>timeuuid</b>. Putting the two together, we are able to obtain the following result:</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-12.27.59-pm.png\"><img class=\"aligncenter size-full wp-image-9517\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-12.27.59-pm.png\" alt=\"\" width=\"670\" height=\"280\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-12.27.59-pm.png 670w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-12.27.59-pm-300x125.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-12.27.59-pm-640x267.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-12.27.59-pm-115x48.png 115w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-12.27.59-pm-258x108.png 258w\" /></a></p><p>So we can use that to our advantage in a YAML profile:</p><div id=\"crayon-5b9a77320c0b8296795884\" class=\"crayon-syntax crayon-theme-classic crayon-font-monaco crayon-os-mac print-yes notranslate\" data-settings=\"minimize scroll-mouseover\"><div class=\"crayon-main\"><table class=\"crayon-table\"><tr class=\"crayon-row\"><td class=\"crayon-nums\" data-settings=\"show\"> <div class=\"crayon-nums-content\"><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p><p>23</p><p>24</p><p>25</p><p>26</p><p>27</p><p>28</p><p>29</p><p>30</p><p>31</p><p>32</p><p>33</p><p>34</p><p>35</p><p>36</p><p>37</p><p>38</p><p>39</p><p>40</p><p>41</p><p>42</p><p>43</p></div></td>\n<td class=\"crayon-code\"><div class=\"crayon-pre\"><p>table_definition: |</p><p>CREATE TABLE twcstest (</p><p>id text,</p><p>time timestamp,</p><p>metric int,</p><p>value blob,</p><p>PRIMARY KEY((id), time)</p><p>) WITH CLUSTERING ORDER BY (time DESC)</p><p>AND compaction = { 'class':'TimeWindowCompactionStrategy', 'compaction_window_unit':'MINUTES', 'compaction_window_size':'20' }</p><p>AND comment='A table to see what happens with TWCS &amp; Stress'</p><p>columnspec:</p><p>- name: id</p><p>  size: fixed(64)</p><p>  population: uniform(1..1500M)</p><p>- name: time</p><p>  cluster: fixed(288)</p><p>- name: value</p><p>  size: fixed(50)</p><p>queries:</p><p>putindata:</p><p>  cql: insert into twcstest (id, time, metric, value) VALUES (?, toTimestamp(now()), ?, ?)</p></div></td>\n</tr></table></div></div><p>Based on that YAML above, we can now insert time series data as part of our stress. Additionally, please be aware that the <b>compaction_window_unit</b> property has been deliberately kept much smaller than is typical of a normal production compaction strategy!</p><p>The only snag to be aware of is that stress will insert timestamps rapidly, so you may want to tweak the values a little to generate suitably sized partitions with respect to your production workload. </p><p><b>That’s great, now how do I select data?</b></p><p>Well, intuitively we would just make use of the same helpful native functions that got us out from the tight spot before. So we may try this:</p><p>We appear to be a little stuck because selects may not be as straightforward as we had expected.</p><ol><li>We could try qualifying with just <b>&lt;=</b>, but then that would be a whole lot of data we select (You aren’t going to do this in Production, are you?), unless <b>id</b> is bucketed…but it isn’t in our situation. </li>\n<li>We could try qualifying with just <b>&gt;=</b>, but then nothing will be returned (You aren’t testing a case like this either, surely).</li>\n</ol><p>Unfortunately for us, it doesn’t look like Cassandra has anything available to help us out here natively. But it certainly has something we can leverage. </p><p><b>UDFs for the win</b></p><p>User defined functions (UDFs) have been added to Cassandra since <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-7395\">2.2</a>. If you aren’t familiar with them, there are examples of them available in a <a href=\"https://www.instaclustr.com/user-defined-functions-and-aggregates/\">previous blog post</a> and the <a href=\"http://cassandra.apache.org/doc/latest/cql/functions.html\">official cassandra documentation</a>. Since Cassandra doesn’t have any other native functions to help us, we can just write our own UDF, as it should be. </p><p>Typically we may expect to want to select a slice up to a certain number of minutes ago. So we want to write a UDF to allow us to do that.</p><p>This UDF is quite self explanatory so I won’t go into too much detail. Needless to say, it returns a <b>bigint</b> of <b>arg </b>minutes ago. </p><p>Here is a test to illustrate just to be safe:</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-1.12.56-pm.png\"><img class=\"aligncenter size-full wp-image-9516\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-1.12.56-pm.png\" alt=\"\" width=\"987\" height=\"950\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-1.12.56-pm.png 987w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-1.12.56-pm-300x289.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-1.12.56-pm-768x739.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-1.12.56-pm-935x900.png 935w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-1.12.56-pm-640x616.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-1.12.56-pm-50x48.png 50w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-1.12.56-pm-112x108.png 112w\" /></a></p><p>Here is our new and improved YAML profile:</p><p>Now, when we execute cassandra-stress with <b>simple1</b>, we can expect just data within a certain time frame instead of selecting the whole partition. We can also keep varying the query to select older data if we like, for example, <b>time &gt;= minutesAgo(600) and time &lt;= minutesAgo(590) </b>for data up to 10 hours ago.</p><p><b>A variation with bucketing</b></p><p>We can also create UDFs that model bucketing behaviour. For example, suppose now we have a schema that has data bucketed, like this:</p><p>And we want to be able to insert data in 5 minute buckets. We can create UDFs like so:</p><p>The UDF <b>bucket</b> is quite self explanatory as well – it just returns the nearest 5 minute bucket smaller than <b>arg</b>. This assumes UTC time and 5 minute buckets, but the code can easily be tailored to be more general. </p><p>However, our UDF doesn’t understand <b>timeuuid</b>. Which is why we need another helper function, which is the function <b>nowInMilliSec</b>(). </p><p>The final UDF generates a random bucket based on a lower and upper bound time. The expected input bounds should be in epoch milliseconds. This will help in selecting old/random data bucketed to within 5 minutes in a range. </p><p>And now here is our new and modified YAML profile to accommodate our desires of having stress follow a bucketed workload:</p><p>1524117600000 happens to be Thursday, April 19, 2018 5:20:00 AM in GMT time while 1524129600000 happens to be Thursday, April 19, 2018 9:20:00 AM. It can be tailored to suit needs. It’s kind of ugly, but it will do the job. </p><p>And there we go: Tap into UDFs to be able to model a TWCS workload with Cassandra stress.</p><p>There’s always an option of writing your own client and using that to perform stress instead, with the obvious benefit that there’s no need to write UDFs and you have control over everything. The downside is that you would have to write code that includes rate limiting and reporting of metrics whereas cassandra stress is the stressing tool that comes with Cassandra out of the box and has very rich statistics, down to latency for each query. </p>",
        "created_at": "2018-09-13T15:11:07+0000",
        "updated_at": "2018-09-13T15:11:12+0000",
        "published_at": "2018-05-09T09:34:14+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 5,
        "domain_name": "www.instaclustr.com",
        "preview_picture": "https://www.instaclustr.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-12.27.59-pm.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12169"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          },
          {
            "id": 962,
            "label": "lambda",
            "slug": "lambda"
          }
        ],
        "is_public": false,
        "id": 12168,
        "uid": null,
        "title": "AWS Lambda with Managed Cassandra - Part 1: Let's Build a POC - Instaclustr",
        "url": "https://www.instaclustr.com/aws-lambda-managed-cassandra-part-1/",
        "content": "<p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/08/picture-lambda-blog-2-1.png\"><img class=\"aligncenter size-full wp-image-11121\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/08/picture-lambda-blog-2-1.png\" alt=\"\" width=\"925\" height=\"471\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/picture-lambda-blog-2-1.png 925w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/picture-lambda-blog-2-1-300x153.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/picture-lambda-blog-2-1-768x391.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/picture-lambda-blog-2-1-640x326.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/picture-lambda-blog-2-1-94x48.png 94w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/picture-lambda-blog-2-1-212x108.png 212w\" /></a></p><p>Serverless architecture is an attractive solution for both small businesses and large enterprises. The former gains the ability to go to market quickly while the latter are able to reduce IT costs by abstracting away complexity. One of the key players in serverless architecture is AWS with its Lambda offering. Lambda is a simple way to execute small portions of code on demand and without the need to provision any servers. Alongside the growth in Lambda’s popularity has come a greater interest in combining it with our Cassandra managed service. As such, we thought it would be a good time to investigate the pros and cons of using Lambda with Cassandra and to share some tips. </p><p>This will be a three-part series of blog posts. The first post will focus on developing a POC at next to no cost and will look like a tutorial. The focus here is mostly functional. The second post will focus on performance. The last will cover security and cost savings.</p><p>In this POC we are going to build a minimalistic REST API with Cassandra as our backend storage. Here is what we will use:</p><ul><li><a href=\"https://console.instaclustr.com/user/signup\">Instaclustr managed service free trial</a> (14 day free trial on t2.small nodes)</li> <li><a href=\"https://aws.amazon.com/lambda/pricing/\">AWS Lambda free tier</a> (1 million requests per month, 400,000 GB-seconds)</li> <li><a href=\"https://aws.amazon.com/api-gateway/pricing/\">AWS API Gateway free tier</a> (1 million API calls per month for new customers)</li> </ul><p>Running this tutorial should incur next to no cost. Obviously, you are responsible for maintaining your usage within the free tier AWS allowance, and <a href=\"https://console.instaclustr.com/user/signup\">Instaclustr free trial</a>. Running this tutorial assumes you have some general AWS knowledge, though you don’t need to have experience with AWS Lambda or AWS API Gateway.</p><p>The architecture is quite simple:</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-2-1.png\"><img class=\"aligncenter size-full wp-image-11120\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-2-1.png\" alt=\"\" width=\"917\" height=\"151\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-2-1.png 917w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-2-1-300x49.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-2-1-768x126.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-2-1-640x105.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-2-1-291x48.png 291w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-2-1-600x99.png 600w\" /></a></p><p>The REST API will be minimalistic: We are going to create a service to create and retrieve orders. Here is an example using Swagger:</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-3-1.png\"><img class=\"aligncenter size-full wp-image-11119\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-3-1.png\" alt=\"\" width=\"1299\" height=\"564\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-3-1.png 1299w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-3-1-300x130.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-3-1-768x333.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-3-1-1024x445.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-3-1-1200x521.png 1200w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-3-1-966x419.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-3-1-640x278.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-3-1-111x48.png 111w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-3-1-249x108.png 249w\" /></a></p><h2>Step1: Create a Cassandra Cluster on AWS.</h2><p>This is extremely easy using Instaclustr’s managed service. You can use the <a href=\"https://console.instaclustr.com/user/signup\">14-day free trial</a> to create a 3 node t2.small cluster. This type of cluster is perfect for basic functional testing. Let’s break down this process:</p><h3>Step 1.a. Create an <a href=\"https://console.instaclustr.com/user/signup\">Instaclustr Account</a>:</h3><p>Once you create your account, you will be able to run a 14 day trial cluster for free.</p><h3>Step 1.b Create a Cluster, with the Following Properties:</h3><h3><b>Name</b>: aws-lambda-small</h3><h3><b>Applications</b>: Apache Cassandra 3.11.2 (latest as of today)</h3><ul><li><b>No add-ons</b></li> <li><b>Infrastructure provider</b>: AWS</li> <li><b>Region</b>: Choose something close to you. I’ll be using US West (Oregon)</li> <li><b>Custom Name</b>: Let’s leave it to the default, i.e. AWS_VPC_US_WEST_2 for me as I am running in Oregon. If you are running in another region, take note as you will need to use the data center name later in this tutorial</li> <li><b>Data Centre Network</b>: Let’s leave it to the default, i.e. 10.224.0.0/16</li> <li><b>Node Size</b>: You will need to choose the Starter node (t2.small)</li> <li><b>EBS Encryption</b>: not required</li> <li><b>Replication Factor</b>: Let’s choose the most common replication factor when using Cassandra: 3</li> <li><b>Nodes</b>: 3</li> <li><b>Network</b>: We are going to tick the box to use private IP for node broadcast. The client will need to connect to Cassandra using the private IP. <ul><li><b>Security</b>: You can let Instaclustr add your local IP address to the firewall, thought that’s not required.</li> <li><b>Password auth</b>: For simplicity, we are going to disable password authentication and user authorization. As your client can only connect to the cluster via private IP, this is not a huge risk for a Proof of Concept. Don’t do that in production.</li> <li><b>Client – Node encryption</b> this is not supported for nodes that are this small</li> </ul></li> </ul><p>And that’s it! Click on <b>Create Cluster</b>, and you will have it running within 5 minutes.</p><h2>Step 2: Configure the AWS VPC Infrastructure.</h2><p>This configuration step is to enable communication between the AWS Lambda you will create, and the Cassandra cluster you just created. It may seem like a boring ‘plumbing’ step, but pay close attention as you will not get traffic to flow correctly if you miss a step.</p><h3>Step 2.a Create an AWS Account.</h3><p>If you don’t have one, create an AWS account. You will need to provide a credit card.</p><h3>Step 2.b Create a VPC in your AWS Account</h3><p>Use the same region you chose for your Cassandra cluster. Importantly, choose a CIDR block that does not overlap with the CIDR block you choose for your Cassandra cluster. For example, 10.225.0.0/16</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-4-1.png\"><img class=\"aligncenter size-full wp-image-11118\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-4-1.png\" alt=\"\" width=\"904\" height=\"558\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-4-1.png 904w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-4-1-300x185.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-4-1-768x474.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-4-1-640x395.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-4-1-78x48.png 78w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-4-1-175x108.png 175w\" /></a></p><h3>Step 2.c. Create a subnet per availability zone.</h3><p>You will want to do that to achieve High Availability with your AWS Lambda. In my case, I use the subnet CIDR block as follow: 10.225.0.0/24 for us-west-2a; 10.225.1.0/24 for us-west-2b; and 10.225.2.0/24 for us-west-2c.</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-5-1.png\"><img class=\"aligncenter size-full wp-image-11117\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-5-1.png\" alt=\"\" width=\"1166\" height=\"621\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-5-1.png 1166w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-5-1-300x160.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-5-1-768x409.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-5-1-1024x545.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-5-1-966x514.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-5-1-640x341.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-5-1-90x48.png 90w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-5-1-203x108.png 203w\" /></a></p><h3>Step 2.d. Request VPC Peering between your Cassandra Cluster and your New VPC.</h3><p>The request needs to be initiated from the Instaclustr console. Look for the Settings section of your cluster at the bottom. You will need to provide your AWS account number, the id of your aws-lambda-small VPC, and it’s corresponding CIDR (in my case, 10.225.0.0/16). You should choose the option to have your VPC network added to the cluster firewall. Once you submit the VPC peering request, you need to accept it on your AWS account, and you need to update your VPC route table to route the traffic to Cassandra. In my case, the traffic to 10.224.0.0/16 needs to be routed to the new VPC peering connection.</p><p><a href=\"https://www.instaclustr.com/support/documentation/cluster-management/using-vpc-peering-aws/\">You can see the full details in this support article</a>. </p><h3>Step 2.e. Create a Security Group.</h3><p>You need to create a security group for you AWS Lambda as the Lambda containers will be executed in the VPC. At a minimum, a lot of outbound traffic to TCP port 9042 (Cassandra port) towards your Cassandra cluster, in my case 10.224.0.0/16</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-6-1.png\"><img class=\"aligncenter size-full wp-image-11116\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-6-1.png\" alt=\"\" width=\"1498\" height=\"713\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-6-1.png 1498w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-6-1-300x143.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-6-1-768x366.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-6-1-1024x487.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-6-1-1200x571.png 1200w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-6-1-966x460.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-6-1-640x305.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-6-1-101x48.png 101w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-6-1-227x108.png 227w\" /></a></p><h2><b>Step 3: Create your AWS Lambda.</b></h2><p>AWS Lambda are small portions of code that can be executed on demand on AWS, without the need for the user to provision an instance. Behind the scenes, Lambda are executed as containers. The very first time the Lambda is executed, AWS will instantiate the container and run the code. Once the Lambda application exits, AWS will freeze the container and keep it around ready to execute new Lambda requests. If it remains unused, the Lambda container is destroyed. </p><p>To achieve the best performance, code executed on Lambda can be split into initialization and execution. For working with Cassandra, we will create the cluster connection (which can take a few seconds), and create the prepared statement in the initialization phase, so that only the first execution will incur the initialization overhead, and subsequent execution will be much faster. For the purpose of this POC, I will create two Lambda that executes Python code. The first Lambda is responsible for creating the Cassandra data model schema and will be called only once. The second Lambda will receive the PUT and GET requests from the API gateway and will execute the write and read code.</p><h3>Step 3.a. Navigate to the AWS Lambda page, and Click to Create a Function</h3><p><b>Name</b>: cassandra-schema-init</p><p><b>Runtime</b>: Python 2.7</p><p><b>Role</b>: Create new role from template(s)</p><ul><li>Role name aws-lambda-small</li> <li>Policy templates: According to the documentation, you should be using “VPCAccess” that will allow the Lambda to create the NetworkInterface to run in the VPC. However, VPCAccess is not in the list of templates as of now. So just leave empty and we will fix that in a few steps.</li> </ul><h3>Step 3.b. Create and Upload the Python Code for cassandra-schema-init</h3><p>You will see that the AWS console lets you edit code directly. However, as we need to add some dependencies (<code>cassandra-driver</code>), we will need to edit the code from our computer and package the code with the dependencies. Change the “Code entry type” to “Upload a .ZIP file.” On your computer, create a directory, such as <code>aws-lambda-schema</code>, and start by downloading the dependencies. </p><p>If you are using Linux you can use the Python package manager ‘pip’ to install everything. To get the cassandra-driver, run this command: <code><code>pip install cassandra-driver -t</code> .</code> Then run this command to get the twisted library: <code><code>pip install twisted -t</code></code>. You may be wondering why we are downloading the twisted library. Twisted lets us reuse an existing Cassandra connection at each invocation of the AWS Lambda. This is the only way to achieve two digit millisecond level performance. Downloading the <code>cassandra-driver</code> and <code>twisted</code> will take a few minutes. Good time for a coffee break.</p><p>The Python code to create the schema is simple. Save the following into the file <code>cassandra_lambda.py</code>:</p><p>Your aws-lambda-schema directory should now contain cassandra_lambda.py, and all the dependencies downloaded by pip. The next step is to zip the contents of the directory and upload it to the AWS console. Importantly, <b>do not zip the directory.</b> <b>You need to zip the contents</b> of the directory in one file. AWS is sensitive about that.</p><h3>Step 3.c. Configure AWS Lambda Settings from the AWS Console</h3><p><b>Function code: </b>If you named your file cassandra_lambda.py, then make sure to update the Handler with <code>cassandra_lambda.lambda_handler</code></p><p><b>The environment variables</b>. In my case given the Cassandra IP addresses and the name of the data center:</p><ul><li>endpoint 10.224.17.181,10.224.97.78,10.224.188.162</li>\n<li>local_dc AWS_VPC_US_WEST_2</li>\n</ul><p>Those are used in the code.</p><p><b>Basic settings</b>: You will want to increase the timeout since initializing the Cassandra connection and updating the schema can take a few seconds. Ten seconds should be enough.</p><p><b>Network section</b>: you need to set your Lambda to execute in your aws-lambda-small VPC. Select the security group and three subnets you created earlier.</p><p><b>Execution Role</b>: You will need to create a new role using a template. The one that was created earlier did not automatically have permissions to create network interfaces. Now that you have chosen the VPC in the Network section, AWS will be able to automatically add the VPC Access policy to the role. Let’s call the role <code>aws-lambda-small-vpc</code>. No need to choose a template from the Policy.</p><p>click on <b>Save</b> at the top right.</p><h3>Step 3.d. Test the Lambda cassandra-schema-init</h3><p>Just click on “Test” at the top right. You will be prompted to configure a test event. This Lambda does not respond to events as it is used only for initializing the Cassandra schema. You can use the default <strong>Hello World</strong> event.</p><p>If everything works well, you should see some success logs on the AWS Lambda console and your schema will be created. Now let’s create the Lambda that will handle the POST and GET calls for your API. In this example, we will create a single Lambda to handle both POST and GET. In practice, you will need to consider the pros and cons of having multiple Lambdas for multiple methods.</p><h3>Step 3.e. Create a New Lambda: cassandra-api</h3><p><b>Name</b>: cassandra-api</p><p><b>Runtime</b>: Python 2.7</p><p><b>Role</b>: Choose an existing role</p><p><b>Existing role</b>: service-role/aws-lambda-small-vpc which is the one you created earlier</p><h3>Step 3.f. Configure the Lambda cassandra-api</h3><p><b>Function code</b>:</p><ul><li>Code entry type: Upload a ZIP file</li>\n<li><strong>Handler:</strong> <code>cassandra_lambda.lambda_handler</code> – That’s assuming you will name your python file cassandra_lambda.py</li>\n</ul><p><b>Environment variables:</b></p><ul><li>endpoint 10.224.18.118,10.224.75.58,10.224.133.133</li>\n<li>local_dc AWS_VPC_US_WEST_2</li>\n</ul><p><b>Basic settings</b>: set the timeout to 10 sec.</p><p><b>Network</b>: use VPC, and choose the existing subnets/securitygroups.</p><p>Click on <b>Save </b>at the top right of the screen.</p><h3>Step 3.g. Create and Upload the Code for cassandra-api</h3><p>Similar to what you did for <code>cassandra-schema-init</code>, you will need to create a directory, download the dependencies with pip, create and edit the file <code>cassandra_lambda.py</code>, zip the contents (<b>remember, don’t zip the parent directory</b>) and finally upload that to AWS. </p><p>Use the following code:</p><div id=\"crayon-5b9a772d2619a141924888\" class=\"crayon-syntax crayon-theme-classic crayon-font-monaco crayon-os-mac print-yes notranslate\" data-settings=\"minimize scroll-mouseover\"><div class=\"crayon-toolbar\" data-settings=\"mouseover overlay hide delay\"><div class=\"crayon-tools\">Python</div></div><div class=\"crayon-main\"><table class=\"crayon-table\"><tr class=\"crayon-row\"><td class=\"crayon-nums\" data-settings=\"show\"> <div class=\"crayon-nums-content\"><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p><p>23</p><p>24</p><p>25</p><p>26</p><p>27</p><p>28</p><p>29</p><p>30</p><p>31</p><p>32</p><p>33</p><p>34</p><p>35</p><p>36</p><p>37</p><p>38</p><p>39</p><p>40</p><p>41</p><p>42</p><p>43</p><p>44</p><p>45</p><p>46</p><p>47</p><p>48</p><p>49</p><p>50</p><p>51</p><p>52</p><p>53</p><p>54</p><p>55</p><p>56</p><p>57</p><p>58</p><p>59</p><p>60</p><p>61</p><p>62</p><p>63</p><p>64</p><p>65</p><p>66</p><p>67</p><p>68</p><p>69</p><p>70</p><p>71</p><p>72</p><p>73</p><p>74</p><p>75</p><p>76</p><p>77</p><p>78</p><p>79</p></div></td>\n<td class=\"crayon-code\"><div class=\"crayon-pre\"><p>import time</p><p>import uuid</p><p>import json</p><p>import os</p><p>import boto3</p><p>from base64 import b64decode</p><p>from cassandra.cluster import Cluster</p><p>from cassandra.auth import PlainTextAuthProvider</p><p>from cassandra.policies import DCAwareRoundRobinPolicy</p><p>from cassandra.io.twistedreactor import TwistedConnection</p><p>from cassandra import ConsistencyLevel</p><p># Keep track of container id, for perf testing.</p><p>container_id=uuid.uuid4()</p><p># Get Cassandra endpoint from aws Lambda env variables</p><p>ENDPOINT = os.environ['endpoint']</p><p>LOCAL_DC = os.environ['local_dc']</p><p># the cassandra session, and prepared statement will</p><p>cassandra_session = None</p><p>cassandra_insert = None</p><p>cassandra_lookup = None</p><p># the code to handle POST calls.</p><p># In practice, you will want to use your favorite API framework, i.e. Flask</p><p>def post(event):</p><p>   myjson = json.loads(event['body'])</p><p>   order_id=uuid.uuid4()</p><p>   name = myjson['name']</p><p>   address = myjson['address']</p><p>   phone = myjson['phone']</p><p>   item = myjson['item']</p><p>   cassandra_session.execute(cassandra_insert, [order_id, name, address, phone, item])</p><p>   return {</p><p>       'isBase64Encoded': False,</p><p>       'statusCode': 200,</p><p>       'body': json.dumps({\"order_id\": str(order_id)}),</p><p>       'headers': {}</p><p>   }</p><p># the code to handle GET calls</p><p># In practice, you will want to use your favorite API framework, i.e. Flask</p><p>def get(event):</p><p>   order_id = event['pathParameters']['id']</p><p>   rows = cassandra_session.execute(cassandra_lookup, [uuid.UUID(order_id)])</p><p>   if not rows:</p><p>       return {</p><p>           'isBase64Encoded': False,</p><p>           'statusCode': 404,</p><p>           'body': {},</p><p>           'headers': {}</p><p>       }</p><p>   return {</p><p>       'isBase64Encoded': False,</p><p>       'statusCode': 200,</p><p>       'body': json.dumps({\"order_id\": order_id,</p><p>                           \"name\": rows[0].name,</p><p>                           \"address\": rows[0].address,</p><p>                           \"phone\": rows[0].phone,</p><p>                           \"item\": rows[0].item}),</p><p>       'headers': {}</p><p>   }</p><p>method_map = {'GET': get, 'POST': post}</p><p>def Lambda_handler(event, context):</p><p>   global container_id, cassandra_session, cassandra_insert, cassandra_lookup</p><p>   print('Running container', container_id)</p><p>   if not cassandra_session:</p><p>       cluster = Cluster(ENDPOINT.split(\",\"),connection_class=TwistedConnection, load_balancing_policy=DCAwareRoundRobinPolicy(local_dc=LOCAL_DC))</p><p>       cassandra_session = cluster.connect()</p><p>       cassandra_session.default_consistency_level = ConsistencyLevel.QUORUM</p><p>       cassandra_insert = cassandra_session.prepare(\"INSERT INTO ks.tb (order_id, name, address, phone, item) VALUES (?, ?, ?, ?, ?)\")</p><p>       cassandra_lookup = cassandra_session.prepare(\"SELECT name, address, phone, item FROM ks.tb WHERE order_id = ?\")</p><p>   method = event['httpMethod']</p><p>   return method_map[method](event)</p></div></td>\n</tr></table></div></div><p>You might want to use the AWS command line interface (aws-cli) at this stage. Assuming you have configured your AWS profile(s), this could look like:</p><p>By the way, as you may have noticed, you will have to zip the python files and the dependencies quite often. To accelerate this, have a look at the -u option of the zip command (Linux / OSX): it will only update the archive with the changed file (which should only be your python file), making the process much faster.</p><h3>Step 3.h Test your cassandra-api</h3><p>If you look at the code of cassandra-api, you will see that the GET and PUT methods expect some json extracted from the body of an HTTP request. We are going to set up two tests: one for GET, and one for POST.</p><p>Click on <strong>Test</strong> (top right)</p><p>Configure your test event:</p><ul><li><strong>Event name:</strong> post</li>\n<li><strong>Content of the event:</strong> use the following.</li>\n</ul><div id=\"crayon-5b9a772d261a4749147695\" class=\"crayon-syntax crayon-theme-classic crayon-font-monaco crayon-os-mac print-yes notranslate\" data-settings=\"minimize scroll-mouseover\"><div class=\"crayon-toolbar\" data-settings=\"mouseover overlay hide delay\"><div class=\"crayon-tools\">JavaScript</div></div><div class=\"crayon-main\"><table class=\"crayon-table\"><tr class=\"crayon-row\"><td class=\"crayon-nums\" data-settings=\"show\"> <div class=\"crayon-nums-content\"><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p><p>23</p><p>24</p><p>25</p><p>26</p><p>27</p><p>28</p><p>29</p><p>30</p><p>31</p><p>32</p><p>33</p><p>34</p><p>35</p><p>36</p></div></td>\n<td class=\"crayon-code\"><div class=\"crayon-pre\"><p>{</p><p> \"resource\": \"/order\",</p><p> \"path\": \"/order\",</p><p> \"httpMethod\": \"POST\",</p><p> \"headers\": null,</p><p> \"queryStringParameters\": null,</p><p> \"pathParameters\": null,</p><p> \"stageVariables\": null,</p><p> \"requestContext\": {</p><p>   \"path\": \"/order\",</p><p>   \"accountId\": \"597284863061\",</p><p>   \"resourceId\": \"rmbyew\",</p><p>   \"stage\": \"test-invoke-stage\",</p><p>   \"requestId\": \"dca8be26-7816-11e8-a831-6dbe534d0ae4\",</p><p>   \"identity\": {</p><p>     \"cognitoIdentityPoolId\": null,</p><p>     \"cognitoIdentityId\": null,</p><p>     \"apiKey\": \"test-invoke-api-key\",</p><p>     \"cognitoAuthenticationType\": null,</p><p>     \"userArn\": \"arn:aws:iam::597284863061:user/christophe-workshop\",</p><p>     \"apiKeyId\": \"test-invoke-api-key-id\",</p><p>     \"userAgent\": \"aws-internal/3\",</p><p>     \"accountId\": \"597284863061\",</p><p>     \"caller\": \"AIDAJBAW4OXNUFHSWCLF4\",</p><p>     \"sourceIp\": \"test-invoke-source-ip\",</p><p>     \"accessKey\": \"ASIAIS5LPR47MN7RRAVA\",</p><p>     \"cognitoAuthenticationProvider\": null,</p><p>     \"user\": \"AIDAJBAW4OXNUFHSWCLF4\"</p><p>   },</p><p>   \"resourcePath\": \"/order\",</p><p>   \"httpMethod\": \"POST\",</p><p>   \"extendedRequestId\": \"JA-z9HWIjoEFqdA=\",</p><p>   \"apiId\": \"f6ugtbwyjg\"</p><p> },</p><p> \"body\": \"{\\n    \\\"name\\\" :\\\"joe\\\",\\n    \\\"address\\\" : \\\"Sydney\\\",\\n    \\\"phone\\\": \\\"0123456789\\\",\\n \\\"item\\\": \\\"pizza\\\"}\"</p><p>}</p></div></td>\n</tr></table></div></div><p>Now test your POST a few times. Keep track of one value for order_id uuid as you will need it in a moment. Did you notice the first invocation took longer (maybe a few seconds), while other invocations took a few milliseconds? That’s because at the first invocation, there is some overhead:</p><ul><li>AWS creates one container to run the Lambda</li>\n<li>AWS create a network interface in the VPC, and attaches it to the Lambda</li>\n<li>The Lambda initializes the Cassandra connection.</li>\n</ul><p>AWS will keep around your container for some indeterminate amount of time. Most user reports that the container will stay around for a few hours after the last invocation. That’s something to keep in mind. Importantly, in our test, we are doing serial invocations of our Lambda. If running in parallel, multiple containers will be created by AWS (Lambda auto scale), and the initialization cost will be paid a few times. We will see in our next blog how to optimize this.</p><p>Let’s try our GET method. For that, create a new Test (Configure test events). Let’s call it “get”, and let’s use the following event:</p><div id=\"crayon-5b9a772d261a8623796114\" class=\"crayon-syntax crayon-theme-classic crayon-font-monaco crayon-os-mac print-yes notranslate\" data-settings=\"minimize scroll-mouseover\"><div class=\"crayon-toolbar\" data-settings=\"mouseover overlay hide delay\"><div class=\"crayon-tools\">JavaScript</div></div><div class=\"crayon-main\"><table class=\"crayon-table\"><tr class=\"crayon-row\"><td class=\"crayon-nums\" data-settings=\"show\"> <div class=\"crayon-nums-content\"><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p><p>23</p><p>24</p><p>25</p><p>26</p><p>27</p><p>28</p><p>29</p><p>30</p><p>31</p><p>32</p><p>33</p><p>34</p><p>35</p><p>36</p><p>37</p><p>38</p><p>39</p></div></td>\n<td class=\"crayon-code\"><div class=\"crayon-pre\"><p>{</p><p> \"resource\": \"/order/{id+}\",</p><p> \"path\": \"/order/INSERT_YOUR_UUID\",</p><p> \"httpMethod\": \"GET\",</p><p> \"headers\": null,</p><p> \"queryStringParameters\": null,</p><p> \"pathParameters\": {</p><p>   \"id\": \"INSERT_YOUR_UUID\"</p><p> },</p><p> \"stageVariables\": null,</p><p> \"requestContext\": {</p><p>   \"path\": \"/order/{id+}\",</p><p>   \"accountId\": \"597284863061\",</p><p>   \"resourceId\": \"wm947q\",</p><p>   \"stage\": \"test-invoke-stage\",</p><p>   \"requestId\": \"f88fdaa2-781a-11e8-8075-79171804d71a\",</p><p>   \"identity\": {</p><p>     \"cognitoIdentityPoolId\": null,</p><p>     \"cognitoIdentityId\": null,</p><p>     \"apiKey\": \"test-invoke-api-key\",</p><p>     \"cognitoAuthenticationType\": null,</p><p>     \"userArn\": \"arn:aws:iam::597284863061:user/christophe-workshop\",</p><p>     \"apiKeyId\": \"test-invoke-api-key-id\",</p><p>     \"userAgent\": \"aws-internal/3\",</p><p>     \"accountId\": \"597284863061\",</p><p>     \"caller\": \"AIDAJBAW4OXNUFHSWCLF4\",</p><p>     \"sourceIp\": \"test-invoke-source-ip\",</p><p>     \"accessKey\": \"ASIAIS5LPR47MN7RRAVA\",</p><p>     \"cognitoAuthenticationProvider\": null,</p><p>     \"user\": \"AIDAJBAW4OXNUFHSWCLF4\"</p><p>   },</p><p>   \"resourcePath\": \"/order/{id+}\",</p><p>   \"httpMethod\": \"GET\",</p><p>   \"extendedRequestId\": \"JBDHtGUFjoEFfEg=\",</p><p>   \"apiId\": \"f6ugtbwyjg\"</p><p> },</p><p> \"body\": null,</p><p> \"isBase64Encoded\": false</p><p>}</p></div></td>\n</tr></table></div></div><p>You will need to update (in two places) the code above to replace INSERT_YOUR_UUID with a uuid of an order you posted earlier. Using this you can test your GET command.</p><p>You now have a one time use Lambda for creating the schema and you have Lambdas that handle POST and GET requests.  For now, you can only trigger a test manually. The next step is to hook this up with the API gateway so that requests can be triggered programmatically from the internet.</p><h2>Step 4: Create the AWS API gateway.</h2><h3>Step 4.a Create an empty API.</h3><p>Navigate to the AWS API Gateway console, and create a New API.</p><p><strong>Name:</strong> cassandra-api</p><p><strong>Endpoint Type:</strong> Regional</p><h3>Step 4.b. Create a new order resource.</h3><p>Click on <strong>Actions</strong> drop-down menu and create a new order Resource:</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-7-1.png\"><img class=\"aligncenter wp-image-11115 size-full\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-7-1.png\" alt=\"\" width=\"1600\" height=\"462\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-7-1.png 1600w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-7-1-300x87.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-7-1-768x222.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-7-1-1024x296.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-7-1-1200x347.png 1200w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-7-1-966x279.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-7-1-640x185.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-7-1-166x48.png 166w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-7-1-374x108.png 374w\" /></a></p><h3>Step 4.c. Create a POST method</h3><p>Make sure the <strong>/order</strong> resource is selected, then click on the drop down menu <strong>Actions</strong> and create a new POST method, configured as follow:</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-8-1.png\"><img class=\"aligncenter size-full wp-image-11114\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-8-1.png\" alt=\"\" width=\"1600\" height=\"618\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-8-1.png 1600w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-8-1-300x116.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-8-1-768x297.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-8-1-1024x396.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-8-1-1200x464.png 1200w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-8-1-966x373.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-8-1-640x247.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-8-1-124x48.png 124w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-8-1-280x108.png 280w\" /></a></p><p>Let’s test your POST method. Make sure to select the POST method, then click on TEST. All you need to do is to add a Request Body string, such as:</p><p>If everything works well, you should get an order_id back. Copy the order_id as you will use it soon.</p><h3>Step 4.d. Create a greedy resource</h3><p>Make sure <strong>/order</strong> resource is select, click on <strong>Actions</strong> and create a new resource configured as follow:</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-9-1.png\"><img class=\"aligncenter size-full wp-image-11113\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-9-1.png\" alt=\"\" width=\"1590\" height=\"535\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-9-1.png 1590w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-9-1-300x101.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-9-1-768x258.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-9-1-1024x345.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-9-1-1200x404.png 1200w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-9-1-966x325.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-9-1-640x215.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-9-1-143x48.png 143w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-9-1-321x108.png 321w\" /></a></p><p>You will see that it automatically created the ANY method. You can delete it, and create a GET method:</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-10-1.png\"><img class=\"aligncenter size-full wp-image-11112\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-10-1.png\" alt=\"\" width=\"1600\" height=\"552\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-10-1.png 1600w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-10-1-300x104.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-10-1-768x265.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-10-1-1024x353.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-10-1-1200x414.png 1200w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-10-1-966x333.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-10-1-640x221.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-10-1-139x48.png 139w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-10-1-313x108.png 313w\" /></a></p><p>Let’s test our GET method. Make sure to select the GET method, then click on TEST. This time you will provide the order_id uuid (from Step 4.c.) in the PATH {id} box. If everything works well, you should get back the order.</p><h3>Step 4.e. Deploy your API.</h3><p>Click on the <strong>Actions</strong> drop down menu, and deploy API. Give it a Deployment stage name, let’s use “test”</p><p>The first thing you will want to do is to throttle your API to a low number, i.e. 1 per seconds, just to make sure your API won’t be called a large number of times, as the cost will apply beyond the free tier.</p><h3>Step 4.f Create an API Key (optional)</h3><p>If you want to control the access to your API, you might consider creating an API Key as follow.</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-11-1.png\"><img class=\"aligncenter size-full wp-image-11111\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-11-1.png\" alt=\"\" width=\"1600\" height=\"670\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-11-1.png 1600w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-11-1-300x126.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-11-1-768x322.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-11-1-1024x429.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-11-1-1200x503.png 1200w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-11-1-966x405.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-11-1-640x268.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-11-1-115x48.png 115w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-11-1-258x108.png 258w\" /></a></p><p>Click on <strong>Show</strong> to revel the key, and keep it for later usage.</p><h3>Step 4.g Create a usage plan for your API key (optional)</h3><p>Usage plan lets you control how an API key can be used. Click on Usage Plans. You might need to click on <strong>Enable Usage Plans</strong> first. The AWS page is a little bit buggy here, and you might need to reload the page a few time with your browser.</p><p>Once you have access to the create button, create your Usage Plan, and associate it with the cassandra-api API, with the test Stage. Then associate the Usage Plan with the API key you created.</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-12-1.png\"><img class=\"aligncenter size-full wp-image-11110\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-12-1.png\" alt=\"\" width=\"1600\" height=\"866\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-12-1.png 1600w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-12-1-300x162.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-12-1-768x416.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-12-1-1024x554.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-12-1-1200x650.png 1200w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-12-1-966x523.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-12-1-640x346.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-12-1-89x48.png 89w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-12-1-200x108.png 200w\" /></a></p><h3>Step 4.h Add Authentication to your GET and POST Method (optional)</h3><p>You will need to navigate to the POST resource, and click on the <b>Method Request</b></p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-13-1.png\"><img class=\"aligncenter size-full wp-image-11109\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-13-1.png\" alt=\"\" width=\"1208\" height=\"367\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-13-1.png 1208w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-13-1-300x91.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-13-1-768x233.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-13-1-1024x311.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-13-1-1200x365.png 1200w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-13-1-966x293.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-13-1-640x194.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-13-1-158x48.png 158w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-13-1-355x108.png 355w\" /></a></p><p>Then, set the API Key Required to True. Repeat for the GET method. Don’t forget to redeploy your API (use the same stages: test).</p><h3>Step 4.i Test your API from your Computer.</h3><p>First, retrieve your API endpoint by navigating to the Stages section of your API and clicking on the POST method. This will provide you with the Invoke URL.</p><p>Second, make sure you have your API Keys.</p><p>You can now do a post from a curl / postman etc… Below is an example with curl, in which I am providing an AWS API KEY. If you didn’t configure the API Key, you don’t need to provide it.</p><p>Assuming the id of the new resource is: <code>fb72a94f-c0d9-4bd5-a355-fc8014d125fd</code>, you can retrieve your resource with the following curl command:</p><p>We built a simple scalable REST API using AWS API Gateway to receive API calls, AWS Lambda to execute code, and Cassandra as our backend storage. This POC can be built at next to no cost as <a href=\"https://console.instaclustr.com/user/signup\">Instaclustr provides 14 </a>day free trial on the small developer cluster, and using AWS gateway / AWS Lambda for this POC should remain within the free tier usage (you might still have some small cost, i.e. a few dollars, for data transfer, or for using cloudwatch). This POC demonstrates the simplicity of using a serverless approach where the code is executed by AWS Lambda without managing any ec2 instances. Furthermore the data is stored in a fully managed, scalable, highly available and low latency database –  Cassandra. </p><p>The next step would be to consider performance, which will be the focus of our next blog post.</p>",
        "created_at": "2018-09-13T15:10:51+0000",
        "updated_at": "2018-09-13T15:10:59+0000",
        "published_at": "2018-08-27T15:59:15+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 18,
        "domain_name": "www.instaclustr.com",
        "preview_picture": "https://www.instaclustr.com/wp-content/uploads/2018/08/picture-lambda-blog-2-1.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12168"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 901,
            "label": "security",
            "slug": "security"
          }
        ],
        "is_public": false,
        "id": 12167,
        "uid": null,
        "title": "Apache Cassandra LDAP Authentication - Instaclustr",
        "url": "https://www.instaclustr.com/apache-cassandra-ldap-authentication/",
        "content": "<p>We’ve seen an increasing need for LDAP integration into Apache Cassandra, and continually hearing of cases where people have written their own LDAP authenticators for Cassandra.<br /></p><p>However, if you search around you’ll have a hard time finding one of these implementations, and you’ll likely have to write one yourself, which is no easy feat.</p><p>So, to solve this issue we’ve created an <a href=\"https://github.com/instaclustr/cassandra-ldap\">open source LDAP authenticator</a> plug-in for Apache Cassandra that goes hand in hand with the existing CassandraAuthorizer implementation. At the moment it supports a basic usage of LDAP which should suffice for most cases, however improvements are welcome if you need to modify it to suit your needs and encouraged to submit pull requests for any enhancements.</p><p>This plug-in authenticator is freely available for anyone for use and is also be included in support scope for customers with Apache Cassandra Enterprise Support from Instaclustr.</p><p>The LDAPAuthenticator is implemented using JNDI, and authentication requests will be made by Cassandra to the LDAP server using the username and password provided by the client. At this time only plain text authentication is supported. </p><p>If you configure a service LDAP user in the ldap.properties file, on startup Cassandra will authenticate the service user and create a corresponding role in the system_auth.roles table. This service user will then be used for future authentication requests received from clients. Alternatively (not recommended), if you have anonymous access enabled for your LDAP server, the authenticator allows authentication without a service user configured. The service user will be configured as a superuser role in Cassandra, and you will need to log in as the service user to define permissions for other users once they have authenticated.</p><p>On successful authentication of a client, a corresponding role will be created in the <code>system_auth.roles</code> table. The password for the role <i>is not</i> stored in the roles table, and credentials will always be passed through directly to the configured LDAP server. The only credentials stored in Cassandra is the Distinguished Name of the user. However, if caching is enabled the password/hashed password will be stored in the cache, in memory only, on the nodes. Permissions-wise, this role will have no access to any keyspaces/tables, so GRANT’s will need to be issued before the user can perform any useful queries.</p><p><br />Once created, the role will never be deleted, and all authentication of the role will be handled through LDAP while the LDAPAuthenticator is in place. Removing or disabling the user in LDAP will disallow future connections as that user, but not clean up the user from <code>system_auth.roles</code>. This can be done manually if so desired and should be done if you wish to switch to a different authentication mechanism.<br />Regarding security, as the authenticator only supports plain text from clients you should ensure you have enabled and are using client encryption in Cassandra. On the LDAP side, you <i>must</i> use LDAPS otherwise credentials will be sent in the clear between Cassandra and the LDAP server. As all SSL configuration is performed through JNDI, simply specifying LDAPS as your protocol for the LDAP server (assuming it’s enabled on your server) will enable LDAPS.  </p><p>On 3.11 and later versions, a cache has been implemented to avoid thrashing your LDAP server. This cache will be populated with the username and either the provided password or a hash of the password based on the cache_hashed_password property in ldap.properties. Note that hashing the password will incur a performance hit as the hash needs to be calculated on each auth. The password/hash is only stored in memory on the Cassandra nodes, so if you don’t enable hashing ensure appropriate security controls are in place for your Cassandra nodes.</p><p>LDAP JNDI properties can be set via the <code>ldap.properties</code> file. Simply specify the desired property and it will be set as part of the servers context. For example, you can set the LDAP read timeout like so:</p><p><code>    com.sun.jndi.ldap.read.timeout: 2000<br /></code><br />These properties and their documentation can be found <a href=\"https://docs.oracle.com/javase/8/docs/technotes/guides/jndi/jndi-ldap.html\">here</a>.</p><p>To transition to the <code>LDAPAuthenticator</code> you can do so in a rolling fashion with no downtime as long as you are using <code>AllowAllAuthenticator</code> to start with, or you handle auth failures from both LDAP and your old password authenticator in your client and try the alternate auth on the next request. Pre-creating the LDAP users in <code>system.roles</code> is also possible however not recommended as you will need to store the LDAP user passwords in Cassandra for it to work. </p><p>Alternatively you can do the switch with downtime with no issues however this requires turning off all the nodes simultaneously. To ensure no errors on startup due to creation of service roles you should start one node first and wait until it’s running before starting the rest of the nodes.<br /></p><p>You can find the LDAP authenticator source code on GitHub <a href=\"https://github.com/instaclustr/cassandra-ldap\">here</a>, with instructions on setup and usage in the README. Currently the authenticator is supported for 2.2, 3.0, and 3.11 versions. Use the corresponding branch in the repo for your desired version.</p>",
        "created_at": "2018-09-13T15:10:18+0000",
        "updated_at": "2018-09-13T15:10:22+0000",
        "published_at": "2018-06-29T15:30:45+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 4,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12167"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1052,
            "label": "tools",
            "slug": "tools"
          }
        ],
        "is_public": false,
        "id": 12166,
        "uid": null,
        "title": "Instaclustr Open-sources Cassandra sstable Analysis Tools",
        "url": "https://www.instaclustr.com/instaclustr-open-sources-cassandra-sstable-analysis-tools/",
        "content": "<p>At Instaclustr we spend a lot of time managing Cassandra clusters – we have team of engineers that 24×7 do nothing but manage Cassandra clusters. Big clusters, tiny clusters, clusters with awesome data models and clusters with less awesome data models – we manage them all.</p><p>Over time, we’ve developed a lot of tricks and tools to help us in this job. We’re happy to announce that, as part of our commitment to the <a href=\"https://www.instaclustr.com/apache-cassandra/\">Apache Cassandra</a> open source community, we’re making our most generally useful tools available for open use.</p><p>The tools (that we’ve imaginatively called ic-tools) supplement the information available from the nodetool utility that is part of core <a href=\"https://www.instaclustr.com/apache-cassandra/\">Apache Cassandra</a>. Whereas nodetool tends to report based on summary statistics maintained as Cassandra services operate, ic-tools directly reads Cassandra’s data files when executed. This allows reporting of more detailed and accurate statistics.</p><p>We’ve found the information available from these tools to be invaluable in answering questions to help diagnose Cassandra issues or just better understand what Cassandra is doing with your data. The information available from the tools is pretty broad. Some highlights that will resonate with many Cassandra users include:</p><ul><li>Partition keys of the largest partitions by data size, number of columns and sstables spanned</li> <li>Information about data age (timespan) of data in sstables</li> <li>Tombstone information including partition keys of partitions with the most tombstones and calculation of potentially reclaimable space if/when tombstones are purged</li> </ul><p>This is just a highlights list of key data. See the <a href=\"https://instaclustr.zendesk.com/hc/en-us/articles/235656328\">help page</a> and examples below for a more complete list.</p><p>The tools are available on a supported basis for our enterprise support customers and on an unsupported basis for the general community (although we’ll probably answer questions on the C* user email list). For users of <a href=\"https://www.instaclustr.com/platform/\">Instaclustr’s Managed Service</a>, our Technical Operations team will run these as needed when working with you to help diagnose issues.</p><p>The source code is published on our <a href=\"https://github.com/instaclustr/cassandra-sstable-tools\">github</a>. We’re more than happy to take pull requests and other suggestions for improvements. We’ll also be talking to the C* project to see if any of this code makes sense in the core project.</p><p>We hope these tools will be as useful for the rest of the Cassandra community as we’ve found them in our work. Let us know in the comments if you have any feedack.</p><div class=\"foogallery foogallery-container foogallery-image-viewer foogallery-link-image foogallery-lightbox-foobox-free fg-center fg-image-viewer fg-light fg-border-thin fg-shadow-outline fg-loading-default fg-loaded-fade-in\" id=\"foogallery-gallery-4274\" data-foogallery=\"{&quot;item&quot;:{&quot;showCaptionTitle&quot;:false,&quot;showCaptionDescription&quot;:true},&quot;lazy&quot;:true,&quot;src&quot;:&quot;data-src-fg&quot;,&quot;srcset&quot;:&quot;data-srcset-fg&quot;}\" data-fg-common-fields=\"1\"><div class=\"fiv-inner\"><div class=\"fiv-ctrls\"><p>Prev</p><label class=\"fiv-count\">1of5</label><p>Next</p></div></div></div><p>Rotate through the gallery above for more information.</p>",
        "created_at": "2018-09-13T15:10:09+0000",
        "updated_at": "2018-09-13T15:10:13+0000",
        "published_at": "2017-02-13T02:23:38+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12166"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1052,
            "label": "tools",
            "slug": "tools"
          }
        ],
        "is_public": false,
        "id": 12165,
        "uid": null,
        "title": "instaclustr/cassandra-sstable-tools",
        "url": "https://github.com/instaclustr/cassandra-sstable-tools",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<pre>$ git clone git@github.com:instaclustr/cassandra-sstable-tools.git\n$ cd cassandra-sstable-tools\n# Select the correct branch for major version (default is cassandra-3.11)\n$ git checkout cassandra-3.11\n$ ant\n</pre>\n<p>You can compile against an older minor version with <code>-Dcassandra.version=&lt;version&gt;</code>. For example:</p>\n<pre>$ ant -Dcassandra.version=3.11.2\n</pre>\n<p>However only the version specified in build.xml is officially supported,\nas compatibility between minor versions can break.</p>\n<p>Copy ic-sstable-tools.jar to Cassandra JAR folder, eg. <code>/usr/share/cassandra</code></p>\n<p>Copy the bin/ic-* files into your $PATH</p>\n<table><thead><tr><th>Command</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>ic-summary</td>\n<td>Summary information about all column families including how much of the data is repaired</td>\n</tr><tr><td>ic-sstables</td>\n<td>Print out metadata for sstables the belong to a column family</td>\n</tr><tr><td>ic-pstats</td>\n<td>Partition size statistics for a column family</td>\n</tr><tr><td>ic-cfstats</td>\n<td>Detailed statistics about cells in a column family</td>\n</tr><tr><td>ic-purge</td>\n<td>Statistics about reclaimable data for a column family</td>\n</tr></tbody></table><h2><a id=\"user-content-ic-summary\" class=\"anchor\" aria-hidden=\"true\" href=\"#ic-summary\"></a>ic-summary</h2>\n<p>Provides summary information about all column families. Useful for finding\nthe largest column families and how much data has been repaired by incremental repairs.</p>\n<h3><a id=\"user-content-usage\" class=\"anchor\" aria-hidden=\"true\" href=\"#usage\"></a>Usage</h3>\n<pre>ic-summary\n</pre>\n<h3><a id=\"user-content-output\" class=\"anchor\" aria-hidden=\"true\" href=\"#output\"></a>Output</h3>\n<table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>Keyspace</td>\n<td>Keyspace the column family belongs to</td>\n</tr><tr><td>Column Family</td>\n<td>Name of column family</td>\n</tr><tr><td>SSTables</td>\n<td>Number of sstables on this node for the column family</td>\n</tr><tr><td>Disk Size</td>\n<td>Compressed size on disk for this node</td>\n</tr><tr><td>Data Size</td>\n<td>Uncompressed size of the data for this node</td>\n</tr><tr><td>Last Repaired</td>\n<td>Maximum repair timestamp on sstables</td>\n</tr><tr><td>Repair %</td>\n<td>Percentage of data marked as repaired</td>\n</tr></tbody></table><h2><a id=\"user-content-ic-sstables\" class=\"anchor\" aria-hidden=\"true\" href=\"#ic-sstables\"></a>ic-sstables</h2>\n<p>Print out sstable metadata for a column family. Useful in helping to tune compaction settings.</p>\n<h3><a id=\"user-content-usage-1\" class=\"anchor\" aria-hidden=\"true\" href=\"#usage-1\"></a>Usage</h3>\n<pre>ic-sstables &lt;keyspace&gt; &lt;column-family&gt;\n</pre>\n<h3><a id=\"user-content-output-1\" class=\"anchor\" aria-hidden=\"true\" href=\"#output-1\"></a>Output</h3>\n<table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>SSTable</td>\n<td>Data.db filename of sstable</td>\n</tr><tr><td>Disk Size</td>\n<td>Size of sstable on disk</td>\n</tr><tr><td>Total Size</td>\n<td>Uncompressed size of data contained in the sstable</td>\n</tr><tr><td>Min Timestamp</td>\n<td>Minimum cell timestamp contained in the sstable</td>\n</tr><tr><td>Max Timestamp</td>\n<td>Maximum cell timestamp contained in the sstable</td>\n</tr><tr><td>Duration</td>\n<td>The time span between minimum and maximum cell timestamps</td>\n</tr><tr><td>Min Deletion Time</td>\n<td>The minimum deletion time</td>\n</tr><tr><td>Max Deletion Time</td>\n<td>The maximum deletion time</td>\n</tr><tr><td>Level</td>\n<td>Leveled Tiered Compaction sstable level</td>\n</tr><tr><td>Keys</td>\n<td>Number of partition keys</td>\n</tr><tr><td>Avg Partition Size</td>\n<td>Average partition size</td>\n</tr><tr><td>Max Partition Size</td>\n<td>Maximum partition size</td>\n</tr><tr><td>Avg Column Count</td>\n<td>Average number of columns in a partition</td>\n</tr><tr><td>Max Column Count</td>\n<td>Maximum number of columns in a partition</td>\n</tr><tr><td>Droppable</td>\n<td>Estimated droppable tombstones</td>\n</tr><tr><td>Repaired At</td>\n<td>Time when marked as repaired by incremental repair</td>\n</tr></tbody></table><h2><a id=\"user-content-ic-pstats\" class=\"anchor\" aria-hidden=\"true\" href=\"#ic-pstats\"></a>ic-pstats</h2>\n<p>Tool for finding largest partitions. Reads the Index.db files so is relatively quick.</p>\n<h3><a id=\"user-content-usage-2\" class=\"anchor\" aria-hidden=\"true\" href=\"#usage-2\"></a>Usage</h3>\n<pre>ic-pstats [-n &lt;num&gt;] [-t &lt;snapshot&gt;] [-f &lt;filter&gt;] &lt;keyspace&gt; &lt;column-family&gt;\n</pre>\n<table><thead><tr><th>-h</th>\n<th>Display help</th>\n</tr></thead><tbody><tr><td>-b</td>\n<td>Batch mode. Uses progress indicator that is friendly for running in batch jobs.</td>\n</tr><tr><td>-n </td>\n<td>Number of partitions to display</td>\n</tr><tr><td>-t </td>\n<td>Snapshot to analyse. Snapshot is created if none is specified.</td>\n</tr><tr><td>-f </td>\n<td>Comma separated list of Data.db sstables to filter on</td>\n</tr></tbody></table><h3><a id=\"user-content-output-2\" class=\"anchor\" aria-hidden=\"true\" href=\"#output-2\"></a>Output</h3>\n<p>Summary: Summary statistics about partitions</p>\n<table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>Count (Size)</td>\n<td>Number of partition keys on this node</td>\n</tr><tr><td>Total (Size)</td>\n<td>Total uncompressed size of all partitions on this node</td>\n</tr><tr><td>Total (SSTable)</td>\n<td>Number of sstables on this node</td>\n</tr><tr><td>Minimum (Size)</td>\n<td>Minimum uncompressed partition size</td>\n</tr><tr><td>Minimum (SSTable)</td>\n<td>Minimum number of sstables a partition belongs to</td>\n</tr><tr><td>Average (Size)</td>\n<td>Average (mean) uncompressed partition size</td>\n</tr><tr><td>Average (SSTable)</td>\n<td>Average (mean) number of sstables a partition belongs to</td>\n</tr><tr><td>std dev. (Size)</td>\n<td>Standard deviation of partition sizes</td>\n</tr><tr><td>std dev. (SSTable)</td>\n<td>Standard deviation of number of sstables for a partition</td>\n</tr><tr><td>50% (Size)</td>\n<td>Estimated 50th percentile of partition sizes</td>\n</tr><tr><td>50% (SSTable)</td>\n<td>Estimated 50th percentile of sstables for a partition</td>\n</tr><tr><td>75% (Size)</td>\n<td>Estimated 75th percentile of partition sizes</td>\n</tr><tr><td>75% (SSTable)</td>\n<td>Estimated 75th percentile of sstables for a partition</td>\n</tr><tr><td>90% (Size)</td>\n<td>Estimated 90th percentile of partition sizes</td>\n</tr><tr><td>90% (SSTable)</td>\n<td>Estimated 90th percentile of sstables for a partition</td>\n</tr><tr><td>95% (Size)</td>\n<td>Estimated 95th percentile of partition sizes</td>\n</tr><tr><td>95% (SSTable)</td>\n<td>Estimated 95th percentile of sstables for a partition</td>\n</tr><tr><td>99% (Size)</td>\n<td>Estimated 99th percentile of partition sizes</td>\n</tr><tr><td>99% (SSTable)</td>\n<td>Estimated 99th percentile of sstables for a partition</td>\n</tr><tr><td>99.9% (Size)</td>\n<td>Estimated 99.9th percentile of partition sizes</td>\n</tr><tr><td>99.9% (SSTable)</td>\n<td>Estimated 99.9th percentile of sstables for a partition</td>\n</tr><tr><td>Maximum (Size)</td>\n<td>Maximum uncompressed partition size</td>\n</tr><tr><td>Maximum (SSTable)</td>\n<td>Maximum number of sstables a partition belongs to</td>\n</tr></tbody></table><p>Largest partitions: The top N largest partitions</p>\n<table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>Key</td>\n<td>The partition key</td>\n</tr><tr><td>Size</td>\n<td>Total uncompressed size of the partition</td>\n</tr><tr><td>SSTable Count</td>\n<td>Number of sstables that contain the partition</td>\n</tr></tbody></table><p>SSTable Leaders: The top N partitions that belong to the most sstables</p>\n<table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>Key</td>\n<td>The partition key</td>\n</tr><tr><td>SSTable Count</td>\n<td>Number of sstables that contain the partition</td>\n</tr><tr><td>Size</td>\n<td>Total uncompressed size of the partition</td>\n</tr></tbody></table><p>SSTables: Metadata about sstables as it relates to partitions.</p>\n<table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>SSTable</td>\n<td>Data.db filename of SSTable</td>\n</tr><tr><td>Size</td>\n<td>Uncompressed size</td>\n</tr><tr><td>Min Timestamp</td>\n<td>Minimum cell timestamp in the sstable</td>\n</tr><tr><td>Max Timestamp</td>\n<td>Maximum cell timestamp in the sstable</td>\n</tr><tr><td>Level</td>\n<td>Leveled Tiered Compaction level of sstable</td>\n</tr><tr><td>Partitions</td>\n<td>Number of partition keys in the sstable</td>\n</tr><tr><td>Avg Partition Size</td>\n<td>Average uncompressed partition size in sstable</td>\n</tr><tr><td>Max Partition Size</td>\n<td>Maximum uncompressed partition size in sstable</td>\n</tr></tbody></table><h2><a id=\"user-content-ic-cfstats\" class=\"anchor\" aria-hidden=\"true\" href=\"#ic-cfstats\"></a>ic-cfstats</h2>\n<p>Tool for getting detailed cell statistics that can help identify issues with data model.</p>\n<h3><a id=\"user-content-usage-3\" class=\"anchor\" aria-hidden=\"true\" href=\"#usage-3\"></a>Usage</h3>\n<pre>ic-cfstats [-r &lt;limit&gt;] [-n &lt;num&gt;] [-t &lt;snapshot&gt;] [-f &lt;filter&gt;] &lt;keyspace&gt; &lt;column-family&gt;\n</pre>\n<table><thead><tr><th>-h</th>\n<th>Display help</th>\n</tr></thead><tbody><tr><td>-b</td>\n<td>Batch mode. Uses progress indicator that is friendly for running in batch jobs.</td>\n</tr><tr><td>-r </td>\n<td>Limit read throughput to ratelimit MB/s</td>\n</tr><tr><td>-n </td>\n<td>Number of partitions to display</td>\n</tr><tr><td>-t </td>\n<td>Snapshot to analyse. Snapshot is created if none is specified.</td>\n</tr><tr><td>-f </td>\n<td>Comma separated list of Data.db sstables to filter on</td>\n</tr></tbody></table><h3><a id=\"user-content-output-3\" class=\"anchor\" aria-hidden=\"true\" href=\"#output-3\"></a>Output</h3>\n<p>Summary: Summary statistics about partitions</p>\n<table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>Count (Size)</td>\n<td>Number of partition keys on this node</td>\n</tr><tr><td>Rows (Size)</td>\n<td>Number of clustering rows</td>\n</tr><tr><td>(deleted)</td>\n<td>Number of clustering row deletions</td>\n</tr><tr><td>Total (Size)</td>\n<td>Total uncompressed size of all partitions on this node</td>\n</tr><tr><td>Total (SSTable)</td>\n<td>Number of sstables on this node</td>\n</tr><tr><td>Minimum (Size)</td>\n<td>Minimum uncompressed partition size</td>\n</tr><tr><td>Minimum (SSTable)</td>\n<td>Minimum number of sstables a partition belongs to</td>\n</tr><tr><td>Average (Size)</td>\n<td>Average (mean) uncompressed partition size</td>\n</tr><tr><td>Average (SSTable)</td>\n<td>Average (mean) number of sstables a partition belongs to</td>\n</tr><tr><td>std dev. (Size)</td>\n<td>Standard deviation of partition sizes</td>\n</tr><tr><td>std dev. (SSTable)</td>\n<td>Standard deviation of number of sstables for a partition</td>\n</tr><tr><td>50% (Size)</td>\n<td>Estimated 50th percentile of partition sizes</td>\n</tr><tr><td>50% (SSTable)</td>\n<td>Estimated 50th percentile of sstables for a partition</td>\n</tr><tr><td>75% (Size)</td>\n<td>Estimated 75th percentile of partition sizes</td>\n</tr><tr><td>75% (SSTable)</td>\n<td>Estimated 75th percentile of sstables for a partition</td>\n</tr><tr><td>90% (Size)</td>\n<td>Estimated 90th percentile of partition sizes</td>\n</tr><tr><td>90% (SSTable)</td>\n<td>Estimated 90th percentile of sstables for a partition</td>\n</tr><tr><td>95% (Size)</td>\n<td>Estimated 95th percentile of partition sizes</td>\n</tr><tr><td>95% (SSTable)</td>\n<td>Estimated 95th percentile of sstables for a partition</td>\n</tr><tr><td>99% (Size)</td>\n<td>Estimated 99th percentile of partition sizes</td>\n</tr><tr><td>99% (SSTable)</td>\n<td>Estimated 99th percentile of sstables for a partition</td>\n</tr><tr><td>99.9% (Size)</td>\n<td>Estimated 99.9th percentile of partition sizes</td>\n</tr><tr><td>99.9% (SSTable)</td>\n<td>Estimated 99.9th percentile of sstables for a partition</td>\n</tr><tr><td>Maximum (Size)</td>\n<td>Maximum uncompressed partition size</td>\n</tr><tr><td>Maximum (SSTable)</td>\n<td>Maximum number of sstables a partition belongs to</td>\n</tr></tbody></table><p>Row Histogram: Histogram of number of rows per partition</p>\n<table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>Percentile</td>\n<td>Minimum, average, standard deviation (std dev.), percentile, maximum</td>\n</tr><tr><td>Count</td>\n<td>Estimated number of rows per partition for the given percentile</td>\n</tr></tbody></table><p>Largest partitions: Partitions with largest uncompressed size</p>\n<table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>Key</td>\n<td>The partition key</td>\n</tr><tr><td>Size</td>\n<td>Total uncompressed size of the partition</td>\n</tr><tr><td>Rows</td>\n<td>Total number of clustering rows in the partition</td>\n</tr><tr><td>(deleted)</td>\n<td>Number of row deletions in the partition</td>\n</tr><tr><td>Tombstones</td>\n<td>Number of cell or range tombstones</td>\n</tr><tr><td>(droppable)</td>\n<td>Number of tombstones that can be dropped as per gc_grace_seconds</td>\n</tr><tr><td>Cells</td>\n<td>Number of cells in the partition</td>\n</tr><tr><td>SSTable Count</td>\n<td>Number of sstables that contain the partition</td>\n</tr></tbody></table><p>Widest partitions: Partitions with the most cells</p>\n<table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>Key</td>\n<td>The partition key</td>\n</tr><tr><td>Rows</td>\n<td>Total number of clustering rows in the partition</td>\n</tr><tr><td>(deleted)</td>\n<td>Number of row deletions in the partition</td>\n</tr><tr><td>Cells</td>\n<td>Number of cells in the partition</td>\n</tr><tr><td>Tombstones</td>\n<td>Number of cell or range tombstones</td>\n</tr><tr><td>(droppable)</td>\n<td>Number of tombstones that can be dropped as per gc_grace_seconds</td>\n</tr><tr><td>Size</td>\n<td>Total uncompressed size of the partition</td>\n</tr><tr><td>SSTable Count</td>\n<td>Number of sstables that contain the partition</td>\n</tr></tbody></table><p>Most Deleted Rows: Partitions with the most row deletions</p>\n<table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>Key</td>\n<td>The partition key</td>\n</tr><tr><td>Rows</td>\n<td>Total number of clustering rows in the partition</td>\n</tr><tr><td>(deleted)</td>\n<td>Number of row deletions in the partition</td>\n</tr><tr><td>Size</td>\n<td>Total uncompressed size of the partition</td>\n</tr><tr><td>SSTable Count</td>\n<td>Number of sstables that contain the partition</td>\n</tr></tbody></table><p>Tombstone Leaders: Partitions with the most tombstones</p>\n<table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>Key</td>\n<td>The partition key</td>\n</tr><tr><td>Tombstones</td>\n<td>Number of cell or range tombstones</td>\n</tr><tr><td>(droppable)</td>\n<td>Number of tombstones that can be dropped as per gc_grace_seconds</td>\n</tr><tr><td>Rows</td>\n<td>Total number of clustering rows in the partition</td>\n</tr><tr><td>Cells</td>\n<td>Number of cells in the partition</td>\n</tr><tr><td>Size</td>\n<td>Total uncompressed size of the partition</td>\n</tr><tr><td>SSTable Count</td>\n<td>Number of sstables that contain the partition</td>\n</tr></tbody></table><p>SSTable Leaders: Partitions that are in the most sstables</p>\n<table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>Key</td>\n<td>The partition key</td>\n</tr><tr><td>SSTable Count</td>\n<td>Number of sstables that contain the partition</td>\n</tr><tr><td>Size</td>\n<td>Total uncompressed size of the partition</td>\n</tr><tr><td>Rows</td>\n<td>Total number of clustering rows in the partition</td>\n</tr><tr><td>Cells</td>\n<td>Number of cells in the partition</td>\n</tr><tr><td>Tombstones</td>\n<td>Number of cell or range tombstones</td>\n</tr><tr><td>(droppable)</td>\n<td>Number of tombstones that can be dropped as per gc_grace_seconds</td>\n</tr></tbody></table><p>SSTables: Metadata about sstables as it relates to partitions.</p>\n<table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>SSTable</td>\n<td>Data.db filename of SSTable</td>\n</tr><tr><td>Size</td>\n<td>Uncompressed size</td>\n</tr><tr><td>Min Timestamp</td>\n<td>Minimum cell timestamp in the sstable</td>\n</tr><tr><td>Max Timestamp</td>\n<td>Maximum cell timestamp in the sstable</td>\n</tr><tr><td>Partitions</td>\n<td>Number of partitions</td>\n</tr><tr><td>(deleted)</td>\n<td>Number of row level partition deletions</td>\n</tr><tr><td>(avg size)</td>\n<td>Average uncompressed partition size in sstable</td>\n</tr><tr><td>(max size)</td>\n<td>Maximum uncompressed partition size in sstable</td>\n</tr><tr><td>Rows</td>\n<td>Total number of clustering rows in sstable</td>\n</tr><tr><td>(deleted)</td>\n<td>Number of row deletions in sstable</td>\n</tr><tr><td>Cells</td>\n<td>Number of cells in the SSTable</td>\n</tr><tr><td>Tombstones</td>\n<td>Number of cell or range tombstones in the SSTable</td>\n</tr><tr><td>(droppable)</td>\n<td>Number of tombstones that are droppable according to gc_grace_seconds</td>\n</tr><tr><td>(range)</td>\n<td>Number of range tombstones</td>\n</tr><tr><td>Cell Liveness</td>\n<td>Percentage of live cells. Does not consider tombstones or cell updates shadowing cells. That is it is percentage of non-tombstoned cells to total number of cells.</td>\n</tr></tbody></table><h2><a id=\"user-content-ic-purge\" class=\"anchor\" aria-hidden=\"true\" href=\"#ic-purge\"></a>ic-purge</h2>\n<p>Finds the largest reclaimable partitions (GCable). Intensive process, effectively does \"fake\" compactions to calculate metrics.</p>\n<h3><a id=\"user-content-usage-4\" class=\"anchor\" aria-hidden=\"true\" href=\"#usage-4\"></a>Usage</h3>\n<pre>ic-purge [-r &lt;limit&gt;] [-n &lt;num&gt;] [-t &lt;snapshot&gt;] [-f &lt;filter&gt;] &lt;keyspace&gt; &lt;column-family&gt;\n</pre>\n<table><thead><tr><th>-h</th>\n<th>Display help</th>\n</tr></thead><tbody><tr><td>-b</td>\n<td>Batch mode. Uses progress indicator that is friendly for running in batch jobs.</td>\n</tr><tr><td>-r </td>\n<td>Limit read throughput to ratelimit MB/s</td>\n</tr><tr><td>-n </td>\n<td>Number of partitions to display</td>\n</tr><tr><td>-t </td>\n<td>Snapshot to analyse. Snapshot is created if none is specified.</td>\n</tr></tbody></table><h3><a id=\"user-content-output-4\" class=\"anchor\" aria-hidden=\"true\" href=\"#output-4\"></a>Output</h3>\n<p>Largest reclaimable partitions: Partitions with the largest amount of reclaimable data</p>\n<table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>Key</td>\n<td>The partition key</td>\n</tr><tr><td>Size</td>\n<td>Total uncompressed size of the partition</td>\n</tr><tr><td>Reclaim</td>\n<td>Reclaimable uncompressed size</td>\n</tr><tr><td>Generations</td>\n<td>SSTable generations the partition belongs to</td>\n</tr></tbody></table></article>",
        "created_at": "2018-09-13T15:10:01+0000",
        "updated_at": "2018-09-13T15:10:03+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 8,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/11550580?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12165"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1052,
            "label": "tools",
            "slug": "tools"
          }
        ],
        "is_public": false,
        "id": 12164,
        "uid": null,
        "title": "Update released for Instaclustr sstable analysis tools for Apache Cassandra - Instaclustr",
        "url": "https://www.instaclustr.com/update-released-instaclustr-sstable-analysis-tools-apache-cassandra/",
        "content": "<header id=\"page-masthead\"><div id=\"page-content\"><div class=\"container\"><div class=\"row\"><div class=\"col-md-6 col-md-offset-0 col-sm-8 col-sm-offset-2\"><div class=\"primary first-p-bold\"><p>Instaclustr is pleased to announce the latest update for it’s open-sourced sstable analysis tools for Apache Cassandra.</p><p>These tools, first released in February 2017, help operators to gain an accurate picture of the on-disk data stored by Cassandra which can be invaluable in diagnose and resolving operational issues.</p><p>A full run-through of the existing functionality of the tools can be found in <a href=\"https://www.instaclustr.com/instaclustr-open-sources-cassandra-sstable-analysis-tools/\">this blog post</a>.</p><p>This latest release, available in source from <a href=\"https://github.com/instaclustr/cassandra-sstable-tools\">Instaclustr Github</a> or in compiled download from our <a href=\"https://www.instaclustr.com/support/documentation/tools/ic-tools-for-cassandra-sstables/\">support page</a>, </p><ul><li>Improved support of TWCS – sorting by maximum timestamp in the sstable listing</li> <li>For Apache Cassandra 3.x, ic-cfstats reports about rows including: <ul><li>Total number of rows</li> <li>Total number of row deletions</li> <li>A row histogram giving number of rows per partition</li> <li>Largest partitions/widest partitions report includes number of rows and how many row deletions</li> <li>Added a Most Deleted Rows section reporting partitions with most row deletions</li> <li>Tombstone Leaders and SSTable Leaders reports number of rows</li> </ul></li> <li>ic-cfstats and ic-pstats includes histogram of partition sizes and sstables/partition in the summary</li> </ul><p>Instaclustr uses these tools to help support customers on our <a href=\"https://www.instaclustr.com/solutions/managed-apache-cassandra/\">Apache Cassandra Managed Service</a> and provides support for the use of the tools for our <a href=\"https://www.instaclustr.com/services/cassandra-support/\">Apache Cassandra Enterprise Support</a> customers.</p></div></div></div></div></div></header>",
        "created_at": "2018-09-13T15:09:49+0000",
        "updated_at": "2018-09-13T15:09:54+0000",
        "published_at": "2018-08-21T16:57:58+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12164"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 902,
            "label": "kubernetes",
            "slug": "kubernetes"
          }
        ],
        "is_public": false,
        "id": 12163,
        "uid": null,
        "title": "Why We Built an Open Source Cassandra-Operator to Run Apache Cassandra on Kubernetes - Instaclustr",
        "url": "https://www.instaclustr.com/why-we-built-apache-cassandra-operator-to-run-on-kubernetes/",
        "content": "<p>As Kubernetes becomes the de facto for container orchestration, more and more developers (and enterprises) want to run Apache Cassandra on Kubernetes. It’s easy to get started with this – especially considering the capabilities that Kubernetes’ <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\">StatefulSets</a> bring to the table. Kubernetes, though, certainly has room to improve when it comes to storing data in-state and understanding how different databases work.</p><p>For example, Kubernetes doesn’t know if you’re writing to a leader or a follower database, to a multi-sharded leader infrastructure, or to a single database instance. StatefulSets – workload API objects used to manage stateful applications – offer the building blocks required for stable unique network identifiers, stable persistent storage, ordered and smooth deployment and scaling, deletion and termination, and automated rolling updates. However, while getting <i>started</i> with Cassandra on Kubernetes might be easy, it can still be a challenge to run and manage (and running Docker is another challenge in itself). </p><p>To overcome some of these hurdles, we decided to build an open source Cassandra-operator that runs and operates Cassandra within Kubernetes. Think of it as Cassandra-as-a-Service on top of Kubernetes. We’ve made this Cassandra-operator open source and freely available on <a href=\"https://github.com/instaclustr/cassandra-operator\">GitHub</a>. It remains a work in progress between myself, others on my team, and a number of partner contributors – but it is functional and ready for use. The Cassandra-operator supports Docker images, which are open source and available as well (via <a href=\"https://github.com/instaclustr/cassandra-operator\">the same link</a>).</p><p>This Cassandra-operator is designed to provide “operations-free” Cassandra: it takes care of deployment and allows users to manage and run Cassandra, in a safe way, within Kubernetes environments. It also makes it simple to utilize consistent and reproducible environments. </p><p>While it’s possible for developers to build scripts for managing and running Cassandra on Kubernetes, the Cassandra-operator offers the advantage of providing the same consistent reproducible environment, as well as the same consistent reproducible set of operations through different production clusters. And this is true across development, staging, and QA environments. Furthermore, because best practices are already built into the operator, development teams are spared from operational concerns and are able to focus on their core capabilities.</p><h2><b>What is a Kubernetes operator?</b></h2><p>A Kubernetes operator consists of two components: a controller and a custom resource definition (CRD). The CRD allows devs to create Cassandra objects in Kubernetes. It’s an extension of Kubernetes that allows us to define custom objects or resources using Kubernetes that our controller can then listen to for any changes to the resource definition. Devs can define an object in Kubernetes that contains configuration options for Cassandra, such as cluster name, node count, jvm tuning options, etc. – all the information you want to give Kubernetes about how to deploy Cassandra. </p><p>You can isolate the Cassandra-operator to a specific Kubernetes namespace, define what kinds of persistent volumes it should use, and more. The Cassandra-operator controller listens to state changes on the Cassandra CRD and will create its own StatefulSets to match those requirements. It will also manage those operations and can ensure repairs, backups, and safe scaling as specified via the CRD. In this way, it leverages the Kubernetes concept of building controllers upon other controllers in order to achieve intelligent and helpful behaviours. </p><h2><b>How does it work?</b></h2><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/09/Page-2.png\"><img class=\"aligncenter size-full wp-image-11231\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/09/Page-2.png\" alt=\"\" width=\"3508\" height=\"2482\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/09/Page-2.png 3508w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/09/Page-2-300x212.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/09/Page-2-768x543.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/09/Page-2-1024x725.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/09/Page-2-1200x849.png 1200w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/09/Page-2-871x616.png 871w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/09/Page-2-640x453.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/09/Page-2-68x48.png 68w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/09/Page-2-153x108.png 153w\" /></a></p><p>Architecturally, the Cassandra controller itself connects to the Kubernetes Master. It listens to state changes and manipulates Pod definitions and CRDs. It then deploys those, waits for changes to occur, and repeats until the entirety of necessary changes is fully completed.</p><p>The Cassandra controller can, of course, perform operations within the Cassandra cluster itself. For example, want to scale down your Cassandra cluster? Instead of manipulating the StatefulSet to handle this task, the controller will first see the CRD change. The node count will change to a lower number (say from six to five). The controller will get that state change, and it will first run a decommission operation on the Cassandra node that’s going to be removed. This ensures that the Cassandra node stops gracefully and that it will redistribute and rebalance the data it held across the remaining nodes. Once the Cassandra controller sees that this has happened successfully, it will modify that StatefulSet definition to allow Kubernetes to finally decommission that particular Pod. Thus, the Cassandra controller brings needed intelligence to the Kubernetes environment to run Cassandra properly and ensure smoother operations.</p><p>As we continue this project and iterate on the Cassandra-operator, our goal is to add new components that will continue to expand the tool’s features and value. A good example is the Cassandra SideCar (included in the diagram above), which will begin to take responsibility for tasks like backups and repairs. Current and future features of the project can be <a href=\"https://github.com/instaclustr/cassandra-operator/issues\">viewed on GitHub</a>. Our goal for the Cassandra-operator is to give devs a powerful open source option for running Cassandra on Kubernetes with a simplicity and grace that has not yet been all that easy to achieve.</p><p><i>Ben Bromhead is CTO at </i><a href=\"https://www.instaclustr.com\"><i>Instaclustr</i></a><i>, which provides a managed service platform of open source technologies such as Apache Cassandra, Apache Spark, Elasticsearch and Apache Kafka.</i></p>",
        "created_at": "2018-09-13T15:08:59+0000",
        "updated_at": "2018-09-13T15:09:06+0000",
        "published_at": "2018-09-13T14:15:14+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 4,
        "domain_name": "www.instaclustr.com",
        "preview_picture": "https://www.instaclustr.com/wp-content/uploads/2018/09/Page-2.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12163"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12162,
        "uid": null,
        "title": "Cassandra Collections: Hidden Tombstones and How to Avoid Them - Instaclustr",
        "url": "https://www.instaclustr.com/cassandra-collections-hidden-tombstones-and-how-to-avoid-them/",
        "content": "<h2>Overview</h2><p>Multi-value data types (sets, lists and maps) are a powerful feature of <a href=\"https://www.instaclustr.com/apache-cassandra/\">Cassandra</a>, aiding you in denormalisation while allowing you to still retrieve and set data at a very fine-grained level. However, some of Cassandra’s behaviour when handling these data types is not always as expected and can cause issues.</p><p>In particular, there can be hidden surprises when you update the value of a collection type column. For simple-type columns, Cassandra performs an update by simply writing a new value for the cell and the most recently written value wins when the data is read. However, when you overwrite a collection Cassandra can’t simply write the new elements because all the existing elements in the map have their own individual cells and would still be returned alongside the new elements whenever a read is performed on the map.</p><h2>The options</h2><p>This leaves Cassandra with two options:</p><ol><li>Perform a read and discover all the existing map elements and either delete them or update them if they were specified in the overwrite.</li> <li>Forget about all existing elements in the map by deleting them.</li> </ol><p>Option 1 doesn’t sound very optimised does it? A read for every write you perform? Ouch.<br />Cassandra chooses option 2, because it just can’t resist those performance gains. It knows you’re performing an overwrite, and that you obviously don’t care about the contents of those columns, so it will delete them for you, and we can all pretend they never existed in the first place.</p><p>Or so we thought… until one day your queries start failing because you’ve hit 100k tombstones. Didn’t expect that, especially when you never delete any data.<br />In most cases, compactions will just handle this problem for you and the tombstones will be gone before you even get close to the query failure limit. However, compaction strategies aren’t perfect and depending on how much you overwrite, plus how well compactions remove those tombstones, there are many cases where this behaviour can become a huge issue. If you are performing many writes, and all of them are overwrites where a collection type is involved, you will be generating a tombstone for every single write.</p><h2>Examples for avoiding the issue</h2><p>I’ve created a very basic schema with a map and a few fields, as below:</p><p>I then inserted a single row and performed a flush:</p><p>And I now have an SSTable in my tombs.staff data directory.</p><p>Using sstable2json to analyse the data, as expected we have one key, a, however it has two locations entries, despite the fact we only did one write.</p><p>This is to do with the map, and the whole overwrite thing I was talking about earlier. Already we can see that C* has written a range tombstone for the locations cell immediately before writing the value that I inserted.</p><p>Now this is kind of a spoiler, as we haven’t actually done any “overwrites” yet, but we’ve identified the feature we’re talking about. This is because in Cassandra, overwrites, updates, and inserts, are really all just the same thing. The insert against the map will do the same thing whether the key already exists or not.</p><p>Anyway, we can see how this delete first strategy begins to work if we simply insert another record with the same key:</p><p>We now have 2 sstables: tombs-staff-ka-1-Data.db and tombs-staff-ka-2-Data.db. And if we run sstable2json on the new SSTable, we see a very similar entry:</p><p>Nothing surprising, and furthermore, if we trigger a major compaction against our 2 SSTables:</p><p>And run sstable2json against our new SSTable…</p><p>We have the latest range tombstone plus the latest insert, and compactions have, as expected,  gotten rid of the previous insert as it knows everything older than the latest range tombstone is moot.</p><p>Now you can start to see where issues can arise when overwriting a key with a collection type. If it weren’t for the compaction, I’d have 2 tombstones for that single row across 2 SSTables. Obviously it’s very likely those SSTables will compact and the tombstones will get cleared out, however things are not always as clear cut, especially when you are frequently overwriting keys and the tombstones get spread across many SSTables of differing sizes, causing tombstone bloat that may not be removed when left up to minor compactions.</p><p>So how can we avoid this potential catastrophe? A simple solution would be to instead store JSON and leave the updates to your application, however there is an alternative. You can use the provided append and subtraction operators. These operators will modify the collection without having to perform a read, and also won’t create any range tombstones. This works for specific use cases where you simply need to insert/append/prepend, however if you frequently find yourself having to rewrite a whole collection you will need to take a different approach. You can also specify a collection as frozen which would give the the desired overwrite behaviour, but you will no longer be able to add and remove elements using the +, -, and [] operators.</p><p>Here is an example of performing collection operations on a list.</p><div id=\"crayon-5b9a5da245d96328149275\" class=\"crayon-syntax crayon-theme-classic crayon-font-monaco crayon-os-mac print-yes notranslate\" data-settings=\"minimize scroll-mouseover\"><div class=\"crayon-main\"><table class=\"crayon-table\"><tr class=\"crayon-row\"><td class=\"crayon-nums\" data-settings=\"show\"> <div class=\"crayon-nums-content\"><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p></div></td>\n<td class=\"crayon-code\"><div class=\"crayon-pre\"><p>ALTER TABLE staff ADD leave_dates list&amp;lt;text&amp;gt;; # Creates a tombstone and an entry in the list    </p><p>insert into staff (id, leave_dates) values ('c', ['20160620']);   </p><p>$ nodetool flush    </p><p>$ sstable2json tombs-staff-ka-6-Data.db    </p><p>[    </p><p>{\"key\": \"c\",     \"cells\": [[\"\",\"\",1466427765961455],</p><p>[\"leave_dates:_\",\"leave_dates:!\",1466427765961454,\"t\",1466427765],   </p><p>[\"leave_dates:484b79b036e711e681757906eb0f5a6e\",\"3230313630363230\",1466427765961455]]}    </p><p>]    </p><p># Prepends an element to the list without creating any additional tombstones    </p><p>UPDATE staff SET leave_dates = [ '20160621' ] + leave_dates where id='c';   </p><p> $ nodetool flush    # The new SSTable has only a single entry in the list, no extra tombstone.   </p><p> # This works the same for appending to the list as well.    </p><p>$ sstable2json tombs-staff-ka-7-Data.db    </p><p>[   </p><p>{\"key\": \"c\",     \"cells\":    [[\"leave_dates:af13b22fb5e911d781757906eb0f5a6e\",\"3230313630363231\",1466427869996855]]}    </p><p>]</p></div></td>\n</tr></table></div></div><p>Be careful when using addition and subtraction on list types, as removing elements from a list can be an expensive operation. Cassandra will have to read in the entire list in order to remove a single entry. Note that this is not true for sets, removing a single entry from a set requires no reads, as Cassandra will simply write a tombstone for the matching cell.</p><p>See the below trace for a deletion from a list, where we can clearly see C* performing a read query before making the modifications.</p><p>The following statements for the SET type result in similar functionality. Note that appending and prepending is non-existent with sets, it is simply add and remove.</p><div id=\"crayon-5b9a5da245d9e680868585\" class=\"crayon-syntax crayon-theme-classic crayon-font-monaco crayon-os-mac print-yes notranslate\" data-settings=\"minimize scroll-mouseover\"><div class=\"crayon-main\"><table class=\"crayon-table\"><tr class=\"crayon-row\"><td class=\"crayon-nums\" data-settings=\"show\"> <div class=\"crayon-nums-content\"><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p></div></td>\n<td class=\"crayon-code\"><div class=\"crayon-pre\"><p>    ALTER TABLE staff ADD leave_dates list&amp;lt;text&amp;gt;;</p><p>    # Creates a tombstone and an entry in the list</p><p>    insert into staff (id, leave_dates) values ('c', ['20160620']);</p><p>    $ nodetool flush</p><p>    $ sstable2json tombs-staff-ka-6-Data.db</p><p>    [</p><p>    {\"key\": \"c\",</p><p>     \"cells\": [[\"\",\"\",1466427765961455],</p><p>    [\"leave_dates:_\",\"leave_dates:!\",1466427765961454,\"t\",1466427765],</p><p>    [\"leave_dates:484b79b036e711e681757906eb0f5a6e\",\"3230313630363230\",1466427765961455]]}</p><p>    ]</p><p>    # Prepends an element to the list without creating any additional tombstones</p><p>    UPDATE staff SET leave_dates = [ '20160621' ] + leave_dates where id='c';</p><p>    $ nodetool flush</p><p>    # The new SSTable has only a single entry in the list, no extra tombstone.</p><p>    # This works the same for appending to the list as well.</p><p>    $ sstable2json tombs-staff-ka-7-Data.db</p><p>    [</p><p>    {\"key\": \"c\",</p><p>     \"cells\": [[\"leave_dates:af13b22fb5e911d781757906eb0f5a6e\",\"3230313630363231\",1466427869996855]]}</p><p>    ]</p></div></td>\n</tr></table></div></div>",
        "created_at": "2018-09-13T15:07:16+0000",
        "updated_at": "2018-09-13T15:07:20+0000",
        "published_at": "2016-11-24T04:29:18+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 6,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12162"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 391,
            "label": "big.data",
            "slug": "big-data"
          }
        ],
        "is_public": false,
        "id": 12161,
        "uid": null,
        "title": "Apache Cassandra: The Big Data Foundation - Instaclustr",
        "url": "https://www.instaclustr.com/apache-cassandra-big-data-foundation/",
        "content": "<h2>Introduction</h2><p>To state the obvious, we here at Instaclustr are massive fans of <a href=\"https://www.instaclustr.com/apache-cassandra/\">Apache Cassandra</a>. We have built our company and our managed service around this database technology and the awesomeness of its capability.</p><p>There are a lot of well documented use cases and amazing companies providing examples of how they are using open source Apache Cassandra, but as a managed service we get to see first hand the power of this amazing technology and what it can do for our diverse customer base.</p><p>Suffice to say that our experience over the last two years has us even more convinced that Apache Cassandra is the foundation technology for the next wave of global-scale applications and solutions.</p><h2>Apache Cassandra use cases</h2><p>It is fair to say that we have probably seen it all with the diverse range of deployments – the good the bad and sometimes the ugly. Here is our take on the most common deployments:</p><ul><li><strong>Security.</strong>  The fraud and threat detection use case is very active in our environment.  We see the application in most cases is related to identifying anomalies through data mining and deep analytics to identify security-related events of interest.</li> <li><strong><a href=\"https://www.instaclustr.com/customers/messaging/\">Messaging</a>.</strong>  Several of our customers have social media and data sharing applications that are being used with messaging services at it’s core.</li> <li><strong><a href=\"https://www.instaclustr.com/customers/iot/\">IoT</a>.</strong>  This is probably the most common of our customers use cases.  We have many customers representing a wide range of industries, using Cassandra as an IoT solution.  We also work with a number of customers who are providing IoT platforms to their own customer base.</li> <li><strong>Recommendation &amp; Personalisation.</strong>  Many of our customers are using the power of personalisation. This is a very common within the AdTech industry, but also some of our customers are building unique learning platforms that are personalized to an individual student.</li> <li><strong>Catalogues &amp; Playlists.</strong> This particular use case we haven’t seen as much of the others but the data models and usage patterns typical seen within catalogues and playlists are usually a small part of a much larger application.</li> </ul><p>The most popular industries? We have a large customer base within the <a href=\"https://www.instaclustr.com/customers/ad-tech/\">AdTech</a> space, where the key metrics of performance and scalability are important.  We also have core customers in the FinTech industry where personalization, high availability and security are all important.   We also have several customers in the EdTech space developing specialized and personalized learning platforms.</p><p>Another interesting insight is that we have an amazingly diverse client base that ranges from personal projects to early stage start-ups all the way through to 140-year-old, billion dollar companies looking to transform and enhance their business.  We can see first hand that you don’t have to be a large company to be working with large datasets.</p><p>With several of our original customers we have been with them on the journey from an initial 3 node cluster, through to large production clusters with separate staging and testing environments.</p><h2>Diverse use cases help us improve everyday</h2><p>The beauty of having grown our customer base so rapidly and widely is that we have benefited from gaining insight and understanding into the wide application of this technology and the details of specific use cases.  This provides us with a unique perspective of the deployment of Apache Cassandra. We see its adaptability, but we also see its complexity and its temperament when it is not handled well.</p><p>We see the specific nuances associated with operating an efficient production grade environment and cluster for all of the different use cases. Having such a wide range of different deployments under our care is giving us an ever increasing richness in our own data that we are now analysing through our <a href=\"https://www.instaclustr.com/monitoring-cassandra-and-it-infrastructure-with-riemann/\">Instametrics monitoring environment</a>.  This is helping us to continually improve our capability and to continue to automate and refine our service offering.</p><p>We have also been in the unique position of growing with our customers and helping them scale, in some cases rapidly.  This also provides us with insight into how to build out a cluster or environment efficiently when an application goes viral, or the application has to ingest vast amounts of data.</p><h2>With great power comes great responsibility</h2><p>There is no doubt that Apache Cassandra provides great power, but the trade-off is that this also comes with a certain level of complexity.   You can’t expect that a database technology like Apache Cassandra can simply scale rapidly, provide high throughput performance and be continuously on without there being some work to do.</p><p>Continual monitoring, maintenance and performance tuning are important activities that must go with any database and associated technology environment to keep it operating efficiently and effectively.  But probably just as important is good design and planning up front.</p><h2>When the data layer is an afterthought</h2><p>In many cases we see that the data layer follows the lead from the application. That is, the time effort and focus at first for many start-ups is the application and what the customer is building on the front-end. This is often necessary to demonstrate a concept to an investor or to simply get things up and running quickly while finding market fit.  This approach means that often the data is an afterthought.</p><p>When the data is an afterthought we often see that the application and database will work okay at the beginning, but it is when they try to scale that things get ugly with Cassandra. If you don’t treat the data layer with a certain amount of mechanical sympathy, and you don’t plan effectively from the start, then there can be consequences down the track.</p><h2>Plan to be big from the start</h2><p>When Apache Cassandra has been selected as the database by an engineer at the beginning of a project, we know that they mean business and that they are planning on their application or solution being global.   What we often find is that while they are thinking big from the start, that sometimes the design and planning isn’t equally as big.</p><p>We see that effective planning and design of the data architecture and infrastructure from the start means that our customers tend to prosper and scaling and performance are rarely an issue. When the team is only thinking big, and not designing and planning big, then problems can arise down the track.</p><p>And yes we can speak from experience.  Bringing your infrastructure and database back from the brink can be a difficult and painful experience.</p><p>You are much better off doing the work up front.  Even if your environment works well initially, it is when you get to the point of having to scale is when you will start to see issues.</p><h2>Conclusion</h2><p>If you are thinking big, then plan to be big. If you design the architecture and infrastructure yourself, get an independent expert with some experience to validate your work.  Check and check again.</p><p>Doing it right the first time will set you up for efficient scaling, high performance and a continuously on environment and save you weeks of pain when you have terabytes of data structured the wrong way.  Your application might work okay at the start, but it is when it comes to the point of scaling that we see most of the issues arise for our customers.</p>",
        "created_at": "2018-09-13T15:07:05+0000",
        "updated_at": "2018-09-13T15:07:07+0000",
        "published_at": "2017-01-31T06:52:36+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 6,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12161"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 51,
            "label": "blog",
            "slug": "blog"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12160,
        "uid": null,
        "title": "Cassandra Archives - Instaclustr",
        "url": "https://www.instaclustr.com/category/technical/cassandra/",
        "content": "<main id=\"main\"><section id=\"content\"><header id=\"page-title\"><div id=\"page-content\"><div class=\"container\"><div class=\"row\"><div class=\"col-sm-9 col-md-offset-1\"><div class=\"loop loop-news\"><article class=\"loop-item loop-item-news\"><a href=\"https://www.instaclustr.com/why-we-built-apache-cassandra-operator-to-run-on-kubernetes/\" class=\"section-fade\"> <div class=\"row\"><div class=\"col-lg-5 col-md-6\"><p class=\"cat-label\">  <i class=\"ion-record\"> Technical   <i class=\"ion-record\"> Technical — Cassandra  </i></i></p><h2 class=\"h3\">Why We Built an Open Source Cassandra-Operator to Run Apache Cassandra on Kubernetes</h2><p class=\"subtitle\"><strong>Thursday 13th September 2018</strong> by Ben Bromhead</p>As Kubernetes becomes the de facto for container orchestration, more and more developers (and enterprises) want to run Apache Cassandra on Kubernetes. It’s easy to get started with this – especially considering the...</div></div></a></article><article class=\"loop-item loop-item-news\"><a href=\"https://www.instaclustr.com/aws-lambda-managed-cassandra-part-1/\" class=\"section-fade\">\n<div class=\"row\"><div class=\"col-lg-5 col-md-6\"><p class=\"cat-label\">  <i class=\"ion-record\"> Technical   <i class=\"ion-record\"> Technical — Cassandra  </i></i></p><h2 class=\"h3\">AWS Lambda with Managed Cassandra - Part 1: Let's Build a POC</h2><p class=\"subtitle\"><strong>Monday 27th August 2018</strong> by Christophe Schmitz</p>Serverless architecture is an attractive solution for both small businesses and large enterprises. The former gains the ability to go to market quickly while the latter are able to reduce IT costs by...</div></div></a></article><article class=\"loop-item loop-item-news\"><a href=\"https://www.instaclustr.com/update-released-instaclustr-sstable-analysis-tools-apache-cassandra/\" class=\"section-fade\">\n<div class=\"row\"><div class=\"col-lg-5 col-md-6\"><p class=\"cat-label\">  <i class=\"ion-record\"> Technical   <i class=\"ion-record\"> Technical — Cassandra  </i></i></p><h2 class=\"h3\">Update released for Instaclustr sstable analysis tools for Apache Cassandra</h2><p class=\"subtitle\"><strong>Tuesday 21st August 2018</strong> by Instaclustr </p>Instaclustr is pleased to announce the latest update for it’s open-sourced sstable analysis tools for Apache Cassandra. These tools, first released in February 2017, help operators to gain an accurate picture of the...</div></div></a></article><article class=\"loop-item loop-item-news\"><a href=\"https://www.instaclustr.com/apache-cassandra-ldap-authentication/\" class=\"section-fade\">\n<div class=\"row\"><div class=\"col-lg-5 col-md-6\"><p class=\"cat-label\">  <i class=\"ion-record\"> Technical   <i class=\"ion-record\"> Technical — Cassandra  </i></i></p><h2 class=\"h3\">Apache Cassandra LDAP Authentication</h2><p class=\"subtitle\"><strong>Friday 29th June 2018</strong> by Kurt Greaves</p>We’ve seen an increasing need for LDAP integration into Apache Cassandra, and continually hearing of cases where people have written their own LDAP authenticators for Cassandra. However, if you search around you’ll have...</div></div></a></article><article class=\"loop-item loop-item-news\"><a href=\"https://www.instaclustr.com/using-cassandra-stress-to-model-a-time-series-workload/\" class=\"section-fade\">\n<div class=\"row\"><div class=\"col-lg-5 col-md-6\"><p class=\"cat-label\">  <i class=\"ion-record\"> Technical   <i class=\"ion-record\"> Technical — Cassandra  </i></i></p><h2 class=\"h3\">Using Cassandra Stress to model a time series workload</h2><p class=\"subtitle\"><strong>Wednesday 9th May 2018</strong> by Instaclustr </p>Motivation When examining whether Cassandra is a good fit for your needs, it is good practice to stress test Cassandra using a workload that looks similar to the expected workload in Production. In...</div></div></a></article><article class=\"loop-item loop-item-news\"><a href=\"https://www.instaclustr.com/apache-kafka-connect-architecture-overview/\" class=\"section-fade\">\n<div class=\"row\"><div class=\"col-lg-5 col-md-6\"><p class=\"cat-label\">  <i class=\"ion-record\"> Technical   <i class=\"ion-record\"> Technical — Cassandra  </i></i></p><h2 class=\"h3\">Apache Kafka Connect Architecture Overview</h2><p class=\"subtitle\"><strong>Wednesday 9th May 2018</strong> by Paul Brebner</p>Kafka Connect is an API and ecosystem of 3rd party connectors that enables Apache Kafka to be scalable, reliable, and easily integrated with other heterogeneous systems (such as Cassandra, Spark, and Elassandra) without...</div></div></a></article><article class=\"loop-item loop-item-news\"><a href=\"https://www.instaclustr.com/february-2018-apache-cassandra-releases/\" class=\"section-fade\">\n<div class=\"row\"><div class=\"col-lg-5 col-md-6\"><p class=\"cat-label\">  <i class=\"ion-record\"> News   <i class=\"ion-record\"> Technical — Cassandra  </i></i></p><h2 class=\"h3\">February 2018 Apache Cassandra Releases</h2><p class=\"subtitle\"><strong>Thursday 22nd March 2018</strong> by Kurt Greaves</p>In February, the Apache Cassandra project issued releases for all currently supported branches of Apache Cassandra. As far as releases go the change list was modest, which shows that we’re seeing fewer bugs...</div></div></a></article><article class=\"loop-item loop-item-news\"><a href=\"https://www.instaclustr.com/benchmark-results-linux-kernel-meltdown-patch-effects-on-apache-cassandra/\" class=\"section-fade\">\n<div class=\"row\"><div class=\"col-lg-5 col-md-6\"><p class=\"cat-label\">  <i class=\"ion-record\"> Technical   <i class=\"ion-record\"> Technical — Cassandra  </i></i></p><h2 class=\"h3\">Benchmark Results: Linux Kernel \"Meltdown\" patch effects on Apache Cassandra 3.11.1</h2><p class=\"subtitle\"><strong>Friday 2nd February 2018</strong> by Instaclustr </p>In our Security Advisory published 8 January, we advised of up to 20% increase in CPU utilization and small increase in latency across managed clusters in AWS and GCP following the rollout of...</div></div></a></article><article class=\"loop-item loop-item-news\"><a href=\"https://www.instaclustr.com/surveying-cassandra-compatible-database-landscape/\" class=\"section-fade\">\n<div class=\"row\"><div class=\"col-lg-5 col-md-6\"><p class=\"cat-label\">  <i class=\"ion-record\"> Technical   <i class=\"ion-record\"> Technical — Cassandra  </i></i></p><h2 class=\"h3\">Surveying the Cassandra-compatible database landscape</h2><p class=\"subtitle\"><strong>Wednesday 17th January 2018</strong> by Ben Slater</p>Overview The popularity of Apache Cassandra and the applicability of it’s development model has seen it clearly emerge as the leading NoSQL technology for scale, performance and availability. One only needs to survey...</div></div></a></article><article class=\"loop-item loop-item-news\"><a href=\"https://diginomica.com/2017/11/28/how-adstage-moved-beyond-startup-scale-with-apache-cassandra-as-a-data-service/\" class=\"section-fade\" target=\"_blank\">\n<div class=\"row\"><div class=\"col-lg-5 col-md-6\"><p class=\"cat-label\">  <i class=\"ion-record\"> News   <i class=\"ion-record\"> Technical — Cassandra  </i></i></p><h2 class=\"h3\">How AdStage moved beyond startup scale with Apache Cassandra as a data service <i class=\"ion-android-open\"></i></h2><p class=\"subtitle\"><strong>Wednesday 13th December 2017</strong> by Instaclustr </p>Startup growth is never a bad thing – but the strain it puts on an IT team is real. Jason Wu of AdStage told me how AdStage addressed its data scale issues, and...</div></div></a></article><article class=\"loop-item loop-item-news\"><a href=\"https://www.instaclustr.com/picknmix-cassandra-spark-zeppelin-elassandra-kibana-kafka/\" class=\"section-fade\">\n<div class=\"row\"><div class=\"col-lg-5 col-md-6\"><p class=\"cat-label\">  <i class=\"ion-record\"> Technical   <i class=\"ion-record\"> Technical — Cassandra   <i class=\"ion-record\"> Technical — Kafka   <i class=\"ion-record\"> Technical — Spark   <i class=\"ion-record\"> Technical — Elasticsearch  </i></i></i></i></i></p><h2 class=\"h3\">Pick‘n’Mix: Cassandra, Spark, Zeppelin, Elassandra, Kibana, &amp; Kafka</h2><p class=\"subtitle\"><strong>Tuesday 5th December 2017</strong> by Paul Brebner</p>Kafkaesque:  \\ käf-kə-ˈesk \\ Marked by a senseless, disorienting, menacing, nightmarishly complexity. One morning when I woke from troubled dreams, I decided to blog about something potentially Kafkaesque: Which Instaclustr managed open-source-as-a-service(s) can be used...</div></div></a></article><article class=\"loop-item loop-item-news\"><a href=\"http://www.computerweekly.com/blog/Open-Source-Insider/Instaclustr-7-easy-steps-to-Cassandra-cluster-migration\" class=\"section-fade\" target=\"_blank\">\n<div class=\"row\"><div class=\"col-lg-5 col-md-6\"><p class=\"cat-label\">  <i class=\"ion-record\"> News   <i class=\"ion-record\"> Technical — Cassandra  </i></i></p><h2 class=\"h3\">Instaclustr: 7 Easy Steps to Cassandra Cluster Migration <i class=\"ion-android-open\"></i></h2><p class=\"subtitle\"><strong>Friday 1st December 2017</strong> by Ben Slater</p> Instaclustr offers managed and supported solutions for a wide range of open source technologies including Apache Cassandra, ScyllaDB, Elasticsearch, Apache Spark, Apache Zeppelin, Kibana and Apache Lucene.  CPO Ben Slater provides their 7-step guide...</div></div></a></article></div><p>1<a href=\"https://www.instaclustr.com/category/technical/cassandra/page/2\" class=\"page-numbers\">2</a><a href=\"https://www.instaclustr.com/category/technical/cassandra/page/3\" class=\"page-numbers\">3</a><a href=\"https://www.instaclustr.com/category/technical/cassandra/page/4\" class=\"page-numbers\">4</a><a href=\"https://www.instaclustr.com/category/technical/cassandra/page/5\" class=\"page-numbers\">5</a><a href=\"https://www.instaclustr.com/category/technical/cassandra/page/6\" class=\"page-numbers\">6</a><a href=\"https://www.instaclustr.com/category/technical/cassandra/page/7\" class=\"page-numbers\">7</a><a href=\"https://www.instaclustr.com/category/technical/cassandra/page/8\" class=\"page-numbers\">8</a><a href=\"https://www.instaclustr.com/category/technical/cassandra/page/9\" class=\"page-numbers\">9</a><a href=\"https://www.instaclustr.com/category/technical/cassandra/page/2\" class=\"next\">Next</a></p></div></div></div></div></header></section></main><aside id=\"contact\"><div class=\"container\"><div class=\"row\"><div class=\"col-lg-3 col-sm-5\"><p></p><h2 class=\"h1\">Get in touch <br />with Instaclustr</h2></div></div></div></aside><p class=\"credit\">Site by <a title=\"Swell Design Group\" href=\"http://swelldesigngroup.com/?utm_campaign=instaclustr\" target=\"_blank\">Swell Design Group</a></p>\n<noscript></noscript>",
        "created_at": "2018-09-13T15:03:44+0000",
        "updated_at": "2018-09-13T15:03:47+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12160"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 901,
            "label": "security",
            "slug": "security"
          }
        ],
        "is_public": false,
        "id": 12159,
        "uid": null,
        "title": "A Benevolent Hacker Is Warning Owners of Unsecured Cassandra Databases",
        "url": "https://www.bleepingcomputer.com/news/security/a-benevolent-hacker-is-warning-owners-of-unsecured-cassandra-databases/",
        "content": "<p><img alt=\"Cassandra logo\" height=\"465\" src=\"https://www.bleepstatic.com/content/hl-images/2017/01/24/Cassandra_logo.jpg\" width=\"1250\" class=\"b-lazy\" /></p><p>An unknown hacker is accessing public and unsecured Apache Cassandra databases and adding an extra table through which it warns server owners that their DB was left exposed to online attacks.</p><p>The first cases of Cassandra databases with this extra table were spotted by a Twitter user that goes by the nickname of  <a href=\"https://twitter.com/dk_effect\" target=\"_blank\" rel=\"nofollow\">DunningKrugerEffect</a>.</p><p>The name of this table is \"<strong>your_db_is_not_secure</strong>,\" and the table doesn't hold any type of information inside.</p><p>The purpose of this table is to warn Cassandra owners that their database can be very easily held for ransom in the upcoming few days if left online unprotected. According to Shodan, there are currently over 2,600 Cassandra database instances left accessible online.</p><p>Since the start of the year, multiple criminal groups have been hijacking database servers left unprotected online, wiping data and requesting a ransom payment.</p><p>First attacks hit <a href=\"https://www.bleepingcomputer.com/news/security/mongodb-apocalypse-professional-ransomware-group-gets-involved-infections-reach-28k-servers/\" target=\"_blank\">MongoDB servers</a> and were quickly followed by attacks against <a href=\"https://www.bleepingcomputer.com/news/security/mongodb-hijackers-move-on-to-elasticsearch-servers/\" target=\"_blank\">ElasticSearch clusters</a>, <a href=\"https://www.bleepingcomputer.com/news/security/database-ransom-attacks-hit-couchdb-and-hadoop-servers/\" target=\"_blank\">Hadoop servers and CouchDB databases</a>.</p><p>All previous attacks have been tracked by Victor Gevers and other members of the GDI.foundation, who created spreadsheets that keep track of ongoing attacks.</p><p>One such spreadsheet is available for Cassandra attacks. These are the latest statistics regarding database ransom attacks:</p><ul><li><a href=\"https://docs.google.com/spreadsheets/d/1QonE9oeMOQHVh8heFIyeqrjfKEViL0poLnY8mAakKhM/edit#gid=1781677175\" target=\"_blank\" rel=\"nofollow\">MongoDB</a> - 40,291 servers</li>\n<li><a href=\"https://docs.google.com/spreadsheets/d/1-txUnn6HFuETgN1mQwI4g_Rk5SY0LEkjgOX8sG0KaFY/edit#gid=0\" target=\"_blank\" rel=\"nofollow\">ElasticSearch</a> - 5,044 servers</li>\n<li><a href=\"https://docs.google.com/spreadsheets/d/18-zmpzp87TX9oIbLwChJ3Fn0ldCGysSm-aoje_VvSSc/edit#gid=0\" target=\"_blank\" rel=\"nofollow\">Apache Hadoop</a> - 186 servers</li>\n<li><a href=\"https://docs.google.com/spreadsheets/d/1iO8nINe1Ia2s40byeOj8BRiXZMpiBkKGJR5AuV7EExY/edit#gid=0\" target=\"_blank\" rel=\"nofollow\">Apache CouchDB</a> - 452 servers</li>\n<li><a href=\"https://docs.google.com/spreadsheets/d/1D8dSqMJuWSzLiwHaf4caSNYkcNpjhmwCW7_Z4QNJHVM/edit#gid=0\" target=\"_blank\" rel=\"nofollow\">Apache Cassandra</a> - 49 servers</li>\n</ul><p>Currently, multiple members of the GDI.foundation \"have been investigating these cases deploying honeypots and getting intel on the attacks,\" Gevers tells Bleeping Computer.</p><p>The GDI.foundation has also been working with local CERT teams and attempting to notify database owners before attackers hijack their data. Despite this, very few server owners heeded their warnings, with many servers still remaining unsecured.</p><p>If you're wondering what other database servers attackers could hit, there are Neo4J, Riak, or Redis systems that have not yet been targeted by these types of ransom attempts.</p>",
        "created_at": "2018-09-13T14:58:58+0000",
        "updated_at": "2018-09-13T14:59:02+0000",
        "published_at": "2017-01-24T20:00:09+0000",
        "published_by": [
          "Catalin Cimpanu"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 1,
        "domain_name": "www.bleepingcomputer.com",
        "preview_picture": "https://www.bleepstatic.com/content/hl-images/2017/01/24/Cassandra_logo.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12159"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 901,
            "label": "security",
            "slug": "security"
          }
        ],
        "is_public": false,
        "id": 12158,
        "uid": null,
        "title": "Apache Cassandra Security - Instaclustr",
        "url": "https://www.instaclustr.com/apache-cassandra-security/",
        "content": "<header id=\"page-masthead\"><div id=\"page-content\"><div class=\"container\"><div class=\"row\"><div class=\"col-md-6 col-md-offset-0 col-sm-8 col-sm-offset-2\"><div class=\"primary first-p-bold\"><p><a href=\"https://www.instaclustr.com/apache-cassandra/\">Apache Cassandra</a> security has hit the new lately with a benevolent hacker attempting to warn owners of unsecured Cassandra databases about their exposure (see <a href=\"https://www.bleepingcomputer.com/news/security/a-benevolent-hacker-is-warning-owners-of-unsecured-cassandra-databases/\">https://www.bleepingcomputer.com/news/security/a-benevolent-hacker-is-warning-owners-of-unsecured-cassandra-databases/</a>).</p><p>At Instaclustr, we take security very seriously. We were confident that our default configurations would not allow access to this type of scan. We have used our central management system to check all clusters that we currently have under management for the presence of the tell-tale keyspace and confirm that none of our managed clusters had been detected by the scan.</p><p>Of course, not being picked up by some random, external scan is no guarantee of security so it’s worth re-capping some of the things we do at Instaclustr to make it easy to maximise the security of your cluster:</p><ul><li>The use of TLS (SSL) and password authentication to connect to Cassandra can be configured with the click of a check box at cluster creation. We even generate sample code for connecting to the cluster to make it as easy as possible.</li> <li>Firewall rules block all access to the cluster by default with exception added at the control of the cluster owner through our console.</li> <li>We support VPC peering and the use of private IPs to minimise public access points through the firewall.</li> <li>We disable access by the default Cassandra user, preventing any attacks using this well-known user.</li> <li>We regularly commission external penetration tests of our clusters and other components of our system.</li> </ul><p>In addition to these current measures, we have a continuing focus on enhanced security technology and processes which benefit all of our customers as they become available. For example, current engineering initiatives include enhanced intrusion detection across all components of our system and additional security certifications.</p><p>At Instaclustr, we’re proud of our capability, focus and record when it comes to security. While it’s not an area that we often talk about publicly we’re more than happy to go into details of our approach with any customers or potential customers – just contact us to set up a chat.</p></div></div></div></div></div></header>",
        "created_at": "2018-09-13T14:58:27+0000",
        "updated_at": "2018-09-13T14:58:31+0000",
        "published_at": "2017-01-31T07:00:57+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12158"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12157,
        "uid": null,
        "title": "Apache Cassandra Documentation : Cassandra Config",
        "url": "http://cassandra.apache.org/doc/latest/configuration/cassandra_config_file.html",
        "content": "<div class=\"section\" id=\"cluster-name\"><h2><code class=\"docutils literal\">cluster_name</code></h2><p>The name of the cluster. This is mainly used to prevent machines in\none logical cluster from joining another.</p><p><em>Default Value:</em> ‘Test Cluster’</p></div><div class=\"section\" id=\"num-tokens\"><h2><code class=\"docutils literal\">num_tokens</code></h2><p>This defines the number of tokens randomly assigned to this node on the ring\nThe more tokens, relative to other nodes, the larger the proportion of data\nthat this node will store. You probably want all nodes to have the same number\nof tokens assuming they have equal hardware capability.</p><p>If you leave this unspecified, Cassandra will use the default of 1 token for legacy compatibility,\nand will use the initial_token as described below.</p><p>Specifying initial_token will override this setting on the node’s initial start,\non subsequent starts, this setting will apply even if initial token is set.</p><p>If you already have a cluster with 1 token per node, and wish to migrate to\nmultiple tokens per node, see <a class=\"reference external\" href=\"http://wiki.apache.org/cassandra/Operations\">http://wiki.apache.org/cassandra/Operations</a></p><p><em>Default Value:</em> 256</p></div><div class=\"section\" id=\"allocate-tokens-for-keyspace\"><h2><code class=\"docutils literal\">allocate_tokens_for_keyspace</code></h2><p><em>This option is commented out by default.</em></p><p>Triggers automatic allocation of num_tokens tokens for this node. The allocation\nalgorithm attempts to choose tokens in a way that optimizes replicated load over\nthe nodes in the datacenter for the replication strategy used by the specified\nkeyspace.</p><p>The load assigned to each node will be close to proportional to its number of\nvnodes.</p><p>Only supported with the Murmur3Partitioner.</p><p><em>Default Value:</em> KEYSPACE</p></div><div class=\"section\" id=\"initial-token\"><h2><code class=\"docutils literal\">initial_token</code></h2><p><em>This option is commented out by default.</em></p><p>initial_token allows you to specify tokens manually.  While you can use it with\nvnodes (num_tokens &gt; 1, above) – in which case you should provide a\ncomma-separated list – it’s primarily used when adding nodes to legacy clusters\nthat do not have vnodes enabled.</p></div><div class=\"section\" id=\"hinted-handoff-enabled\"><h2><code class=\"docutils literal\">hinted_handoff_enabled</code></h2><p>See <a class=\"reference external\" href=\"http://wiki.apache.org/cassandra/HintedHandoff\">http://wiki.apache.org/cassandra/HintedHandoff</a>\nMay either be “true” or “false” to enable globally</p><p><em>Default Value:</em> true</p></div><div class=\"section\" id=\"hinted-handoff-disabled-datacenters\"><h2><code class=\"docutils literal\">hinted_handoff_disabled_datacenters</code></h2><p><em>This option is commented out by default.</em></p><p>When hinted_handoff_enabled is true, a black list of data centers that will not\nperform hinted handoff</p><p><em>Default Value (complex option)</em>:</p><div class=\"highlight-default\"><div class=\"highlight\"><pre>#    - DC1\n#    - DC2\n</pre></div></div></div><div class=\"section\" id=\"max-hint-window-in-ms\"><h2><code class=\"docutils literal\">max_hint_window_in_ms</code></h2><p>this defines the maximum amount of time a dead host will have hints\ngenerated.  After it has been dead this long, new hints for it will not be\ncreated until it has been seen alive and gone down again.</p><p><em>Default Value:</em> 10800000 # 3 hours</p></div><div class=\"section\" id=\"hinted-handoff-throttle-in-kb\"><h2><code class=\"docutils literal\">hinted_handoff_throttle_in_kb</code></h2><p>Maximum throttle in KBs per second, per delivery thread.  This will be\nreduced proportionally to the number of nodes in the cluster.  (If there\nare two nodes in the cluster, each delivery thread will use the maximum\nrate; if there are three, each will throttle to half of the maximum,\nsince we expect two nodes to be delivering hints simultaneously.)</p><p><em>Default Value:</em> 1024</p></div><div class=\"section\" id=\"max-hints-delivery-threads\"><h2><code class=\"docutils literal\">max_hints_delivery_threads</code></h2><p>Number of threads with which to deliver hints;\nConsider increasing this number when you have multi-dc deployments, since\ncross-dc handoff tends to be slower</p><p><em>Default Value:</em> 2</p></div><div class=\"section\" id=\"hints-directory\"><h2><code class=\"docutils literal\">hints_directory</code></h2><p><em>This option is commented out by default.</em></p><p>Directory where Cassandra should store hints.\nIf not set, the default directory is $CASSANDRA_HOME/data/hints.</p><p><em>Default Value:</em>  /var/lib/cassandra/hints</p></div><div class=\"section\" id=\"hints-flush-period-in-ms\"><h2><code class=\"docutils literal\">hints_flush_period_in_ms</code></h2><p>How often hints should be flushed from the internal buffers to disk.\nWill <em>not</em> trigger fsync.</p><p><em>Default Value:</em> 10000</p></div><div class=\"section\" id=\"max-hints-file-size-in-mb\"><h2><code class=\"docutils literal\">max_hints_file_size_in_mb</code></h2><p>Maximum size for a single hints file, in megabytes.</p><p><em>Default Value:</em> 128</p></div><div class=\"section\" id=\"hints-compression\"><h2><code class=\"docutils literal\">hints_compression</code></h2><p><em>This option is commented out by default.</em></p><p>Compression to apply to the hint files. If omitted, hints files\nwill be written uncompressed. LZ4, Snappy, and Deflate compressors\nare supported.</p><p><em>Default Value (complex option)</em>:</p><div class=\"highlight-default\"><div class=\"highlight\"><pre>#   - class_name: LZ4Compressor\n#     parameters:\n#         -\n</pre></div></div></div><div class=\"section\" id=\"batchlog-replay-throttle-in-kb\"><h2><code class=\"docutils literal\">batchlog_replay_throttle_in_kb</code></h2><p>Maximum throttle in KBs per second, total. This will be\nreduced proportionally to the number of nodes in the cluster.</p><p><em>Default Value:</em> 1024</p></div><div class=\"section\" id=\"authenticator\"><h2><code class=\"docutils literal\">authenticator</code></h2><p>Authentication backend, implementing IAuthenticator; used to identify users\nOut of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthenticator,\nPasswordAuthenticator}.</p><ul class=\"simple\"><li>AllowAllAuthenticator performs no checks - set it to disable authentication.</li>\n<li>PasswordAuthenticator relies on username/password pairs to authenticate\nusers. It keeps usernames and hashed passwords in system_auth.roles table.\nPlease increase system_auth keyspace replication factor if you use this authenticator.\nIf using PasswordAuthenticator, CassandraRoleManager must also be used (see below)</li>\n</ul><p><em>Default Value:</em> AllowAllAuthenticator</p></div><div class=\"section\" id=\"role-manager\"><h2><code class=\"docutils literal\">role_manager</code></h2><p>Part of the Authentication &amp; Authorization backend, implementing IRoleManager; used\nto maintain grants and memberships between roles.\nOut of the box, Cassandra provides org.apache.cassandra.auth.CassandraRoleManager,\nwhich stores role information in the system_auth keyspace. Most functions of the\nIRoleManager require an authenticated login, so unless the configured IAuthenticator\nactually implements authentication, most of this functionality will be unavailable.</p><ul class=\"simple\"><li>CassandraRoleManager stores role data in the system_auth keyspace. Please\nincrease system_auth keyspace replication factor if you use this role manager.</li>\n</ul><p><em>Default Value:</em> CassandraRoleManager</p></div><div class=\"section\" id=\"roles-validity-in-ms\"><h2><code class=\"docutils literal\">roles_validity_in_ms</code></h2><p>Validity period for roles cache (fetching granted roles can be an expensive\noperation depending on the role manager, CassandraRoleManager is one example)\nGranted roles are cached for authenticated sessions in AuthenticatedUser and\nafter the period specified here, become eligible for (async) reload.\nDefaults to 2000, set to 0 to disable caching entirely.\nWill be disabled automatically for AllowAllAuthenticator.</p><p><em>Default Value:</em> 2000</p></div><div class=\"section\" id=\"permissions-validity-in-ms\"><h2><code class=\"docutils literal\">permissions_validity_in_ms</code></h2><p>Validity period for permissions cache (fetching permissions can be an\nexpensive operation depending on the authorizer, CassandraAuthorizer is\none example). Defaults to 2000, set to 0 to disable.\nWill be disabled automatically for AllowAllAuthorizer.</p><p><em>Default Value:</em> 2000</p></div><div class=\"section\" id=\"credentials-validity-in-ms\"><h2><code class=\"docutils literal\">credentials_validity_in_ms</code></h2><p>Validity period for credentials cache. This cache is tightly coupled to\nthe provided PasswordAuthenticator implementation of IAuthenticator. If\nanother IAuthenticator implementation is configured, this cache will not\nbe automatically used and so the following settings will have no effect.\nPlease note, credentials are cached in their encrypted form, so while\nactivating this cache may reduce the number of queries made to the\nunderlying table, it may not  bring a significant reduction in the\nlatency of individual authentication attempts.\nDefaults to 2000, set to 0 to disable credentials caching.</p><p><em>Default Value:</em> 2000</p></div><div class=\"section\" id=\"partitioner\"><h2><code class=\"docutils literal\">partitioner</code></h2><p>The partitioner is responsible for distributing groups of rows (by\npartition key) across nodes in the cluster.  You should leave this\nalone for new clusters.  The partitioner can NOT be changed without\nreloading all data, so when upgrading you should set this to the\nsame partitioner you were already using.</p><p>Besides Murmur3Partitioner, partitioners included for backwards\ncompatibility include RandomPartitioner, ByteOrderedPartitioner, and\nOrderPreservingPartitioner.</p><p><em>Default Value:</em> org.apache.cassandra.dht.Murmur3Partitioner</p></div><div class=\"section\" id=\"data-file-directories\"><h2><code class=\"docutils literal\">data_file_directories</code></h2><p><em>This option is commented out by default.</em></p><p>Directories where Cassandra should store data on disk.  Cassandra\nwill spread data evenly across them, subject to the granularity of\nthe configured compaction strategy.\nIf not set, the default directory is $CASSANDRA_HOME/data/data.</p><p><em>Default Value (complex option)</em>:</p><div class=\"highlight-default\"><div class=\"highlight\"><pre>#     - /var/lib/cassandra/data\n</pre></div></div></div><div class=\"section\" id=\"commitlog-directory\"><h2><code class=\"docutils literal\">commitlog_directory</code></h2><p><em>This option is commented out by default.</em>\ncommit log.  when running on magnetic HDD, this should be a\nseparate spindle than the data directories.\nIf not set, the default directory is $CASSANDRA_HOME/data/commitlog.</p><p><em>Default Value:</em>  /var/lib/cassandra/commitlog</p></div><div class=\"section\" id=\"cdc-enabled\"><h2><code class=\"docutils literal\">cdc_enabled</code></h2><p>Enable / disable CDC functionality on a per-node basis. This modifies the logic used\nfor write path allocation rejection (standard: never reject. cdc: reject Mutation\ncontaining a CDC-enabled table if at space limit in cdc_raw_directory).</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"cdc-raw-directory\"><h2><code class=\"docutils literal\">cdc_raw_directory</code></h2><p><em>This option is commented out by default.</em></p><p>CommitLogSegments are moved to this directory on flush if cdc_enabled: true and the\nsegment contains mutations for a CDC-enabled table. This should be placed on a\nseparate spindle than the data directories. If not set, the default directory is\n$CASSANDRA_HOME/data/cdc_raw.</p><p><em>Default Value:</em>  /var/lib/cassandra/cdc_raw</p></div><div class=\"section\" id=\"disk-failure-policy\"><h2><code class=\"docutils literal\">disk_failure_policy</code></h2><p>Policy for data disk failures:</p><dl class=\"docutils\"><dt>die</dt>\n<dd>shut down gossip and client transports and kill the JVM for any fs errors or\nsingle-sstable errors, so the node can be replaced.</dd>\n<dt>stop_paranoid</dt>\n<dd>shut down gossip and client transports even for single-sstable errors,\nkill the JVM for errors during startup.</dd>\n<dt>stop</dt>\n<dd>shut down gossip and client transports, leaving the node effectively dead, but\ncan still be inspected via JMX, kill the JVM for errors during startup.</dd>\n<dt>best_effort</dt>\n<dd>stop using the failed disk and respond to requests based on\nremaining available sstables.  This means you WILL see obsolete\ndata at CL.ONE!</dd>\n<dt>ignore</dt>\n<dd>ignore fatal errors and let requests fail, as in pre-1.2 Cassandra</dd>\n</dl><p><em>Default Value:</em> stop</p></div><div class=\"section\" id=\"commit-failure-policy\"><h2><code class=\"docutils literal\">commit_failure_policy</code></h2><p>Policy for commit disk failures:</p><dl class=\"docutils\"><dt>die</dt>\n<dd>shut down the node and kill the JVM, so the node can be replaced.</dd>\n<dt>stop</dt>\n<dd>shut down the node, leaving the node effectively dead, but\ncan still be inspected via JMX.</dd>\n<dt>stop_commit</dt>\n<dd>shutdown the commit log, letting writes collect but\ncontinuing to service reads, as in pre-2.0.5 Cassandra</dd>\n<dt>ignore</dt>\n<dd>ignore fatal errors and let the batches fail</dd>\n</dl><p><em>Default Value:</em> stop</p></div><div class=\"section\" id=\"key-cache-size-in-mb\"><h2><code class=\"docutils literal\">key_cache_size_in_mb</code></h2><p>Maximum size of the key cache in memory.</p><p>Each key cache hit saves 1 seek and each row cache hit saves 2 seeks at the\nminimum, sometimes more. The key cache is fairly tiny for the amount of\ntime it saves, so it’s worthwhile to use it at large numbers.\nThe row cache saves even more time, but must contain the entire row,\nso it is extremely space-intensive. It’s best to only use the\nrow cache if you have hot rows or static rows.</p><p>NOTE: if you reduce the size, you may not get you hottest keys loaded on startup.</p><p>Default value is empty to make it “auto” (min(5% of Heap (in MB), 100MB)). Set to 0 to disable key cache.</p></div><div class=\"section\" id=\"key-cache-save-period\"><h2><code class=\"docutils literal\">key_cache_save_period</code></h2><p>Duration in seconds after which Cassandra should\nsave the key cache. Caches are saved to saved_caches_directory as\nspecified in this configuration file.</p><p>Saved caches greatly improve cold-start speeds, and is relatively cheap in\nterms of I/O for the key cache. Row cache saving is much more expensive and\nhas limited use.</p><p>Default is 14400 or 4 hours.</p><p><em>Default Value:</em> 14400</p></div><div class=\"section\" id=\"key-cache-keys-to-save\"><h2><code class=\"docutils literal\">key_cache_keys_to_save</code></h2><p><em>This option is commented out by default.</em></p><p>Number of keys from the key cache to save\nDisabled by default, meaning all keys are going to be saved</p><p><em>Default Value:</em> 100</p></div><div class=\"section\" id=\"row-cache-class-name\"><h2><code class=\"docutils literal\">row_cache_class_name</code></h2><p><em>This option is commented out by default.</em></p><p>Row cache implementation class name. Available implementations:</p><dl class=\"docutils\"><dt>org.apache.cassandra.cache.OHCProvider</dt>\n<dd>Fully off-heap row cache implementation (default).</dd>\n<dt>org.apache.cassandra.cache.SerializingCacheProvider</dt>\n<dd>This is the row cache implementation availabile\nin previous releases of Cassandra.</dd>\n</dl><p><em>Default Value:</em> org.apache.cassandra.cache.OHCProvider</p></div><div class=\"section\" id=\"row-cache-size-in-mb\"><h2><code class=\"docutils literal\">row_cache_size_in_mb</code></h2><p>Maximum size of the row cache in memory.\nPlease note that OHC cache implementation requires some additional off-heap memory to manage\nthe map structures and some in-flight memory during operations before/after cache entries can be\naccounted against the cache capacity. This overhead is usually small compared to the whole capacity.\nDo not specify more memory that the system can afford in the worst usual situation and leave some\nheadroom for OS block level cache. Do never allow your system to swap.</p><p>Default value is 0, to disable row caching.</p><p><em>Default Value:</em> 0</p></div><div class=\"section\" id=\"row-cache-save-period\"><h2><code class=\"docutils literal\">row_cache_save_period</code></h2><p>Duration in seconds after which Cassandra should save the row cache.\nCaches are saved to saved_caches_directory as specified in this configuration file.</p><p>Saved caches greatly improve cold-start speeds, and is relatively cheap in\nterms of I/O for the key cache. Row cache saving is much more expensive and\nhas limited use.</p><p>Default is 0 to disable saving the row cache.</p><p><em>Default Value:</em> 0</p></div><div class=\"section\" id=\"row-cache-keys-to-save\"><h2><code class=\"docutils literal\">row_cache_keys_to_save</code></h2><p><em>This option is commented out by default.</em></p><p>Number of keys from the row cache to save.\nSpecify 0 (which is the default), meaning all keys are going to be saved</p><p><em>Default Value:</em> 100</p></div><div class=\"section\" id=\"counter-cache-size-in-mb\"><h2><code class=\"docutils literal\">counter_cache_size_in_mb</code></h2><p>Maximum size of the counter cache in memory.</p><p>Counter cache helps to reduce counter locks’ contention for hot counter cells.\nIn case of RF = 1 a counter cache hit will cause Cassandra to skip the read before\nwrite entirely. With RF &gt; 1 a counter cache hit will still help to reduce the duration\nof the lock hold, helping with hot counter cell updates, but will not allow skipping\nthe read entirely. Only the local (clock, count) tuple of a counter cell is kept\nin memory, not the whole counter, so it’s relatively cheap.</p><p>NOTE: if you reduce the size, you may not get you hottest keys loaded on startup.</p><p>Default value is empty to make it “auto” (min(2.5% of Heap (in MB), 50MB)). Set to 0 to disable counter cache.\nNOTE: if you perform counter deletes and rely on low gcgs, you should disable the counter cache.</p></div><div class=\"section\" id=\"counter-cache-save-period\"><h2><code class=\"docutils literal\">counter_cache_save_period</code></h2><p>Duration in seconds after which Cassandra should\nsave the counter cache (keys only). Caches are saved to saved_caches_directory as\nspecified in this configuration file.</p><p>Default is 7200 or 2 hours.</p><p><em>Default Value:</em> 7200</p></div><div class=\"section\" id=\"counter-cache-keys-to-save\"><h2><code class=\"docutils literal\">counter_cache_keys_to_save</code></h2><p><em>This option is commented out by default.</em></p><p>Number of keys from the counter cache to save\nDisabled by default, meaning all keys are going to be saved</p><p><em>Default Value:</em> 100</p></div><div class=\"section\" id=\"saved-caches-directory\"><h2><code class=\"docutils literal\">saved_caches_directory</code></h2><p><em>This option is commented out by default.</em></p><p>saved caches\nIf not set, the default directory is $CASSANDRA_HOME/data/saved_caches.</p><p><em>Default Value:</em>  /var/lib/cassandra/saved_caches</p></div><div class=\"section\" id=\"commitlog-sync\"><h2><code class=\"docutils literal\">commitlog_sync</code></h2><p><em>This option is commented out by default.</em></p><p>commitlog_sync may be either “periodic” or “batch.”</p><p>When in batch mode, Cassandra won’t ack writes until the commit log\nhas been fsynced to disk.  It will wait\ncommitlog_sync_batch_window_in_ms milliseconds between fsyncs.\nThis window should be kept short because the writer threads will\nbe unable to do extra work while waiting.  (You may need to increase\nconcurrent_writes for the same reason.)</p><p><em>Default Value:</em> batch</p></div><div class=\"section\" id=\"commitlog-sync-batch-window-in-ms\"><h2><code class=\"docutils literal\">commitlog_sync_batch_window_in_ms</code></h2><p><em>This option is commented out by default.</em></p><p><em>Default Value:</em> 2</p></div><div class=\"section\" id=\"id1\"><h2><code class=\"docutils literal\">commitlog_sync</code></h2><p>the other option is “periodic” where writes may be acked immediately\nand the CommitLog is simply synced every commitlog_sync_period_in_ms\nmilliseconds.</p><p><em>Default Value:</em> periodic</p></div><div class=\"section\" id=\"commitlog-sync-period-in-ms\"><h2><code class=\"docutils literal\">commitlog_sync_period_in_ms</code></h2><p><em>Default Value:</em> 10000</p></div><div class=\"section\" id=\"commitlog-segment-size-in-mb\"><h2><code class=\"docutils literal\">commitlog_segment_size_in_mb</code></h2><p>The size of the individual commitlog file segments.  A commitlog\nsegment may be archived, deleted, or recycled once all the data\nin it (potentially from each columnfamily in the system) has been\nflushed to sstables.</p><p>The default size is 32, which is almost always fine, but if you are\narchiving commitlog segments (see commitlog_archiving.properties),\nthen you probably want a finer granularity of archiving; 8 or 16 MB\nis reasonable.\nMax mutation size is also configurable via max_mutation_size_in_kb setting in\ncassandra.yaml. The default is half the size commitlog_segment_size_in_mb * 1024.</p><p>NOTE: If max_mutation_size_in_kb is set explicitly then commitlog_segment_size_in_mb must\nbe set to at least twice the size of max_mutation_size_in_kb / 1024</p><p><em>Default Value:</em> 32</p></div><div class=\"section\" id=\"commitlog-compression\"><h2><code class=\"docutils literal\">commitlog_compression</code></h2><p><em>This option is commented out by default.</em></p><p>Compression to apply to the commit log. If omitted, the commit log\nwill be written uncompressed.  LZ4, Snappy, and Deflate compressors\nare supported.</p><p><em>Default Value (complex option)</em>:</p><div class=\"highlight-default\"><div class=\"highlight\"><pre>#   - class_name: LZ4Compressor\n#     parameters:\n#         -\n</pre></div></div></div><div class=\"section\" id=\"seed-provider\"><h2><code class=\"docutils literal\">seed_provider</code></h2><p>any class that implements the SeedProvider interface and has a\nconstructor that takes a Map&lt;String, String&gt; of parameters will do.</p><p><em>Default Value (complex option)</em>:</p><div class=\"highlight-default\"><div class=\"highlight\"><pre># Addresses of hosts that are deemed contact points.\n# Cassandra nodes use this list of hosts to find each other and learn\n# the topology of the ring.  You must change this if you are running\n# multiple nodes!\n- class_name: org.apache.cassandra.locator.SimpleSeedProvider\n  parameters:\n      # seeds is actually a comma-delimited list of addresses.\n      # Ex: \"&lt;ip1&gt;,&lt;ip2&gt;,&lt;ip3&gt;\"\n      - seeds: \"127.0.0.1\"\n</pre></div></div></div><div class=\"section\" id=\"concurrent-reads\"><h2><code class=\"docutils literal\">concurrent_reads</code></h2><p>For workloads with more data than can fit in memory, Cassandra’s\nbottleneck will be reads that need to fetch data from\ndisk. “concurrent_reads” should be set to (16 * number_of_drives) in\norder to allow the operations to enqueue low enough in the stack\nthat the OS and drives can reorder them. Same applies to\n“concurrent_counter_writes”, since counter writes read the current\nvalues before incrementing and writing them back.</p><p>On the other hand, since writes are almost never IO bound, the ideal\nnumber of “concurrent_writes” is dependent on the number of cores in\nyour system; (8 * number_of_cores) is a good rule of thumb.</p><p><em>Default Value:</em> 32</p></div><div class=\"section\" id=\"concurrent-writes\"><h2><code class=\"docutils literal\">concurrent_writes</code></h2><p><em>Default Value:</em> 32</p></div><div class=\"section\" id=\"concurrent-counter-writes\"><h2><code class=\"docutils literal\">concurrent_counter_writes</code></h2><p><em>Default Value:</em> 32</p></div><div class=\"section\" id=\"concurrent-materialized-view-writes\"><h2><code class=\"docutils literal\">concurrent_materialized_view_writes</code></h2><p>For materialized view writes, as there is a read involved, so this should\nbe limited by the less of concurrent reads or concurrent writes.</p><p><em>Default Value:</em> 32</p></div><div class=\"section\" id=\"file-cache-size-in-mb\"><h2><code class=\"docutils literal\">file_cache_size_in_mb</code></h2><p><em>This option is commented out by default.</em></p><p>Maximum memory to use for sstable chunk cache and buffer pooling.\n32MB of this are reserved for pooling buffers, the rest is used as an\ncache that holds uncompressed sstable chunks.\nDefaults to the smaller of 1/4 of heap or 512MB. This pool is allocated off-heap,\nso is in addition to the memory allocated for heap. The cache also has on-heap\noverhead which is roughly 128 bytes per chunk (i.e. 0.2% of the reserved size\nif the default 64k chunk size is used).\nMemory is only allocated when needed.</p><p><em>Default Value:</em> 512</p></div><div class=\"section\" id=\"buffer-pool-use-heap-if-exhausted\"><h2><code class=\"docutils literal\">buffer_pool_use_heap_if_exhausted</code></h2><p><em>This option is commented out by default.</em></p><p>Flag indicating whether to allocate on or off heap when the sstable buffer\npool is exhausted, that is when it has exceeded the maximum memory\nfile_cache_size_in_mb, beyond which it will not cache buffers but allocate on request.</p><p><em>Default Value:</em> true</p></div><div class=\"section\" id=\"disk-optimization-strategy\"><h2><code class=\"docutils literal\">disk_optimization_strategy</code></h2><p><em>This option is commented out by default.</em></p><p>The strategy for optimizing disk read\nPossible values are:\nssd (for solid state disks, the default)\nspinning (for spinning disks)</p><p><em>Default Value:</em> ssd</p></div><div class=\"section\" id=\"memtable-heap-space-in-mb\"><h2><code class=\"docutils literal\">memtable_heap_space_in_mb</code></h2><p><em>This option is commented out by default.</em></p><p>Total permitted memory to use for memtables. Cassandra will stop\naccepting writes when the limit is exceeded until a flush completes,\nand will trigger a flush based on memtable_cleanup_threshold\nIf omitted, Cassandra will set both to 1/4 the size of the heap.</p><p><em>Default Value:</em> 2048</p></div><div class=\"section\" id=\"memtable-offheap-space-in-mb\"><h2><code class=\"docutils literal\">memtable_offheap_space_in_mb</code></h2><p><em>This option is commented out by default.</em></p><p><em>Default Value:</em> 2048</p></div><div class=\"section\" id=\"memtable-cleanup-threshold\"><h2><code class=\"docutils literal\">memtable_cleanup_threshold</code></h2><p><em>This option is commented out by default.</em></p><p>memtable_cleanup_threshold is deprecated. The default calculation\nis the only reasonable choice. See the comments on  memtable_flush_writers\nfor more information.</p><p>Ratio of occupied non-flushing memtable size to total permitted size\nthat will trigger a flush of the largest memtable. Larger mct will\nmean larger flushes and hence less compaction, but also less concurrent\nflush activity which can make it difficult to keep your disks fed\nunder heavy write load.</p><p>memtable_cleanup_threshold defaults to 1 / (memtable_flush_writers + 1)</p><p><em>Default Value:</em> 0.11</p></div><div class=\"section\" id=\"memtable-allocation-type\"><h2><code class=\"docutils literal\">memtable_allocation_type</code></h2><p>Specify the way Cassandra allocates and manages memtable memory.\nOptions are:</p><dl class=\"docutils\"><dt>heap_buffers</dt>\n<dd>on heap nio buffers</dd>\n<dt>offheap_buffers</dt>\n<dd>off heap (direct) nio buffers</dd>\n<dt>offheap_objects</dt>\n<dd>off heap objects</dd>\n</dl><p><em>Default Value:</em> heap_buffers</p></div><div class=\"section\" id=\"commitlog-total-space-in-mb\"><h2><code class=\"docutils literal\">commitlog_total_space_in_mb</code></h2><p><em>This option is commented out by default.</em></p><p>Total space to use for commit logs on disk.</p><p>If space gets above this value, Cassandra will flush every dirty CF\nin the oldest segment and remove it.  So a small total commitlog space\nwill tend to cause more flush activity on less-active columnfamilies.</p><p>The default value is the smaller of 8192, and 1/4 of the total space\nof the commitlog volume.</p><p><em>Default Value:</em> 8192</p></div><div class=\"section\" id=\"memtable-flush-writers\"><h2><code class=\"docutils literal\">memtable_flush_writers</code></h2><p><em>This option is commented out by default.</em></p><p>This sets the number of memtable flush writer threads per disk\nas well as the total number of memtables that can be flushed concurrently.\nThese are generally a combination of compute and IO bound.</p><p>Memtable flushing is more CPU efficient than memtable ingest and a single thread\ncan keep up with the ingest rate of a whole server on a single fast disk\nuntil it temporarily becomes IO bound under contention typically with compaction.\nAt that point you need multiple flush threads. At some point in the future\nit may become CPU bound all the time.</p><p>You can tell if flushing is falling behind using the MemtablePool.BlockedOnAllocation\nmetric which should be 0, but will be non-zero if threads are blocked waiting on flushing\nto free memory.</p><p>memtable_flush_writers defaults to two for a single data directory.\nThis means that two  memtables can be flushed concurrently to the single data directory.\nIf you have multiple data directories the default is one memtable flushing at a time\nbut the flush will use a thread per data directory so you will get two or more writers.</p><p>Two is generally enough to flush on a fast disk [array] mounted as a single data directory.\nAdding more flush writers will result in smaller more frequent flushes that introduce more\ncompaction overhead.</p><p>There is a direct tradeoff between number of memtables that can be flushed concurrently\nand flush size and frequency. More is not better you just need enough flush writers\nto never stall waiting for flushing to free memory.</p><p><em>Default Value:</em> 2</p></div><div class=\"section\" id=\"cdc-total-space-in-mb\"><h2><code class=\"docutils literal\">cdc_total_space_in_mb</code></h2><p><em>This option is commented out by default.</em></p><p>Total space to use for change-data-capture logs on disk.</p><p>If space gets above this value, Cassandra will throw WriteTimeoutException\non Mutations including tables with CDC enabled. A CDCCompactor is responsible\nfor parsing the raw CDC logs and deleting them when parsing is completed.</p><p>The default value is the min of 4096 mb and 1/8th of the total space\nof the drive where cdc_raw_directory resides.</p><p><em>Default Value:</em> 4096</p></div><div class=\"section\" id=\"cdc-free-space-check-interval-ms\"><h2><code class=\"docutils literal\">cdc_free_space_check_interval_ms</code></h2><p><em>This option is commented out by default.</em></p><p>When we hit our cdc_raw limit and the CDCCompactor is either running behind\nor experiencing backpressure, we check at the following interval to see if any\nnew space for cdc-tracked tables has been made available. Default to 250ms</p><p><em>Default Value:</em> 250</p></div><div class=\"section\" id=\"index-summary-capacity-in-mb\"><h2><code class=\"docutils literal\">index_summary_capacity_in_mb</code></h2><p>A fixed memory pool size in MB for for SSTable index summaries. If left\nempty, this will default to 5% of the heap size. If the memory usage of\nall index summaries exceeds this limit, SSTables with low read rates will\nshrink their index summaries in order to meet this limit.  However, this\nis a best-effort process. In extreme conditions Cassandra may need to use\nmore than this amount of memory.</p></div><div class=\"section\" id=\"index-summary-resize-interval-in-minutes\"><h2><code class=\"docutils literal\">index_summary_resize_interval_in_minutes</code></h2><p>How frequently index summaries should be resampled.  This is done\nperiodically to redistribute memory from the fixed-size pool to sstables\nproportional their recent read rates.  Setting to -1 will disable this\nprocess, leaving existing index summaries at their current sampling level.</p><p><em>Default Value:</em> 60</p></div><div class=\"section\" id=\"trickle-fsync\"><h2><code class=\"docutils literal\">trickle_fsync</code></h2><p>Whether to, when doing sequential writing, fsync() at intervals in\norder to force the operating system to flush the dirty\nbuffers. Enable this to avoid sudden dirty buffer flushing from\nimpacting read latencies. Almost always a good idea on SSDs; not\nnecessarily on platters.</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"trickle-fsync-interval-in-kb\"><h2><code class=\"docutils literal\">trickle_fsync_interval_in_kb</code></h2><p><em>Default Value:</em> 10240</p></div><div class=\"section\" id=\"storage-port\"><h2><code class=\"docutils literal\">storage_port</code></h2><p>TCP port, for commands and data\nFor security reasons, you should not expose this port to the internet.  Firewall it if needed.</p><p><em>Default Value:</em> 7000</p></div><div class=\"section\" id=\"ssl-storage-port\"><h2><code class=\"docutils literal\">ssl_storage_port</code></h2><p>SSL port, for encrypted communication.  Unused unless enabled in\nencryption_options\nFor security reasons, you should not expose this port to the internet.  Firewall it if needed.</p><p><em>Default Value:</em> 7001</p></div><div class=\"section\" id=\"listen-address\"><h2><code class=\"docutils literal\">listen_address</code></h2><p>Address or interface to bind to and tell other Cassandra nodes to connect to.\nYou _must_ change this if you want multiple nodes to be able to communicate!</p><p>Set listen_address OR listen_interface, not both.</p><p>Leaving it blank leaves it up to InetAddress.getLocalHost(). This\nwill always do the Right Thing _if_ the node is properly configured\n(hostname, name resolution, etc), and the Right Thing is to use the\naddress associated with the hostname (it might not be).</p><p>Setting listen_address to 0.0.0.0 is always wrong.</p><p><em>Default Value:</em> localhost</p></div><div class=\"section\" id=\"listen-interface\"><h2><code class=\"docutils literal\">listen_interface</code></h2><p><em>This option is commented out by default.</em></p><p>Set listen_address OR listen_interface, not both. Interfaces must correspond\nto a single address, IP aliasing is not supported.</p><p><em>Default Value:</em> eth0</p></div><div class=\"section\" id=\"listen-interface-prefer-ipv6\"><h2><code class=\"docutils literal\">listen_interface_prefer_ipv6</code></h2><p><em>This option is commented out by default.</em></p><p>If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address\nyou can specify which should be chosen using listen_interface_prefer_ipv6. If false the first ipv4\naddress will be used. If true the first ipv6 address will be used. Defaults to false preferring\nipv4. If there is only one address it will be selected regardless of ipv4/ipv6.</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"broadcast-address\"><h2><code class=\"docutils literal\">broadcast_address</code></h2><p><em>This option is commented out by default.</em></p><p>Address to broadcast to other Cassandra nodes\nLeaving this blank will set it to the same value as listen_address</p><p><em>Default Value:</em> 1.2.3.4</p></div><div class=\"section\" id=\"listen-on-broadcast-address\"><h2><code class=\"docutils literal\">listen_on_broadcast_address</code></h2><p><em>This option is commented out by default.</em></p><p>When using multiple physical network interfaces, set this\nto true to listen on broadcast_address in addition to\nthe listen_address, allowing nodes to communicate in both\ninterfaces.\nIgnore this property if the network configuration automatically\nroutes  between the public and private networks such as EC2.</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"internode-authenticator\"><h2><code class=\"docutils literal\">internode_authenticator</code></h2><p><em>This option is commented out by default.</em></p><p>Internode authentication backend, implementing IInternodeAuthenticator;\nused to allow/disallow connections from peer nodes.</p><p><em>Default Value:</em> org.apache.cassandra.auth.AllowAllInternodeAuthenticator</p></div><div class=\"section\" id=\"start-native-transport\"><h2><code class=\"docutils literal\">start_native_transport</code></h2><p>Whether to start the native transport server.\nThe address on which the native transport is bound is defined by rpc_address.</p><p><em>Default Value:</em> true</p></div><div class=\"section\" id=\"native-transport-port\"><h2><code class=\"docutils literal\">native_transport_port</code></h2><p>port for the CQL native transport to listen for clients on\nFor security reasons, you should not expose this port to the internet.  Firewall it if needed.</p><p><em>Default Value:</em> 9042</p></div><div class=\"section\" id=\"native-transport-port-ssl\"><h2><code class=\"docutils literal\">native_transport_port_ssl</code></h2><p><em>This option is commented out by default.</em>\nEnabling native transport encryption in client_encryption_options allows you to either use\nencryption for the standard port or to use a dedicated, additional port along with the unencrypted\nstandard native_transport_port.\nEnabling client encryption and keeping native_transport_port_ssl disabled will use encryption\nfor native_transport_port. Setting native_transport_port_ssl to a different value\nfrom native_transport_port will use encryption for native_transport_port_ssl while\nkeeping native_transport_port unencrypted.</p><p><em>Default Value:</em> 9142</p></div><div class=\"section\" id=\"native-transport-max-threads\"><h2><code class=\"docutils literal\">native_transport_max_threads</code></h2><p><em>This option is commented out by default.</em>\nThe maximum threads for handling requests (note that idle threads are stopped\nafter 30 seconds so there is not corresponding minimum setting).</p><p><em>Default Value:</em> 128</p></div><div class=\"section\" id=\"native-transport-max-frame-size-in-mb\"><h2><code class=\"docutils literal\">native_transport_max_frame_size_in_mb</code></h2><p><em>This option is commented out by default.</em></p><p>The maximum size of allowed frame. Frame (requests) larger than this will\nbe rejected as invalid. The default is 256MB. If you’re changing this parameter,\nyou may want to adjust max_value_size_in_mb accordingly.</p><p><em>Default Value:</em> 256</p></div><div class=\"section\" id=\"native-transport-max-concurrent-connections\"><h2><code class=\"docutils literal\">native_transport_max_concurrent_connections</code></h2><p><em>This option is commented out by default.</em></p><p>The maximum number of concurrent client connections.\nThe default is -1, which means unlimited.</p><p><em>Default Value:</em> -1</p></div><div class=\"section\" id=\"native-transport-max-concurrent-connections-per-ip\"><h2><code class=\"docutils literal\">native_transport_max_concurrent_connections_per_ip</code></h2><p><em>This option is commented out by default.</em></p><p>The maximum number of concurrent client connections per source ip.\nThe default is -1, which means unlimited.</p><p><em>Default Value:</em> -1</p></div><div class=\"section\" id=\"rpc-address\"><h2><code class=\"docutils literal\">rpc_address</code></h2><p>The address or interface to bind the native transport server to.</p><p>Set rpc_address OR rpc_interface, not both.</p><p>Leaving rpc_address blank has the same effect as on listen_address\n(i.e. it will be based on the configured hostname of the node).</p><p>Note that unlike listen_address, you can specify 0.0.0.0, but you must also\nset broadcast_rpc_address to a value other than 0.0.0.0.</p><p>For security reasons, you should not expose this port to the internet.  Firewall it if needed.</p><p><em>Default Value:</em> localhost</p></div><div class=\"section\" id=\"rpc-interface\"><h2><code class=\"docutils literal\">rpc_interface</code></h2><p><em>This option is commented out by default.</em></p><p>Set rpc_address OR rpc_interface, not both. Interfaces must correspond\nto a single address, IP aliasing is not supported.</p><p><em>Default Value:</em> eth1</p></div><div class=\"section\" id=\"rpc-interface-prefer-ipv6\"><h2><code class=\"docutils literal\">rpc_interface_prefer_ipv6</code></h2><p><em>This option is commented out by default.</em></p><p>If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address\nyou can specify which should be chosen using rpc_interface_prefer_ipv6. If false the first ipv4\naddress will be used. If true the first ipv6 address will be used. Defaults to false preferring\nipv4. If there is only one address it will be selected regardless of ipv4/ipv6.</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"broadcast-rpc-address\"><h2><code class=\"docutils literal\">broadcast_rpc_address</code></h2><p><em>This option is commented out by default.</em></p><p>RPC address to broadcast to drivers and other Cassandra nodes. This cannot\nbe set to 0.0.0.0. If left blank, this will be set to the value of\nrpc_address. If rpc_address is set to 0.0.0.0, broadcast_rpc_address must\nbe set.</p><p><em>Default Value:</em> 1.2.3.4</p></div><div class=\"section\" id=\"rpc-keepalive\"><h2><code class=\"docutils literal\">rpc_keepalive</code></h2><p>enable or disable keepalive on rpc/native connections</p><p><em>Default Value:</em> true</p></div><div class=\"section\" id=\"internode-send-buff-size-in-bytes\"><h2><code class=\"docutils literal\">internode_send_buff_size_in_bytes</code></h2><p><em>This option is commented out by default.</em></p><p>Uncomment to set socket buffer size for internode communication\nNote that when setting this, the buffer size is limited by net.core.wmem_max\nand when not setting it it is defined by net.ipv4.tcp_wmem\nSee also:\n/proc/sys/net/core/wmem_max\n/proc/sys/net/core/rmem_max\n/proc/sys/net/ipv4/tcp_wmem\n/proc/sys/net/ipv4/tcp_wmem\nand ‘man tcp’</p></div><div class=\"section\" id=\"internode-recv-buff-size-in-bytes\"><h2><code class=\"docutils literal\">internode_recv_buff_size_in_bytes</code></h2><p><em>This option is commented out by default.</em></p><p>Uncomment to set socket buffer size for internode communication\nNote that when setting this, the buffer size is limited by net.core.wmem_max\nand when not setting it it is defined by net.ipv4.tcp_wmem</p></div><div class=\"section\" id=\"incremental-backups\"><h2><code class=\"docutils literal\">incremental_backups</code></h2><p>Set to true to have Cassandra create a hard link to each sstable\nflushed or streamed locally in a backups/ subdirectory of the\nkeyspace data.  Removing these links is the operator’s\nresponsibility.</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"snapshot-before-compaction\"><h2><code class=\"docutils literal\">snapshot_before_compaction</code></h2><p>Whether or not to take a snapshot before each compaction.  Be\ncareful using this option, since Cassandra won’t clean up the\nsnapshots for you.  Mostly useful if you’re paranoid when there\nis a data format change.</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"auto-snapshot\"><h2><code class=\"docutils literal\">auto_snapshot</code></h2><p>Whether or not a snapshot is taken of the data before keyspace truncation\nor dropping of column families. The STRONGLY advised default of true\nshould be used to provide data safety. If you set this flag to false, you will\nlose data on truncation or drop.</p><p><em>Default Value:</em> true</p></div><div class=\"section\" id=\"column-index-size-in-kb\"><h2><code class=\"docutils literal\">column_index_size_in_kb</code></h2><p>Granularity of the collation index of rows within a partition.\nIncrease if your rows are large, or if you have a very large\nnumber of rows per partition.  The competing goals are these:</p><ul class=\"simple\"><li>a smaller granularity means more index entries are generated\nand looking up rows withing the partition by collation column\nis faster</li>\n<li>but, Cassandra will keep the collation index in memory for hot\nrows (as part of the key cache), so a larger granularity means\nyou can cache more hot rows</li>\n</ul><p><em>Default Value:</em> 64</p></div><div class=\"section\" id=\"column-index-cache-size-in-kb\"><h2><code class=\"docutils literal\">column_index_cache_size_in_kb</code></h2><p>Per sstable indexed key cache entries (the collation index in memory\nmentioned above) exceeding this size will not be held on heap.\nThis means that only partition information is held on heap and the\nindex entries are read from disk.</p><p>Note that this size refers to the size of the\nserialized index information and not the size of the partition.</p><p><em>Default Value:</em> 2</p></div><div class=\"section\" id=\"concurrent-compactors\"><h2><code class=\"docutils literal\">concurrent_compactors</code></h2><p><em>This option is commented out by default.</em></p><p>Number of simultaneous compactions to allow, NOT including\nvalidation “compactions” for anti-entropy repair.  Simultaneous\ncompactions can help preserve read performance in a mixed read/write\nworkload, by mitigating the tendency of small sstables to accumulate\nduring a single long running compactions. The default is usually\nfine and if you experience problems with compaction running too\nslowly or too fast, you should look at\ncompaction_throughput_mb_per_sec first.</p><p>concurrent_compactors defaults to the smaller of (number of disks,\nnumber of cores), with a minimum of 2 and a maximum of 8.</p><p>If your data directories are backed by SSD, you should increase this\nto the number of cores.</p><p><em>Default Value:</em> 1</p></div><div class=\"section\" id=\"compaction-throughput-mb-per-sec\"><h2><code class=\"docutils literal\">compaction_throughput_mb_per_sec</code></h2><p>Throttles compaction to the given total throughput across the entire\nsystem. The faster you insert data, the faster you need to compact in\norder to keep the sstable count down, but in general, setting this to\n16 to 32 times the rate you are inserting data is more than sufficient.\nSetting this to 0 disables throttling. Note that this account for all types\nof compaction, including validation compaction.</p><p><em>Default Value:</em> 16</p></div><div class=\"section\" id=\"sstable-preemptive-open-interval-in-mb\"><h2><code class=\"docutils literal\">sstable_preemptive_open_interval_in_mb</code></h2><p>When compacting, the replacement sstable(s) can be opened before they\nare completely written, and used in place of the prior sstables for\nany range that has been written. This helps to smoothly transfer reads\nbetween the sstables, reducing page cache churn and keeping hot rows hot</p><p><em>Default Value:</em> 50</p></div><div class=\"section\" id=\"cas-contention-timeout-in-ms\"><h2><code class=\"docutils literal\">cas_contention_timeout_in_ms</code></h2><p>How long a coordinator should continue to retry a CAS operation\nthat contends with other proposals for the same row</p><p><em>Default Value:</em> 1000</p></div><div class=\"section\" id=\"streaming-keep-alive-period-in-secs\"><h2><code class=\"docutils literal\">streaming_keep_alive_period_in_secs</code></h2><p><em>This option is commented out by default.</em></p><p>Set keep-alive period for streaming\nThis node will send a keep-alive message periodically with this period.\nIf the node does not receive a keep-alive message from the peer for\n2 keep-alive cycles the stream session times out and fail\nDefault value is 300s (5 minutes), which means stalled stream\ntimes out in 10 minutes by default</p><p><em>Default Value:</em> 300</p></div><div class=\"section\" id=\"streaming-connections-per-host\"><h2><code class=\"docutils literal\">streaming_connections_per_host</code></h2><p><em>This option is commented out by default.</em></p><p>Limit number of connections per host for streaming\nIncrease this when you notice that joins are CPU-bound rather that network\nbound (for example a few nodes with big files).</p><p><em>Default Value:</em> 1</p></div><div class=\"section\" id=\"phi-convict-threshold\"><h2><code class=\"docutils literal\">phi_convict_threshold</code></h2><p><em>This option is commented out by default.</em></p><p>phi value that must be reached for a host to be marked down.\nmost users should never need to adjust this.</p><p><em>Default Value:</em> 8</p></div><div class=\"section\" id=\"endpoint-snitch\"><h2><code class=\"docutils literal\">endpoint_snitch</code></h2><p>endpoint_snitch – Set this to a class that implements\nIEndpointSnitch.  The snitch has two functions:</p><ul class=\"simple\"><li>it teaches Cassandra enough about your network topology to route\nrequests efficiently</li>\n<li>it allows Cassandra to spread replicas around your cluster to avoid\ncorrelated failures. It does this by grouping machines into\n“datacenters” and “racks.”  Cassandra will do its best not to have\nmore than one replica on the same “rack” (which may not actually\nbe a physical location)</li>\n</ul><p>CASSANDRA WILL NOT ALLOW YOU TO SWITCH TO AN INCOMPATIBLE SNITCH\nONCE DATA IS INSERTED INTO THE CLUSTER.  This would cause data loss.\nThis means that if you start with the default SimpleSnitch, which\nlocates every node on “rack1” in “datacenter1”, your only options\nif you need to add another datacenter are GossipingPropertyFileSnitch\n(and the older PFS).  From there, if you want to migrate to an\nincompatible snitch like Ec2Snitch you can do it by adding new nodes\nunder Ec2Snitch (which will locate them in a new “datacenter”) and\ndecommissioning the old ones.</p><p>Out of the box, Cassandra provides:</p><dl class=\"docutils\"><dt>SimpleSnitch:</dt>\n<dd>Treats Strategy order as proximity. This can improve cache\nlocality when disabling read repair.  Only appropriate for\nsingle-datacenter deployments.</dd>\n<dt>GossipingPropertyFileSnitch</dt>\n<dd>This should be your go-to snitch for production use.  The rack\nand datacenter for the local node are defined in\ncassandra-rackdc.properties and propagated to other nodes via\ngossip.  If cassandra-topology.properties exists, it is used as a\nfallback, allowing migration from the PropertyFileSnitch.</dd>\n<dt>PropertyFileSnitch:</dt>\n<dd>Proximity is determined by rack and data center, which are\nexplicitly configured in cassandra-topology.properties.</dd>\n<dt>Ec2Snitch:</dt>\n<dd>Appropriate for EC2 deployments in a single Region. Loads Region\nand Availability Zone information from the EC2 API. The Region is\ntreated as the datacenter, and the Availability Zone as the rack.\nOnly private IPs are used, so this will not work across multiple\nRegions.</dd>\n<dt>Ec2MultiRegionSnitch:</dt>\n<dd>Uses public IPs as broadcast_address to allow cross-region\nconnectivity.  (Thus, you should set seed addresses to the public\nIP as well.) You will need to open the storage_port or\nssl_storage_port on the public IP firewall.  (For intra-Region\ntraffic, Cassandra will switch to the private IP after\nestablishing a connection.)</dd>\n<dt>RackInferringSnitch:</dt>\n<dd>Proximity is determined by rack and data center, which are\nassumed to correspond to the 3rd and 2nd octet of each node’s IP\naddress, respectively.  Unless this happens to match your\ndeployment conventions, this is best used as an example of\nwriting a custom Snitch class and is provided in that spirit.</dd>\n</dl><p>You can use a custom Snitch by setting this to the full class name\nof the snitch, which will be assumed to be on your classpath.</p><p><em>Default Value:</em> SimpleSnitch</p></div><div class=\"section\" id=\"dynamic-snitch-reset-interval-in-ms\"><h2><code class=\"docutils literal\">dynamic_snitch_reset_interval_in_ms</code></h2><p>controls how often to reset all host scores, allowing a bad host to\npossibly recover</p><p><em>Default Value:</em> 600000</p></div><div class=\"section\" id=\"dynamic-snitch-badness-threshold\"><h2><code class=\"docutils literal\">dynamic_snitch_badness_threshold</code></h2><p>if set greater than zero and read_repair_chance is &lt; 1.0, this will allow\n‘pinning’ of replicas to hosts in order to increase cache capacity.\nThe badness threshold will control how much worse the pinned host has to be\nbefore the dynamic snitch will prefer other replicas over it.  This is\nexpressed as a double which represents a percentage.  Thus, a value of\n0.2 means Cassandra would continue to prefer the static snitch values\nuntil the pinned host was 20% worse than the fastest.</p><p><em>Default Value:</em> 0.1</p></div><div class=\"section\" id=\"server-encryption-options\"><h2><code class=\"docutils literal\">server_encryption_options</code></h2><p>Enable or disable inter-node encryption\nJVM defaults for supported SSL socket protocols and cipher suites can\nbe replaced using custom encryption options. This is not recommended\nunless you have policies in place that dictate certain settings, or\nneed to disable vulnerable ciphers or protocols in case the JVM cannot\nbe updated.\nFIPS compliant settings can be configured at JVM level and should not\ninvolve changing encryption settings here:\n<a class=\"reference external\" href=\"https://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/FIPS.html\">https://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/FIPS.html</a>\n<em>NOTE</em> No custom encryption options are enabled at the moment\nThe available internode options are : all, none, dc, rack</p><p>If set to dc cassandra will encrypt the traffic between the DCs\nIf set to rack cassandra will encrypt the traffic between the racks</p><p>The passwords used in these options must match the passwords used when generating\nthe keystore and truststore.  For instructions on generating these files, see:\n<a class=\"reference external\" href=\"http://download.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html#CreateKeystore\">http://download.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html#CreateKeystore</a></p><p><em>Default Value (complex option)</em>:</p><div class=\"highlight-default\"><div class=\"highlight\"><pre>internode_encryption: none\nkeystore: conf/.keystore\nkeystore_password: cassandra\ntruststore: conf/.truststore\ntruststore_password: cassandra\n# More advanced defaults below:\n# protocol: TLS\n# algorithm: SunX509\n# store_type: JKS\n# cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]\n# require_client_auth: false\n# require_endpoint_verification: false\n</pre></div></div></div><div class=\"section\" id=\"client-encryption-options\"><h2><code class=\"docutils literal\">client_encryption_options</code></h2><p>enable or disable client/server encryption.</p><p><em>Default Value (complex option)</em>:</p><div class=\"highlight-default\"><div class=\"highlight\"><pre>enabled: false\n# If enabled and optional is set to true encrypted and unencrypted connections are handled.\noptional: false\nkeystore: conf/.keystore\nkeystore_password: cassandra\n# require_client_auth: false\n# Set trustore and truststore_password if require_client_auth is true\n# truststore: conf/.truststore\n# truststore_password: cassandra\n# More advanced defaults below:\n# protocol: TLS\n# algorithm: SunX509\n# store_type: JKS\n# cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]\n</pre></div></div></div><div class=\"section\" id=\"internode-compression\"><h2><code class=\"docutils literal\">internode_compression</code></h2><p>internode_compression controls whether traffic between nodes is\ncompressed.\nCan be:</p><dl class=\"docutils\"><dt>all</dt>\n<dd>all traffic is compressed</dd>\n<dt>dc</dt>\n<dd>traffic between different datacenters is compressed</dd>\n<dt>none</dt>\n<dd>nothing is compressed.</dd>\n</dl><p><em>Default Value:</em> dc</p></div><div class=\"section\" id=\"inter-dc-tcp-nodelay\"><h2><code class=\"docutils literal\">inter_dc_tcp_nodelay</code></h2><p>Enable or disable tcp_nodelay for inter-dc communication.\nDisabling it will result in larger (but fewer) network packets being sent,\nreducing overhead from the TCP protocol itself, at the cost of increasing\nlatency if you block for cross-datacenter responses.</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"tracetype-query-ttl\"><h2><code class=\"docutils literal\">tracetype_query_ttl</code></h2><p>TTL for different trace types used during logging of the repair process.</p><p><em>Default Value:</em> 86400</p></div><div class=\"section\" id=\"tracetype-repair-ttl\"><h2><code class=\"docutils literal\">tracetype_repair_ttl</code></h2><p><em>Default Value:</em> 604800</p></div><div class=\"section\" id=\"transparent-data-encryption-options\"><h2><code class=\"docutils literal\">transparent_data_encryption_options</code></h2><p>Enables encrypting data at-rest (on disk). Different key providers can be plugged in, but the default reads from\na JCE-style keystore. A single keystore can hold multiple keys, but the one referenced by\nthe “key_alias” is the only key that will be used for encrypt opertaions; previously used keys\ncan still (and should!) be in the keystore and will be used on decrypt operations\n(to handle the case of key rotation).</p><p>It is strongly recommended to download and install Java Cryptography Extension (JCE)\nUnlimited Strength Jurisdiction Policy Files for your version of the JDK.\n(current link: <a class=\"reference external\" href=\"http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html\">http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html</a>)</p><p>Currently, only the following file types are supported for transparent data encryption, although\nmore are coming in future cassandra releases: commitlog, hints</p><p><em>Default Value (complex option)</em>:</p><div class=\"highlight-default\"><div class=\"highlight\"><pre>enabled: false\nchunk_length_kb: 64\ncipher: AES/CBC/PKCS5Padding\nkey_alias: testing:1\n# CBC IV length for AES needs to be 16 bytes (which is also the default size)\n# iv_length: 16\nkey_provider:\n  - class_name: org.apache.cassandra.security.JKSKeyProvider\n    parameters:\n      - keystore: conf/.keystore\n        keystore_password: cassandra\n        store_type: JCEKS\n        key_password: cassandra\n</pre></div></div></div><div class=\"section\" id=\"tombstone-warn-threshold\"><h2><code class=\"docutils literal\">tombstone_warn_threshold</code></h2><div class=\"section\" id=\"safety-thresholds\"><h3>SAFETY THRESHOLDS #</h3><p>When executing a scan, within or across a partition, we need to keep the\ntombstones seen in memory so we can return them to the coordinator, which\nwill use them to make sure other replicas also know about the deleted rows.\nWith workloads that generate a lot of tombstones, this can cause performance\nproblems and even exaust the server heap.\n(<a class=\"reference external\" href=\"http://www.datastax.com/dev/blog/cassandra-anti-patterns-queues-and-queue-like-datasets\">http://www.datastax.com/dev/blog/cassandra-anti-patterns-queues-and-queue-like-datasets</a>)\nAdjust the thresholds here if you understand the dangers and want to\nscan more tombstones anyway.  These thresholds may also be adjusted at runtime\nusing the StorageService mbean.</p><p><em>Default Value:</em> 1000</p></div></div><div class=\"section\" id=\"tombstone-failure-threshold\"><h2><code class=\"docutils literal\">tombstone_failure_threshold</code></h2><p><em>Default Value:</em> 100000</p></div><div class=\"section\" id=\"batch-size-warn-threshold-in-kb\"><h2><code class=\"docutils literal\">batch_size_warn_threshold_in_kb</code></h2><p>Log WARN on any multiple-partition batch size exceeding this value. 5kb per batch by default.\nCaution should be taken on increasing the size of this threshold as it can lead to node instability.</p><p><em>Default Value:</em> 5</p></div><div class=\"section\" id=\"batch-size-fail-threshold-in-kb\"><h2><code class=\"docutils literal\">batch_size_fail_threshold_in_kb</code></h2><p>Fail any multiple-partition batch exceeding this value. 50kb (10x warn threshold) by default.</p><p><em>Default Value:</em> 50</p></div><div class=\"section\" id=\"unlogged-batch-across-partitions-warn-threshold\"><h2><code class=\"docutils literal\">unlogged_batch_across_partitions_warn_threshold</code></h2><p>Log WARN on any batches not of type LOGGED than span across more partitions than this limit</p><p><em>Default Value:</em> 10</p></div><div class=\"section\" id=\"compaction-large-partition-warning-threshold-mb\"><h2><code class=\"docutils literal\">compaction_large_partition_warning_threshold_mb</code></h2><p>Log a warning when compacting partitions larger than this value</p><p><em>Default Value:</em> 100</p></div><div class=\"section\" id=\"gc-log-threshold-in-ms\"><h2><code class=\"docutils literal\">gc_log_threshold_in_ms</code></h2><p><em>This option is commented out by default.</em></p><p>GC Pauses greater than 200 ms will be logged at INFO level\nThis threshold can be adjusted to minimize logging if necessary</p><p><em>Default Value:</em> 200</p></div><div class=\"section\" id=\"gc-warn-threshold-in-ms\"><h2><code class=\"docutils literal\">gc_warn_threshold_in_ms</code></h2><p><em>This option is commented out by default.</em></p><p>GC Pauses greater than gc_warn_threshold_in_ms will be logged at WARN level\nAdjust the threshold based on your application throughput requirement. Setting to 0\nwill deactivate the feature.</p><p><em>Default Value:</em> 1000</p></div><div class=\"section\" id=\"max-value-size-in-mb\"><h2><code class=\"docutils literal\">max_value_size_in_mb</code></h2><p><em>This option is commented out by default.</em></p><p>Maximum size of any value in SSTables. Safety measure to detect SSTable corruption\nearly. Any value size larger than this threshold will result into marking an SSTable\nas corrupted.</p><p><em>Default Value:</em> 256</p></div><div class=\"section\" id=\"back-pressure-enabled\"><h2><code class=\"docutils literal\">back_pressure_enabled</code></h2><p>Back-pressure settings #\nIf enabled, the coordinator will apply the back-pressure strategy specified below to each mutation\nsent to replicas, with the aim of reducing pressure on overloaded replicas.</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"back-pressure-strategy\"><h2><code class=\"docutils literal\">back_pressure_strategy</code></h2><p>The back-pressure strategy applied.\nThe default implementation, RateBasedBackPressure, takes three arguments:\nhigh ratio, factor, and flow type, and uses the ratio between incoming mutation responses and outgoing mutation requests.\nIf below high ratio, outgoing mutations are rate limited according to the incoming rate decreased by the given factor;\nif above high ratio, the rate limiting is increased by the given factor;\nsuch factor is usually best configured between 1 and 10, use larger values for a faster recovery\nat the expense of potentially more dropped mutations;\nthe rate limiting is applied according to the flow type: if FAST, it’s rate limited at the speed of the fastest replica,\nif SLOW at the speed of the slowest one.\nNew strategies can be added. Implementors need to implement org.apache.cassandra.net.BackpressureStrategy and\nprovide a public constructor accepting a Map&lt;String, Object&gt;.</p></div><div class=\"section\" id=\"otc-coalescing-strategy\"><h2><code class=\"docutils literal\">otc_coalescing_strategy</code></h2><p><em>This option is commented out by default.</em></p><p>Coalescing Strategies #\nCoalescing multiples messages turns out to significantly boost message processing throughput (think doubling or more).\nOn bare metal, the floor for packet processing throughput is high enough that many applications won’t notice, but in\nvirtualized environments, the point at which an application can be bound by network packet processing can be\nsurprisingly low compared to the throughput of task processing that is possible inside a VM. It’s not that bare metal\ndoesn’t benefit from coalescing messages, it’s that the number of packets a bare metal network interface can process\nis sufficient for many applications such that no load starvation is experienced even without coalescing.\nThere are other benefits to coalescing network messages that are harder to isolate with a simple metric like messages\nper second. By coalescing multiple tasks together, a network thread can process multiple messages for the cost of one\ntrip to read from a socket, and all the task submission work can be done at the same time reducing context switching\nand increasing cache friendliness of network message processing.\nSee CASSANDRA-8692 for details.</p><p>Strategy to use for coalescing messages in OutboundTcpConnection.\nCan be fixed, movingaverage, timehorizon, disabled (default).\nYou can also specify a subclass of CoalescingStrategies.CoalescingStrategy by name.</p><p><em>Default Value:</em> DISABLED</p></div><div class=\"section\" id=\"otc-coalescing-window-us\"><h2><code class=\"docutils literal\">otc_coalescing_window_us</code></h2><p><em>This option is commented out by default.</em></p><p>How many microseconds to wait for coalescing. For fixed strategy this is the amount of time after the first\nmessage is received before it will be sent with any accompanying messages. For moving average this is the\nmaximum amount of time that will be waited as well as the interval at which messages must arrive on average\nfor coalescing to be enabled.</p><p><em>Default Value:</em> 200</p></div><div class=\"section\" id=\"otc-coalescing-enough-coalesced-messages\"><h2><code class=\"docutils literal\">otc_coalescing_enough_coalesced_messages</code></h2><p><em>This option is commented out by default.</em></p><p>Do not try to coalesce messages if we already got that many messages. This should be more than 2 and less than 128.</p><p><em>Default Value:</em> 8</p></div><div class=\"section\" id=\"otc-backlog-expiration-interval-ms\"><h2><code class=\"docutils literal\">otc_backlog_expiration_interval_ms</code></h2><p><em>This option is commented out by default.</em></p><p>How many milliseconds to wait between two expiration runs on the backlog (queue) of the OutboundTcpConnection.\nExpiration is done if messages are piling up in the backlog. Droppable messages are expired to free the memory\ntaken by expired messages. The interval should be between 0 and 1000, and in most installations the default value\nwill be appropriate. A smaller value could potentially expire messages slightly sooner at the expense of more CPU\ntime and queue contention while iterating the backlog of messages.\nAn interval of 0 disables any wait time, which is the behavior of former Cassandra versions.</p><p><em>Default Value:</em> 200</p></div><div class=\"section\" id=\"ideal-consistency-level\"><h2><code class=\"docutils literal\">ideal_consistency_level</code></h2><p><em>This option is commented out by default.</em></p><p>Track a metric per keyspace indicating whether replication achieved the ideal consistency\nlevel for writes without timing out. This is different from the consistency level requested by\neach write which may be lower in order to facilitate availability.</p><p><em>Default Value:</em> EACH_QUORUM</p></div>",
        "created_at": "2018-09-13T14:58:09+0000",
        "updated_at": "2018-09-13T14:58:23+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 38,
        "domain_name": "cassandra.apache.org",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12157"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 254,
            "label": "nosql",
            "slug": "nosql"
          }
        ],
        "is_public": false,
        "id": 12156,
        "uid": null,
        "title": "Cassandra’s Place in the NoSQL World - Instaclustr",
        "url": "https://www.instaclustr.com/cassandras-place-nosql-world/",
        "content": "<header id=\"page-masthead\"><div id=\"page-content\"><div class=\"container\"><div class=\"row\"><div class=\"col-md-6 col-md-offset-0 col-sm-8 col-sm-offset-2\"><div class=\"primary first-p-bold\"><p>A question that we commonly get asked is “how does <a href=\"https://www.instaclustr.com/apache-cassandra/\">Apache Cassandra</a> compare to NoSQL technology X?”. While the easy answer is to say “It’s just better”, the truth of course isn’t that simple. NoSQL encompasses a much more diverse range of technologies than the relational database world with specific NoSQL products suited to particular use cases.</p><p>The definition I often use (OK, made up) for NoSQL is any database technology designed for a more specialised use case to in order to overcome the limitations of RDBMS technology in terms of:</p><ul><li>data size;</li> <li>transaction throughout;</li> <li>reliability and manageability;</li> <li>flexibility of data schema; and/or</li> <li>cost of hardware.</li> </ul><p>One implication of this definition is that it doesn’t make sense to say that NoSQL generally is “better” than relational databases. If you want to compare  NoSQL technology X with technology Y then you need to understand what you want to use it for.</p><p>Perhaps the better question to ask is which use cases are <a href=\"https://www.instaclustr.com/solutions/managed-apache-cassandra/\">Cassandra </a>and other popular NoSQL technologies such as <a href=\"https://www.mongodb.com/\">MongoDB</a> and <a href=\"https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html\">Hadoop/HDFS</a> best suited to? The table below provides a summary of the characteristics</p><p>In summary, Cassandra is a great choice where:</p><ul><li>you need an operational database that can extend to supporting analytics;</li> <li>you don’t want to be limited in ability to scale; and</li> <li>you want the highest possible levels of availability.</li> </ul><table><tbody><tr><td> </td><td><b>Cassandra</b></td> <td><b>Hadoop/HDFS</b></td> <td><b>Mongo</b></td> </tr><tr><td><strong>Primary use cases</strong></td> <td>Large-scale operational database; <p>Structured data store for analytics engines</p></td> <td>Big data analytics database</td> <td>Flexible JSON database for rapid development</td> </tr><tr><td><strong>Development Model</strong></td> <td>Apache Foundation community maintained</td> <td>Apache Foundation <p>community maintained</p></td> <td>Proprietary development released under AGPL open source</td> </tr><tr><td><strong>Reliability</strong></td> <td>Extreme reliability, masterless and replicated. No failover required. <p>Full bi-directional multi-datacenter support</p></td> <td>High availability with automated master fail-over.</td> <td>High availability with multiple replicas and automated failover.</td> </tr><tr><td><strong>Read/write latency</strong></td> <td>Typically 5-15 milliseconds for standard operations. Consistent as dataset grows.</td> <td>Engineered for batch throughput rather than latency.</td> <td>Similar to Cassandra for simple operations. More complex querying capability can lead to greater variability.</td> </tr><tr><td><strong>Scalability</strong></td> <td>No practical limits. Operational clusters in the multi-PB range<br />(eg Apple).</td> <td>No practical  limits. Multi-PB scale not uncommon.</td> <td>Can scale to TB and beyond but requires sharding and is therefore less manageable at very large scale.</td> </tr><tr><td><strong>Query Language</strong></td> <td>CQL, an SQL-like language</td> <td>Map-reduce API plus many add-ons available (inc SQL)</td> <td>API and JSON based queries. </td> </tr><tr><td><strong>Data Model</strong></td> <td>Structured tables but allows for sparse value and multi-value fields</td> <td>Schema-less</td> <td>Schema-less JSON</td> </tr></tbody></table></div></div></div></div></div></header>",
        "created_at": "2018-09-13T14:57:24+0000",
        "updated_at": "2018-09-13T14:57:27+0000",
        "published_at": "2017-05-03T23:40:42+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12156"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 995,
            "label": "testing",
            "slug": "testing"
          },
          {
            "id": 1232,
            "label": "stress",
            "slug": "stress"
          }
        ],
        "is_public": false,
        "id": 12155,
        "uid": null,
        "title": "Deep Diving cassandra-stress - Part 3 (Using YAML Profiles) - Instaclustr",
        "url": "https://www.instaclustr.com/deep-diving-cassandra-stress-part-3-using-yaml-profiles/",
        "content": "<p>In the previous two posts of this series (<a href=\"https://www.instaclustr.com/deep-diving-into-cassandra-stress-part-1/\">Part 1</a> and <a href=\"https://www.instaclustr.com/deep-diving-into-cassandra-stress-part-2/\">Part 2</a>) I covered some of the basic commands of <a href=\"https://www.instaclustr.com/apache-cassandra/\">cassandra</a>-stress. In this post I will start looking at the use of the stress YAML file for more advanced stress scenarios, particularly where you want to run stress against a schema that matches one you are planning to use for your application.</p><p>It’s worth noting in the intro that cassandra-stress with a YAML file use a significantly (80%?) different set of code to the standard read/write/mixed commands. So, some assumptions and learnings from the standard commands won’t hold for YAML-driven stress. To cite one example, when running based on YAML, cassandra-stress does not validate that data returned from a select has the expected values as it does with read or mixed.</p><h5>For this article, I’ll reference the following YAML specification file:</h5><p>Before explaining the contents here, let’s see what happens when we run it with the following simple scenario:<br /><code>cassandra-stress user profile=file:///eg-files/stressprofilemixed.yaml no-warmup ops(insert=1) n=100 -rate threads=1 -node x.x.x.x</code></p><p>After running this on an empty cluster, I ran <code>select count(*) from eventsrawtest;</code>. The result? 345 rows – probably not you would have guessed. Here’s how cassandra-stress gets to that:</p><ul><li>n=100 counts number of insert batches, not number of individual insert operations</li>\n<li>Each batch will contain 1 partition’s data (due to partitions=fixed(1) setting) and all 15 of the rows in the partition. There are 15 rows in every partition as the single cluster key (time) has a cluster setting of fixed(15). All the rows in the partition will be included in the batch due to the select: fixed(10)/10 setting (ie changing this to say fixed(5)/10 would result in half the rows from the partition being include in any given batch).</li>\n<li>100 batches of 15 rows each gets you to 1500 rows so how did we end up with 345? This is due (primarily, in this case) to the relatively small range of potential values for the bucket_time. This results in a high overlap in the partition key values that end up getting generated by the uniform distributions. To demonstrate, changing the population of bucket_time to uniform(1..1288) results in 540 rows. In most cases, you want to initially insert data with no overlap to build up a base data set for testing. To facilitate this, I’ve recently submitted a cassandra-stress enhancement that provides sequential generation of seed values the same as used with the write command (<a href=\"https://issues.apache.org/jira/browse/CASSANDRA-12490\">https://issues.apache.org/jira/browse/CASSANDRA-12490</a>). Changing the uniform() distribution to seq() results in the expected 1500 rows being inserted by this command.</li>\n</ul><h5>Let’s look at some of the other column settings:</h5><ul><li><strong>population</strong> – determines the distribution of seed values used in the random data generation. By controlling the distribution of the seed values you control the distribution of the actual inserted values. So, for example uniform(1..100) will allow for up 100 different values each with the same chance of being selected. guassian(1..100) will also allow for up to 100 different values but as they will follow a guassian (otherwise known as normal or bell-curve) distribution, the values around the middle will have a much higher chance of being selected than the values at the extremes (so there will be a set of values more likely to get repeated and some which will occur very infrequently).</li>\n<li><strong>size</strong> – determines the size (length in bytes) of the of the values created for the field.</li>\n<li><strong>cluster</strong> – only applies to clustering columns, specifies the number of values for the column appearing in a single partition. The maximum number of rows in a partition is therefore the product of the maximum number of row of each clustering column (eg max(row1) * max(row 2) * max(row3)).</li>\n</ul><h5>We covered most of the insert settings in the introductory points but here’s a recap:</h5><ul><li><strong>partitions</strong>: the number of different partitions to include in each generated insert batch. Once a partition is chosen for inclusion in a batch, all rows in the partition will become eligible for inclusion and then be filtered according to the select setting. Using, uniform(1..5) would result in each batches containing between 1 and 5 partitions worth of data (with an equal chance of each number in the range).</li>\n<li><strong>batchtype</strong>: logged or unlogged – determines the cassandra batch type to use</li>\n<li><strong>select</strong>: select determines the portion of rows from a partition to select (at random) for inclusion in particular batch. So, for example, fixed(5)/10 would include 50% of rows from the selected partition in each batch. uniform(1..10)/10 would result in between 10% and 100% of rows in the partition being included in the batch with a different select percentage being randomly picked for each partition in each batch.</li>\n</ul><h5>The final section the yaml file that bears some explanation is the queries section. For each query, you specify:</h5><ul><li>A name for the query (pull-for-rollup, get-a-value) which are used to refers to the queries when specifying the mix of operations through the cassandra-stress command line.</li>\n<li><strong>cql</strong> – The actual query with ? characters where values from the population will be substituted in.</li>\n<li><strong>fields</strong> – either samerow or multirow. For samerow, the key values to use for the select will be picked at random (following the same general population rules as for insert) for the list of row keys that has been generated for inserting. For multirow, each of the column values making up the key will be independently randomly selected so there is a chance of generating keys for the selection parameters that don’t exist in the set of data that will/could be inserted according the the population settings.</li>\n</ul><h5>The ops command specified as part of the command line controls the mix of different operations to run. Take for example the following command:</h5><p><code>cassandra-stress user profile=file:///eg-files/stressprofilemixed.yaml ops(insert=1, pull_for_rollup=1, get-value=10) n=120 -node x.x.x.x</code></p><p>This will execute insert batches, pull_for_rollup queries and get-value queries in the ratio 1:1:10. So for this specific example, we’d get 10 inserts, 10 pull_for_rollup queries and 100 get-value queries.</p><p>Hopefully that’s explained the key information you need to use a YAML profile for running cassandra stress. In future instalments I’ll take a look at some of the remaining command line options and walk through a full end-to-end example of designing and executing a test.</p><p>Click here for <a href=\"https://www.instaclustr.com/blog/2016/08/19/deep-diving-into-cassandra-stress-part-1/\">Part One: </a><a href=\"https://www.instaclustr.com/deep-diving-into-cassandra-stress-part-1/\">Deep Diving into Cassandra Stress</a><br />Click here for <a href=\"https://www.instaclustr.com/deep-diving-into-cassandra-stress-part-2/\">Part Two: Mixed Command</a></p>",
        "created_at": "2018-09-13T14:57:14+0000",
        "updated_at": "2018-09-13T15:07:27+0000",
        "published_at": "2016-08-24T05:03:19+0000",
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 5,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12155"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 995,
            "label": "testing",
            "slug": "testing"
          },
          {
            "id": 1232,
            "label": "stress",
            "slug": "stress"
          }
        ],
        "is_public": false,
        "id": 12154,
        "uid": null,
        "title": "Deep Diving into cassandra-stress - Part 2 (Mixed Command) - Instaclustr",
        "url": "https://www.instaclustr.com/deep-diving-into-cassandra-stress-part-2/",
        "content": "<header id=\"page-masthead\"><div id=\"page-content\"><div class=\"container\"><div class=\"row\"><div class=\"col-md-6 col-md-offset-0 col-sm-8 col-sm-offset-2\"><div class=\"primary first-p-bold\"><p>This is the second of a series of articles explain the operations of cassandra-stress. See here for <a href=\"https://www.instaclustr.com/deep-diving-into-cassandra-stress-part-1/\" target=\"_blank\" rel=\"noopener\">Part 1</a>.</p><p>This series of blogs has been created as part of my prep for my Cassandra summit talk ‘Load Testing Cassandra Applications’.</p><p>In this post we’ll examine the operation of the mixed command in <a href=\"https://www.instaclustr.com/apache-cassandra/\">Cassandra</a> stress and along with that some option for changing the schema used by stress (without using the yaml configuration for a complete custom run).</p><p>Firstly, let’s look at the most simple invocation of the mixed command:</p><p><code>cassandra-stress mixed n=100 -node x.x.x.x</code></p><p>First thing to note about this command is that for it to work you need to have run a write command to create the schema and populate data before the run. The write command should be run with at least the same number of operations as the mixed command to ensure that read commands will all retrieve data. For example, to prepare for the above command you should run:</p><p><code>cassandra-stress write n=100 -node x.x.x.x</code></p><p>If you haven’t written the right data before you run the mixed test then you will receive errors like the following:</p><p><code>java.io.IOException: Operation x0 on key(s) [38374d394b504d353430]: Data returned was not validated</code></p><p>This is because cassandra-stress validates the data returned from the select operations against the generated data set expected for the row (and in this case no data was returned).</p><p>So, once you’re set up correctly, what will the command actually do? The key default settings that come into play are:</p><ul><li>Consistency level: LOCAL_ONE – particularly significant for reads as it means only a single node will need to perform each read (vs multiple nodes for the more common QUORUM consistency levels)</li> <li>Command Ratios: {READ=1.0, WRITE=1.0} – the operations will be split evenly between reads and writes</li> <li>Command Clustering Distribution: clustering=GAUSSIAN(1..10) – determines the number of consecutive read or write operations to conduct consecutively. So, in this case will perform between 1 to 10 reads before performing between 1 to 10 writes and then switching back to reads. Due to this clustering, the actual ratio of reads to writes may differ from the target specified in command ratios for small runs such as this. However, it should even out over the course of a longer run.</li> <li>Population Distribution: Gaussian: min=1,max=100,mean=50.500000,stdev=16.500000 – unlike the write command, which uses a sequence from 1 to number of ops, mixed uses randomly generated numbers from 1 to the number of ops as part of the seed for the actual values. As the range is the same as write operations, the values are guaranteed to overlap between write and mixed but the the mixed run will potentially not visit all possible values and will visit them in a random order (of seed values).</li> <li>Rate: Auto, min threads = 4, max threads = 1000 – cassandra-stress will perform multiple runs, start at 4 threads and increasing threads with each run until the total operation rate fails to increase for three consecutive runs. The number of threads used will double with each run up to 16 threads and then increase by 1.5 times with each run after that.</li> </ul><p>The rate setting illustrates an interesting point in using cassandra-stress: generally, increasing the number of client threads will increase the load (throughput) you are placing on the cluster. This is because all operations are executed synchronously (in order to time them) and so more threads allow more operations to be simultaneously thrown at the cluster. Of course, like any multi-threaded system, there is an overhead incurred managing threads on the stress client and a limit to how much work the client can do before it becomes a bottleneck to the test. So, increase threads to increase the load on your cluster but check when you hit the limit that it’s actually your cluster reaching its limit not your stress client.</p><p>That’s it for the mixed command and associated info. Future installments will look at some of the other key options and the yaml configuration file.</p><p>Click here for <a href=\"https://www.instaclustr.com/deep-diving-into-cassandra-stress-part-1/\">Part One: D</a><a href=\"https://www.instaclustr.com/deep-diving-into-cassandra-stress-part-1/\" target=\"_blank\" rel=\"noopener\">eep Diving into Cassandra Stress</a><br />Click here for <a href=\"https://www.instaclustr.com/deep-diving-cassandra-stress-part-3-using-yaml-profiles/\" target=\"_blank\" rel=\"noopener\">Part Three: Using YAML Profiles</a></p></div></div></div></div></div></header>",
        "created_at": "2018-09-13T14:57:04+0000",
        "updated_at": "2018-09-13T14:57:10+0000",
        "published_at": "2016-08-23T04:16:17+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12154"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 995,
            "label": "testing",
            "slug": "testing"
          },
          {
            "id": 1232,
            "label": "stress",
            "slug": "stress"
          }
        ],
        "is_public": false,
        "id": 12153,
        "uid": null,
        "title": "Deep Diving into cassandra-stress (Part 1) - Instaclustr",
        "url": "https://www.instaclustr.com/deep-diving-into-cassandra-stress-part-1/",
        "content": "<header id=\"page-masthead\"><div id=\"page-content\"><div class=\"container\"><div class=\"row\"><div class=\"col-md-6 col-md-offset-0 col-sm-8 col-sm-offset-2\"><div class=\"primary first-p-bold\"><h2>Overview</h2><p>This is the first in a series of blog post I’m planning to create as part of my prep for my Cassandra summit talk ‘Load Testing Cassandra Applications’.</p><p>cassandra-stress is a great utility for stress testing <a href=\"https://www.instaclustr.com/apache-cassandra/\">Cassandra</a>. However, available documentation is a little sparse and it not always entirely clear what load cassandra-stress will generate in a given situation. In this series of blog posts, I plan to walk through a number of cassandra stress scenarios examining exactly how cassandra-stress behaves.</p><p>For this series, I will be using the latest 3.x version of cassandra-stress. If I notice any differences related to a particular point version of Cassandra I will call them out.</p><p>In this first post, I will look at what is about the most basic cassandra-stress command you can run:<br /><code>cassandra-stress write n=10 -node x.x.x.x</code></p><p>I chose this command for two reasons: firstly, using a simple command will allow us to look at some of the basic functions that apply across any cassandra-stress command and secondly, in almost all scenarios, you will want to execute a write to populate a cluster with data before running a read or mixed scenario.</p><h2>cassandra-stress</h2><p>Let’s start by looking at that components of the command itself:</p><ul><li><code>cassandra-stress</code>: invokes a shell script which in turn invokes the main function of the Java class org.apache.cassandra.stress.Stress</li> <li><code>write</code>: execute write operations (other options being read, mixed, user, counter_write and counter_read)</li> <li><code>n=10</code>: execute 10 operations</li> <li><code>-node x.x.x.x</code>: the address of a node in the cluster to establish the initial connection</li> </ul><p>Filling in with defaults, here’s all the settings cassandra-stress will actually use for the run (from my in-progress implementation of <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-11914\" target=\"_blank\" rel=\"noopener noreferrer\">CASSANDRA-11914</a>):</p><h2>Step by Step</h2><p>As you can see, there is a lot going on behind the scenes. So. let’s walk-through step by step what cassandra-stress actually does when you execute this command:</p><ol><li> <ol><li>The options provided through the command line are parsed and filled in with a whole range of default options as necessary (will touch on important defaults below and more in future articles). Interestingly, at this stage a valid cassandra.yaml will need to be loaded. However, as far as I could tell this is just a side effect of cassandra-stress using some core Cassandra classes and the contents of the cassandra.yaml have no effect on the actual cassandra-stress operations.</li>\n<li>The Cassandra Java driver is used to connect to the node specified in the command line. From this node, the driver retrieves the node list and token map for the cluster and then initiates a connection to each node in the cluster.</li>\n<li>A create keyspace (if one doesn’t already exist) command is executed with the following definition:<br />While this definition is a reasonable choice for a simple default, it’s important to note that this is unlikely to be representative of a keyspace you would want to run in production. By far the most common production scenario would be to use NetworkTopologyStrategy and a replication factor of 3. To have cassandra-stress create a keyspace with this strategy you would need to drop any existing keyspace and add the following parameters to the cassandra-stress command line:<br /><code>-schema replication(strategy=NetworkTopologyStrategy,DC_NAME=3)</code><br />Replace DC_NAME with the actual name of your Cassandra data center. On some systems you may also need to escape the brackets ie. <code>replication\\(...\\)</code></li>\n<li>Once the keyspace is created, cassandra-stress creates two tables in the keyspace: standard1 and counter1. We’ll ignore counter1 for now as it’s not used in this test. The definition of the standard1 table created is as follows:<br />While, again, this is a reasonable choice for a simple default, there are a few characteristics to keep in mind if you are trying to draw conclusions from performance using this table definition:\n<ul><li>There is no clustering key, so 1 row per partition – potentially very different performance to a scenario with many rows per partition.</li>\n<li>Compression is disabled – the overhead of compression is typically not huge but could be significant.</li>\n<li>Compact Storage is enabled – this is not enabled by default and will result in smaller representation of the data on disk (although minimal difference with Cassandra 3.x).</li>\n</ul><p>I’ll cover options for using different schemas in a later installment of this series.</p></li>\n<li>cassandra-stress will attempt to make a jmx connection to the nodes in the cluster to collect garbage collection stats. If the attempt fails, the run will proceed without collection garbage collection stats.</li>\n<li>Next, cassandra-stress will run a warmup. This is a standard practice in load testing to reduce variation from start-up variation such as code being loaded to memory and JVM hotspot compilers. The number of warm-up iterations is the lesser 25% of the target of operations or 50k – 2 operations in this trivial example. The warm-up operations are basically the same as the test operations except not timed so I won’t go into them in detail.</li>\n<li>We’ve entered the actual load test phase. cassandra-stress creates 200 client threads and begins executing the target number of operations. In a real test, using the <strong>-rate</strong> option to control the number of client threads is a good way to control load on the cluster.<br />The first attempted operation will create a CQL prepared statement as follows:<br /><code>UPDATE \"standard1\" SET \"C0\" = ?,\"C1\" = ?,\"C2\" = ?,\"C3\" = ?,\"C4\" = ? WHERE KEY=?</code><br />Although we were probably expecting an INSERT statement, updates and inserts are identical in terms of Cassandra implementation so we can expect performance to be the same.This prepared statement will then be will be executed 10 times with different, random data generated for each execution. The statement will be executed with consistent level LOCAL_ONE.cassandra-stress seeds the random generation with a static string plus the column name and a seed number which for the write command defaults to sequentially used numbers from 1 to the number of operations. That means that each column will get different values but the set of values generated will be the same over multiples runs. Generating a static set of values is necessary for read tests but does have the side effect that if you were to run our sample operation (write n=10) 1000 times the end result would still be just 10 rows of data in the table.</li>\n<li>Finally, cassandra-stress prints its results. Here’s an example from a run of this command:\n<p>Many of these results are self explanatory but some bear further explanation:<br /><strong>Op rate</strong> is the rate of execution commands. <strong>Partition rate</strong> is that rate that partitions were visited (updated or read) by those commands and <strong>Row rate</strong> is that rate that rows were visited. For simple, single-row commands all three rates will be equal. The rates will vary in more complex scenarios where a single operation might visit multiple partitions and rows.<br />Similarly, <strong>Total partitions</strong> is the total number of partitions visited during the test. It’s worth noting that this is not unique partitions so even in some write-only scenarios it may not reflect the total number of partitions created by the test.<br />The <strong>GC</strong> statistics report on garbage collection and are zero in this case as JMX ports were blocked to the test cluster.</p></li>\n</ol></li>\n</ol><h2>Conclusion</h2><p>Well, that’s a lot to write about a simple test that inserts 10 rows into a table. Putting it together has helped improved my understanding of cassandra-stress, I hope it’s useful for you too. In future installments, I’ll look some more into the different data generation operations, mixed test and customers schemas using the YAML configuration file. Let me know in the comments if there are any particular areas of interest for the future articles.</p><p>Click here for <a href=\"https://www.instaclustr.com/blog/2016/08/23/deep-diving-into-cassandra-stress-part-2/\" target=\"_blank\" rel=\"noopener\">Part Two: Mixed Command</a><br />Click here for <a href=\"https://www.instaclustr.com/deep-diving-cassandra-stress-part-3-using-yaml-profiles/\" target=\"_blank\" rel=\"noopener\">Part Three: Using YAML Profiles</a></p></div></div></div></div></div></header>",
        "created_at": "2018-09-13T14:56:56+0000",
        "updated_at": "2018-09-13T14:57:01+0000",
        "published_at": "2016-08-19T01:20:07+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 6,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12153"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 995,
            "label": "testing",
            "slug": "testing"
          },
          {
            "id": 1232,
            "label": "stress",
            "slug": "stress"
          }
        ],
        "is_public": false,
        "id": 12152,
        "uid": null,
        "title": "Testing Multiple Tables with Cassandra Stress - Instaclustr",
        "url": "https://www.instaclustr.com/testing-multiple-tables-cassandra-stress/",
        "content": "<header id=\"page-masthead\"><div id=\"page-content\"><div class=\"container\"><div class=\"row\"><div class=\"col-md-6 col-md-offset-0 col-sm-8 col-sm-offset-2\"><div class=\"primary first-p-bold\"><p>The cassandra-stress tool is a powerful tool for benchmarking <a href=\"https://www.instaclustr.com/apache-cassandra/\">Cassandra</a> performance. It allows quite sophisticated specification of data and loads profiles to run against almost any table definition you can create in Cassandra. We’ve previously published detailed blog posts on the use of cassandra-stress: <a href=\"https://www.instaclustr.com/deep-diving-into-cassandra-stress-part-1/\">Part 1</a>, <a href=\"https://www.instaclustr.com/deep-diving-into-cassandra-stress-part-2/\">Part 2</a> and <a href=\"https://www.instaclustr.com/deep-diving-cassandra-stress-part-3-using-yaml-profiles/\">Part 3</a>.</p><p>One significant limitation of cassandra-stress has been that it is only able to execute operations against once table at a time. You could work around that by running multiple instances of cassandra-stress but that was not ideal.</p><p>I recently submitted a patch for <a href=\"https://www.instaclustr.com/apache-cassandra/\">Apache Cassandra</a> that now enables multiple tables to be stressed simultaneously with cassandra-stress (<a href=\"https://issues.apache.org/jira/browse/CASSANDRA-8780\">https://issues.apache.org/jira/browse/CASSANDRA-8780</a>). This blog post provides some more explanation of how to use this new feature. (While the feature won’t hit release until Cassandra 4.0, it’s pretty easy to download the code and build cassandra-stress yourself if you want to use it in the meantime.)</p><p>The three core changes you need to know to stress multiple tables in one run are as follows:</p><ol><li>The profile= command line argument now accepts a comma delimited list of profile yaml files.</li> <li> Profile yaml files can now optionally contain a specname attribute which provide a way to identify the profile. If it’s not specified, the specname is inferred as &lt;keyspace&gt;.&lt;table&gt;.</li> <li>When specifying operation counts using the ops= command line argument you can prefix them with a specname to refer to an operation from a particular profile (eg spec1.insert). If you don’t specify a   specname, the specname from the first listed yaml file will be inferred.</li> </ol><p>The inferred specnames means that existing single yaml file cassandra-stress configurations will continue to run without requiring any change.</p><p>The following provides an example of how this can be used in practice:</p><p>(add all the other standard arguments you would pass to a cassandra-stress run).</p><p>Within table1.yaml, might look something like:</p><div id=\"crayon-5b9a6445f3531231561943\" class=\"crayon-syntax crayon-theme-sublime-text crayon-font-monaco crayon-os-pc print-yes notranslate\" data-settings=\"minimize scroll-mouseover\"><div class=\"crayon-main\"><table class=\"crayon-table\"><tr class=\"crayon-row\"><td class=\"crayon-nums\" data-settings=\"show\"> <div class=\"crayon-nums-content\"><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p><p>23</p><p>24</p><p>25</p><p>26</p><p>27</p><p>28</p><p>29</p><p>30</p><p>31</p><p>32</p><p>33</p><p>34</p><p>35</p></div></td>\n<td class=\"crayon-code\"><div class=\"crayon-pre\"><p># Keyspace name and create CQL</p><p>#</p><p>specname: t1</p><p>keyspace: stressexample</p><p>keyspace_definition: |</p><p>CREATE KEYSPACE stressexample WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'};</p><p>#</p><p># Table name and create CQL</p><p>#</p><p>table: test1</p><p>table_definition: |</p><p>CREATE TABLE test5 (</p><p>pk int,</p><p>val text,</p><p>PRIMARY KEY (pk)</p><p>)</p><p>columnspec:</p><p>- name: pk</p><p>size: fixed(64)</p><p>population: seq(1..100000)</p><p>#</p><p># Specs for insert queries</p><p>#</p><p>insert:</p><p>partitions: fixed(1) # 1 partition per batch</p><p>batchtype: UNLOGGED # use unlogged batches</p><p>select: fixed(10)/10 # no chance of skipping a row when generating inserts</p><p>#</p><p># Read queries to run against the schema</p><p>#</p><p>queries:</p><p>single_read:</p><p>cql: select * from test1 where pk = ?</p><p>fields: samerow</p></div></td>\n</tr></table></div></div><p>table2.yaml would be similar but with specname=t2.</p><p>When you run this command cassandra-stress will first ensure that keyspaces and tables specified in each of the yaml files are created, creating them itself if necessary. It will then execute operations in the ratios specified in the ops argument – in this case, 10% inserts to table test1 as specified in table1.yaml, 10% reads using the single_read query definition and 80% inserts in the table specified in table2.yaml.</p><p>One interesting feature is that the multiple yaml files can all reference the same table in your cassandra cluster. I can see this being useful, for instance, where you want to simulate one read/write pattern against the bulk of your partitions while simultaneously simulating a different pattern against a small number of hot partitions. One thing to be aware of with this approach is that the data overlap between two different specs addressing the same table is hard to predict and so may bear close inspection if it’s important to your scenario.</p><p>I really hope this feature is useful to the Cassandra community. Let me know in the comments section (<a href=\"http://www.mail-archive.com/user@cassandra.apache.org/\">or the Cassandra user mailing list</a>) if you have any queries or suggestions.</p></div></div></div></div></div></header>",
        "created_at": "2018-09-13T14:56:48+0000",
        "updated_at": "2018-09-13T14:56:53+0000",
        "published_at": "2017-05-16T05:19:23+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12152"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 23,
            "label": "elasticsearch",
            "slug": "elasticsearch"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12151,
        "uid": null,
        "title": "Elasticsearch on Cassandra Features Overview - Instaclustr",
        "url": "https://www.instaclustr.com/elassandra-features-overview/",
        "content": "<p>Instaclustr recently announced coming availability of our Managed Service for <a href=\"http://www.elassandra.io/\">Elassandra</a>, which delivers Elasticsearch integrated with Apache Cassandra. This blog post drills down to explain the features and advantages of Elassandra – the best of Cassandra and ElasticSearch.</p><p>Elassandra adds the power of ElasticSearch’s fast, powerful indexing and querying to Cassandra.  By embedding Elasticsearch into Cassandra’s resilient, distributed architecture, Elassandra delivers a solution which combines the functionality of both Cassandra and Elasticsearch in a single integrated solution with improved availability features and simpler management.</p><p>Here are some of the key features:</p><ul><li><strong>Elassandra provides all the indexing and query capabilities of ElasticSearch</strong>.  This includes incredibly fast indexing, querying and analysis of structured and semistructured data.  It allows complex querying of large data sets.  Real-time indexing allows data to become searchable as soon as it is inserted into the database.</li> </ul><ul><li><strong>Powerful, flexible querying of Cassandra tables</strong>.  By design, Cassandra supports a relatively simple range of query operations, generally limited to simple primary key or range lookups.  Elassandra allows the full power of the Elasticsearch Query Language (Query DSL)  to be used to search data in Cassandra tables, including the ability to produce compound statements comprising filter and query blocks and relevance score calculations.</li> </ul><ul><li><strong>Elassandra is Cassandra</strong>.  At its core, Elassandra is the same engine as Cassandra and therefore is automatically part of the Cassandra ecosystem.  Tools and applications which work with Cassandra can be used with little or no change with Elassandra.  This means that the same familiar tools (e.g. nodetool, cqlsh) are used to manage Elassandra, and operations such as repair and compaction are done in the usual way.  Tools like Spark, Storm, Kafka and Zeppelin all work with Elassandra’s Cassandra API.</li> </ul><ul><li><strong>Elassandra is Elasticsearch</strong>.  Elassandra supports Elasticsearch’s RESTful API, and fits right into the Elassandra ecosystem.  Tools like Kibana, Logstash, Beats, JDBC driver, Spark and Kafka all work with Elassandra’s Elasticsearch API.</li> </ul><ul><li><strong>Flexible APIs</strong>.  Data can be loaded in Elassandra through Cassandra’s CQL, or via ElasticSearch’s RESTful API.  Regardless of which approach is used, Elassandra ensures that data is stored in a Cassandra table, and indexed in Elasticsearch.  A wide variety of language bindings can be used to access these APIs.  For CQL these include: Java, Python, Ruby, C#/.Net, Nodejs, PHP, C++, Closure, Scala, Clojure, Erlang, Go, Haskell &amp; Rust, and for Elasticsearch: Java, JavaScript, Groovy, .NET, PHP, Perl, Python, Ruby and others many other via JSON/RESTful interface.</li> </ul><ul><li><strong>Simplified data pipeline</strong>.  A single Elassandra cluster can do the work of several components: a Cassandra cluster, an ElasticSearch cluster, and ETL processes to replicate and synchronise between clusters.  Once transactional data is loaded into an Elassandra cluster, it can be searched directly without extra pipeline steps to load from Cassandra to Elassandra.</li> </ul><ul><li><strong>Masterless Distributed Architecture</strong>.   While standalone Elasticsearch has a master node, which introduces a single point of failure which must be carefully managed.  By running Elasticsearch on Cassandra’s fully distributed, masterless distributed architecture, the single point of failure is eliminated.</li> </ul><ul><li><strong>In Elassandra, Elasticsearch indexes are managed Cassandra as secondary indexes</strong>.  Elassandra fully supports other Cassandra features including: Cluster Replication between data centres; integration with Cassandra backups; and Integration with Cassandra node add/remove functions.</li> </ul><ul><li><strong>Provisioned in minutes, fully integrated, fully managed</strong>.  With Instaclustr, you can have an Elassandra cluster provisioned in minutes, and managed and supported by Instaclustr.  Optionally, your cluster can also be automatically fully configured and integrated with Zeppelin, Spark and Kibana, allowing you to quickly build and deploy your application.</li> </ul>",
        "created_at": "2018-09-13T14:52:02+0000",
        "updated_at": "2018-09-13T14:56:31+0000",
        "published_at": "2017-05-25T06:06:46+0000",
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12151"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1215,
            "label": "networking",
            "slug": "networking"
          }
        ],
        "is_public": false,
        "id": 12150,
        "uid": null,
        "title": "Demystifying Cassandra’s broadcast_address - Instaclustr",
        "url": "https://www.instaclustr.com/demystifying-cassandras-broadcast_address/",
        "content": "<p>When configuring <a href=\"https://www.instaclustr.com/apache-cassandra/\">Apache Cassandra</a> to work in a new environment or with a new application or service we sometimes find ourselves asking <em>“What’s the difference between broadcast_address and broadcast_rpc_address again?”</em>.</p><p>The difference is <strong>broadcast_address</strong> relates to <strong>gossip</strong> and <strong>node to node</strong> <strong>communications</strong>, whereas <strong>broadcast_rpc_address</strong> is associated with <strong>client connections</strong>. Read on for more details.</p><p>The Cassandra configuration file has a few interdependent properties related to communication, which can take a bit of concentration to make sense of. Here is a (hopefully) easy to understand explanation.</p><h2>Node to node communication (i.e. gossip)</h2><p>Cassandra will bind to the <strong>listen_address</strong> or <strong>listen_interface</strong> and listen on the storage_port or ssl_storage_port for gossip. In most cases, these properties may be omitted, resulting in Cassandra binding to the hostname’s IP address (Cassandra uses <a href=\"https://docs.oracle.com/javase/8/docs/api/java/net/InetAddress.html#getLocalHost--\" target=\"_blank\" rel=\"noopener\">InetAddress.getLocalHost()</a>). <strong>Note:</strong> setting listen_address to “localhost” results in Cassandra binding to the loopback interface (not recommended as it only works as a 1-node cluster).</p><p><strong>broadcast_address</strong> is reported to nodes for peer discovery. Topologies that span separate networks need this set to a public address. If this property is omitted, the listen_address will be broadcast to nodes.  <strong>Note:</strong> Nodes can be configured to gossip via the local network and use public addresses for nodes outside its local network by setting <code>prefer_local=true</code> in <em>cassandra-rackdc.properties</em> and using certain endpoint_snitches (such as GossipingPropertyFileSnitch or Ec2MultiRegionSnitch).</p><h2>Client to node communication</h2><h4>rpc_address, rpc_interface and broadcast_rpc_address</h4><p>By “client” I mean Cassandra drivers and clqsh. The drivers may use the Thrift transport or the Native transport (CQL binary protocol). Cqlsh uses the Native transport.</p><p>Cassandra will bind to the <strong>rpc_address</strong> or <strong>rpc_interface</strong> and listen on rpc_port and native_transport_port for client connections. If these properties are omitted, Cassandra will bind to the hostname’s IP address (and would need to be specified to a locally running cqlsh because <code><strong>cqlsh</strong></code> = <code><strong>cqlsh</strong> <strong>&lt;loopback address&gt;</strong></code>).</p><p><strong>broadcast_rpc_address</strong> is a property available in Cassandra 2.1 and above. It is reported to clients during cluster discovery and as cluster metadata. It is useful for clients outside the cluster’s local network. This property is typically either:</p><ul><li>the public address if most clients are outside the cluster’s local network</li> <li>the local network address if most clients are in the cluster’s local network</li> </ul><p>If this property is omitted, rpc_address will be reported to clients.</p><p><strong>Note 1:</strong> If there are a mix of clients inside and outside the local network, use an AddressTranslator policy to compensate for unreachable addresses (only available for Java and Python drivers at the time of writing. <a href=\"https://www.instaclustr.com/apache-cassandra-deployed-on-private-and-public-networks/\" target=\"_blank\" rel=\"noopener\">Here is a Java example</a>.)</p><p><strong>Note 2: </strong><em>rpc_address</em> may be set to 0.0.0.0. In this case, Cassandra binds to all available interfaces, including loopback, which is used by cqlsh when no host is specified. But 0.0.0.0 is not routable, so Cassandra will use a different property to determine the address to broadcast to clients:</p><ul><li>for Cassandra 2.1 and later: broadcast_rpc_address must be set and will be reported to clients.</li> <li>for Cassandra prior to 2.1: broadcast_address (or listen_address if omitted) will be reported to clients.</li> </ul><p>Summary:</p><table><thead><tr><th>Cassandra Version</th> <th>Purpose</th> <th>Properties</th> <th>Typical Setting</th> </tr></thead><tbody><tr><td>All</td> <td>gossip</td> <td>listen_address or listen_interface (with storage_port or ssl_storage_port)</td> <td>Omit to bind to InetAddress.getLocalHost()</td> </tr><tr><td>All</td> <td>peer discovery (within the cluster)</td> <td>broadcast_address else listen_address</td> <td>public address</td> </tr><tr><td>All</td> <td>client requests (CQL and Thrift)</td> <td>rpc_address or rpc_interface (with rpc_port and native_transport_port)</td> <td>Omit to bind to InetAddress.getLocalHost()</td> </tr><tr><td>2.1 and later</td> <td>cluster discovery | metadata (to the client)</td> <td>broadcast_rpc_address else rpc_address</td> <td>Omit to broadcast InetAddress.getLocalHost()</td> </tr><tr><td>2.0 and prior</td> <td>cluster discovery | metadata (to the client)</td> <td>rpc_address or broadcast_address if 0.0.0.0</td> <td>Omit to broadcast InetAddress.getLocalHost()</td> </tr></tbody></table>",
        "created_at": "2018-09-13T14:51:37+0000",
        "updated_at": "2018-09-13T14:51:40+0000",
        "published_at": "2016-10-11T03:20:04+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12150"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 1196,
            "label": "metric collection",
            "slug": "metric-collection"
          }
        ],
        "is_public": false,
        "id": 12149,
        "uid": null,
        "title": "Third Contact with a Monolith: Part C - In the Pod - Instaclustr",
        "url": "https://www.instaclustr.com/third-contact-monolith-part-c-pod/",
        "content": "<p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/09/In-the-pod-Instaclustr.jpg\"> </a></p><aside class=\"content-cta\"><div class=\"primary\"><h4>Related Articles:</h4></div></aside><br /><img class=\"aligncenter wp-image-6883 size-full\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/09/In-the-pod-Instaclustr.jpg\" alt=\"Third Contact with a Monolith PArt C In the Pod Instaclustr\" width=\"640\" height=\"601\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/In-the-pod-Instaclustr.jpg 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/In-the-pod-Instaclustr-300x282.jpg 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/In-the-pod-Instaclustr-51x48.jpg 51w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/In-the-pod-Instaclustr-115x108.jpg 115w\" /><h2>A simple classification problem: Will the Monolith react? Is it safe?!</h2><p>Maybe a cautious approach to a bigger version of the Monolith (2km long) in a POD that is only 2m in diameter is advisable.   What do we know about how Monoliths react to stimuli? A simple classification problem consists of the category (label) “no reaction” (0) or “reaction” (1), and the stimuli tried (features which will be used to predict the label).  In the following table, the first column in the label to be predicted (positive, 1, or negative, 0), the remaining columns are the features, and each row is an example:</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/09/Classification-problem-table-instaclustr.png\"><img class=\"aligncenter wp-image-6886 size-medium\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/09/Classification-problem-table-instaclustr-300x292.png\" alt=\"classification problem table Instaclustr\" width=\"300\" height=\"292\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Classification-problem-table-instaclustr-300x292.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Classification-problem-table-instaclustr-768x747.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Classification-problem-table-instaclustr-1024x996.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Classification-problem-table-instaclustr-633x616.png 633w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Classification-problem-table-instaclustr-640x622.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Classification-problem-table-instaclustr-49x48.png 49w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Classification-problem-table-instaclustr-111x108.png 111w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Classification-problem-table-instaclustr.png 1088w\" /></a></p><p>This problem is trivial as a positive reaction only occurred for Touch OR Sunlight (the camera flash was a co-incidence), the features are all binary, and only a single feature at a time is true. As a result of extensive tests on the Monolith imagine that there are lots more examples and data available, each feature is a floating point number, all features don’t have values for all examples, there are potentially 1,000s of features, and a sufficiently accurate classification rule may require an arbitrary number of features.</p><p>Given that HAL has been unplugged we have to analyse the data ourselves. What do we have available in the POD?</p><p>One approach is to use an appropriately 1960’s-vintage machine learning algorithm suitable for simple binary classification problems. Decision tree algorithms were invented in the 1960’s (Concept Learning System, by a Psychologist, Earl Hunt), were improved in the 1980’s (e.g. ID3, C4.5, and <a href=\"https://link.springer.com/content/pdf/10.1023%2FA%3A1022699322624.pdf\">FOIL</a>, which references my 1988 machine learning algorithm, <i>Gargantubrain</i>), and are still useful. Spark’s Machine Learning Library (MLLib) has a number of regression and classification algorithms, including decision trees, that improve on the originals by running them “transparently” in parallel on multiple servers for scalability. </p><p><a href=\"https://spark.apache.org/mllib/\">Spark’s scalable ML library is available here</a>. It’s easy to download Spark and run MLLib examples locally, but the scalability benefits are best realised after deployment to an <a href=\"https://www.instaclustr.com/solutions/managed-apache-spark/\">Instaclustr managed Spark cluster.</a> Here’s the documentation on the <a href=\"https://spark.apache.org/docs/latest/mllib-decision-tree.html\">MLLib decision tree</a> algorithm.</p><h2>Training</h2><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/09/download-1.jpeg\"><img class=\"aligncenter wp-image-6888 size-full\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/09/download-1.jpeg\" alt=\"Training Instaclustr\" width=\"198\" height=\"254\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/download-1.jpeg 198w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/download-1-37x48.jpeg 37w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/download-1-84x108.jpeg 84w\" /></a></p><p>We’ll have a look at the example Java code, starting from the call to the decision tree algorithm to build the model from the examples, and working out from there. <a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/tree/DecisionTree.html\">DecisionTree</a> and <a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/tree/model/DecisionTreeModel.html\">DecisionTreeModel</a> documents are relevant.  Here’s the code.  Note the API for trainClassifier:</p><p>trainClassifier(JavaRDD&lt;LabeledPoint&gt; input, int numClasses,<br />java.util.Map&lt;Integer,Integer&gt; categoricalFeaturesInfo, String<br />impurity, int maxDepth, int maxBins)</p><p>The first argument is a JavaRDD of LabeledPoint. The other arguments are for control of the algorithm:</p><p>// Set parameters for DecisionTree learning.</p><p>// Empty categoricalFeaturesInfo indicates all features are continuous.</p><p>Integer numClasses = 2;</p><p>Map&lt;Integer, Integer&gt; categoricalFeaturesInfo = new HashMap&lt;&gt;();</p><p>String impurity = “gini”; // or “entropy”</p><p>Integer maxDepth = 5;</p><p>Integer maxBins = 32;</p><p>// Train DecisionTree model</p><p>DecisionTreeModel model = <b>DecisionTree.trainClassifier</b>(<b>trainingData</b>, numClasses, categoricalFeaturesInfo, impurity, maxDepth, maxBins);</p><p>System.out.println(“Learned classification tree model:\\n” + model.toDebugString());</p><p>What is a JavaRDD and LabeledPoint?</p><h2>LabeledPoint</h2><h2><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/09/pinch-point-arrow-shape-label-lb-2785.png\"><img class=\"size-medium wp-image-6889 aligncenter\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/09/pinch-point-arrow-shape-label-lb-2785-154x300.png\" alt=\"Pinch point Instaclustr\" width=\"154\" height=\"300\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/pinch-point-arrow-shape-label-lb-2785-154x300.png 154w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/pinch-point-arrow-shape-label-lb-2785-316x616.png 316w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/pinch-point-arrow-shape-label-lb-2785-328x640.png 328w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/pinch-point-arrow-shape-label-lb-2785-25x48.png 25w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/pinch-point-arrow-shape-label-lb-2785-55x108.png 55w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/pinch-point-arrow-shape-label-lb-2785.png 410w\" /><br /></a>             .</h2><p><a href=\"https://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point\" target=\"_blank\" rel=\"noopener\">https://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point</a></p><p><a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/regression/LabeledPoint.html\" target=\"_blank\" rel=\"noopener\">https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/regression/LabeledPoint.html</a></p><p>Decision trees are a type of supervised learner, so we need a way of telling the algorithm what class (positive or negative for binary classifiers) each training example is.  LabeledPoint is a single labelled example (a “point” in n-dimensional space). It’s a tuple consisting of a Double label (either 0 or 1 for negative or positive examples), and a Vector of features, either dense or sparse.  Features are numbered from 0 to n. These are the examples from the documentation. Note the Vectors import. This is important as the default Spark Vector is NOT CORRECT.</p><p>import org.apache.spark.mllib.linalg.Vectors;<br />import org.apache.spark.mllib.regression.LabeledPoint;</p><p>// Create a labeled point with a positive label and a dense feature vector.</p><p>// There are three features, with values 1, 0, 3.<br />LabeledPoint pos = new LabeledPoint(1.0, Vectors.dense(1.0, 0.0, 3.0));</p><p>// Create a labeled point with a negative label and a sparse feature vector.</p><p>// There are two features, 0 and 2, with values 1 and 3 respectively.</p><p>LabeledPoint neg = new LabeledPoint(0.0, Vectors.sparse(3, new int[] {0, 2}, new double[] {1.0, 3.0}));</p><h2>Resilient Distributed Datasets (RDDs)</h2><p><a href=\"https://spark.apache.org/docs/latest/rdd-programming-guide.html\" target=\"_blank\" rel=\"noopener\">https://spark.apache.org/docs/latest/rdd-programming-guide.html</a></p><p><a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/api/java/JavaRDD.html\" target=\"_blank\" rel=\"noopener\">https://spark.apache.org/docs/latest/api/java/org/apache/spark/api/java/JavaRDD.html</a></p><h6>From the documentation: </h6><p><em>“Spark revolves around the concept of a resilient distributed dataset (RDD), which is a fault-tolerant collection of elements that can be operated on in parallel. There are two ways to create RDDs: parallelizing</em><em> an existing collection in your driver </em>program<em> or referencing a dataset in an external storage system… “</em> (such as Cassandra)</p><p><a href=\"https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-rdd.html\" target=\"_blank\" rel=\"noopener\">This blog has a good explanation of the Spark architecture</a>, and explains that the features of RDDs are:</p><ul><li>Resilient, i.e. fault-tolerant with the help of <a href=\"https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-rdd.html#lineage\">RDD lineage graph</a> and so able to recompute missing or damaged partitions due to node failures.</li>\n<li>Distributed with data residing on multiple nodes in a <a href=\"https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-cluster.html\">cluster</a>.</li>\n<li>Dataset is a collection of <a href=\"https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-rdd-partitions.html\">partitioned data</a> with primitive values or values of values, e.g. tuples or other objects (that represent records of the data you work with).</li>\n</ul><p>RDD’s are also immutable and can be cached. Fault-tolerance depends on the execution model which computes a Directed Acyclic Graph (DAG) of stages for each job, runs stages in optimal locations based on data location, shuffles data as required, and re-runs failed stages. </p><h2>LIBSVM – sparse data format</h2><p><a href=\"https://spark.apache.org/docs/2.0.2/mllib-data-types.html\">https://spark.apache.org/docs/2.0.2/mllib-data-types.html</a></p><p><a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/util/MLUtils.html\">https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/util/MLUtils.html</a></p><p>Where does the RDD training data come from? In the example code I read it from a local file using MLUtils.loadLibSVMFile() (I’ll come back to the parameters later):</p><p>JavaRDD&lt;LabeledPoint&gt; data = MLUtils.loadLibSVMFile(sc, path).toJavaRDD();</p><h6>From the documentation:</h6><p><em>“It is very common in practice to have sparse training data. MLlib supports reading training examples stored in LIBSVM format, which is the default format used by LIBSVM and LIBLINEAR. It is a text format in which each line represents a labeled sparse feature vector using the following format:</em></p><p><em>label index1:value1 index2:value2 …</em></p><p><em>where the indices are one-based and in ascending order. After loading, the feature indices are converted to zero-based.”</em></p><p>For the above Monolith example the LIBSVM input file looks like this (assuming that feature values of “0” indicate non-existent data):</p><p><strong>1.0 1:1.0<br />0.0 2:1.0</strong><br /><strong>0.0 3:1.0</strong><br /><strong>0.0 4:1.0</strong><br /><strong>0.0 5:1.0</strong><br /><strong>0.0 6:1.0</strong><br /><strong>0.0 7:1.0</strong><br /><strong>1.0 8:1.0</strong><br /><strong>0.0 9:1.0</strong><br /><strong>0.0 10:1.0</strong></p><p>This doesn’t seem very “user friendly” as we have lost the feature names. I wonder if there is a better data format?</p><h2>Splitting (the data, not the atom)</h2><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/09/Splitting-the-data-not-the-atom-instaclustr.jpg\"><img class=\"aligncenter wp-image-6890 size-full\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/09/Splitting-the-data-not-the-atom-instaclustr.jpg\" alt=\"splitting the data, not the atom instaclustr\" width=\"356\" height=\"199\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Splitting-the-data-not-the-atom-instaclustr.jpg 356w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Splitting-the-data-not-the-atom-instaclustr-300x168.jpg 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Splitting-the-data-not-the-atom-instaclustr-86x48.jpg 86w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Splitting-the-data-not-the-atom-instaclustr-193x108.jpg 193w\" /></a></p><p>Once we have the training data in the correct format for the algorithm (JavaRDD&lt;LabeledPoint&gt;), but before we train the model, we need to split it into two random subsets for training and testing. JavaRDD.randomSplit() does this and takes parameters:</p><p>double[] weights – weights for splits, will be normalized if they don’t sum to 1</p><p>long seed – random seed</p><p>// Split sample RDD into two sets, 60% training data, 40% testing data. 11 is a seed.</p><p>   JavaRDD&lt;LabeledPoint&gt;[] splits = data.randomSplit(new double[]{0.6, 0.4}, 11L);</p><p>   JavaRDD&lt;LabeledPoint&gt; trainingData = splits[0].cache();  // cache the data</p><p>   JavaRDD&lt;LabeledPoint&gt; testData = splits[1];</p><p>Notice the cache() call for trainingData.  What does it do? Spark is Lazy, it doesn’t evaluate RDD’s until an action forces it to. Hence RDD’s can be evaluated multiple times which is expensive. cache() creates an in memory cache “checkpoint” of an RDD which can be reused. The most obvious case is when an RDD is used multiple times (i.e. iteration), or for branching transformations (i.e. multiple different RDD’s are computed from an original RDD), in which case the original should be cached.<br /><a href=\"https://stackoverflow.com/questions/28981359/why-do-we-need-to-call-cache-or-persist-on-a-rdd\" target=\"_blank\" rel=\"noopener\">https://stackoverflow.com/questions/28981359/why-do-we-need-to-call-cache-or-persist-on-a-rdd</a><br /></p><p>Note that for this example the <i>initial data </i>should be cached as we use it later to count the total, positive and negative examples.</p><h2>Evaluating the Model on the Test Data – Thumbs down or up?</h2><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-Data-2-Instaclustr.png\"><img class=\"aligncenter wp-image-6892 size-medium_large\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-Data-2-Instaclustr-768x521.png\" alt=\"Evaluating the model on the test data Instaclustr\" width=\"768\" height=\"521\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-Data-2-Instaclustr-768x521.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-Data-2-Instaclustr-300x204.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-Data-2-Instaclustr-1024x695.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-Data-2-Instaclustr-907x616.png 907w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-Data-2-Instaclustr-640x434.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-Data-2-Instaclustr-71x48.png 71w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-Data-2-Instaclustr-159x108.png 159w\" /></a></p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-data-1-Instaclustr.png\"><img class=\"aligncenter wp-image-6893 size-medium_large\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-data-1-Instaclustr-768x394.png\" alt=\"Evaluating the model on the test data Instaclustr\" width=\"768\" height=\"394\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-data-1-Instaclustr-768x394.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-data-1-Instaclustr-300x154.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-data-1-Instaclustr-1024x525.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-data-1-Instaclustr-966x496.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-data-1-Instaclustr-640x328.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-data-1-Instaclustr-94x48.png 94w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-data-1-Instaclustr-210x108.png 210w\" /></a></p><p>We trained a decision tree model above. What can we do with it? We trained it on the trainingData subset of examples leaving us with the testData to evaluate it on. MLLib computes some useful evaluation metrics which are documented here:</p><p><a href=\"https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html\" target=\"_blank\" rel=\"noopener\">https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html</a></p><p><a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/evaluation/BinaryClassificationMetrics.html\" target=\"_blank\" rel=\"noopener\">https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/evaluation/BinaryClassificationMetrics.html</a></p><p>Here’s the code which computes the evaluation metrics:</p><p>// Compute evaluation metrics.<br />BinaryClassificationMetrics metrics = new BinaryClassificationMetrics(predictionAndLabels.rdd());</p><p>The highlighted parameter is an RDD of (prediction, label) pairs for a subset of examples. I.e. (0, 0) (0, 1) (1, 0) (1, 0). I.e. for each example, run the model and return a tuple of predicted label and actual example label. Here’s the complete code that does this on the testData and then computes the evaluation metrics:</p><p>// For every example in testData, p, replace it by a Tuple of (predicted category, labelled category)</p><p>// E.g. (1.0,0.0) (0.0,0.0) (0.0,0.0) (0.0,1.0)</p><p>JavaPairRDD&lt;Object, Object&gt; predictionAndLabels = testData.mapToPair(p -&gt;</p><p>    new Tuple2&lt;&gt;(model.predict(p.features()), p.label()));</p><p>// Compute evaluation metrics.</p><p>BinaryClassificationMetrics metrics = new BinaryClassificationMetrics(predictionAndLabels.rdd());</p><p>How does this work? There may be a few unfamiliar things in this code which we’ll explore: Tuple2, JavaPairRDD, mapToPair, features() and label(). </p><h4>Tuple(ware)</h4><p>Java doesn’t have a built-in Tuple type, so you have to use the scala.Tuple2 class. </p><h4>MAP</h4><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/09/Here-be-dragons-Instaclustr.jpg\"><img class=\"aligncenter wp-image-6894 size-full\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/09/Here-be-dragons-Instaclustr.jpg\" alt=\"MAP RDD transformation Instaclustr\" width=\"363\" height=\"234\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Here-be-dragons-Instaclustr.jpg 363w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Here-be-dragons-Instaclustr-300x193.jpg 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Here-be-dragons-Instaclustr-74x48.jpg 74w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Here-be-dragons-Instaclustr-168x108.jpg 168w\" /></a></p><p>Map is an RDD transformation. Transformations pass each dataset element through a function and return a new RDD representing the result. Actions return a result after running a computation on a dataset. Transformations are lazy and are not computed until required by an action. In the above example, for every example in testData, p, it is replaced by a Tuple of (predicted category, labelled category). These values are computed by running model.predict() on all the features in the example, and using the label() of the example.</p><p>See LabeledPoint <a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/regression/LabeledPoint.html\" target=\"_blank\" rel=\"noopener\">documentation</a> for features() and label() methods.</p><p><a href=\"https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/api/java/JavaPairRDD.html\" target=\"_blank\" rel=\"noopener\">JavaPairRDD</a> is a (key, value) version of RDD. Instead of the general map function you need to use mapToPair.</p><p><i>Map</i> (and <i>mapToPair</i>) are examples of “Higher Order Functions”. These are used a lot in Spark but are really pretty ancient and were first practically used in 1960 in <a href=\"http://www-formal.stanford.edu/jmc/recursive.pdf\" target=\"_blank\" rel=\"noopener\">LISP</a>. Disturbingly the abstract sounds like an early attempt at HAL:</p><p><i>“A programming system called LISP … was designed to facilitate experiments … whereby a machine could be instructed to … exhibit “common sense” in carrying out its instructions.”</i></p><p>I used LISP for a real AI project (once), but (it ((had too) (many) brackets) (for (((me (Lots of Idiotic Spurious Parentheses))) ???!!! (are these balanced?!).</p><p>preferred(I, Prolog).</p><p>The following code prints out the main evaluation metrics, precision, recall and F.</p><p>JavaRDD&lt;Tuple2&lt;Object, Object&gt;&gt; precision = metrics.precisionByThreshold().toJavaRDD();</p><p>   System.out.println(“Precision by threshold: ” + precision.collect());</p><p>   JavaRDD&lt;Tuple2&lt;Object, Object&gt;&gt; recall = metrics.recallByThreshold().toJavaRDD();</p><p>   System.out.println(“Recall by threshold: ” + recall.collect());</p><p>   JavaRDD&lt;Tuple2&lt;Object, Object&gt;&gt; f = metrics.fMeasureByThreshold().toJavaRDD();</p><p>   System.out.println(“F by threshold: ” + f.collect());</p><p>Note that the metrics are designed for algorithms that can have multiple threshold values (i.e. the classification can have an associated probability).  However, the decision tree algorithm we are using is a simple yes/no binary classification.  A “confusion matrix” is a simple way of understanding the evaluation metrics. For each of the two states of reality (no reaction or reaction from the monolith) the model can make a true or false prediction giving four possible outcomes: Correct: TN = True Negative (the model correctly predicted no reaction), TP = True Positive (correctly predicted a reaction). And Incorrect: FP = False Positive (predicted a reaction but there was no reaction), FN = False Negative (predicted no reaction but there was a reaction).  FP is often called a Type I error, and FN a type II error. </p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/09/Model-prediction-Instaclustr.png\"><img class=\"aligncenter wp-image-6895 size-large\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/09/Model-prediction-Instaclustr-1024x382.png\" alt=\"Model Prediction Instaclustr\" width=\"1024\" height=\"382\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Model-prediction-Instaclustr-1024x382.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Model-prediction-Instaclustr-300x112.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Model-prediction-Instaclustr-768x287.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Model-prediction-Instaclustr-966x360.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Model-prediction-Instaclustr-640x239.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Model-prediction-Instaclustr-129x48.png 129w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Model-prediction-Instaclustr-290x108.png 290w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Model-prediction-Instaclustr.png 1544w\" /></a></p><p><b>Precision</b> = (TP)/(TP+FP), the proportion of <i>predicted</i> <i>positives</i> that were actually positive (the right column).</p><p><b>Recall = </b>(TP)/(TP+FN), the proportion of <i>actual</i> <i>positives</i> that were correctly predicted as positive (the bottom row). </p><p><a href=\"https://en.wikipedia.org/wiki/F1_score\" target=\"_blank\" rel=\"noopener\">F is the average of precision and recall.</a></p><p>And the results (on the extended When Will the Monolith React? data) were as follows:</p><p>Precision = 71%</p><p>Recall = 45%</p><p>F = 56%</p><p>Precision is better than recall. 71% of the models predicted “Reaction” cases were in fact a “Reaction”. However, the model only correctly predicted 45% of all the actual “Reaction” cases correctly. The reason for using precision and recall metrics is to check if the model performance is purely the result of guessing. For example, if only 20% of examples are positive, then just guessing will result in a model “accuracy” approaching 80%.</p><h4>FILTER</h4><p>Another common Spark transformation is filter.   We’ll use filter to count the number of positive and negative examples in the data to check if our model is any better than guessing. filter() takes a function as the argument, applies it to each element in the dataset, and only returns the element if the function evaluates to true. </p><p>This code calculates that there are 884 examples, 155 positive and 729 negative, giving:</p><p>probability of positive example = 0.1753393665158371</p><p>probability of negative example = 0.8246606334841629</p><p>This tells us that just guessing would result in close to 82% accuracy. The actual model accuracy for the example is 85% (which requires extra code, not shown).</p><h2>Spark Context</h2><p><a href=\"http://spark.apache.org/docs/latest/rdd-programming-guide.html\" target=\"_blank\" rel=\"noopener\">http://spark.apache.org/docs/latest/rdd-programming-guide.html</a></p><p><a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html\" target=\"_blank\" rel=\"noopener\">https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html</a></p><p>This leaves us with the final but “important bit” of the code at the start.  How is Spark actually run?  A SparkContext object tells Spark how to access a cluster, and a SparkConf has information about your application.  Given that we are just running Spark locally just pass “local” to setMaster.</p><p>SparkConf conf = new SparkConf().setAppName(“Java Decision Tree Classification Example”);<br />conf.setMaster(“local”);<br />SparkContext sc = new SparkContext(conf);<br />String path = “WillTheMonolithReact.txt”;</p><p><a href=\"https://spark.apache.org/docs/latest/cluster-overview.html\" target=\"_blank\" rel=\"noopener\">To run Spark on a cluster</a> you need (a) a cluster with Spark set-up (e.g. an Instaclustr cluster with Spark add-on), and (b) <a href=\"https://support.instaclustr.com/hc/en-us/articles/213097877-Getting-Started-with-Instaclustr-Spark-Cassandra\" target=\"_blank\" rel=\"noopener\">to know more about the Spark architecture and how to package and submit applications</a>.</p><p>How does this help with our approach to the Monolith? Maybe raising the POD’s manipulators in “greeting”? Will it be friends?</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/09/2001-4d.jpeg\"><img class=\"alignnone size-medium wp-image-6896\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/09/2001-4d-300x140.jpeg\" alt=\"\" width=\"300\" height=\"140\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/2001-4d-300x140.jpeg 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/2001-4d-103x48.jpeg 103w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/2001-4d-232x108.jpeg 232w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/2001-4d.jpeg 638w\" /></a></p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/09/2001-4e.jpeg\"><img class=\"alignnone size-medium wp-image-6897\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/09/2001-4e-300x140.jpeg\" alt=\"\" width=\"300\" height=\"140\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/2001-4e-300x140.jpeg 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/2001-4e-103x48.jpeg 103w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/2001-4e-232x108.jpeg 232w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/2001-4e.jpeg 638w\" /></a></p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/09/2001-4f.jpeg\"><img class=\"alignnone size-medium wp-image-6898\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/09/2001-4f-300x140.jpeg\" alt=\"\" width=\"300\" height=\"140\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/2001-4f-300x140.jpeg 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/2001-4f-103x48.jpeg 103w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/2001-4f-232x108.jpeg 232w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/2001-4f.jpeg 638w\" /></a></p><h4>NOTE 1</h4><p>The actual data I used for this example was from the Instametrics example introduced in previous blogs, with the goal of predicting long JVM Garbage Collections in advance. I took a small sample of JVM-related metrics, and computed the min, avg and max for each 5 minute bucket. These became the features. For the label I determined if there was a long GC in the next 5 minute bucket (1) or not (0).  The real data has 1,000s of metrics, so next blog we’re going to need some serious Spark processing even just to produce the training data, and explore the interface to Cassandra, and the suitability of Cassandra for Sparse data.</p><h4>NOTE 2</h4><p>Space travel is sooooo slow, in the years we’ve been on board Spark has changed from RDD to DataFrames. I’ll revise the code for the next blog.</p><p><i>Guide </i><a href=\"https://spark.apache.org/docs/latest/ml-guide.html\" target=\"_blank\" rel=\"noopener\"><i>https://spark.apache.org/docs/latest/ml-guide.html</i></a></p><p><i>As of Spark 2.0, the </i><a href=\"https://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds\" target=\"_blank\" rel=\"noopener\"><i>RDD</i></a><i>-based APIs in the </i><i>spark.mllib</i><i> package have entered maintenance mode. The primary Machine Learning API for Spark is now the </i><a href=\"https://spark.apache.org/docs/latest/sql-programming-guide.html\" target=\"_blank\" rel=\"noopener\"><i>DataFrame</i></a><i>-based API in the </i><i>spark.ml</i><i> package.</i></p>",
        "created_at": "2018-09-13T14:50:21+0000",
        "updated_at": "2018-09-13T14:50:40+0000",
        "published_at": "2017-09-29T09:16:28+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 13,
        "domain_name": "www.instaclustr.com",
        "preview_picture": "https://www.instaclustr.com/wp-content/uploads/2017/09/In-the-pod-Instaclustr.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12149"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 996,
            "label": "monitoring",
            "slug": "monitoring"
          }
        ],
        "is_public": false,
        "id": 12148,
        "uid": null,
        "title": "Third Contact With a Monolith - Beam Me Down Scotty - Instaclustr",
        "url": "https://www.instaclustr.com/third-contact-monolith-beam-scotty/",
        "content": "<p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Space-Odyssey-image-Instaclustr.jpg\"><br /><img class=\"aligncenter wp-image-6851 size-full\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Space-Odyssey-image-Instaclustr.jpg\" alt=\"Third Contact with a monolith Part B instaclustr\" width=\"596\" height=\"1340\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Space-Odyssey-image-Instaclustr.jpg 596w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Space-Odyssey-image-Instaclustr-133x300.jpg 133w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Space-Odyssey-image-Instaclustr-455x1024.jpg 455w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Space-Odyssey-image-Instaclustr-534x1200.jpg 534w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Space-Odyssey-image-Instaclustr-274x616.jpg 274w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Space-Odyssey-image-Instaclustr-285x640.jpg 285w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Space-Odyssey-image-Instaclustr-21x48.jpg 21w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Space-Odyssey-image-Instaclustr-48x108.jpg 48w\" /></a></p><h2>Regression Analysis is (relatively) easy</h2><p>Hypothesis: <em>Using only a subset of GC metrics we can compute linear regression functions using only heap space used to predict when the next GC occurs. To do this we don’t need access to all the metrics per host, just a subset. And we can extend it in the future to use multiple variables and/or regression classification to predict which GCs are likely to be “long”.</em></p><p>Let’s look at some sample heap space data and GC time information. Here’s a graph showing heap space used increasing over time (x-axis, seconds) and then a GC kicking in (orange line), resulting in a reduction in the heap space used. The GC value is the duration of the last GC so changes from approximately 7s to 9s.</p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/15-min-sample-heap-and-lastGC-duration.png\"><img class=\"aligncenter wp-image-6852 size-large\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/15-min-sample-heap-and-lastGC-duration-1024x529.png\" alt=\"15 min sample heap and lastGC duration\" width=\"1024\" height=\"529\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/15-min-sample-heap-and-lastGC-duration-1024x529.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/15-min-sample-heap-and-lastGC-duration-300x155.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/15-min-sample-heap-and-lastGC-duration-768x397.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/15-min-sample-heap-and-lastGC-duration-966x499.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/15-min-sample-heap-and-lastGC-duration-640x331.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/15-min-sample-heap-and-lastGC-duration-93x48.png 93w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/15-min-sample-heap-and-lastGC-duration-209x108.png 209w\" /></a></p><p>The following graph shows a linear regression function fitted to the portion of the heap space used graph in the interval between GCs. It shows a significant linear correlation with R- squared 0.71.  </p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Trend-line-for-heap-instaclustr-graph.png\"><img class=\"aligncenter wp-image-6853 size-large\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Trend-line-for-heap-instaclustr-graph-1024x566.png\" alt=\"Trend line for heap Instaclustr graph\" width=\"1024\" height=\"566\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Trend-line-for-heap-instaclustr-graph-1024x566.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Trend-line-for-heap-instaclustr-graph-300x166.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Trend-line-for-heap-instaclustr-graph-768x424.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Trend-line-for-heap-instaclustr-graph-966x534.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Trend-line-for-heap-instaclustr-graph-640x354.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Trend-line-for-heap-instaclustr-graph-87x48.png 87w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Trend-line-for-heap-instaclustr-graph-195x108.png 195w\" /></a></p><p>Is this useful? Assuming we know (or can compute) the amount of heap used that triggers a GC then yes. This may be node-specific (depending on how much heap is available), may change (as more memory is allocated), and may be a percentage (which we have to compute, for this example it was 70%). Obviously, the slope of the function will vary across and even within nodes depending on the frequency of GC (which may depend on many other variables which we’ll look at next time), so the function needs to be computed for each node for each interval between GCs. This requires us to do a few things: Find when each GC occurs so we can determine the time period from the end of the last GC to the start of the next GC to use as the sample period to compute the regression function over; for each interval normalise the actual timestamps to relative time (in seconds) from the start of the interval; keep track of the amount of heap used that triggers the GC (or compute the percentage used); and find sufficient samples for a node to make predictions and check the accuracy and usefulness of the results.</p><p>Some preliminary inspection of sample data showed a few problems. Using GC startTime and endTime seemed to be the obvious way of finding when GCs occurred. However, there was something odd about the times as they were in the distant past, in fact, they were about 300 times less than expected. Oh, that five minutes isn’t it. Turns out that they had been truncated to bucket_time making them useless for my purpose. Another thing to watch for with Cassandra timestamps is that they are in UTC. If you convert them to a Java Date type they will be in local time. The <a href=\"https://docs.oracle.com/javase/8/docs/api/java/time/Instant.html\">Java Instant class</a> is a better representation once you read the timestamps on the client side (Instant is UTC, and has lots of useful time comparison methods).</p><p>Is there a way of using either GC duration and/or collectionCount to compute the GC start/end times? Yes, in theory. Both approaches worked ok, but I noticed large gaps in the timestamps for most nodes which made using duration tricky. The final version used collectionCount and discarded GC events that were more than 1 count different.   But why were there gaps in the data?</p><h2>Materialization accidents – “beam me down Scotty!”</h2><p>My suspicion turned to the materialized views (MVs) I had to created to make the queries easier. Obviously, MVs take time to populate from an existing table, but how do you know if they are complete? There are two system tables to check, one shows build in progress, and one shows built views completed  (system.built_views): </p><h6>keyspace_name | view_name              | generation_number | last_token<br />———————-+————————-+—————————-+—————————–<br />instametrics       | host_service_time  |             16563                 | -9178511905308517795<br />instametrics       | host_service_value |             16563                | -9178511746231999096</h6><p>So this probably explained the data gaps – a few days after creation the MVs were still being built. In fact I suspect they have got “stuck” (anyone know how to unstick them? I tried dropping a MV and trying to create another but this resulted in other nasty things). Given its sci-fi origins, I should have been wary of <a href=\"https://www.instaclustr.com/apache-cassandra/\">Cassandra</a> Materialization! The Star Trek Transporter was notoriously accident prone often resulting in duplicates, combined life forms, being turned inside out, or just plain non-existence, whoops. <a href=\"https://www.thoughtco.com/worst-transporter-accidents-on-star-trek-4046426\">Worst Star Trek Transporter accidents!</a></p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Mad-Star-Blecch.jpg\"><img class=\"aligncenter wp-image-6855 size-full\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Mad-Star-Blecch.jpg\" alt=\"\" width=\"610\" height=\"554\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Mad-Star-Blecch.jpg 610w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Mad-Star-Blecch-300x272.jpg 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Mad-Star-Blecch-53x48.jpg 53w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Mad-Star-Blecch-119x108.jpg 119w\" /></a></p><p>I finally got the code working and ran it on a subset of 100 nodes. However, it was only finding a relatively small number of GCs per node due to the data gaps. I realized that the default read consistency level is only one, which means that only data from the first node it is found on is returned. I tried increasing the consistency level (to QUORUM) which resulted in more GCs found. See code on GIST:</p><p>Each regression analysis uses only the first half of the available data, and if the R-squared value is over 0.5 then the computed function is used to predict the next GC time, and the percentage error is computed and averaged over all results founds and reported as follows:</p><p><strong>Total time = 428,469ms</strong></p><p><strong>avgPerErr = 21.59 gcAvgInterval = 13,447.88s used 42.0 out of 218.0</strong></p><p>The analysis took 428 seconds, for the regressions that had a sufficiently high correlation (about 20%), the average percentage prediction error for the time of the next GC was 22%. The average time between GCs was 13,447 seconds, so the average error is about 3,000 seconds. So this approach would work well, but for only 20% of cases. There’s obviously something more complicated going on for the majority of cases requiring us to get even closer to the monolith, we need to install the analysis code closer to the data. </p><aside class=\"content-cta\"><div class=\"primary\"><h4>Related Articles:</h4></div></aside>",
        "created_at": "2018-09-13T14:49:49+0000",
        "updated_at": "2018-09-13T14:49:55+0000",
        "published_at": "2017-09-20T16:30:18+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 4,
        "domain_name": "www.instaclustr.com",
        "preview_picture": "https://www.instaclustr.com/wp-content/uploads/2017/09/Space-Odyssey-image-Instaclustr.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12148"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12147,
        "uid": null,
        "title": "Third contact with a Monolith - Long Range Sensor Scan - Instaclustr",
        "url": "https://www.instaclustr.com/third-contact-monolith-long-range-sensor-scan/",
        "content": "<p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/6de5fa5f6be3a4d882e36395e3def498-a-space-odyssey-stanley-kubrick.jpg\"><img class=\"aligncenter wp-image-6433 size-full\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/6de5fa5f6be3a4d882e36395e3def498-a-space-odyssey-stanley-kubrick.jpg\" alt=\"2001 a space odyssey - Third Contatc with the monolith Instaclustr\" width=\"736\" height=\"1127\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/6de5fa5f6be3a4d882e36395e3def498-a-space-odyssey-stanley-kubrick.jpg 736w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/6de5fa5f6be3a4d882e36395e3def498-a-space-odyssey-stanley-kubrick-196x300.jpg 196w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/6de5fa5f6be3a4d882e36395e3def498-a-space-odyssey-stanley-kubrick-669x1024.jpg 669w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/6de5fa5f6be3a4d882e36395e3def498-a-space-odyssey-stanley-kubrick-402x616.jpg 402w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/6de5fa5f6be3a4d882e36395e3def498-a-space-odyssey-stanley-kubrick-418x640.jpg 418w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/6de5fa5f6be3a4d882e36395e3def498-a-space-odyssey-stanley-kubrick-31x48.jpg 31w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/6de5fa5f6be3a4d882e36395e3def498-a-space-odyssey-stanley-kubrick-71x108.jpg 71w\" /></a></p><p><img class=\"aligncenter wp-image-6819 size-full\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Distance-of-the-Planets-from-the-sun-in-Astronomical-Units-Instaclustr.png\" alt=\"Distance of the Planets from the sun in Astronomical Units Instaclustr\" width=\"599\" height=\"196\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Distance-of-the-Planets-from-the-sun-in-Astronomical-Units-Instaclustr.png 599w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Distance-of-the-Planets-from-the-sun-in-Astronomical-Units-Instaclustr-300x98.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Distance-of-the-Planets-from-the-sun-in-Astronomical-Units-Instaclustr-147x48.png 147w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Distance-of-the-Planets-from-the-sun-in-Astronomical-Units-Instaclustr-330x108.png 330w\" /></p><h6><strong>Earth to Mars distance = 0.52 AU (1.52-1AU, 78M km)</strong></h6><h6><strong>Earth to Jupiter distance = 4.2 AU (5.2-1AU, 628M km)</strong></h6><p>It’s a long way to Jupiter, would you like to:</p><h6><strong>(a) sleep the whole way in suspended animation?  (bad choice, you don’t wake up)</strong></h6><h6><strong>(b) be embodied as HAL the AI? (go crazy and get unplugged)?</strong></h6><h6><strong>(c) be one of the two crew to stay awake the entire journey (only one of you survives)?</strong></h6><p>What can we do to find out more about our eventual destination the meantime and relieve the boredom?</p><p>In the traditions of the best sci-fi, there are always tricks to speed things up or cut down on the special effects budget. In <i>Star Trek</i> these were Sensors to Scan distant phenomenon:</p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Spock-and-the-Enterprise-bridge-sensor-display.jpg\"><img class=\"aligncenter wp-image-6820 size-large\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Spock-and-the-Enterprise-bridge-sensor-display-1024x765.jpg\" alt=\"Spock and the Enterprise bridge sensor display\" width=\"1024\" height=\"765\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Spock-and-the-Enterprise-bridge-sensor-display.jpg 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Spock-and-the-Enterprise-bridge-sensor-display-300x224.jpg 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Spock-and-the-Enterprise-bridge-sensor-display-768x574.jpg 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Spock-and-the-Enterprise-bridge-sensor-display-825x616.jpg 825w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Spock-and-the-Enterprise-bridge-sensor-display-640x478.jpg 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Spock-and-the-Enterprise-bridge-sensor-display-64x48.jpg 64w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Spock-and-the-Enterprise-bridge-sensor-display-145x108.jpg 145w\" /></a></p><p>And the Transporter which used Materialization and Dematerialization to beam people to and from the ship and the surface of planets. </p><p>Energize!</p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Star-Trek-Energize-Instaclustr.png\"><img class=\"wp-image-6821 size-large aligncenter\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Star-Trek-Energize-Instaclustr-1024x764.png\" alt=\"Start Trek - Energize Instaclustr\" width=\"1024\" height=\"764\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Star-Trek-Energize-Instaclustr-1024x764.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Star-Trek-Energize-Instaclustr-300x224.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Star-Trek-Energize-Instaclustr-768x573.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Star-Trek-Energize-Instaclustr-825x616.png 825w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Star-Trek-Energize-Instaclustr-640x478.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Star-Trek-Energize-Instaclustr-64x48.png 64w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Star-Trek-Energize-Instaclustr-145x108.png 145w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Star-Trek-Energize-Instaclustr.png 1522w\" /></a></p><p>At the risk of mixing sci-fi movie metaphors:</p><p><em>“Captain’s Log, Stardate 1672.1. Warp drive offline. We’re running on auxiliary impulse engines to Jupiter to explore an anomaly. Our mission is to explore a snapshot of the Instametrics monitoring data on a Cassandra cluster that has appeared around the orbit of Jupiter. Our goal is to predict long JVM garbage collection durations at least five minutes in advance. Initiating long-range sensor scans after which I will attempt a risky long-range transportation down to the surface of the anomaly.”</em></p><p>Let’s get <a href=\"https://www.instaclustr.com/apache-cassandra/\">Cassandra</a> to do all the hard work to reduce the amount of data we need to transfer from the cluster to my laptop. There are two approaches that may result in some useful insights. Calculating some summary statistics, and creating a materialized view.</p><p>The first challenge is connecting to the Cassandra cluster. Unlike the trial cluster example I used previously, this cluster had been created with private IP broadcast_rpc_adresses rather than public IP addresses. Does this mean I can’t connect to the cluster from outside the private address range at all? Not exactly. You can still connect via a single public IP address, which will then become the only controller node your client communicates with. For testing, this is probably ok, but there is a <a href=\"https://www.instaclustr.com/apache-cassandra-deployed-on-private-and-public-networks/\">better solution</a> using VPC peering and the private IP addresses.</p><p>The next step is to see what JVM GC related metrics were collected. </p><p>Connecting via the cqlshell, we can run some CQL commands. </p><p>Describe instametrics;</p><p>Reveals that there are some meta-data tables which contain all the host names, and all the metric (“service”) names for each host:</p><p>How many nodes are there in this sample data set? </p><p>cqlsh&gt; select count(host) from instametrics.host;</p><p><b>system.count(host)<br /></b>————————–<br /><b>                               591</b></p><p>The next question I had was how many metrics are there per node? Using a sample of the host names found in the host table I ran this query a few times:</p><p>select count(service) from instametrics.service_per_host where host=’random host name’;</p><p>The somewhat surprising answer was between 1 and lots (16122!). About 1000 seemed an average so this means about 591*1000 metrics in total = 591,000. </p><p>From this distance, we therefore need to focus on just a few metrics. Selecting all the metrics available for one host we find that the following are the only metrics directly related to JVM GC.</p><p><b>collectionCount</b> is the count of the GC, and <b>collectionTime</b> is the total GC collection time, since last JVM restart.</p><h6><strong>/cassandra/jvm/gc/ConcurrentMarkSweep/collectionCount                                                                               </strong></h6><h6><strong>/cassandra/jvm/gc/ConcurrentMarkSweep/collectionTime</strong></h6><p><b>duration</b> is the time of the last occurring GC, and <b>startTime</b> and <b>endTime</b> are the time of the start and end of the last occurring GC.</p><h6><strong>/cassandra/jvm/gc/ConcurrentMarkSweep/lastGc/duration</strong></h6><h6><strong>/cassandra/jvm/gc/ConcurrentMarkSweep/lastGc/endTime</strong></h6><h6><strong>/</strong>cassandra<strong>/</strong>jvm<strong>/</strong>gc<strong>/ConcurrentMarkSweep/</strong>lastGc<strong>/</strong>startTime</h6><p>I normally start a data analysis by looking at statistics for some of the metrics of interest, such as distribution (e.g. a CDF graph) or a histogram.  But to compute these statistics you need access to all the data…</p><p>Kirk: “Mister Spock. Full sensor scan of the anomaly, please.”</p><p>Spock:  “Captain,  the results are not logical – I’m picking up what looks like Buckets – Fascinating!”</p><p>Buckets can be problematic:</p><ul><li>A drop in the bucket</li>\n<li>To kick the bucket</li>\n<li>There’s a hole in my bucket</li>\n</ul><p>The table with the actual values for the hosts and services is defined as follows:</p><p>The partition key requires values for <b>host, bucket_time </b>and<b> service</b> to select a subset of rows.   What’s bucket_time? Based on the name of the table and inspecting a sample of the rows, it’s the timestamp truncated to 5 minute bucket periods. How do we find the range of bucket_time values for the table? Here’s a few ways… You just “know it” (but I don’t).  The range has been recorded in another table (not that I can see). Or, start from now and search backwards in time by 5 minute intervals. This assumes that the data is really from the past and not the future. Start from the Epoch and search forward? Do a binary search between the Epoch and now? Look at samples of the data? This revealed that at least some of the data was from the early part of 2016. This would give us a starting point to search forwards and backwards in time until gaps are detected when we can assume they are the temporal boundaries of the data (assuming there aren’t large gaps in the data).</p><p>An alternative approach is to make a copy of the table with bucket_time removed from the partition key. Is this possible? Well, yes, in theory. <a href=\"http://cassandra.apache.org/doc/latest/cql/mvs.html\">Materialized VIews </a>are possible in Cassandra, and can even be created from existing tables. I created two materialized views, one with the values ordered (descending), and one with the time ordered (ascending).</p><p>Note that the rules for Materialized Views are very specific. You have to include all the columns from the original PRIMARY KEY in the new table (but they can be in the partition or cluster keys and in different orders), you can include at most one extra column in the new primary key, and the select has to be present and include at least one column – even though the partition key columns are selected by default). The idea was to use the first materialized view to get some summary statistics of the GC durations (e.g. min, average, max), and the second for queries over specific time periods and metrics (to find GCs, and heap memory used between GCs).</p><p>This worked as expected, taking 172 seconds, with these results:</p><p>Total time = 172343, nodes = 591</p><p>Overall stats: Min = 47.0</p><p>Overall stats: Avg = 6555.39</p><p>Overall stats: Max = 56095.0</p><p>The maximum GC duration was 56s, with an “average” of 6.5s.</p><p>Here’s the partial code I used:</p><p>On closer inspection, the average values were incorrect, as the GC duration values are repeated for multiple timestamps.  However, I also obtained the max GC duration for each node, revealing that about 200 nodes (out of 591) had maximum GC durations greater than 10s. This is a good start for “remote” data analysis.</p><aside class=\"content-cta\"><div class=\"primary\"><h4>Related Articles:</h4></div></aside>",
        "created_at": "2018-09-13T14:49:16+0000",
        "updated_at": "2018-09-13T14:49:23+0000",
        "published_at": "2017-09-14T16:31:02+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 6,
        "domain_name": "www.instaclustr.com",
        "preview_picture": "https://www.instaclustr.com/wp-content/uploads/2017/09/6de5fa5f6be3a4d882e36395e3def498-a-space-odyssey-stanley-kubrick.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12147"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1233,
            "label": "data.modeling",
            "slug": "data-modeling"
          }
        ],
        "is_public": false,
        "id": 12146,
        "uid": null,
        "title": "Cassandra NoSQL Data Model Design - Instaclustr",
        "url": "https://www.instaclustr.com/cassandra-nosql-data-model-design-2/",
        "content": "<h2>Abstract</h2><p>This paper describes the process that we follow at<br />Instaclustr to design a <a href=\"https://www.instaclustr.com/apache-cassandra/\">Cassandra</a> data model for our<br />customers.</p><p>While not a prescriptive, formal process it does define<br />phases and steps that our team follows when we are<br />design a new data model for our customers:</p><p><strong>Phase 1: Understand the data</strong></p><p><strong>Phase 2: Define the entities</strong></p><p><strong>Phase 3: Review &amp; tune</strong></p><p>As well as defining the process we also provide a<br />worked example based on building a database to<br />store and retrieve log messages from multiple servers.</p><h2>Overview</h2><p>We recently published a blog post on the most common data modelling mistakes that we see with<br /><a href=\"https://www.instaclustr.com/apache-cassandra/\">Cassandra</a>. This post was very popular and led me to think about what advice we could provide on<br />how to approach designing your Cassandra data model so as to come up with a quality design that<br />avoids the traps.</p><p>There are a number of good articles around that with rules and patterns to fit your data model into:</p><p><a href=\"https://www.instaclustr.com/resource/6-step-guide-to-apache-cassandra-data-modelling-white-paper/\" target=\"_blank\" rel=\"noopener\">6 Step Guide to Apache Cassandra Data Modelling </a></p><p>and</p><p><a href=\"https://support.instaclustr.com/hc/en-us/articles/207071957-Data-Modelling-Recommended-Practices\" target=\"_blank\" rel=\"noopener\">Data Modelling Recommended Practices</a></p><p>However, we haven’t found a step by step guide to analysing your data to determine how to fit in<br />these rules and patterns. This white paper is a quick attempt at filling that gap.</p><h2>Phase 1: Understand the data</h2><p>This phase has two distinct steps that are both designed to gain a good understanding of the data that<br />you are modelling and the access patterns required.</p><h3>Define the data domain</h3><p>The first step is to get a good understanding of your data domain. As someone very familiar with<br />relation data modelling, I tend to sketch (or at least think) ER diagrams to understand the entities,<br />their keys and relationships. However, if you’re familiar with another notation then it would likely<br />work just as well. The key things you need to understand at a logical level are:</p><p>• What are the entities (or objects) in your data model?<br />• What are the primary key attributes of the entities?<br />• What are the relationships between the entities (i.e. references from one to the other)?<br />• What is the relative cardinality of the relationships (i.e. if you have a one to many is it one to<br />10 or one to 10,000 on average)?</p><p>Basically, these are the same things you’d expect in from logical ER model (although we probably<br />don’t need a complete picture of all the attributes) along with a complete understanding of the<br />cardinality of relationships that you’d normally need for a relational model.<br />An understanding of the demographics of key attributes (cardinality, distribution) will also be useful<br />in finalising your Cassandra model. Also, understand which key attributes are fixed and which change<br />over the life of a record.</p><h3>Define the required access patterns</h3><p>The next step, or quite likely a step carried out in conjunction with step 1, is to understand how you<br />will need to access your data:</p><ul><li>List out the paths you will follow to access the data, such as: <ul><li>Start with a customer id, search for transactions in a date range and then look up all the<br />details about a particular transaction from the search resultsStart with a particular server and metric, retrieve x metrics values in ascending age</li> <li>Start with a particular server and metric, retrieve x metrics values in ascending age starting at a particular point in time.</li> <li>For a given sensor, retrieve all readings of multiple metrics for a given day.</li> <li>For a given sensor, retrieve the current value.</li> </ul></li> </ul><ul><li>Remember that any updates of a record are an access path that needs to be considered</li> <li>Determine which accesses are the most crucial from a performance point of view – are there some which need to be as quick as possible while performance requirements for others allow time for multiple reads or range scans?</li> <li>Remember that you need a pretty complete understanding of how you will access your data at this stage – part of the trade-off for Cassandra’s performance, reliability and scalability is a fairly restricted set of methods for accessing data in a particular table.</li> </ul><h2>Phase 2: Understand the entities</h2><p>This phase has two specific steps designed to gain an understanding of both the primary and<br />secondary entities associated with the data.</p><h3>Identify primary access entities</h3><p>Now we’re moving from analysing your data domain and application requirements to starting to<br />design your data model. You really want to be pretty solid on steps 1 and 2 before moving on to this<br />stage.</p><p>The idea here is to denormalize your data into the smallest number of tables possible based on your<br />access patterns. For each lookup by key that your access patterns require, you will need a table to<br />satisfy that lookup. I’ve coined the term primary access entity to describe the entity your using for<br />the lookup (for example, a lookup by client id is using client as the primary access entity, a lookup by<br />server and metric name is using a server-metric entity as the primary access entity).</p><p>The primary access entity defines the partition level (or grain if you’re familiar with dimensional<br />modelling) of the resulting denormalized table (i.e. there will be one partition in the table for each<br />instance of the primary access entity).</p><p>You may choose to satisfy some access patterns using secondary indexes rather than complete replicas<br />of the data with a different primary access entity. Keep in mind that columns in include in a secondary<br />index should have a significantly lower cardinality than the table being indexed and be aware of the<br />frequency of updates of the indexed value.</p><p>For the example access patterns above, we would define the following primary access entities:</p><ul><li>customer and transaction (get a list of transactions from the customer entity and then use that<br />to look up transaction details from the transaction entity)</li> <li>server-metric</li> <li>sensor</li> <li>sensor</li> </ul><h3>Allocate secondary entities</h3><p>The next step is to find a place to store the data that belongs to entities that have not been chosen as<br />primary access entities (I’ll call these entities secondary entities). You can choose to:</p><ul><li><strong> Push down</strong> by taking data from a parent secondary entity (one side) of a one to many<br />relationship and storing multiple copies of it at the primary access entity level (for example,<br />storing customer phone number in each customer order record); or</li> <li><strong>Push up</strong> by taking data from the child secondary entity (many side) of a one to many<br />relationship and storing it at the primary access entity level either by use of cluster keys or by<br />use of multi-value types (list and maps) (for example adding a list of line items to a transaction<br />level table).</li> </ul><p>For some secondary entities, there will only be one related primary access entity and so there is no<br />need to choose where and which direction to push. For other entities, you will need to choose will<br />need to choose which primary access entities to push the data into.</p><p>For optimal read performance, you should push a copy of the data to every primary access entity that<br />is used as an access path for the data in the secondary entity.</p><p>However, this comes at an insert/update performance and application complexity cost of maintaining<br />multiple copies the data. This trade-off between read performance and data maintenance cost needs<br />to be judged in the context of the specific performance requirements of your application.</p><p>The other decision to be made at this stage is between using a cluster key or a multi-value type for<br />pushing up. In general:</p><ul><li>Use a clustering key where there is only one child secondary entity to push up and particularly<br />where the child secondary entity itself has children to roll-up.</li> <li>Use multi-value types where there are multiple child entities to push up into the primary entity</li> </ul><p>Note that these rules are probably oversimplified but serve as a starting point for more detailed<br />consideration.</p><h2>Phase 3: Review &amp; Tune</h2><p>The last phase provides an opportunity to review the data model, test and to tune as necessary.</p><h3>Review partition &amp; cluster keys</h3><p>Entering this stage, you have all the data you need to store allocated to a table or tables and your<br />tables support accessing that data according to your required access patterns. The next step is to<br />check that the resulting data model makes efficient use of Cassandra and, if not, to adjust. The items<br />to check and adjust at this stage are:</p><ul><li><strong>Do your partition keys have sufficient cardinality?</strong> If not, it may be necessary to move<br />columns from the clustering key to the partition key (e.g. changing primary key (client_id,<br />timestamp) to primary key ((client_id, timestamp))) or introduce new columns which group<br />multiple cluster keys into partitions (e.g. changing primary key (client_id, timestamp) to<br />primary key ((client_id, day), timestamp).</li> <li><strong>Will the values in your partition keys be updated frequently?</strong> Updates of a primary key<br />value will result in deletion and re-insertion of the record which can result in issues with<br />tombstones. For example, trying to maintain a table with all clients of a particular status, you<br />might have primary key (status, client ID). However, this will result in a delete and re-insert<br />every time a client’s status changes. This would be a good candidate to use a set or list data<br />type rather than including client ID as the cluster key.</li> <li><strong>Is the number of records in each partition bounded?</strong> Extremely large partitions and/or very<br />unevenly sized partitions can cause issues. For example, if you have a client_updates table<br />with primary key (client_id, update_timestamp) there is potentially no limit to how many times<br />a particular client record can be update and you may have significant unevenness if you have a<br />small number of clients that have been around for 10 years and most clients only having<br />a day or two’s history. This is another example where it’s useful to introduce new columns<br />which group multiple cluster keys into partitions partitions (e.g. changing primary key (client_<br />id, update_timestamp) to primary key ((client_id, month), update_timestamp).</li> </ul><h3>Test and tune</h3><p>The final step is perhaps the most important – test your data model and tune it as required. Keep in<br />mind that issues like partitions or rows growing too large or tombstones building up in a table may<br />only become visible after days (or longer) of use under real-world load. It’s therefore important to test<br />as closely as possible to real-world load and to monitor closely for any warning signs (the nodetool<br />cfstats and cfhistograms commands are very useful for this).</p><p>At this stage you may also consider tuning some of the settings that effect the physical storage of your<br />data. For example:</p><ul><li>changing compaction strategy;</li> <li>reducing gc_grace_seconds if you are only deleting data using TTL; <strong>or</strong></li> <li>setting caching options.</li> </ul><h2>A Worked Example</h2><p>To illustrate this, I’ll walk through a basic example based on building a database to store and retrieve<br />log messages from multiple servers. Note this is quite simplified compared to most real-world<br />requirements.</p><h3>Step 1: Define the data domain</h3><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design.png\"><img class=\"aligncenter wp-image-6928 size-large\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-1024x616.png\" alt=\"Defining the data model domain Instaclustr Data Model design\" width=\"1024\" height=\"616\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-1024x616.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-300x180.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-768x462.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-966x581.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-640x385.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-80x48.png 80w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-180x108.png 180w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design.png 1576w\" /></a></p><p>The previous ER diagram illustrated the data domain. We have:</p><ul><li>Lots (millions) of log messages which have a timestamp and a body. Although message ID is<br />shown as the primary key in the ER diagram, message time plus message type is an alternate<br />primary key.</li> <li>Each log message has a message type and types are further grouped into a message category<br />(for example, a message type might be “out of memory error” and category might be “error”).<br />There a couple of hundred message types and around 20 categories.</li> <li>Each log message comes from a message source. The message source is the server that<br />generated the message. There are 1000s of servers in our system. Each message source has a<br />source type to categorise the source (e.g. red hat server, ubuntu server, windows server,<br />router, etc.). There are around 20 source types. There are ~10,000 messages per source per<br />day.</li> <li>The message body can be parsed and stored as multiple message parts (basically key, value<br />pairs). There is typically less than 20 parts per message.</li> </ul><h3>Step 2: Define the required access patterns</h3><p>We need to be able to:</p><ul><li>Retrieve all available information about the most recent 10 messages for a given source (and<br />be able to work back in time from there).</li> <li>Retrieve all available information about the most recent 10 message for a given source type.</li> </ul><h3>Step 3: Identify primary access entities</h3><p>There are two primary access entities here – source and source type. The cardinality (~20) of source<br />type makes it a good candidate for a secondary index so we will use source as the primary access<br />entity and add a secondary index for source type.</p><h3>Step 4: Allocate secondary entities</h3><p>In this example, this step is relatively simple as all data needs to roll into the log source primary access<br />entity. So we:</p><ul><li>Push down source type name</li> <li>Push down message category and message type to log message</li> <li>Push up log message as the clustering key for the new entity</li> <li>Push up message part as a map type with.</li> </ul><p>The end result is that would be a single table with a partition key of source ID and a clustering key of<br />(message time, message type).</p><h3>Step 5: Review partition and cluster keys</h3><p>Checking these partition and cluster keys against the checklist:</p><ul><li>Do your partition keys have sufficient cardinality? Yes, there are 1000s of sources.</li> <li>Will the values in your partition keys being updated frequently? No, all the data is write-once.</li> <li>Is the number of records in each partition bounded? No – messages could build up indefinitely over time.</li> </ul><p>So, we need to address the unbound partition size. A typical pattern to address that in time series<br />data such as this is to introduce a grouping of time periods into the cluster key. In this case 10,000<br />messages per day is a reasonable number to include in one partition so we’ll use day as part of our<br />partition key.</p><p>The resulting Cassandra table will look some like:</p><h2>Conclusion</h2><p>Hopefully, this process and basic example will help you start to get familiar with Cassandra data<br />modelling. We’ve only covered a basic implementation that fits well with Cassandra, however there<br />are many other examples on the web which can help you work through more complex requirements.<br />Instaclustr also provides our customers with data modelling review and assistance, so get in touch<br />with us if you need some hands-on assistance.</p>",
        "created_at": "2018-09-13T14:47:12+0000",
        "updated_at": "2018-09-13T14:48:58+0000",
        "published_at": "2017-10-05T12:22:11+0000",
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 11,
        "domain_name": "www.instaclustr.com",
        "preview_picture": "https://www.instaclustr.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-1024x616.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12146"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12145,
        "uid": null,
        "title": "Apache Cassandra Scalability: Allow Filtering and Partition Keys - Instaclustr",
        "url": "https://www.instaclustr.com/apache-cassandra-scalability-allow-filtering-partition-keys/",
        "content": "<p>The ‘ALLOW FILTERING’ clause in <a href=\"https://www.instaclustr.com/apache-cassandra/\">Cassandra</a>  CQL provides greatly increased flexibility of querying. However, this flexibility comes at a substantial performance cost that should be aware of before using ‘ALLOW FILTERING’.  This post explains the costs and benefits of ALLOW FILTERING.</p><p>The data storage and query operations in Cassandra work on top of partitioned data which is similar to most of other Big Data management systems. As most people would be familiar with from relational databases, each table definition has a primary key defined which can be a single attribute or combination of 2 or more. One or more attributes from the primary key are used to specify the ‘partition key’ for the table.</p><p>Example:</p><p>Here, the primary key is composed of (machine, cpu,mtime) out of which (machine, cpu) are used as partition key and (mtime) is used as clustering key.</p><p>The purpose of a partition key is to split the data into partitions where an entire partition is stored on a single node in the cluster (with each node storing many partitions). When data is read or written from the cluster, a function called <i>Partitioner</i> is used to compute the hash value of the partition key. This hash value is used to determine the node/partition which contains that row. The clustering key is used further to search for a row within a given partition. </p><p>Select queries in Apache Cassandra look a lot like select queries from a relational database. However, they are significantly more restricted. The attributes allowed in ‘where’ clause of Cassandra query must include the full partition key and additional clauses may only reference the clustering key columns or a secondary index of the table being queried. </p><p>Requiring the partition key attributes in the ‘where’ helps Cassandra to maintain constant result-set retrieval time as the cluster is scaled-out by allowing Cassandra to determine the partition, and thus the node (and even data files on disk), that the query must be directed to. </p><p>If a query does not specify the values for all the columns from the primary key  in the ‘where’ clause, Cassandra will not execute it and give the following warning :</p><p>‘<i>InvalidRequest: Error from server: code=2200 [Invalid query] message=”Cannot execute this query as it might involve data filtering and thus may have unpredictable performance. If you want to execute this query despite the performance unpredictability, use ALLOW FILTERING</i>” ‘</p><p>You can use execute queries that use a secondary index without ALLOW FILTERING – more on that later.</p><p>The reason behind this warning is that when the complete partition key is not included in the WHERE clause, there is no way for Cassandra to identify the node which contains the required results, and thus it will need to scan the complete dataset on each node to ensure it has found the required data. Hence, the query execution time is not proportional to the size of the result returned but rather to the total amount of data stored in the referenced table. One implication of this, other than initial poor performance, is that query performance is likely to get worse as your cluster scales (unlike partition key queries which exhibit constant response times while scaling)  </p><p>As I mentioned early, you can also execute queries that reference a secondary index without specifying a partition key value or using ALLOW FILTERING.</p><p>Let’s take an example to understand this:</p><p>Consider the following table holding user profiles with their year of birth (with a secondary index on it) and country of residence:</p><p><b>CREATE</b> <b>INDEX</b> <b>ON</b> <b>users</b>(birth_year);</p><p>Then the following queries are valid:</p><p><b>Query 1: SELECT</b> <b>*</b> <b>FROM</b> <b>users</b>;</p><p><b>Query 2: SELECT</b> <b>*</b> <b>FROM</b> <b>users</b> <b>WHERE</b> birth_year <b>=</b> 1981;</p><p>However, the following query will be rejected:</p><p><b>Query 3: SELECT</b> <b>*</b> <b>FROM</b> <b>users</b> <b>WHERE</b> birth_year <b>=</b> 1981 <b>AND</b> country <b>=</b> ‘AU’;</p><p>This is because Cassandra cannot guarantee that it won’t have to scan a large amount of data even if the result of the query is small. Typically, it will scan all the index entries for users born in 1981 even if only a handful are actually from Australia. However, if you “know what you are doing”, you can force the execution of this query by using ALLOW FILTERING and so the following query is valid:</p><p><b>Query 3 (with ALLOW FILTERING):SELECT * FROM users WHERE birth_year = 1981 AND country = ‘AU’ ALLOW FILTERING;</b></p><p>When using secondary indexes, it is important to understand that all three of these queries will need to hit every node in the cluster (or at least a complete set of replicas) in order to return results. In queries like this in Cassandra, one node in the cluster will act as the coordinator node, send out the query to the other nodes in the cluster that need to participate and collating the results to send back to the client. For Query 2, the coordinator will need to send a subquery  to each node in the cluster which in turn will look up the index for the data it stores and return matching rows to the coordinator which in turn returns them to the client. Query 3 is executed in much the same manner but the individual nodes not only look up the index and return data but also filter it before returning. </p><p>Thus for Query 3 the size of the result set is not related to the amount of data scanned and ALLOW FILTERING is required. However, both Query 2 and Query 3 require all nodes in the cluster to participate in the query and thus the amount of work required to complete the query increases as the cluster size grows and you will not see the linear horizontal scaling that Cassandra is famous for.</p><p>So, while ALLOW FILTERING should definitely be avoided, any query that does not specify a partition key should also be avoided to enable your Cassandra cluster to scale.</p>",
        "created_at": "2018-09-13T14:46:49+0000",
        "updated_at": "2018-09-13T14:47:01+0000",
        "published_at": "2017-10-31T13:54:18+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 4,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12145"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 931,
            "label": "data.engineering",
            "slug": "data-engineering"
          },
          {
            "id": 1351,
            "label": "migration",
            "slug": "migration"
          }
        ],
        "is_public": false,
        "id": 12144,
        "uid": null,
        "title": "Instaclustr: 7 easy steps to Cassandra cluster migration",
        "url": "https://www.computerweekly.com/blog/Open-Source-Insider/Instaclustr-7-easy-steps-to-Cassandra-cluster-migration",
        "content": "<p><em>This is a guest post for the Computer Weekly Open Source Insider blog written by <a href=\"https://www.linkedin.com/in/ben-slater-2720562/\">Ben Slater</a> in his capacity as chief product officer at Instaclustr.</em></p><p><em><a href=\"https://www.instaclustr.com/\">Instaclustr</a> positions itself as firm offering managed and supported solutions for Apache Cassandra, ScyllaDB, Elasticsearch, Apache Spark, Apache Zeppelin, Kibana and Apache Lucene. </em></p><p>Indeed, Instaclustr is known for its willingness to describe itself as a managed open source as a service company, if that expression actually exists.</p><p>The original title in full for this piece was: Migrating Your Cassandra Cluster – with Zero Downtime – in 7 Easy Steps.</p><p>Slater’s moves for writing this piece are (obviously) directed at companies who are looking to move a live Apache Cassandra deployment to a new location.</p><p>With this task in mind, it is (obviously) natural that these same companies will have some concerns, such as how you can keep Cassandra clusters 100% available throughout the process.</p><p>Arguing that if your application is able to remain online throughout connection setting changes, Slater says it can also remain fully available during this transition.</p><p><strong>NOTE:</strong> For extra protection and peace of mind, the following technique also includes a rapid rollback strategy to return to your original configuration, up until the moment the migration is completed.</p><p><strong>Slater writes as follows:</strong></p><p>Here’s a recommended 7-step Cassandra cluster migration order-of-operations that will avoid any downtime:</p><section class=\"section main-article-chapter\" data-menu-title=\"1) Get your existing environment ready\"><h3 class=\"section-title\"><i class=\"icon\" data-icon=\"1\">1) Get your existing environment ready</i></h3><p>First of all, make sure that your application is using a datacentre-aware load balancing policy, as well as LOCAL_*. Also, check that <u>all</u> of the keyspaces that will be copied over to the new cluster are set to use NetworkTopologyStrategy as their replication strategy. It’s also recommended that all keyspaces use this replication strategy when created, as altering this later can become complicated.</p></section><section class=\"section main-article-chapter\" data-menu-title=\"2) Create the new cluster\"><h3 class=\"section-title\"><i class=\"icon\" data-icon=\"1\">2) Create the new cluster</i></h3><p>Now it’s time to create the new cluster that you’ll be migrating to. A few things to be careful about here: be sure that the new cluster and the original cluster use the same Cassandra version and cluster name. Also, the new datacenter name that you use must be different from the name of the existing datacenter.</p></section><section class=\"section main-article-chapter\" data-menu-title=\"3) Join the clusters together\"><h3 class=\"section-title\"><i class=\"icon\" data-icon=\"1\">3) Join the clusters together</i></h3><p>To do this, first make any necessary firewall rule changes in order to allow the clusters to be joined, remembering that some changes to the source cluster may also be necessary. Then, change the new cluster’s seed nodes – and start them. Once this is done, the new cluster will be a second datacenter in the original cluster.</p></section><section class=\"section main-article-chapter\" data-menu-title=\"4) Change the replication settings\"><h3 class=\"section-title\"><i class=\"icon\" data-icon=\"1\">4) Change the replication settings<strong> </strong></i></h3><p>Next, in the existing cluster, update the replication settings for the keyspaces that will be copied, so that data will now be replicated with the new datacenter as the destination.</p></section><section class=\"section main-article-chapter\" data-menu-title=\"5) Copy the data to the new cluster\"><h3 class=\"section-title\"><i class=\"icon\" data-icon=\"1\">5) Copy the data to the new cluster</i></h3><p>When the clusters are joined together, Cassandra will begin to replicate writes to the new cluster. It’s still necessary, however, to copy any existing data over with the nodetool rebuild function. It’s a best practice to perform this function on the new cluster one or two nodes at a time, so as not to place an overwhelming streaming load on the existing cluster.</p></section><section class=\"section main-article-chapter\" data-menu-title=\"6) Change over the application’s connection points\"><h3 class=\"section-title\"><i class=\"icon\" data-icon=\"1\">6) Change over the application’s connection points</i></h3><p>After all uses of the rebuild function are completed, each of the clusters will contain a complete copy of the data being migrated, which Cassandra will keep in sync automatically. It’s now time to change the initial connection points of your application over to the nodes in the new cluster. Once this is completed, all reads and writes will be served by the new cluster, and will subsequently be replicated in the original cluster. Finally, it’s smart to run a repair function across the cluster, in order to ensure that all data has been replicated successfully from the original.<strong> </strong></p></section><section class=\"section main-article-chapter\" data-menu-title=\"7) Shut down the original cluster\"><h3 class=\"section-title\"><i class=\"icon\" data-icon=\"1\">7) Shut down the original cluster</i></h3><p>Complete the process with a little post-migration clean up, removing the original cluster. First, change the firewall rules to disconnect the original cluster from the new one. Then, update the replication settings in the new cluster to cease replication of data to the original cluster. Lastly, shut the original cluster down.</p><p>There you have it: your Apache Cassandra deployment has been fully migrated, with zero downtime, low risk and in a manner completely seamless and transparent from the perspective of your end users.</p><p><em>You can follow </em><em><a href=\"https://twitter.com/instaclustr?lang=en\">Instaclustr on Twitter</a>.</em></p><p><img class=\"wp-align alignnone size-full wp-image-1934\" src=\"https://itknowledgeexchange.techtarget.com/open-source-insider/files/2017/11/111P0AlzIUIAA0fc-.jpg\" alt=\"\" width=\"580\" height=\"441\" srcset=\"https://itknowledgeexchange.techtarget.com/open-source-insider/files/2017/11/111P0AlzIUIAA0fc-.jpg 580w, https://itknowledgeexchange.techtarget.com/open-source-insider/files/2017/11/111P0AlzIUIAA0fc--300x228.jpg 300w\" /></p></section>",
        "created_at": "2018-09-13T14:46:22+0000",
        "updated_at": "2018-09-13T14:46:33+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "www.computerweekly.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12144"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 23,
            "label": "elasticsearch",
            "slug": "elasticsearch"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 1122,
            "label": "zeppelin",
            "slug": "zeppelin"
          }
        ],
        "is_public": false,
        "id": 12143,
        "uid": null,
        "title": "Pick‘n’Mix: Cassandra, Spark, Zeppelin, Elassandra, Kibana, & Kafka - Instaclustr",
        "url": "https://www.instaclustr.com/picknmix-cassandra-spark-zeppelin-elassandra-kibana-kafka/",
        "content": "<h2><b>Kafkaesque</b>:</h2><h3> \\ käf-kə-ˈesk \\</h3><p><em>Marked by a senseless, disorienting, menacing, nightmarishly complexity.</em></p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafkaesque-comic.png\"><img class=\"alignnone size-full wp-image-8009\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafkaesque-comic.png\" alt=\"Kafkaesque Cassandra, Spark, Zeppelin, Elassandra, Kibana, &amp; Kafka\" width=\"734\" height=\"524\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafkaesque-comic.png 734w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafkaesque-comic-300x214.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafkaesque-comic-640x457.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafkaesque-comic-67x48.png 67w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafkaesque-comic-151x108.png 151w\" /></a></p><p><a href=\"https://www.gutenberg.org/files/5200/5200-h/5200-h.htm\" target=\"_blank\" rel=\"noopener\">One morning when I woke from troubled dreams</a>, I decided to blog about something potentially Kafkaesque: Which Instaclustr managed open-source-as-a-service(s) can be used together (current and future)? Which combinations are actually possible? Which ones are realistically sensible? And which are nightmarishly Kafkaesque!?</p><p>In previous blogs, I’ve explored Instaclustr managed <a href=\"https://www.instaclustr.com/apache-cassandra/\">Apache Cassandra</a>, <a href=\"https://www.instaclustr.com/apache-spark/\">Spark</a> (batch), Spark Streaming, Spark MLLib, and Zeppelin.</p><p>Instaclustr also supports managed <a href=\"https://www.instaclustr.com/solutions/managed-elasticsearch/\" target=\"_blank\" rel=\"noopener\">Elassandra</a> and <a href=\"https://www.instaclustr.com/fantasy-basketball-data-exploration/\" target=\"_blank\" rel=\"noopener\">Kibana</a>. Elassandra is an integrated <a href=\"https://github.com/elastic/elasticsearch\" target=\"_blank\" rel=\"noopener\">Elasticsearch</a> and Cassandra service which computes secondary indexes for data and supports fast queries over the indexes. Kibana is an open source data visualization plugin for Elasticsearch.  Together with <a href=\"https://github.com/elastic/logstash\" target=\"_blank\" rel=\"noopener\">Logstash</a> they form the “Elastic stack” (previously the ELK stack).</p><p><a href=\"https://www.instaclustr.com/apache-kafka/\">Apache Kafka</a>, a distributed streaming platform (massively horizontally scalable, high-throughput low-latency, high-reliability, high-availability real-time streaming data processing), is another popular service in the same Open Source ecosystem as Cassandra, Spark and Elasticsearch.  Kafka is on the Instaclustr product roadmap for 2018, and we have <a href=\"https://www.instaclustr.com/support/documentation/apache-spark/spark-streaming-kafka-and-cassandra-tutorial/\" target=\"_blank\" rel=\"noopener\">a tutorial on spark streaming with kafka and cassandra to wet your appetite. </a></p><p>Rather than jumping straight into a deep dive of Elassandra and/or Kafka I’m going to take a more architectural perspective. I started by putting all the services of interest on a diagram, and then connecting them together based on documented support for each integration combination and direction (source and/or sink):</p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-1.png\"><img class=\"alignnone size-full wp-image-8010\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-1.png\" alt=\"Kafka Architecture Diagram\" width=\"4349\" height=\"2240\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-1.png 4349w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-1-300x155.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-1-768x396.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-1-1024x527.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-1-966x498.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-1-640x330.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-1-93x48.png 93w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-1-210x108.png 210w\" /></a></p><p>Note that Cassandra, Elassandra, Spark (batch) and Spark Streaming, Spark MLLib, Zeppelin and Kibana are tightly integrated, and support most logically possible interactions. Instaclustr also co-locates all of these services on the same nodes by default. </p><p>I’ve spent some time examining the Kafka documentation to check what related ecosystem services it can connect to, and in what direction. Kafka supports Source and Sink Connectors which enable integration with numerous other services. Lots of different event sources are supported, enabling data to be ingested from both external and internal devices and systems. AWS S3 is supported as a Kafka sink only, and JDBC as both source and sink. Elassandra is supported as a sink only, and Spark Streaming and Cassandra as source and sink. </p><p>Also note that implicitly most services can “talk to” themselves (i.e. read data from, process data, and write data back. This is what the card replacement rule achieves). What’s more interesting is that they can also interact with themselves on the same or different <i>clusters</i>, and for the same or different <i>locations</i> (e.g. in another AWS AZ, or in another region). The diagram shows a Service interacting with itself (same cluster), another instance of the service in the same location (different cluster), and another instance in a different cluster and location (different location):</p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-2.png\"><img class=\"alignnone size-full wp-image-8011\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-2.png\" alt=\"Kafka Architecture Diagram\" width=\"3631\" height=\"1973\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-2.png 3631w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-2-300x163.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-2-768x417.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-2-1024x556.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-2-966x525.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-2-640x348.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-2-88x48.png 88w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-2-199x108.png 199w\" /></a></p><p>This opens up powerful internal service architectural richness and use cases. For example: Differentiation of clusters running the same service (e.g. write-intensive Cassandra cluster feeding data into a read-intensive cassandra cluster); A Kafka cluster dedicated to ingestion only, connecting to others for processing; mirroring or replicating data from one Cassandra cluster (location) to another (e.g. using Spark to read from a Cassandra cluster in one location and write to a Cassandra cluster in another location); Peer-to-Peer Kafka clusters, where each cluster subscribes to events that are local to all other Kafka clusters and aggregates the events locally), etc.</p><h2>Kafka – some key features</h2><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Some-key-features-of-Apache-Kafka.png\"><img class=\"alignnone size-full wp-image-8012\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Some-key-features-of-Apache-Kafka.png\" alt=\"Key features of Apache Kafka\" width=\"604\" height=\"513\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Some-key-features-of-Apache-Kafka.png 604w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Some-key-features-of-Apache-Kafka-300x255.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Some-key-features-of-Apache-Kafka-57x48.png 57w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Some-key-features-of-Apache-Kafka-127x108.png 127w\" /></a></p><p>The main Kafka APIs are Connectors, Producers, Consumers and Streams. Kafka is stream event-based, and producers publish events onto one or more topics. Topics are multi-subscriber and can have zero or more consumers that process events. Kafka maintains a (partitioned) immutable commit log of events for each topic, and therefore keeps all published events for a specified retention period. This approach for message processing has a number of benefits. The more obvious benefits are speed, fault-tolerance, high concurrency and scalability. The surprising benefits are that consumers and producers can be very loosely coupled, and events can be shared! More than one consumer can consume the same event, and consumers also control which events to consume – they can consume new events and also re-consume past events.</p><p>Kafka’s performance is claimed to be constant with respect to data size, so storing events for an arbitrary length of time (as long as you have disk space!) is encouraged, by design. Because events can be processed more than once, by the same or different consumers, what do we end up with? A database for streaming events!</p><p>Let’s explore some permutations of the ecosystem of services (not all permutations will be covered in this blog), starting with Kafka.  In answer to the question “What is Kafka good for?”, the Kafka documentation suggests two broad classes of application. The focus of this blog is on the first use case – getting (streaming) data between systems.</p><ol><li>Building real-time streaming data pipelines that reliably get data between systems or applications (this blog)</li> <li>Building real-time streaming applications that transform or react to the streams of data (next blog).</li> </ol><h2>Use Case: Kafka as a Database (Teenagers bedroom. Stuff goes in, stuff rarely comes out).</h2><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-database-Instaclustr.png\"><img class=\"alignnone size-full wp-image-8013\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-database-Instaclustr.png\" alt=\"Kafka as a database Instaclustr\" width=\"965\" height=\"447\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-database-Instaclustr.png 965w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-database-Instaclustr-300x139.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-database-Instaclustr-768x356.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-database-Instaclustr-640x296.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-database-Instaclustr-104x48.png 104w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-database-Instaclustr-233x108.png 233w\" /></a></p><p>Kafka only, one or more source connectors, producer(s) publishing to topic(s). No consumers:</p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-3.png\"><img class=\"alignnone size-full wp-image-8015\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-3.png\" alt=\"Kafka Architecture diagram\" width=\"2948\" height=\"1266\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-3.png 2948w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-3-300x129.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-3-768x330.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-3-1024x440.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-3-966x415.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-3-640x275.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-3-112x48.png 112w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-3-251x108.png 251w\" /></a></p><p>This is a trivial, and somewhat counterintuitive use case for Kafka but illustrates one of the surprising benefits of the architecture, that it is designed from the “bed” up as an event streaming <i>database</i> – not just for event movement. All the events arriving will be published to topic(s), and persisted to disk. Events can subsequently be consumed multiple times by multiple consumers, who do not have to be subscribed yet. Is this interesting? Yes! It suggests lots of powerful use cases around event persistence, and reprocessing/replaying of events, and adding derived events (e.g. failure handling, support for multiple consumers and purposes for DevOps to maintain derived stateful data back in Kafka for future use, as well as for processing events from past, present and future, including predictions, in a unified manner).</p><h2>Use Case: Kafka as a temporary Buffer (Doctors waiting room)</h2><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-temporary-Buffer.png\"><img class=\"alignnone size-full wp-image-8016\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-temporary-Buffer.png\" alt=\"Kafka as a temporary buffer\" width=\"798\" height=\"524\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-temporary-Buffer.png 798w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-temporary-Buffer-300x197.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-temporary-Buffer-768x504.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-temporary-Buffer-640x420.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-temporary-Buffer-73x48.png 73w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-temporary-Buffer-164x108.png 164w\" /></a></p><p>This pattern has one Kafka cluster feeding into another one:</p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-4.png\"><img class=\"alignnone size-full wp-image-8017\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-4.png\" alt=\"Kafka Architecture Diagram\" width=\"3326\" height=\"1266\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-4.png 3326w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-4-300x114.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-4-768x292.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-4-1024x390.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-4-966x368.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-4-640x244.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-4-126x48.png 126w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-4-284x108.png 284w\" /></a></p><p>This “Buffer” (waiting room) pattern has a Kafka cluster dedicated solely to event ingestion, and another cluster for the event consumers.  This leverages the ability of Kafka to store events indefinitely, and isolate event producers from consumers.  The event production and consumption rates can be significantly different with no loss of events or overloading of consumers. This pattern is ideal for use cases where an incoming event storm can temporarily exceed the processing capacity of the consumers cluster, or if there is some other temporary failure or slowdown preventing the consumers processing events in real-time. The Ingestion cluster buffers all the events until the consumers are ready to process them again. In the wild, this buffer pattern is used by <a href=\"https://medium.com/netflix-techblog/kafka-inside-keystone-pipeline-dd5aeabaf6bb\" target=\"_blank\" rel=\"noopener\">Netflix</a>.</p><p>Kafka can act as a event buffer, concentrator, and router in-front of other services in our ecosystem as well. For example, Cassandra, Spark streaming or Elassandra can all be sinks for Kafka events.</p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-5.png\"><img class=\"alignnone size-full wp-image-8032\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-5.png\" alt=\"Kafka Architecture Diagram 5\" width=\"3326\" height=\"1635\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-5.png 3326w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-5-300x147.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-5-768x378.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-5-1024x503.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-5-966x475.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-5-640x315.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-5-98x48.png 98w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-5-220x108.png 220w\" /></a></p><h2>Use Case: Kafka active-passive replication</h2><p>In the Use Cases so far we’ve only used Kafka as a pass-through buffer or longer term persistence mechanism. Kafka producers and consumers can publish/subscribe to/from multiple topics, enabling more complex topologies to be created. In particular, some less obvious patterns can be used to support <a href=\"https://www.slideshare.net/GuozhangWang/building-stream-infrastructure-across-multiple-data-centers-with-apache-kafka\">data replication across multiple Kafka clusters and locations</a>.  </p><p>There are a couple of use cases for data replication across clusters/locations. One is for reliability/redundancy and is often called active-passive replication. Data from the source (active) cluster is copied to the passive (target) cluster.  The “passive” cluster can be used in case of failure of the active cluster, or it can be used to reduce latency for consumers that are geo-located near it.</p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-6.png\"><img class=\"alignnone size-full wp-image-8072\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-6.png\" alt=\"Kafka Architecture Diagram\" width=\"3905\" height=\"1519\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-6.png 3905w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-6-300x117.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-6-768x299.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-6-1024x398.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-6-966x376.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-6-640x249.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-6-123x48.png 123w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-6-278x108.png 278w\" /></a></p><h2>Use Case: Kafka active-active replication</h2><p>A more interesting use case is when unique events are collected at different locations, and must be shared among all the locations. This can be between just two locations, or many (P2P). This is an active-active pattern and can be viewed as a generalisation of the active-passive pattern as each cluster acts as both a source and a target for every other cluster, and the events copied from other clusters need to be merged with the events from the local cluster in a new topic (Topic 2 in the diagram below), from which consumers can get all the events. Note that it has to be different topic otherwise you get an event loop!</p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-7.png\"><img class=\"alignnone size-full wp-image-8073\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-7.png\" alt=\"Kafka Architecture Diagram\" width=\"5012\" height=\"1632\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-7.png 5012w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-7-300x98.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-7-768x250.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-7-1024x333.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-7-966x315.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-7-640x208.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-7-147x48.png 147w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-7-332x108.png 332w\" /></a></p><h2>(Magic) Mirror Maker</h2><p><a href=\"http://www.kyotojournal.org/renewal/the-magic-mirror-maker/\"><i>In Japan, bronze mirrors are known as magic mirrors, or makkyo</i></a><i> (魔鏡). One side is brightly polished, while an embossed design decorates the reverse side. Remarkably, when light is directed onto the face of the mirror, and reflected to a flat surface, an image “magically” </i><i>appears (usually the one featured on its back):</i></p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.40.38-am.png\"><img class=\"alignnone size-full wp-image-8074\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.40.38-am.png\" alt=\"Magic Mirror Maker Makkyo\" width=\"2004\" height=\"1250\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.40.38-am.png 2004w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.40.38-am-300x187.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.40.38-am-768x479.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.40.38-am-1024x639.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.40.38-am-966x603.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.40.38-am-640x399.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.40.38-am-77x48.png 77w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.40.38-am-173x108.png 173w\" /></a></p><p>For the use cases involving events being moved between Kafka clusters, how can this be achieved? One obvious mechanism is just to pretend that the clusters are “local”, and read data from the source cluster topic with a consumer and publish it to another topic on the target cluster. This approach can work with low-latency WANs (e.g. clusters on the same AWS AZ). However, there are also a number of more sophisticated solutions. <a href=\"https://kafka.apache.org/documentation.html#basic_ops_mirror_maker\">Mirror maker</a> can be used (which also just reads data from the source cluster using a consumer and publishes it to the target cluster using a producer!). Will mirror maker actually work for the active-active use case given that mirror maker can only read/write to/from topics of the same name? <a href=\"https://www.linkedin.com/pulse/2-way-replication-apache-kafka-marcio-andrada\">Maybe, here’s a trick.</a> More sophisticated solutions exist, including uReplicator from Uber.</p><ul><li><a href=\"https://eng.uber.com/ureplicator/\">https://eng.uber.com/ureplicator/</a></li> <li><a href=\"https://github.com/uber/uReplicator\">https://github.com/uber/uReplicator</a></li> <li><a href=\"https://github.com/uber/uReplicator/wiki/uReplicator-Design\">https://github.com/uber/uReplicator/wiki/uReplicator-Design</a></li> </ul><h2>Next blog:</h2><h4>What’s the difference between Spark and Kafka Streaming? Event reprocessing/replaying, unifying stream and batch processing, producing and using state, fun with time, large messages, topic discovery, and more!</h4><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.56.25-am.png\"><img class=\"alignnone size-full wp-image-8078\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.56.25-am.png\" alt=\"Kafka Comic\" width=\"1060\" height=\"1332\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.56.25-am.png 1060w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.56.25-am-239x300.png 239w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.56.25-am-768x965.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.56.25-am-815x1024.png 815w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.56.25-am-955x1200.png 955w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.56.25-am-490x616.png 490w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.56.25-am-509x640.png 509w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.56.25-am-38x48.png 38w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.56.25-am-86x108.png 86w\" /></a></p><aside class=\"content-cta\"><div class=\"primary\"><h4>Related Articles:</h4></div></aside>",
        "created_at": "2018-09-13T14:45:33+0000",
        "updated_at": "2018-09-13T14:45:51+0000",
        "published_at": "2017-12-05T12:26:07+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 8,
        "domain_name": "www.instaclustr.com",
        "preview_picture": "https://www.instaclustr.com/wp-content/uploads/2017/11/Kafkaesque-comic.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12143"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1333,
            "label": "shell",
            "slug": "shell"
          }
        ],
        "is_public": false,
        "id": 12094,
        "uid": null,
        "title": "eevans/cdsh",
        "url": "https://github.com/eevans/cdsh",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>A simple wrapper script around <a href=\"https://www.netfort.gr.jp/%7Edancer/software/dsh.html.en\" rel=\"nofollow\">DSH</a>\nfor <a href=\"http://cassandra.apache.org\" rel=\"nofollow\">Apache Cassandra</a>.</p>\n<h2><a id=\"user-content-description\" class=\"anchor\" aria-hidden=\"true\" href=\"#description\"></a>Description</h2>\n<p><code>dsh</code> restricts you to flat groups lists, <code>cdsh</code> wraps <code>dsh</code> to allow you to define your\nclusters in a single YAML file (<code>~/.cdsh</code> by default), and then specify the hosts for remote\ncommands using arguments for cluster name, data-center, and rack.</p>\n<h2><a id=\"user-content-usage\" class=\"anchor\" aria-hidden=\"true\" href=\"#usage\"></a>Usage</h2>\n<pre>usage: cdsh [-h] [-c CLUSTER] [-d DATA_CENTER] [-r RACK] [--config CONFIG]\n            [-P]\n            [args [args ...]]\nA dsh wrapper for Cassandra\npositional arguments:\n  args                  arguments to dsh\noptional arguments:\n  -h, --help            show this help message and exit\n  -c CLUSTER, --cluster CLUSTER\n                        cluster name\n  -d DATA_CENTER, --data-center DATA_CENTER\n                        data-center name\n  -r RACK, --rack RACK  rack name(s)\n  --config CONFIG       yaml configuration file\n  -P, --print-hosts     output matching hosts (no command run)\n  -e EXCLUDES, --exclude EXCLUDES\n                        hosts to exclude (glob)\n</pre>\n<h2><a id=\"user-content-examples\" class=\"anchor\" aria-hidden=\"true\" href=\"#examples\"></a>Examples</h2>\n<p>Executing a command on all nodes of cluster <code>default</code>:</p>\n<pre>$ cdsh -c default -- uname -r\n</pre>\n<p>Executing a command on <code>rack1</code> of cluster <code>test</code> in data-center <code>datacenter1</code>:</p>\n<pre>$ cdsh -c test -d datacenter1 -r rack1 -- nodetool setstreamthroughput 200\n</pre>\n<p>Using the host list for other commands:</p>\n<pre>$ for i in `cdsh -c default -P`; do rsync cassandra.yaml $i:/etc/cassandra; done\n</pre>\n</article>",
        "created_at": "2018-09-07T20:25:17+0000",
        "updated_at": "2018-09-07T20:25:22+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/64348?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12094"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1262,
            "label": "time.series",
            "slug": "time-series"
          }
        ],
        "is_public": false,
        "id": 12092,
        "uid": null,
        "title": "Advanced Time Series Data Modelling",
        "url": "https://www.datastax.com/dev/blog/advanced-time-series-data-modelling",
        "content": "<h2><a href=\"https://www.datastax.com/wp-content/uploads/2015/12/nest.jpg\"><img class=\"alignnone size-full wp-image-43180\" src=\"https://www.datastax.com/wp-content/uploads/2015/12/nest.jpg\" alt=\"nest\" srcset=\"https://www.datastax.com/wp-content/uploads/2015/12/nest.jpg 640w, https://www.datastax.com/wp-content/uploads/2015/12/nest-250x125.jpg 250w, https://www.datastax.com/wp-content/uploads/2015/12/nest-120x60.jpg 120w\" /></a></h2><h2>Collecting Time Series Vs Storing Time Series</h2><p>Cassandra is well known as the database of choice when collecting time series events. These may be messages, events or similar transactions that have a time element to them. If you are not familiar on how Cassandra holds time series, there is a useful data modelling tutorial on the DataStax academy website.</p><p><a href=\"https://academy.datastax.com/demos/getting-started-time-series-data-modeling\" target=\"_blank\">https://academy.datastax.com/demos/getting-started-time-series-data-modeling</a></p><p>In this document I will try to explain some of the pros and cons of using time series in Cassandra and show some techniques and tips which make make your application better not just for now but also 5 years down the line.</p><p><strong>Choosing you long term storage</strong></p><p>Choosing your long term storage is not really a trivial thing. In most applications there are business requirements about how long data will need to be held for and sometimes these requirements change. More and more, business want and are required to hold data for longer. For example, a lot of financial companies must keep audit data for up to seven years.</p><p><strong>Using some sample applications.</strong></p><p>We will look at some examples and see how time series is used for each.</p><p>1. A credit card account which shows transaction for a particular account number. Data is streamed in real time.</p><p>2. Collecting energy data for a smart meter. Data comes from files sent from devices after one day of activity.</p><p>3. Tick data for a financial instrument. Data is streamed in real time.</p><p>All of the above use cases are time series examples and would benefit from using Cassandra. But when we look at the queries and retention policies for this data we may look at different ways of storing them.</p><p><strong>Clustering columns for time series.</strong></p><p>The credit card application will need to query a users transactions and show them to the user. They will need to be in descending order with the latest transaction first. The data may be paged over multiple pages. This data needs to be kept for 7 years.</p><p>Using a simply clustering column in the table definition, will allow all the transactions for a particular account to be on one row for extremely fast retrieval.</p><p>Our table model would be similar to this</p><pre>create table if not exists latest_transactions(&#13;\n credit_card_no text,&#13;\n transaction_time timestamp,&#13;\n transaction_id text,&#13;\n user_id text,&#13;\n location text,&#13;\n items map&lt;text, double&gt;,&#13;\n merchant text,&#13;\n amount double,&#13;\n status text,&#13;\n notes text,&#13;\n PRIMARY KEY (credit_card_no, transaction_time)&#13;\n) WITH CLUSTERING ORDER BY ( transaction_time desc);</pre><p>The smart meter application is a little different. The data will come in for each meter no with data every 30 mins of increments to the meter value. Eg. 00:00 - 13, 00:30 11, 01:00 3......23:30 10. So the daily amount is an aggregation of all the data points together.</p><p>The business requirement state that the data must be held for 5 years and a days data will always be looked up together. Cassandra's has column type of Map which can be used to hold our daily readings in a format of time offset and value.</p><p>Our table model would look something like this</p><pre>create table if not exists smart_meter_reading (&#13;\n meter_id int,&#13;\n date timestamp,&#13;\n source_id text,&#13;\n readings map&lt;text, double&gt;,&#13;\n PRIMARY KEY(meter_id, date)&#13;\n) WITH CLUSTERING ORDER BY(date desc);</pre><p>This seems sensible until we look at how much data we will be holding and for how long. This application has 10 Million meters and hopes to double that over the next 3 years. If we start at 10M customers holding 365*5 years of data which 48 columns of offset data per day (every half hour), this can quickly add up to over 50o billion points (a map of 48 entries is held as 48 columns) and we haven't talked about the increase over those years. Since we don't have to query the reading individually it might suit better to look at other storage capabilities. A map can be simply transformed to and from a JSON string which would allow us to hold the same data but not have the over head of all the columns.</p><pre>create table if not exists smart_meter_reading (&#13;\n meter_id int,&#13;\n date timestamp,&#13;\n source_id text,&#13;\n readings text,&#13;\n PRIMARY KEY(meter_id, date)&#13;\n) WITH CLUSTERING ORDER BY(date desc);</pre><p>So instead of 500 billion points we now have 5 billion.</p><p>Now we finally look at application no 3. In this case we have data streaming to our application for thousands of different instruments. We can expect on 100,000 ticks a day on some of the instruments. If the requirement is to hold this data long term and be able to create different views of the data for charting capabilities, the storage requirement will be extremely large.</p><p><strong>Collecting data vs Storing data.</strong></p><p>We can collect the data in the traditional way using a clustering column with a table like so</p><pre>CREATE TABLE tick_data ( &#13;\n symbol text,&#13;\n date timestamp,&#13;\n time_of_day timestamp,&#13;\n value double,&#13;\n PRIMARY KEY ((symbol,time_of_day), date)&#13;\n) WITH CLUSTERING ORDER BY (date DESC);</pre><p>When we think of keeping this data long term we have to understand the implications of having billions of columns in our tables. Our normal queries will be charting the last 5 days of instrument data in 15 min intervals or show the open, high, low and close of an instrument for the last 10 days in 30 mins intervals. So 99% of the queries will be looking at the whole days data. In this example we can then change the long term storage for this table to create a second table which handles any requests for data that is not today. At the end of each day we can compress and the store the data more efficiently for the rest of its life in the database.</p><p>For example we can use the following</p><pre>CREATE TABLE tick_data_binary ( &#13;\n symbol text,&#13;\n date timestamp;&#13;\n dates blob,&#13;\n ticks blob,&#13;\n PRIMARY KEY ((symbol,date))&#13;\n);</pre><p>Inserting into the tick_data_binary table can sustain inserts and reads of around 5 million ticks per server,  compared to 25000 for the tick_data table. The tick_data_binary table is also three times less storage that the tick_data table. This is not surprising as instead of holding 100,000 TTLs for all the columns, in the binary example we only hold 1. But there are bigger advantages when it comes to Cassandra's management services like compaction and repair. Compaction needs to be able to search for tombstones(deleted columns) which means that the more columns we have, the longer compaction can take. A similar problem arises in repair as this is in fact a compaction job. Comparing the repair time of a table with clustering columns and a table with binary data shows an increase of 10 times for the clustering table over the binary table.</p><h2>Trade offs</h2><p>There are always trade offs to each of the models above. For example the binary data in particular can't be filtered using CQL, the filtering needs to happen in some code. This post isn't supposed to be a catch all for time series applications but it it is supposed to help with the modelling of your data, both current and future, and the thought process that goes into that. In particular, don't be afraid to change the data model structure once its usefulness has decreased.</p><p>Check out <a href=\"https://academy.datastax.com/tutorials\">https://academy.datastax.com/tutorials</a> for more information of Cassandra and data modelling. Also have a look at the certification options that you can achieve <a href=\"https://academy.datastax.com/certifications\">https://academy.datastax.com/certifications</a>.</p><p>For examples of this data models and see the github projects below.</p><p><a href=\"https://github.com/DataStaxCodeSamples/datastax-creditcard-demo\" target=\"_blank\">https://github.com/DataStaxCodeSamples/datastax-creditcard-demo</a></p><p><a href=\"https://github.com/DataStaxCodeSamples/datastax-iot-demo\" target=\"_blank\">https://github.com/DataStaxCodeSamples/datastax-iot-demo</a></p><p><a href=\"https://github.com/DataStaxCodeSamples/datastax-tickdata-comparison\" target=\"_blank\">https://github.com/DataStaxCodeSamples/datastax-tickdata-comparison</a></p><p><a href=\"https://github.com/DataStaxCodeSamples/datastax-tickdb-full\" target=\"_blank\">https://github.com/DataStaxCodeSamples/datastax-tickdb-full</a></p><hr /><p><a href=\"https://www.datastax.com/\">DataStax</a> has many ways for you to advance in your career and knowledge. \n</p><p>You can take <a href=\"https://academy.datastax.com/user/register?destination=home&amp;utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_register\" target=\"_self\" title=\"academy.datastax.com\">free classes</a>, <a href=\"https://academy.datastax.com/certifications?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_certifications\" target=\"_self\" title=\"academy.datastax.com/certifications\">get certified</a>, or read <a href=\"https://www.datastax.com/dbas-guide-to-nosql\" target=\"_self\" title=\"dbas-guide-to-nosql\">one of our many white papers</a>.\n</p><p><a href=\"https://academy.datastax.com/user/register?destination=home&amp;utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_register\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"academy.datastax.com\">register for classes</a>\n</p><p><a href=\"https://academy.datastax.com/certifications?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_certifications\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"academy.datastax.com/certifications\">get certified</a>\n</p><p><a href=\"http://www.datastax.com/dbas-guide-to-nosql?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_dbasguidetonosql\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"dbas-guide-to-nosql\">DBA's Guide to NoSQL</a>\n</p><br class=\"clear\" /><div id=\"mto_newsletter_121316_Css\"><p>Subscribe for newsletter:</p><br /></div>",
        "created_at": "2018-09-07T15:31:24+0000",
        "updated_at": "2018-09-07T15:31:44+0000",
        "published_at": "2015-12-15T17:09:39+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 6,
        "domain_name": "www.datastax.com",
        "preview_picture": "https://www.datastax.com/wp-content/uploads/2015/12/nest1.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12092"
          }
        }
      }
    ]
  }
}