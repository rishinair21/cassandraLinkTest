{
  "page": 2,
  "limit": 200,
  "pages": 3,
  "total": 465,
  "_links": {
    "self": {
      "href": "http://leaves.anant.us:82/api/entries?sort=created&order=desc&tags=cassandra&since=0&page=2&perPage=200"
    },
    "first": {
      "href": "http://leaves.anant.us:82/api/entries?sort=created&order=desc&tags=cassandra&since=0&page=1&perPage=200"
    },
    "last": {
      "href": "http://leaves.anant.us:82/api/entries?sort=created&order=desc&tags=cassandra&since=0&page=3&perPage=200"
    },
    "next": {
      "href": "http://leaves.anant.us:82/api/entries?sort=created&order=desc&tags=cassandra&since=0&page=3&perPage=200"
    },
    "previous": {
      "href": "http://leaves.anant.us:82/api/entries?sort=created&order=desc&tags=cassandra&since=0&page=1&perPage=200"
    }
  },
  "_embedded": {
    "items": [
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 16,
            "label": "starter.template",
            "slug": "starter.template"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          }
        ],
        "is_public": false,
        "id": 11798,
        "uid": null,
        "title": "instaclustr/sample-KafkaSparkCassandra",
        "url": "https://github.com/instaclustr/sample-KafkaSparkCassandra",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>Introductory sample scala app using Apache Spark Streaming to accept data from Kafka and write a summary to Cassandra.</p>\n<p>This sample has been built with the following versions:</p>\n<ul><li>Scala 2.11.8</li>\n<li>Kafka 1.1</li>\n<li>Spark 2.1.1</li>\n<li>Spark Cassandra Connector 2.3.0</li>\n<li>Cassandra 3.11.2</li>\n</ul><p>For a detailed, step by step guide on setting up and running this sample see the tutorial: TBA</p>\n</article>",
        "created_at": "2018-08-03T00:13:49+0000",
        "updated_at": "2018-08-03T00:13:58+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/11550580?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11798"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 996,
            "label": "monitoring",
            "slug": "monitoring"
          },
          {
            "id": 1275,
            "label": "diagnostics",
            "slug": "diagnostics"
          }
        ],
        "is_public": false,
        "id": 11797,
        "uid": null,
        "title": "smartcat-labs/cassandra-diagnostics",
        "url": "https://github.com/smartcat-labs/cassandra-diagnostics",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>Monitoring and audit power kit for Apache Cassandra.</p>\n<p><a href=\"https://travis-ci.org/smartcat-labs/cassandra-diagnostics\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/76a416d18123babf572d1379913c56645af9672f/68747470733a2f2f7472617669732d63692e6f72672f736d6172746361742d6c6162732f63617373616e6472612d646961676e6f73746963732e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/smartcat-labs/cassandra-diagnostics.svg?branch=master\" /></a>\n<a href=\"https://maven-badges.herokuapp.com/maven-central/io.smartcat/cassandra-diagnostics/\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/b2f836e75fad32d5b5e32102a53017aae8c71a64/68747470733a2f2f6d6176656e2d6261646765732e6865726f6b756170702e636f6d2f6d6176656e2d63656e7472616c2f696f2e736d6172746361742f63617373616e6472612d646961676e6f73746963732f62616467652e737667\" alt=\"Maven Central\" data-canonical-src=\"https://maven-badges.herokuapp.com/maven-central/io.smartcat/cassandra-diagnostics/badge.svg\" /></a>\n<a href=\"https://bintray.com/smartcat-labs/maven/cassandra-diagnostics/_latestVersion\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/c32ac0c09babdcfc567b7c56d8980e56ce5833f8/68747470733a2f2f6170692e62696e747261792e636f6d2f7061636b616765732f736d6172746361742d6c6162732f6d6176656e2f63617373616e6472612d646961676e6f73746963732f696d616765732f646f776e6c6f61642e737667\" alt=\"Download\" data-canonical-src=\"https://api.bintray.com/packages/smartcat-labs/maven/cassandra-diagnostics/images/download.svg\" /></a></p>\n<h2><a id=\"user-content-introduction\" class=\"anchor\" aria-hidden=\"true\" href=\"#introduction\"></a>Introduction</h2>\n<p>Cassandra Diagnostics is an extension for Apache Cassandra server node implemented as Java agent. It uses bytecode instrumentation to augment Cassandra node with additional functionalities. The following images depicts the position of Cassandra Diagnostics in a Apache Cassandra based system.</p>\n<p><a target=\"_blank\" href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/diagrams/cassandra-diagnostics.png?raw=true\"><img src=\"https://github.com/smartcat-labs/cassandra-diagnostics/raw/dev/diagrams/cassandra-diagnostics.png?raw=true\" alt=\"Placement diagram\" /></a></p>\n<p>Cassandra Diagnostics has a modular architecture. On one side it has connectors for different versions of Apache Cassandra nodes or Cassandra Java Driver and on the other it has various reporters to send measurement to different collecting/monitoring tools. In between lies the core with a set of metrics processing modules. Reusable code goes to commons.</p>\n<p><a target=\"_blank\" href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/diagrams/architecture-diagram.png?raw=true\"><img src=\"https://github.com/smartcat-labs/cassandra-diagnostics/raw/dev/diagrams/architecture-diagram.png?raw=true\" alt=\"Architecture diagram\" /></a></p>\n<h3><a id=\"user-content-cassandra-diagnostic-commons\" class=\"anchor\" aria-hidden=\"true\" href=\"#cassandra-diagnostic-commons\"></a>Cassandra Diagnostic Commons</h3>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-commons\">Cassandra Diagnostics Commons</a> holds interface for core, connector and reports and it provides signature all the modules need to confront to be able to work together.</p>\n<h3><a id=\"user-content-cassandra-connector\" class=\"anchor\" aria-hidden=\"true\" href=\"#cassandra-connector\"></a>Cassandra Connector</h3>\n<p>Connector is a module which hooks into the query path and extract information for diagnostics. Bytecode instrumentation is used to augment existing Cassandra code with additional functionality. It uses low priority threads to execute the diagnostics information extraction with minimal performance impact to the target code (Cassandra node or application/driver).</p>\n<p>Currently Cassandra Diagnostics implements the following connector implementation:</p>\n<ul><li>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-connector21\">Cassandra Connector 2.1</a> is a connector implementation for Cassandra node for Cassandra version 2.1.x.</p>\n</li>\n<li>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-connector30\">Cassandra Connector 3.0</a> is a connector implementation for Cassandra node for Cassandra version 3.0.x.</p>\n</li>\n<li>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-driver-connector\">Cassandra Driver Connector</a> is a connector implementation for Datastax's Cassandra driver for diagnostics on the application side.</p>\n</li>\n</ul><h3><a id=\"user-content-cassandra-core\" class=\"anchor\" aria-hidden=\"true\" href=\"#cassandra-core\"></a>Cassandra Core</h3>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-core\">Cassandra Diagnostics Core</a> is glue between connector and reporters. It holds all the modules for diagnostics, it has business logic for measurement and it decides what will be measured and what would be skipped. Its job is to load provided configuration or to setup sensible defaults.</p>\n<h3><a id=\"user-content-modules\" class=\"anchor\" aria-hidden=\"true\" href=\"#modules\"></a>Modules</h3>\n<p>There are default module implementations which serve as core features. Modules use configured reporters to report their activity.</p>\n<p>Please read <a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-core/COREMODULES.md\">core modules README</a> for more information and configuraion options for the modules.Core module implementations:</p>\n<h4><a id=\"user-content-heartbeat-module\" class=\"anchor\" aria-hidden=\"true\" href=\"#heartbeat-module\"></a>Heartbeat Module</h4>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-core/COREMODULES.md#heartbeat-module\">Heartbeat Module</a> produces messages to provide feedback that the diagnostics agent is loaded and working. Typical usage is with Log Reporter where it produces INFO message in configured intervals.\nDefault reporting interval is 15 minutes.</p>\n<h4><a id=\"user-content-slow-query-module\" class=\"anchor\" aria-hidden=\"true\" href=\"#slow-query-module\"></a>Slow Query Module</h4>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-core/COREMODULES.md#slow-query-module\">Slow Query Module</a> is monitoring execution time of each query and if it is above configured threshold it reports the value and query type using configured reporters.\nDefault query execution time threshold is 25 milliseconds.</p>\n<h4><a id=\"user-content-request-rate-module\" class=\"anchor\" aria-hidden=\"true\" href=\"#request-rate-module\"></a>Request Rate Module</h4>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-core/COREMODULES.md#request-rate-module\">Request Rate Module</a> uses codahale metrics library to create rate measurement of executed queries. Rates are reported for configurable statement types and consistency levels using configured reporters in configured periods.\nDefault reporting interval is 1 second.</p>\n<h4><a id=\"user-content-metrics-module\" class=\"anchor\" aria-hidden=\"true\" href=\"#metrics-module\"></a>Metrics Module</h4>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-core/COREMODULES.md#metrics-module\">Metrics Module</a> collects Cassandra's metrics, which are exposed over JMX, and ships them using predefined reporters. Metrics package names configuration is the same as a default metrics config reporter uses.\nDefault reporting interval is 1 second.</p>\n<h4><a id=\"user-content-status-module\" class=\"anchor\" aria-hidden=\"true\" href=\"#status-module\"></a>Status Module</h4>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-core/COREMODULES.md#status-module\">Status Module</a> is used to report Cassandra information exposed over JMX. It reports compaction information as a single measurement.\nDefault reporting interval is 1 minute.</p>\n<h4><a id=\"user-content-cluster-health-module\" class=\"anchor\" aria-hidden=\"true\" href=\"#cluster-health-module\"></a>Cluster Health Module</h4>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-core/COREMODULES.md#cluster-health-module\">Cluster Health Module</a> is used to report the health status of the nodes such as which nodes are marked as DOWN by gossiper. It uses the information exposed over JMX.\nDefault reporting interval is 10 seconds.</p>\n<h4><a id=\"user-content-hiccup-module\" class=\"anchor\" aria-hidden=\"true\" href=\"#hiccup-module\"></a>Hiccup Module</h4>\n<p>Module based on <a href=\"https://github.com/giltene/jHiccup\">jHiccup</a> that logs and reports platform hiccups including JVM stalls. Default reporting period is 5 seconds and reporter values and percentiles from 90 to 100 and Mean and Max values.</p>\n<h3><a id=\"user-content-reporters\" class=\"anchor\" aria-hidden=\"true\" href=\"#reporters\"></a>Reporters</h3>\n<p>Reporters take measurement from core and wrap them up in implementation specific format so it can be sent to reporters target (i.e. Influx reporter transforms measurement to influx query and stores it to InfluxDB).</p>\n<p>Reporter implementations:</p>\n<h4><a id=\"user-content-log-reporter\" class=\"anchor\" aria-hidden=\"true\" href=\"#log-reporter\"></a>Log Reporter</h4>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-core/src/main/java/io/smartcat/cassandra/diagnostics/reporter/LogReporter.java\">LogReporter</a> uses the Cassandra logger system to report measurement (this is default reporter and part of core). Reports are logged at the <code>INFO</code> log level in the following pattern:</p>\n<pre>Measurement {} [time={}, value={}, tags={}, fields={}]\n</pre>\n<p>Values for <code>time</code> is given in milliseconds. <code>tags</code> are used to better specify measurement and provide additional searchable labels and fields is a placeholder for additional fields connected to this measurement. Example can be Slow Query measurement, where <code>value</code> is execution time of query, <code>tags</code> can be type of statement (UPDATE or SELECT) so you can differentiate and search easy and <code>fields</code> can hold actual statement, which is not something you want to search against but it is valuable metadata for measurement.</p>\n<h4><a id=\"user-content-riemann-reporter\" class=\"anchor\" aria-hidden=\"true\" href=\"#riemann-reporter\"></a>Riemann Reporter</h4>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-reporter-riemann/README.md\">RiemannReporter</a> sends measurements towards <a href=\"http://riemann.io/\" rel=\"nofollow\">Riemann server</a>.</p>\n<h4><a id=\"user-content-influx-reporter\" class=\"anchor\" aria-hidden=\"true\" href=\"#influx-reporter\"></a>Influx Reporter</h4>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-reporter-influx/README.md\">InfluxReporter</a> sends measurements towards <a href=\"https://www.influxdata.com/time-series-platform/influxdb/\" rel=\"nofollow\">Influx database</a>.</p>\n<h4><a id=\"user-content-telegraf-reporter\" class=\"anchor\" aria-hidden=\"true\" href=\"#telegraf-reporter\"></a>Telegraf Reporter</h4>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-reporter-telegraf/README.md\">Telegraf Reporter</a> sends measurements towards <a href=\"https://github.com/influxdata/telegraf\">Telegraf agent</a>.</p>\n<h4><a id=\"user-content-datadog-reporter\" class=\"anchor\" aria-hidden=\"true\" href=\"#datadog-reporter\"></a>Datadog Reporter</h4>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-reporter-datadog/README.md\">Datadog Reporter</a> sends measurements towards <a href=\"https://github.com/DataDog/dd-agent\">Datadog Agent</a> using UDP.</p>\n<h4><a id=\"user-content-kafka-reporter\" class=\"anchor\" aria-hidden=\"true\" href=\"#kafka-reporter\"></a>Kafka Reporter</h4>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-reporter-kafka/README.md\">Kafka Reporter</a> sends measurements towards <a href=\"https://kafka.apache.org/\" rel=\"nofollow\">Kafka</a>.</p>\n<h4><a id=\"user-content-prometheus-reporter\" class=\"anchor\" aria-hidden=\"true\" href=\"#prometheus-reporter\"></a>Prometheus Reporter</h4>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-reporter-prometheus/README.md\">Prometheus Reporter</a> exposes measurements to be scraped by <a href=\"https://prometheus.io\" rel=\"nofollow\">Prometheus server</a>.</p>\n<h2><a id=\"user-content-configuration\" class=\"anchor\" aria-hidden=\"true\" href=\"#configuration\"></a>Configuration</h2>\n<p>Cassandra Diagnostics uses an external configuration file in YAML format. You can see default configuration in <a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-core/src/main/resources/cassandra-diagnostics-default.yml\">cassandra-diagnostics-default.yml</a>. The default name of the config file is <code>cassandra-diagnostics.yml</code> and it is expected to be found on the classpath. This can be changed using property <code>cassandra.diagnostics.config</code>.\nFor example, the configuration can be set explicitly by changing <code>cassandra-env.sh</code> and adding the following line:</p>\n<pre>JVM_OPTS=\"$JVM_OPTS -Dcassandra.diagnostics.config=some-other-cassandra-diagnostics-configuration.yml\"\n</pre>\n<p>The following is an example of the configuration file:</p>\n<pre>global:\n  systemName: \"smartcat-cassandra-cluster\"\nreporters:\n  - reporter: io.smartcat.cassandra.diagnostics.reporter.LogReporter\nmodules:\n  - module: io.smartcat.cassandra.diagnostics.module.slowquery.SlowQueryModule\n    measurement: queryReport\n    options:\n      slowQueryThresholdInMilliseconds: 1\n    reporters:\n      - io.smartcat.cassandra.diagnostics.reporter.LogReporter\n</pre>\n<p>Specific query reporter may require additional configuration options. Those options are specified using <code>options</code> property. The following example shows a configuration options in case of <code>RiemannReporter</code> and it shows how you can configure specific modules to use this reporter:</p>\n<pre>global:\n  systemName: \"smartcat-cassandra-cluster\"\n# Reporters\nreporters:\n  - reporter: io.smartcat.cassandra.diagnostics.reporter.LogReporter\n  - reporter: io.smartcat.cassandra.diagnostics.reporter.RiemannReporter\n    options:\n      riemannHost: 127.0.0.1\n      riemannPort: 5555 #Optional\n      batchEventSize: 50 #Optional\n# Modules\nmodules:\n  - module: io.smartcat.cassandra.diagnostics.module.requestrate.RequestRateModule\n    measurement: requestRate\n    options:\n      period: 1\n      timeunit: SECONDS\n    reporters:\n      - io.smartcat.cassandra.diagnostics.reporter.LogReporter\n      - io.smartcat.cassandra.diagnostics.reporter.RiemannReporter\n</pre>\n<p>By default all measurements are reported with hostname queried with <a href=\"http://docs.oracle.com/javase/7/docs/api/java/net/InetAddress.html\" rel=\"nofollow\">InetAddress</a> java class. If required, hostname can be set using a hostname variable in configuration file:</p>\n<pre>global:\n  systemName: \"smartcat-cassandra-cluster\"\n  hostname: \"test-hostname\"\nreporters:\netc...\n</pre>\n<p>It is important to name system under observation because measurements can be collected by various systems. Hostname is not enough, it is easy to imagine one host having Cassandra node and Kafka node both emitting measurement and we want to group those by system. By default \"cassandra-cluster\" will be used but it is advised to override this to have unique grouping of measurements:</p>\n<pre>global:\n  systemName: \"cassandra-cluster\"\n  hostname: \"test-hostname\"\n</pre>\n<h2><a id=\"user-content-information-provider\" class=\"anchor\" aria-hidden=\"true\" href=\"#information-provider\"></a>Information provider</h2>\n<p>Being deployed on the node itself, diagnostics connector should provide a connection to the node over JMX by wrapping the Cassandra's NodeProbe class with provides access to all actions and metrics exposed over JMX. This is configured in the <code>connector</code> part of the configuration which sits in the root of diagnostics config.</p>\n<pre>connector:\n  jmxHost: 127.0.0.1\n  jmxPort: 7199\n  jmxAuthEnabled: false #Optional\n  jmxUsername: username #Optional\n  jmxPassword: password #Optional\n</pre>\n<p>Status module uses information provided by connector in order to collect info data.</p>\n<h2><a id=\"user-content-control-and-configuration-api\" class=\"anchor\" aria-hidden=\"true\" href=\"#control-and-configuration-api\"></a>Control and Configuration API</h2>\n<p>Cassandra Diagnostics exposes a control and configuration API. This API currently offers the following operations:</p>\n<pre>- getVersion - returns the actual Cassandra Diagnostics version.\n- reload - reloads the configuration\n</pre>\n<p>This API is exposed over JMX and HTTP protocols.</p>\n<p>The Diagnostics API JMX MXBean could be found under the following object name:</p>\n<pre>package io.smartcat.cassandra.diagnostics.api:type=DiagnosticsApi\n</pre>\n<p>The HTTP API is controlled using the following options in the <code>global</code> section in the configuration file:</p>\n<pre>global:\n  # controls if HTTP API is turned on. 'true' by default.\n  httpApiEnabled: true\n  # specifies the host/address part for listening TCP socket. '127.0.0.1' by default.\n  httpApiHost: 127.0.0.1\n  # specifies the port number for the listening TCP socket. '8998' by default.\n  httpApiPort: 8998\n  # if API authorization is enabled, API key must be provided through the 'Authorization' header\n  httpApiAuthEnabled: false\n  # API access key\n  httpApiKey: \"diagnostics-api-key\"\n</pre>\n<p>It implements the following endpoints for mapping HTTP requests to API operations:</p>\n<ul><li><code>GET /version</code> for <code>getVersion</code></li>\n<li><code>POST /reload</code> for <code>reload</code></li>\n</ul><h2><a id=\"user-content-installation\" class=\"anchor\" aria-hidden=\"true\" href=\"#installation\"></a>Installation</h2>\n<p>Script for automated installation is <a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/bash-installer/README.md\">also available</a>.</p>\n<p>Cassandra Diagnostics consists of the following three components:</p>\n<ul><li>Cassandra Diagnostics Core</li>\n<li>Cassandra Diagnostics Connector</li>\n<li>Cassandra Diagnostics Reporter</li>\n</ul><p>Every of these components is packaged into its own JAR file (accompanied with necessary dependencies). These JAR files are available for download on <a href=\"https://mvnrepository.com/artifact/io.smartcat\" rel=\"nofollow\">Maven Central</a> and need to be present on the classpath.</p>\n<p>Pay attention to the fact that Cassandra Diagnostics Connector has to be aligned with the used Cassandra version. For example, <code>cassandra-diagnostics-connector21</code> should be used with Cassandra 2.1.</p>\n<p>Also note that more than one Cassandra Diagnostics Reporter can be used at the same time. That means that all respective JAR files have to be put on the classpath. The only exception to this rule is in case of <code>LogReporter</code> that is built in Cassandra Diagnostics Core and no Reporter JAR has to be added explicitly.</p>\n<p>Place <code>cassandra-diagnostics-core-VERSION.jar</code>, <code>cassandra-diagnostics-connector21-VERSION.jar</code> and required Reporter JARs (e.g. <code>cassandra-diagnostics-reporter-influx-VERSION-all.jar</code>) into Cassandra <code>lib</code> directory.</p>\n<p>Create and place the configuration file <code>cassandra-diagnostics.yml</code> into Cassandra's <code>conf</code> directory.\nAdd the following line at the end of <code>conf/cassandra-env.sh</code>:</p>\n<pre>JVM_OPTS=\"$JVM_OPTS -javaagent:$CASSANDRA_HOME/lib/cassandra-diagnostics-core-VERSION.jar -Dcassandra.diagnostics.config=cassandra-diagnostics.yml\"\n</pre>\n<h2><a id=\"user-content-usage\" class=\"anchor\" aria-hidden=\"true\" href=\"#usage\"></a>Usage</h2>\n<p>Upon Cassandra node start, the Diagnostics agent kicks in and instrument necessary target classes to inject diagnostics additions.\n<code>LogReporter</code> repors slow queries in <code>logs/system.log</code> at <code>INFO</code> level.\nThe dynamic configuration could be inspected/changed using <code>jconsole</code> and connecting to <code>org.apache.cassandra.service.CassandraDaemon</code>.</p>\n<h2><a id=\"user-content-build-and-deploy\" class=\"anchor\" aria-hidden=\"true\" href=\"#build-and-deploy\"></a>Build and deploy</h2>\n<p>Build and deploy process is described <a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/BUILDANDDEPLOY.md\">here</a>.</p>\n<h2><a id=\"user-content-license-and-development\" class=\"anchor\" aria-hidden=\"true\" href=\"#license-and-development\"></a>License and development</h2>\n<p>Cassandra Diagnostics is licensed under the liberal and business-friendly <a href=\"http://www.apache.org/licenses/LICENSE-2.0.html\" rel=\"nofollow\">Apache Licence, Version 2.0</a> and is freely available on GitHub. Cassandra Diagnostics is further released to the repositories of Maven Central and on JCenter. The project is built using <a href=\"http://maven.apache.org/\" rel=\"nofollow\">Maven</a>. From your shell, cloning and building the project would go something like this:</p>\n<pre>git clone https://github.com/smartcat-labs/cassandra-diagnostics.git\ncd cassandra-diagnostics\nmvn package\n</pre>\n</article>",
        "created_at": "2018-08-03T00:09:56+0000",
        "updated_at": "2018-08-03T00:10:02+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 8,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/12434092?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11797"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 35,
            "label": "docker",
            "slug": "docker"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 956,
            "label": "streaming",
            "slug": "streaming"
          }
        ],
        "is_public": false,
        "id": 11794,
        "uid": null,
        "title": "Yannael/kafka-sparkstreaming-cassandra",
        "url": "https://github.com/Yannael/kafka-sparkstreaming-cassandra",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>This Dockerfile sets up a complete streaming environment for experimenting with Kafka, Spark streaming (PySpark), and Cassandra. It installs</p>\n<ul><li>Kafka 0.10.2.1</li>\n<li>Spark 2.1.1 for Scala 2.11</li>\n<li>Cassandra 3.7</li>\n</ul><p>It additionnally installs</p>\n<ul><li>Anaconda distribution 4.4.0 for Python 2.7.10</li>\n<li>Jupyter notebook for Python</li>\n</ul>\n<p>Run container using <a href=\"https://hub.docker.com/r/yannael/kafka-sparkstreaming-cassandra\" rel=\"nofollow\">DockerHub image</a></p>\n<pre>docker run -p 4040:4040 -p 8888:8888 -p 23:22 -ti --privileged yannael/kafka-sparkstreaming-cassandra\n</pre>\n<p>See following video for usage demo.\n<br /><a href=\"https://www.youtube.com/watch?v=XxCFo7BzNQ8\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/61da6654e141eb080dce4055dbd84d7c47231037/687474703a2f2f696d672e796f75747562652e636f6d2f76692f587843466f37427a4e51382f302e6a7067\" alt=\"Demo\" width=\"480\" height=\"360\" border=\"10\" data-canonical-src=\"http://img.youtube.com/vi/XxCFo7BzNQ8/0.jpg\" style=\"text-align: center;\" /></a></p>\n<p>Note that any changes you make in the notebook will be lost once you exit de container. In order to keep the changes, it is necessary put your notebooks in a folder on your host, that you share with the container, using for example</p>\n<pre>docker run -v `pwd`:/home/guest/host -p 4040:4040 -p 8888:8888 -p 23:22 -ti --privileged yannael/kafka-sparkstreaming-cassandra\n</pre>\n<p>Note:</p>\n<ul><li>The \"-v <code>pwd</code>:/home/guest/host\" shares the local folder (i.e. folder containing Dockerfile, ipynb files, etc...) on your computer - the 'host') with the container in the '/home/guest/host' folder.</li>\n<li>Port are shared as follows:\n<ul><li>4040 bridges to Spark UI</li>\n<li>8888 bridges to the Jupyter Notebook</li>\n<li>23 bridges to SSH</li>\n</ul></li>\n</ul><p>SSH allows to get a onnection to the container</p>\n<pre>ssh -p 23 guest@containerIP\n</pre>\n<p>where 'containerIP' is the IP of th container (127.0.0.1 on Linux). Password is 'guest'.</p>\n<h3><a id=\"user-content-start-services\" class=\"anchor\" aria-hidden=\"true\" href=\"#start-services\"></a>Start services</h3>\n<p>Once run, you are logged in as root in the container. Run the startup_script.sh (in /usr/bin) to start</p>\n<ul><li>SSH server. You can connect to the container using user 'guest' and password 'guest'</li>\n<li>Cassandra</li>\n<li>Zookeeper server</li>\n<li>Kafka server</li>\n</ul><pre>startup_script.sh\n</pre>\n<h3><a id=\"user-content-connect-create-cassandra-table-open-notebook-and-start-streaming\" class=\"anchor\" aria-hidden=\"true\" href=\"#connect-create-cassandra-table-open-notebook-and-start-streaming\"></a>Connect, create Cassandra table, open notebook and start streaming</h3>\n<p>Connect as user 'guest' and go to 'host' folder (shared with the host)</p>\n<pre>su guest\n</pre>\n<p>Start Jupyter notebook</p>\n<pre>notebook\n</pre>\n<p>and connect from your browser at port host:8888 (where 'host' is the IP for your host. If run locally on your computer, this should be 127.0.0.1 or 192.168.99.100, check Docker documentation)</p>\n<h4><a id=\"user-content-start-kafka-producer\" class=\"anchor\" aria-hidden=\"true\" href=\"#start-kafka-producer\"></a>Start Kafka producer</h4>\n<p>Open kafkaSendDataPy.ipynb and run all cells.</p>\n<h4><a id=\"user-content-start-kafka-receiver\" class=\"anchor\" aria-hidden=\"true\" href=\"#start-kafka-receiver\"></a>Start Kafka receiver</h4>\n<p>Open kafkaReceiveAndSaveToCassandraPy.ipynb and run cells up to start streaming. Check in subsequent cells that Cassandra collects data properly.</p>\n<h4><a id=\"user-content-connect-to-spark-ui\" class=\"anchor\" aria-hidden=\"true\" href=\"#connect-to-spark-ui\"></a>Connect to Spark UI</h4>\n<p>It is available in your browser at port 4040</p>\n<p>The container is based on CentOS 6 Linux distribution. The main steps of the building process are</p>\n<ul><li>Install some common Linux tools (wget, unzip, tar, ssh tools, ...), and Java (1.8)</li>\n<li>Create a guest user (UID important for sharing folders with host!, see below), and install Spark and sbt, Kafka, Anaconda and Jupyter notbooks for the guest user</li>\n<li>Go back to root user, and install startup script (for starting SSH and Cassandra services), sentenv.sh script to set up environment variables (JAVA, Kafka, Spark, ...), spark-default.conf, and Cassandra</li>\n</ul><h3><a id=\"user-content-user-uid\" class=\"anchor\" aria-hidden=\"true\" href=\"#user-uid\"></a>User UID</h3>\n<p>In the Dockerfile, the line</p>\n<pre>RUN useradd guest -u 1000\n</pre>\n<p>creates the user under which the container will be run as a guest user. The username is 'guest', with password 'guest', and the '-u' parameter sets the linux UID for that user.</p>\n<p>In order to make sharing of folders easier between the container and your host, <strong>make sure this UID matches your user UID on the host</strong>. You can see what your host UID is with</p>\n<pre>echo $UID\n</pre>\n<h3><a id=\"user-content-clone-this-repository\" class=\"anchor\" aria-hidden=\"true\" href=\"#clone-this-repository\"></a>Clone this repository</h3>\n<pre>git clone https://github.com/Yannael/kafka-sparkstreaming-cassandra\n</pre>\n<h3><a id=\"user-content-build\" class=\"anchor\" aria-hidden=\"true\" href=\"#build\"></a>Build</h3>\n<p>From Dockerfile folder, run</p>\n<pre>docker build -t kafka-sparkstreaming-cassandra .\n</pre>\n<p>It may take about 30 minutes to complete.</p>\n<h3><a id=\"user-content-run\" class=\"anchor\" aria-hidden=\"true\" href=\"#run\"></a>Run</h3>\n<pre>docker run -v `pwd`:/home/guest/host -p 4040:4040 -p 8888:8888 -p 23:22 -ti --privileged kafka-sparkstreaming-cassandra\n</pre>\n</article>",
        "created_at": "2018-08-03T00:07:38+0000",
        "updated_at": "2018-08-03T00:07:53+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/976414?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11794"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1269,
            "label": "hdfs",
            "slug": "hdfs"
          }
        ],
        "is_public": false,
        "id": 11776,
        "uid": null,
        "title": "tuplejump/snackfs-release",
        "url": "https://github.com/tuplejump/snackfs-release",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\"><p><a href=\"http://tuplejump.github.io/calliope/snackfs.html\" rel=\"nofollow\">SnackFS @ Calliope</a></p>\n<p>SnackFS is our bite-sized, lightweight HDFS compatible FileSystem built over Cassandra.\nWith it's unique fat driver design it requires no additional SysOps or setup on the Cassanndra Cluster. All you have to do is point to your Cassandra cluster and you are ready to go.</p>\n<p>As SnackFS was written as a dropin replacement for HDFS, your existing HDFS backed applications not only run as-is on SnackFS, but they also run faster!\nSnackFS cluster is also more resilient than a HDFS cluster as there is no SPOF like the NameNode.</p>\n<h2><a id=\"user-content-prerequisites\" class=\"anchor\" aria-hidden=\"true\" href=\"#prerequisites\"></a>Prerequisites</h2>\n<ol><li>\n<p>SBT : It can be set up from the instructions <a href=\"http://www.scala-sbt.org/release/docs/Getting-Started/Setup.html#installing-sbt\" rel=\"nofollow\">here</a>.</p>\n</li>\n<li>\n<p>Cassandra(v1.2.12) : Instructions can be found <a href=\"http://wiki.apache.org/cassandra/GettingStarted\" rel=\"nofollow\">here</a>. An easier alternative would be using <a href=\"https://github.com/pcmanus/ccm\">CCM</a></p>\n</li>\n</ol><h2><a id=\"user-content-using-snackfs\" class=\"anchor\" aria-hidden=\"true\" href=\"#using-snackfs\"></a>Using SnackFS</h2>\n<h3><a id=\"user-content-use-the-binary\" class=\"anchor\" aria-hidden=\"true\" href=\"#use-the-binary\"></a>Use the binary</h3>\n<ul><li>\n<p>You can download the SnackFS distribution built with <a href=\"http://bit.ly/1eKV1ae\" rel=\"nofollow\">Scala 2.9.x here</a> and <a href=\"http://bit.ly/1jI7vVw\" rel=\"nofollow\">Scala 2.10.x here</a></p>\n</li>\n<li>\n<p>To add SnackFS to your SBT project use,</p>\n</li>\n</ul><p>For SBT</p>\n<div class=\"highlight highlight-source-scala\"><pre>\"com.tuplejump\" %% \"snackfs\" % \"0.6.1-EA\"</pre></div>\n<ul><li>To add SnackFS to your Maven project use,\nwith Scala 2.9.3 use,</li>\n</ul><div class=\"highlight highlight-text-xml\"><pre>&lt;dependency&gt;\n  &lt;groupId&gt;com.tuplejump&lt;/groupId&gt;\n  &lt;artifactId&gt;snackfs_2.9.3&lt;/artifactId&gt;\n  &lt;version&gt;0.6.1-EA&lt;/version&gt;\n&lt;/dependency&gt;</pre></div>\n<p>And with Scala 2.10.3,</p>\n<div class=\"highlight highlight-text-xml\"><pre>&lt;dependency&gt;\n  &lt;groupId&gt;com.tuplejump&lt;/groupId&gt;\n  &lt;artifactId&gt;snackfs_2.10&lt;/artifactId&gt;\n  &lt;version&gt;0.6.1-EA&lt;/version&gt;\n&lt;/dependency&gt;</pre></div>\n<h3><a id=\"user-content-build-from-source\" class=\"anchor\" aria-hidden=\"true\" href=\"#build-from-source\"></a>Build from Source</h3>\n<ol><li>\n<p>Checkout the source from <a href=\"http://github.com/tuplejump/snackfs\">http://github.com/tuplejump/snackfs</a></p>\n</li>\n<li>\n<p>To build SnackFS distribution run sbt's dist command in the project directory</p>\n</li>\n</ol><pre>[snackfs]$ sbt dist\n</pre>\n<p>This will result in a \"snackfs-{version}.tgz\" file in the \"target\" directory of \"snackfs\".\nExtract \"snackfs-{version}.tgz\" to the desired location.</p>\n<ol start=\"3\"><li>\n<p>Start Cassandra (default setup for snackfs assumes its a cluster with 3 nodes)</p>\n</li>\n<li>\n<p>It is possible to configure the file system by updating core-site.xml.\nThe following properties can be added.</p>\n<ul><li>snackfs.cassandra.host (default 127.0.0.1)</li>\n<li>snackfs.cassandra.port (default 9160)</li>\n<li>snackfs.consistencyLevel.write (default QUORUM)</li>\n<li>snackfs.consistencyLevel.read (default QUORUM)</li>\n<li>snackfs.keyspace (default snackfs)</li>\n<li>snackfs.subblock.size (default 8 MB (8 * 1024 * 1024))</li>\n<li>snackfs.block.size (default 128 MB (128 * 1024 * 1024))</li>\n<li>snackfs.replicationFactor (default 3)</li>\n<li>snackfs.replicationStrategy (default org.apache.cassandra.locator.SimpleStrategy)</li>\n</ul></li>\n<li>\n<p>SnackFS Shell provides the fs commands similar to Hadoop Shell. For example to create a directory,</p>\n</li>\n</ol><pre>[Snackfs(extracted)]$bin/snackfs -mkdir snackfs:///random\n</pre>\n<p>###To build and use with Hadoop</p>\n<ol><li>\n<p>Setup Apache Hadoop v1.0.4.(<a href=\"http://hadoop.apache.org/#Getting+Started\" rel=\"nofollow\">http://hadoop.apache.org/#Getting+Started</a>). The base directory will be referred as 'hadoop-1.0.4' in the following steps.</p>\n</li>\n<li>\n<p>Execute the following commands in the snackfs project directory.</p>\n</li>\n</ol><pre>[snackfs]$ sbt package\n</pre>\n<p>This will result in a \"snackfs_&lt;scala_version&gt;-&lt;version&gt;.jar\" file in the \"target/scala-&lt;scala_version&gt;\" directory of \"snackfs\".\nCopy the jar to 'hadoop-1.0.4/lib'.</p>\n<ol start=\"3\"><li>\n<p>Copy all the jars in snackfs/lib_managed and scala-library-&lt;scala_version&gt;.jar\n(located at '~/.ivy2/cache/org.scala-lang/scala-library/jars') to 'hadoop-1.0.4/lib'.</p>\n</li>\n<li>\n<p>Copy snackfs/src/main/resources/core-site.xml to 'hadoop-1.0.4/conf'</p>\n</li>\n<li>\n<p>Start Cassandra (default setup for snackfs assumes its a cluster with 3 nodes)</p>\n</li>\n<li>\n<p>Hadoop fs commands can now be run using snackfs. For example,</p>\n</li>\n</ol><pre>[hadoop-1.0.4]$ bin/hadoop fs -mkdir snackfs:///random\n</pre>\n<p>###To configure logging,</p>\n<h4><a id=\"user-content-in-system-environment\" class=\"anchor\" aria-hidden=\"true\" href=\"#in-system-environment\"></a>In System Environment</h4>\n<p>Set SNACKFS_LOG_LEVEL in the Shell to one of the following Values</p>\n<ul><li>DEBUG</li>\n<li>INFO</li>\n<li>ERROR</li>\n<li>ALL</li>\n<li>OFF</li>\n</ul><p>Default value if not set if ERROR</p>\n<p>####In code (for further control/tuning)\nIf you want your logs in a File, update LogConfiguration.scala like below</p>\n<div class=\"highlight highlight-source-scala\"><pre>val config = new LoggerFactory(\"\", Option(Level.ALL), List(FileHandler(\"logs\")), true)</pre></div>\n<p>The arguments for LoggerFactory are</p>\n<ol><li>node - Name of the logging node. (\"\") is the top-level logger.</li>\n<li>level - Log level for this node. Leaving it None implies the parent logger's level.</li>\n<li>handlers - Where to send log messages.</li>\n<li>useParents - indicates if log messages are passed up to parent nodes.To stop at this node level, set it to false</li>\n</ol><p>Additional logging configuration details can be found <a href=\"https://github.com/twitter/util/tree/master/util-logging#configuring\">here</a></p>\n</article>",
        "created_at": "2018-08-02T23:27:09+0000",
        "updated_at": "2018-08-02T23:27:17+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/3493976?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11776"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 963,
            "label": "akka",
            "slug": "akka"
          },
          {
            "id": 1041,
            "label": "smack",
            "slug": "smack"
          }
        ],
        "is_public": false,
        "id": 11770,
        "uid": null,
        "title": "killrweather/killrweather",
        "url": "https://github.com/killrweather/killrweather",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>KillrWeather is a reference application (which we are constantly improving) showing how to easily leverage and integrate <a href=\"http://spark.apache.org\" rel=\"nofollow\">Apache Spark</a>,\n<a href=\"http://cassandra.apache.org\" rel=\"nofollow\">Apache Cassandra</a>, and <a href=\"http://kafka.apache.org\" rel=\"nofollow\">Apache Kafka</a> for fast, streaming computations in asynchronous <a href=\"http://akka.io\" rel=\"nofollow\">Akka</a> event-driven environments. This application focuses on the use case of  <strong><a href=\"https://github.com/killrweather/killrweather/wiki/4.-Time-Series-Data-Model\">time series data</a></strong>.</p>\n<h2><a id=\"user-content-sample-use-case\" class=\"anchor\" aria-hidden=\"true\" href=\"#sample-use-case\"></a>Sample Use Case</h2>\n<p>I need fast access to historical data  on the fly for  predictive modeling  with real time data from the stream.</p>\n<h2><a id=\"user-content-basic-samples\" class=\"anchor\" aria-hidden=\"true\" href=\"#basic-samples\"></a>Basic Samples</h2>\n<p><a href=\"https://github.com/killrweather/killrweather/tree/master/killrweather-examples/src/main/scala/com/datastax/killrweather\">Basic Spark, Kafka, Cassandra Samples</a></p>\n<h2><a id=\"user-content-reference-application\" class=\"anchor\" aria-hidden=\"true\" href=\"#reference-application\"></a>Reference Application</h2>\n<p><a href=\"https://github.com/killrweather/killrweather/tree/master/killrweather-app/src/main/scala/com/datastax/killrweather\">KillrWeather Main App</a></p>\n<h2><a id=\"user-content-time-series-data\" class=\"anchor\" aria-hidden=\"true\" href=\"#time-series-data\"></a>Time Series Data</h2>\n<p>The use of time series data for business analysis is not new. What is new is the ability to collect and analyze massive volumes of data in sequence at extremely high velocity to get the clearest picture to predict and forecast future market changes, user behavior, environmental conditions, resource consumption, health trends and much, much more.</p>\n<p>Apache Cassandra is a NoSQL database platform particularly suited for these types of Big Data challenges. Cassandra’s data model is an excellent fit for handling data in sequence regardless of data type or size. When writing data to Cassandra, data is sorted and written sequentially to disk. When retrieving data by row key and then by range, you get a fast and efficient access pattern due to minimal disk seeks – time series data is an excellent fit for this type of pattern. Apache Cassandra allows businesses to identify meaningful characteristics in their time series data as fast as possible to make clear decisions about expected future outcomes.</p>\n<p>There are many flavors of time series data. Some can be windowed in the stream, others can not be windowed in the stream because queries are not by time slice but by specific year,month,day,hour. Spark Streaming lets you do both.</p>\n<h2><a id=\"user-content-start-here\" class=\"anchor\" aria-hidden=\"true\" href=\"#start-here\"></a>Start Here</h2>\n<h3><a id=\"user-content-clone-the-repo\" class=\"anchor\" aria-hidden=\"true\" href=\"#clone-the-repo\"></a>Clone the repo</h3>\n<pre>git clone https://github.com/killrweather/killrweather.git\ncd killrweather\n</pre>\n<h3><a id=\"user-content-build-the-code\" class=\"anchor\" aria-hidden=\"true\" href=\"#build-the-code\"></a>Build the code</h3>\n<p>If this is your first time running SBT, you will be downloading the internet.</p>\n<pre>cd killrweather\nsbt compile\n# For IntelliJ users, this creates Intellij project files, but as of\n# version 14x you should not need this, just import a new sbt project.\nsbt gen-idea\n</pre>\n<h3><a id=\"user-content-setup-for-linux--mac---3-steps\" class=\"anchor\" aria-hidden=\"true\" href=\"#setup-for-linux--mac---3-steps\"></a>Setup (for Linux &amp; Mac) - 3 Steps</h3>\n<p>1.<a href=\"http://cassandra.apache.org/download/\" rel=\"nofollow\">Download the latest Cassandra</a> and open the compressed file.</p>\n<p>2.Start Cassandra - you may need to prepend with sudo, or chown /var/lib/cassandra. On the command line:</p>\n<pre>./apache-cassandra-{version}/bin/cassandra -f\n</pre>\n<p>3.Run the setup cql scripts to create the schema and populate the weather stations table.\nOn the command line start a cqlsh shell:</p>\n<pre>cd /path/to/killrweather/data\npath/to/apache-cassandra-{version}/bin/cqlsh\n</pre>\n<h3><a id=\"user-content-setup-for-windows---3-steps\" class=\"anchor\" aria-hidden=\"true\" href=\"#setup-for-windows---3-steps\"></a>Setup (for Windows) - 3 Steps</h3>\n<ol><li>\n<p><a href=\"http://www.planetcassandra.org/cassandra\" rel=\"nofollow\">Download the latest Cassandra</a> and double click the installer.</p>\n</li>\n<li>\n<p>Chose to run the Cassandra automatically during start-up</p>\n</li>\n<li>\n<p>Run the setup cql scripts to create the schema and populate the weather stations table.\nOn the command line start a <code>cqlsh</code> shell:</p>\n</li>\n</ol><pre>    cd c:/path/to/killrweather\n    c:/pat/to/cassandara/bin/cqlsh\n</pre>\n<h3><a id=\"user-content-in-cql-shell\" class=\"anchor\" aria-hidden=\"true\" href=\"#in-cql-shell\"></a>In CQL Shell:</h3>\n<p>You should see:</p>\n<pre> Connected to Test Cluster at 127.0.0.1:9042.\n [cqlsh {latest.version} | Cassandra {latest.version} | CQL spec {latest.version} | Native protocol {latest.version}]\n Use HELP for help.\n cqlsh&gt;\n</pre>\n<p>Run the scripts, then keep the cql shell open querying once the apps are running:</p>\n<pre> cqlsh&gt; source 'create-timeseries.cql';\n cqlsh&gt; source 'load-timeseries.cql';\n</pre>\n<h3><a id=\"user-content-run\" class=\"anchor\" aria-hidden=\"true\" href=\"#run\"></a>Run</h3>\n<h4><a id=\"user-content-logging\" class=\"anchor\" aria-hidden=\"true\" href=\"#logging\"></a>Logging</h4>\n<p>You will see this in all 3 app shells because log4j has been explicitly taken off the classpath:</p>\n<pre>log4j:WARN No appenders could be found for logger (kafka.utils.VerifiableProperties).\nlog4j:WARN Please initialize the log4j system properly.\n</pre>\n<p>What we are really trying to isolate here is what is happening in the apps with regard to the event stream.\nYou can add log4j locally.</p>\n<p>To change any package log levels and see more activity, simply modify</p>\n<h4><a id=\"user-content-from-command-line\" class=\"anchor\" aria-hidden=\"true\" href=\"#from-command-line\"></a>From Command Line</h4>\n<p>1.Start <code>KillrWeather</code></p>\n<pre>cd /path/to/killrweather\nsbt app/run\n</pre>\n<p>As the <code>KillrWeather</code> app initializes, you will see Akka Cluster start, Zookeeper and the Kafka servers start.</p>\n<p>For all three apps in load-time you see the Akka Cluster node join and start metrics collection. In deployment with multiple nodes of each app\nthis would leverage the health of each node for load balancing as the rest of the cluster nodes join the cluster:</p>\n<p>2.Start the Kafka data feed app\nIn a second shell run:</p>\n<pre>sbt clients/run\n</pre>\n<p>You should see:</p>\n<pre>Multiple main classes detected, select one to run:\n[1] com.datastax.killrweather.KafkaDataIngestionApp\n[2] com.datastax.killrweather.KillrWeatherClientApp\n</pre>\n<p>Select <code>KafkaDataIngestionApp</code>, and watch the shells for activity. You can stop the data feed or let it keep running.\nAfter a few seconds you should see data by entering this in the cqlsh shell:</p>\n<pre>cqlsh&gt; select * from isd_weather_data.raw_weather_data;\n</pre>\n<p>This confirms that data from the ingestion app has published to Kafka, and that raw data is\nstreaming from Spark to Cassandra from the <code>KillrWeatherApp</code>.</p>\n<pre>cqlsh&gt; select * from isd_weather_data.daily_aggregate_precip;\n</pre>\n<p>Unfortunately the precips are mostly 0 in the samples (To Do).</p>\n<p>3.Open a third shell and again enter this but select <code>KillrWeatherClientApp</code>:</p>\n<pre>sbt clients/run\n</pre>\n<p>This api client runs queries against the raw and the aggregated data from the kafka stream.\nIt sends requests (for varying locations and dates/times) and for some, triggers further aggregations\nin compute time which are also saved to Cassandra:</p>\n<ul><li>current weather</li>\n<li>daily temperatures</li>\n<li>monthly temperatures</li>\n<li>monthly highs and low temperatures</li>\n<li>daily precipitations</li>\n<li>top-k precipitation</li>\n</ul><p>Next I will add some forecasting with ML :)</p>\n<p>Watch the app and client activity in request response of weather data and aggregation data.\nBecause the querying of the API triggers even further aggregation of data from the originally\naggregated daily roll ups, you can now see a new tier of temperature and precipitation aggregation:\nIn the cql shell:</p>\n<pre>cqlsh&gt; select * from isd_weather_data.daily_aggregate_temperature;\ncqlsh&gt; select * from isd_weather_data.daily_aggregate_precip;\n</pre>\n<h4><a id=\"user-content-from-an-ide\" class=\"anchor\" aria-hidden=\"true\" href=\"#from-an-ide\"></a>From an IDE</h4>\n<ol><li>Run the app <a href=\"https://github.com/killrweather/killrweather/blob/master/killrweather-app/src/main/scala/com/datastax/killrweather/KillrWeatherApp.scala\">com.datastax.killrweather.KillrWeatherApp</a></li>\n<li>Run the kafka data ingestion server <a href=\"https://github.com/killrweather/killrweather/blob/master/killrweather-clients/src/main/scala/com/datastax/killrweather/KafkaDataIngestionApp.scala\">com.datastax.killrweather.KafkaDataIngestionApp</a></li>\n<li>Run the API client <a href=\"https://github.com/killrweather/killrweather/blob/master/killrweather-clients/src/main/scala/com/datastax/killrweather/KillrWeatherClientApp.scala\">com.datastax.killrweather.KillrWeatherClientApp</a></li>\n</ol><p>To close the cql shell:</p>\n<pre>cqlsh&gt; quit;\n</pre>\n</article>",
        "created_at": "2018-08-02T01:51:28+0000",
        "updated_at": "2018-08-02T01:51:43+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 5,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/9054253?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11770"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 907,
            "label": "mesos",
            "slug": "mesos"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 963,
            "label": "akka",
            "slug": "akka"
          },
          {
            "id": 1041,
            "label": "smack",
            "slug": "smack"
          }
        ],
        "is_public": false,
        "id": 11768,
        "uid": null,
        "title": "A Brief History of the SMACK Stack",
        "url": "https://chiefscientist.org/a-brief-history-of-the-smack-stack-f382547e91fe?gi=d78bedf3f6f9",
        "content": "<p id=\"d291\" class=\"graf graf--p graf-after--h3\">The term SMACK Stack was widely popularized in the San Francisco/Dublin Scala/Spark/Reactive Systems meetups and By the Bay series of conferences (Scala and Data). Since it took a life of its own, this is an abridged chronology on how it came about. I’m surely missing something, since it’s a view from (by) the Bay — please comment with any corrections or suggestions, as well as updates. I’m not elaborating the technical merits of the SMACK Stack here, as they are covered at <a href=\"http://noetl.org\" data-href=\"http://noetl.org\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">NoETL.org</a>.</p><p id=\"3242\" class=\"graf graf--p graf-after--p\">I’ve first seen the phrase SMACK Stack in a <a href=\"https://twitter.com/jamie_allen/status/614045214018158592\" data-href=\"https://twitter.com/jamie_allen/status/614045214018158592\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">tweet by Jamie Allen</a>, attributed to Oliver White, on June 25, 2015. Jamie is a veteran Typesafe/Lightbend executive and Scala/Akka/Reactive practitioner, and Oliver is their Chief Storyteller. By that time, our long-established plan to run <a href=\"http://2015.bigdatascala.bythebay.io/pipeline.html\" data-href=\"http://2015.bigdatascala.bythebay.io/pipeline.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">the first end-to-end data pipeline training</a> at Scala By the Bay and Big Data Scala 2015 was in full smack. We brought together the actual five companies comprising the SMACK Stack with their own trainers, training materials, and top OSS contributors, having worked relentlessly to create the actual SMACK Stack training, defining the SMACK Stack by doing and teaching.</p><p id=\"5350\" class=\"graf graf--p graf-after--p\">The original <a href=\"https://chiefscientist.org/sfscala.org\" data-href=\"sfscala.org\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">SF Scala</a> announcement went out on December 23, 2014. Here it is:</p><blockquote id=\"fb08\" class=\"graf graf--blockquote graf-after--p\"><div>We are starting two new conferences in 2015: Big Data Scala for complete data pipelines and “big” data science, and Text By the Bay for applied NLP/text mining. Our flagship Scala By the Bay conference continues into its third year, growing to 500, and directly followed by Big Data Scala.</div></blockquote><blockquote id=\"e59b\" class=\"graf graf--blockquote graf-after--blockquote\"><a href=\"https://chiefscientist.org/text.bythebay.io\" data-href=\"text.bythebay.io\" class=\"markup--anchor markup--blockquote-anchor\" target=\"_blank\">text.bythebay.io</a><div><br /><a href=\"https://chiefscientist.org/scala.bythebay.io\" data-href=\"scala.bythebay.io\" class=\"markup--anchor markup--blockquote-anchor\" target=\"_blank\">scala.bythebay.io</a><br /><a href=\"https://chiefscientist.org/bigdatascala.org\" data-href=\"bigdatascala.org\" class=\"markup--anchor markup--blockquote-anchor\" target=\"_blank\">bigdatascala.org</a></div></blockquote><blockquote id=\"5dfd\" class=\"graf graf--blockquote graf-after--blockquote\"><div>Both previous installments of SBTB sold out completely.</div></blockquote><blockquote id=\"aad9\" class=\"graf graf--blockquote graf-after--blockquote\"><div>You can find the talks from the previous conferences and meetups at Functional.TV. The 2014 conference is now at</div></blockquote><blockquote id=\"e1df\" class=\"graf graf--blockquote graf-after--blockquote\"><div>2014.scala.bythebay.io (photos from 2014 SBTB)</div></blockquote><blockquote id=\"3b38\" class=\"graf graf--blockquote graf-after--blockquote\"><div><strong class=\"markup--strong markup--blockquote-strong\">We’re also creating a new kind of an all-day tutorial — Complete Pipeline Training, where in one day we’ll go through an end-to-end datapipeline in Scala, running Akka APIs on Mesos and pumping data through Kafka into Spark. Each segment will be taught by the respective company driving the component — Mesosphere, Typesafe, Confluent, and Databricks</strong>.</div></blockquote><p id=\"98de\" class=\"graf graf--p graf-after--blockquote\">The acronym SMACK was used publicly in the official announcement of the training. Here it is in the context of collecting questions for the closing panel at Big Data Scala with Martin Odersky, creator of Scala and cofounder of Typesafe (now Lightbend), who keynoted it.</p><figure id=\"0a91\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*THEpX1HaETt6rw6w80-7Fg.png\" data-width=\"2632\" data-height=\"1678\" data-action=\"zoom\" data-action-value=\"1*THEpX1HaETt6rw6w80-7Fg.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*THEpX1HaETt6rw6w80-7Fg.png\" alt=\"image\" /></div></figure><p id=\"4f8a\" class=\"graf graf--p graf-after--figure\">The C for Cassandra was added when we invited Ryan Knight and Evan Chan. Ryan transitioned from Typesafe to DataStax, and Evan combined Spark and Cassandra for fast OLAP.</p><p id=\"739c\" class=\"graf graf--p graf-after--p\">Here’s the final lineup as delivered on the Complete Pipeline Training day</p><p id=\"4b42\" class=\"graf graf--p graf-after--p\">— <strong class=\"markup--strong markup--p-strong\">S: Scala and Spark</strong> =&gt; Typesafe, Databricks; every trainer was a fairly well-known Scala developer. Chris Fregly represented Databricks and Advanced Spark. Nilanjan Raychaudhuri helped with the Akka presentation. For Spark, we used <a href=\"http://spark-notebook.io\" data-href=\"http://spark-notebook.io\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">Spark Notebook</a> as the GUI, presented by its creator Andy Petrella.</p><p id=\"6e57\" class=\"graf graf--p graf-after--p\">— <strong class=\"markup--strong markup--p-strong\">M: Mesos</strong> =&gt; Mesosphere. Jason Swartz</p><p id=\"3fe6\" class=\"graf graf--p graf-after--p\">— <strong class=\"markup--strong markup--p-strong\">A: Akka</strong> =&gt; Typesafe, Duncan De Vore, Nilanjan with slides, Ryan Knight emeritus.</p><p id=\"91a9\" class=\"graf graf--p graf-after--p\">— <strong class=\"markup--strong markup--p-strong\">C: Cassandra</strong>: Ryan Knight, for a while of Typesafe and then of DataStax, helped across the stack. Evan Chan (of Spark Job Server and FiloDB fame) is recognized for marrying Spark and Cassandra and was also instrumental.</p><p id=\"f58b\" class=\"graf graf--p graf-after--p\">— <strong class=\"markup--strong markup--p-strong\">K: Kafka</strong> =&gt; Confluent. Jesse Anderson was fielded by Confluent as its official training provider. Ewen Cheslak-Postava helped with the Kafka segment in the docker.</p><p id=\"0fbb\" class=\"graf graf--p graf-after--p\">Here’s the github repo used in the training: <a href=\"https://github.com/bythebay/pipeline\" data-href=\"https://github.com/bythebay/pipeline\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">https://github.com/bythebay/pipeline</a>.</p><p id=\"3576\" class=\"graf graf--p graf-after--p\">It’s important to note that Helena Edelson wrote <a href=\"https://github.com/killrweather/killrweather\" data-href=\"https://github.com/killrweather/killrweather\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">killrweather</a>, a project implementing the SMACK Stack, earlier, and presented it at <a href=\"http://pnwscala.org/2014/index.html\" data-href=\"http://pnwscala.org/2014/index.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">PNWScala</a> 2014 (among other venues). When Ryan and Evan joined the training we considered using killrweather and would have loved to have Helena join us (but she deferred to Ryan). We ended up with a different codebase to simplify setup and allow for Spark Notebook as a Spark GUI. Helena is a pioneer — and now a lead innovator — of SMACK Stack.</p><p id=\"7ada\" class=\"graf graf--p graf-after--p\">Originally, we worked with Mesosphere to spin up a training cluster for every student. (Mesosphere was able to secure GCE credits, but it was not ready to run on GCE.) At that point, Mesosphere messaging was mostly on fully utilizing the datacenter. Immediately after Big Data Scala, <a href=\"https://mesosphere.com/blog/2015/08/20/mesosphere-infinity-youre-4-words-away-from-a-complete-big-data-system/\" data-href=\"https://mesosphere.com/blog/2015/08/20/mesosphere-infinity-youre-4-words-away-from-a-complete-big-data-system/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">Mesosphere announced <strong class=\"markup--strong markup--p-strong\">dcos infinity</strong></a>, inspired by <a href=\"https://mesosphere.com/blog/2015/07/24/learn-everything-you-need-to-know-about-scala-and-big-data-in-oakland/\" data-href=\"https://mesosphere.com/blog/2015/07/24/learn-everything-you-need-to-know-about-scala-and-big-data-in-oakland/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">the SMACK training By the Bay</a>. Infinity spins up Kafka, Spark and Cassandra working together. The SMACK Stack training By the Bay was the catalyst that lead to <em class=\"markup--em markup--p-em\">dcos infinity</em> — I pitched it to Flo directly (my good friend Jason Swartz was a devmanager at Mesosphere and we discussed it over lunch on the deck), and Flo put us together with Matt Trifiro, the CMO, with whom we continue to collaborate. It is important to observe that the data pipeline offering encapsulated by <em class=\"markup--em markup--p-em\">dcos infinity</em> heralded a new direction for Mesosphere, from data center utilization to data pipeline integration as a product. They’ve not called it “<em class=\"markup--em markup--p-em\">dcos smack”</em> to allow for database vendor neutrality. (We’ve also discussed “<em class=\"markup--em markup--p-em\">dcos khrabrov</em>” as an easily googlable alias.:)</p><p id=\"62ce\" class=\"graf graf--p graf-after--p\">The training was an incredible success — we had a 100 people at Galvanize going over the whole stack in one day, using a fully dockerized setup we provided on USB. It took five sweaty guys in an AirBnB — that Andy Petrella rented — to finalize the night before, after which I took it home to replicate on a device running under Windows. Of course it failed, and I manually copied the USBs into the night. But thanks to Nitro docker genius Ben Rizkowski, it worked great. Ben even popped in a Slack support channel, on a Sunday, while house-hunting in the East Bay.</p><p id=\"f0ba\" class=\"graf graf--p graf-after--p\"><a href=\"http://2015.bigdatascala.bythebay.io\" data-href=\"http://2015.bigdatascala.bythebay.io\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">Big Data Scala</a> was held for the first time together with the training. It was keynoted by Martin Odersky, the creator of Scala, Matei Zaharia, the creator of Spark, Jay Kreps, the creator of Kafka, and Mike Olson, the CSO of Cloudera. Here we had most of the creators of the SMACK stack in person, as well as its key proponents and integrators. We’ve unveiled <a href=\"http://noetl.org\" data-href=\"http://noetl.org\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">NoETL.org</a> there as a way to popularize the reactive, streaming approach to data, as opposed to batching and dumping/reparsing text from HDFS. NoETL is in fact another way to describe what SMACK is good for, as a new way to write data pipelines.</p><p id=\"b487\" class=\"graf graf--p graf-after--p\">Since all of original SMACK Stack trainers are public speakers and OSS leads, the notion spread fast. Notably, Evan Chan moved forward with <a href=\"https://github.com/tuplejump/FiloDB\" data-href=\"https://github.com/tuplejump/FiloDB\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">FiloDB</a>, combining Cassandra and Spark to support OLAP. Evan presented his work at SF Scala and SF Spark, as well as Scala By the Bay and Big Data Scala, and subsequently teamed up with Helena Edelson to advance FiloDB and SMACK Stack in industry. Evan ran a Q&amp;A with O’Reilly in the Fall 2015. At the same time we took SMACK Stack to Europe via the Nitro office, the home of the Dublin Spark meetup. We also regularly host Andy Petrella at there regularly who further spread the word. Dean Wampler, the Lightbend Big Data architect, will present SMACK Stack at the O’Reilly Architecture conference this Fall (2016) — see how <a href=\"https://twitter.com/deanwampler/status/752510307562586112\" data-href=\"https://twitter.com/deanwampler/status/752510307562586112\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">the SMACKing already began</a>!</p><p id=\"b52d\" class=\"graf graf--p graf-after--p\">The SMACK Stack concept had become such a hit that I and other Nitro practitioners were approached by publishers to put together a book describing it. I’m helping drive such a project, and we’re looking for co-authors — ping me if you’re interested.</p><p id=\"747f\" class=\"graf graf--p graf-after--p\">It’s a good time to remind all SMACK lovers that Scala By the Bay and Big Data Scala are held at Twitter this year, employing its own SMACK Stack that you can see every day in action — e.g. when you like a tweet and someone gets a notification immediately, for millions of tweets. We’re calling the whole conference Scale+Scala By the Bay, or simply <a href=\"http://scala.bythebay.io\" data-href=\"http://scala.bythebay.io\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">Scalæ By the Bay</a>, and run three tracks over three days, all joined in harmony. The tracks are as follows:</p><p id=\"518f\" class=\"graf graf--p graf-after--p\">— Thoughtful Sofware Engineering with Types</p><p id=\"2ce3\" class=\"graf graf--p graf-after--p\">— Reactive Systems and Streaming</p><p id=\"39b3\" class=\"graf graf--p graf-after--p\">— Data Pipelines for Data Engineering and Machine Learning. This is SMACK Stack in action.</p><p id=\"0f17\" class=\"graf graf--p graf-after--p graf--trailing\">SMACK is a registered trademark of By the Bay, LLC (pending). It was first used in our SMACK training commercially, and nowhere else before it. We trained a hundred people to build a whole big data backend in a day, and they took this knowledge to the world. It’s important this history, however brief, is acknowledged when using the term SMACK stack.</p>",
        "created_at": "2018-08-02T01:44:33+0000",
        "updated_at": "2018-08-02T01:44:45+0000",
        "published_at": "2016-07-11T19:41:56+0000",
        "published_by": [
          "Alexy Khrabrov"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 7,
        "domain_name": "chiefscientist.org",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*THEpX1HaETt6rw6w80-7Fg.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11768"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 23,
            "label": "elasticsearch",
            "slug": "elasticsearch"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1262,
            "label": "time.series",
            "slug": "time-series"
          },
          {
            "id": 1263,
            "label": "bigtable",
            "slug": "bigtable"
          }
        ],
        "is_public": false,
        "id": 11766,
        "uid": null,
        "title": "heroic",
        "url": "https://spotify.github.io/heroic/#!/index",
        "content": null,
        "created_at": "2018-08-02T01:38:02+0000",
        "updated_at": "2018-08-02T01:38:17+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": null,
        "language": null,
        "reading_time": 0,
        "domain_name": "spotify.github.io",
        "preview_picture": null,
        "http_status": null,
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11766"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 23,
            "label": "elasticsearch",
            "slug": "elasticsearch"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1262,
            "label": "time.series",
            "slug": "time-series"
          },
          {
            "id": 1263,
            "label": "bigtable",
            "slug": "bigtable"
          }
        ],
        "is_public": false,
        "id": 11765,
        "uid": null,
        "title": "spotify/heroic",
        "url": "https://github.com/spotify/heroic",
        "content": "<p><a href=\"https://travis-ci.org/spotify/heroic\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/1ff3a276d500da8dc03f2a43b9a1b0dff28420f8/68747470733a2f2f7472617669732d63692e6f72672f73706f746966792f6865726f69632e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/spotify/heroic.svg?branch=master\" /></a>\n<a href=\"https://codecov.io/gh/spotify/heroic\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/7cdbc66d3bc1891024a9ec3ce3fbba2cd9c17cf3/68747470733a2f2f696d672e736869656c64732e696f2f636f6465636f762f632f6769746875622f73706f746966792f6865726f69632e737667\" alt=\"Codecov\" data-canonical-src=\"https://img.shields.io/codecov/c/github/spotify/heroic.svg\" /></a>\n<a href=\"https://github.com/spotify/heroic/blob/master/LICENSE\"><img src=\"https://camo.githubusercontent.com/dd47e65cd53623e602a1491c03d88ff626f291c2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f73706f746966792f6865726f69632e737667\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/spotify/heroic.svg\" /></a>\n<a href=\"https://gitter.im/spotify/heroic\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/0a833308629f72d089428d515a6d3887df94efd1/68747470733a2f2f6261646765732e6769747465722e696d2f73706f746966792f6865726f69632e737667\" alt=\"Join the chat at https://gitter.im/spotify/heroic\" data-canonical-src=\"https://badges.gitter.im/spotify/heroic.svg\" /></a></p><p>A scalable time series database based on Bigtable, Cassandra, and Elasticsearch.\nGo to <a href=\"https://spotify.github.io/heroic/\" rel=\"nofollow\">https://spotify.github.io/heroic/</a> for documentation, please join <code>#heroic at Freenode</code> if you need help or want to chat.</p><p>This project adheres to the <a href=\"https://github.com/spotify/code-of-conduct/blob/master/code-of-conduct.md\">Open Code of Conduct</a>.\nBy participating, you are expected to honor this code.</p><p><strong>Stability Disclaimer:</strong>\nHeroic is an evolving project, and should in its current state be considered <em>unstable</em>.\nDo not use in production unless you are willing to spend time with it, experiment and contribute.\nNot doing so might result in losing your data to goblins. It is currently not on a release schedule and is not versioned. At Spotify we rely on multiple <em>release forks</em> that we actively maintain and flip between.</p><h2>Building</h2><p>Java 8 is required.</p><p>There are some repackaged dependencies that you have to make available, you do\nthis by running <code>tools/install-repackaged</code>.</p><div class=\"highlight highlight-source-shell\"><pre>$ tools/install-repackaged\nInstalling repackaged/x\n...</pre></div><p>After this, the project is built using Maven:</p><div class=\"highlight highlight-source-shell\"><pre>$ mvn package</pre></div><p>This will cause the <code>heroic-dist</code> module to produce a shaded jar that contains\nall required dependencies.</p><h2>Running</h2><p>After building, the entry point of the service is\n<a href=\"https://github.com/spotify/heroic/blob/master/heroic-dist/src/main/java/com/spotify/heroic/HeroicService.java\"><code>com.spotify.heroic.HeroicService</code></a>.\nThe following is an example of how this can be run:</p><pre>$ java -cp $PWD/heroic-dist/target/heroic-dist-0.0.1-SNAPSHOT-shaded.jar com.spotify.heroic.HeroicService &lt;config&gt;\n</pre><p>For help on how to write a configuration file, see the <a href=\"http://spotify.github.io/heroic/#!/docs/config\" rel=\"nofollow\">Configuration Section</a> of the official documentation.</p><p>Heroic has been tested with the following services:</p><h4>Logging</h4><p>Logging is captured using <a href=\"http://www.slf4j.org/\" rel=\"nofollow\">SLF4J</a>, and forwarded to\n<a href=\"http://logging.apache.org/log4j/\" rel=\"nofollow\">Log4j</a>.</p><p>To configure logging, define the <code>-Dlog4j.configurationFile=&lt;path&gt;</code>\nparameter. You can use <a href=\"https://github.com/spotify/heroic/blob/master/docs/log4j2-file.xml\">docs/log4j2-file.xml</a> as a base.</p><h2>Testing</h2><p>We run unit tests with Maven:</p><pre>$ mvn test\n</pre><p>A more comprehensive test suite is enabled with the <code>environment=test</code>\nproperty.</p><pre>$ mvn -D environment=test verify\n</pre><p>This adds:</p><ul><li><a href=\"http://checkstyle.sourceforge.net/\" rel=\"nofollow\">Checkstyle</a></li>\n<li><a href=\"http://findbugs.sourceforge.net/\" rel=\"nofollow\">FindBugs</a></li>\n<li><a href=\"http://maven.apache.org/surefire/maven-failsafe-plugin/\" rel=\"nofollow\">Integration Tests with Maven Failsafe Plugin</a></li>\n<li><a href=\"http://eclemma.org/jacoco/\" rel=\"nofollow\">Coverage Reporting with Jacoco</a></li>\n</ul><p>It is strongly recommended that you run the full test suite before setting up a\npull request, otherwise it will be rejected by Travis.</p><h4>Remote Integration Tests</h4><p>Integration tests are configured to run remotely depending on a set of system\nproperties.</p><h5>Elasticsearch</h5><table><thead><tr><th>Property</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td><code>-D elasticsearch.version=&lt;version&gt;</code></td>\n<td>Use the given client version when building the project</td>\n</tr><tr><td><code>-D it.elasticsearch.remote=true</code></td>\n<td>Run Elasticsearch tests against a remote database</td>\n</tr><tr><td><code>-D it.elasticsearch.seed=&lt;seed&gt;</code></td>\n<td>Use the given seed (default: <code>localhost</code>)</td>\n</tr><tr><td><code>-D it.elasticsearch.clusterName=&lt;clusterName&gt;</code></td>\n<td>Use the given cluster name (default: <code>elasticsearch</code>)</td>\n</tr></tbody></table><h5>Datastax</h5><table><thead><tr><th>Property</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td><code>-D datastax.version=&lt;version&gt;</code></td>\n<td>Use the given client version when building the project</td>\n</tr><tr><td><code>-D it.datastax.remote=true</code></td>\n<td>Run Datastax tests against a remote database</td>\n</tr><tr><td><code>-D it.datastax.seed=&lt;seed&gt;</code></td>\n<td>Use the given seed (default: <code>localhost</code>)</td>\n</tr></tbody></table><h5>Bigtable</h5><table><thead><tr><th>Property</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td><code>-D bigtable.version=&lt;version&gt;</code></td>\n<td>Use the given client version when building the project</td>\n</tr><tr><td><code>-D it.bigtable.remote=true</code></td>\n<td>Run Bigtable tests against a remote database</td>\n</tr><tr><td><code>-D it.bigtable.project=&lt;project&gt;</code></td>\n<td>Use the given project</td>\n</tr><tr><td><code>-D it.bigtable.zone=&lt;zone&gt;</code></td>\n<td>Use the given zone</td>\n</tr><tr><td><code>-D it.bigtable.instance=&lt;instance&gt;</code></td>\n<td>Use the given instance</td>\n</tr><tr><td><code>-D it.bigtable.credentials=&lt;credentials&gt;</code></td>\n<td>Use the given credentials file</td>\n</tr></tbody></table><p>The following is an example Elasticsearch remote integration test:</p><pre>$&gt; mvn -P integration-tests \\\n    -D elasticsearch.version=1.7.5 \\\n    -D it.elasticsearch.remote=true \\\n    clean verify\n</pre><h4>Full Cluster Tests</h4><p>Full cluster tests are defined in <a href=\"https://github.com/spotify/heroic/blob/master/heroic-dist/src/test/java\">heroic-dist/src/test/java</a>.</p><p>This way, they have access to all the modules and parts of Heroic.</p><p>The <a href=\"https://github.com/spotify/heroic/blob/master/rpc/jvm\">JVM RPC</a> module is specifically designed to allow for rapid\nexecution of integration tests. It allows multiple cores to be defined and\ncommunicate with each other in the same JVM instance.</p><h4>Coverage</h4><p><a href=\"https://codecov.io/gh/spotify/heroic/branch/master\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/ba63688333e04608fe9f9cfdadaec029dec32c58/68747470733a2f2f636f6465636f762e696f2f67682f73706f746966792f6865726f69632f6272616e63682f6d61737465722f6772617068732f696369636c652e737667\" alt=\"Coverage\" data-canonical-src=\"https://codecov.io/gh/spotify/heroic/branch/master/graphs/icicle.svg\" /></a></p><p>There's an ongoing project to improve test coverage.\nClicking the above graph will bring you to <a href=\"https://codecov.io/gh/spotify/heroic/branches/master\" rel=\"nofollow\">codecov.io</a>, where you can find areas to focus on.</p><h4>Speedy Building</h4><p>For a speedy build without tests and checks, you can run:</p><div class=\"highlight highlight-source-shell\"><pre>$ mvn -D maven.test.skip=true package</pre></div><h4>Building a Debian Package</h4><p>This project does not provide a single debian package, this is primarily\nbecause the current nature of the service (alpha state) does not mesh well with\nstable releases.</p><p>Instead, you are encouraged to build your own using the provided scripts in\nthis project.</p><p>First run the <code>prepare-sources</code> script:</p><div class=\"highlight highlight-source-shell\"><pre>$ debian/bin/prepare-sources myrel 1</pre></div><p><code>myrel</code> will be the name of your release, it will be part of your package name\n<code>debian-myrel</code>, it will also be suffixed to all helper tools (e.g.\n<code>heroic-myrel</code>).</p><p>For the next step you'll need a Debian environment:</p><div class=\"highlight highlight-source-shell\"><pre>$ dpkg-buildpackage -uc -us</pre></div><p>If you encounter problems, you can troubleshoot the build with <code>DH_VERBOSE</code>:</p><div class=\"highlight highlight-source-shell\"><pre>$ env DH_VERBOSE=1 dpkg-buildpackage -uc -us</pre></div><h2>Contributing</h2><p>Fork the code at <a href=\"https://github.com/spotify/heroic\">https://github.com/spotify/heroic</a></p><p>Make sure you format the code using the provided formatter in <a href=\"https://github.com/spotify/heroic/blob/master/idea\">idea</a>. Even if you disagree\nwith the way it is formatted, consistency is more important.\nFor special cases, see <a href=\"#bypassing-validation\">Bypassing Validation</a>.</p><p>If possible, limit your changes to one module per commit.\nIf you add new, or modify existing classes. Keep that change to a single commit while maintaing\nbackwards compatible behaviour. Deprecate any old APIs as appropriate with <code>@Deprecated</code> and\nadd documentation for how to use the new API.</p><p>The first line of the commit should be formatted with <code>[module1,module2] my message</code>.</p><p><code>module1</code> and <code>module2</code> are paths to the modules affected with any <code>heroic-</code> prefix stripped.\nSo if your change affects <code>heroic-core</code> and <code>metric/bigtable</code>, the message should say\n<code>[core,metric/bigtable] did x to y</code>.</p><p>If more than <em>3 modules</em> are affected by a commit, use <code>[all]</code>.\nFor other cases, adapt to the format of existing commit messages.</p><p>Before setting up a pull request, run the comprehensive test suite as specified in\n<a href=\"#testing\">Testing</a>.</p><h4>Module Orientation</h4><p>The Heroic project is split into a couple of modules.</p><p>The most critical one is <a href=\"https://github.com/spotify/heroic/blob/master/heroic-component\"><code>heroic-component</code></a>. It contains\ninterfaces, value objects, and the basic set of dependencies necessary to glue\ndifferent components together.</p><p>Submodules include <a href=\"https://github.com/spotify/heroic/blob/master/metric\"><code>metric</code></a>, <a href=\"https://github.com/spotify/heroic/blob/master/suggest\"><code>suggest</code></a>,\n<a href=\"https://github.com/spotify/heroic/blob/master/metadata\"><code>metadata</code></a>, and <a href=\"https://github.com/spotify/heroic/blob/master/aggregation\"><code>aggregation</code></a>. The first three\ncontain various implementations of the given backend type, while the latter\nprovides aggregation methods.</p><p><a href=\"https://github.com/spotify/heroic/blob/master/heroic-core\"><code>heroic-core</code></a> contains the\n<a href=\"https://github.com/spotify/heroic/blob/master/heroic-core/src/main/java/com/spotify/heroic/HeroicCore.java\"><code>com.spotify.heroic.HeroicCore</code></a>\nclass which is the central building block for setting up a Heroic instance.</p><p><a href=\"https://github.com/spotify/heroic/blob/master/heroic-elasticsearch-utils\"><code>heroic-elasticsearch-utils</code></a> is a collection of\nutilities for interacting with Elasticsearch. This is separate since we have\nmore than one backend that needs to talk with elasticsearch.</p><p><a href=\"https://github.com/spotify/heroic/blob/master/heroic-parser\"><code>heroic-parser</code></a> provides an Antlr4 implementation of\n<a href=\"https://github.com/spotify/heroic/blob/master/heroic-component/src/main/java/com/spotify/heroic/grammar/QueryParser.java\"><code>com.spotify.heroic.grammar.QueryParser</code></a>,\nwhich is used to parse the Heroic DSL.</p><p><a href=\"https://github.com/spotify/heroic/blob/master/heroic-shell\"><code>heroic-shell</code></a> contains\n<a href=\"https://github.com/spotify/heroic/blob/master/heroic-shell/src/main/java/com/spotify/heroic/HeroicShell.java\"><code>com.spotify.heroic.HeroicShell</code></a>,\na shell capable of either running a standalone, or connecting to an existing\nHeroic instance for administration.</p><p><a href=\"https://github.com/spotify/heroic/blob/master/heroic-all\"><code>heroic-all</code></a> contains dependencies and references to all modules\nthat makes up a Heroic distribution. This is also where profiles are defined\nsince they need to have access to all dependencies.</p><p>Anything in the <a href=\"https://github.com/spotify/heroic/blob/master/repackaged\"><code>repackaged</code></a> directory is dependencies that\ninclude one or more Java packages that must be relocated to avoid conflicts.\nThese are exported under the <code>com.spotify.heroic.repackaged</code> groupId.</p><p>Finally there is <a href=\"https://github.com/spotify/heroic/blob/master/heroic-dist\"><code>heroic-dist</code></a>, a small project that depends on\n<a href=\"https://github.com/spotify/heroic/blob/master/heroic-all\"><code>heroic-all</code></a>, <a href=\"https://github.com/spotify/heroic/blob/master/heroic-shell\"><code>heroic-shell</code></a>, and a logging\nimplementation. Here is where everything is bound together into a distribution\n— a shaded jar. It also provides the entry-point for services, namely\n<a href=\"https://github.com/spotify/heroic/blob/master/heroic-dist/src/main/java/com/spotify/heroic/HeroicService.java\"><code>com.spotify.heroic.HeroicService</code></a>.</p><h4>Bypassing Validation</h4><p>To bypass automatic formatting and checkstyle validation you can use the\nfollowing stanza:</p><div class=\"highlight highlight-source-java\"><pre>// @formatter:off\nfinal List&lt;String&gt; list = ImmutableList.of(\n   \"Welcome to...\",\n   \"... The Wild West\"\n);\n// @formatter:on</pre></div><p>To bypass a FindBugs error, you should use the <code>@SupressFBWarnings</code> annotation.</p><div class=\"highlight highlight-source-java\"><pre>@SupressFBWarnings(value=\"FINDBUGS_ERROR_CODE\", justification=\"I Know Better Than FindBugs\")\npublic class IKnowBetterThanFindbugs() {\n    // ...\n}</pre></div><h2>HeroicShell</h2><p>Heroic comes with a shell that contains many useful tasks, these can either\nbe run in a readline-based shell with some basic completions and history, or\nstandalone.</p><p>You can use the following helper script to run the shell directly from the\nproject.</p><div class=\"highlight highlight-source-shell\"><pre>$ tools/heroic-shell [opts]</pre></div><p>There are a few interesting options available, most notably is <code>--connect</code> that\nallows the shell to connect to a remote heroic instance.</p><p>See <code>-h</code> for a full listing of options.</p><p>You can run individual tasks in <em>standalone</em> mode, giving you a bit more\noptions (like redirecting output) through the following.</p><div class=\"highlight highlight-source-shell\"><pre>$ tools/heroic-shell &lt;heroic-options&gt; -- com.spotify.heroic.shell.task.&lt;task-name&gt; &lt;task-options&gt;</pre></div><p>There are also profiles that can be activated with the <code>-P &lt;profile&gt;</code> switch,\navailable profiles are listed in <code>--help</code>.</p><h2>Repackaged Dependencies</h2><p>These are third-party dependencies that has to be repackaged to avoid binary\nincompatibilities with dependencies.</p><p>Every time these are upgraded, they must be inspected for new conflicts.\nThe easiest way to do this, is to build the project and look at the warnings\nfor the shaded jar.</p><pre>$&gt; mvn clean package -D maven.test.skip=true\n...\n[WARNING] foo-3.5.jar, foo-4.5.jar define 10 overlapping classes:\n[WARNING]   - com.foo.ConflictingClass\n...\n</pre><p>This would indicate that there is a package called foo with overlapping\nclasses.</p><p>You can find the culprit using the <code>dependency</code> plugin.</p><pre>$&gt; mvn package dependency:tree\n</pre>",
        "created_at": "2018-08-02T01:36:20+0000",
        "updated_at": "2018-08-02T01:38:24+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 7,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/251374?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11765"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 207,
            "label": "system",
            "slug": "system"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          },
          {
            "id": 985,
            "label": "cluster",
            "slug": "cluster"
          }
        ],
        "is_public": false,
        "id": 11764,
        "uid": null,
        "title": "riptano/ccm",
        "url": "https://github.com/riptano/ccm",
        "content": "<p>A script/library to create, launch and remove an Apache Cassandra cluster on\nlocalhost.</p><p>The goal of ccm and ccmlib is to make it easy to create, manage and destroy a\nsmall Cassandra cluster on a local box. It is meant for testing a Cassandra cluster.</p><h2>Requirements</h2><ul><li>\n<p>A working python installation (tested to work with python 2.7).</p>\n</li>\n<li>\n<p>pyYAML (<a href=\"http://pyyaml.org/\" rel=\"nofollow\">http://pyyaml.org/</a> -- <code>sudo easy_install pyYaml</code>)</p>\n</li>\n<li>\n<p>six (<a href=\"https://pypi.org/project/six/\" rel=\"nofollow\">https://pypi.org/project/six/</a> -- <code>sudo easy_install six</code>)</p>\n</li>\n<li>\n<p>ant (<a href=\"http://ant.apache.org/\" rel=\"nofollow\">http://ant.apache.org/</a>, on Mac OS X, <code>brew install ant</code>)</p>\n</li>\n<li>\n<p>psutil (<a href=\"https://pypi.org/project/psutil/\" rel=\"nofollow\">https://pypi.org/project/psutil/</a>)</p>\n</li>\n<li>\n<p>Java (which version depends on the version of Cassandra you plan to use. If\nunsure, use Java 7 as it is known to work with current versions of Cassandra).</p>\n</li>\n<li>\n<p>If you want to create multiple node clusters, the simplest way is to use\nmultiple loopback aliases. On modern linux distributions you probably don't\nneed to do anything, but on Mac OS X, you will need to create the aliases with</p>\n<pre>sudo ifconfig lo0 alias 127.0.0.2 up\nsudo ifconfig lo0 alias 127.0.0.3 up\n...\n</pre>\n<p>Note that the usage section assumes that at least 127.0.0.1, 127.0.0.2 and\n127.0.0.3 are available.</p>\n</li>\n</ul><h3>Optional Requirements</h3><ul><li>Paramiko (<a href=\"http://www.paramiko.org/\" rel=\"nofollow\">http://www.paramiko.org/</a>): Paramiko adds the ability to execute CCM\nremotely; <code>pip install paramiko</code></li>\n</ul><p><strong>Note</strong>: The remote machine must be configured with an SSH server and a working\nCCM. When working with multiple nodes each exposed IP address must be\nin sequential order. For example, the last number in the 4th octet of\na IPv4 address must start with <code>1</code> (e.g. 192.168.33.11). See\n<a href=\"https://github.com/riptano/ccm/blob/master/misc/Vagrantfile\">Vagrantfile</a> for help with configuration of remote\nCCM machine.</p><h2>Known issues</h2><p>Windows only:</p><ul><li><code>node start</code> pops up a window, stealing focus.</li>\n<li>cqlsh started from ccm show incorrect prompts on command-prompt</li>\n<li>non nodetool-based command-line options fail (sstablesplit, scrub, etc)</li>\n<li>To install psutil, you must use the .msi from pypi. pip install psutil will not work</li>\n<li>You will need ant.bat in your PATH in order to build C* from source</li>\n<li>You must run with an Unrestricted Powershell Execution-Policy if using Cassandra 2.1.0+</li>\n<li>Ant installed via <a href=\"https://chocolatey.org/\" rel=\"nofollow\">chocolatey</a> will not be found by ccm, so you must create a symbolic\nlink in order to fix the issue (as administrator):\n<ul><li>cmd /c mklink C:\\ProgramData\\chocolatey\\bin\\ant.bat C:\\ProgramData\\chocolatey\\bin\\ant.exe</li>\n</ul></li>\n</ul><p>Remote Execution only:</p><ul><li>Using <code>--config-dir</code> and <code>--install-dir</code> with <code>create</code> may not work as\nexpected; since the configuration directory and the installation directory\ncontain lots of files they will not be copied over to the remote machine\nlike most other options for cluster and node operations</li>\n<li>cqlsh started from ccm using remote execution will not start\nproperly (e.g.<code>ccm --ssh-host 192.168.33.11 node1 cqlsh</code>); however\n<code>-x &lt;CMDS&gt;</code> or <code>--exec=CMDS</code> can still be used to execute a CQLSH command\non a remote node.</li>\n</ul><h2>Installation</h2><p>ccm uses python distutils so from the source directory run:</p><pre>sudo ./setup.py install\n</pre><p>ccm is available on the <a href=\"https://pypi.org/project/ccm/\" rel=\"nofollow\">Python Package Index</a>:</p><pre>pip install ccm\n</pre><p>There is also a <a href=\"https://github.com/Homebrew/homebrew/blob/master/Library/Formula/ccm.rb\">Homebrew package</a> available:</p><pre>brew install ccm\n</pre><h2>Usage</h2><p>Let's say you wanted to fire up a 3 node Cassandra cluster.</p><h3>Short version</h3><pre>ccm create test -v 2.0.5 -n 3 -s\n</pre><p>You will of course want to replace <code>2.0.5</code> by whichever version of Cassandra\nyou want to test.</p><h3>Longer version</h3><p>ccm works from a Cassandra source tree (not the jars). There are two ways to\ntell ccm how to find the sources:</p><ol><li>\n<p>If you have downloaded <em>and</em> compiled Cassandra sources, you can ask ccm\nto use those by initiating a new cluster with:</p>\n<p>ccm create test --install-dir=&lt;path/to/cassandra-sources&gt;</p>\n<p>or, from that source tree directory, simply</p>\n<pre> ccm create test\n</pre>\n</li>\n<li>\n<p>You can ask ccm to use a released version of Cassandra. For instance to\nuse Cassandra 2.0.5, run</p>\n<pre> ccm create test -v 2.0.5\n</pre>\n<p>ccm will download the binary (from <a href=\"http://archive.apache.org/dist/cassandra\" rel=\"nofollow\">http://archive.apache.org/dist/cassandra</a>),\nand set the new cluster to use it. This means\nthat this command can take a few minutes the first time you\ncreate a cluster for a given version. ccm saves the compiled\nsource in <code>~/.ccm/repository/</code>, so creating a cluster for that\nversion will be much faster the second time you run it\n(note however that if you create a lot of clusters with\ndifferent versions, this will take up disk space).</p>\n</li>\n</ol><p>Once the cluster is created, you can populate it with 3 nodes with:</p><pre>ccm populate -n 3\n</pre><p>For Mac OSX, create a new interface for every node besides the first, for example if you populated your cluster with 3 nodes, create interfaces for 127.0.0.2 and 127.0.0.3 like so:</p><pre>sudo ifconfig lo0 alias 127.0.0.2\nsudo ifconfig lo0 alias 127.0.0.3\n</pre><p>Note these aliases will disappear on reboot. For permanent network aliases on Mac OSX see <a target=\"_blank\" href=\"https://github.com/riptano/ccm/blob/master/NETWORK_ALIASES.md\"><img src=\"https://github.com/riptano/ccm/raw/master/NETWORK_ALIASES.md\" alt=\"Network Aliases\" /></a>.</p><p>After that execute:</p><pre>ccm start\n</pre><p>That will start 3 nodes on IP 127.0.0.[1, 2, 3] on port 9160 for thrift, port\n7000 for the internal cluster communication and ports 7100, 7200 and 7300 for JMX.\nYou can check that the cluster is correctly set up with</p><pre>ccm node1 ring\n</pre><p>You can then bootstrap a 4th node with</p><pre>ccm add node4 -i 127.0.0.4 -j 7400 -b\n</pre><p>(populate is just a shortcut for adding multiple nodes initially)</p><p>ccm provides a number of conveniences, like flushing all of the nodes of\nthe cluster:</p><pre>ccm flush\n</pre><p>or only one node:</p><pre>ccm node2 flush\n</pre><p>You can also easily look at the log file of a given node with:</p><pre>ccm node1 showlog\n</pre><p>Finally, you can get rid of the whole cluster (which will stop the node and\nremove all the data) with</p><pre>ccm remove\n</pre><p>The list of other provided commands is available through</p><pre>ccm\n</pre><p>Each command is then documented through the <code>-h</code> (or <code>--help</code>) flag. For\ninstance <code>ccm add -h</code> describes the options for <code>ccm add</code>.</p><h3>Remote Usage (SSH/Paramiko)</h3><p>All the usage examples above will work exactly the same for a remotely\nconfigured machine; however remote options are required in order to establish a\nconnection to the remote machine before executing the CCM commands:</p><table><thead><tr><th align=\"left\">Argument</th>\n<th align=\"left\">Value</th>\n<th align=\"left\">Description</th>\n</tr></thead><tbody><tr><td align=\"left\">--ssh-host</td>\n<td align=\"left\">string</td>\n<td align=\"left\">Hostname or IP address to use for SSH connection</td>\n</tr><tr><td align=\"left\">--ssh-port</td>\n<td align=\"left\">int</td>\n<td align=\"left\">Port to use for SSH connection<br />Default is 22</td>\n</tr><tr><td align=\"left\">--ssh-username</td>\n<td align=\"left\">string</td>\n<td align=\"left\">Username to use for username/password or public key authentication</td>\n</tr><tr><td align=\"left\">--ssh-password</td>\n<td align=\"left\">string</td>\n<td align=\"left\">Password to use for username/password or private key passphrase using public key authentication</td>\n</tr><tr><td align=\"left\">--ssh-private-key</td>\n<td align=\"left\">filename</td>\n<td align=\"left\">Private key to use for SSH connection</td>\n</tr></tbody></table><h4>Special Handling</h4><p>Some commands require files to be located on the remote server. Those commands\nare pre-processed, file transfers are initiated, and updates are made to the\nargument value for the remote execution of the CCM command:</p><table><thead><tr><th align=\"left\">Parameter</th>\n<th align=\"left\">Description</th>\n</tr></thead><tbody><tr><td align=\"left\"><code>--dse-credentials</code></td>\n<td align=\"left\">Copy local DSE credentials file to remote server</td>\n</tr><tr><td align=\"left\"><code>--node-ssl</code></td>\n<td align=\"left\">Recursively copy node SSL directory to remote server</td>\n</tr><tr><td align=\"left\"><code>--ssl</code></td>\n<td align=\"left\">Recursively copy SSL directory to remote server</td>\n</tr></tbody></table><h4>Short Version</h4><pre>ccm --ssh-host=192.168.33.11 --ssh-username=vagrant --ssh-password=vagrant create test -v 2.0.5 -n 3 -i 192.168.33.1 -s\n</pre><p><strong>Note</strong>: <code>-i</code> is used to add an IP prefix during the create process to ensure\nthat the nodes communicate using the proper IP address for their node</p><h3>Source Distribution</h3><p>If you'd like to use a source distribution instead of the default binary each time (for example, for Continuous Integration), you can prefix cassandra version with <code>source:</code>, for example:</p><pre>ccm create test -v source:2.0.5 -n 3 -s\n</pre><h3>Automatic Version Fallback</h3><p>If 'binary:' or 'source:' are not explicitly specified in your version string, then ccm will fallback to building the requested version from git if it cannot access the apache mirrors.</p><h3>Git and GitHub</h3><p>To use the latest version from the <a href=\"https://git-wip-us.apache.org/repos/asf?p=cassandra.git\" rel=\"nofollow\">canonical Apache Git repository</a>, use the version name <code>git:branch-name</code>, e.g.:</p><pre>ccm create trunk -v git:trunk -n 5\n</pre><p>and to download a branch from a GitHub fork of Cassandra, you can prefix the repository and branch with <code>github:</code>, e.g.:</p><pre>ccm create patched -v github:jbellis/trunk -n 1\n</pre><h3>Bash command-line completion</h3><p>ccm has many sub-commands for both cluster commands as well as node commands, and sometimes you don't quite remember the name of the sub-command you want to invoke. Also, command lines may be long due to long cluster or node names.</p><p>Leverage bash's <em>programmable completion</em> feature to make ccm use more pleasant. Copy <code>misc/ccm-completion.bash</code> to somewhere in your home directory (or /etc if you want to make it accessible to all users of your system) and source it in your <code>.bash_profile</code>:</p><pre>. ~/scripts/ccm-completion.bash\n</pre><p>Once set up, <code>ccm sw&lt;tab&gt;</code> expands to <code>ccm switch</code>, for example. The <code>switch</code> sub-command has extra completion logic to help complete the cluster name. So <code>ccm switch cl&lt;tab&gt;</code> would expand to <code>ccm switch cluster-58</code> if cluster-58 is the only cluster whose name starts with \"cl\". If there is ambiguity, hitting <code>&lt;tab&gt;</code> a second time shows the choices that match:</p><pre>$ ccm switch cl&lt;tab&gt;\n    ... becomes ...\n$ ccm switch cluster-\n    ... then hit tab twice ...\ncluster-56  cluster-85  cluster-96\n$ ccm switch cluster-8&lt;tab&gt;\n    ... becomes ...\n$ ccm switch cluster-85\n</pre><p>It dynamically determines available sub-commands based on the ccm being invoked. Thus, users running multiple ccm's (or a ccm that they are continuously updating with new commands) will automagically work.</p><p>The completion script relies on ccm having two hidden subcommands:</p><ul><li>show-cluster-cmds - emits the names of cluster sub-commands.</li>\n<li>show-node-cmds - emits the names of node sub-commands.</li>\n</ul><p>Thus, it will not work with sufficiently old versions of ccm.</p><h2>Remote debugging</h2><p>If you would like to connect to your Cassandra nodes with a remote debugger you have to pass the <code>-d</code> (or <code>--debug</code>) flag to the populate command:</p><pre>ccm populate -d -n 3\n</pre><p>That will populate 3 nodes on IP 127.0.0.[1, 2, 3] setting up the remote debugging on ports 2100, 2200 and 2300.\nThe main thread will not be suspended so you don't have to connect with a remote debugger to start a node.</p><p>Alternatively you can also specify a remote port with the <code>-r</code> (or <code>--remote-debug-port</code>) flag while adding a node</p><pre>ccm add node4 -r 5005 -i 127.0.0.4 -j 7400 -b\n</pre><h2>Where things are stored</h2><p>By default, ccm stores all the node data and configuration files under <code>~/.ccm/cluster_name/</code>.\nThis can be overridden using the <code>--config-dir</code> option with each command.</p><h2>DataStax Enterprise</h2><p>CCM 2.0 supports creating and interacting with DSE clusters. The --dse\noption must be used with the <code>ccm create</code> command. See the <code>ccm create -h</code>\nhelp for assistance.</p><h2>CCM Lib</h2><p>The ccm facilities are available programmatically through ccmlib. This could\nbe used to implement automated tests against Cassandra. A simple example of\nhow to use ccmlib follows:</p><pre>import ccmlib.cluster\nCLUSTER_PATH=\".\"\ncluster = ccmlib.cluster.Cluster(CLUSTER_PATH, 'test', cassandra_version='2.1.14')\ncluster.populate(3).start()\n[node1, node2, node3] = cluster.nodelist()\n# do some tests on the cluster/nodes. To connect to a node through thrift,\n# the host and port to a node is available through\n#   node.network_interfaces['thrift']\ncluster.flush()\nnode2.compact()\n# do some other tests\n# after the test, you can leave the cluster running, you can stop all nodes\n# using cluster.stop() but keep the data around (in CLUSTER_PATH/test), or\n# you can remove everything with cluster.remove()\n</pre><p>--\nSylvain Lebresne <a href=\"mailto:sylvain@datastax.com\">sylvain@datastax.com</a></p>",
        "created_at": "2018-08-02T01:32:06+0000",
        "updated_at": "2018-08-02T01:33:33+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 9,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/275430?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11764"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 10,
            "label": "api",
            "slug": "api"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 941,
            "label": "ruby",
            "slug": "ruby"
          }
        ],
        "is_public": false,
        "id": 11761,
        "uid": null,
        "title": "Cassandra-web by avalanche123",
        "url": "http://avalanche123.com/cassandra-web/",
        "content": "<p>A web interface to Apache Cassandra with AngularJS and server-sent events.</p><h2>\nInstallation</h2><div class=\"highlight highlight-bash\"><pre>$ gem install cassandra-web</pre></div><h2>\nUsage</h2><div class=\"highlight highlight-bash\"><pre>$ cassandra-web</pre></div><p>Run <code>cassandra-web -h</code> for futher help.</p><h2>\nHow it works</h2><p>Cassandra web consists of an HTTP API powered by <a href=\"https://github.com/sinatra/sinatra\">Sinatra</a> and a thin HTML5/JavaScript frontend powered by <a href=\"https://angularjs.org/\">AngularJS</a>.</p><p>When you run <code>cassandra-web</code> script, it starts a <a href=\"http://code.macournoyer.com/thin/\">Thin web server</a> on a specified address, which defaults to <code>localhost:3000</code>. Openning <code>http://localhost:3000</code>, or whatever address you've specified in the browser, loads the AngularJS application and it starts interacting with the HTTP API of <code>cassandra-web</code>. This api uses the <a href=\"http://datastax.github.io/ruby-driver/\">Ruby Driver</a> to communicate with an <a href=\"http://cassandra.apache.org/\">Apache Cassandra</a> cluster.</p><p>When the frontend has fully loaded, <a href=\"https://github.com/avalanche123/cassandra-web/blob/master/app/public/js/cassandra.js#L108\">it subscribes to <code>/events</code> API endpoint</a>, and begins receiving <a href=\"http://www.w3.org/TR/2012/WD-eventsource-20120426/\">Server Sent Events</a>. <a href=\"https://github.com/avalanche123/cassandra-web/blob/master/app/helpers/sse.rb#L43-L56\">The API uses an event listener, which is registered with the <code>Cluster</code> instance created by the Ruby Driver, to stream events</a> such as <a href=\"https://github.com/avalanche123/cassandra-web/blob/master/app/helpers/sse.rb#L29-L39\">schema</a> and <a href=\"https://github.com/avalanche123/cassandra-web/blob/master/app/helpers/sse.rb#L13-L27\">node status</a> changes to update the user interface without having to refresh the page.</p><p>You can see this feature in action by creating a keyspace using the execute button in the top-right corner of the UI and executing the following statement:</p><pre>CREATE KEYSPACE example WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}\n</pre><p>If the statement executed successfully, you should see a new keyspace show up on the left side of the UI.</p><p><img src=\"https://raw.githubusercontent.com/avalanche123/cassandra-web/master/animation.gif\" alt=\"Alt text\" title=\"Create Keyspace\" /></p><p>The web server, Thin, used by <code>cassandra-web</code> is asynchronous and uses only a single thread to handle requests. This enables efficient handling multiple of long running connections, which is a requirement for streaming and Server Sent Events, but also means that the application cannot perform blocking operations during request handling, since it would hang up all connections for the duration of the blocking operation. <code>cassandra-web</code> therefore uses Asynchronous Execution feature of the Ruby Driver to not block on statements execution. <a href=\"https://github.com/avalanche123/cassandra-web/blob/master/app.rb#L88\">The application executes statements asynchronously, receiving a future from the Ruby Driver</a>. <a href=\"https://github.com/avalanche123/cassandra-web/blob/master/app/helpers/async.rb#L7-L40\">It then registers future completion listeners to send a response (or error) whenever it becomes available</a>.</p><h2>\nCredits</h2><p>Cassandra web is possible because of the following awesome technologies (in no particular order):</p><ul><li><a href=\"http://cassandra.apache.org/\">Apache Cassandra</a></li>\n<li><a href=\"http://datastax.github.io/ruby-driver/\">DataStax Ruby Driver for Apache Cassandra</a></li>\n<li><a href=\"https://github.com/sinatra/sinatra\">Sinatra</a></li>\n<li><a href=\"https://angularjs.org/\">AngularJS</a></li>\n<li><a href=\"http://getbootstrap.com/\">Twitter Bootstrap</a></li>\n<li><a href=\"http://code.macournoyer.com/thin/\">Thin</a></li>\n<li><a href=\"http://www.w3.org/TR/2012/WD-eventsource-20120426/\">Server Sent Events</a></li>\n<li><a href=\"http://prismjs.com/\">PrismJS</a></li>\n<li><a href=\"http://codemirror.net/\">CodeMirror</a></li>\n<li>and many others...</li>\n</ul>",
        "created_at": "2018-08-02T00:25:15+0000",
        "updated_at": "2018-08-02T00:25:24+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 1,
        "domain_name": "avalanche123.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11761"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 35,
            "label": "docker",
            "slug": "docker"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 996,
            "label": "monitoring",
            "slug": "monitoring"
          },
          {
            "id": 1025,
            "label": "grafana",
            "slug": "grafana"
          },
          {
            "id": 1108,
            "label": "graphite",
            "slug": "graphite"
          }
        ],
        "is_public": false,
        "id": 11740,
        "uid": null,
        "title": "SoftwareMill blog: Cassandra Monitoring - part II - Graphite/InfluxDB & Grafana on Docker",
        "url": "https://softwaremill.com/cassandra-monitoring-part-2/",
        "content": "<p><em>This is the second part of the Cassandra Monitoring miniseries, index of all parts below:</em></p><ol><li><em><a href=\"https://softwaremill.com/cassandra-monitoring-part-1/\">Cassandra Monitoring - part I - Introduction</a></em></li>\n<li><em><a href=\"https://softwaremill.com/cassandra-monitoring-part-2/\">Cassandra Monitoring - part II - Graphite/InfluxDB &amp; Grafana on Docker</a></em></li>\n</ol><p>In this blogpost we will continue exploring the topic of <a href=\"http://cassandra.apache.org/\">Cassandra</a> metric reporters mentioned in <a href=\"https://softwaremill.com/cassandra-monitoring-part-1/\">Part I</a>. Our goal is to configure a reporter that sends metrics to an external time series database. For visualization we will use <a href=\"http://grafana.org/\">Grafana</a>, which can read data directly from various time series databases. We are going to heavily leverage <a href=\"https://www.docker.com/\">Docker</a>, so that we can omit the irrelevant setup details of various projects. To make it easier to set up a full working example, we have prepared a <a href=\"https://www.docker.com/products/docker-compose\">Docker Compose</a> script in our <a href=\"https://github.com/softwaremill/cassandra-monitoring\">GitHub repository</a>.</p><p>As a prerequisite for following this post, please install <a href=\"https://docs.docker.com/engine/installation/\">Docker</a> and <a href=\"https://docs.docker.com/compose/\">Docker Compose</a> on your machine. For instructions please see the linked docs.</p><p>We are going to describe two configuration variants - Cassandra-Graphite-Grafana and Cassandra-InfluxDB-Grafana over the Graphite protocol.</p><h2 id=\"network\">Network</h2><p>Both variants will require 3 containers each. Cassandra and Grafana will need access to the time series store. Links between Docker containers are currently a <a href=\"https://docs.docker.com/v1.11/engine/userguide/networking/default_network/dockerlinks/\">legacy</a> feature, so we are going to use <a href=\"https://docs.docker.com/v1.11/engine/userguide/networking/dockernetworks/\">Docker networking</a>. We are going to create just one network and attach all containers to it. Each of the containers will be accessible in our network under a hostname identical to their name.</p><p>To create the network you need to run:</p><pre>docker network create monitoring-network\n</pre><p>Note:\nIn our Docker Compose example scripts we do not set up a network explicitly, because it generates one <a href=\"https://docs.docker.com/v1.11/compose/networking/\">automatically</a> for the application.</p><p>Let's start with setting up a time series database. We will now describe two mutually exclusive configuration options. Remember that you only need one of them to get the monitoring working.</p><h3 id=\"graphite\">Graphite</h3><p>Unfortunately there is no official <a href=\"https://graphiteapp.org/\">Graphite</a> Docker image, so we have to use one of the non-official ones from <a href=\"https://hub.docker.com/r/sitespeedio/graphite/\">Docker Hub</a>.</p><p>In order to run the selected image, execute:</p><pre>docker run -d  -p 8080:80 -p 2003:2003 --net monitoring-network \\\n    --name graphite sitespeedio/graphite:0.9.14\n</pre><p>This means that we are running the <code>sitespeedio/graphite:0.9.14</code> image, creating a container named <code>graphite</code>, attached to network <code>monitoring-network</code>, mapping the container’s internal ports to local <code>8080</code> and <code>2003</code>. Everything is going to work in the background (<code>-d</code>).</p><p>The <code>http://localhost:8080</code> presents a simple web interface. Port <code>2003</code> is for the API. Default user and password is <code>guest</code> and <code>guest</code>.</p><p>Graphite installation without Docker is a bit complicated, so one of the Graphite developers has created a separate <a href=\"https://github.com/obfuscurity/synthesize\">project</a> purely for making the installation easier.</p><p><img src=\"https://softwaremill.com/images/uploads/2016/07/cassandra-monitoring-2b-graphite.71ca7b7e.png\" alt=\"Graphite Web UI\" /></p><p>Although the UI is a bit outdated, it is still capable of drawing graphs; moreover, Graphite itself also provides a graph rendering API. Graphite is a very <a href=\"http://graphite.readthedocs.io/en/latest/overview.html\">powerful</a> tool and it <a href=\"https://graphite.readthedocs.io/en/0.9.15/functions.html\">implements numerous functions</a> which can be applied to the data.</p><p>Messages sent to Graphite have a very simple format: <code>metric_path value timestamp\\n</code>. This means that a single metric path can have only a single value for a given timestamp.</p><h3 id=\"influxdb\">InfluxDB</h3><p><a href=\"https://influxdata.com/time-series-platform/influxdb/\">InfluxDB</a> is a relatively new <a href=\"https://github.com/influxdata/influxdb\">open source</a> time series database written in <a href=\"https://golang.org/\">Go</a>. The 1.0 release is planned for summer 2016. Official InfluxDB Docker images are available on <a href=\"https://hub.docker.com/_/influxdb/\">Docker Hub</a>. In this post we are using InfluxDB 0.13.0.</p><p>In our demonstration we are going to report data from Cassandra using the Graphite format, so we need to enable InfluxDB support for receiving data in this format. In order to do that, we have to pass one additional environment variable when creating the container:</p><pre>docker run -d -p 8083:8083 -p 8086:8086 \\\n    -e INFLUXDB_GRAPHITE_ENABLED=true \\\n    --net monitoring-network  \\\n    --name influxdb  \\\n    influxdb:0.13.0\n</pre><p>The <code>http://localhost:8083</code> presents a simple web UI. Port <code>8086</code> is for the API. Default user and password is <code>admin</code> and <code>admin</code>.</p><p><img src=\"https://softwaremill.com/images/uploads/2016/07/cassandra-monitoring-2b-influxdb.38631e4c.png\" alt=\"InfluxDB Web UI\" /></p><p>InfluxDB <a href=\"https://docs.influxdata.com/influxdb/v0.13/concepts/key_concepts/\">data model</a> can be more complex than Graphite. For a given timestamp and measurement name, it is possible to store multiple fields and, additionally, every measurement can be described using multiple tags. Tags are indexed, so it is possible to query measurements efficiently, e.g. for a specific node or environment. However, in this blog post we are configuring InfluxDB to receive data using the Graphite protocol, so it does not leverage all these features. A single measurement for a given timestamp only contains one column named <code>value</code>.</p><p>Querying data stored in InfluxDB is possible via <a href=\"https://docs.influxdata.com/influxdb/v0.13/query_language/data_exploration/\">InfluxQL</a>, which is an SQL-like language.</p><h2 id=\"cassandra\">Cassandra</h2><p>Official Cassandra images exist on <a href=\"https://hub.docker.com/_/cassandra/\">Docker Hub</a> - however,  they do not satisfy our needs. There are 3 things which need to be done in order to report metrics from Cassandra to an external system:</p><ol><li>Cassandra distribution is minimalistic and does not include the JAR used for Graphite reporting. For Cassandra &gt;= 2.2 the <a href=\"http://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-graphite/3.1.0/metrics-graphite-3.1.0.jar\"><code>metrics-graphite-3.1.0.jar</code></a> is required and for &lt;= 2.1 the <a href=\"http://repo1.maven.org/maven2/com/yammer/metrics/metrics-graphite/2.2.0/metrics-graphite-2.2.0.jar\"><code>metrics-graphite-2.2.0.jar</code></a>.</li>\n<li>Reporting <a href=\"https://github.com/apache/cassandra/blob/trunk/conf/metrics-reporter-config-sample.yaml\">configuration</a> needs to be created.</li>\n<li>Edit <code>cassandra-env.sh</code> in order to add the following line: <code>JVM_OPTS=\"$JVM_OPTS -Dcassandra.metricsReporterConfigFile=&lt;reporting-configuration&gt;.yaml\"</code> with the path to the configuration created in the previous step.</li>\n</ol><p><br />In this example we are going to use Cassandra 3.7.</p><p>Adding required files is pretty simple when you use a standalone Cassandra distribution; however, for Docker we need to create a new image. We will go with a \"minimalistic\" approach and extend the official one. To build the image, you need to create a directory in which we will place a <code>Dockerfile</code>, reporting configuration and the appropriate <code>.jar</code>.</p><h3 id=\"cassandra-graphite-grafana-\">Cassandra-Graphite-Grafana:</h3><p><code>Dockerfile</code>:</p><pre>FROM cassandra:3.7\nCOPY graphite.yaml /etc/cassandra/\nRUN echo \"JVM_OPTS=\\\"\\$JVM_OPTS -Dcassandra.metricsReporterConfigFile=graphite.yaml\\\"\" \\\n    &gt;&gt; /etc/cassandra/cassandra-env.sh\nCOPY metrics-graphite-3.1.0.jar /usr/share/cassandra/lib/\n</pre><p><code>graphite.yaml</code>:</p><pre>graphite:\n-\n  period: 60\n  timeunit: 'SECONDS'\n  prefix: 'Node1'\n  hosts:\n  - host: 'graphite'\n    port: 2003\n  predicate:\n    color: \"white\"\n    useQualifiedName: true\n    patterns:\n    - \".*\"\n</pre><h3 id=\"cassandra-influxdb-grafana-over-graphite-protocol-\">Cassandra-InfluxDB-Grafana over Graphite protocol:</h3><p><code>Dockerfile</code>:</p><pre>FROM cassandra:3.7\nCOPY influxdb.yaml /etc/cassandra/\nRUN echo \"JVM_OPTS=\\\"\\$JVM_OPTS -Dcassandra.metricsReporterConfigFile=influxdb.yaml\\\"\" \\\n    &gt;&gt; /etc/cassandra/cassandra-env.sh\nCOPY metrics-graphite-3.1.0.jar /usr/share/cassandra/lib/\n</pre><p><code>influxdb.yaml</code>:</p><pre>graphite:\n-\n  period: 60\n  timeunit: 'SECONDS'\n  prefix: 'Node1'\n  hosts:\n  - host: 'influxdb'\n    port: 2003\n  predicate:\n    color: \"white\"\n    useQualifiedName: true\n    patterns:\n    - \".*\"\n</pre><h3 id=\"differences\">Differences</h3><p>Both of these setups use the Graphite protocol, so they are almost identical. The only difference is the hostname of the time series database. For Graphite, we reference the <code>graphite</code> container via its hostname and, for InfluxDB, the <code>influxdb</code> hostname. The <code>prefix</code> value will be prepended to every metric name - this way you can make metrics coming from different nodes distinguishable.</p><p>Additionally, in both cases you need to put the <a href=\"http://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-graphite/3.1.0/metrics-graphite-3.1.0.jar\"><code>metrics-graphite-3.1.0.jar</code></a> in the same directory with the <code>Dockerfile</code> and reporting configuration.</p><p>Both of these Cassandra scenarios are available on GitHub\nfor <a href=\"https://github.com/softwaremill/cassandra-monitoring/tree/master/part-II-cassandra-graphite-grafana/cassandra-graphite\">Graphite</a> and for <a href=\"https://github.com/softwaremill/cassandra-monitoring/tree/master/part-II-cassandra-influx-via-graphite-grafana/cassandra-influx-via-graphite\">InfluxDB over Graphite protocol</a>.</p><h3 id=\"running\">Running</h3><p>In order to build the image, you have to execute the following command in the directory with the <code>Dockerfile</code>:</p><pre>docker build -t cassandra-graphite .\n</pre><p>Then create the container and attach it to our network:</p><pre>docker run -d -p 9042:9042 --net monitoring-network \\\n    --name cassandra-graphite cassandra-graphite\n</pre><p>Please watch Cassandra logs for errors with <code>docker logs -f cassandra-graphite</code>. Every 60 seconds Cassandra should report its metrics to the corresponding time series database. You can check it in the web UI or using the CLI.</p><p>For Graphite, open <code>http://localhost:8080</code> and look at the tree on the left side:</p><p><img src=\"https://softwaremill.com/images/uploads/2016/07/cassandra-monitoring-2b-graphite-with-metrics.4770ac5d.png\" alt=\"Graphite with Metics\" /></p><p>For InfluxDB, open <code>http://localhost:8083</code>, in the right upper corner set the <code>graphite</code> database (the database created by InfluxDB-Graphite integration is by default called <code>graphite</code>) and execute <code>SHOW MEASUREMENTS</code> query:</p><p><img src=\"https://softwaremill.com/images/uploads/2016/07/cassandra-monitoring-2b-influx-with-metrics.1db87f9e.png\" alt=\"InfluxDB with Metics\" /></p><h2 id=\"grafana\">Grafana</h2><p>Fortunately for Grafana there are official images on <a href=\"https://hub.docker.com/r/grafana/grafana/\">Docker Hub</a>. In this post we are using Grafana 3.1.0. We already have Cassandra running, metrics are being stored in Graphite or InfluxDB, so as the last step we need to visualize them.</p><p>In order to run Grafana execute:</p><pre>docker run -d -p 3000:3000 --net monitoring-network --name grafana grafana/grafana:3.1.0\n</pre><p>Grafana UI should be working under <code>http://localhost:3000</code>. Default user and password is <code>admin</code> and <code>admin</code>.  If you haven't used Grafana before see the <a href=\"http://docs.grafana.org/guides/gettingstarted/\">Getting started</a> guide for UI walkthrough.</p><p>In order to draw a graph you first need to create a Data Source referring to the specific time series database. You can click it through Grafana UI or just execute the API e.g. using <code>curl</code>:</p><h3>Graphite</h3><pre>curl 'http://admin:admin@127.0.0.1:3000/api/datasources' -X POST \\\n-H 'Content-Type: application/json;charset=UTF-8' \\\n--data-binary '{\"name\":\"graphite\",\"type\":\"graphite\",\"url\":\"http://graphite:80\",\n\"access\":\"proxy\",\"isDefault\":true,\n\"basicAuth\":true,\"basicAuthUser\":\"guest\",\"basicAuthPassword\":\"guest\"}'\n</pre><p>Url <code>http://graphite:80</code> refers to Graphite container hostname.</p><h3>InfluxDB</h3><pre>curl 'http://admin:admin@127.0.0.1:3000/api/datasources' -X POST \\\n-H 'Content-Type: application/json;charset=UTF-8' \\\n--data-binary '{\"name\":\"influx\",\"type\":\"influxdb\",\"url\":\"http://influxdb:8086\",\n\"access\":\"proxy\",\"isDefault\":true,\"database\":\"graphite\",\"user\":\"admin\",\"password\":\"admin\"}'\n</pre><p>Url <code>http://influxdb:8086</code> refers to InfluxDB container hostname.</p><h3 id=\"graphs\">Graphs</h3><p>Query editors in Grafana differ among various data sources. As you can see below, for Graphite the editor does not display the whole metric list at once, but allows to browse it in a tree-like manner, similarly to the Graphite UI. It is possible to apply a function to a chosen metric. In contrast, for the InfluxDB data source Grafana offers a more SQL-like editor, where all metrics for all nodes are shown in a single list.</p><p><img src=\"https://softwaremill.com/images/uploads/2016/07/cassandra-monitoring-2b-grafana-graphite.de2257ab.png\" alt=\"Graphite Graphing\" />\nGraphite</p><p><img src=\"https://softwaremill.com/images/uploads/2016/07/cassandra-monitoring-2b-grafana-influx.4141607b.png\" alt=\"InfluxDB Graphing\" />\nInfluxDB</p><p>We will focus more on graphing in one of the next parts of this blog series.</p><p>This post does not present a full, production ready configuration. When using Docker, it is important to remember that the data stored inside the container might be lost e.g. when a new version of an application is deployed. That is why it is recommended to use <a href=\"https://docs.docker.com/engine/tutorials/dockervolumes/\">Docker Volumes</a> for storing data. For more details please also read the docs of the specific Docker images.</p><p>Additionally be aware that by default Docker <a href=\"http://blog.viktorpetersson.com/post/101707677489/the-dangers-of-ufw-docker\">adds its own <code>iptables</code> rules</a> potentially making the ports exposed by a container accessible from remote hosts.</p><p>In production you must never use default passwords, always remember to change them!</p><p>Monitoring Cassandra using Graphite or InfluxDB (with data sent over Graphite protocol) is very similar. There are differences on the Grafana level, where Graphite integration seems to be a bit more mature. However, Graphite development was practically <a href=\"https://news.ycombinator.com/item?id=8740021\">dead</a> over the last years. Grafana creators have recently posted <a href=\"http://grafana.org/blog/2016/07/06/dear-graphite-users-and-developers.html\">a piece of information</a> that they want to revive Graphite. In contrast, InfluxDB undergoes a very intensive development process, which e.g. improves performance with almost every release.</p><p>If you'd like to quickly run one of the described variants, see our <a href=\"https://github.com/softwaremill/cassandra-monitoring\">GitHub</a> leveraging <a href=\"https://www.docker.com/products/docker-compose\">Docker Compose</a>.</p><p>In the next part of this series we will focus on the Cassandra-InfluxDB-Grafana stack. We will show improvements over Cassandra-InfluxDB reporting (omitting Graphite protocol) and the resulting advantages on the Grafana level.</p>",
        "created_at": "2018-08-01T01:13:43+0000",
        "updated_at": "2018-08-01T01:13:58+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 9,
        "domain_name": "softwaremill.com",
        "preview_picture": "https://softwaremill.com/images/uploads/2016/07/cassandra-monitoring-2b-graphite.71ca7b7e.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11740"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 996,
            "label": "monitoring",
            "slug": "monitoring"
          }
        ],
        "is_public": false,
        "id": 11739,
        "uid": null,
        "title": "SoftwareMill blog: Cassandra Monitoring - part I - Introduction",
        "url": "https://softwaremill.com/cassandra-monitoring-part-1/",
        "content": "<p><em>This is the first part of the Cassandra Monitoring miniseries, index of all parts below:</em></p><ol><li><em><a href=\"https://softwaremill.com/cassandra-monitoring-part-1/\">Cassandra Monitoring - part I - Introduction</a></em></li>\n<li><em><a href=\"https://softwaremill.com/cassandra-monitoring-part-2/\">Cassandra Monitoring - part II - Graphite/InfluxDB &amp; Grafana on Docker</a></em></li>\n</ol><p>In this series we would like to focus on the Cassandra NoSQL database monitoring. If you would like to read more about general metric collection then you can find a great post on the <a href=\"https://www.datadoghq.com/blog/monitoring-101-collecting-data/\">DataDog Blog</a>. Here, we are not going to focus on <strong>what</strong> specifically you can gather from Cassandra, but rather <strong>how</strong>. Again, for details about different Cassandra metrics see the <a href=\"https://www.datadoghq.com/blog/how-to-monitor-cassandra-performance-metrics/\">another DataDog  blogpost</a>.\nIn the upcoming parts we are also going to present our open source contributions which make Cassandra monitoring easier and more effective.</p><p>Everybody who uses Cassandra knows <a href=\"http://docs.datastax.com/en/cassandra/3.x/cassandra/tools/toolsNodetool.html\"><code>nodetool</code></a>. It is a basic tool, bundled in the Cassandra distribution, for node management and <a href=\"http://docs.datastax.com/en/cassandra/3.0/cassandra/operations/opsMonitoring.html?scroll=opsMonitoring__opsMonitoringNodetool\">statistics gathering</a>. Under the hood it is just a Python console application. Nodetool shows <a href=\"https://www.datadoghq.com/blog/how-to-monitor-cassandra-performance-metrics/\">cluster status, compactions, bootstrap streams and much more</a>. It is a very important source of information, but it's just a CLI tool without any storage or visualization capabilities. For comfortable monitoring, and to get a better understanding of what hides behind all these numbers, we need something more, preferably with a GUI.</p><p>It is worth noting that Cassandra commiters <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-11939\">find it important</a> to not change output structure of <code>nodetool</code>, because people might have scripts based on them.</p><p><img src=\"https://softwaremill.com/images/uploads/2016/07/cassandra-monitoring-1-nodetool.b6516be0.png\" alt=\"Nodetool\" /></p><h2 id=\"jmx-reporters\">JMX &amp; Reporters</h2><p>Cassandra exposes all its metrics via JMX (by default on port <code>7199</code>).</p><p>JMX can be read e.g. with <code>jconsole</code> or <code>jvisualvm</code> with <code>VisualVM-MBeans plugin</code> (both tools bundled in JDK distributions).\nThe JMX interface also offers some management features! For example under <code>org.apache.cassandra.db.StorageService</code> you can find operations related to node removal, drain, table snapshoting and more.</p><p><img src=\"https://softwaremill.com/images/uploads/2016/07/cassandra-monitoring-1-jmx.21763bc4.png\" alt=\"JMX operations\" /></p><p>Note: by default remote JMX is disabled. If you really need it, you can enable it in <code>cassandra-env.sh</code>.</p><p>For metrics gathering Cassandra internally leverages <a href=\"http://metrics.dropwizard.io/\"><code>io.dropwizard.metrics</code></a>  (only from version <code>2.2</code>, previously library was named <code>com.yammer.metrics</code> and to be more confusing  <code>io.dropwizard.metrics</code> uses <code>com.codahale.metrics</code> package names). Those are the metrics presented via JMX. However, it is possible to access them in a different way. Cassandra 2.0.2 and up allows to configure reporters, so that every configured period Cassandra forwards those metrics e.g. to <a href=\"https://graphiteapp.org/\">Graphite</a>. This is implemented by <a href=\"https://github.com/addthis/metrics-reporter-config\"><code>metrics-reporter-config</code></a> library  (see <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-4430\">CASSANDRA-4430</a>) and provides a nice automatic way to process metrics in different systems, store and display them or check for alarms.</p><p>We will cover the concept of reporters in more detail, in the next part of this blogpost series.</p><h2 id=\"datastax-opscenter\">DataStax OpsCenter</h2><p><a href=\"http://www.datastax.com/products/datastax-opscenter\">OpsCenter</a> is a monitoring and management solution. It is also capable of system monitoring. Every node needs to have an OpsCenter agent installed, which sends data to the main OpsCenter service, which in turn stores them in a Cassandra keyspace. It is recommended to have a separate Cassandra cluster for storing OpsCenter data, so that OpsCenter activity won't be seen among the presented metrics. The application is also able to manage the cluster, add/remove nodes and more. However, the \"free\" OpsCenter is compatible with the open source Cassandra up to version 2.1. The new OpsCenter 6.0 is available only for DataStax Enterprise 4.7+ (based on Cassandra 2.1) and 5.0 (based on Cassandra 3.0). <a href=\"http://docs.datastax.com/en/landing_page/doc/landing_page/compatibility.html?scroll=compatibilityDocument__opsc-compatibility\">The Documentation</a> shows more detailed compatibility matrix.</p><p>In other words if your cluster uses open source Cassandra 2.2 or 3.x then OpsCenter is not for you.</p><p><img src=\"https://softwaremill.com/images/uploads/2016/07/cassandra-monitoring-1-opscenter.2a9791e8.jpg\" alt=\"OpsCenter\" /><a href=\"http://www.datastax.com/wp-content/themes/datastax-2014-08/images/products/OpsCenter-Screenshot-VisualMonitoringandTuning.jpg\">Source</a></p><h2 id=\"datadog\">DataDog</h2><p>In contrast, <a href=\"https://www.datadoghq.com/\">DataDog</a> is a SaaS solution. It is capable of monitoring, but also supports a lot of other databases and services. However it is <a href=\"https://www.datadoghq.com/pricing/\">free</a> for only no more than 5 hosts with major <a href=\"https://www.datadoghq.com/pricing/\">limitations</a>.</p><p>DataDog requires an agent installed on every Cassandra node. It reads Cassandra <a href=\"https://github.com/DataDog/dd-agent/blob/master/dogstream/cassandra.py\">logs</a> and metrics using JMX. The agent is <a href=\"https://github.com/DataDog/dd-agent\">open source</a> so you can check what exactly it’s doing.</p><p><img src=\"https://softwaremill.com/images/uploads/2016/07/cassandra-monitoring-1-datadog.acf7c047.png\" alt=\"DataDog\" /><a href=\"https://www.datadoghq.com/blog/how-to-monitor-cassandra-performance-metrics/\">Source</a></p><h2 id=\"conclusions\">Conclusions</h2><p>There are a lot of options for Cassandra monitoring (and management), however none of them are perfect. If you are still using open source Cassandra 2.1 or below, or DataStax Enterprise, then you can use OpsCenter. If you are open to Cloud and SaaS then DataDog monitoring might be for you. Otherwise, you might be interested in Cassandra reporters and solutions based on <a href=\"https://graphiteapp.org/\">Graphite</a> or <a href=\"https://influxdata.com/time-series-platform/influxdb/\">InfluxDB</a> and <a href=\"http://grafana.org/\">Grafana</a> which we will describe in the <a href=\"https://softwaremill.com/cassandra-monitoring-part-2/\">next parts</a> of this blog series. We will compare the different options and show how to configure them for different Cassandra versions.</p><p>If you want to dive deeper into the topic of metrics, then these links might be interesting for you (some quoted already in the article):</p><ul><li><a href=\"https://www.datadoghq.com/blog/monitoring-101-collecting-data/\">Monitoring 101: Collecting the right data</a></li>\n<li><a href=\"https://www.datadoghq.com/blog/how-to-monitor-cassandra-performance-metrics/\">How to monitor Cassandra performance metrics</a></li>\n<li><a href=\"https://www.datadoghq.com/blog/how-to-collect-cassandra-metrics/\">How to collect Cassandra metrics</a></li>\n<li><a href=\"https://www.datadoghq.com/blog/monitoring-cassandra-with-datadog/\">Monitoring Cassandra with Datadog</a></li>\n<li><a href=\"https://medium.com/@mlowicki/alternatives-to-datastax-opscenter-8ad893efe063#.wpetbdrj9\">Alternatives to DataStax OpsCenter</a></li>\n<li><a href=\"https://medium.com/@mlowicki/cassandra-metrics-and-their-use-in-grafana-1f0dc33f9cca#.37bkpooc4\">Cassandra metrics and their use in Grafana</a></li>\n</ul>",
        "created_at": "2018-08-01T01:13:35+0000",
        "updated_at": "2018-08-01T01:13:40+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 4,
        "domain_name": "softwaremill.com",
        "preview_picture": "https://softwaremill.com/images/uploads/2016/07/cassandra-monitoring-1-nodetool.b6516be0.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11739"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 12,
            "label": "video",
            "slug": "video"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 11658,
        "uid": null,
        "title": "DuyHai Doan - New Cassandra 3 features that change your (developer) life",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=qihbEBBs8mI",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/qihbEBBs8mI?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-07-28T16:46:32+0000",
        "updated_at": "2018-07-28T16:46:44+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/qihbEBBs8mI/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11658"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 894,
            "label": "distributed",
            "slug": "distributed"
          },
          {
            "id": 909,
            "label": "platform",
            "slug": "platform"
          }
        ],
        "is_public": false,
        "id": 11657,
        "uid": null,
        "title": "DuyHai DOAN - Big Data 101, all the foundations you need to bootstrap a new project in 2017",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=rmAnni-jajA",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/rmAnni-jajA?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-07-28T16:45:20+0000",
        "updated_at": "2018-07-28T16:45:36+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/rmAnni-jajA/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11657"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 391,
            "label": "big.data",
            "slug": "big-data"
          }
        ],
        "is_public": false,
        "id": 11656,
        "uid": null,
        "title": "DuyHai DOAN - Big Data 101, all the foundations you need to bootstrap a new project in 2017",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=rmAnni-jajA",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/rmAnni-jajA?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-07-28T16:45:15+0000",
        "updated_at": "2018-08-07T22:27:29+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/rmAnni-jajA/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11656"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1233,
            "label": "data.modeling",
            "slug": "data-modeling"
          }
        ],
        "is_public": false,
        "id": 11612,
        "uid": null,
        "title": "A Shortcut to Awesome: Cassandra Data Modeling By Example (Jon Haddad…",
        "url": "https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016",
        "content": "A Shortcut to Awesome: Cassandra Data Modeling By Example (Jon Haddad…\n      \n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            </a><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div>\n    <div class=\"wrapper\"><p>Successfully reported this slideshow.</p><div id=\"slideview-container\" class=\"\"><div class=\"row\"><div id=\"main-panel\" class=\"small-12 large-8 columns\"><div class=\"sectionElements\"><div class=\"playerWrapper\"><div><div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\">A Shortcut to Awesome: Cassandra Data Modeling By Example (Jon Haddad, The Last Pickle) | C* Summit 2016<div class=\"stage valign-first-slide\"><div class=\"slide_container\"><section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-1-638.jpg?cb=1474068828\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-1-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-1-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-1-1024.jpg?cb=1474068828\" alt=\"JON HADDAD&#10;THE LAST PICKLE&#10;LEARN DATA MODELING BY EXAMPLE&#10;THIS IS&#10;AWESOME!!!&#10;\" /></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-2-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-2-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-2-1024.jpg?cb=1474068828\" alt=\"WHAT’S THE LAST&#10;PICKLE DO?&#10;\" /></i></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-3-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-3-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-3-1024.jpg?cb=1474068828\" alt=\"WE HELP MAKE YOU A&#10;TEAM OF EXPERTS&#10;\" /></i></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-4-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-4-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-4-1024.jpg?cb=1474068828\" alt=\"&gt; 50 YEARS COMBINED&#10;EXPERIENCE&#10;\" /></i></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-5-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-5-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-5-1024.jpg?cb=1474068828\" alt=\"WHO IS THIS GUY?&#10;\" /></i></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-6-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-6-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-6-1024.jpg?cb=1474068828\" alt=\"15 YEARS&#10;EXPERIENCE&#10;\" /></i></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-7-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-7-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-7-1024.jpg?cb=1474068828\" alt=\"4 YEARS WITH CASSANDRA&#10;\" /></i></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-8-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-8-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-8-1024.jpg?cb=1474068828\" alt=\"LEARNING HOW TO&#10;CASSANDRA&#10;\" /></i></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-9-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-9-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-9-1024.jpg?cb=1474068828\" alt=\"WHAT’S YOUR&#10;BACKGROUND?&#10;\" /></i></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-10-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-10-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-10-1024.jpg?cb=1474068828\" alt=\"ORACLE!&#10;MYSQL!&#10;POSTGRES!&#10;\" /></i></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-11-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-11-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-11-1024.jpg?cb=1474068828\" alt=\"CQL LOOKS&#10;LIKE SQL&#10;\" /></i></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-12-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-12-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-12-1024.jpg?cb=1474068828\" alt=\"BAD&#10;ASSUMPTIONS&#10;\" /></i></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-13-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-13-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-13-1024.jpg?cb=1474068828\" alt=\"3RD&#10;NORMAL&#10;FORM?&#10;\" /></i></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-14-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-14-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-14-1024.jpg?cb=1474068828\" alt=\"WHERE’S&#10;MY&#10;JOINS?&#10;\" /></i></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-15-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-15-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-15-1024.jpg?cb=1474068828\" alt=\"SECONDARY&#10;INDEX?&#10;\" /></i></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-16-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-16-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-16-1024.jpg?cb=1474068828\" alt=\"DO IT WRONG&#10;TRY TO DATA MODELGET ANGRY&#10;WATCH VIDEOS &amp; READ&#10;\" /></i></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-17-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-17-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-17-1024.jpg?cb=1474068828\" alt=\"EVERYTHING I&#10;KNOW IS WRONG&#10;\" /></i></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-18-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-18-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-18-1024.jpg?cb=1474068828\" alt=\"LEARN BY&#10;EXAMPLE&#10;\" /></i></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-19-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-19-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-19-1024.jpg?cb=1474068828\" alt=\"CASSANDRA DATASET&#10;MANAGER&#10;\" /></i></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-20-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-20-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-20-1024.jpg?cb=1474068828\" alt=\"CDM&#10;\" /></i></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-21-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-21-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-21-1024.jpg?cb=1474068828\" alt=\"APT FOR CASSANDRA&#10;DATA&#10;\" /></i></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-22-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-22-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-22-1024.jpg?cb=1474068828\" alt=\"INSTALL DATA TO YOUR&#10;CASSANDRA CLUSTER&#10;\" /></i></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-23-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-23-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-23-1024.jpg?cb=1474068828\" alt=\"cdm install &lt;dataset&gt;&#10;\" /></i></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-24-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-24-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-24-1024.jpg?cb=1474068828\" alt=\"jhaddad@rustyrazorblade ~$ cdm list&#10;Starting CDM&#10;Datasets:&#10;movielens&#10;killrvideo&#10;killrweather&#10;Finished.&#10;\" /></i></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-25-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-25-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-25-1024.jpg?cb=1474068828\" alt=\"jhaddad@rustyrazorblade ~$ cdm install movielens&#10;Starting CDM&#10;Installing movielens&#10;Checking for repo at /Users/jhaddad/.cd...\" /></i></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-26-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-26-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-26-1024.jpg?cb=1474068828\" alt=\"jhaddad@rustyrazorblade ~/dev/cassandra$ cqlsh&#10;Connected to Test Cluster at 127.0.0.1:9042.&#10;[cqlsh 5.0.1 | Cassandra 3.10-...\" /></i></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-27-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-27-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-27-1024.jpg?cb=1474068828\" alt=\"WHAT CAN WE DO WITH IT?&#10;▸ Learn by example&#10;▸ Blog posts / Tutorials&#10;▸ Jupyter notebooks&#10;▸ Reference applications&#10;▸ Data Mo...\" /></i></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-28-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-28-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-28-1024.jpg?cb=1474068828\" alt=\"MANAGING&#10;REFERENCE / TEST DATA&#10;\" /></i></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-29-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-29-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-29-1024.jpg?cb=1474068828\" alt=\"DATASETS&#10;\" /></i></section><section data-index=\"30\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-30-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-30-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-30-1024.jpg?cb=1474068828\" alt=\"MOVIELENS&#10;\" /></i></section><section data-index=\"31\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-31-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-31-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-31-1024.jpg?cb=1474068828\" alt=\"DETAILS&#10;▸GroupLens Research Project&#10;▸University of Minnesota&#10;▸100K ratings&#10;▸1K users&#10;▸1700 movies&#10;\" /></i></section><section data-index=\"32\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-32-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-32-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-32-1024.jpg?cb=1474068828\" alt=\"cqlsh:movielens&gt; select id, avg_rating, genres, name&#10;... from movies limit 1;&#10;@ Row 1&#10;------------+-----------------------...\" /></i></section><section data-index=\"33\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-33-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-33-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-33-1024.jpg?cb=1474068828\" alt=\"cqlsh:movielens&gt; select * from users limit 1;&#10;@ Row 1&#10;------------+--------------------------------------&#10;id | b52fcdfc-0e...\" /></i></section><section data-index=\"34\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-34-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-34-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-34-1024.jpg?cb=1474068828\" alt=\"BLOG: WORKING&#10;RELATIONALLY WITH&#10;CASSANDRA&#10;\" /></i></section><section data-index=\"35\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-35-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-35-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-35-1024.jpg?cb=1474068828\" alt=\"CONNECTING&#10;CASSANDRA&#10;DATA WITH&#10;GRAPHFRAMES&#10;\" /></i></section><section data-index=\"36\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-36-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-36-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-36-1024.jpg?cb=1474068828\" alt=\"cdm install killrweather&#10;\" /></i></section><section data-index=\"37\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-37-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-37-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-37-1024.jpg?cb=1474068828\" alt=\"Helena Edelson&#10;Patrick McFadin&#10;\" /></i></section><section data-index=\"38\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-38-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-38-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-38-1024.jpg?cb=1474068828\" alt=\"cdm install killrvideo&#10;\" /></i></section><section data-index=\"39\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-39-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-39-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-39-1024.jpg?cb=1474068828\" alt=\"Luke Tillman&#10;Patrick McFadin&#10;\" /></i></section><section data-index=\"40\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-40-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-40-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-40-1024.jpg?cb=1474068828\" alt=\"UPCOMING&#10;DATA SETS&#10;\" /></i></section><section data-index=\"41\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-41-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-41-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-41-1024.jpg?cb=1474068828\" alt=\"openﬂights.org&#10;‣ airports&#10;‣ ﬂight data&#10;\" /></i></section><section data-index=\"42\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-42-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-42-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-42-1024.jpg?cb=1474068828\" alt=\"HEALTH CARE&#10;▸ Cancer Genome Atlas Project&#10;▸ Ebola cases&#10;▸ Healthcare ﬁnancial data&#10;▸ Dani Traphagen&#10;\" /></i></section><section data-index=\"43\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-43-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-43-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-43-1024.jpg?cb=1474068828\" alt=\"NYC TAXI DATA&#10;▸ pick up / drop off times &amp; locations&#10;▸ trip distances&#10;▸ itemized fares&#10;▸ rate types&#10;▸ payment types&#10;\" /></i></section><section data-index=\"44\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-44-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-44-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-44-1024.jpg?cb=1474068828\" alt=\"SOCIAL DATA&#10;▸Higgs Twitter Data&#10;▸Foursquare&#10;▸Enron executive emails&#10;\" /></i></section><section data-index=\"45\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-45-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-45-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-45-1024.jpg?cb=1474068828\" alt=\"HOW TO&#10;CONTRIBUTE&#10;\" /></i></section><section data-index=\"46\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-46-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-46-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-46-1024.jpg?cb=1474068828\" alt=\"https://github.com/riptano/cdm-java&#10;\" /></i></section><section data-index=\"47\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-47-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-47-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-47-1024.jpg?cb=1474068828\" alt=\"ADD FEATURES&#10;\" /></i></section><section data-index=\"48\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-48-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-48-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-48-1024.jpg?cb=1474068828\" alt=\"SUGGEST DATASETS&#10;\" /></i></section><section data-index=\"49\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-49-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-49-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-49-1024.jpg?cb=1474068828\" alt=\"CREATE A DATASET&#10;▸ create a git repo&#10;▸ datasets.yaml&#10;▸ schema.cql&#10;▸ insert data&#10;▸ “cdm dump”&#10;▸ cdm install .&#10;▸ create a PR...\" /></i></section><section data-index=\"50\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-50-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-50-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-50-1024.jpg?cb=1474068828\" alt=\"@RUSTYRAZORBLADE&#10;THANK YOU, KIND HUMANS&#10;\" /></i></section><section data-index=\"51\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-51-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-51-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-51-1024.jpg?cb=1474068828\" alt=\"A Shortcut to Awesome: Cassandra Data Modeling By Example (Jon Haddad, The Last Pickle) | C* Summit 2016\" /></i></section><section data-index=\"52\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-52-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-52-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-52-1024.jpg?cb=1474068828\" alt=\"A Shortcut to Awesome: Cassandra Data Modeling By Example (Jon Haddad, The Last Pickle) | C* Summit 2016\" /></i></section><section data-index=\"53\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-53-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-53-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-53-1024.jpg?cb=1474068828\" alt=\"A Shortcut to Awesome: Cassandra Data Modeling By Example (Jon Haddad, The Last Pickle) | C* Summit 2016\" /></i></section><section data-index=\"54\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-54-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-54-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-54-1024.jpg?cb=1474068828\" alt=\"A Shortcut to Awesome: Cassandra Data Modeling By Example (Jon Haddad, The Last Pickle) | C* Summit 2016\" /></i></section><section data-index=\"55\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-55-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-55-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-55-1024.jpg?cb=1474068828\" alt=\"A Shortcut to Awesome: Cassandra Data Modeling By Example (Jon Haddad, The Last Pickle) | C* Summit 2016\" /></i></section><section data-index=\"56\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-56-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-56-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-56-1024.jpg?cb=1474068828\" alt=\"A Shortcut to Awesome: Cassandra Data Modeling By Example (Jon Haddad, The Last Pickle) | C* Summit 2016\" /></i></section><section data-index=\"57\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-57-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-57-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-57-1024.jpg?cb=1474068828\" alt=\"A Shortcut to Awesome: Cassandra Data Modeling By Example (Jon Haddad, The Last Pickle) | C* Summit 2016\" /></i></section><section data-index=\"58\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-58-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-58-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-58-1024.jpg?cb=1474068828\" alt=\"A Shortcut to Awesome: Cassandra Data Modeling By Example (Jon Haddad, The Last Pickle) | C* Summit 2016\" /></i></section><div class=\"j-next-container next-container\"><div class=\"content-container\"><div class=\"next-slideshow-wrapper\"><div class=\"j-next-slideshow next-slideshow\"><p>Upcoming SlideShare</p></div><p>Loading in …5</p><p>×</p></div></div></div></div></div></div></div></div></div><div class=\"slideshow-info-container\" itemscope=\"itemscope\" itemtype=\"https://schema.org/MediaObject\"><div class=\"slideshow-tabs-container show-for-medium-up\"><ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    0 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\">\n                \n                  1 Like\n                \n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\">\n                Statistics\n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\">\n                Notes\n              </i></a>\n            </li>\n          </ul><div class=\"tabs-content\"><div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\"><ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"itemscope\"><li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"AnandRao21\" rel=\"nofollow\" href=\"https://www.slideshare.net/AnandRao21?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Anand Rao\n                            \n                              \n                                , \n                                Sr. SOA Architect\n                              \n                              \n                                 at \n                                Cengage Learning\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n              </ul></div><div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No Downloads</p></div><div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No notes for slide</p></div></div></div><div class=\"notranslate transcript add-padding-right j-transcript\"><ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    JON HADDAD\nTHE LAST PICKLE\nLEARN DATA MODELING BY EXAMPLE\nTHIS IS\nAWESOME!!!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-2-638.jpg?cb=1474068828\" title=\"WHAT’S THE LAST&#10;PICKLE DO?&#10;\" target=\"_blank\">\n        2.\n      </a>\n    WHAT’S THE LAST\nPICKLE DO?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-3-638.jpg?cb=1474068828\" title=\"WE HELP MAKE YOU A&#10;TEAM OF EXPERTS&#10;\" target=\"_blank\">\n        3.\n      </a>\n    WE HELP MAKE YOU A\nTEAM OF EXPERTS\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-4-638.jpg?cb=1474068828\" title=\"&gt; 50 YEARS COMBINED&#10;EXPERIENCE&#10;\" target=\"_blank\">\n        4.\n      </a>\n    &gt; 50 YEARS COMBINED\nEXPERIENCE\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-5-638.jpg?cb=1474068828\" title=\"WHO IS THIS GUY?&#10;\" target=\"_blank\">\n        5.\n      </a>\n    WHO IS THIS GUY?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-6-638.jpg?cb=1474068828\" title=\"15 YEARS&#10;EXPERIENCE&#10;\" target=\"_blank\">\n        6.\n      </a>\n    15 YEARS\nEXPERIENCE\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-7-638.jpg?cb=1474068828\" title=\"4 YEARS WITH CASSANDRA&#10;\" target=\"_blank\">\n        7.\n      </a>\n    4 YEARS WITH CASSANDRA\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-8-638.jpg?cb=1474068828\" title=\"LEARNING HOW TO&#10;CASSANDRA&#10;\" target=\"_blank\">\n        8.\n      </a>\n    LEARNING HOW TO\nCASSANDRA\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-9-638.jpg?cb=1474068828\" title=\"WHAT’S YOUR&#10;BACKGROUND?&#10;\" target=\"_blank\">\n        9.\n      </a>\n    WHAT’S YOUR\nBACKGROUND?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-10-638.jpg?cb=1474068828\" title=\"ORACLE!&#10;MYSQL!&#10;POSTGRES!&#10;\" target=\"_blank\">\n        10.\n      </a>\n    ORACLE!\nMYSQL!\nPOSTGRES!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-11-638.jpg?cb=1474068828\" title=\"CQL LOOKS&#10;LIKE SQL&#10;\" target=\"_blank\">\n        11.\n      </a>\n    CQL LOOKS\nLIKE SQL\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-12-638.jpg?cb=1474068828\" title=\"BAD&#10;ASSUMPTIONS&#10;\" target=\"_blank\">\n        12.\n      </a>\n    BAD\nASSUMPTIONS\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-13-638.jpg?cb=1474068828\" title=\"3RD&#10;NORMAL&#10;FORM?&#10;\" target=\"_blank\">\n        13.\n      </a>\n    3RD\nNORMAL\nFORM?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-14-638.jpg?cb=1474068828\" title=\"WHERE’S&#10;MY&#10;JOINS?&#10;\" target=\"_blank\">\n        14.\n      </a>\n    WHERE’S\nMY\nJOINS?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-15-638.jpg?cb=1474068828\" title=\"SECONDARY&#10;INDEX?&#10;\" target=\"_blank\">\n        15.\n      </a>\n    SECONDARY\nINDEX?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-16-638.jpg?cb=1474068828\" title=\"DO IT WRONG&#10;TRY TO DATA MODELGET ANGRY&#10;WATCH VIDEOS &amp; READ&#10;\" target=\"_blank\">\n        16.\n      </a>\n    DO IT WRONG\nTRY TO DATA MODELGET ANGRY\nWATCH VIDEOS &amp; READ\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-17-638.jpg?cb=1474068828\" title=\"EVERYTHING I&#10;KNOW IS WRONG&#10;\" target=\"_blank\">\n        17.\n      </a>\n    EVERYTHING I\nKNOW IS WRONG\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-18-638.jpg?cb=1474068828\" title=\"LEARN BY&#10;EXAMPLE&#10;\" target=\"_blank\">\n        18.\n      </a>\n    LEARN BY\nEXAMPLE\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-19-638.jpg?cb=1474068828\" title=\"CASSANDRA DATASET&#10;MANAGER&#10;\" target=\"_blank\">\n        19.\n      </a>\n    CASSANDRA DATASET\nMANAGER\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-20-638.jpg?cb=1474068828\" title=\"CDM&#10;\" target=\"_blank\">\n        20.\n      </a>\n    CDM\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-21-638.jpg?cb=1474068828\" title=\"APT FOR CASSANDRA&#10;DATA&#10;\" target=\"_blank\">\n        21.\n      </a>\n    APT FOR CASSANDRA\nDATA\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-22-638.jpg?cb=1474068828\" title=\"INSTALL DATA TO YOUR&#10;CASSANDRA CLUSTER&#10;\" target=\"_blank\">\n        22.\n      </a>\n    INSTALL DATA TO YOUR\nCASSANDRA CLUSTER\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-23-638.jpg?cb=1474068828\" title=\"cdm install &lt;dataset&gt;&#10;\" target=\"_blank\">\n        23.\n      </a>\n    cdm install &lt;dataset&gt;\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-24-638.jpg?cb=1474068828\" title=\"jhaddad@rustyrazorblade ~$ cdm list&#10;Starting CDM&#10;Datasets:&#10;...\" target=\"_blank\">\n        24.\n      </a>\n    jhaddad@rustyrazorblade ~$ cdm list\nStarting CDM\nDatasets:\nmovielens\nkillrvideo\nkillrweather\nFinished.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-25-638.jpg?cb=1474068828\" title=\"jhaddad@rustyrazorblade ~$ cdm install movielens&#10;Starting C...\" target=\"_blank\">\n        25.\n      </a>\n    jhaddad@rustyrazorblade ~$ cdm install movielens\nStarting CDM\nInstalling movielens\nChecking for repo at /Users/jhaddad/.cdm/movielens\nPulling latest\nCDM is using dataset path: /Users/jhaddad/.cdm/movielens\ncqlsh -e \"DROP KEYSPACE IF EXISTS movielens; CREATE KEYSPACE movielens\nWITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}\"\nSchema: /Users/jhaddad/.cdm/movielens/schema.cql\nLoading data\ncqlsh -k movielens -e \"COPY movies FROM '/Users/jhaddad/.cdm/movielens/data/\nmovies.csv'\"\ncqlsh -k movielens -e \"COPY users FROM '/Users/jhaddad/.cdm/movielens/data/\nusers.csv'\"\ncqlsh -k movielens -e \"COPY ratings_by_user FROM '/Users/jhaddad/.cdm/movielens\ndata/ratings_by_user.csv'\"\ncqlsh -k movielens -e \"COPY original_movie_map FROM '/Users/jhaddad/.cdm/\nmovielens/data/original_movie_map.csv'\"\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-26-638.jpg?cb=1474068828\" title=\"jhaddad@rustyrazorblade ~/dev/cassandra$ cqlsh&#10;Connected to...\" target=\"_blank\">\n        26.\n      </a>\n    jhaddad@rustyrazorblade ~/dev/cassandra$ cqlsh\nConnected to Test Cluster at 127.0.0.1:9042.\n[cqlsh 5.0.1 | Cassandra 3.10-SNAPSHOT | CQL spec 3.4.3 | Native protocol v4]\nUse HELP for help.\ncqlsh&gt; use movielens ;\ncqlsh:movielens&gt; desc tables;\nmovies users ratings_by_user original_movie_map ratings_by_movie\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-27-638.jpg?cb=1474068828\" title=\"WHAT CAN WE DO WITH IT?&#10;▸ Learn by example&#10;▸ Blog posts / T...\" target=\"_blank\">\n        27.\n      </a>\n    WHAT CAN WE DO WITH IT?\n▸ Learn by example\n▸ Blog posts / Tutorials\n▸ Jupyter notebooks\n▸ Reference applications\n▸ Data Models for presentations\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-28-638.jpg?cb=1474068828\" title=\"MANAGING&#10;REFERENCE / TEST DATA&#10;\" target=\"_blank\">\n        28.\n      </a>\n    MANAGING\nREFERENCE / TEST DATA\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-29-638.jpg?cb=1474068828\" title=\"DATASETS&#10;\" target=\"_blank\">\n        29.\n      </a>\n    DATASETS\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-30-638.jpg?cb=1474068828\" title=\"MOVIELENS&#10;\" target=\"_blank\">\n        30.\n      </a>\n    MOVIELENS\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-31-638.jpg?cb=1474068828\" title=\"DETAILS&#10;▸GroupLens Research Project&#10;▸University of Minnesot...\" target=\"_blank\">\n        31.\n      </a>\n    DETAILS\n▸GroupLens Research Project\n▸University of Minnesota\n▸100K ratings\n▸1K users\n▸1700 movies\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-32-638.jpg?cb=1474068828\" title=\"cqlsh:movielens&gt; select id, avg_rating, genres, name&#10;... fr...\" target=\"_blank\">\n        32.\n      </a>\n    cqlsh:movielens&gt; select id, avg_rating, genres, name\n... from movies limit 1;\n@ Row 1\n------------+--------------------------------------\nid | 76a38f64-94d8-4b8f-b830-a40af96f8d20\navg_rating | 3.16667\ngenres | {'Drama'}\nname | Little Lord Fauntleroy (1936)\n(1 rows)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-33-638.jpg?cb=1474068828\" title=\"cqlsh:movielens&gt; select * from users limit 1;&#10;@ Row 1&#10;-----...\" target=\"_blank\">\n        33.\n      </a>\n    cqlsh:movielens&gt; select * from users limit 1;\n@ Row 1\n------------+--------------------------------------\nid | b52fcdfc-0eaf-4432-9896-aa22db56edb2\naddress | 0322 Mattie Ramp Apt. 177\nage | 37\ncity | South Fremont\ngender | M\nname | Harrold Hills\noccupation | administrator\nzip | 06513\n(1 rows)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-34-638.jpg?cb=1474068828\" title=\"BLOG: WORKING&#10;RELATIONALLY WITH&#10;CASSANDRA&#10;\" target=\"_blank\">\n        34.\n      </a>\n    BLOG: WORKING\nRELATIONALLY WITH\nCASSANDRA\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-35-638.jpg?cb=1474068828\" title=\"CONNECTING&#10;CASSANDRA&#10;DATA WITH&#10;GRAPHFRAMES&#10;\" target=\"_blank\">\n        35.\n      </a>\n    CONNECTING\nCASSANDRA\nDATA WITH\nGRAPHFRAMES\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-36-638.jpg?cb=1474068828\" title=\"cdm install killrweather&#10;\" target=\"_blank\">\n        36.\n      </a>\n    cdm install killrweather\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-37-638.jpg?cb=1474068828\" title=\"Helena Edelson&#10;Patrick McFadin&#10;\" target=\"_blank\">\n        37.\n      </a>\n    Helena Edelson\nPatrick McFadin\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-38-638.jpg?cb=1474068828\" title=\"cdm install killrvideo&#10;\" target=\"_blank\">\n        38.\n      </a>\n    cdm install killrvideo\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-39-638.jpg?cb=1474068828\" title=\"Luke Tillman&#10;Patrick McFadin&#10;\" target=\"_blank\">\n        39.\n      </a>\n    Luke Tillman\nPatrick McFadin\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-40-638.jpg?cb=1474068828\" title=\"UPCOMING&#10;DATA SETS&#10;\" target=\"_blank\">\n        40.\n      </a>\n    UPCOMING\nDATA SETS\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-41-638.jpg?cb=1474068828\" title=\"openﬂights.org&#10;‣ airports&#10;‣ ﬂight data&#10;\" target=\"_blank\">\n        41.\n      </a>\n    openﬂights.org\n‣ airports\n‣ ﬂight data\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-42-638.jpg?cb=1474068828\" title=\"HEALTH CARE&#10;▸ Cancer Genome Atlas Project&#10;▸ Ebola cases&#10;▸ H...\" target=\"_blank\">\n        42.\n      </a>\n    HEALTH CARE\n▸ Cancer Genome Atlas Project\n▸ Ebola cases\n▸ Healthcare ﬁnancial data\n▸ Dani Traphagen\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-43-638.jpg?cb=1474068828\" title=\"NYC TAXI DATA&#10;▸ pick up / drop off times &amp; locations&#10;▸ trip...\" target=\"_blank\">\n        43.\n      </a>\n    NYC TAXI DATA\n▸ pick up / drop off times &amp; locations\n▸ trip distances\n▸ itemized fares\n▸ rate types\n▸ payment types\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-44-638.jpg?cb=1474068828\" title=\"SOCIAL DATA&#10;▸Higgs Twitter Data&#10;▸Foursquare&#10;▸Enron executiv...\" target=\"_blank\">\n        44.\n      </a>\n    SOCIAL DATA\n▸Higgs Twitter Data\n▸Foursquare\n▸Enron executive emails\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-45-638.jpg?cb=1474068828\" title=\"HOW TO&#10;CONTRIBUTE&#10;\" target=\"_blank\">\n        45.\n      </a>\n    HOW TO\nCONTRIBUTE\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-46-638.jpg?cb=1474068828\" title=\"https://github.com/riptano/cdm-java&#10;\" target=\"_blank\">\n        46.\n      </a>\n    https://github.com/riptano/cdm-java\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-47-638.jpg?cb=1474068828\" title=\"ADD FEATURES&#10;\" target=\"_blank\">\n        47.\n      </a>\n    ADD FEATURES\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-48-638.jpg?cb=1474068828\" title=\"SUGGEST DATASETS&#10;\" target=\"_blank\">\n        48.\n      </a>\n    SUGGEST DATASETS\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-49-638.jpg?cb=1474068828\" title=\"CREATE A DATASET&#10;▸ create a git repo&#10;▸ datasets.yaml&#10;▸ sche...\" target=\"_blank\">\n        49.\n      </a>\n    CREATE A DATASET\n▸ create a git repo\n▸ datasets.yaml\n▸ schema.cql\n▸ insert data\n▸ “cdm dump”\n▸ cdm install .\n▸ create a PR on cdm-java\nOMG BEST\nDATASET EVER\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-50-638.jpg?cb=1474068828\" title=\"@RUSTYRAZORBLADE&#10;THANK YOU, KIND HUMANS&#10;\" target=\"_blank\">\n        50.\n      </a>\n    @RUSTYRAZORBLADE\nTHANK YOU, KIND HUMANS\n \n  </li>\n              </ol></div></div></div><aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\"><dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\"><ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"66068424\" title=\"Train the Trainer\" href=\"https://www.linkedin.com/learning/train-the-trainer?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Train the Trainer\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Train the Trainer\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=AlyMjKo3rICZ%2Fa%2BXWxTdS9vSGcA%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-kXCas-tafZXDqf8fcZLSiol4UeioAmQY7fOqvRDDnEI69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Train the Trainer</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"66068424\" title=\"PowerPoint: Using Photos and Video Effectively for Great Presentations\" href=\"https://www.linkedin.com/learning/powerpoint-using-photos-and-video-effectively-for-great-presentations?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_PowerPoint: Using Photos and Video Effectively for Great Presentations\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"PowerPoint: Using Photos and Video Effectively for Great Presentations\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=3%2F2ZO%2Bb4W0FRGtSVNQkeStwUPPA%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-gXyKq-NefYXPuecXdZLSioVkXcCsJlAAxeuyhRDXjE469LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>PowerPoint: Using Photos and Video Effectively for Great Presentations</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"66068424\" title=\"PowerPoint 2016: Tips and Tricks\" href=\"https://www.linkedin.com/learning/powerpoint-2016-tips-and-tricks?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_PowerPoint 2016: Tips and Tricks\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"PowerPoint 2016: Tips and Tricks\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=nIcokYvYRZ365i4sqNHaREMZ6bI%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lUyKj_tWfZH_ucMPfZLSiol8eeywDlAE0e-moQTPjFI69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>PowerPoint 2016: Tips and Tricks</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"17917955\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Cassandra By Example: Data Modelling with CQL3\" href=\"https://www.slideshare.net/jericevans/cassandra-by-example-data-modelling-with-cql3\">\n    \n    <div class=\"related-content\"><p>Cassandra By Example: Data Modelling with CQL3</p><p>Eric Evans</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"105785146\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"How to Power Innovation with Geo-Distributed Data Management in Hybrid Cloud\" href=\"https://www.slideshare.net/DataStax/data-staxwebinar-howtopower-innovationwithgeodistributed-datainhybridcloudfor-slideshare\">\n    \n    <div class=\"related-content\"><p>How to Power Innovation with Geo-Distributed Data Management in Hybrid Cloud</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"102824145\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"How to Evaluate Cloud Databases for eCommerce\" href=\"https://www.slideshare.net/DataStax/how-to-evaluate-cloud-databases-for-ecommerce\">\n    \n    <div class=\"related-content\"><p>How to Evaluate Cloud Databases for eCommerce</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"95288307\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Webinar: DataStax Enterprise 6: 10 Ways to Multiply the Power of Apache Cassandra™ Without the Complexity\" href=\"https://www.slideshare.net/DataStax/webinar-datastax-enterprise-6-10-ways-to-multiply-the-power-of-apache-cassandra-without-the-complexity-95288307\">\n    \n    <div class=\"related-content\"><p>Webinar: DataStax Enterprise 6: 10 Ways to Multiply the Power of Apache Cassa...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"93695860\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Webinar: DataStax and Microsoft Azure: Empowering the Right-Now Enterprise with Real-Time Apps at Cloud Scale\" href=\"https://www.slideshare.net/DataStax/webinar-datastax-and-microsoft-azure-empowering-the-rightnow-enterprise-with-realtime-apps-at-cloud-scale-93695860\">\n    \n    <div class=\"related-content\"><p>Webinar: DataStax and Microsoft Azure: Empowering the Right-Now Enterprise wi...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"88770359\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Webinar - Real-Time Customer Experience for the Right-Now Enterprise featuring Forrester Research\" href=\"https://www.slideshare.net/DataStax/webinar-realtime-customer-experience-for-the-rightnow-enterprise-featuring-forrester-research\">\n    \n    <div class=\"related-content\"><p>Webinar - Real-Time Customer Experience for the Right-Now Enterprise featurin...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"88713641\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Datastax - The Architect's guide to customer experience (CX)\" href=\"https://www.slideshare.net/DataStax/datastax-the-architects-guide-to-customer-experience-cx\">\n    \n    <div class=\"related-content\"><p>Datastax - The Architect's guide to customer experience (CX)</p><p>DataStax</p></div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div></div><footer>\n          <div class=\"row\"><div class=\"columns\"><ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div></div>\n          \n          <div class=\"row\"><div class=\"columns\"><p class=\"copyright text-center\">LinkedIn Corporation © 2018</p></div></div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div>\n    <div class=\"row\"><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></div>\n        \n    </div>\n    \n    \n    \n  \n    \n    \n  \n  \n  <noscript>\n    </noscript>",
        "created_at": "2018-07-27T03:41:36+0000",
        "updated_at": "2018-07-27T03:41:43+0000",
        "published_at": null,
        "published_by": [
          "DataStax"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 4,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/cdm-160915175143-thumbnail-4.jpg?cb=1474068828",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11612"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 217,
            "label": "tool",
            "slug": "tool"
          }
        ],
        "is_public": false,
        "id": 11609,
        "uid": null,
        "title": "hawkular/cassalog",
        "url": "https://github.com/hawkular/cassalog",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<div id=\"user-content-preamble\">\n<div>\n<div>\n<p>Cassalog is a schema change management library and tool for\n<a href=\"http://cassandra.apache.org\" rel=\"nofollow\">Apache Cassandra</a> that can be used with\napplications running on the JVM.</p>\n</div>\n</div>\n</div>\n<div>\n<h2 id=\"user-content-why\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#why\"></a>Why?</h2>\n<div>\n<div>\n<p>Just as application code evolves and changes so do database schemas. If you are\nbuilding an application and intend to support upgrading from one version to\nanother, then managing schema changes is essential. If you are lucky, you might\nbe able to get by with running some simple upgrade scripts to bring the schema\nup to date with the new version. This likely will not work however if you\nsupport multiple upgrade paths. For example, suppose we have versions 1 and 2,\nand are introducing version 3 of an application. We want to allow upgrading to\nversion 3 from either 1 or 2 in addition to upgrading from 1 to 2.</p>\n</div>\n<div>\n<p>You could add schema upgrade logic to application code, but that is often a\nless that ideal solution as it convolutes the code base. Fortunately, there are\ntool for managing schema changes like <a href=\"http://www.liquibase.org/\" rel=\"nofollow\">Liquibase</a>,\n<a href=\"http://flywaydb.org/\" rel=\"nofollow\">Flyway</a>, and\n<a href=\"http://guides.rubyonrails.org/active_record_basics.html\" rel=\"nofollow\">Active Record</a> for Ruby\non Rails applications. These tools however, are designed specifically for\nrelational databases. I previously spent time trying to patch Liquibase to\nsupport Cassandra but found that it was not a good fit. Cassalog is designed\nsolely for use with Cassandra, not for any other database systems.</p>\n</div>\n<div>\n<p>Cassalog is written in Groovy. There are several reasons for this. First,\nGroovy offers great interoperability with Java, making it usable and accessible\nto application running on the JVM. Groovy’s dynamic and meta programming\nfeatures make it easy to write domain specific languages. Groovy has multi-line\nstrings and string interpolation out of the box, both of which can be really\nuseful for writing schema change scripts. Lastly, with Cassalog schema changes\nare not written in XML or JSON. Instead they are written as Groovy scripts\ngiving you the full power and flexibility of Groovy.</p>\n</div>\n</div>\n</div>\n<div>\n<h2 id=\"user-content-usage\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#usage\"></a>Usage</h2>\n<div>\n<div>\n<p>The Cassalog class is the primary class with which you will interact.</p>\n</div>\n<div>\n<div>\n<div class=\"highlight highlight-source-groovy\"><pre>// Groovy\ndef script = // load schema change script\ndef session = // obtain DataStax driver Session object\ndef cassalog = new Cassalog(session: session)\ncassalog.execute(script)</pre></div>\n</div>\n</div>\n<div>\n<div>\n<div class=\"highlight highlight-source-java\"><pre>// Java\nURI script = // load schema change script\nSession session = // obtain DataStax driver Session object\nCassalog cassalog = new Cassalog();\ncassalog.setSession(session);\ncassalog.execute(script);</pre></div>\n</div>\n</div>\n<div>\n<p>And here is what a cassalog script might look like,</p>\n</div>\n<div>\n<div>\n<div class=\"highlight highlight-source-groovy\"><pre>createKeyspace {\n  version '0.1'\n  name 'my_keyspace'\n  author 'admin'\n  description 'Set up a keyspace for unit tests'\n}\nschemaChange {\n  version '0.1.1'\n  author 'admin'\n  description 'Create table for storing time series data'\n  cql \"\"\"\nCREATE TABLE metrics (\n    id uuid,\n    time timeuuid,\n    value double,\n    PRIMARY KEY (id, time)\n)\n\"\"\"\n}</pre></div>\n</div>\n</div>\n<div>\n<table><tbody><tr><td>\n</td>\n<td>\nSchema changes are applied in the order that they are declared in the\nscript(s) regardless of the assigned versions.\n</td>\n</tr></tbody></table></div>\n</div>\n</div>\n<div>\n<h2 id=\"user-content-features\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#features\"></a>Features</h2>\n<div>\n<div>\n<ul><li>\n<p>Tagging</p>\n</li>\n<li>\n<p>Execute arbitrary Groovy / Java code in schema change scripts</p>\n</li>\n<li>\n<p>Pass variables to scripts</p>\n</li>\n<li>\n<p>Changes can stored across multiple scripts</p>\n</li>\n<li>\n<p>Schema change detection</p>\n</li>\n</ul></div>\n<div>\n<h3 id=\"user-content-tagging\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#tagging\"></a>Tagging</h3>\n<div>\n<p>You can specify tags when running Cassalog, e.g.</p>\n</div>\n<div>\n<div>\n<div class=\"highlight highlight-source-groovy\"><pre>// Groovy\ndef script = // load schema change script\ndef session = // obtain DataStax driver Session object\ndef cassalog = new Cassalog(session: session)\ncassalog.execute(script, ['dev', 'test_data'])</pre></div>\n</div>\n</div>\n<div>\n<div>\n<div class=\"highlight highlight-source-java\"><pre>// Java\nURI script = // load schema change script\nSession session = // obtain DataStax driver Session object\nCassalog cassalog = new Cassalog();\ncassalog.setSession(session);\ncassalog.execute(script, Collections.asList(\"dev\", \"test_data\"));</pre></div>\n</div>\n</div>\n<div>\n<p>Cassalog will apply schema changes that have not already been run and that</p>\n</div>\n<div>\n<ul><li>\n<p>Dot not specify any tags or</p>\n</li>\n<li>\n<p>Specify tags and include the <code>dev</code> and <code>test_data</code> tags</p>\n</li>\n</ul></div>\n</div>\n<div>\n<h3 id=\"user-content-execute-arbitrary-code\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#execute-arbitrary-code\"></a>Execute arbitrary code</h3>\n<div>\n<p>Cassandra is frequently used for time series data. Suppose we have a metrics\ntable, and we want to generate some sample data for tests.</p>\n</div>\n<div>\n<div>\n<div class=\"highlight highlight-source-groovy\"><pre>schemaChange {\n  version '1.0'\n  cql \"\"\"\nCREATE TABLE metrics (\n    id text PRIMARY KEY,\n    time timestamp,\n    value double\n)\n\"\"\"\n}\ntestData = []\nrandom = new Random\n10.times { i -&gt;\n  testData &lt;&lt; \"INSERT INTO metrics (id, time, value) VALUES ('$i', ${new Date().time + 100}, ${random.nextDouble()})\"\n}\nschemaChange {\n  version '1.0.1'\n  tags 'test_data'\n  cql testData\n}</pre></div>\n</div>\n</div>\n<div>\n<p>This script first calls the <code>schemaChange</code> function to create the metrics table.\nThe next few lines generate a list of INSERT statements with some test data.\nFinally, we have another call to <code>schemaChange</code>. It specifies the test_data\ntag and passes the <code>testData</code> list to the <code>cql</code> parameter.</p>\n</div>\n</div>\n<div>\n<h3 id=\"user-content-pass-variables-to-scripts\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#pass-variables-to-scripts\"></a>Pass variables to scripts</h3>\n<div>\n<p>You can pass arbitrary variables to scripts, not just strings.</p>\n</div>\n<div>\n<div>\n<div class=\"highlight highlight-source-groovy\"><pre>// Groovy\ndef vars = [\n  metricIds: ['M1', 'M2', 'M3'],\n  startDate: new Date()\n  maxValue: 100,\n  minValue: 50\n]\ncassalog.execute(script, vars)</pre></div>\n</div>\n</div>\n<div>\n<div>\n<div class=\"highlight highlight-source-java\"><pre>// Java\nMap&lt;String, ?&gt; vars = ImmutableMap.of(\n    \"metricIds\", asList(\"M1\", \"M2\", \"M3\"),\n    \"startDate\", new Date(),\n    \"maxValue\", 100,\n    \"minValue\", 50\n);\ncassalog.execute(script, vars);</pre></div>\n</div>\n</div>\n</div>\n<div>\n<h3 id=\"user-content-changes-can-stored-across-multiple-scripts\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#changes-can-stored-across-multiple-scripts\"></a>Changes can stored across multiple scripts</h3>\n<div>\n<p>You can use the <code>include</code> function to store changes in multiple script to\nkeep your schema changes more modular and better organized.</p>\n</div>\n<div>\n<div>\n<div class=\"highlight highlight-source-groovy\"><pre>include '/dbchanges/base_tables.groovy'\ninclude '/dbchanges/seed_data.groovy'</pre></div>\n</div>\n</div>\n<div>\n<p>The <code>include</code> function currently takes a single string argument that should\nspecify the absolute path of a script on the classpath or from the configured <code>baseScriptsPath</code>.</p>\n</div>\n<div>\n<p><code>baseScriptsPath</code> is an absolute path to where the other include scripts are located e.g. <code>/Users/john/cassalog/scripts</code>.</p>\n</div>\n</div>\n<div>\n<h3 id=\"user-content-schema-change-detection\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#schema-change-detection\"></a>Schema change detection</h3>\n<div>\n<p>Cassalog does not store the CQL code associated with each schema change. It\ncomputes a hash of the CQL and stores that instead. If the hash in the change\nlog differs from the hash of the CQL in the source script, Cassalog will throw\na ChangeSetAlteredException.</p>\n</div>\n<div>\n<p>You will need to manually resolve the issue that caused the\nChangeSetAlteredException. Cassandra does not support transactions like a\nrelational database, so there no rollback functionality to fall back on.</p>\n</div>\n</div>\n</div>\n</div>\n<div>\n<h2 id=\"user-content-change-log-table\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#change-log-table\"></a>Change Log Table</h2>\n<div>\n<div>\n<p>All schema changes are recorded in the change log table, <em>cassalog</em>. The table\nwill be created the first time Cassalog is run. Change log data looks like,</p>\n</div>\n<div>\n<div>\n<pre>bucket | revision | applied_at               | author | description | hash         | version  | tags\n--------+----------+--------------------------+--------+-----------------------------------------------------+\n     0 |        0 | 2016-01-28 11:09:54-0500 | admin | First table  | 0xe361957eeb |      1.0 | {'legacy'}\n     0 |        1 | 2016-01-28 11:09:54-0500 | admin | Second table | 0xf336e725d4 |      1.1 | {'legacy'}\n     0 |        2 | 2016-01-28 11:09:55-0500 | admin | Third table  | 0xcecef5f840 |      1.2 | {'legacy', 'dev'}\n     0 |        3 | 2016-01-28 11:09:55-0500 | admin | Fourth table | 0x4b5d24b77c |      1.3 | {'legacy'}</pre>\n</div>\n</div>\n<div>\n<p>Here is a brief overview of the schema.</p>\n</div>\n<div>\n<div>\n<pre>CREATE TABLE cassalog (\n    bucket int,\n    revision int,\n    applied_at timestamp,\n    author text,\n    description text,\n    hash blob,\n    version text,\n    tags set&lt;text&gt;,\n    PRIMARY KEY (bucket, revision)\n)</pre>\n</div>\n</div>\n<div>\n<p><strong>author</strong><br />The username, or email address, etc. of the person making the change. This is\nan optional field and can be null.</p>\n</div>\n<div>\n<p><strong>description</strong><br />A summary of the changes. This is an optional field and can be null.</p>\n</div>\n<div>\n<p><strong>hash</strong><br />Cassalog does not store the CQL statements that it executes. Instead it stores a\nhash that uniquely identifies the CQL statement(s). Cassalog generates this\nhash value.</p>\n</div>\n<div>\n<p><strong>version</strong><br />The version can be an arbitrary string. It should be a unique identifier for the\nchange; however, Cassalog does not enforce uniqueness. This is a required field.</p>\n</div>\n<div>\n<p><strong>tags</strong><br />An optional set of user-supplied tags.</p>\n</div>\n<div>\n<p><strong>revision</strong><br />Cassalog assigns a revision number to each change that it applies. It uses the\nrevision number to keep track of the order in which changes are applied. If the\norder of schema changes in a source script is changed, then a\nChangeSetAlteredException will be thrown.</p>\n</div>\n<div>\n<p><strong>bucket</strong><br />Cassalog stores multiple rows per physical partition. This is a revision offset.\nThe bucket size defaults to 100.</p>\n</div>\n</div>\n</div>\n<div>\n<h2 id=\"user-content-building-from-source\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#building-from-source\"></a>Building from Source</h2>\n<div>\n<div>\n<p>Cassandra is built with Maven and requires a JVM version 1.7 or later. Test\nexecution requires a running Cassandra cluster (which can be a single node) with\na node listening on 127.0.0.1. Cassandra 2.0 or later should be used.</p>\n</div>\n<div>\n<div>\n<div class=\"highlight highlight-source-shell\"><pre>git clone https://github.com/jsanda/cassalog.git\ncd cassalog\nmvn install</pre></div>\n</div>\n</div>\n<div>\n<table><tbody><tr><td>\n</td>\n<td>\nIf you want to build without having a running Cassandra instance, you can\nrun <code>mvn install -DskipTests</code>\n</td>\n</tr></tbody></table></div>\n</div>\n</div>\n<div>\n<h2 id=\"user-content-setting-up-cassandra-for-development-or-testing\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#setting-up-cassandra-for-development-or-testing\"></a>Setting up Cassandra for development or testing</h2>\n<div>\n<div>\n<p>As Cassalog evolves and looks to support different versions of Cassandra and\nCQL, ccm is the likely tool of choice to use for testing against different\nversions.</p>\n</div>\n</div>\n</div></article>",
        "created_at": "2018-07-26T19:39:40+0000",
        "updated_at": "2018-08-07T22:58:13+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 6,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/10179627?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11609"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 883,
            "label": "java",
            "slug": "java"
          },
          {
            "id": 1287,
            "label": "spring",
            "slug": "spring"
          }
        ],
        "is_public": false,
        "id": 11607,
        "uid": null,
        "title": "Spring Data for Apache Cassandra",
        "url": "http://projects.spring.io/spring-data-cassandra/",
        "content": "<p>The learning curve for developing applications with Apache Cassandra is significantly reduced when using Spring Data for Apache Cassandra. With the power to stay at a high level with annotated POJOs, or at a low level with high performance data ingestion capabilities, the Spring Data for Apache Cassandra templates are sure to meet every application need.</p><h2 id=\"features\">Features</h2><ul><li>Build repositories based on common Spring Data interfaces</li>\n<li>Support for synchronous and asynchronous data operations</li>\n<li>Asynchronous callback support</li>\n<li>Support for XML based Keyspace creation and CQL Table creation</li>\n<li>JavaConfig and XML Support for all Cluster and Session Capabilities</li>\n<li>Exception Translation to the familiar Spring DataAccessException hierarchy</li>\n<li>Convenient QueryBuilders to eliminate the need to learn CQL</li>\n<li>Automatic implementation of Repository interfaces including support for custom query methods</li>\n<li>Based on the latest DataStax Enterprise CQL Java Driver</li>\n</ul><h2 id=\"quick-start\">Quick Start</h2>",
        "created_at": "2018-07-26T19:39:19+0000",
        "updated_at": "2018-08-07T22:35:34+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 0,
        "domain_name": "projects.spring.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11607"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 33,
            "label": "internet.architecture",
            "slug": "internet-architecture"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 253,
            "label": "analytics",
            "slug": "analytics"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 956,
            "label": "streaming",
            "slug": "streaming"
          }
        ],
        "is_public": false,
        "id": 11605,
        "uid": null,
        "title": "Streaming Analytics Architecture Delivers Real-Time Insights",
        "url": "https://www.techworld.com/data/streaming-analytics-architecture-delivers-real-time-insights-3608526/",
        "content": "<p>Accenture had the opportunity to stand-up such a streaming analytics platform that delivers insights in near-real time meaning that the time from when sensor data arrives in the system until when insight is delivered happens in the order of seconds.</p><p>Recently, we were asked to help a client manage their water distribution network that had hundreds of thousands of tagged sensors – measuring for example, water flow and pressure.  This client had already embraced analytics for reporting and capacity planning supported by a batch architecture that easily met daily turn-around times to analyze all the data.  From the time sensor readings were ingested, the existing batch architecture could deliver insights into the capacity and health of their water network within 15 minutes.</p> \n<header class=\"articleHeader\"><figure><img src=\"https://cdn2.techworld.com/graphics/fallbackimages/30/Big-data-binary-code-futuristic_thumb800.jpg\" alt=\"Big data binary code futuristic\" /></figure></header> \n \n \n<p>The client was interested in adding real-time views onto the health and events along their water network to improve operational efficiency that ultimately improves customer satisfaction.  The objective was to present dynamic views of the current state of the water network refreshed on the order of seconds.</p> \n<p><img class=\"lz\" title=\"Accenture IoT illustration\" alt=\"Accenture IoT illustration\" height=\"858\" width=\"1388\" src=\"https://cdn3.techworld.com/cmsdata/blogentries/3608526/accenture.png\" /></p><noscript>\n<img title=\"Accenture IoT illustration\" alt=\"Accenture IoT illustration\" src=\"https://cdn3.techworld.com/cmsdata/blogentries/3608526/accenture.png\" height=\"858\" width=\"1388\" /></noscript> \n<p> To begin with, we focused on the detection and prediction of leakage events.  Their existing batch architecture was too slow; even a turnaround in minutes wastes precious moments before remediation efforts can begin.  Real-time monitoring combined with predictive algorithms manages operational pressure to proactively reduce high and low pressure spikes that serve as a primary cause of leaks.  These improvements result in avoiding leaks, prolonging the longevity of assets, and reducing disruption to customers.<br /> <br />For streaming analytics, the challenge lies in handling at speed the large number of parallel computations kicked-off by arriving sensor data.  Each time a sensor reading takes part in a number of online algorithms that need to be computed on the order of seconds.  We provide a sample of these algorithms implemented to give some insight into the complexity that the solution handles at real-time.</p> \n<p>First, arriving sensor data is not periodic and is assumed to be unreliable.  Different streams of sensor data come in at different time ticks (or intervals of time) be it on the order of days, minutes, or seconds.  Before computations and visualizations occur, we first need processing to normalize streamed data across different sources according to common time ticks (e.g., every 5 seconds or 1 minute). In the event of missed sensor readings, calculations estimate the values for the missed value to allow for operations to progress, but then require additional calculations for backfilling and reconciliation upon recovery of lost data (e.g., when network outage is restored or when a sensor comes back online).   </p> \n \n<p>To do prediction, an algorithm projects readings forward into time against a function.  An online curve-fitting algorithm continuously trains the function based on the arriving streamed data.  </p> \n<p>Further analytics includes processing complex events like when a series of sensor readings that convey various high and low thresholds on critical pressure points.  These indicate detection of a leakage event, which triggers follow-up actions.  <br />In addition, we implemented more complex computations like those for estimating impact of the most recent usage and operations on reservoir supply that combine streamed data with historical data (e.g., over the past 7-days) fetched from the data store.</p> \n<p>To handle these computations, we implemented a lambda architecture pattern  based on that of many of today’s internet companies like Twitter, Facebook, and LinkedIn.  In the same way that these internet companies balance real-time analytics on live social feeds against batch analytics over a vast quantity of historical user data, we do the same with balancing streaming and batch analytics on sensor data.  To that end, we added a parallel data flow for streaming analytics to the existing batch flow.</p> \n<p>To start, we created data publishers for pushing data from the client’s sensor network into our platform.  For this client, it meant getting data from their <a rel=\"nofollow\" target=\"_blank\" href=\"http://www.osisoft.com/\">OSI Pi historian</a> that serves as a gateway for a majority of the sensor data and for REST APIs.  We ingested this data into a stream processor<a rel=\"nofollow\" target=\"_blank\" href=\"http://aws.amazon.com/kinesis/\"> Amazon Web Services’ (AWS) Kinesis</a>.  <a rel=\"nofollow\" target=\"_blank\" href=\"http://spark.apache.org/\">Apache Spark Streaming</a> runs computations for real-time detection and analytics, and <a rel=\"nofollow\" target=\"_blank\" href=\"http://aws.amazon.com/dynamodb/\">AWS Dynamo</a> and stores the time-series data and results of our processing.  A Java API tier obfuscates data store interfaces for querying the results that are then visualized in a dashboard built using D3.</p> \n<p>Over the course of six weeks, the team created the reference implementation for the water network.  For the initial build-out we leveraged as-a-Service Amazon Web Services components that were ready as-a-Service for agility sake.  The team then hardened this architecture, re-platformed onto Apache Cassandra and Kafka to allow for an on-premise option in addition to our AWS cloud-based implementation, load tested it with ingestion data at over 3000 events per second, and packaged its components for reuse.  </p> \n<p>The result is an architecture proven for leakage detection in water networks, but is now prebuilt and ready to onboard new streaming analytics applications and domains.</p> \n \n \n<p> The next opportunity can reuse the same architecture and even the same sensor health algorithms (e.g., for dealing with unreliable sensor data) accelerating the time to standup a solution by allowing the project to focus on onboarding the domain specific algorithms.<br /> </p> \n<p><strong> Posted by Teresa Tung, Ph.D.  <a rel=\"nofollow\" target=\"_blank\" href=\"http://www.accenture.com\">Accenture </a>Technology Labs' Next Generation Software Architecture R&amp;D group</strong></p>",
        "created_at": "2018-07-26T19:36:49+0000",
        "updated_at": "2018-08-07T22:36:32+0000",
        "published_at": null,
        "published_by": [
          "Accenture Technology Labs Staff"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 4,
        "domain_name": "www.techworld.com",
        "preview_picture": "https://cdn2.techworld.com/graphics/fallbackimages/30/Big-data-binary-code-futuristic_thumb800.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11605"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 25,
            "label": "react",
            "slug": "react"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 177,
            "label": "graphql",
            "slug": "graphql"
          },
          {
            "id": 883,
            "label": "java",
            "slug": "java"
          },
          {
            "id": 956,
            "label": "streaming",
            "slug": "streaming"
          }
        ],
        "is_public": false,
        "id": 11604,
        "uid": null,
        "title": "Building a Custom Spark Connector for Near Real-Time Speech-to-Text Transcription - Developer Blog",
        "url": "https://www.microsoft.com/developerblog/2017/11/01/building-a-custom-spark-connector-for-near-real-time-speech-to-text-transcription/",
        "content": "<img src=\"https://www.microsoft.com/developerblog//wp-content/uploads/fortis-radio-to-text-spark-450x300.png\" alt=\"image\" /><p>The <a href=\"http://aka.ms/fortis\">Fortis project</a> is a social data ingestion, analysis, and visualization platform. Originally developed in collaboration with the <a href=\"https://www.unocha.org/\">United Nations Office for the Coordination of Humanitarian Affairs</a> (UN OCHA), Fortis provides planners and scientists with tools to gain insight from social media, public websites and custom data sources.  UN OCHA mobilizes humanitarian aid for crises such as famines, epidemics, and war.</p><p>To understand crisis situations and formulate appropriate mitigations, UN OCHA needs access to tremendous amounts of data and intelligence, from a wide range of sources including newspapers, social media, and radio.</p><p>Fortis already has support for social media and web content, but UN OCHA wanted to be able to incorporate content from local radio broadcasts. Working together, we created a solution based on Spark Streaming to extract textual information from radio in near real-time by developing a new <a href=\"https://github.com/CatalystCode/SpeechToText-WebSockets-Java/\">Java client</a> for Azure Cognitive Services’ speech-to-text APIs. This code story delves into our Fortis solution by providing examples of Spark Streaming custom receivers needed to consume Azure Cognitive Service’s speech-to-text API and showing how to integrate with Azure Cognitive Service’s speech-to-text protocol.</p><h2>A Spark Streaming pipeline for analyzing radio</h2><p>We covered the Fortis pipeline in a <a href=\"https://www.microsoft.com/developerblog/2017/05/10/graphql-providing-context-into-global-crisiss-and-social-public-data-sources/\">previous code story</a>. Below is a brief summary of the pipeline:</p><ol><li>Real-time data sources (such as Facebook, Twitter, and news feeds) generate events</li><li>Events are filtered and analyzed by Spark Streaming</li><li>Spark stores events and aggregations in Cassandra for reporting uses</li></ol><p>A NodeJS and GraphQL services layer sends the data from Cassandra to a ReactJS dashboard.</p><p><img class=\"alignnone size-large wp-image-4891\" src=\"https://codestoryedge.azureedge.net/developerblog/wp-content/uploads/radio-codestory-fortis-diagram-1024x447.png\" alt=\"Overview of the Fortis pipeline\" width=\"780\" height=\"340\" srcset=\"https://www.microsoft.com/developerblog//wp-content/uploads/radio-codestory-fortis-diagram-1024x447.png 1024w, https://www.microsoft.com/developerblog//wp-content/uploads/radio-codestory-fortis-diagram-300x131.png 300w, https://www.microsoft.com/developerblog//wp-content/uploads/radio-codestory-fortis-diagram-768x335.png 768w\" /></p><p>To add radio analysis to this pipeline, we made three simplifying assumptions:</p><ul><li>The raw radio stream is freely accessible on the web via HTTP</li><li>The content published via the radio stream can be discretized into short segments</li><li>The radio content can be meaningfully analyzed via its textual transcription</li></ul><p>Once framed in this way, we see that we need two pieces to ingest radio into a Spark Streaming pipeline:</p><ol><li>A mechanism to get transcribed radio content into our Spark cluster for processing via our text-based analyses <a href=\"https://www.microsoft.com/developerblog/2017/05/10/graphql-providing-context-into-global-crisiss-and-social-public-data-sources/\">as described in a previous code story</a></li><li>A process to receive audio from a radio feed (such as the <a href=\"http://bbcwssc.ic.llnwd.net/stream/bbcwssc_mp1_ws-einws\">BBC World Service</a> or Radio France International) and convert it to text</li></ol><p>The following sections will cover each of these pieces.</p><h2>Consuming arbitrary APIs via Spark Streaming custom receivers</h2><p>Spark’s <a href=\"https://spark.apache.org/docs/latest/streaming-custom-receivers.html\">custom receivers</a> are a powerful mechanism to turn APIs into real-time data sources for Spark Streaming. A custom receiver needs to implement just a few methods. For example, in Scala:</p><p>After we define our custom receiver, we can now turn its underlying data source into a Spark stream that is easy to consume via the usual high-level Spark APIs:</p><p>We make heavy use of custom receivers in the Fortis project as a simple way to turn APIs into streaming data-sources. Fortis has published packages to integrate with the <a href=\"https://github.com/CatalystCode/streaming-facebook\">Facebook API</a>, <a href=\"https://github.com/CatalystCode/streaming-reddit\">Reddit API</a>, <a href=\"https://github.com/CatalystCode/streaming-instagram\">Instagram API</a>, <a href=\"https://github.com/CatalystCode/streaming-bing\">Bing News API</a> and <a href=\"https://github.com/CatalystCode/streaming-rss-html\">RSS feeds</a>. Similarly, the radio-to-text functionality described below is wrapped in a <a href=\"https://github.com/CatalystCode/project-fortis-spark/tree/f2b8d2dec955c5f78eed00ff884a86a40b5b255b/src/main/scala/com/microsoft/partnercatalyst/fortis/spark/sources/streamwrappers/radio\">custom radio receiver</a> to integrate it into our Spark pipeline.</p><h2>Converting radio to text in near real-time</h2><p>After solving the problem of how to integrate custom data sources into Spark, we next require a solution to convert a stream of audio, such as from an online radio broadcast, to a stream of text.</p><p>Azure Cognitive Services has been offering speech-to-text capabilities for more than 10 languages for a long time via the <a href=\"https://azure.microsoft.com/en-ca/services/cognitive-services/speech/\">Bing Speech API</a>. However, the API is based on a request-response paradigm which is not suited to our streaming use case as it would require us to buffer large audio clips in the radio receiver, send the chunks to the speech-to-text service and wait for a transcribed response. This approach would introduce latency and HTTP request overhead.</p><p>However, Azure Cognitive Services recently published a <a href=\"https://docs.microsoft.com/en-us/azure/cognitive-services/speech/api-reference-rest/websocketprotocol\">speech-to-text protocol</a> that works via WebSockets which are well suited for a streaming scenario. We simply open a single bi-directional communication channel via a WebSocket connection and use it to continuously send audio to the speech-to-text service and receive back the transcribed results via the same channel.</p><p>A reference implementation of this speech-to-text approach via WebSockets exists for <a href=\"https://github.com/Azure-Samples/SpeechToText-WebSockets-Javascript\">Javascript</a> and an unofficial implementation exists for <a href=\"https://github.com/noopkat/ms-bing-speech-service\">NodeJS</a>. We created a new implementation of the protocol in <a href=\"https://github.com/CatalystCode/SpeechToText-WebSockets-Java/\">Java</a> so that we can leverage the WebSocket speech-to-text from our Scala-on-Spark pipeline.</p><h3>How does the WebSocket speech-to-text protocol work?</h3><p>After creating a Bing Speech API resource in Azure and noting the access key, we’re ready to start implementing the speech-to-text WebSocket protocol.</p><p><img class=\"alignnone size-full wp-image-4892\" src=\"https://codestoryedge.azureedge.net/developerblog/wp-content/uploads/radio-codestory-azure-dashboard.png\" alt=\"Creating the required resources in Azure\" width=\"564\" height=\"271\" srcset=\"https://codestoryedge.azureedge.net/developerblog/wp-content/uploads/radio-codestory-azure-dashboard.png 564w, https://codestoryedge.azureedge.net/developerblog/wp-content/uploads/radio-codestory-azure-dashboard-300x144.png 300w\" /></p><p>The speech-to-text WebSocket protocol specifies three phases that clients must implement:</p><ol><li>Establishing the speech-to-text WebSocket connection</li><li>Transcribing audio via the speech-to-text WebSocket</li><li>Closing the speech-to-text WebSocket connection</li></ol><p>In the first phase, the client identifies itself with the speech-to-text service and establishes the WebSocket connection. After identification, the client is free to send audio to the speech-to-text service and receive back transcriptions. Eventually, the speech-to-text service will detect the end of the audio stream. When the end is detected, the speech-to-text service will send the client an alert that the audio stream is done getting messages. The client will respond to the service’s end-of-the-audio message by closing the WebSocket speech-to-text connection.</p><p>The full end-to-end flow is illustrated in the diagram below and each phase is described in more detail in the following sections.</p><p><img class=\"alignnone size-large wp-image-4893\" src=\"https://codestoryedge.azureedge.net/developerblog/wp-content/uploads/radio-codestory-tts-pipeline-1024x807.png\" alt=\"WebSocket speech-to-text messages flow\" width=\"780\" height=\"615\" srcset=\"https://www.microsoft.com/developerblog//wp-content/uploads/radio-codestory-tts-pipeline-1024x807.png 1024w, https://www.microsoft.com/developerblog//wp-content/uploads/radio-codestory-tts-pipeline-300x236.png 300w, https://www.microsoft.com/developerblog//wp-content/uploads/radio-codestory-tts-pipeline-768x605.png 768w, https://www.microsoft.com/developerblog//wp-content/uploads/radio-codestory-tts-pipeline.png 1348w\" /></p><h4>Phase 1: Establishing the speech-to-text WebSocket connection</h4><p>To kick off a speech-to-text session, we need to establish a connection with the Bing speech-to-text WebSocket service and identify our client via our API token for the Bing service. Note that all authentication and configuration information must be passed in via query parameters on the WebSocket connection URL since many WebSocket clients don’t have support for headers that would typically be used for sending information like API tokens. For example, using the Java <a href=\"https://github.com/TakahikoKawasaki/nv-websocket-client\">NV WebSocket client</a>, we’d connect to the speech-to-text service as follows:</p><p>After the WebSocket connection is established, we must send additional identifying information about our client (like the operating system used, the audio source being transcribed, etc.) to the speech-to-text service via a WebSocket text message and then we’re ready to transcribe audio:</p><h4>Phase 2: Transcribing audio via the speech-to-text WebSocket</h4><p>Once the client has successfully identified itself with the speech-to-text service, it can start sending chunks of audio for transcription, via binary WebSocket messages:</p><p>The messages follow a specific schema:</p><p><img class=\"alignnone size-large wp-image-4894\" src=\"https://www.microsoft.com/developerblog//wp-content/uploads/radio-codestory-tts-audio-message-schema-1024x592.png\" alt=\"Format of speech-to-text audio transfer messages\" width=\"780\" height=\"451\" srcset=\"https://codestoryedge.azureedge.net/developerblog/wp-content/uploads/radio-codestory-tts-audio-message-schema-1024x592.png 1024w, https://codestoryedge.azureedge.net/developerblog/wp-content/uploads/radio-codestory-tts-audio-message-schema-300x173.png 300w, https://codestoryedge.azureedge.net/developerblog/wp-content/uploads/radio-codestory-tts-audio-message-schema-768x444.png 768w\" /></p><p>Note that the audio data sent to the speech-to-text service must be single-channel mono WAV audio sampled at 16kHz with 16 bits per sample. The speech-to-text service does not validate the format of the audio messages and will silently return incorrect transcriptions on receipt of messages with malformed encodings.</p><p>Most online radio is in MP3 rather than WAV format, making an audio format conversion step necessary for the radio input to be processed by the speech-to-text service. The Java ecosystem has a lack of great open source, permissively licensed streaming MP3 decoders; as a result, our client resorts to converting small batches of MP3 audio to WAV on disk using the <a href=\"https://github.com/pdudits/soundlibs/tree/master/jlayer/src/javazoom/jl/converter\">javazoom converter</a> and then <a href=\"https://github.com/CatalystCode/SpeechToText-WebSockets-Java/blob/182cb301d41427a7a8dd9221ad91aa7922f2d559/src/main/java/com/github/catalystcode/fortis/speechtotext/websocket/MessageSender.java#L58-L101\">resampling</a> the WAV using the javax.sound APIs with the <a href=\"http://www.tritonus.org\">Tritonus plugin</a>. Hopefully, in the future, the trip to disk will be made unnecessary by moving to a streaming or in-memory MP3 decoder.</p><p>After the client starts sending audio messages to the speech-to-text service, the service will transcribe the audio and send the transcription results back to the client. Via WebSocket, transcriptions will be sent with a JSON payload for the client to parse and process. Note that the service may choose to batch multiple input audio messages sent by the client into a single output transcription message returned to the client.</p><h4>Phase 3: Closing the speech-to-text WebSocket connection</h4><p>When the client wishes to close the connection with the speech-to-text service, it must execute three steps in sequence:</p><ol><li>Send an audio message as described in Phase 2, but with a zero-byte audio payload</li><li>Send telemetry as a JSON WebSocket text message about its perception of the speech-to-text service’s performance</li><li>Shut down the WebSocket connection</li></ol><p>Sending the connection-end telemetry is like the client-information telemetry described in Phase 1 above. The schema for the JSON payload of the message is <a href=\"https://docs.microsoft.com/en-us/azure/cognitive-services/speech/api-reference-rest/websocketprotocol#telemetry-schema\">well documented</a>.</p><h2>Summary</h2><p>In many parts of the world, essential information is broadcast via traditional sources such as radio. To formulate effective crisis response plans, the United Nations Office for the Coordination of Humanitarian Affairs needs to be able to access and analyze these data sources in a timely manner.</p><p>To solve this problem, we created a <a href=\"https://github.com/CatalystCode/SpeechToText-WebSockets-Java/\">Java client</a> for the Azure Cognitive Services <a href=\"https://azure.microsoft.com/en-ca/services/cognitive-services/speech/\">speech-to-text</a> WebSocket protocol. We then fed the transcribed radio into a pipeline based on Spark Streaming for further analysis, augmentation, and aggregation. This solution enabled us to ingest and analyze radio in near real-time.</p><p>Our <a href=\"https://github.com/CatalystCode/SpeechToText-WebSockets-Java/\">Java client</a> is reusable across a wide range of text-to-speech scenarios that require time-efficient speech-to-text transcription in more than 10 languages including English, French, Spanish, German and Chinese.</p><h2>Resources</h2><ul><li><a href=\"https://github.com/CatalystCode/SpeechToText-WebSockets-Java/\">Java client for speech-to-text WebSocket protocol</a></li><li><a href=\"https://github.com/Azure-Samples/SpeechToText-WebSockets-Javascript\">JavaScript client for speech-to-text WebSocket protocol</a></li><li><a href=\"https://github.com/noopkat/ms-bing-speech-service\">NodeJS client for speech-to-text WebSocket protocol</a></li><li><a href=\"https://github.com/CatalystCode/project-fortis-spark\">Fortis project using speech-to-text WebSocket and Spark custom receivers</a></li><li><a href=\"https://azure.microsoft.com/en-ca/services/cognitive-services/speech/\">Bing speech-to-text documentation</a></li></ul>",
        "created_at": "2018-07-26T19:02:21+0000",
        "updated_at": "2018-08-07T22:37:14+0000",
        "published_at": "2017-11-01T07:31:36+0000",
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 8,
        "domain_name": "www.microsoft.com",
        "preview_picture": "https://www.microsoft.com/developerblog//wp-content/uploads/fortis-radio-to-text-spark.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11604"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 190,
            "label": "sql",
            "slug": "sql"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          }
        ],
        "is_public": false,
        "id": 11593,
        "uid": null,
        "title": "Introducing FiloDB",
        "url": "http://velvia.github.io/Introducing-FiloDB/",
        "content": "<p>If you are a big data analyst, or build big data solutions for fast analytical queries, you are likely familiar with columnar storage technologies.  The open source <a href=\"http://parquet.io\">Parquet</a> file format for HDFS saves space and powers query engines from Spark to Impala and more, while cloud solutions like Amazon Redshift use columnar storage to speed up queries and minimize I/O.  Being a file format, Parquet is much more challenging to work with directly for real-time data ingest.  For applications like IoT, time-series, and event data analytics, many developers have turned to NoSQL databases such as Apache Cassandra, due to their combination of high write scalability and the ease of using an idempotent, primary key-based database API.  Most NoSQL databases are not designed for fast, bulk analytical scans, but instead for highly concurrent key-value lookups.  What is missing is a solution that combines the ease of use of a database API, the scalability of NoSQL databases, with columnar storage technology for fast analytics.</p><h2 id=\"introducing-filodb--distributed--versioned--columnar\">Introducing FiloDB.  Distributed.  Versioned.  Columnar.</h2><p>I am excited to announce FiloDB, a new open-source distributed columnar database from <a href=\"http://www.tuplejump.com\">TupleJump</a>.  FiloDB is designed to ingest streaming data of various types, including machine, event, and time-series data, and run very fast analytical queries over them.  In four-letter acronyms, it is an OLAP solution, not OLTP.</p><ul><li><strong>Distributed</strong> - FiloDB is designed from the beginning to run on best-of-breed distributed, scale-out storage platforms such as Apache Cassandra.  Queries run in parallel in Apache Spark for scale-out ad-hoc analysis.</li>\n  <li><strong>Columnar</strong> - FiloDB brings breakthrough performance levels for analytical queries by using a columnar storage layout with different space-saving techniques like dictionary compression.  The performance is comparable to Parquet, and one to two orders of magnitude faster than Spark on Cassandra 2.x for analytical queries.  For the POC performance comparison, please see the <a href=\"http://github.com/velvia/cassandra-gdelt\">cassandra-gdelt</a> repo.</li>\n  <li><strong>Versioned</strong> - At the same time, row-level, column-level operations and built in versioning gives FiloDB far more flexibility than can be achieved using file-based technologies like Parquet alone.</li>\n</ul><p><img src=\"http://velvia.github.io/images/filodb_architecture.png\" alt=\"FiloDB architecture\" /></p><h2 id=\"your-database-for-fast-streaming--big-data\">Your Database for Fast Streaming + Big Data</h2><p>FiloDB is designed for <strong>streaming</strong> applications.  Enable easy exactly-once ingestion from Apache Kafka for streaming events, time series, and IoT applications - yet enable extremely fast ad-hoc analysis using the ease of use of SQL.  Each row is keyed by a partition and sort key, and writes using the same key are idempotent.  Idempotent writes enables exactly-once storage of event data.  FiloDB does the hard work of keeping data stored in an efficient and read-optimized, sorted format.</p><h2 id=\"filodb--cassandra--spark--lightning-fast-analytics\">FiloDB + Cassandra + Spark = Lightning-fast Analytics</h2><p>FiloDB leverages <a href=\"https://cassandra.apache.org/\">Apache Cassandra</a> as its storage engine, and <a href=\"http://spark.apache.org\">Apache Spark</a> as its compute layer.  Apache Cassandra is one of the most widely deployed, rock-solid distributed databases in use today, with very well understood operational characteristics.  Many folks are combining Apache Spark with their Cassandra tables for much richer analytics on their Cassandra data than is possible with just the native Cassandra CQL interface.  However, loading massive amounts of data from Cassandra into Spark can still be very slow, especially for analytics and ad-hoc queries such as averaging or computing the correlation between two columns of data.  This is because Cassandra CQL tables stores data in a row-oriented manner.  FiloDB brings the benefits of efficient columnar storage and the flexibility and richness of Apache Spark to the rock solid storage technology of Cassandra, speeding up analytical queries by up to 100x over Cassandra 2.x.</p><h2 id=\"easy-ingestion--sql--jdbc--spark-ml\">Easy Ingestion + SQL + JDBC + Spark ML</h2><p>FiloDB uses Apache Spark SQL and DataFrames as the main query mechanism.  This lets you run familiar SQL queries over your data, and easily connect tools such as Tableau to query your data, using Spark’s JDBC connector.  At the same time, the full power of Spark is available for your data, including the machine learning MLlib library and GraphX for graph processing.</p><p>Ingesting data is also very easy through Spark DataFrames.  This means you can easily ingest data from any JDBC data source, Parquet and Avro files, Cassandra tables, and much, much, more.  This includes easily inserting data from Spark Streaming and Apache Kafka.</p><h2 id=\"simplify-your-analytics-stack--filodb--smack-for-everything\">Simplify your Analytics Stack.  FiloDB + SMACK for everything.</h2><p>Use Kafka + Spark + Cassandra + FiloDB to power your entire Lamba architecture implementation.  There is no need to implement a complex Lambda dual ingestion pipeline with both Cassandra and Hadoop!   You can use the <a href=\"http://noetl.org\">SMACK</a> stack (Spark/Scala, Mesos, Akka, Cassandra, Kafka) for a much bigger portion of your analytics stack than before, reducing infrastructure investment.</p><h2 id=\"whats-in-the-name\">What’s in the name?</h2><p><img src=\"http://velvia.github.io/images/Dantat.jpg\" alt=\"Dan tat\" /></p><p>I love desserts, and Filo dough is an essential ingredient.  One can think of columns and versions of data as layers, and FiloDB wrapping the layers in a yummy high-performance analytical database engine.</p><h2 id=\"proudly-built-with-the-typesafe-stack\">Proudly built with the Typesafe Stack</h2><p>FiloDB is built with the Typesafe reactive stack for high-performance distributed computing and asynchronous I/O - Scala, Spark, and <a href=\"http://akka.io\">Akka</a>.</p><h2 id=\"come-to-the-talk-at-cassandra-summit\">Come to the Talk at Cassandra Summit!</h2><p>If you’d like to learn more, I encourage you to come on over to <a href=\"http://cassandrasummit-datastax.com/?source=\">Cassandra Summit</a> in Santa Clara, where I’ll be speaking about FiloDB and Spark and Cassandra, on Thursday September 24th at the Santa Clara Convention Center!  Or feel free to reach out.  The repo and more details such as the roadmap will be unveiled at the talk.</p>",
        "created_at": "2018-07-26T15:14:59+0000",
        "updated_at": "2018-08-03T19:04:40+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 4,
        "domain_name": "velvia.github.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11593"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1232,
            "label": "stress",
            "slug": "stress"
          }
        ],
        "is_public": false,
        "id": 11592,
        "uid": null,
        "title": "velvia/cassandra-gdelt",
        "url": "https://github.com/velvia/cassandra-gdelt",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\"><h2><a id=\"user-content-cassandra-gdelt\" class=\"anchor\" aria-hidden=\"true\" href=\"#cassandra-gdelt\"></a>cassandra-gdelt</h2>\n<p>Just a simple test of different Cassandra layouts and their impact on the query / IO speed of the GDELT dataset.</p>\n<h3><a id=\"user-content-setup\" class=\"anchor\" aria-hidden=\"true\" href=\"#setup\"></a>Setup</h3>\n<p><a href=\"http://data.gdeltproject.org/events/index.html\" rel=\"nofollow\">GDELT dataset</a>, 1979-1984, 4,037,539 records, 57 columns* (There are actually 60 columns, but the last 3 columns are not populated for that date range)</p>\n<p>(NOTE: the current code is designed only to parse data from the link above, which is TAB-delimited, and does NOT have a header row)</p>\n<ul><li>Local MacBook Pro,  2.3GHz Core i7, 16GB RAM, SSD</li>\n<li>Cassandra 2.1.6, installed locally with one node using CCM</li>\n<li>Benchmark run using <code>sbt run</code></li>\n</ul><p>NOTE: if you run this benchmark with C* 2.0.x, <code>GdeltCaseClass2</code> ingestion may crash C*.</p>\n<p>Query and disk space benchmarks were run after a compaction cycle, with snapshots removed.</p>\n<h3><a id=\"user-content-benchmark-results---gdeltcaseclass\" class=\"anchor\" aria-hidden=\"true\" href=\"#benchmark-results---gdeltcaseclass\"></a>Benchmark Results - GdeltCaseClass</h3>\n<p>This is a simple layout with one primary key, so one physical row per record.  One would think that Cassandra could do a massive multiget_slice to save time on reading one column out of 20, but it doesn't save much time.</p>\n<p>LZ4 disk compression is enabled.</p>\n<p>Space taken up by all records: 1.9GB</p>\n<table><thead><tr><th align=\"left\">What</th>\n<th align=\"left\">Time</th>\n<th align=\"left\">Records/sec</th>\n</tr></thead><tbody><tr><td align=\"left\">Ingestion from CSV</td>\n<td align=\"left\">1927 s</td>\n<td align=\"left\">2091 rec/s</td>\n</tr><tr><td align=\"left\">Read every column</td>\n<td align=\"left\">505 s</td>\n<td align=\"left\">7998 rec/s</td>\n</tr><tr><td align=\"left\">Read 1 col (monthYear)</td>\n<td align=\"left\">504 s</td>\n<td align=\"left\">8005 rec/s</td>\n</tr></tbody></table><h3><a id=\"user-content-benchmark-results---gdeltcaseclass2\" class=\"anchor\" aria-hidden=\"true\" href=\"#benchmark-results---gdeltcaseclass2\"></a>Benchmark Results - GdeltCaseClass2</h3>\n<p>This is an improvement on GdeltCaseClass, with use of both a partition key which is a simple grouping of the primary keys, and a clustering key, to effect wide rows for faster linear reads from disk.  In addition, the names of the columns have been shortened to use up less disk space.</p>\n<p>Space taken up by all records: 2.2GB</p>\n<table><thead><tr><th align=\"left\">What</th>\n<th align=\"left\">Time</th>\n<th align=\"left\">Records/sec</th>\n</tr></thead><tbody><tr><td align=\"left\">Ingestion from CSV</td>\n<td align=\"left\">3190 s</td>\n<td align=\"left\">1300 rec/s</td>\n</tr><tr><td align=\"left\">Read every column</td>\n<td align=\"left\">941 s</td>\n<td align=\"left\">4417 rec/s</td>\n</tr><tr><td align=\"left\">Read 1 col (monthYear)</td>\n<td align=\"left\">707 s</td>\n<td align=\"left\">5880 rec/s</td>\n</tr></tbody></table><p>Oh no, what happened?  It seems it is slower to query, and it takes up more space on disk as well (logically, wide rows takes up more space uncompressed due to compound column names).  This is actually not entirely surprising, because reading only one column from such a layout, due to the way Cassandra lays out its data, essentially means having to scan all the data in a partition, and using clustering keys is inefficient because of how Cassandra prefixes clustering keys to the column names.  With the skinny layout, Cassandra is actually able to skip part of the row when reading.</p>\n<p>However, let's say you were not scanning the whole table but instead had a WHERE clause to filter on your partition key.  In this case, this layout would be a huge win over the other one -- only data from one partition needs to be read.  Thus, pick a layout based on your needs.</p>\n<h3><a id=\"user-content-compact-storage\" class=\"anchor\" aria-hidden=\"true\" href=\"#compact-storage\"></a>COMPACT STORAGE</h3>\n<p>COMPACT STORAGE is the old Cassandra 0.x - 1.x way of storing things - write your record as a blob of your choice (JSON, Protobuf, etc.) and Cassandra writes it as a single cell (physical row, column, blob).  It is not supposed to be supported going forward.  You need to parse the blob yourself, and the entire blob must be read.  If you use Protobuf or other binary format, then CQLSH won't show you the contents (in this case we are using a trick and joining the text fields together using a special \\001 character, which shows up in a different color).  Writes are fast, but reads are still much slower than columnar layout.</p>\n<p>Disk space: 260M</p>\n<table><thead><tr><th align=\"left\">What</th>\n<th align=\"left\">Time</th>\n<th align=\"left\">Records/sec</th>\n</tr></thead><tbody><tr><td align=\"left\">Ingestion from CSV</td>\n<td align=\"left\">78.6 s</td>\n<td align=\"left\">52877 rec/s</td>\n</tr><tr><td align=\"left\">Read every column</td>\n<td align=\"left\">81.8 s</td>\n<td align=\"left\">50850 rec/s</td>\n</tr><tr><td align=\"left\">Read 1 col (monthYear)</td>\n<td align=\"left\">81.8 s</td>\n<td align=\"left\">50850 rec/s</td>\n</tr></tbody></table><h3><a id=\"user-content-columnar-layout\" class=\"anchor\" aria-hidden=\"true\" href=\"#columnar-layout\"></a>Columnar Layout</h3>\n<div class=\"highlight highlight-source-sql\"><pre>CREATE TABLE data (\n  dataset text,\n  version int,\n  shard int,\n  columnname text,\n  rowid int,\n  bytes blob,\n  PRIMARY KEY ((dataset, version, shard), columnname, rowid)\n)</pre></div>\n<p>This layout places values of the same column from different rows together, and also serializes multiple row values into one cell.</p>\n<p>Dictionary encoding enabled for about 75% of column chunks\n(auto-detection with a 50% cardinality threshold for enabling dictionary encoding)</p>\n<p>Space taken up by records:  266MB .... !!!\n(LZ4 Compressed SSTable size; uncompressed actual ByteBuffers are 918MB)</p>\n<table><thead><tr><th align=\"left\">What</th>\n<th align=\"left\">Time</th>\n<th align=\"left\">Records/sec</th>\n</tr></thead><tbody><tr><td align=\"left\">Ingestion from CSV</td>\n<td align=\"left\">93.0 s</td>\n<td align=\"left\">43324 rec/s</td>\n</tr><tr><td align=\"left\">Read every column</td>\n<td align=\"left\">8.6 s</td>\n<td align=\"left\">468,980 rec/s</td>\n</tr><tr><td align=\"left\">Read 1 col (monthYear)</td>\n<td align=\"left\">0.23 s</td>\n<td align=\"left\"><strong>17.4 million rec/s</strong></td>\n</tr></tbody></table><p>The speedup and compactness is shocking.</p>\n<ul><li>On ingest - roughly 20-35x faster and 7x less disk space (of course this is from CSV with essentially no primary key, append only, probably not realistic)</li>\n<li>On reads - 42x to 59x faster for reads of all columns, and 355 - 2190x faster for read of a single column\n<ul><li>Granted, the speedup is for parsing an integer column, which is the most compact and benefits the most from efficient I/O; parsing a string column will not be quite as fast (though dictionary-encoded columns are very fast in deserialization)</li>\n</ul></li>\n<li>Dictionary encoding saves a huge amount of space, cutting the actual storage\nspace to one-third of the columnar one, both uncompressed and compressed. (sorry results without dictionary encoding are not available since a code change is needed to test that out)\n(Interesting that the compression ratio seems constant)</li>\n</ul><p>Is this for real?  Gathering stats of the data being read shows that it is:</p>\n<ul><li><code>GdeltDataTableQuery</code> compiles stats which show that every column is being read, the # of shards, chunks, and bytes seem to all make sense.  Evidently LZ4 is compressing data to roughly 1/4 of the total size of all the bytebuffers.  This debunks the theory that perhaps not all the data is being read.</li>\n<li>For the monthYear col, exactly 4037539 elements are being read back, and a top K of the monthYear values matches exactly with values derived from the original source CSV file</li>\n</ul><p>Also, FlatBuffers leaves lots of zeroes in the binary output, so there is plenty of room for improvement, plus the code for parsing the binary FlatBuffers has not been optimized at all.... plus LZ4 and different C* side compression schemes and their effects too.</p>\n<h3><a id=\"user-content-capn-proto\" class=\"anchor\" aria-hidden=\"true\" href=\"#capn-proto\"></a>Cap'n Proto</h3>\n<p>This is not yet working due to bugs in capnproto-java.  Notes for the setup:</p>\n<ol><li>\n<p>Build and install Cap'n Proto Schema Compiler</p>\n</li>\n<li>\n<p>Clone and build the java repo using <a href=\"https://dwrensha.github.io/capnproto-java/index.html\" rel=\"nofollow\">these instructions</a></p>\n</li>\n<li>\n<p>To build the schema, do something like</p>\n<pre> capnp compile -o../../capnproto-java/capnpc-java -I../../capnproto-java/compiler/src/main/schema column_storage.capnp\n</pre>\n</li>\n</ol><h3><a id=\"user-content-for-additional-investigation\" class=\"anchor\" aria-hidden=\"true\" href=\"#for-additional-investigation\"></a>For Additional Investigation</h3>\n<p>Right now the above comparison is just for C*, LZ4 C* disk compression, using the Phantom client.  Much more testing and performance evaluation would be needed to compare against, for example, Parquet, and to isolate the effects of</p>\n<ul><li>C* itself, and the disk compression scheme used</li>\n<li>Effects of the Phantom client</li>\n<li>FlatBuffers vs Capt'n Proto</li>\n</ul><p>Another good dataset to test against is NYC Taxi Trip data: <a href=\"http://www.andresmh.com/nyctaxitrips/\" rel=\"nofollow\">http://www.andresmh.com/nyctaxitrips/</a>.   There are two helper scripts: <code>split_csv.sh</code> helps break up a big CSV into smaller CSVs with header intact, and <code>socrata_pointify.sh</code> adds a point column from lat and long columns.</p>\n<h2><a id=\"user-content-spark-on-cassandra\" class=\"anchor\" aria-hidden=\"true\" href=\"#spark-on-cassandra\"></a>Spark on Cassandra</h2>\n<p>The above can also be used to compare Spark on Cassandra reads.  First start up the spark shell, like this (we ensure there is only one thread for all tests for an even comparison):</p>\n<pre>bin/spark-shell \\\n              --packages com.datastax.spark:spark-cassandra-connector_2.10:1.4.0-M3 \\\n              --conf spark.cassandra.connection.host=127.0.0.1 --conf spark.cassandra.input.split.size_in_mb=256 \\\n              --conf spark.sql.shuffle.partitions=4 \\\n              --driver-memory 5G --master \"local[1]\"\n</pre>\n<p>The split size is to ensure that the GDELT2 wide row table doesn't get too many splits when reading.  This is really tricky to configure by the way, as it is global but you will probably need different settings for different tables.</p>\n<p>To load a Cassandra table into a Spark DataFrame, do something like this:</p>\n<div class=\"highlight highlight-source-scala\"><pre>val df = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").\n                    option(\"table\", \"gdelt2\").\n                    option(\"keyspace\", \"test\").load\ndf.registerTempTable(\"gdelt\")</pre></div>\n<p>Here are the queries used.  Query 1:</p>\n<pre>df.select(count(\"numarticles\")).show\n</pre>\n<p>Query 2:</p>\n<pre>sqlContext.sql(\"SELECT a1name, AVG(avgtone) AS tone FROM gdelt GROUP BY a1name ORDER BY tone DESC\").show\n</pre>\n<p>Query 3:</p>\n<pre>sqlContext.sql(\"SELECT AVG(avgtone), MIN(avgtone), MAX(avgtone) FROM gdelt WHERE MonthYear = 198012\").show\n</pre>\n<p>NOTE: the above are for querying the Cassandra CQL tables, in which the code shortens column names for performance and storage compactness reasons.  For Parquet and FiloDB, use the regular, capitalized column names (<code>Actor1Name</code> instead of <code>a1name</code>, <code>AvgTone</code> instead of <code>avgtone</code>, etc.)</p>\n<p>TODO: instructions for querying the COMPACT STORAGE table</p>\n<p>For testing Spark SQL cached tables, do the following:</p>\n<pre>sqlContext.cacheTable(\"gdelt\")\nsqlContext.sql(\"select avg(numarticles), avg(avgtone) from gdelt group by a1name\").show\n</pre>\n<p>NOTE: For any DataFrame DSL (Scala) queries, make sure to get back a new DataFrame after the <code>cacheTable</code> operation, like this: <code>val df1 = sqlContext.table(\"gdelt\")</code>.  The cached DataFrame reference is not the same as the original!</p>\n<p>For FiloDB, the table is loaded just a tiny bit differently:</p>\n<pre>df = sqlContext.read.format(\"filodb.spark\").option(\"dataset\", \"gdelt\").option(\"splits_per_node\", \"1\").load\n</pre>\n<p>For Parquet, it is even more straightforward:</p>\n<pre>df = sqlContext.load(\"/path/to/my/1979-1984.parquet\")\n</pre>\n<p>For comparisons, I generated the Parquet file from one of the above Cassandra tables (not the COMPACT STORAGE one) by saving it out, like this:</p>\n<pre>df.save(\"/path/to/my/1979-1984.parquet\")\n</pre>\n</article>",
        "created_at": "2018-07-26T15:12:50+0000",
        "updated_at": "2018-07-26T15:13:02+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 7,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/1062875?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11592"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 182,
            "label": "mongo",
            "slug": "mongo"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          }
        ],
        "is_public": false,
        "id": 11590,
        "uid": null,
        "title": "Apache Cassandra turns 10",
        "url": "https://www.zdnet.com/article/apache-cassandra-turns-10/",
        "content": "<figure class=\"image  image-medium shortcode-image\"><img src=\"https://zdnet1.cbsistatic.com/hub/i/r/2018/07/21/55ed8a96-6a44-4c71-b77c-7c645fa5ca30/resize/370xauto/a884e9c7912167555670d90cdbf72607/cassandra.gif\" class=\"\" alt=\"cassandra.gif\" height=\"auto\" width=\"370\" /></figure><p>The past couple years have seen a number of 10-year milestones being passed, like the decade anniversaries of <a href=\"https://aws.amazon.com/10year/\">Amazon Web Services</a>, <a href=\"https://www.linkedin.com/pulse/mongodb-world-2017-lonely-story-versus-john-de-goes/\">MongoDB</a>, <a href=\"https://www.cloudera.com/promos/hadoop10.html\">Hadoop</a> and many others. And so in 2018, it's <a href=\"https://cassandra.apache.org/\">Apache Cassandra's</a> turn. Today, Apache Cassandra has morphed into a modest ecosystem where there is one principle commercial platform supplier -- <a href=\"http://datastax.com/\">DataStax</a> -- supplemented by a <a href=\"https://wiki.apache.org/cassandra/ThirdPartySupport\">small collection of companies</a> delivering third-party support. It combines the versatility of a table-oriented database with the speed and efficiency of a key-value store.</p><p>But make no mistake about it -- the fact that there aren't a dozen vendors of Cassandra distros doesn't hide up the fact that Cassandra is a very popular database. It is one of a quartet of NoSQL databases that rank in <a href=\"https://db-engines.com/en/ranking\">db-Engine's</a> top ten. And in itself, Cassandra has carved out a niche for continuous online systems that can carry up to PBytes of data. Like other \"wide column\" databases that began life as key-value stores, Cassandra was first known for fast writes, but over the years, read performance has caught up.</p><p>For instance, when you get film recommendations served up on Netflix, they come from an application running on Cassandra. It has carved presence with maintaining of online user profiles, shopping carts, fraud detection, and increasingly, real-time mobile and IoT applications. For that matter, so have most of Cassandra's prime NoSQL competitors like <a href=\"https://mongodb.com/\">MongoDB</a>, <a href=\"https://aws.amazon.com/dynamodb/\">DynamoDB</a>, and <a href=\"https://azure.microsoft.com/en-us/services/cosmos-db/\">Cosmos DB</a>.</p><p>As this is 10th birthday time, it makes sense to look at Cassandra's beginnings. The story is a familiar one. An Internet giant -- Facebook -- needed a more scalable, always-on database alternative for its inbox feature and created Cassandra back in 2008 based on the <a href=\"https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf\">Dynamo paper</a> published by Amazon. After open sourcing it, <a href=\"https://www.linkedin.com/in/jbellis/\">Jonathan Ellis</a>, an engineer at Rackspace at the time, saw its potential as a distributed database for powering cloud applications, and a year later, drew venture backing to cofound what is now DataStax with then-colleague <a href=\"https://www.linkedin.com/in/mattpfeil/\">Matt Pfeil</a>.</p><p>The biggest source of confusion early on was with Hadoop. Because of some <a href=\"https://www.theregister.co.uk/2011/03/23/cassandra_mashed_with_hadoop/\">ridiculous historical coincidences</a>, Cassandra got lumped into the Hadoop project where it still appears on the <a href=\"http://hadoop.apache.org/\">Apache project page</a>. That implies that Cassandra is an in-kind replacement for <a href=\"https://hbase.apache.org/\">HBase</a>. Well kinda and kinda not. Although both were initially designed to run as online production systems for big data, HBase requires HDFS, YARN, and Zookeeper to run, whereas Cassandra doesn't require Hadoop components and runs on its own cluster. Then there are other architectural differences, such as that HBase runs with Hadoop hierarchical topology, whereas Cassandra works in more of a peer-to-peer mode.</p><p><strong>Comparison to the usual suspects</strong></p><p>Hadoop flirtations notwithstanding, how does Cassandra differentiate from the usual NoSQL suspects? We'll start with the biggest differentiator: query language. Cassandra also has a <a href=\"https://cassandra.apache.org/doc/latest/cql/\">query language</a> that is much more like SQL compared to most rivals except <a href=\"https://www.couchbase.com/products/n1ql\">Couchbase</a>.</p>\n    <section class=\"sharethrough-top\" data-component=\"medusaContentRecommendation\" data-medusa-content-recommendation-options=\"{&quot;promo&quot;:&quot;promo_ZD_recommendation_sharethrough_top_in_article_desktop&quot;,&quot;spot&quot;:&quot;dfp-in-article&quot;}\">\n    </section><p>Compared to MongoDB, Cassandra was more write-friendly, but as both databases matured, differences in read and write performance are no longer as stark. Cassandra was initially designed as a tabular database for key-value data (compared to MongoDB's more object-like model), but in time was evolved to accommodate JSON documents. There are still basic differences in database topology: Cassandra was designed for higher availability writes with its multi-master architecture, whereas MongoDB uses a single master, but suggests <a href=\"https://www.mongodb.com/blog/post/active-active-application-architectures-with-mongodb\">managing sharding</a> for higher availability writes.</p><p>Among cloud-native counterparts, Cassandra shares lineage with Amazon DynamoDB. A detailed comparison can be found <a href=\"https://www.beyondthelines.net/databases/dynamodb-vs-cassandra/\">here</a>. But at high level, the obvious difference is where they run: DynamoDB only runs in <a href=\"http://aws.amazon.com/\">AWS</a> as a managed service (and likewise for Microsoft Azure Cosmos DB on <a href=\"http://azure.microsoft.com/\">Azure</a>); Cassandra, on the other hand, can run anywhere, but as managed service, <a href=\"https://www.datastax.com/products/datastax-managed-cloud\" target=\"_blank\">DataStax Managed Cloud Service</a> has only been introduced recently. Cassandra and DynamoDB both let you tune consistency levels -- Cassandra offers five options for consistency while DynamoDB narrows it down to two (eventual or strong). </p><p>Compared to Microsoft Azure Cosmos DB, the biggest difference is multi-model that is core to the Azure offering; by comparison, the commercial version of Cassandra -- <a href=\"https://www.datastax.com/products/datastax-enterprise\">DataStax Enterprise</a> -- is just starting on this road, as it is still integrating its graph model.</p><p><strong>Are we in a post-relational world?</strong></p><p>Given that four NoSQL databases have now made it to the mainstream (based on developer interest charted by db-Engines), one would think that the matter has been settled about the role that these platforms play. One would be wrong.</p><p>There's still healthy debate. On one side, there's the irrational exuberance of being in a <a href=\"https://www.datastax.com/2018/07/cassandras-journey-via-the-five-stages-of-grief\">post-relational world</a>. Yes, NoSQL databases have become very popular among database developers. And yes, DataStax does have its share of <a href=\"https://www.oracle.com/database/technologies/index.html\" target=\"_blank\">Oracle</a> run-ins, but these are going to be wins from outside of Oracle's core back office base. Actually, DataStax and Oracle are frenemies, as <a href=\"https://www.datastax.com/products/datastax-enterprise\" target=\"_blank\">DataStax Enterprise</a> (DSE) is one of the first third-party databases to become <a href=\"https://blogs.oracle.com/cloud-infrastructure/datastax-certified-nosql-cassandra-clusters-on-bare-metal-cloud\" target=\"_blank\">officially supported</a> in the Oracle Public Cloud's <a href=\"https://cloud.oracle.com/cloud-infrastructure\" target=\"_blank\">bare metal services</a>, but we digress.</p><p>Fortuitously, having spoken with  <a href=\"https://www.datastax.com/author/patrickdatastax-com\" target=\"_blank\">Patrick McFadin</a>, the five-stages-of-grief author, we've found his insights to be far more nuanced than his blog post would suggest. But there are many others taking more extreme views based on the notion of <a href=\"https://www.techopedia.com/2/32000/trends/big-data/why-the-world-is-moving-toward-nosql-databases\">big data becoming the mainstream</a>. On the other side, there's the constituency that still believes that <a href=\"https://read.acloud.guru/serverless-superheroes-lynn-langit-on-big-data-nosql-and-google-versus-aws-f4427dc8679c\">NoSQL is overhyped</a>.</p><p>Reality is much grayer. The fact that NoSQL databases like Cassandra allow schema to vary does not mean that they lack schema, or that developers should not bother with optimizing the database for specific types of schema. In a NoSQL database, schema still matters and so does table layout. Even if you don't design the data model exactly for the queries that you're going throw at it, you still need to consider which data the app will touch when laying out the tables.</p><p>Don't count relational out either. If your application or use case requires strict ACID guarantees and data with referential integrity, relational is going to be your choice. If the use case involves complex analytical queries, you have a couple options. You could go the NoSQL route if you denormalize the data to improve performance; design the application so you don't have to rely on complex table joins; and take advantage of the Spark connectors that are becoming checkbox items with commercial NoSQL databases like DataStax Enterprise. But if the purpose of the database is solely for analytics, NoSQL won't be the right route.</p><figure class=\"image  image-medium shortcode-image\"><img src=\"https://zdnet1.cbsistatic.com/hub/i/r/2018/07/21/c30e5d5f-c52d-409a-9076-4db9a09329a0/resize/370xauto/5bb3013fbc6e0bea9b81942248c45bea/datastaxlogo.png\" class=\"\" alt=\"datastaxlogo.png\" height=\"auto\" width=\"370\" /></figure><p><strong>DataStax and Cassandra today</strong></p><p>So what gives with Apache Cassandra and DataStax, the company that for most of its history was most closely associated with the database and open source project? It boils down to the nature of the open source project. Unlike MongoDB, which controls the underlying open source project and <a href=\"https://www.mongodb.com/community/licensing\">licenses the database</a> under <a href=\"https://www.gnu.org/licenses/agpl-3.0.en.html\">AGPL 3</a> license (which requires developers to contribute back to the community), Cassandra is an official Apache Foundation project that is governed by the Apache license.</p><p>So DataStax does not own or control Cassandra, and a couple years ago, <a href=\"https://sdtimes.com/apache/jonathan-ellis-steps-cassandra-project/\">stepped back</a> from <a href=\"https://www.datastax.com/2016/08/a-look-back-a-look-forward\">leadership</a> of the project. DataStax still contributes and maintains presence on the Cassandra project, but the bulk of its energies are in building the enterprise platform features around it. In essence, DataStax is becoming more of a classic \"open core\" software company, a strategy that is not all that different from <a href=\"http://cloudera.com/\">Cloudera's</a> on Hadoop.</p><p>With Cassandra at 10, DataStax still embraces the platform but views it as the starting point for additional features. It is reaching out to accommodate analytics and search with Spark connectivity and new search functions that have been added to its CQL query language. Then there is the addition of graph, which came from the 2015 acquisition of Aurelius that brought the leaders of the <a href=\"https://tinkerpop.apache.org/\">Apache TinkerPop</a> project to DataStax. While DataStax is still working to fully integrate graph into its implementation of Cassandra, in the <a href=\"https://www.datastax.com/products/datastax-enterprise-6\">DSE 6.0</a> release, you can load graph and Cassandra tables at the same time onto your cluster. And the company is now meeting cloud frenemies like Amazon head-on by rolling out the DataStax Managed Cloud service on AWS and Azure</p><p>There's a reason that we've been seeing all these tenth anniversaries in the big data space over the past few years. That's because in the first decade of the 2000s, a backlash formed against the post-Y2K consensus that we were at the end of times where n-tier was the de facto standard application architecture; .NET and Java were the predominant application development stacks; and relational databases were entrenched as the enterprise standard. Notably, it was the experiences of Internet companies like Amazon and Google who subsequently overthrew the enterprise IT order whose experiences with the limitations of the post-2000 technology stack gave rise to the innovations that are now hitting middle age.</p><p>A decade in, Cassandra is no longer the new kid on the block. But the database has become one of the fixtures of modern operational systems, and the company most associated with it is using it as a jumping off point to a broader platform.</p>",
        "created_at": "2018-07-25T23:47:44+0000",
        "updated_at": "2018-07-25T23:47:50+0000",
        "published_at": "2018-07-24T12:00:00+0000",
        "published_by": [
          "Tony Baer (Ovum)"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 7,
        "domain_name": "www.zdnet.com",
        "preview_picture": "https://zdnet3.cbsistatic.com/hub/i/r/2018/07/24/8f1d3195-a797-4ac9-84ed-e5df8d436128/thumbnail/770x578/42a0211ae1b12054513feb251e7502bb/image-2018-07-24-at-11-54-18-am.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11590"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1151,
            "label": "resources",
            "slug": "resources"
          },
          {
            "id": 1174,
            "label": "videos",
            "slug": "videos"
          }
        ],
        "is_public": true,
        "id": 11160,
        "uid": "5b57841c0897d7.47605420",
        "title": "Best Practices for Running Apache Cassandra on AWS",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=IuJldwJLyFM",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/IuJldwJLyFM?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-07-24T19:55:08+0000",
        "updated_at": "2018-07-24T19:55:08+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/IuJldwJLyFM/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11160"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1151,
            "label": "resources",
            "slug": "resources"
          },
          {
            "id": 1157,
            "label": "blogs",
            "slug": "blogs"
          }
        ],
        "is_public": true,
        "id": 11159,
        "uid": "5b5784143a7247.03635053",
        "title": "Pythian",
        "url": "https://www.pythian.com/blog/",
        "content": null,
        "created_at": "2018-07-24T19:55:00+0000",
        "updated_at": "2018-07-24T19:55:00+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": null,
        "language": null,
        "reading_time": 0,
        "domain_name": "www.pythian.com",
        "preview_picture": null,
        "http_status": null,
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11159"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1151,
            "label": "resources",
            "slug": "resources"
          },
          {
            "id": 1157,
            "label": "blogs",
            "slug": "blogs"
          }
        ],
        "is_public": true,
        "id": 11158,
        "uid": "5b57840749d536.45681186",
        "title": "Blog",
        "url": "https://www.datastax.com/blog",
        "content": "Blog | DataStax\n\n<noscript>\n\n\n\n<div class=\"DS17\"><div class=\"connect-us\"><a href=\"https://www.datastax.com/contactus\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Mail.svg\" alt=\"email icon\" />email</a><a href=\"https://www.datastax.com/company#offices\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Phone.svg\" alt=\"phone icon\" />call</a></div></div><header class=\"DS17\"><div class=\"container\"><div class=\"wrapper\"><div class=\"logo\"><a href=\"https://www.datastax.com/\"><img src=\"https://www.datastax.com/templates/dist/images/logo-header.png\" alt=\"DataStax logo\" /></a><a href=\"https://www.datastax.com/\"><img src=\"https://www.datastax.com/templates/dist/images/new_logo.png\" alt=\"DataStax logo\" /></a></div></div></div>\n  \n</header>\n    \n      <div class=\"DS17\"><div class=\"use-case\"><div class=\"wrapper\"><div class=\"two-col text-light-blue\"><h6>Customer Experience</h6><ul><li><a href=\"https://www.datastax.com/use-cases/customer-360\">Customer 360</a></li>\n          <li><a href=\"https://www.datastax.com/personalization\">Personalization &amp; Recommendations</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/loyalty-programs\">Loyalty Programs</a></li>\n          <li><a href=\"https://www.datastax.com/fraud-detection\">Consumer Fraud Detection</a></li>\n        </ul></div><div class=\"two-col text-light-green\"><h6><a href=\"#\">Enterprise Optimization</a></h6><ul><li><a href=\"https://www.datastax.com/use-cases/ecommerce\">eCommerce</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/identity-management\">Identity Management</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/security\">Security and Compliance</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/supply-chain\">Supply Chain</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/inventory-management\">Inventory Management</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/asset-monitoring\">Asset Monitoring</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/logistics\">Logistics</a></li>\n        </ul></div></div></div></div>\n  \n    \n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\n</noscript>",
        "created_at": "2018-07-24T19:54:47+0000",
        "updated_at": "2018-07-24T19:54:47+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 0,
        "domain_name": "www.datastax.com",
        "preview_picture": "https://www.datastax.com/wp-content/themes/datastax-2014-08/images/common/DataStax_Web_Social_DefaultGenericV2_1024x351_wide.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11158"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1151,
            "label": "resources",
            "slug": "resources"
          },
          {
            "id": 1173,
            "label": "troubleshooting and tuning",
            "slug": "troubleshooting-and-tuning"
          }
        ],
        "is_public": true,
        "id": 11157,
        "uid": "5b578400b7cc89.22172525",
        "title": "Tuning Java resources",
        "url": "https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/opsTuneJVM.html",
        "content": "<p class=\"shortdesc\">Tuning the Java Virtual Machine (JVM) can improve performance or reduce high memory\n    consumption.</p><p class=\"p\">Tuning the Java Virtual Machine (JVM) can improve performance or reduce high memory\n      consumption. </p><div class=\"p\">On this page:<ul class=\"ul\"><li class=\"li\"><a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/opsTuneJVM.html#opsTuneJVM__about-gc\">About garbage collection</a></li>\n        <li class=\"li\"><a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/opsTuneJVM.html#opsTuneJVM__choose-gc\">Choosing a Java garbage collector</a></li>\n        <li class=\"li\"><a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/opsTuneJVM.html#opsTuneJVM__setting-cms-gc\">Setting CMS as the Java garbage collector</a></li>\n        <li class=\"li\"><a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/opsTuneJVM.html#opsTuneJVM__tuning-the-java-heap\">Determining the heap size</a></li>\n        <li class=\"li\"><a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/opsTuneJVM.html#opsTuneJVM__how-cassandra-uses-memory\">How DataStax Enterprise uses memory</a> - Read first for a better\n          understanding of the settings and recommendations on this page.</li>\n        <li class=\"li\"><a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/opsTuneJVM.html#opsTuneJVM__adjusting_params_for_other_Cassandra_services\">Adjusting JVM parameters for other DataStax Enterprise services</a></li>\n        <li class=\"li\"><a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/opsTuneJVM.html#opsTuneJVM__jmx-options\">Other JMX options</a></li>\n      </ul></div><section class=\"section\" id=\"opsTuneJVM__choose-gc\"><h2 class=\"title sectiontitle\">Choosing a Java garbage collector</h2><div class=\"p\">DataStax Enterprise 5.1 uses the garbage first collector (G1) by default. G1 is recommended\n        for the following reasons:<ul class=\"ul\"><li class=\"li\">Heap sizes from 16 GB to 64 GB. <p class=\"p\">G1 performs better than CMS (concurrent-mark-sweep)\n              for larger heaps because it scans the regions of the heap containing the most garbage\n              objects first, and compacts the heap on-the-go, while CMS stops the application when\n              performing garbage collection.</p></li>\n          <li class=\"li\">The workload is variable, that is, the cluster is performing the different processes\n            all the time.</li>\n          <li class=\"li\">For future proofing, as CMS will be deprecated in Java 9.</li>\n          <li class=\"li\">G1 is easier to configure.</li>\n          <li class=\"li\">G1 is self tuning.</li>\n          <li class=\"li\">You only need to set MAX_HEAP_SIZE.</li>\n        </ul>However, G1 incurs some latency due to profiling.</div><div class=\"p\">CMS is recommended only in the following circumstances:<ul class=\"ul\"><li class=\"li\">You have the time and expertise to manually tune and test garbage collection. <p class=\"p\">Be\n              aware that allocating more memory to the heap can result in diminishing performance as\n              the garbage collection facility increases the amount of database metadata in heap\n              memory.</p></li>\n          <li class=\"li\">Heap sizes are smaller than 16 GB.</li>\n          <li class=\"li\">The workload is fixed, that is, the cluster performs the same processes all the\n            time.</li>\n          <li class=\"li\">The environment requires the lowest latency possible. </li>\n        </ul><p>Note: For help configuring CMS, contact the <a class=\"xref\" href=\"https://www.datastax.com/products/services\" target=\"_blank\">DataStax Services team</a>.</p></div></section><section class=\"section\" id=\"opsTuneJVM__setting-cms-gc\"><h2 class=\"title sectiontitle\">Setting CMS as the Java garbage collector</h2><ol class=\"ol\"><li class=\"li\">Open .</li>\n        <li class=\"li\">Comment out all lines in the <code data-swiftype-name=\"codeph\" data-swiftype-type=\"text\" class=\"ph codeph\">### GI Settings</code> section.</li>\n        <li class=\"li\">Uncomment all the <code data-swiftype-name=\"codeph\" data-swiftype-type=\"text\" class=\"ph codeph\">### CMS Settings</code> section</li>\n      </ol></section><section class=\"section\" id=\"opsTuneJVM__tuning-the-java-heap\"><h2 class=\"title sectiontitle\">Determining the heap size</h2><div class=\"p\">The database automatically calculates the maximum heap size (MAX_HEAP_SIZE) based on this\n        formula:<pre>max(min(1/2 ram, 1024 megabytes), min(1/4 ram, 32765 megabytes))</pre></div><div class=\"p\">For\n        production use, use these guidelines to adjust heap size for your environment:<ul class=\"ul\"><li class=\"li\">Heap size is usually between ¼ and ½ of system memory.</li>\n          <li class=\"li\">Do not devote all memory to heap because it is also used for offheap cache and file\n            system cache.</li>\n          <li class=\"li\">Always enable GC logging when adjusting GC.</li>\n          <li class=\"li\">Adjust settings gradually and test each incremental change.</li>\n          <li class=\"li\">Enable parallel processing for GC, particularly when using DSE Search.</li>\n          <li class=\"li\">The GCInspector class logs information about any garbage collection\n            that takes longer than 200 ms. Garbage collections that occur frequently and take a\n            moderate length of time (seconds) to complete, indicate excessive garbage collection\n            pressure on the JVM. In addition to adjusting the garbage collection options, other\n            remedies include adding nodes, and lowering cache sizes.</li>\n          <li class=\"li\">For a node using G1, DataStax recommends a MAX_HEAP_SIZE as large as possible, up to\n            64 GB.</li>\n        </ul><p>Note:  For more tuning tips, see <a class=\"xref\" href=\"http://blog.ragozin.info/2012/03/secret-hotspot-option-improving-gc.html\" target=\"_blank\">Secret HotSpot option improving GC pauses on large\n            heaps</a>.</p></div><p class=\"p\"><strong class=\"ph b\">MAX_HEAP_SIZE</strong></p><div class=\"p\">The recommended maximum heap size\n        depends on which GC is used:<table class=\"table frame-all\"><caption>G1 for newer computers (8+ cores) with up to 256 GB RAM\n                16 GB to 32765 MB.See <a class=\"xref\" href=\"http://java-performance.info/over-32g-heap-java\" target=\"_blank\">Java performance tuning</a>.\n              CMS for newer computers (8+ cores) with up to 256 GB RAM\n                No more 16 GB.\n              Older computers\n                Typically 8 GB.\n              </caption></table></div><div class=\"p\">The easiest way to determine the optimum heap size for your environment is:<ol class=\"ol\"><li class=\"li\">Set the maximum heap size in the  file to\n            a high arbitrary value on a single node. For example, when using\n              G1:<pre>-Xms48G\n-Xmx48G</pre><p class=\"p\">Set the min (-Xms) and max (-Xmx) heap\n              sizes to the same value to avoid stop-the-world GC pauses during resize, and to lock\n              the heap in memory on startup which prevents any of it from being swapped\n            out.</p></li>\n          <li class=\"li\">Enable GC logging.</li>\n          <li class=\"li\">Check the logs to view the heap used by that node and use that value for setting the\n            heap size in the cluster:</li>\n        </ol><p>Note: This method decreases performance for the test node, but generally does not\n          significantly reduce cluster performance.</p></div><p class=\"p\">If you don't see improved performance, contact the <a class=\"xref\" href=\"https://www.datastax.com/products/services\" target=\"_blank\">DataStax\n          Services team</a> for additional help.</p><p class=\"p\"><strong class=\"ph b\">HEAP_NEWSIZE</strong></p><div class=\"p\">For CMS, you may also need to adjust HEAP_NEWSIZE. This setting determines the amount of\n        heap memory allocated to newer objects or<em class=\"ph i\">young generation</em>. The database calculates\n        the default value for this property in megabytes (MB) as the lesser of:<ul class=\"ul\"><li class=\"li\">100 times the number of cores</li>\n          <li class=\"li\">¼ of MAX_HEAP_SIZE</li>\n        </ul></div><div class=\"p\">As a starting point, set HEAP_NEWSIZE to 100 MB per physical CPU core. For\n        example, for a modern 8-core+ machine:<pre>-Xmn800M</pre></div><p class=\"p\">A larger\n        HEAP_NEWSIZE leads to longer GC pause times. For a smaller HEAP_NEWSIZE, GC pauses are\n        shorter but usually more expensive.</p>See<a class=\"xref\" href=\"https://docs.datastax.com/en/dse-planning/doc/planning/planningHardware.html#planningHardware__memory\" target=\"_blank\">Recommended minimum memory for dedicated hardware and virtual\n        environments</a>.</section><section class=\"section\" id=\"opsTuneJVM__how-cassandra-uses-memory\"><h2 class=\"title sectiontitle\">How DataStax Enterprise uses memory</h2><div class=\"p\">The database performs the following major operations within JVM heap:<ul class=\"ul\" id=\"opsTuneJVM__ul_enc_3dg_vw\"><li class=\"li\">To perform reads, the database maintains the following components in heap memory:<ul class=\"ul\"><li class=\"li\">Bloom filters</li>\n              <li class=\"li\">Partition summary</li>\n              <li class=\"li\">Partition key cache</li>\n              <li class=\"li\">Compression offsets</li>\n              <li class=\"li\">SSTable index summary</li>\n            </ul><p class=\"p\">This metadata resides in memory and is proportional to total data. Some of the\n              components <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbInternals/dbIntAboutReads.html\" target=\"_blank\">grow proportionally to the size of total\n                memory</a>.</p></li>\n          <li class=\"li\">The database gathers replicas for a read or for anti-entropy repair and compares the\n            replicas in heap memory.</li>\n          <li class=\"li\">Data written to the database is first stored in memtables in heap memory. Memtables\n            are flushed to SSTables on disk.</li>\n        </ul></div><div class=\"p\">To improve performance, the database also uses off-heap memory as follows:<ul class=\"ul\" id=\"opsTuneJVM__ul_uxp_c2g_vw\"><li class=\"li\">Page cache. The database uses additional memory as page cache when reading files on\n            disk.</li>\n          <li class=\"li\">The Bloom filter and compression offset maps reside off-heap.</li>\n          <li class=\"li\">The database can store cached rows in native memory, outside the Java heap. This\n            reduces JVM heap requirements, which helps keep the heap size in the sweet spot for JVM\n            garbage collection performance. </li>\n        </ul></div></section><section class=\"section\" id=\"opsTuneJVM__adjusting_params_for_other_Cassandra_services\"><h2 class=\"title sectiontitle\">Adjusting JVM parameters for other DataStax Enterprise services</h2><ul class=\"ul\"><li class=\"li\"><strong class=\"ph b\">DSE Search</strong>: Some users have reported that increasing the stack size improves\n          performance under Tomcat. <div class=\"p\">To increase the stack size, uncomment and modify the default\n            setting in thefile.<pre># Per-thread stack size.\nJVM_OPTS=\"$JVM_OPTS -Xss256k\"</pre>Also,\n            decreasing the memtable space to make room for search caches can improve performance.\n            Modify the memtable space by changing the<a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/config/configCassandra_yaml.html#configCassandra_yaml__memtable_heap_space_in_mb\">memtable_heap_space_in_mb</a>and<a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/config/configCassandra_yaml.html#configCassandra_yaml__memtable_offheap_space_in_mb\">memtable_offheap_space_in_mb</a>properties in thefile.</div></li>\n        <li class=\"li\">\n          <p class=\"p\"><strong class=\"ph b\">MapReduce</strong>: Because MapReduce runs outside the JVM, changes to the JVM do not\n            affect Analytics/Hadoop operations directly.</p>\n        </li>\n      </ul></section><section class=\"section\" id=\"opsTuneJVM__jmx-options\"><h2 class=\"title sectiontitle\">Other JMX options</h2><p class=\"p\">DataStax Enterprise exposes other statistics and management operations via Java Management\n        Extensions (JMX). <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/opsMonitoring.html#opsMonitoringJconsole\">JConsole</a>, the <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/opsMonitoring.html#opsMonitorNodetool\" title=\"Get statistics using nodetool commands.\">nodetool</a> are JMX-compliant management tools.</p><p class=\"p\">Configure the database for JMX management by editing these properties in\n        .</p><ul class=\"ul\"><li class=\"li\">com.sun.management.jmxremote.port: sets the port on which the\n          database listens from JMX connections. </li>\n        <li class=\"li\">com.sun.management.jmxremote.ssl: enables or disables SSL for JMX. </li>\n        <li class=\"li\">com.sun.management.jmxremote.authenticate: enables or disables remote\n          authentication for JMX. </li>\n        <li class=\"li\">-Djava.rmi.server.hostname: sets the interface hostname or IP that\n          JMX should use to connect. Uncomment and set if you are having trouble connecting. </li>\n      </ul><p>Note: By default, you can interact with DataStax Enterprise using JMX on port 7199 without\n        authentication.</p></section><section class=\"section\"></section>",
        "created_at": "2018-07-24T19:54:40+0000",
        "updated_at": "2018-07-24T19:54:40+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 5,
        "domain_name": "docs.datastax.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11157"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1151,
            "label": "resources",
            "slug": "resources"
          },
          {
            "id": 1173,
            "label": "troubleshooting and tuning",
            "slug": "troubleshooting-and-tuning"
          }
        ],
        "is_public": true,
        "id": 11156,
        "uid": "5b5783fcbf4fe3.58982921",
        "title": "Secret HotSpot option improving GC pauses on large heaps",
        "url": "http://blog.ragozin.info/2012/03/secret-hotspot-option-improving-gc.html",
        "content": "<div><p>\n<a href=\"http://www.blogger.com/2011/07/openjdk-patch-cutting-down-gc-pause.html\">my\nPatch mentioned in this post</a> (<i><a href=\"http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7068625\">RFE-7068625</a>)</i>\nfor JVM garbage collector was accepted into HotSpot JDK code base and available starting from 7u40 version of HotSport JVM from Oracle.</p></div>\n<div class=\"MsoNormal\">\n<br />\nThis was a reason for me to redo some of my GC benchmarking experiments. I have already mentioned ParGCCardsPerStrideChunk\nin article related to patch. This time, I decided study effect of this option more closely.</div>\n<div class=\"MsoNormal\">\n<br />\nParallel copy collector (ParNew), responsible for young\ncollection in CMS, use ParGCCardsPerStrideChunk\n value to control granularity of tasks\ndistributed between worker threads. Old space is broken into strides of equal\nsize and each worker responsible for processing (find dirty pages, find old to\nyoung references, copy young objects etc) a subset of strides. Time to process each\nstride may vary greatly, so workers may steal work from each other. For that\nreason number of strides should be greater than number of workers. </div>\n<div class=\"MsoNormal\">\n<br />\nBy default ParGCCardsPerStrideChunk\n=256 (card is 512 bytes, so it would be 128KiB of heap space per stride) which means that 28GiB heap\nwould be broken into 224 thousands of strides. Provided that number of parallel\nGC threads is usually 4 orders of magnitude less, this is probably too many.</div>\n<h4>\nSynthetic benchmark</h4>\n<div class=\"MsoNormal\">\nFirst, I have run GC benchmark from <a href=\"http://www.blogger.com/2011/07/openjdk-patch-cutting-down-gc-pause.html\">previous\narticle</a> using 2k, 4k and 8K for this option. HotSpot JVM 7u3 was used in\nexperiment. </div>\n<div class=\"MsoNormal\">\n<div class=\"separator\">\n<a href=\"http://1.bp.blogspot.com/-JqGI0UUFw_8/T3NivPutmSI/AAAAAAAAKTs/X5H4eqqOdOs/s1600/blog-25.png\"><img border=\"0\" height=\"242\" src=\"http://1.bp.blogspot.com/-JqGI0UUFw_8/T3NivPutmSI/AAAAAAAAKTs/X5H4eqqOdOs/s400/blog-25.png\" width=\"400\" alt=\"image\" /></a></div>\n<br /></div>\n<div class=\"MsoNormal\">\nIt seems that default value (256 cards per strides) is too\nsmall even for moderate size heaps. I decided to continue my experiments with\nstride size 4k as it shows most consistent improvement across whole range of\nheap sizes.</div>\n<div class=\"MsoNormal\">\n<br />\nBenchmark above is synthetic and very simple. Next step is\nto choose more realistic use case. I usual, my choice is to use <a href=\"http://www.oracle.com/technetwork/middleware/coherence/overview/index.html\">Oracle\nCoherence</a> storage node as my guinea pig.</div>\n<h4>\nBenchmarking Coherence storage node</h4>\n<div class=\"MsoNormal\">\nIn this experiment I’m filling cache node with objects (object\n70% of old space filled with live objects), then put it under mixed read/write\nload and measuring young GC pauses of JVM. Experiment was conducted with two\ndifferent heap sizes (28 GiB and 14 GiB), young space for both cases was\nlimited by 128MiB, compressed pointers were enabled.</div>\n<h5>\nCoherence node with 28GiB of heap</h5>\n<table border=\"1\" cellpadding=\"0\" class=\"MsoNormalTable\" style=\"border-spacing: 0px;\"><tbody><tr><td valign=\"top\" style=\"width: 193px;\"><div class=\"MsoNormal\">\n<b>JVM</b></div>\n</td>\n  <td valign=\"top\" style=\"width: 90px;\"><div class=\"MsoNormal\">\n<b>Avg. pause</b></div>\n</td>\n  <td valign=\"top\" style=\"width: 108px;\"><div class=\"MsoNormal\">\n<b>Improvement</b></div>\n</td>\n </tr><tr><td valign=\"top\" style=\"width: 193px;\"><div class=\"MsoNormal\">\n7u3</div>\n</td>\n  <td valign=\"bottom\" style=\"width: 90px;\"><div class=\"MsoNormal\" style=\"text-align: right;\">\n0.0697</div>\n</td>\n  <td valign=\"bottom\" style=\"width: 108px;\"><div class=\"MsoNormal\" style=\"text-align: right;\">\n0</div>\n</td>\n </tr><tr><td valign=\"top\" style=\"width: 193px;\"><div class=\"MsoNormal\">\n7u3, stride=4k</div>\n</td>\n  <td valign=\"bottom\" style=\"width: 90px;\"><div class=\"MsoNormal\" style=\"text-align: right;\">\n0.045</div>\n</td>\n  <td valign=\"bottom\" style=\"width: 108px;\"><div class=\"MsoNormal\" style=\"text-align: right;\">\n35.4%</div>\n</td>\n </tr><tr><td valign=\"top\" style=\"width: 193px;\"><div class=\"MsoNormal\">\n<a href=\"http://blog.ragozin.info/2011/07/openjdk-patch-cutting-down-gc-pause.html\">Patched OpenJDK 7</a></div>\n</td>\n  <td valign=\"bottom\" style=\"width: 90px;\"><div class=\"MsoNormal\" style=\"text-align: right;\">\n0.0546</div>\n</td>\n  <td valign=\"bottom\" style=\"width: 108px;\"><div class=\"MsoNormal\" style=\"text-align: right;\">\n21.7%</div>\n</td>\n </tr><tr><td valign=\"top\" style=\"width: 193px;\"><div class=\"MsoNormal\">\n<a href=\"http://blog.ragozin.info/2011/07/openjdk-patch-cutting-down-gc-pause.html\">Patched OpenJDK 7</a>,\n  stride=4k</div>\n</td>\n  <td valign=\"bottom\" style=\"width: 90px;\"><div class=\"MsoNormal\" style=\"text-align: right;\">\n0.0284</div>\n</td>\n  <td valign=\"bottom\" style=\"width: 108px;\"><div class=\"MsoNormal\" style=\"text-align: right;\">\n59.3%</div>\n</td>\n </tr></tbody></table><h5>\nCoherence node with 14GiB of heap</h5>\n<table border=\"1\" cellpadding=\"0\" class=\"MsoNormalTable\" style=\"border-spacing: 0px;\"><tbody><tr><td valign=\"top\" style=\"width: 193px;\"><div class=\"MsoNormal\">\n<b>JVM</b></div>\n</td>\n  <td valign=\"top\" style=\"width: 90px;\"><div class=\"MsoNormal\">\n<b>Avg. pause</b></div>\n</td>\n  <td valign=\"top\" style=\"width: 108px;\"><div class=\"MsoNormal\">\n<b>Improvement</b></div>\n</td>\n </tr><tr><td valign=\"top\" style=\"width: 193px;\"><div class=\"MsoNormal\">\n7u3</div>\n</td>\n  <td valign=\"bottom\" style=\"width: 90px;\"><div class=\"MsoNormal\" style=\"text-align: right;\">\n0.05</div>\n</td>\n  <td valign=\"bottom\" style=\"width: 108px;\"><div class=\"MsoNormal\" style=\"text-align: right;\">\n0</div>\n</td>\n </tr><tr><td valign=\"top\" style=\"width: 193px;\"><div class=\"MsoNormal\">\n7u3, stride=4k</div>\n</td>\n  <td valign=\"bottom\" style=\"width: 90px;\"><div class=\"MsoNormal\" style=\"text-align: right;\">\n0.0322</div>\n</td>\n  <td valign=\"bottom\" style=\"width: 108px;\"><div class=\"MsoNormal\" style=\"text-align: right;\">\n35.6%</div>\n</td>\n </tr></tbody></table><div class=\"MsoNormal\">\nThis test is close enough to real live Coherence work\nprofile and such improvement of GC pause time has practical importance. I have\nalso included JVM built from OpenJDK trunk with enabled <i><a href=\"http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7068625\">RFE-7068625</a>\n</i>patch for 28 GiB test, as expected effect of patch is cumulative with\nstride size tuning.</div>\n<div class=\"MsoNormal\">\n<h4>\nStock JVMs from Oracle are supported</h4>\nGood news is that you do not have to wait for next version\nof JVM, ParGCCardsPerStrideChunk\noption is available in all Java 7 HotSpot JVMs and most recent Java 6 JVMs. But\nthis option is classified as diagnostic so you should enable diagnostic options\nto use it.</div>\n<div>\n<div class=\"MsoNormal\">\n-XX:+UnlockDiagnosticVMOptions </div>\n<div class=\"MsoNormal\">\n-XX:ParGCCardsPerStrideChunk=4096</div>\n</div>\n<div>\n<br /></div>\n<div></div>",
        "created_at": "2018-07-24T19:54:36+0000",
        "updated_at": "2018-07-24T19:54:36+0000",
        "published_at": "2012-03-28T00:00:00+0000",
        "published_by": [
          "Alexey Ragozin"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 2,
        "domain_name": "blog.ragozin.info",
        "preview_picture": "http://1.bp.blogspot.com/-JqGI0UUFw_8/T3NivPutmSI/AAAAAAAAKTs/X5H4eqqOdOs/w1200-h630-p-k-no-nu/blog-25.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11156"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1128,
            "label": "tutorials",
            "slug": "tutorials"
          },
          {
            "id": 1151,
            "label": "resources",
            "slug": "resources"
          }
        ],
        "is_public": true,
        "id": 11153,
        "uid": "5b5783f2584c52.03160098",
        "title": "Basic Rules of Cassandra Data Modeling",
        "url": "https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling",
        "content": "<div class=\"DXDvBlgCtaAD_wrp1\"><p>Learn more about Apache Cassandra and data modeling</p></div><p>Picking the right <a href=\"http://planetcassandra.org/getting-started-with-time-series-data-modeling/\">data model</a> is the hardest part of using Cassandra.  If you have a relational background, CQL will look familiar, but the way you use it can be very different.  The goal of this post is to explain the basic rules you should keep in mind when designing your schema for Cassandra.  If you follow these rules, you'll get pretty good performance out of the box.  Better yet, your performance should scale linearly as you add nodes to the cluster.</p><h2>Non-Goals</h2><p>Developers coming from a relational background usually carry over rules about relational modeling and try to apply them to Cassandra.  To avoid wasting time on rules that don't really matter with Cassandra, I want to point out some <em>non</em>-goals:</p><h3>Minimize the Number of Writes</h3><p>Writes in Cassandra aren't free, but they're awfully cheap.  Cassandra is optimized for high write throughput, and almost all writes are equally efficient <a href=\"#footnote\"><sup>[1]</sup></a>.  If you can perform extra writes to improve the efficiency of your read queries, it's almost always a good tradeoff.  Reads tend to be more expensive and are much more difficult to tune.</p><h3>Minimize Data Duplication</h3><p>Denormalization and duplication of data is a fact of life with Cassandra.  Don't be afraid of it.  Disk space is generally the cheapest resource (compared to CPU, memory, disk IOPs, or network), and Cassandra is architected around that fact.  In order to get the most efficient reads, you often need to duplicate data.</p><p>Besides, Cassandra doesn't have <code>JOIN</code>s, and you don't really want to use those in a distributed fashion.</p><h2>Basic Goals</h2><p>These are the two high-level goals for your data model:</p><ol><li>Spread data evenly around the cluster</li>\n<li>Minimize the number of partitions read</li>\n</ol><p>There are other, lesser goals to keep in mind, but these are the most important. For the most part, I will focus on the basics of achieving these two goals.  There are other fancy tricks you can use, but you should know how to evaluate them, first.</p><h3>Rule 1: Spread Data Evenly Around the Cluster</h3><p>You want every node in the cluster to have roughly the same amount of data. Cassandra makes this easy, but it's not a given.  Rows are spread around the cluster based on a hash of the <a href=\"https://www.datastax.com/documentation/cassandra/2.1/cassandra/architecture/architectureDataDistributeHashing_c.html\"><em>partition key</em></a>, which is the first element of the <a href=\"https://www.datastax.com/documentation/cql/3.0/cql/ddl/ddl_compound_keys_c.html\"><code>PRIMARY KEY</code></a>.  So, the key to spreading data evenly is this: <b>pick a good primary key</b>.  I'll explain how to do this in a bit.</p><h3>Rule 2: Minimize the Number of Partitions Read</h3><p>Partitions are groups of rows that share the same partition key.  When you issue a read query, you want to read rows from as few partitions as possible.</p><p>Why is this important?  Each partition may reside on a different node. The coordinator will generally need to issue separate commands to separate nodes for each partition you request.  This adds a lot of overhead and increases the variation in latency.  Furthermore, even on a single node, it's more expensive to read from multiple partitions than from a single one due to the way rows are stored.</p><h3>Conflicting Rules?</h3><p>If it's good to minimize the number of partitions that you read from, why not put everything in a single big partition?  You would end up violating Rule #1, which is to spread data evenly around the cluster.</p><p>The point is, these two goals often conflict, so you'll need to try to balance them.</p><h2>Model Around Your Queries</h2><p>The way to minimize partition reads is to model your data to fit your queries. Don't model around relations.  Don't model around objects.  Model around your queries.  Here's how you do that:</p><h3>Step 1: Determine What Queries to Support</h3><p>Try to determine <em>exactly</em> what queries you need to support.  This can include a lot of considerations that you may not think of at first.  For example, you may need to think about:</p><ul><li>Grouping by an attribute</li>\n<li>Ordering by an attribute</li>\n<li>Filtering based on some set of conditions</li>\n<li>Enforcing uniqueness in the result set</li>\n<li>etc ...</li>\n</ul><p>Changes to just one of these query requirements will frequently warrant a data model change for maximum efficiency.</p><h3>Step 2: Try to create a table where you can satisfy your query by reading (roughly) one partition</h3><p>In practice, this generally means you will use roughly one table per query pattern. If you need to support multiple query patterns, you usually need more than one table.</p><p>To put this another way, each table should pre-build the \"answer\" to a high-level query that you need to support.  If you need different types of answers, you usually need different tables.  This is how you optimize for reads.</p><p>Remember, data duplication is okay.  Many of your tables may repeat the same data.</p><h2>Applying the Rules: Examples</h2><p>To show some examples of a good throught process, I will walk you through the design of a data model for some simple problems.</p><h3>Example 1: User Lookup</h3><p>The high-level requirement is \"we have users and want to look them up\".  Let's go through the steps:</p><p><b>Step 1</b>: <em>Determine what specific queries to support</em><br />Let's say we want to either be able to look up a user by their username or their email.  With either lookup method, we should get the full set of user details.</p><p><b>Step 2</b>: <em>Try to create a table where you can satisfy your query by reading (roughly) one partition</em><br />Since we want to get the full details for the user with either lookup method, it's best to use two tables:</p><pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nCREATE TABLE users_by_username (&#13;\n    username text PRIMARY KEY,&#13;\n    email text,&#13;\n    age int&#13;\n)&#13;\n&#13;\nCREATE TABLE users_by_email (&#13;\n    email text PRIMARY KEY,&#13;\n    username text,&#13;\n    age int&#13;\n)&#13;\n</pre><p>Now, let's check the two rules for this model:</p><p><b>Spreads data evenly?</b> Each user gets their own partition, so yes.<br /><b>Minimal partitions read?</b> We only have to read one partition, so yes.</p><p>Now, let's suppose we tried to optimize for the <em>non</em>-goals, and came up with this data model instead:</p><pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nCREATE TABLE users (&#13;\n    id uuid PRIMARY KEY,&#13;\n    username text,&#13;\n    email text,&#13;\n    age int&#13;\n)&#13;\n&#13;\nCREATE TABLE users_by_username (&#13;\n    username text PRIMARY KEY,&#13;\n    id uuid&#13;\n)&#13;\n&#13;\nCREATE TABLE users_by_email (&#13;\n    email text PRIMARY KEY,&#13;\n    id uuid&#13;\n)&#13;\n</pre><p>This data model also spreads data evenly, but there's a downside: we now have to read two partitions, one from <code>users_by_username</code> (or <code>users_by_email</code>) and then one from <code>users</code>.  So reads are roughly twice as expensive.</p><h3>Example 2: User Groups</h3><p>Now the high-level requirement has changed.  Users are in groups, and we want to get all users in a group.</p><p><b>Step 1</b>: <em>Determine what specific queries to support</em><br />We want to get the full user info for every user in a particular group.  Order of users does not matter.</p><p><b>Step 2</b>: <em>Try to create a table where you can satisfy your query by reading (roughly) one partition</em><br />How do we fit a group into a partition?  We can use a compound <code>PRIMARY KEY</code> for this:</p><pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nCREATE TABLE groups (&#13;\n    groupname text,&#13;\n    username text,&#13;\n    email text,&#13;\n    age int,&#13;\n    PRIMARY KEY (groupname, username)&#13;\n)&#13;\n</pre><p>Note that the <code>PRIMARY KEY</code> has two components: <code>groupname</code>, which is the partitioning key, and <code>username</code>, which is called the clustering key.  This will give us one partition per <code>groupname</code>.  Within a particular partition (group), rows will be ordered by <code>username</code>.  Fetching a group is as simple as doing the following:</p><pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nSELECT * FROM groups WHERE groupname = ?&#13;\n</pre><p>This satisfies the goal of minimizing the number of partitions that are read, because we only need to read one partition.  However, it doesn't do so well with the first goal of evenly spreading data around the cluster.  If we have thousands or millions of small groups with hundreds of users each, we'll get a pretty even spread.  But if there's one group with millions of users in it, the entire burden will be shouldered by one node (or one set of replicas).</p><p>If we want to spread the load more evenly, there are a few strategies we can use. The basic technique is to add another column to the PRIMARY KEY to form a compound partition key.  Here's one example:</p><pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nCREATE TABLE groups (&#13;\n    groupname text,&#13;\n    username text,&#13;\n    email text,&#13;\n    age int,&#13;\n    hash_prefix int,&#13;\n    PRIMARY KEY ((groupname, hash_prefix), username)&#13;\n)&#13;\n</pre><p>The new column, <code>hash_prefix</code>, holds a prefix of a hash of the username.  For example, it could be the first byte of the hash modulo four. Together with <code>groupname</code>, these two columns form the compound partition key.  Instead of a group residing on one partition, it's now spread across four partitions.  Our data is more evenly spread out, but we now have to read four times as many partitions.  This is an example of the two goals conflicting.  You need to find a good balance for your particular use case.  If you do a lot of reads and groups don't get too large, maybe changing the modulo value from four to two would be a good choice.  On the other hand, if you do very few reads, but any given group can grow very large, changing from four to ten would be a better choice.</p><p>There are other ways to split up a partition, which I will cover in the next example.</p><p>Before we move on, let me point out something else about this data model: we're duplicating user info potentially many times, once for each group. You might be tempted to try a data model like this to reduce duplication:</p><pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nCREATE TABLE users (&#13;\n    id uuid PRIMARY KEY,&#13;\n    username text,&#13;\n    email text,&#13;\n    age int&#13;\n)&#13;\n&#13;\nCREATE TABLE groups (&#13;\n    groupname text,&#13;\n    user_id uuid,&#13;\n    PRIMARY KEY (groupname, user_id)&#13;\n)&#13;\n</pre><p>Obviously, this minimizes duplication.  But how many partitions do we need to read? If a group has 1000 users, we need to read 1001 partitions.  This is probably 100x more expensive to read than our first data model.  If reads need to be efficient at all, this isn't a good model.  On the other hand, if reads are extremely infrequent, but updates to user info (say, the username) are extremely common, this data model might actually make sense.  Make sure to take your read/update ratio into account when designing your schema.</p><h3>Example 3: User Groups by Join Date</h3><p>Suppose we continue with the previous example of groups, but need to add support for getting the X newest users in a group.</p><p>We can use a similar table to the last one:</p><pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nCREATE TABLE group_join_dates (&#13;\n    groupname text,&#13;\n    joined timeuuid,&#13;\n    username text,&#13;\n    email text,&#13;\n    age int,&#13;\n    PRIMARY KEY (groupname, joined)&#13;\n)&#13;\n</pre><p>Here we're using a <code>timeuuid</code> (which is like a timestamp, but avoids collisions) as the clustering column.  Within a group (partition), rows will be ordered by the time the user joined the group.  This allows us to get the newest users in a group like so:</p><pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nSELECT * FROM group_join_dates&#13;\n    WHERE groupname = ?&#13;\n    ORDER BY joined DESC&#13;\n    LIMIT ?&#13;\n</pre><p>This is reasonably efficient, as we're reading a slice of rows from a single partition.  However, instead of always using <code>ORDER BY joined DESC</code>, which makes the query less efficient, we can simply reverse the clustering order:</p><pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nCREATE TABLE group_join_dates (&#13;\n    groupname text,&#13;\n    joined timeuuid,&#13;\n    username text,&#13;\n    email text,&#13;\n    age int,&#13;\n    PRIMARY KEY (groupname, joined)&#13;\n) WITH CLUSTERING ORDER BY (joined DESC)&#13;\n</pre><p>Now we can use the slightly more efficient query:</p><pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nSELECT * FROM group_join_dates&#13;\n    WHERE groupname = ?&#13;\n    LIMIT ?&#13;\n</pre><p>As with the previous example, we could have problems with data being spread evenly around the cluster if any groups get too large.  In that example, we split partitions somewhat randomly, but in this case, we can utilize our knowledge about the query patterns to split partitions a different way: by a time range.</p><p>For example, we might split partitions by date:</p><pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nCREATE TABLE group_join_dates (&#13;\n    groupname text,&#13;\n    joined timeuuid,&#13;\n    join_date text,&#13;\n    username text,&#13;\n    email text,&#13;\n    age int,&#13;\n    PRIMARY KEY ((groupname, join_date), joined)&#13;\n) WITH CLUSTERING ORDER BY (joined DESC)&#13;\n</pre><p>We're using a compound partition key again, but this time we're using the join date.  Each day, a new partition will start.  When querying the X newest users, we will first query today's partition, then yesterday's, and so on, until we have X users.  We may have to read multiple partitions before the limit is met.</p><p>To minimize the number of partitions you need to query, try to select a time range for splitting partitions that will typically let you query only one or two partitions.  For example, if we usually need the ten newest users, and groups usually acquire three users per day, we should split by four-day ranges instead of a single day <a href=\"#footnote\"><sup>[2]</sup></a>.</p><h2>Summary</h2><p>The basic rules of data modeling covered here apply to all (currently) existing versions of Cassandra, and are very likely to apply to all future versions.  Other lesser data modeling problems, such as <a href=\"https://medium.com/@foundev/domain-modeling-around-deletes-1cc9b6da0d24\">dealing with tombstones</a>, may also need to be considered, but these problems are more likely to change (or be mitigated) by future versions of Cassandra.</p><p>Besides the basic strategies covered here, some of Cassandra's fancier features, like <a href=\"https://www.datastax.com/dev/blog/cql3_collections\">collections</a>, <a href=\"https://www.datastax.com/dev/blog/cql-in-2-1\">user-defined types</a>, and <a href=\"https://www.datastax.com/documentation/cql/3.1/cql/cql_reference/refStaticCol.html\">static columns</a>, can also be used to reduce the number of partitions that you need to read to satisfy a query.  Don't forget to consider these options when designing your schema.</p><p>Hopefully I've given you some useful fundamental tools for evaluating different schema designs.  If you want to go further, I suggest taking <a href=\"https://academy.datastax.com/courses/ds220-data-modeling?dxt=blogposting\">Datastax's free, self-paced online data modeling course (DS220)</a>.  Good luck!</p><p>[1]: Notable exceptions: <a href=\"https://www.datastax.com/dev/blog/whats-new-in-cassandra-2-1-a-better-implementation-of-counters\">counters</a>, <a href=\"https://www.datastax.com/dev/blog/lightweight-transactions-in-cassandra-2-0\">lightweight transactions</a>, and <a href=\"http://cassandra.apache.org/doc/cql3/CQL.html#collections\">inserting into the middle of a list collection</a>.</p><p>[2]: I suggest using a timestamp truncated by some number of seconds.  For example, to handle four-day ranges, you might use something like this:</p><pre class=\"brush: python; gutter: true; title: ; notranslate\" title=\"\">&#13;\nnow = time()&#13;\nfour_days = 4 * 24 * 60 * 60&#13;\nshard_id = now - (now % four_days)&#13;\n</pre><hr /><p><a href=\"https://www.datastax.com/\">DataStax</a> has many ways for you to advance in your career and knowledge. \n</p><p>You can take <a href=\"https://academy.datastax.com/user/register?destination=home&amp;utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_register\" target=\"_self\" title=\"academy.datastax.com\">free classes</a>, <a href=\"https://academy.datastax.com/certifications?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_certifications\" target=\"_self\" title=\"academy.datastax.com/certifications\">get certified</a>, or read <a href=\"https://www.datastax.com/dbas-guide-to-nosql\" target=\"_self\" title=\"dbas-guide-to-nosql\">one of our many white papers</a>.\n</p><p><a href=\"https://academy.datastax.com/user/register?destination=home&amp;utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_register\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"academy.datastax.com\">register for classes</a>\n</p><p><a href=\"https://academy.datastax.com/certifications?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_certifications\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"academy.datastax.com/certifications\">get certified</a>\n</p><p><a href=\"http://www.datastax.com/dbas-guide-to-nosql?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_dbasguidetonosql\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"dbas-guide-to-nosql\">DBA's Guide to NoSQL</a>\n</p><br class=\"clear\" /><div id=\"mto_newsletter_121316_Css\"><p>Subscribe for newsletter:</p><br /></div>",
        "created_at": "2018-07-24T19:54:26+0000",
        "updated_at": "2018-07-24T19:54:26+0000",
        "published_at": "2015-02-02T16:50:05+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 12,
        "domain_name": "www.datastax.com",
        "preview_picture": "https://www.datastax.com/wp-content/themes/datastax-2014-08/images/blog/DataModelingPostBannerAd_cover_brght.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11153"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1128,
            "label": "tutorials",
            "slug": "tutorials"
          },
          {
            "id": 1151,
            "label": "resources",
            "slug": "resources"
          }
        ],
        "is_public": true,
        "id": 11152,
        "uid": "5b5783ec170b22.98669396",
        "title": "Installing the Cassandra / Spark OSS Stack",
        "url": "http://tobert.github.io/post/2014-07-15-installing-cassandra-spark-stack.html",
        "content": "<h2>Init</h2><p>As mentioned in my <a href=\"http://tobert.github.io/post/2014-07-14-portacluster-system-imaging.html\">portacluster system imaging</a> post,\nI am performing this install on 1 admin node (node0) and 6 worker nodes (node[1-6]) running 64-bit Arch Linux.\nMost of what I describe in this post should work on other Linux variants with minor adjustments.</p><h2>Overview</h2><p>When assembling an analytics stack, there are usually myriad choices to make. For this build, I decided to\nbuild the smallest stack possible that lets me run Spark queries on Cassandra data. As configured it is\nnot highly available since the Spark master is standalone. (note: Datastax Enterprise Spark's master has\nHA based on Cassandra). It's a decent tradeoff for portacluster, since\nI can run the master on the admin node which doesn't get rebooted/reimaged constantly. I'm also going to\nskip HDFS or some kind of HDFS replacement for now. Options I plan to look at later are GlusterFS's HDFS\nadapter and <a href=\"http://pithos.io\">Pithos</a> as an S3 adapter. In the end, the stack is simply Cassandra and\nSpark with the <a href=\"https://github.com/datastax/spark-cassandra-connector\">spark-cassandra-connector</a>.</p><h2>Responsible Configuration</h2><p>For this post I've used my <a href=\"https://github.com/tobert/perl-ssh-tools\">perl-ssh-tools</a> suite. The intent\nis to show what needs to be done and one way to do it. For production deployments, I recommend using\nyour favorite configuration management tool.</p><p>perl-ssh-tools uses a configuration similar to dsh, which uses simple files with one\nhost per line. I use two lists below. Most commands run on the fleet of workers. Because\ncl-run.pl provides more than ssh commands, it's also used to run commands on node0 using\nits --incl flag e.g. <code>cl-run.pl --list all --incl node0</code>.</p><pre>cat .dsh/machines.workers\nnode1\nnode2\nnode3\nnode4\nnode5\nnode6\n</pre><p><code>machines.all</code> is the same with node0 added.</p><h2>Install Cassandra</h2><p>My first pass at this section involved setting up a package repo, but since I don't have time to package\nSpark properly right now, I'm going to use the tarball distros of Cassandra and Spark to keep it simple.\n<a href=\"https://github.com/joschi\">joschi</a> maintains a package on the <a href=\"https://aur.archlinux.org/packages/cassandra/\">AUR</a>\nbut I have chosen not to use it for this install.\nI'm also using the Arch packages of OpenJDK, which isn't supported by Datastax, but works fine for hacking.\nThe JDK is pre-installed on my Arch image, it's as simple as <code>sudo pacman -S extra/jdk7-openjdk</code>.</p><p>First, I downloaded the Cassandra tarball from <a href=\"http://cassandra.apache.org/download/\">apache.org</a> to\nnode0 in /srv/public/tgz. Then on the worker nodes, it gets downloaded and expanded in /opt.</p><pre>pkg=\"apache-cassandra-2.0.9-bin.tar.gz\"\nsudo curl -o /srv/public/tgz/$pkg \\\n  http://mirrors.gigenet.com/apache/cassandra/2.0.9/apache-cassandra-2.0.9-bin.tar.gz\ncl-run.pl --list workers -c \"curl http://node0/tgz/$pkg |sudo tar -C /opt -xzf -\"\ncl-run.pl --list workers -c \"sudo ln -s /opt/apache-cassandra-2.0.9 /opt/cassandra\"\n</pre><p>To make it easier to do upgrades without regenerating the configuration, I\nrelocate the conf dir to /etc/cassandra to match what packages do. This assumes there\nis no existing /etc/cassandra.</p><pre>cl-run.pl --list workers -c \"sudo mv /opt/cassandra/conf /etc/cassandra\"\ncl-run.pl --list workers -c \"sudo ln -s /etc/cassandra /opt/cassandra/conf\"\n</pre><p>I will start Cassandra with a systemd unit, so I push that out as well. This unit\nfile runs Cassandra out of the tarball as the cassandra user with the stdout/stderr going\nto the systemd journal (view with <code>journalctl -f</code>). I also included some\nulimit settings and bump the OOM score downwards to make it less likely that the kernel\nwill kill Cassandra when out of memory. Since we're going to be running two large JVM apps\non each worker node, this unit also enables cgroups so Cassandra can be given priority\nover Spark. Finally, since the target machines have 16GB of RAM, the heap needs to be\nset to 8GB (cassandra-env.sh calculates 3995M which is way too low).</p><pre>cat &gt; cassandra.service &lt;&lt;EOF\n[Unit]\nDescription=Cassandra Tarball\nAfter=network.target\n[Service]\nUser=cassandra\nGroup=cassandra\nRuntimeDirectory=cassandra\nPIDFile=/run/cassandra/cassandra.pid\nExecStart=/opt/cassandra/bin/cassandra -f -p /run/cassandra/cassandra.pid\nStandardOutput=journal\nStandardError=journal\nOOMScoreAdjust=-500\nLimitNOFILE=infinity\nLimitMEMLOCK=infinity\nLimitNPROC=infinity\nLimitAS=infinity\nEnvironment=MAX_HEAP_SIZE=8G HEAP_NEWSIZE=1G CASSANDRA_HEAPDUMP_DIR=/srv/cassandra/log\nCPUAccounting=true\nCPUShares=1000\n[Install]\nWantedBy=multi-user.target\nEOF\ncl-sendfile.pl --list workers -x -l cassandra.service -r /etc/systemd/system/multi-user.target.wants/cassandra.service\ncl-run.pl --list workers -c \"sudo systemctl daemon-reload\"\n</pre><p>Since all Cassandra data is being redirected to /srv/cassandra and it's going to run as the\ncassandra user, those need to be created.</p><pre>cat &gt; cassandra-user.sh &lt;&lt;EOF\nmkdir -p /srv/cassandra/{log,data,commitlogs,saved_caches}\n(grep -q '^cassandra:' /etc/group)  || groupadd -g 1234 cassandra\n(grep -q '^cassandra:' /etc/passwd) || useradd -u 1234 -c \"Apache Cassandra\" -g cassandra -s /bin/bash -d /srv/cassandra cassandra\nchown -R cassandra:cassandra /srv/cassandra\nEOF\ncl-run.pl --list workers -x -s cassandra-user.sh\n</pre><h2>Configure Cassandra</h2><p>Before starting Cassandra I want to make a few changes to the standard configurations. I'm not a big\nfan of LSB so I redirect all of the /var files to /srv/cassandra so they're all in one place. There's\nonly one SSD in the target systems so the commit log goes on the same drive.</p><p>I configured portacluster nodes to have a bridge in front of the default interface, making br0 the default interface.</p><pre>cat cassandra-config.sh\nip=$(ip addr show br0 |perl -ne 'if ($_ =~ /inet (\\d+\\.\\d+\\.\\d+\\.\\d+)/) { print $1 }')\nperl -i.bak -pe \"\n  s/^(cluster_name:).*/\\$1 'Portable Cluster'/;\n  s/^(listen|rpc)_address:.*/\\${1}_address: $ip/;\n  s|/var/lib|/srv|;\n  s/(\\s+-\\s+seeds:).*/\\$1 '192.168.10.11,192.168.10.12,192.168.10.13,192.168.10.14,192.168.10.15,192.168.10.16'/\n\" /opt/cassandra/conf/cassandra.yaml\n# EOF\ncl-run.pl --list workers -x -s cassandra-config.sh\n</pre><p>The default log4-server.propterties has log4j printing to stdout. This is not desirable in a background\nservice configuration, so I remove it. The logs are also now written to /srv/cassandra/log.</p><pre>cat &gt; log4j-server.properties &lt;&lt;EOF\nlog4j.rootLogger=INFO,R\nlog4j.appender.R=org.apache.log4j.RollingFileAppender\nlog4j.appender.R.maxFileSize=20MB\nlog4j.appender.R.maxBackupIndex=20\nlog4j.appender.R.layout=org.apache.log4j.PatternLayout\nlog4j.appender.R.layout.ConversionPattern=%5p [%t] %d{ISO8601} %F (line %L) %m%n\nlog4j.appender.R.File=/srv/cassandra/log/system.log\nlog4j.logger.org.apache.thrift.server.TNonblockingServer=ERROR\nEOF\ncl-sendfile.pl --list workers -x -l log4j-server.properties -r /opt/cassandra/conf/log4j-server.properties\n</pre><p>And with that, Cassandra is ready to start.</p><pre>cl-run.pl --list workers -c \"sudo systemctl start cassandra.service\"\nssh node3 tail -f /srv/cassandra/log/system.log\n</pre><h2>Installing Spark</h2><p>The process for Spark is quite similar, except that unlike Cassandra, it has a master.</p><p>Since I'm not using any Hadoop components, any of the builds should be fine so I used the\nhadoop2 build.</p><pre>pkg=\"spark-1.0.1-bin-hadoop2.tgz\"\nsudo curl -o /srv/public/tgz/$pkg http://d3kbcqa49mib13.cloudfront.net/spark-1.0.1-bin-hadoop2.tgz\ncl-run.pl --list all -c \"curl http://node0/tgz/$pkg |sudo tar -C /opt -xzf -\"\ncl-run.pl --list all -c \"sudo ln -s /opt/spark-1.0.1-bin-hadoop2 /opt/spark\"\ncl-run.pl --list all -c \"sudo mv /opt/spark/conf /etc/spark\"\ncl-run.pl --list all -c \"sudo ln -s /etc/spark /opt/spark/conf\"\n</pre><p>Create /srv/spark and the spark user.</p><pre>cat &gt; spark-user.sh &lt;&lt;EOF\nmkdir -p /srv/spark/{logs,work,tmp,pids}\n(grep -q '^spark:' /etc/group)  || groupadd -g 4321 spark\n(grep -q '^spark:' /etc/passwd) || useradd -u 4321 -c \"Apache Spark\" -g spark -s /bin/bash -d /srv/spark spark\nchown -R spark:spark /srv/spark\n# make spark tmp world writable and sticky\nchmod 4755 /srv/spark/tmp\nEOF\ncl-run.pl --list all -x -s spark-user.sh\n</pre><h2>Configuring Spark</h2><p>Many of Spark's settings are controlled by environment variables. Since I want all volatile data\nin /srv, many of these need to be changed. Spark will pick up spark-env.sh automatically.</p><p>The Intel NUC systems I'm running this stack on have 4 cores and 16G of RAM, so I'll give\nSpark 2 cores and 4G of memory for now.</p><p>One line worth calling out is the <code>SPARK_WORKER_PORT=9000</code>. It can be any port. If you don't set\nit, every time a work is restarted the master will have a stale entry for a while. It's not\na big deal but I like it better this way.</p><pre>cat &gt; spark-env.sh &lt;&lt;EOF\nexport SPARK_WORKER_CORES=\"2\"\nexport SPARK_WORKER_MEMORY=\"4g\"\nexport SPARK_DRIVER_MEMORY=\"2g\"\nexport SPARK_REPL_MEM=\"4g\"\nexport SPARK_WORKER_PORT=9000\nexport SPARK_CONF_DIR=\"/etc/spark\"\nexport SPARK_TMP_DIR=\"/srv/spark/tmp\"\nexport SPARK_PID_DIR=\"/srv/spark/pids\"\nexport SPARK_LOG_DIR=\"/srv/spark/logs\"\nexport SPARK_WORKER_DIR=\"/srv/spark/work\"\nexport SPARK_LOCAL_DIRS=\"/srv/spark/tmp\"\nexport SPARK_COMMON_OPTS=\"$SPARK_COMMON_OPTS -Dspark.kryoserializer.buffer.mb=32 \"\nLOG4J=\"-Dlog4j.configuration=file://$SPARK_CONF_DIR/log4j.properties\"\nexport SPARK_MASTER_OPTS=\" $LOG4J -Dspark.log.file=/srv/spark/logs/master.log \"\nexport SPARK_WORKER_OPTS=\" $LOG4J -Dspark.log.file=/srv/spark/logs/worker.log \"\nexport SPARK_EXECUTOR_OPTS=\" $LOG4J -Djava.io.tmpdir=/srv/spark/tmp/executor \"\nexport SPARK_REPL_OPTS=\" -Djava.io.tmpdir=/srv/spark/tmp/repl/\\$USER \"\nexport SPARK_APP_OPTS=\" -Djava.io.tmpdir=/srv/spark/tmp/app/\\$USER \"\nexport PYSPARK_PYTHON=\"/bin/python2\"\nEOF\n</pre><p>spark-submit and other tools may use spark-defaults.conf to find the master and other configuration items.</p><pre>cat &gt; spark-defaults.conf &lt;&lt;EOF\nspark.master            spark://node0.pc.datastax.com:7077\nspark.executor.memory   512m\nspark.eventLog.enabled  true\nspark.serializer        org.apache.spark.serializer.KryoSerializer\nEOF\n</pre><p>The systemd units are a little less complex than Cassandra's. The spark-master.service unit\nshould only exist on node0, while every other node runs spark-worker. Spark workers are given\na weight of 100 compared to Cassandra's weight of 1000 so that Cassandra is given priority over\nSpark without starving it entirely.</p><pre>cat &gt; spark-worker.service &lt;&lt;EOF\n[Unit]\nDescription=Spark Worker\nAfter=network.target\n[Service]\nType=forking\nUser=spark\nGroup=spark\nExecStart=/opt/spark/sbin/start-slave.sh 1 spark://node0.pc.datastax.com:7077\nStandardOutput=journal\nStandardError=journal\nLimitNOFILE=infinity\nLimitMEMLOCK=infinity\nLimitNPROC=infinity\nLimitAS=infinity\nCPUAccounting=true\nCPUShares=100\n[Install]\nWantedBy=multi-user.target\nEOF\n</pre><p>The master unit is similar and only gets installed on node0. Since it is not competing\nfor resources, there's no need to turn on cgroups for now.</p><pre>cat &gt; spark-master.service &lt;&lt;EOF\n[Unit]\nDescription=Spark Master\nAfter=network.target\n[Service]\nType=forking\nUser=spark\nGroup=spark\nExecStart=/opt/spark/sbin/start-master.sh 1\nStandardOutput=journal\nStandardError=journal\nLimitNOFILE=infinity\nLimitMEMLOCK=infinity\nLimitNPROC=infinity\nLimitAS=infinity\n[Install]\nWantedBy=multi-user.target\nEOF\n</pre><p>Now deploy all of these configs. Relocate the spark config into /etc/spark and copy\na couple templates, then write all the files there. spark-env.sh goes on all nodes.\nThe unit files are described above. Finally,\na command is run to instruct systemd to read the new unit files.</p><pre>cl-run.pl --list all -c \"sudo cp /opt/spark/conf/log4j.properties.template /opt/spark/conf/log4j.properties\"\ncl-run.pl --list all -c \"sudo cp /opt/spark/conf/fairscheduler.xml.template /opt/spark/conf/fairscheduler.xml\"\ncl-sendfile.pl --list all -x -l spark-env.sh -r /etc/spark/spark-env.sh\ncl-sendfile.pl --list all -x -l spark-defaults.conf -r /etc/spark/spark-defaults.conf\ncl-sendfile.pl --list workers -x -l spark-worker.service -r /etc/systemd/system/multi-user.target.wants/spark-worker.service\ncl-sendfile.pl --list all --incl node0 -x -l spark-master.service -r /etc/systemd/system/multi-user.target.wants/spark-master.service\ncl-run.pl --list all -c \"sudo systemctl daemon-reload\"\n</pre><p>With all of that done, it's time to turn on Spark to see if it works.</p><pre>cl-run.pl --list all --incl node0 -c \"sudo systemctl start spark-master.service\"\ncl-run.pl --list workers -c \"sudo systemctl start spark-worker.service\"\n</pre><p>Now I can browse to the Spark master webui.</p><p><img src=\"http://tobert.github.io/images/spark-master-screenshot-2014-07-15.jpg\" alt=\"screenshot\" /></p><h2>Installing spark-cassandra-connector</h2><p>The connector is now published in Maven and can be installed easiest using ivy on the\ncommand line. Ivy can pull all dependencies as well as the connector jar, saving a lot of\nfiddling around. In addition, while ivy can download the connector directly, it will\nend up pulling down all of Cassandra and Spark. The script fragment below pulls down only what\nis necessary to run the connector against a pre-built Spark.</p><p>This is only really needed for the spark-shell so it can access Cassandra. Most projects\nshould include the necessary jars in a fat jar rather than pushing these packages\nto every node.</p><p>I run these commands on node0 since that's where I usually work with spark-shell. To run it on\nanother machine, Spark will have to be present and match the version of the cluster, then this\nsame process will get everything needed to use the connector.</p><pre>cat &gt; download-connector.sh &lt;&lt;EOF\nmkdir /opt/connector\ncd /opt/connector\nrm *.jar\ncurl -o ivy-2.3.0.jar \\\n  'http://search.maven.org/remotecontent?filepath=org/apache/ivy/ivy/2.3.0/ivy-2.3.0.jar'\ncurl -o spark-cassandra-connector_2.10-1.0.0-beta1.jar \\\n  'http://search.maven.org/remotecontent?filepath=com/datastax/spark/spark-cassandra-connector_2.10/1.0.0-beta1/spark-cassandra-connector_2.10-1.0.0-beta1.jar'\nivy () { java -jar ivy-2.3.0.jar -dependency \\$* -retrieve \"[artifact]-[revision](-[classifier]).[ext]\"; }\nivy org.apache.cassandra cassandra-thrift 2.0.9\nivy com.datastax.cassandra cassandra-driver-core 2.0.3\nivy joda-time joda-time 2.3\nivy org.joda joda-convert 1.6\nrm -f *-{sources,javadoc}.jar\nEOF\nsudo bash download-connector.sh\n</pre><h2>Using spark-cassandra-connector With spark-shell</h2><p>All that's left to get started with the connector now is to get spark-shell to pick it up. The easiest\nway I've found is to set the classpath with --driver-class-path then restart the context in the REPL\nwith the necessary classes imported to make sc.cassandraTable() visible.</p><p>The newly loaded methods will not show up in tab completion. I don't know why.</p><pre>/opt/spark/bin/spark-shell --driver-class-path $(echo /opt/connector/*.jar |sed 's/ /:/g')\n</pre><p>It will print a bunch of log information then present scala&gt; prompt.</p><pre>scala&gt; sc.stop\n</pre><p>Now that the context is stopped, it's time to import the connector.</p><pre>scala&gt; import com.datastax.spark.connector._\nscala&gt; val conf = new SparkConf()\nscala&gt; conf.set(\"cassandra.connection.host\", \"node1.pc.datastax.com\")\nscala&gt; val sc = new SparkContext(\"local[2]\", \"Cassandra Connector Test\", conf)\nscala&gt; val table = sc.cassandraTable(\"keyspace\", \"table\")\nscala&gt; table.count\n</pre><p>To make sure everything is working, I ran some code I'm working on for my 2048 game analytics\nproject. Each context gets an application webui that displays job status.</p><p><img src=\"http://tobert.github.io/images/spark-stages-screenshot-2014-07-15.jpg\" alt=\"screenshot\" /></p><h2>Conclusion</h2><p>It was a lot of work getting here, but what we have at the end is a Spark shell that can\naccess tables in Cassandra as RDDs with types pre-mapped and ready to go.</p><p>There are some things that can be improved upon. I will likely package all of this into\na Docker image at some point. For now, I need it up and running for some demos that will\nbe running on portacluster at <a href=\"http://www.oscon.com/oscon2014\">OSCON 2014</a>.</p>",
        "created_at": "2018-07-24T19:54:20+0000",
        "updated_at": "2018-07-24T19:54:20+0000",
        "published_at": "2014-07-15T00:00:00+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 13,
        "domain_name": "tobert.github.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11152"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1128,
            "label": "tutorials",
            "slug": "tutorials"
          },
          {
            "id": 1151,
            "label": "resources",
            "slug": "resources"
          }
        ],
        "is_public": true,
        "id": 11151,
        "uid": "5b5783e5c0e858.98924635",
        "title": "Tuning DSE Search – Indexing latency and query latency",
        "url": "https://www.datastax.com/dev/blog/tuning-dse-search",
        "content": "<p>DataStax Enterprise offers out of the box search indexing for your Apache Cassandra™ data. The days of double writes or ETL's between separate DBMS and Search clusters are gone. I have my CQL table, I execute the following API call, and (boom) my Cassandra data is available for:</p><p>1) full text/fuzzy search</p><p>2) ad hoc Lucene secondary index powered filtering, and</p><p>3) geospatial searchHere is my API call:</p><pre>$ bin/dsetool create_core &lt;keyspace&gt;.&lt;table&gt; generateResources=true reindex=true&#13;\n</pre><p>or if you prefer curl (or are using basic auth) use the following:</p><pre>$ curl \"http://localhost:8983/solr/admin/cores?action=CREATE&amp;name=&lt;keyspace&gt;.&lt;table&gt;&amp;generateResources=true\"&#13;\n</pre><p>Rejoice! we are in inverted index, single cluster, operational simplicity bliss!</p><p>The remainder of this post will be focused on <strong>advanced tuning</strong> for DSE Search both for <strong>a)</strong> search indexing latency (the time it takes for data to be searchable after it has been inserted through cql), and <strong>b)</strong> search query latency (timings for your search requests).</p><h2 id=\"indexinglatency\">Indexing latency</h2><p>In this section I'll talk about the kinds of things we can do in order to</p><p>1) instrument and monitor DSE Search indexing and<br />2) tune indexing for lower latencies and increased performance</p><p><strong>Note</strong>: DSE Search ships with Real Time (RT) indexing which will give you faster indexing with 4.7.3, especially when it comes to the tails of your latency distribution. Here's one of our performance tests. It shows you real time vs near-real time indexing as of 4.7.0:</p><p><img class=\"\" src=\"https://s3.amazonaws.com/uploads.hipchat.com/6528/1116934/P10Ckn4e4cijTf0/upload.png\" alt=\"indexing chart\" width=\"488\" height=\"302\" /></p><p>Perhaps more importantly, as you get machines with more cores, you can continue to increase your indexing performance linearly:<br /><img class=\"\" src=\"https://s3.amazonaws.com/uploads.hipchat.com/6528/1116934/5OFvz6SgZsl68b1/Screen%20Shot%202016-03-15%20at%2011.03.22%20PM.png\" alt=\"rt vs nrt\" width=\"480\" height=\"302\" /></p><p>Be aware, however, that you should only run one RT search core per cluster since it is significantly more resource hungry than near-real time (NRT).</p><p><strong>Side note on GC</strong>: Because solr and Cassandra run on the same JVM in DSE Search and the indexing process generates a lot of java objects, running Search requires a larger JVM Heap. When running traditional <strong>CMS</strong>, we recommend a 14gb heap with about 2gb new gen. Consider the Stump's <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-8150\">CASSANDRA-8150</a> settings when running search with CMS. <strong>G1GC</strong> has been found to perform quite well with search workloads, I personally run with a 25gb heap (do not set new gen with G1, the whole point of G1 is that it sets it itself based on your workload!) and <code>gc_pause_ms</code> at about 1000 (go higher for higher throughput or lower to minimize latencies / p99's; don't go below 500). Update (thanks mc) you configure this setting in cassandra-env.sh.</p><h3 id=\"1instrumentation\">1) Instrumentation</h3><p><strong>Index Pool Stats:</strong></p><p>DSE Search parallelizes the indexing process and allocates work to a thread pool for indexing of your data.</p><p>Using JMX, you can see statistics on your indexing threadpool depth, completion, timings, and whether backpressure is active.</p><p>This is important because if your indexing queues get too deep, we risk having too much heap pressure =&gt; OOM's. Backpressure will throttle commits and eventually load shed if search can't keep up with an indexing workload. Backpressure gets triggered when the queues get too large.</p><p>The mbean is called:</p><pre>com.datastax.bdp.search.&lt;keyspace&gt;.&lt;table&gt;.IndexPool&#13;\n</pre><p><img class=\"\" src=\"https://s3.amazonaws.com/uploads.hipchat.com/6528/1116934/ObHJEFKOEuQnLm8/upload.png\" alt=\"Indexing queues\" width=\"523\" height=\"287\" /></p><p><strong>Commit/Update Stats:</strong></p><p>You can also see statistics on indexing performance (in microseconds) based on the particular stage of the indexing process for both <code>commit</code>s and <code>update</code>s.</p><p><strong>Commit:</strong></p><p>The stages are:</p><ul><li><code>FLUSH</code> - Comprising the time spent by flushing the async indexing queue.</li>\n<li><code>EXECUTE</code> - Comprising the time spent by actually executing the commit on the index.</li>\n</ul><p>The mbean is called:</p><ul><li><code>com.datastax.bdp.search.&lt;keyspace&gt;.&lt;table&gt;.CommitMetrics</code></li>\n</ul><p><strong>Update:</strong></p><p>The stages are:</p><ul><li><code>WRITE</code> - Comprising the time spent to convert the Solr document and write it into Cassandra (only available when indexing via the Solrj HTTP APIs). If you're using cql this will be 0.</li>\n<li><code>QUEUE</code> - Comprising the time spent by the index update task into the index pool.</li>\n<li><code>PREPARE</code>- Comprising the time spent preparing the actual index update.</li>\n<li><code>EXECUTE</code> - Comprising the time spent to actually executing the index update on Lucene.</li>\n</ul><p>The mbean is:</p><ul><li><code>`com.datastax.bdp.search.&lt;keyspace&gt;.&lt;table&gt;.UpdateMetrics` </code></li>\n</ul><p><img class=\"\" src=\"https://s3.amazonaws.com/uploads.hipchat.com/6528/1116934/CLrZrQNbRatrmck/upload.png\" alt=\"indexing stats\" width=\"524\" height=\"308\" /></p><p>Here, the average latency for the QUEUE stage of the <code>update</code> is 767 micros. See our docs for more details on the <a href=\"https://docs.datastax.com/en/datastax_enterprise/5.0/datastax_enterprise/srch/metricsMBeans.html\">metrics mbeans</a> and their stages.</p><h3 id=\"2tuning\">2) Tuning</h3><p>Almost everything in c* and DSE is configurable. Here's the key levers to get you better search indexing performance. Based on what you see in your instrumentation you can tune accordingly.</p><p>The main lever is <code>soft autocommit</code>, that's the minimum amount of time that will go by before queries are available for search. With RT we can set it to 250 ms or even as low as 100ms--given the right hardware. Tune this based on your SLA's.</p><p>The next most important lever is concurrency per core (or <code>max_solr_concurrency_per_core</code>). You can usually set this to number of CPU cores available to maximize indexing throughput.</p><p>Backpressure threshold will become more important as your load increases. Larger boxes can handle higher bp thresholds.</p><p>Don't forget to set up the ramBuffer to 2gb per the docs when you turn on RT indexing.</p><h2 id=\"querylatency\">Query Latency</h2><p>Now, I'll go over how we can monitor query performance in DSE Search, identify issues, and some of the tips / tricks we can use to improve search query performance. I will cover how to:</p><p>1) instrument and monitor DSE Search indexing and<br />2) tune indexing for lower latencies and increased performance.</p><p>Simliar to how search indexing performance scales with CPU's, search query performance scales with RAM. Keeping your search indexes in OS page cache is the biggest thing you can do to minimize latencies; so scale deliberately!</p><h3>1) Instrumentation</h3><p>There are multiple tools available for monitoring search performance.</p><h4 id=\"opscenter\">OpsCenter:</h4><p>OpsCenter supports a few search metrics that can be configured per node, datacenter, and solr core:</p><p>1) search latencies<br />2) search requests<br />3) index size<br />4) search timeouts<br />5) search errors</p><p><img class=\"\" src=\"https://s3.amazonaws.com/uploads.hipchat.com/6528/1116934/uoq1hLRQ58AZBhn/Screen%20Shot%202016-03-15%20at%2011.47.12%20PM.png\" alt=\"opscenter\" width=\"368\" height=\"226\" /></p><h4 id=\"metricsmbeans\">Metrics mbeans:</h4><p>In the same way that indexing has performance metrics, DSE Search <a href=\"https://docs.datastax.com/en/datastax_enterprise/4.0/datastax_enterprise/srch/srchQryMbean.html\">query performance metrics</a> are available through JMX and can be useful for troubleshooting perofrmance issues. We can use the <code>query.name</code> parameter in your DSE Search queries to capture metrics for specifically tagged queries.</p><p><strong>Query:</strong></p><p>The stages of <code>query</code> are:</p><ul><li><code>COORDINATE</code> - Comprises the total amount of time spent by the coordinator node to distribute the query and gather/process results from shards. This value is computed only on query coordinator nodes.</li>\n<li><code>EXECUTE</code> - Comprises the time spent by a single shard to execute the actual index query. This value is computed on the local node executing the shard query.</li>\n<li><code>RETRIEVE</code> - Comprises the time spent by a single shard to retrieve the actual data from Cassandra. This value will be computed on the local node hosting the requested data.</li>\n</ul><p>The mbean is:</p><ul><li><code>com.datastax.bdp.search.&lt;keyspace&gt;.&lt;table&gt;.QueryMetrics </code></li>\n</ul><h4 id=\"querytracing\">Query Tracing:</h4><p>When using <code>solr_query</code> via cql, query tracing can provide useful information as to where a particular query spent time in the cluster.</p><p>Query tracing is available in cqlsh <code>tracing on</code>, in DevCenter (in the tab at the bottom of the screen), and via probabilistic tracing which is configurable via <a href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsSetTraceProbability.html\">nodetool</a>.</p><p>When users complain about a slow query and you need to find out what it is, the DSE Search slow query log is a good starting point.</p><pre>dsetool perf solrslowlog enable&#13;\n</pre><p>Stores to a table in cassandra in the <code>dse_perf.solr_slow_sub_query_log</code> table</p><h3>2) Tuning</h3><p>Now let's focus on some tips for how you can improve search query performance.</p><h3 id=\"indexsize\">Index size</h3><p>Index size is so important that, I wrote <a href=\"http://www.sestevez.com/solr-space-saving-profile/\">a separate post</a> just on that subject.</p><h4 id=\"qvsfq\">Q vs. FQ</h4><p>In order to take advantage of the solr filter cache, build your queries using fq not q. The filter cache is the only solr cache that persists across commits so don't spend time or valuable RAM trying to leverage the other caches.</p><h4 id=\"solrqueryrouting\">Solr query routing</h4><p>Partition routing is a great multi-tennancy feature in DSE Search that lets you limit the amount of fan out that a search query will take under the hood. Essentially, you're able to specify a Cassandra partition that you are interested in limiting your search to. This will limit the number of nodes that DSE Search requires to fullfil your request.</p><h4 id=\"usedocvaluesforfacetingandsorting\">Use docvalues for Faceting and Sorting.</h4><p>To get improved performance and to avoid OOMs from the field cache, always remember to turn on docvalues on fields that you will be sorting and faceting over. This may become mandatory in DSE at some point so plan ahead.</p><h3 id=\"otherdsedifferentiators\">Other DSE Differentiators</h3><p>If you're comparing DSE Search against other search offerings / technologies, the following two differentiators are unique to DSE Search.</p><h4 id=\"faulttolerantdistributedqueries\">Fault tolerant distributed queries</h4><p>If a node dies during a query, we retry the query on another node.</p><h4 id=\"nodehealth\">Node health</h4><p>Node health and shard router behavior.<br />DSE Search monitors node health and makes distributed query routing decisions based on the following:</p><p>1) Uptime: a node that just started may well be lacking the most up-to-date data (to be repaired via HH or AE).<br />2) Number of dropped mutations.<br />3) Number of hints the node is a target for.<br />4) \"failed reindex\" status.</p><p>All you need to take advantage of this is be on a modern DSE version.</p><hr /><p><a href=\"https://www.datastax.com/\">DataStax</a> has many ways for you to advance in your career and knowledge. \n</p><p>You can take <a href=\"https://academy.datastax.com/user/register?destination=home&amp;utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_register\" target=\"_self\" title=\"academy.datastax.com\">free classes</a>, <a href=\"https://academy.datastax.com/certifications?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_certifications\" target=\"_self\" title=\"academy.datastax.com/certifications\">get certified</a>, or read <a href=\"https://www.datastax.com/dbas-guide-to-nosql\" target=\"_self\" title=\"dbas-guide-to-nosql\">one of our many white papers</a>.\n</p><p><a href=\"https://academy.datastax.com/user/register?destination=home&amp;utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_register\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"academy.datastax.com\">register for classes</a>\n</p><p><a href=\"https://academy.datastax.com/certifications?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_certifications\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"academy.datastax.com/certifications\">get certified</a>\n</p><p><a href=\"http://www.datastax.com/dbas-guide-to-nosql?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_dbasguidetonosql\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"dbas-guide-to-nosql\">DBA's Guide to NoSQL</a>\n</p><br class=\"clear\" /><div id=\"mto_newsletter_121316_Css\"><p>Subscribe for newsletter:</p><br /></div>",
        "created_at": "2018-07-24T19:54:13+0000",
        "updated_at": "2018-07-24T19:54:13+0000",
        "published_at": "2016-03-17T22:04:52+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 7,
        "domain_name": "www.datastax.com",
        "preview_picture": "https://s3.amazonaws.com/uploads.hipchat.com/6528/1116934/P10Ckn4e4cijTf0/upload.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11151"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 748,
            "label": "documentation",
            "slug": "documentation"
          },
          {
            "id": 1151,
            "label": "resources",
            "slug": "resources"
          }
        ],
        "is_public": true,
        "id": 11149,
        "uid": "5b5783dab93f29.86582613",
        "title": "DataStax Docs",
        "url": "https://docs.datastax.com/en/landing_page/doc/landing_page/current.html",
        "content": "<p class=\"shortdesc\">Documentation for configuring, upgrading, and deploying DataStax Enterprise, DataStax\n    OpsCenter, CQL, DataStax Drivers, DataStax Studio, and DataStax DevCenter.</p><p></p><table class=\"table frame-all\"><caption>Simba ODBC Driver for Spark <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/spark/simbaOdbcDriverLinux.html\" target=\"_blank\">(Linux)</a> /\n                                                  <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/spark/simbaOdbcDriverWindows.html\" target=\"_blank\">(Windows)</a>\n                                                  Simba ODBC Driver for Spark <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-dev/datastax_enterprise/spark/simbaOdbcDriverLinux.html\" target=\"_blank\">(Linux)</a> /\n                                                  <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-dev/datastax_enterprise/spark/simbaOdbcDriverWindows.html\" target=\"_blank\">(Windows)</a>\n                                                  <a class=\"xref\" href=\"https://downloads.datastax.com/odbc-cql/2.5.6.1011/Simba%20Cassandra%20ODBC%20Install%20and%20Configuration%20Guide.pdf\" target=\"_blank\">DataStax ODBC\n                                                  Driver for Apache Cassandra and DataStax\n                                                  Enterprise with CQL connector</a>\n                                                <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/spark/simbaJdbcDriver.html\" target=\"_blank\">Simba JDBC Driver\n                                                  for Spark</a>\n                                                  <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-dev/datastax_enterprise/spark/simbaJdbcDriver.html\" target=\"_blank\">Simba JDBC Driver\n                                                  for Spark</a>\n                                                  \n                                                </caption></table><p></p><table class=\"table frame-all\"><caption><a class=\"xref\" href=\"https://docs.datastax.com/en/developer/cpp-driver/latest\" target=\"_blank\">C/C++ OSS\n                                                  driver</a>\n                                                \n                                                  <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/csharp-driver/latest\" target=\"_blank\">C# OSS\n                                                  driver</a>\n                                                <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/java-driver/latest\" target=\"_blank\">Java OSS\n                                                  driver</a>\n                                                <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/nodejs-driver/latest\" target=\"_blank\">Node.js OSS\n                                                  driver</a>\n                                                <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/php-driver/latest\" target=\"_blank\">PHP OSS\n                                                  driver</a>\n                                                <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/python-driver/latest\" target=\"_blank\">Python OSS\n                                                  driver</a>\n                                                  \n                                                <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/ruby-driver/latest\" target=\"_blank\">Ruby OSS\n                                                  driver</a>\n                                                </caption></table><p></p><table class=\"table frame-all\"><caption><a class=\"xref\" href=\"https://docs.datastax.com/en/developer/cpp-driver-dse/latest\" target=\"_blank\">C/C++ DSE\n                                                  driver</a>\n                                                <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/csharp-driver-dse/latest\" target=\"_blank\">C# DSE\n                                                  driver</a> / <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/csharp-dse-graph/latest\" target=\"_blank\">DSE Graph\n                                                  Extension</a>\n                                                <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/java-driver-dse/latest\" target=\"_blank\">Java DSE driver\n                                                  </a>(DSE Graph Extension included)\n                                                <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/nodejs-driver-dse/latest\" target=\"_blank\">Node.js DSE driver\n                                                  </a>/ <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/nodejs-dse-graph/latest\" target=\"_blank\">DSE Graph\n                                                  Extension</a>\n                                                <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/php-driver-dse/latest\" target=\"_blank\">PHP DSE\n                                                  driver</a>\n                                                <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/python-dse-driver/latest\" target=\"_blank\">Python DSE\n                                                  driver</a> / <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/python-dse-graph/latest\" target=\"_blank\">DSE Graph\n                                                  Extension</a>\n                                                <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/ruby-driver-dse/latest\" target=\"_blank\">Ruby DSE\n                                                  driver</a>\n                                                </caption></table><section class=\"section\"><table class=\"table z_lp-table frame-none\"><caption><a class=\"xref\" href=\"https://docs.datastax.com/ja/landing_page-jajp/doc/landing_page/current.html\" target=\"_blank\">Japanese docs: 日本語ドキュメント</a>\n            \n            Getting startedPlanning and installing\n                  softwareDSE Installation Guide <a class=\"xref\" href=\"https://docs.datastax.com/en/install/doc/\" target=\"_blank\">6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/install/installTOC.html\" target=\"_blank\">5.1</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/datastax_enterprise/5.0/datastax_enterprise/install/installTOC.html\" target=\"_blank\">5.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/datastax_enterprise/4.8/datastax_enterprise/install/installTOC.html\" target=\"_blank\">4.8</a><br />                  OpsCenter Installation Guide <a class=\"xref\" href=\"https://docs.datastax.com/en/install/doc/install60/opscInstallOpsc.html\" target=\"_blank\">6.5</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/opscenter/6.1/opsc/install/opscInstallOpsc_g.html\" target=\"_blank\">6.1</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/opscenter/6.0/opsc/install/opscInstallOpsc_g.html\" target=\"_blank\">6.0</a>Architecture Guide <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-arch/\" target=\"_blank\">6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-arch/\" target=\"_blank\">5.1</a><a class=\"xref\" href=\"https://docs.datastax.com/en/landing_page/doc/landing_page/docker.html\" title=\"Use DataStax-provided Docker images to learn DataStax Enterprise (DSE), DataStax OpsCenter, and DataStax Studio, try new ideas, and test and demonstrate an application.\">Docker</a> <a class=\"xref\" href=\"https://docs.datastax.com/en/dse-planning/doc/\" target=\"_blank\">Planning Guide</a>Upgrading\n                  software<a class=\"xref\" href=\"https://docs.datastax.com/en/upgrade/doc/upgrade/datastax_enterprise/upgrdDSE.html\" target=\"_blank\">Upgrading DSE</a><a class=\"xref\" href=\"https://docs.datastax.com/en/upgrade/doc/upgrade/datastax_enterprise/upgrdCstarToDSE.html\" target=\"_blank\">Upgrading from Apache Cassandra</a>™<a class=\"xref\" href=\"https://docs.datastax.com/en/upgrade/doc/upgrade/opscenter/upgdOpsc.html\" target=\"_blank\">Upgrading OpsCenter</a>LearningTutorials<a class=\"xref\" href=\"https://docs.datastax.com/en/playlist/doc/\" target=\"_blank\">Playlist tutorial</a><a class=\"xref\" href=\"https://docs.datastax.com/en/tutorials/kerberos/\" target=\"_blank\">Kerberos tutorial</a><a class=\"xref\" href=\"https://www.datastax.com/what-we-offer/products-services/sandbox\" target=\"_blank\">DataStax Sandbox</a>Additional\n                  resources<a class=\"xref\" href=\"https://academy.datastax.com/\" target=\"_blank\">DataStax Academy</a><a class=\"xref\" href=\"https://academy.datastax.com/developer-blog\" target=\"_blank\">DataStax Developer Blog</a><a class=\"xref\" href=\"https://support.datastax.com/\" target=\"_blank\">DataStax Support</a>DSE Security <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/security/securityTOC.html\" target=\"_blank\"> 6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/search/searchTOC.html\" target=\"_blank\">5.1</a>DSE utilitiesAdditional\n                  tools<a class=\"xref\" href=\"https://docs.datastax.com/en/dsbulk/doc/index.html\" target=\"_blank\">DataStax Bulk Loader</a> for DSE 4.8-6.0DSE Graph Loader <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-dev/datastax_enterprise/graph/dgl/dglOverview.html\" target=\"_blank\">6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-dev/datastax_enterprise/graph/dgl/dglOverview.html\" target=\"_blank\">5.1</a>DSE stress tools <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/tools/stressToolsTOC.html\" target=\"_blank\">6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/tools/stressToolsTOC.html\" target=\"_blank\">5.1</a>dsetool <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/tools/dsetool/dsetool.html\" target=\"_blank\">6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/tools/dsetool/dsetool.html\" target=\"_blank\">5.1</a>nodetool <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/tools/nodetool/toolsNodetool.html\" target=\"_blank\">6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/tools/nodetool/toolsNodetool.html\" target=\"_blank\">5.1</a>SSTable utilities <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/tools/toolsSStables/toolsSSTableUtilitiesTOC.html\" target=\"_blank\">6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/tools/toolsSStables/toolsSSTableUtilitiesTOC.html\" target=\"_blank\">5.1</a>\n              Developing and administering\n                  applicationsDSE\n                    <sup class=\"ph sup\">Administrator Guide <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/\" target=\"_blank\">6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/\" target=\"_blank\">5.1</a>Developer Guide <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-dev/\" target=\"_blank\">6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-dev/\" target=\"_blank\">5.1</a><a class=\"xref\" href=\"https://docs.datastax.com/en/dse-trblshoot/doc/\" target=\"_blank\">Troubleshooting Guide</a><a class=\"xref\" href=\"https://docs.datastax.com/en/datastax_enterprise/5.0/\" target=\"_blank\">DSE 5.0</a> (<a class=\"xref\" href=\"https://docs.datastax.com/en/cassandra/3.0/\" target=\"_blank\">C* 3.0</a>) | <a class=\"xref\" href=\"https://docs.datastax.com/en/datastax_enterprise/4.8/\" target=\"_blank\">4.8</a> (<a class=\"xref\" href=\"https://docs.datastax.com/en/cassandra/2.1/\" target=\"_blank\">C* 2.1</a>) <sup class=\"ph sup\"><br />Tools and\n                  extensionsCQL <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/cql/\" target=\"_blank\"> 6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/cql/\" target=\"_blank\">5.1</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/cql/3.3/cql/cqlIntro.html\" target=\"_blank\">3.3</a> <sup class=\"ph sup\"><a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-dev/datastax_enterprise/studio/studioToc.html\" target=\"_blank\">Studio 6.0</a> for DSE 6.0<a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-dev/datastax_enterprise/studio/stdToc.html\" target=\"_blank\">Studio 2.0</a> for DSE 5.0, 5.1<a class=\"xref\" href=\"https://docs.datastax.com/en/developer/devcenter/doc/\" target=\"_blank\">DevCenter</a> for DSE 4.8, 5.0<a class=\"xref\" href=\"https://docs.datastax.com/en/landing_page/doc/landing_page/apiDocs.html\" title=\"The following links provide access to DataStax API documentation.\">DataStax API</a>DSE\n                  OpsCenterOpsCenter <a class=\"xref\" href=\"https://docs.datastax.com/en/opscenter/6.5/\" target=\"_blank\">6.5</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/opscenter/6.1/\" target=\"_blank\">6.1</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/opscenter/6.0/\" target=\"_blank\">6.0</a>Lifecycle Manager <a class=\"xref\" href=\"https://docs.datastax.com/en/opscenter/6.5/opsc/LCM/opscLCM.html\" target=\"_blank\">6.5</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/opscenter/6.1/opsc/LCM/opscLCM.html\" target=\"_blank\">6.1</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/opscenter/6.0/opsc/LCM/opscLCM.html\" target=\"_blank\">6.0</a>DataStax\n                  driversDSE advanced functionalityAdvanced\n                featuresDSE Analytics <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/analytics/analyticsOverview.html\" target=\"_blank\">6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/analytics/analyticsTOC.html\" target=\"_blank\">5.1</a>DSE Graph <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/graph/graphTOC.html\" target=\"_blank\">6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/graph/graphTOC.html\" target=\"_blank\">5.1</a>DSE Search <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/search/searchTOC.html\" target=\"_blank\">6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/search/searchTOC.html\" target=\"_blank\">5.1</a>\n            </sup></sup></sup></caption></table></section>",
        "created_at": "2018-07-24T19:54:02+0000",
        "updated_at": "2018-07-24T19:54:02+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "docs.datastax.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11149"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 748,
            "label": "documentation",
            "slug": "documentation"
          },
          {
            "id": 1151,
            "label": "resources",
            "slug": "resources"
          }
        ],
        "is_public": true,
        "id": 11148,
        "uid": "5b5783d6cac8b5.74611097",
        "title": "Documentation",
        "url": "http://cassandra.apache.org/doc/latest/",
        "content": "<p>This documentation is currently a work-in-progress and contains a number of TODO sections.\n    <a href=\"http://cassandra.apache.org/doc/latest/bugs.html\">Contributions</a> are welcome.</p><h3>Main documentation</h3><table class=\"contentstable doc-landing-table\" style=\"margin: auto;\"><tr><td class=\"left-column\">\n      <p class=\"biglink\"><a class=\"biglink\" href=\"http://cassandra.apache.org/doc/latest/getting_started/index.html\">Getting started</a><br />Newbie friendly starting point</p>\n    </td>\n    <td class=\"right-column\">\n      <p class=\"biglink\"><a class=\"biglink\" href=\"http://cassandra.apache.org/doc/latest/operating/index.html\">Operating Cassandra</a><br />The operator's corner</p>\n    </td>\n  </tr><tr><td class=\"left-column\">\n      <p class=\"biglink\"><a class=\"biglink\" href=\"http://cassandra.apache.org/doc/latest/architecture/index.html\">Cassandra Architecture</a><br />Cassandra's big picture</p>\n    </td>\n    <td class=\"right-column\">\n      <p class=\"biglink\"><a class=\"biglink\" href=\"http://cassandra.apache.org/doc/latest/tools/index.html\">Cassandra's Tools</a><br />cqlsh, nodetool, ...</p>\n    </td>\n  </tr><tr><td class=\"left-column\">\n      <p class=\"biglink\"><a class=\"biglink\" href=\"http://cassandra.apache.org/doc/latest/data_modeling/index.html\">Data Modeling</a><br />Or how to make square pegs fit round holes</p>\n    </td>\n    <td class=\"right-column\">\n      <p class=\"biglink\"><a class=\"biglink\" href=\"http://cassandra.apache.org/doc/latest/troubleshooting/index.html\">Troubleshooting</a><br />What to look for when you have a problem</p>\n    </td>\n  </tr><tr><td class=\"left-column\">\n      <p class=\"biglink\"><a class=\"biglink\" href=\"http://cassandra.apache.org/doc/latest/cql/index.html\">Cassandra Query Language</a><br />CQL reference documentation</p>\n    </td>\n    <td class=\"right-column\">\n      <p class=\"biglink\"><a class=\"biglink\" href=\"http://cassandra.apache.org/doc/latest/development/index.html\">Cassandra Development</a><br />Learn how to improve Cassandra and contribute patches</p>\n    </td>\n  </tr><tr><td class=\"left-column\">\n      <p class=\"biglink\"><a class=\"biglink\" href=\"http://cassandra.apache.org/doc/latest/faq/index.html\">FAQs</a><br />Frequently Asked Questions (with answers!)</p>\n    </td>\n    <td class=\"right-column\">\n      <p class=\"biglink\"><a class=\"biglink\" href=\"http://cassandra.apache.org/doc/latest/configuration/index.html\">Configuration</a><br />Cassandra's handles and knobs</p>\n    </td>\n  </tr></table><h3>Meta informations</h3><ul><li><a class=\"biglink\" href=\"http://cassandra.apache.org/doc/latest/bugs.html\">Reporting bugs</a></li>\n  <li><a class=\"biglink\" href=\"http://cassandra.apache.org/doc/latest/contactus.html\">Contact us</a></li>\n</ul><h3>Documentation for older releases</h3><p>The Cassandra Query Language (CQL) documentation for older releases are:\n</p><ul><li><a href=\"http://cassandra.apache.org/doc/old/CQL-3.0.html\">CQL for the 3.0 series</a></li>\n  <li><a href=\"http://cassandra.apache.org/doc/old/CQL-2.2.html\">CQL for the 2.2 series</a></li>\n  <li><a href=\"http://cassandra.apache.org/doc/old/CQL-2.1.html\">CQL for the 2.1 series</a></li>\n</ul>",
        "created_at": "2018-07-24T19:53:58+0000",
        "updated_at": "2018-07-24T19:53:58+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 0,
        "domain_name": "cassandra.apache.org",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11148"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1135,
            "label": "packages",
            "slug": "packages"
          },
          {
            "id": 1166,
            "label": "admin-monitor",
            "slug": "admin-monitor"
          }
        ],
        "is_public": true,
        "id": 11147,
        "uid": "5b5783a1b90d00.22334937",
        "title": "DataStax Enterprise OpsCenter",
        "url": "https://www.datastax.com/products/datastax-opscenter",
        "content": "DataStax Enterprise OpsCenter | DataStax\n\n<noscript>\n\n\n\n<div class=\"DS17\"><div class=\"connect-us\"><a href=\"https://www.datastax.com/contactus\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Mail.svg\" alt=\"email icon\" />email</a><a href=\"https://www.datastax.com/company#offices\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Phone.svg\" alt=\"phone icon\" />call</a></div></div><header class=\"DS17\"><div class=\"container\"><div class=\"wrapper\"><div class=\"logo\"><a href=\"https://www.datastax.com/\"><img src=\"https://www.datastax.com/templates/dist/images/logo-header.png\" alt=\"DataStax logo\" /></a><a href=\"https://www.datastax.com/\"><img src=\"https://www.datastax.com/templates/dist/images/new_logo.png\" alt=\"DataStax logo\" /></a></div></div></div>\n  \n</header><section class=\"cards wow fadeInUp dse-cards no-padding-bottom\" data-wow-delay=\"0.5s\"><div class=\"container\"><p>&#13;\n        </p><h3> DSE OpsCenter Features</h3>&#13;<div class=\"column-wrapper text-center\"><div class=\"three-col wow fadeInUp col card-blue bg-gray\"><p>&#13;\n              </p><h4>Operational Simplicity</h4>&#13;<div class=\"icon-box\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Operational_Simplicity.svg\" alt=\"operational simplicity icon\" /></div><p>Eliminate operational headaches with automatic backups, reduced manual operations, and rapid performance issue detection.</p></div><div class=\"three-col wow fadeInUp col card-blue bg-gray\"><p>&#13;\n              </p><h4>Visual Monitoring</h4>&#13;<div class=\"icon-box\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Visual_Monitoring.svg\" alt=\"visual monitoring icon\" /></div><p>Easily spot outliers and performance bottlenecks via customized dashboards with real-time and historical system metrics.</p></div><div class=\"three-col wow fadeInUp col card-blue bg-gray\"><p>&#13;\n              </p><h4>Enterprise-Ready</h4>&#13;<div class=\"icon-box\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Enterprise-Ready.svg\" alt=\"Enterprise ready icon\" /></div><p>Automatic failover and fine-grained roles and permissions ensure always-on, integrated, and secure management and monitoring.</p></div></div></div></section><section class=\"content-with-media wow fadeInUp dse-content-with-media bg-white\" data-wow-delay=\"0.5s\"><section class=\"intro-with-content  dse-intro bg-gray\"><div class=\"container\"><div class=\"wrapper text-center\"><div class=\"logo wow fadeInUp\"><img src=\"https://www.datastax.com/templates/dist/images/Lock.png\" alt=\"circular icon with lock and key hole\" /></div><div class=\"content wow fadeInUp\"><h3>Large-Scale Operations on Autopilot</h3><p>Make your database management and monitoring as simple as the push of a button to increase operational simplicity and productivity and reduce operational overhead.</p></div><div class=\"col-wrapper\"><div class=\"three-col wow fadeInUp\"><h6>Automated Data Synchronization</h6><p>DSE NodeSync significantly reduces manual repair operations and eliminates manual repair-related cluster outages, resulting in lower operational cost and shorter support cycles. OpsCenter provides a user-friendly view of NodeSync operations for easy monitoring and troubleshooting.</p></div><div class=\"three-col wow fadeInUp\"><h6>Simplified Upgrades</h6><p>Upgrade Service in OpsCenter LifeCycle Manager enables you to perform patch upgrades of DSE clusters at the data center, rack, or node level with up to 60% less manual involvement. You can easily clone your existing config profile to ensure compatibility with DSE upgrades, which is key for running business-critical applications.</p></div><div class=\"three-col wow fadeInUp\"><h6>Full and Continuous Backups</h6><p>Our backup service delivers full backup and disaster recovery protection for DSE clusters, including the ability to visually schedule the backup and restore of hundreds of nodes at a point in time, visually monitor backup and restore tasks, and clone database clusters.</p></div><div class=\"three-col wow fadeInUp\"><h6> End-to-End Performance Visibility </h6><p>Performance Service collects key metrics to assess the health of DSE nodes, stores them, and makes them accessible visually or via command-line tools, allowing you to capture granular statistics at the system, user, or statement level and provide context-specific recommendations to resolve performance bottlenecks.</p></div><div class=\"three-col wow fadeInUp\"><h6>Seamless Enterprise Integration</h6><p>A comprehensive set of OpsCenter RESTful APIs allows you to easily provision, monitor, and execute maintenance tasks using your favorite scripting language and integrate powerful functionality into your existing tools and workflows.</p></div><div class=\"three-col wow fadeInUp\"><h6>Comprehensive Cluster Health Management</h6><p>Our Best Practice Service periodically scans database clusters and automatically detects and reports issues that threaten the cluster’s security, availability, or performance. Our Capacity Service accumulates and analyzes cluster health and resource utilization metrics to help you understand cluster performance over time as well as predict future usage and growth.</p></div></div></div></div></section><section class=\"full-width-cta no-padding dse-full-cta\"><div class=\"bg-img\"><img src=\"https://www.datastax.com/templates/dist/images/opscenter/Enterprise_OpsCenter_Image3.jpg\" alt=\"two women software developers learning about DataStax Graph Academy\" /></div><div class=\"v-middle-wrapper wow fadeInUp\"><div class=\"v-middle-inner\"><div class=\"v-middle\"><div class=\"text-center content-650 content-wrapper\"><h3>Monitoring Your Cluster with DSE OpsCenter</h3><p>Get started on DSE OpsCenter using this free course on DataStax Academy.</p><a href=\"https://academy.datastax.com/units/monitoring-your-cluster-opscenter\" class=\"btn-default\">Sign Up</a></div></div></div></div></section><section class=\"latest-block\"><section class=\"cta round-left-bottom no-padding\"><div class=\"DS17\"><div class=\"use-case\"><div class=\"wrapper\"><div class=\"two-col text-light-blue\"><h6>Customer Experience</h6><ul><li><a href=\"https://www.datastax.com/use-cases/customer-360\">Customer 360</a></li>\n          <li><a href=\"https://www.datastax.com/personalization\">Personalization &amp; Recommendations</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/loyalty-programs\">Loyalty Programs</a></li>\n          <li><a href=\"https://www.datastax.com/fraud-detection\">Consumer Fraud Detection</a></li>\n        </ul></div><div class=\"two-col text-light-green\"><h6><a href=\"#\">Enterprise Optimization</a></h6><ul><li><a href=\"https://www.datastax.com/use-cases/ecommerce\">eCommerce</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/identity-management\">Identity Management</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/security\">Security and Compliance</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/supply-chain\">Supply Chain</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/inventory-management\">Inventory Management</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/asset-monitoring\">Asset Monitoring</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/logistics\">Logistics</a></li>\n        </ul></div></div></div></div>\n\t\n\t\n</section></section></section></noscript>",
        "created_at": "2018-07-24T19:53:05+0000",
        "updated_at": "2018-07-24T19:53:05+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 2,
        "domain_name": "www.datastax.com",
        "preview_picture": "https://www.datastax.com/wp-content/themes/datastax-2014-08/images/common/DataStax_Web_Social_DefaultGenericV2_1024x351_wide.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11147"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 1135,
            "label": "packages",
            "slug": "packages"
          }
        ],
        "is_public": true,
        "id": 11146,
        "uid": "5b57835573c0e3.06974438",
        "title": "Stratio/deep-spark",
        "url": "https://github.com/Stratio/deep-spark",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\"><p>*Disclaimer: As of 01/06/2015 this project has been deprecated. Thank you for your understanding and continued help throughout the project's life.</p>\n<p>Deep is a thin integration layer between Apache Spark and several NoSQL datastores.\nWe actually support Apache Cassandra, MongoDB, Elastic Search, Aerospike, HDFS, S3 and any database accessible through JDBC, but in the near future we will add support for sever other datastores.</p>\n<ul><li>JIRA: <a href=\"https://deep-spark.atlassian.net\" rel=\"nofollow\">https://deep-spark.atlassian.net</a></li>\n</ul>\n<p>In order to compile the deep-jdbc module is necessary to add the Oracle ojdbc driver into your local repository. You can download it from the URL: <a href=\"http://www.oracle.com/technetwork/database/features/jdbc/default-2280470.html\" rel=\"nofollow\">http://www.oracle.com/technetwork/database/features/jdbc/default-2280470.html</a>. When you are on the web you must click in \"Accept License Agreement\" and later downlad ojdbc7.jar library. You need a free oracle account to download the official driver.</p>\n<p>To install the ojdbc driver in your local repository you must execute the command below:</p>\n<blockquote>\n<p>mvn install:install-file -Dfile= -DgroupId=com.oracle -DartifactId=ojdbc7  -Dversion=12.1.0.2 -Dpackaging=jar</p>\n</blockquote>\n<p>After that you can compile Deep executing the following steps:</p>\n<blockquote>\n<p>cd deep-parent</p>\n</blockquote>\n<blockquote>\n<p>mvn clean install</p>\n</blockquote>\n<p>If you want to create a Deep distribution you must execute the following steps:</p>\n<blockquote>\n<p>cd deep-scripts</p>\n</blockquote>\n<blockquote>\n<p>make-distribution-deep.sh</p>\n</blockquote>\n<p>During the creation you'll see the following question:</p>\n<blockquote>\n<p>What tag want to use for Aerospike native repository?</p>\n</blockquote>\n<p>You must type 0.7.0 and press enter.</p>\n<p>The integration is <em>not</em> based on the Cassandra's Hadoop interface.</p>\n<p>Deep comes with an user friendly API that lets developers create Spark RDDs mapped to Cassandra column families.\nWe provide two different interfaces:</p>\n<ul><li>\n<p>The first one will let developers map Cassandra tables to plain old java objects (POJOs), just like if you were using any other ORM. We call this API the 'entity objects' API.\nThis abstraction is quite handy, it will let you work on RDD (under the hood Deep will transparently map Cassandra's columns to entity properties).\nYour domain entities must be correctly annotated using Deep annotations (take a look at deep-core example entities in package com.stratio.deep.core.entity).</p>\n</li>\n<li>\n<p>The second one is a more generic 'cell' API, that will let developerss work on RDD&lt;com.stratio.deep.entity.Cells&gt; where a 'Cells' object is a collection of com.stratio.deep.entity.Cell objects.\nColumn metadata is automatically fetched from the data store. This interface is a little bit more cumbersome to work with (see the example below),\nbut has the advantage that it doesn't require the definition of additional entity classes.\nExample: you have a table called 'users' and you decide to use the 'Cells' interface. Once you get an instance 'c' of the Cells object,\nto get the value of column 'address' you can issue a c.getCellByName(\"address\").getCellValue().\nPlease, refer to the Deep API documentation to know more about the Cells and Cell objects.</p>\n</li>\n</ul><p>We encourage you to read the more comprehensive documentation hosted on the <a href=\"http://www.openstratio.org/\" rel=\"nofollow\">Openstratio website</a>.</p>\n<p>Deep comes with an example sub project called 'deep-examples' containing a set of working examples, both in Java and Scala.\nPlease, refer to the deep-example project README for further information on how to setup a working environment.</p>\n<p>Spark-MongoDB connector is based on Hadoop-mongoDB.</p>\n<p>Support for MongoDB has been added in version 0.3.0.</p>\n<p>We provide two different interfaces:</p>\n<ul><li>\n<p>ORM API, you just have to annotate your POJOs with Deep annotations and magic will begin, you will be able to connect MongoDB with Spark using your own model entities.</p>\n</li>\n<li>\n<p>Generic cell API, you do not need to specify the collection's schema or add anything to your POJOs, each document will be transform to an object \"Cells\".</p>\n</li>\n</ul><p>We added a few working examples for MongoDB in deep-examples subproject, take a look at:</p>\n<p>Entities:</p>\n<ul><li>com.stratio.deep.examples.java.ReadingEntityFromMongoDB</li>\n<li>com.stratio.deep.examples.java.WritingEntityToMongoDB</li>\n<li>com.stratio.deep.examples.java.GroupingEntityWithMongoDB</li>\n</ul><p>Cells:</p>\n<ul><li>com.stratio.deep.examples.java.ReadingCellFromMongoDB</li>\n<li>com.stratio.deep.examples.java.WritingCellToMongoDB</li>\n<li>com.stratio.deep.examples.java.GroupingCellWithMongoDB</li>\n</ul><p>You can check out our first steps guide here:</p>\n<p><a href=\"https://github.com/Stratio/deep-spark/blob/master/doc/src/site/sphinx/t20-first-steps-deep-mongodb.rst\">First steps with Deep-MongoDB</a></p>\n<p>We are working on further improvements!</p>\n<p>Support for ElasticSearch has been added in version 0.5.0.</p>\n<p>Support for Aerospike has been added in version 0.6.0.</p>\n<p>Examples:</p>\n<p>Entities:</p>\n<ul><li>com.stratio.deep.examples.java.ReadingEntityFromAerospike</li>\n<li>com.stratio.deep.examples.java.WritingEntityToAerospike</li>\n<li>com.stratio.deep.examples.java.GroupingEntityWithAerospike</li>\n</ul><p>Cells:</p>\n<ul><li>com.stratio.deep.examples.java.ReadingCellFromAerospike</li>\n<li>com.stratio.deep.examples.java.WritingCellToAerospike</li>\n<li>com.stratio.deep.examples.java.GroupingCellWithAerospike</li>\n</ul>\n<p>Support for JDBC has been added in version 0.7.0.</p>\n<p>Examples:</p>\n<p>Entities:</p>\n<ul><li>package com.stratio.deep.examples.java.ReadingEntityWithJdbc</li>\n<li>package com.stratio.deep.examples.java.WritingEntityWithJdbc</li>\n</ul><p>Cells:</p>\n<ul><li>package com.stratio.deep.examples.java.ReadingCellWithJdbc</li>\n<li>package com.stratio.deep.examples.java.WritingCellWithJdbc</li>\n</ul>\n<ul><li>Cassandra, we tested versions from 1.2.8 up to 2.0.11 (for Spark &lt;=&gt; Cassandra integration).</li>\n<li>MongoDB, we tested the integration with MongoDB versions 2.2, 2.4 y 2.6 using Standalone, Replica Set and Sharded Cluster (for Spark &lt;=&gt; MongoDB integration).</li>\n<li>ElasticSearch, 1.3.0+</li>\n<li>Aerospike, 3.3.0+</li>\n<li>Spark 1.1.1</li>\n<li>Apache Maven &gt;= 3.0.4</li>\n<li>Java 1.7</li>\n<li>Scala 2.10.3</li>\n</ul>\n<ul><li>\n<p>Clone the project</p>\n</li>\n<li>\n<p>To configure a development environment in Eclipse: import as Maven project. In IntelliJ: open the project by selecting the deep-parent POM file</p>\n</li>\n<li>\n<p>Install the project in you local maven repository. Enter deep-parent subproject and perform: mvn clean install (add -DskipTests to skip tests)</p>\n</li>\n<li>\n<p>Put Deep to work on a working cassandra + spark cluster. You have several options:</p>\n<ul><li>\n<p>Download a pre-configured Stratio platform VM <a href=\"http://www.stratio.com/\" rel=\"nofollow\">Stratio's BigData platform (SDS)</a>.\nThis VM will work on both Virtualbox and VMWare, and comes with a fully configured distribution that also includes Stratio Deep. We also distribute the VM with several preloaded datasets in Cassandra. This distribution will include Stratio's customized Cassandra distribution containing our powerful <a href=\"https://github.com/Stratio/stratio-cassandra\">open-source lucene-based secondary indexes</a>, see Stratio documentation for further information.\nOnce your VM is up and running you can test Deep using the shell. Enter /opt/sds and run bin/stratio-deep-shell.</p>\n</li>\n<li>\n<p>Install a new cluster using the Stratio installer. Please refer to Stratio's website to download the installer and its documentation.</p>\n</li>\n<li>\n<p>You already have a working Cassandra server on your development machine: you need a spark+deep bundle, we suggest to create one by running:</p>\n<p><code>cd deep-scripts</code></p>\n<p><code>./make-distribution-deep.sh</code></p>\n</li>\n</ul><p>this will build a Spark distribution package with StratioDeep and Cassandra's jars included (depending on your machine this script could take a while, since it will compile Spark from sources).\nThe package will be called <code>spark-deep-distribution-X.Y.Z.tgz</code>, untar it to a folder of your choice, enter that folder and issue a <code>./stratio-deep-shell</code>, this will start an interactive shell where you can test StratioDeep (you may have noticed this is will start a development cluster started with MASTER=\"local\").</p>\n<ul><li>\n<p>You already have a working installation os Cassandra and Spark on your development machine: this is the most difficult way to start testing Deep, but you know what you're doing you will have to</p>\n<ol><li>copy the Stratio Deep jars to Spark's 'jars' folder (<code>$SPARK_HOME/jars</code>).</li>\n<li>copy Cassandra's jars to Spark's 'jar' folder.</li>\n<li>copy Datastax Java Driver jar (v 2.0.x) to Spark's 'jar' folder.</li>\n<li>start spark shell and import the following:</li>\n</ol><p><code>import com.stratio.deep.commons.annotations._</code></p>\n<p><code>import com.stratio.deep.commons.config._</code></p>\n<p><code>import com.stratio.deep.commons.entity._</code></p>\n<p><code>import com.stratio.deep.core.context._</code></p>\n<p><code>import com.stratio.deep.cassandra.config._</code></p>\n<p><code>import com.stratio.deep.cassandra.extractor._</code></p>\n<p><code>import com.stratio.deep.mongodb.config._</code></p>\n<p><code>import com.stratio.deep.mongodb.extractor._</code></p>\n<p><code>import com.stratio.deep.es.config._</code></p>\n<p><code>import com.stratio.deep.es.extractor._</code></p>\n<p><code>import com.stratio.deep.aerospike.config._</code></p>\n<p><code>import com.stratio.deep.aerospike.extractor._</code></p>\n<p><code>import org.apache.spark.rdd._</code></p>\n<p><code>import org.apache.spark.SparkContext._</code></p>\n<p><code>import org.apache.spark.sql.api.java.JavaSQLContext</code></p>\n<p><code>import org.apache.spark.sql.api.java.JavaSchemaRDD</code></p>\n<p><code>import org.apache.spark.sql.api.java.Row</code></p>\n<p><code>import scala.collection.JavaConversions._</code></p>\n</li>\n</ul></li>\n</ul><p>Once you have a working development environment you can finally start testing Deep. This are the basic steps you will always have to perform in order to use Deep:</p>\n<ul><li><strong>Build an instance of a configuration object</strong>: this will let you tell Deep the Cassandra endpoint, the keyspace, the table you want to access and much more.\nIt will also let you specify which interface to use (the domain entity or the generic interface).\nWe have a factory that will help you create a configuration object using a fluent API. Creating a configuration object is an expensive operation.\nPlease take the time to read the java and scala examples provided in 'deep-examples' subproject and to read the comprehensive documentation at <a href=\"https://github.com/Stratio/deep-spark/blob/release/0.6/doc/t10-first-steps-deep-cassandra.md\">OpenStratio website</a>.</li>\n<li><strong>Create an RDD</strong>: using the DeepSparkContext helper methods and providing the configuration object you've just instantiated.</li>\n<li><strong>Perform some computation over this RDD(s)</strong>: this is up to you, we only help you fetching the data efficiently from Cassandra, you can use the powerful <a href=\"https://spark.apache.org/docs/1.1.1/api/java/index.html\" rel=\"nofollow\">Spark API</a>.</li>\n<li><strong>(optional) write the computation results out to Cassandra</strong>: we provide a way to efficiently save the result of your computation to Cassandra.\nIn order to do that you must have another configuration object where you specify the output keyspace/column family. We can create the output column family for you if needed.\nPlease, refer to the comprehensive Stratio Deep documentation at <a href=\"https://github.com/Stratio/deep-spark/blob/release/0.6/doc/about.md\">Stratio website</a>.</li>\n</ul>\n<ul><li><strong>Build an instance of a configuration object</strong>: this will let you tell Stratio Deep the MongoDB endpoint, the MongoDB database and collection you want to access and much more.\nIt will also let you specify which interface to use (the domain entity).\nWe have a factory that will help you create a configuration object using a fluent API. Creating a configuration object is an expensive operation.\nPlease take the time to read the java and scala examples provided in 'deep-examples' subproject and to read the comprehensive Deep documentation at <a href=\"https://github.com/Stratio/deep-spark/blob/release/0.6/doc/t20-first-steps-deep-mongodb.md\">OpenStratio website</a>.</li>\n<li><strong>Create an RDD</strong>: using the DeepSparkContext helper methods and providing the configuration object you've just instantiated.</li>\n<li><strong>Perform some computation over this RDD(s)</strong>: this is up to you, we only help you fetching the data efficiently from MongoDB, you can use the powerful <a href=\"https://spark.apache.org/docs/1.1.1/api/java/index.html\" rel=\"nofollow\">Spark API</a>.</li>\n<li><strong>(optional) write the computation results out to MongoDB</strong>: we provide a way to efficiently save the result of your computation to MongoDB.</li>\n</ul>\n<p>From version 0.4.x, Deep supports multiple datastores, in order to correctly implement this new feature Deep has undergone an huge refactor between versions 0.2.9 and 0.4.x. To port your code to the new version you should take into account a few changes we made.</p>\n<h2><a id=\"user-content-new-project-structure\" class=\"anchor\" aria-hidden=\"true\" href=\"#new-project-structure\"></a>New Project Structure</h2>\n<p>From version 0.4.x, Deep supports multiple datastores, in your project you should import only the maven dependency you will use: deep-cassandra, deep-mongodb, deep-elasticsearch or deep-aerospike.</p>\n<h2><a id=\"user-content-changes-to-comstratiodeepentitycells\" class=\"anchor\" aria-hidden=\"true\" href=\"#changes-to-comstratiodeepentitycells\"></a>Changes to 'com.stratio.deep.entity.Cells'</h2>\n<ul><li>Until version 0.4.x the 'Cells' was implicitly associated to a record coming from a specific table. When performing a join in Spark, 'Cell' objects coming from different tables are mixed into an single 'Cells' object.\nDeep now keeps track of the original table a Cell object comes from, changing the internal structure of 'Cells', where each 'Cell' is associated to its 'table'.\n<ol><li>If you are a user of 'Cells' objects returned from Deep, nothing changes for you. The 'Cells' API keeps working as usual.</li>\n<li>If you manually create 'Cells' objects you can keep using the original API, in this case each Cell you add to your Cells object is automatically associated to a default table name.</li>\n<li>You can specify the default table name, or let Deep chose an internal default table name for you.</li>\n<li>We added a new constructor to 'Cells' accepting the default table name. This way the 'old' API will always manipulate 'Cell' objects associated to the specified default table.</li>\n<li>For each method manipulating the content of a 'Cells' object, we added a new method that also accepts the table name: if you call the method\t whose signature does <em>not</em> have the table name, the table action is performed over the Cell associated to the default table, otherwise the action is performed over the 'Cell'(s) associated to the specified table.</li>\n<li>size() y isEmpty() will compute their results taking into account all the 'Cell' objects contained.</li>\n<li>size(String tableName) and isEmpty(tableName) compute their result taking into account only the 'Cell' objects associated to the specified table.</li>\n<li>Obviously, when dealing with Cells objects, Deep always associates a Cell to the correct table name.</li>\n</ol></li>\n</ul><p>Examples:</p>\n<pre>Cells cells1 = new Cells(); // instantiate a Cells object whose default table name is generated internally.\nCells cells2 = new Cells(\"my_default_table\"); // creates a new Cells object whose default table name is specified by the user\ncells2.add(new Cell(...)); // adds to the 'cells2' object a new Cell object associated to the default table\ncells2.add(\"my_other_table\", new Cell(...)); // adds to the 'cells2' object a new Cell associated to \"my_other_table\"  \n</pre>\n<h2><a id=\"user-content-changes-to-objects-hierarchy\" class=\"anchor\" aria-hidden=\"true\" href=\"#changes-to-objects-hierarchy\"></a>Changes to objects hierarchy</h2>\n<ul><li>IDeepJobConfig interface has been splitted into ICassandraDeepJobConfig and IMongoDeepJobConfig sub-interfaces. Each sub-interface exposes only the configuration properties that make sense for each data base.\ncom.stratio.deep.config.DeepJobConfigFactory's factory methods now return the proper subinterface.</li>\n<li><strong>DeepSparkContext</strong> has been splitted into <strong>CassandraDeepSparkContext</strong> and <strong>MongoDeepSparkContext</strong>.</li>\n<li><strong>DeepJobConfigFactory</strong> has been renamed to <strong>ConfigFactory</strong> (to reduce verbosity).</li>\n</ul><h2><a id=\"user-content-rdd-creation\" class=\"anchor\" aria-hidden=\"true\" href=\"#rdd-creation\"></a>RDD creation</h2>\n<p>Methods used to create Cell and Entity RDD has been merged into one single method:</p>\n<ul><li><strong>DeepSparkContext</strong>: createRDD(...)</li>\n</ul></article>",
        "created_at": "2018-07-24T19:51:49+0000",
        "updated_at": "2018-07-24T19:51:49+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 11,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/5228027?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11146"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1052,
            "label": "tools",
            "slug": "tools"
          },
          {
            "id": 1135,
            "label": "packages",
            "slug": "packages"
          }
        ],
        "is_public": true,
        "id": 11145,
        "uid": "5b57834a677951.14844004",
        "title": "Cassandra Parameters for Dummies",
        "url": "https://www.ecyrd.com/cassandracalculator/",
        "content": "<p>This simple form allows you to try out different values for your <a href=\"http://cassandra.apache.org\">Apache Cassandra</a> cluster\nand see what the impact is for your application.</p><div><p><label for=\"N\">Cluster size</label>\n  </p>\n<p><label for=\"RF\">Replication Factor</label>\n  </p>\n<p><label for=\"W\">Write Level</label>\n  </p>\n<p><label for=\"R\">Read Level</label>\n  </p>\n<hr /><div class=\"calculated\">Your reads are<p>\"Consistent\" means that for this particular Read/Write level combo, all nodes will \"see\" the same data.  \"Eventually consistent\" means\n        that you might get old data from some nodes and new data for others until the data has been replicated across all devices.  The idea is that this way you can\n        increase read/write speeds and improve tolerance against dead nodes.</p></div>\n<div class=\"calculated\">You can survive the loss of  without impacting the application.<p>How many nodes can go down without application noticing? This is a lower bound - in large clusters, you could lose more nodes and if they happen to be handling different parts of the keyspace, then you wouldn't notice either.</p></div>\n<div class=\"calculated\">You can survive the loss of  without data loss.<p>How many nodes can go down without physically losing data? This is a lower bound - in large clusters, you could lose more nodes and if they happen to be handling different parts of the keyspace, then you wouldn't notice either.</p></div>\n<div class=\"calculated\">You are really reading from  every time.<p>The more nodes you read from, more network traffic ensues, and the bigger the latencies involved.  Cassandra read operation won't return until at least this many nodes have responded with some data value.</p></div>\n<div class=\"calculated\">You are really writing to  every time.<p>The more nodes you write to, more network traffic ensues, and the bigger the latencies involved. Cassandra write operation won't return until at least this many nodes have acknowledged receiving the data.</p></div>\n<div class=\"calculated\">Each node holds  of your data.<p>The bigger your cluster is, the more the data gets distributed across your nodes.  If you are using the RandomPartitioner, or are very\n   good at distributing your keys when you use OrderedPartitioner, this is how much data each of your nodes has to handle.  This is also how much\n   of your keyspace becomes inaccessible for each node that you lose beyond the safe limit, above.</p></div></div>",
        "created_at": "2018-07-24T19:51:38+0000",
        "updated_at": "2018-07-24T19:51:38+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 1,
        "domain_name": "www.ecyrd.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11145"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1052,
            "label": "tools",
            "slug": "tools"
          },
          {
            "id": 1135,
            "label": "packages",
            "slug": "packages"
          }
        ],
        "is_public": true,
        "id": 11144,
        "uid": "5b57833d0756b2.13851326",
        "title": "Reaper: Easy Repair Management for Apache Cassandra",
        "url": "http://cassandra-reaper.io/",
        "content": "Reaper: Easy Repair Management for Apache Cassandra\n    <div id=\"all\"><header>\n        </header><section><div class=\"home-carousel\"><div class=\"container\"><div class=\"homepage owl-carousel\"><div class=\"item\"><div class=\"row\"><div class=\"col-sm-5 right\"><ul class=\"list-style-none\"><li>Simple web based UI</li>\n  <li>Full and incremental support</li>\n  <li>Supports all Cassandra versions</li>\n</ul></div><div class=\"col-sm-7\"><img class=\"img-responsive\" src=\"http://cassandra-reaper.io/img/carousel/repairs.png\" alt=\"\" /></div></div></div><div class=\"item\"><div class=\"row\"><div class=\"col-sm-5 right\"><ul class=\"list-style-none\"><li>Set and forget</li>\n  <li>No crontab required</li>\n  <li>Automatically scales with your cluster</li>\n</ul></div><div class=\"col-sm-7\"><img class=\"img-responsive\" src=\"http://cassandra-reaper.io/img/carousel/schedules.png\" alt=\"\" /></div></div></div><div class=\"item\"><div class=\"row\"><div class=\"col-sm-5 right\"><ul class=\"list-style-none\"><li>Healthy nodes are green</li>\n  <li>Downed nodes are red</li>\n</ul></div><div class=\"col-sm-7\"><img class=\"img-responsive\" src=\"http://cassandra-reaper.io/img/carousel/cluster-view.jpg\" alt=\"\" /></div></div></div></div></div></div></section><section class=\"bar background-white\"><div class=\"container\"><div class=\"col-md-12\"><div class=\"row\"><div class=\"col-md-4\"><div class=\"box-simple\"><h3>Easy to use web interface</h3><p>Point and click repair administration.  Set up a weekly repair schedule in minutes.</p></div></div><div class=\"col-md-4\"><div class=\"box-simple\"><h3>Manage multiple clusters</h3><p>Centralized repair for hundreds of clusters.</p></div></div><div class=\"col-md-4\"><div class=\"box-simple\"><h3>Open source, always free</h3><p>Apache Licensed.  Based on the original Spotify reaper codebase.  Adopted with love by <a href=\"http://thelastpickle.com/\">The Last Pickle.</a></p></div></div></div></div></div></section><div id=\"copyright\"><div class=\"container\"><div class=\"col-md-12\"><p class=\"pull-right\">\n              Template by <a href=\"http://bootstrapious.com/free-templates\">Bootstrapious</a>.\n              \n              Ported to Hugo by <a href=\"https://github.com/devcows/hugo-universal-theme\">DevCows</a>\n            </p></div></div></div></div>",
        "created_at": "2018-07-24T19:51:25+0000",
        "updated_at": "2018-07-24T19:51:25+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 0,
        "domain_name": "cassandra-reaper.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11144"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1135,
            "label": "packages",
            "slug": "packages"
          },
          {
            "id": 1163,
            "label": "libraries",
            "slug": "libraries"
          }
        ],
        "is_public": true,
        "id": 11143,
        "uid": "5b578335734169.53530064",
        "title": "Caffinitas",
        "url": "http://caffinitas.org/mapper/",
        "content": null,
        "created_at": "2018-07-24T19:51:17+0000",
        "updated_at": "2018-07-24T19:51:17+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": null,
        "language": null,
        "reading_time": 0,
        "domain_name": "caffinitas.org",
        "preview_picture": null,
        "http_status": null,
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11143"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1161,
            "label": "general",
            "slug": "general"
          },
          {
            "id": 1162,
            "label": "relational -> cassandra",
            "slug": "relational-cassandra"
          }
        ],
        "is_public": true,
        "id": 11142,
        "uid": "5b5783188dc203.28625366",
        "title": "MySQL to Cassandra Migrations",
        "url": "https://academy.datastax.com/planet-cassandra/mysql-to-cassandra-migration",
        "content": "<p>For 15+ years, Oracle’s MySQL has been a de facto infrastructure piece in web applications, enjoying wide adoption. This is for good reason: MySQL provides a solid relational database that enables companies to build systems that perform well in many use cases. Yet, even its strongest supporters admit that it is not architected to tackle the new wave of big data applications. Modern businesses that need to manage big data use cases are turning to Apache Cassandra to replace MySQL.</p><p>Migrating from MySQL to Cassandra: General Info</p><h3>Is Cassandra Right for Your Application?</h3><p>A new class of databases (sometimes referred to as “NoSQL”) have been developed and designed with 18+ years worth of lessons learned from traditional relational databases such as MySQL. Cassandra (and other distributed or “NoSQL” databases) aim to make the “right” tradeoffs to<br />ultimately deliver a database that provides the scalability, redundancy, and performance needed in todays applications. Although MySQL may have performed well for you in the past, new business requirements and/or the need to both scale and improve the reliability of your application might mean that MySQL is no longer the correct fit.</p><p>Before committing any further time towards a MySQL to Cassandra migration, ask yourself:<br />“Is MySQL currently preventing development of new features or providing acceptable uptime, reliability, and scalability for my users?”</p><p>“No”: Not only should you not migrate to Cassandra, but also you most likely should not be considering a migration to any alternative database. Migrating an application to a new database is a very difficult, time consuming, and error-prone process.</p><p>“Yes”: Then hopefully you’ve found a helpful resource to help guide and plan your migration from MySQL to Cassandra. There are many databases<br />available, all with their various advantages, disadvantages and tradeoffs. This article is not an attempt to portray Cassandra as a perfect solution; in fact, Cassandra’s tradeoffs, advantages, and disadvantages will be highlighted. Hopefully this will help you make a decision that is both informed and educated; not one motivated by marketing hype or change for the sake of change.</p><p>Don’t try to shove a square peg in a round hole!</p><ul><li>Cassandra is not a relational database.</li>\n<li>Cassandra is not a 100%/“drop-in” replacement for MySQL.</li>\n<li>Simply migrating existing code to Cassandra without modifying and rethinking your existing data model will not result in perfect uptime or fix performance bottlenecks for your application. In fact, it might make things worse.</li>\n</ul><h3>Key Terminology</h3><p>The following overview of Cassandra terminology provides descriptions and their MySQL equivalent. The goal is to introduce the most basic terms and concepts required to get a basic understanding of Cassandra. To read more on the key terms and architecture of Cassandra you can find more detail in the <a href=\"http://www.datastax.com/documentation/cassandra/2.0/cassandra/architecture/architectureIntro_c.html\">Cassandra architecture documentation</a> or for a higher level overview visit the “<a href=\"http://planetcassandra.org/what-is-apache-cassandra/\">What is Cassandra</a>” page on Planet Cassandra.</p><p><img alt=\"\" src=\"https://academy.datastax.com/sites/default/files/keyterms-1.png\" /></p><p><img alt=\"\" src=\"https://academy.datastax.com/sites/default/files/keyterms-2.png\" /></p><p><img alt=\"\" src=\"https://academy.datastax.com/sites/default/files/keyterms-3.png\" /></p><h3>How is Data Handled?</h3><p>At a very high level, Cassandra operates by dividing all data evenly around a cluster of nodes, which can be visualized as aring. Nodes generally run on commodity hardware. Each Cassandra node in the cluster is responsible for and assigned a token range (which is essentially a range of hashes defined by a partitioner, which defaults to Murmur3Partitioner in Cassandra v1.2+). By default this hash range is defined with a maximum number of possible hash values ranging from 0 to 2^127-1.</p><p>Each update or addition of data contains a unique row key (also known as a primary key). The primary key is hashed to determine a replica (or node) responsible for a token range inclusive of a given row key. The data is then stored in the cluster<strong>n</strong> times (where <strong>n</strong> is defined by the keyspace’s replication factor), or once on each replica responsible a given query’s row key. All nodes in Cassandra are peers and a client’s read or write request can be sent to any node in the cluster, regardless of whether or not that node actually contains and is responsible for the requested data. There is no concept of a master or slave, and nodes dynamically learn about each other and the state and health of other nodes thru the gossip protocol. A node that receives a client query is referred to as the coordinator for the client operation; it facilitates communication between all replica nodes responsible for the query (contacting at least n replica nodes to satisfy the query’s consistency level) and prepares and returns a result to the client.</p><h3>Reads and Writes</h3><p>Clients may interface with Cassandra for reads and writes via either the native binary protocol or Thrift. CQL queries can be made over both transports. As a general recommendation, if you are just getting started with Cassandra you should stick to the native binary protocol and CQL and ignore Thrift.</p><p>When a client performs a read or write request, the coordinator node contacts the number of required replicas to satisfy the consistency level included with each request. For example, if a read request is processed using QUORUM consistency, and the keyspace was created with a “replication factor” of 3, 2 of the 3 replicas for the requested data would be contacted, their results merged, and a single result returned to the client. With write requests, the coordinator node will send a write requests with all mutated columns to all replica nodes for a given row key.</p><h3>Processing a Local Update</h3><p>When an update is processed – also known as a mutation — an entry is first added to the commit log, which ensures durability of the transaction. Next, it is also added to the memtable. A memtable is a bounded in memory write-back cache that contains recent writes which have not yet been flushed to an SSTable (a permanent, immutable, and serialized on disk copy of the tables data).</p><p>When updates cause a memtable to reach it’s configured maximum in-memory size, the memtable is flushed to an immutable SSTable, persisting the data from the memtable permanently on disk while making room for future updates. In the event of a crash or node failure, events are replayed from the commit log, which prevents the loss of any data from memtables that had not been flushed to disk prior to an unexpected event such as a power outage or crash.</p><h3>Distributed Computing</h3><p>Distributed logic and designs will inevitably cause an increase in complexity in application logic. When done right however, the rewards are obvious and easy to appreciate. Operationally, while it might be possible to get away with a single non-sharded MySQL instance installed via apt-get/emerge/yum/etc., operations with Cassandra need to be taken seriously to achieve desired performance and uptime of the cluster. Or, if you currently shard data across multiple MySQL instances, knowing that Cassandra deals with sharding and replication for you might be a huge benefit and upsell for Cassandra. But, unfortunately there is no such thing as a free lunch. For example, although Cassandra will remove all of your homegrown database abstraction and sharding code, you ultimately ended simply moving that logic from your code to Cassandra. Luckily, given the number of people and corporations of all sizes using Cassandra in production combined with an engaged and involved community, it’s fair to assume and argue that Cassandra’s equivalent of your MySQL sharding code will be better than your old homegrown solution.</p><h2>Development Considerations</h2><h3>Be Thoughtful About Your Data Model</h3><p>Creating a thoughtful and conscious data model in Cassandra from the very beginning is very important. A bad data model can easily ruin and erase any of the benefits you want by migrating to Cassandra in the first place. With MySQL, the lack of a thoughtful or poor data model can frequently be worked around and accommodated thanks to the various relational database features (for example, the use of complex JOINS).<br />While these MySQL queries might be slow and expensive, given enough time and resources it’s possible to get the exact desired result from the dataset. With Cassandra, it is much harder to retroactively “fix” a poor data model. First, the lack of JOINS in Cassandra removes complex reads as a hacked solution to a bad data model. Additionally, thanks to the power and architecture of Cassandra, it becomes very easy to store more rows and data than imaginable with MySQL. With increased amounts of data stored, comes an increased complexity in successfully getting the exact data needed within the given performance boundaries required by your application. A SELECT query containing only 30 rows will return quickly and predictably. Performing a query over 5 million rows requires processing significantly more IO. Just as more data in MySQL made complex JOINS more difficult, accommodating for a Cassandra data model that requires the iteration over multiple nodes and rows will be slow, inefficient, and most likely not work at all. Obviously, faster database responses are always better in any application; so don’t let your data model be the cause of slow database latency in your application!</p><h3>Denormalization</h3><p>Denormalization is the concept that a data model should be designed so that a given query can be served from the results from one row and query. Instead of doing multiple reads from multiple tables and rows to gather all the required data for a response, instead modify your application logic to insert the required data multiple times into every row that might need it in the future. This way, all required data can be available in just one read which prevents multiple lookups.</p><h2>Operational Considerations</h2><h3>Optimization and Tuning Cassandra</h3><p>There are lots of options to tweak in Cassandra. Much like turning the treble, bass, and volume nobs of your car’s sound system all to 11 won’t sound very good to your ears, it’s easy to do more harm than good when “optimizing” Cassandra and it’s many nobs and dials.</p><p>Options such as key cache and row cache are two great examples. In a MySQL world, much of the configuration tuning is spent on optimizing the various amounts of cache allocated. In the Cassandra world, these settings actually tend to decrease node and cluster stability. Cassandra is written in Java, and thus it must operate within the limitations of Java. One of the biggest considerations is Garbage Collection and the maximum size of the heap possible without running into large garbage collection related issues, which will crater the performance of Cassandra. As of JDK7 with CMS (the default in Cassandra 1.2.x and 2.0.x) the maximum recommended size of the heap is 8GB. This 8GB must be shared between all of the various Cassandra components. 2GB allocated to the key cache will (obviously) put another 2GB of pressure on the heap. Caches are an optimization not a requirement, so allocating more memory to caches should be considered as part of the big picture. If you can allocate the full 8GB to Cassandra, a suggestion would be to start with allocating no more than 768MB to the key cache (key_cache_size_in_mb) and 0MB to the row cache (row_cache_size_in_mb).</p><p>Another example is multithreaded_compaction. While this might seem like an obvious option to enable, in most cases leaving this option disabled can actually improve overall cluster stability and performance. In many cases, less is more.</p><h2>Migration Plan Considerations</h2><h3>Maintaining Data Integrity</h3><p>Sometimes the most difficult component of a migration is not in writing a set of reliable scripts to read from MySQL and insert into Cassandra, but trivial coding mistakes that can cause significant data discrepancies between the MySQL and Cassandra versions of the data.</p><p>Because migrating from MySQL to Cassandra will most likely require a change in your data model, the logic required to “convert” your relational MySQL data to it’s de-normalized form is the hardest part of the migration and certainly has the biggest risk.</p><p>Treat your migration scripts and logic not as one-off instances, but production quality code that can be run in any order, at any time. Mistakes in migration logic that result in an inconsistent version of the migrated data in Cassandra most likely will have a much greater impact than other dataset migration related bugs.</p><h3>Get to Know Bulk Loading</h3><p>Regardless of your migration strategy, in almost all cases you will have to perform an initial bulk import of your existing MySQL data into Cassandra. While it might be tempting to simply iterate over every MySQL result and then insert that result one mutation at a time into Cassandra, a more efficient way is to use the Cassandra Bulk Loader. At a high level, the Bulk Loader requires you to create a CSV file containing all of the rows and columns that need to be loaded into Cassandra. Using the Java class SSTableSimpleUnsortedWriter, you can create an SSTable from your CSV file, which can then be loaded directly into Cassandra using SSTableloader.</p><p>For more details and code samples reference the article at <a href=\"http://www.datastax.com/dev/blog/bulk-loading\">http://www.datastax.com/dev/blog/bulk-loading</a></p><h3>Migration Methods</h3><p>Sync Data Method:<br />When migrating to Cassandra and choosing a new data model might significantly increase your database workload. Alternatively, you might still need a live dataset in MySQL after the initial migration for legacy scripts that have not yet been migrated to use Cassandra.</p><p>Syncing from MySQL to Cassandra<br />In some cases it might not be practicable to add Cassandra to a legacy application. In this case it might be necessary to have an external process sync data from MySQL to Cassandra while running both new and old logic in parallel.</p><p>Suggestion:<br />Add a timestamp column to the MySQL table to be synced. With each update to MySQL also update the timestamp with the last updated time. At a scheduled interval then do a SELECT query from all MySQL shards where the last updated timestamp is greater than or equal to the time your last sync started.</p><p>Syncing from Cassandra back to MySQL<br />Some data models will be hard to sync from Cassandra back to MySQL (for example time series data). However, rows containing more de-normalized<br />“metadata”-like information can be synced.</p><p>What won’t work: Creating a sync script that executes via cron every n minutes and attempts to do a SELECT * FROM TABLE from Cassandra (and<br />then update and insert all of those records into MySQL) is a recipe for failure. Inherent to Cassandra’s design is that data is sharded across multiple nodes by a hash of it’s key. Performing a SELECT * query is a Cassandra anti-pattern and should be avoided. Iterating through every key across all nodes and returning a single paged dataset is both inefficient and impractical.</p><p>1st Suggestion:<br />Implement a queue that your application additionally writes to when it modifies a row in Cassandra. Have a script consume from this queue and de-duplicate the modified keys on a time interval and then bulk insert updates into MySQL.</p><p>2nd Suggestion:<br />If the data can be updated less frequently into MySQL, you could write a Hadoop Map/Reduce job that iterates over the column families that you need to sync. This solution gives a practicable and reproducible way to iterate through all keys in a column family. Using this approach as an additional sanity option to resolve missed updates from other incremental sync options.</p><p>3rd Suggestion:<br />Another option if you can afford a greater delay in the delta between updates from Cassandra back to MySQL is to use a tool such as SSTable2JSON to dump a column families SSTables into a JSON format, which can then be parsed and then used to update MySQL. This is a pretty heavy-handed method. Additionally, you’ll have to write logic to ensure you dump the SSTables from all nodes to get the entire column family.</p><p>Write Twice and Forget Method:<br />If you are able to modify your existing application to also interface with Cassandra, you can initially start your migration by writing database updates twice, once to MySQL and an additional time to Cassandra. Once you have all new updates being written to both MySQL and Cassandra, you can run a migration script that pages through all your existing MySQL data and inserts those records into Cassandra.</p><p>Initially, you might want to implement this second write to Cassandra as a completely non-blocking, write and forget, operation. If you experience initial issues during your Cassandra deployment, make sure not to impact your existing application when Cassandra is down.</p><p>Once you are satisfied with the fire-and-forget writes, you can slowly modify your application logic to start performing reads from Cassandra instead of MySQL. Thanks to the dual writes, if you run into issues, simply revert back to doing reads from MySQL.</p><h2>Use Cases and Migration Resources</h2><h3>Use Cases</h3><p><a href=\"http://planetcassandra.org/blog/post/youve-got-scale-aol-migrates-from-mysql-to-apache-cassandra-for-8x-improvement/\">AOL</a><br />AOL migrated their article index, in use for several AOL technologies form MySQL. The result was an 8X increase in writes, and considering the move to Cassandra as a “big win”.</p><p><a href=\"http://planetcassandra.org/blog/interview/coursera-migrates-to-the-top-of-the-class-moves-to-cassandra-for-an-always-on-on-demand-classroom/\">Coursera</a></p><p>Coursera was experiencing unexpected downtime, due to the RDBMS’ single point of failure.  In addition, Cassandra has enabled Coursera to become more dynamic; introducing their over 9 million users to an always available, on-demand course system.</p><p><a href=\"http://www.datastax.com/wp-content/uploads/2011/06/DataStax-CaseStudy-Mahalo.pdf\">Mahalo</a><br />Mahalo’s search technology was forced to move off of MySQL to Cassandra as their primary data store in order to realize lower costs and higher performance and scalability.</p><p><a href=\"http://planetcassandra.org/blog/post/scaling-in-the-cloud-with-cassandra-at-pantheon\">Pantheon Systems</a><br />Pantheon Systems, offering a platform for Drupal websites in the cloud, migrated to Cassandra primarily for greater scalability and ease of use.</p><p><a href=\"http://planetcassandra.org/blog/post/scoopit-turns-to-apache-cassandra-the-latest-and-best-technology-when-mysql-fails-to-keep-up\">Scoop.it</a><br />Scoop.it’s content curation publishing platform experienced the limitations of MySQL for handling their data growth and moved to Apache Cassandra for scalability and requirement of no downtime.</p><p><a href=\"http://planetcassandra.org/blog/post/ampushs-migration-from-mysql-to-cassandra-for-data-volume-high-availability-and-performance\">Ampush</a><br />Ampush’s migration from MySQL to Cassandra due to their increase in data volume, high availability and performance requirements which only Cassandra could satisfy.</p><p><a href=\"http://planetcassandra.org/blog/post/barracuda-networks-and-cassandra---battling-the-zombies\">Barracuda Networks</a><br />Barracudna Networks were not able to monitor customer threats in real-time with MySQL and went to Cassandra for the scalability and availability benefits.</p><p><a href=\"http://planetcassandra.org/blog/post/cassandra-summit-2013-cabs-cassandra-and-hailo-mysql-to-cassandra-by-dave-gardner\">Hailo</a><br />Hailo has leveraged Cassandra to build one of the most successful startups in European history. This presentation looks at how Hailo grew from a simple MySQL-backed infrastructure to a resilient Cassandra-backed system running in three data centers globally.</p><p><a href=\"http://www.datastax.com/wp-content/uploads/2011/04/DataStax-CS-Ooyala.pdf\">Ooyala</a><br />Ooyala chose Apache Cassandra for its elastic scalability and high performance – especially when their MySQL environment was not meeting customer service levels – to help their customers take a more strategic approach when delivering a digital video experience.</p><p><a href=\"http://planetcassandra.org/blog/post/appssavvy-fixes-mysql-scalability-by-switching-to-apache-cassandra\">AppsSavvy</a><br />AppsSavvy’s targeted advertising delivery solution moved from MySQL to Cassandra for increased scalability and performance under load.</p><p><a href=\"http://planetcassandra.org/blog/post/dating-site-zoosk-breaks-up-with-mysql-migrates-to-apache-cassandra-for-persistent-notifications\">Zoosk</a><br />Zoosk’s persistent notification system was moved off of MySQL and onto Apache Cassandra because it is a superior database for their high volume of writes of time series data.</p><p><a href=\"http://planetcassandra.org/blog/post/agentis-energy-stores-over-15-billion-records-of-time-series-usage-data-in-apache-cassandra\">Agentis</a><br />Agentis Energy had to move to Cassandra once the scale of their data became unmanageable on MySQL as they now store over 15 billion records of time series usage energy usage data.</p><h3>Migration Resources</h3><p>Whitepaper: <a href=\"http://www.datastax.com/wp-content/uploads/2012/08/WP-DataStax-MySQLtoCassandra.pdf\">Why Migrate From MySQL to Cassandra?</a> By Robin Schumacher<br />This whitepaper discusses the ‘why’ and ‘how’ to migrate from MySQL to Cassandra as well as what a good migration candidate looks like.</p><p>Hindsight is 20/20: <a href=\"http://www.youtube.com/watch?v=gW4jEOKRB04\" target=\"_blank\">MySQL to Cassandra</a>. This webinar offers a brief intro to how Barracuda Networks uses Cassandra and the ways in which they are replacing their MySQL infrastructure, with Cassandra including lessons learned. A slideshare from this presentation is available as well: <a href=\"http://www.slideshare.net/planetcassandra/c-summit-2013-hindsight-is-2020-mysql-to-cassandra-by-michael-kjellman\">Hindsight is 20/20: MySQL to Cassandra</a></p><p>5 lessons learned by Zoosk for <a href=\"https://about.zoosk.com/en/engineering-blog/moving-persistent-notifications-from-mysql-to-cassandra/\" target=\"_blank\">moving persistent notifications from MySQL to Apache Cassandra</a> in order to support very high volumes of write while minimizing write latency.</p><h3>About the Author</h3><p>Michael Kjellman is a San Francisco based Software Engineer. Michael works across multiple products, technologies, and languages. He primarily works on Barracuda’s spam infrastructure and web filter classification data. Follow him on Twitter at<a href=\"https://twitter.com/mkjellman\">@mkjellman</a>.</p>",
        "created_at": "2018-07-24T19:50:48+0000",
        "updated_at": "2018-07-24T19:50:48+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 16,
        "domain_name": "academy.datastax.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11142"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1161,
            "label": "general",
            "slug": "general"
          },
          {
            "id": 1162,
            "label": "relational -> cassandra",
            "slug": "relational-cassandra"
          }
        ],
        "is_public": true,
        "id": 11141,
        "uid": "5b578315187192.66551135",
        "title": "Relational Databases vs. NoSQL",
        "url": "https://www.datastax.com/relational-database-to-nosql",
        "content": "<noscript>\n\n\n\n<div class=\"DS17\"><div class=\"connect-us\"><a href=\"https://www.datastax.com/contactus\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Mail.svg\" alt=\"email icon\" />email</a><a href=\"https://www.datastax.com/company#offices\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Phone.svg\" alt=\"phone icon\" />call</a></div></div><header class=\"DS17\"><div class=\"container\"><div class=\"wrapper\"><div class=\"logo\"><a href=\"https://www.datastax.com/\"><img src=\"https://www.datastax.com/templates/dist/images/logo-header.png\" alt=\"DataStax logo\" /></a><a href=\"https://www.datastax.com/\"><img src=\"https://www.datastax.com/templates/dist/images/new_logo.png\" alt=\"DataStax logo\" /></a></div></div></div>\n  \n</header><div id=\"DXDIV0\"><div id=\"MainBody\"><div id=\"MainChannel\" class=\"width-100 clearfix\"><div id=\"Content\" class=\"\"><div id=\"ContentChannel\"><div class=\"dx_stlfntmobmod_pge\" id=\"dx_thispage_div1\"><div class=\"thismb2content1width1\"><div class=\"dx_hnltstd_lt_div dx_marginvertical_approx68pxless\"><div class=\"dxfnts_ds_f20l30\">Technology that can scale, perform and deliver continuous availability is the difference between today’s successful online applications and those that fail. Relational databases (RDBMS) have struggled to keep up with the wave of modernization, leading to the rise of NoSQL as the most viable database option for online Web and mobile applications.<p>The path to understanding whether a NoSQL technology like DataStax Enterprise is right for your business as either a complementary technology to an RDBMS or as a complete replacement is a three step approach.&#13;\n            </p></div></div></div></div></div></div><div class=\"DXpgA2AclassV1\"><p>SHARE THIS PAGE</p></div></div></div></div> \n\t \n              \n  <div class=\"DS17\"><div class=\"use-case\"><div class=\"wrapper\"><div class=\"two-col text-light-blue\"><h6>Customer Experience</h6><ul><li><a href=\"https://www.datastax.com/use-cases/customer-360\">Customer 360</a></li>\n          <li><a href=\"https://www.datastax.com/personalization\">Personalization &amp; Recommendations</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/loyalty-programs\">Loyalty Programs</a></li>\n          <li><a href=\"https://www.datastax.com/fraud-detection\">Consumer Fraud Detection</a></li>\n        </ul></div><div class=\"two-col text-light-green\"><h6><a href=\"#\">Enterprise Optimization</a></h6><ul><li><a href=\"https://www.datastax.com/use-cases/ecommerce\">eCommerce</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/identity-management\">Identity Management</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/security\">Security and Compliance</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/supply-chain\">Supply Chain</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/inventory-management\">Inventory Management</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/asset-monitoring\">Asset Monitoring</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/logistics\">Logistics</a></li>\n        </ul></div></div></div></div>\n  \n    \n\t\n\t\n\t\n\t\n\t\n\t\n</noscript>",
        "created_at": "2018-07-24T19:50:45+0000",
        "updated_at": "2018-07-24T19:50:45+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 0,
        "domain_name": "www.datastax.com",
        "preview_picture": "https://www.datastax.com/wp-content/themes/datastax-2014-08/images/common/DataStax_Web_Social_DefaultGenericV2_1024x351_wide.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11141"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1233,
            "label": "data.modeling",
            "slug": "data-modeling"
          }
        ],
        "is_public": false,
        "id": 11070,
        "uid": null,
        "title": "Data Modelling Recommended Practices - Instaclustr",
        "url": "https://www.instaclustr.com/support/documentation/cluster-management/data-modelling-recommended-practices/",
        "content": "<strong>Item</strong> <strong>Rational</strong> <strong><em>Schema</em></strong>  Is it properly denormalised? Does it require multiple queries to fetch information, or could the table just include info from the other table? Is there potential to consolidate data from multiple tables? Developers from relational background may tend to normalised models resulting in inefficient use of Cassandra. Partition key cardinality allows high number of partitions (minimum 100,000 possible preferred) A low number of partitions will lead to inefficient read and writes and increase risk of unevenly sized partitions Partition key prevents substantial skewing of partitions? If it is possible for a small number of partitions to have vastly higher numbers of rows than average (say 100x) then this can cause significantly uneven performance and disk usage. Using collections (maps,list,set)? Number of elements is 64k, keep the total size of the collect small (&lt;1MB) as the map is not paged. Very large collections can negatively  impact read/write performance. Is  gc_grace_seconds changed from default (864000)? If so, is that appropriate and impact considered? Lowering gc_crace_seconds results in space being reclaimed more quickly after deletes but runs small risk of “resurrected deletes” given we only run repairs weekly. Is caching set to KEYS_ONLY or NONE? Row caching for Cassandra 2.0 is often not effective. 2.1 row caching features may be effective if tuned correctly (see <a href=\"http://www.datastax.com/dev/blog/row-caching-in-cassandra-2-1\">row-caching-in-cassandra-2-1</a> and <a href=\"http://docs.datastax.com/en/cassandra/2.1/cassandra/operations/ops_monitoring_cache_c.html\">Cassandra Docs</a>) Is chosen compaction strategy appropriate?  <ul><li>SizeTieredCompactionStrategy: default and suitable as a starting point for most uses cases with balance of reads and writes</li> <li>LevelledCompactionStrategy: does more compaction work to improve read performance. Generally used if high ratio of reads to writes.</li> <li>DateTieredCompactionStrategy: useful for data where data is “hot” when first written but sees less access over time.</li> <li>Check that the compaction strategy is appropriately tuned (see <a href=\"http://docs.datastax.com/en/cassandra/2.1/cassandra/operations/ops_configure_compaction_t.html\">Cassandra Docs</a>) defaults are usually ok, but DTCS requires specific compaction options set to be effective.</li> </ul> Are counters used? Instaclustr only supports the use of counters with Cassandra 2.1 as Cassandra 2.0 counters are unreliable in many circumstances.   <strong><em>Secondary Indexes</em></strong>  Is cardinality of secondary index low? Cardinality of index should be at least an order of magnitude lower and preferable at least 100x lower than indexed table. <p>Also secondary indexes on boolean columns are not effective.</p> <p>See <a href=\"http://docs.datastax.com/en/cql/3.1/cql/ddl/ddl_when_use_index_c.html\" target=\"_blank\" rel=\"noopener noreferrer\">Cassandra Docs</a> and</p> <p><a href=\"http://www.wentnet.com/blog/?p=77\">http://www.wentnet.com/blog/?p=77</a></p> Is the indexed column frequently updated/deleted? Overhead of maintaining index will be incurred on each update/delete and may also result in excessive tombstones in the index table.   <strong><em>Queries</em></strong>  Are there logged batches used? If so, are they relatively small (&lt;100) Logged batches require coordinate node to control all operations and can result in very high load on coordinator node for large batches. Logged batches are only required for atomic operations across multiple rows/tables (not performance). Are there unlogged batches? If so, are they small (&lt;100) or on the same partition key? Unlogged batches can improve performance but need to either be small or on a single partition key otherwise they can negatively impact performance. Not that unlogged batches do not provide atomic operations. For large range queries, is the client paging through results? Paging is necessary to read large results sets without memory constraints. Most drivers have inbuilt paging support but needs to be explicitly turned on in query code.   <p>Does the query on the index lookup a row in a large partition?</p>   <p>Whole partition will be scanned to find matching rows – potentially expensive reads.</p>",
        "created_at": "2018-07-24T12:44:51+0000",
        "updated_at": "2018-09-13T14:48:52+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11070"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1233,
            "label": "data.modeling",
            "slug": "data-modeling"
          }
        ],
        "is_public": false,
        "id": 11069,
        "uid": null,
        "title": "6-step-guide-to-apache-cassandra-data-modelling-white-paper",
        "url": "https://www.instaclustr.com/resource/6-step-guide-to-apache-cassandra-data-modelling-white-paper/",
        "content": null,
        "created_at": "2018-07-24T12:44:20+0000",
        "updated_at": "2018-09-13T14:48:24+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": null,
        "language": null,
        "reading_time": 0,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": null,
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11069"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 11068,
        "uid": null,
        "title": "Cassandra NoSQL Data Model Design  - High Scalability -",
        "url": "http://highscalability.com/blog/2017/11/13/cassandra-nosql-data-model-design.html",
        "content": "<div class=\"journal-entry-tag journal-entry-tag-post-title\">   <img title=\"date\" alt=\"date\" class=\"inline-icon date-icon\" src=\"http://highscalability.com/universal/images/transparent.png\" />Monday, November 13, 2017 at 8:56AM </div><div class=\"body\">\n  \n        \n        \n        \n          <p><img src=\"https://c1.staticflickr.com/5/4535/24521720278_d4be4b74b5_q.jpg\" alt=\"\" style=\"float: right;\" /></p>\n<p>We at <a href=\"https://www.instaclustr.com/\">Instaclustr</a> recently published a blog post on the most common data modelling mistakes that we see with Cassandra. This post was very popular and led me to think about what advice we could provide on how to approach designing your Cassandra data model so as to come up with a quality design that avoids the traps.</p>\n<p>There are a number of good articles around that with rules and patterns to fit your data model into: <a rel=\"noopener\" href=\"https://www.instaclustr.com/resource/6-step-guide-to-apache-cassandra-data-modelling-white-paper/\" target=\"_blank\">6 Step Guide to Apache Cassandra Data Modelling</a> and <a rel=\"noopener\" href=\"https://support.instaclustr.com/hc/en-us/articles/207071957-Data-Modelling-Recommended-Practices\" target=\"_blank\">Data Modelling Recommended Practices</a>.</p>\n<p>However, we haven’t found a step by step guide to analysing your data to determine how to fit in these rules and patterns. This white paper is a quick attempt at filling that gap.</p>\n<h2>Phase 1: Understand the data</h2>\n<p>This phase has two distinct steps that are both designed to gain a good understanding of the data that you are modelling and the access patterns required.</p>\n<h3>Define the data domain</h3>\n<p>The first step is to get a good understanding of your data domain. As someone very familiar with relation data modelling, I tend to sketch (or at least think) ER diagrams to understand the entities, their keys and relationships. However, if you’re familiar with another notation then it would likely work just as well. The key things you need to understand at a logical level are:</p>\n<p>• What are the entities (or objects) in your data model?<br />• What are the primary key attributes of the entities?<br />• What are the relationships between the entities (i.e. references from one to the other)?<br />• What is the relative cardinality of the relationships (i.e. if you have a one to many is it one to 10 or one to 10,000 on average)?</p>\n<p>Basically, these are the same things you’d expect in from logical ER model (although we probably don’t need a complete picture of all the attributes) along with a complete understanding of the cardinality of relationships that you’d normally need for a relational model. An understanding of the demographics of key attributes (cardinality, distribution) will also be useful in finalising your Cassandra model. Also, understand which key attributes are fixed and which change over the life of a record.</p>\n<h2>Define the required access patterns</h2>\n<p>The next step, or quite likely a step carried out in conjunction with step 1, is to understand how you will need to access your data:</p>\n<ul><li>List out the paths you will follow to access the data, such as:           \n<ul><li>Start with a customer id, search for transactions in a date range and then look up all the details about a particular transaction from the search resultsStart with a particular server and metric, retrieve x metrics values in ascending age</li>\n<li>Start with a particular server and metric, retrieve x metrics values in ascending age starting at a particular point in time.</li>\n<li>For a given sensor, retrieve all readings of multiple metrics for a given day.</li>\n<li>For a given sensor, retrieve the current value.</li>\n</ul></li>\n</ul><ul><li>Remember that any updates of a record are an access path that needs to be considered</li>\n<li>Determine which accesses are the most crucial from a performance point of view – are there some which need to be as quick as possible while performance requirements for others allow time for multiple reads or range scans?</li>\n<li>Remember that you need a pretty complete understanding of how you will access your data at this stage – part of the trade-off for Cassandra’s performance, reliability and scalability is a fairly restricted set of methods for accessing data in a particular table.</li>\n</ul><h2>Phase 2: Understand the entities</h2>\n<p>This phase has two specific steps designed to gain an understanding of both the primary and secondary entities associated with the data.</p>\n<h3>Identify primary access entities</h3>\n<p>Now we’re moving from analysing your data domain and application requirements to starting to design your data model. You really want to be pretty solid on steps 1 and 2 before moving on to this stage.</p>\n<p>The idea here is to denormalize your data into the smallest number of tables possible based on your access patterns. For each lookup by key that your access patterns require, you will need a table to satisfy that lookup. I’ve coined the term primary access entity to describe the entity your using for the lookup (for example, a lookup by client id is using client as the primary access entity, a lookup by server and metric name is using a server-metric entity as the primary access entity).</p>\n<p>The primary access entity defines the partition level (or grain if you’re familiar with dimensional modelling) of the resulting denormalized table (i.e. there will be one partition in the table for each instance of the primary access entity).</p>\n<p>You may choose to satisfy some access patterns using secondary indexes rather than complete replicas of the data with a different primary access entity. Keep in mind that columns in include in a secondary index should have a significantly lower cardinality than the table being indexed and be aware of the frequency of updates of the indexed value.</p>\n<p>For the example access patterns above, we would define the following primary access entities:</p>\n<ul><li>customer and transaction (get a list of transactions from the customer entity and then use that to look up transaction details from the transaction entity)</li>\n<li>server-metric</li>\n<li>sensor</li>\n<li>sensor</li>\n</ul><h3>Allocate secondary entities</h3>\n<p>The next step is to find a place to store the data that belongs to entities that have not been chosen as primary access entities (I’ll call these entities secondary entities). You can choose to:</p>\n<ul><li>Push down by taking data from a parent secondary entity (one side) of a one to many relationship and storing multiple copies of it at the primary access entity level (for example, storing customer phone number in each customer order record); or</li>\n<li>Push up by taking data from the child secondary entity (many side) of a one to many relationship and storing it at the primary access entity level either by use of cluster keys or by use of multi-value types (list and maps) (for example adding a list of line items to a transaction level table).</li>\n</ul><p>For some secondary entities, there will only be one related primary access entity and so there is no need to choose where and which direction to push. For other entities, you will need to choose will need to choose which primary access entities to push the data into.</p>\n<p>For optimal read performance, you should push a copy of the data to every primary access entity that is used as an access path for the data in the secondary entity.</p>\n<p>However, this comes at an insert/update performance and application complexity cost of maintaining multiple copies the data. This trade-off between read performance and data maintenance cost needs to be judged in the context of the specific performance requirements of your application.</p>\n<p>The other decision to be made at this stage is between using a cluster key or a multi-value type for pushing up. In general:</p>\n<ul><li>Use a clustering key where there is only one child secondary entity to push up and particularly where the child secondary entity itself has children to roll-up.</li>\n<li>Use multi-value types where there are multiple child entities to push up into the primary entity</li>\n</ul><p>Note that these rules are probably oversimplified but serve as a starting point for more detailed consideration.</p>\n<h2>Phase 3: Review &amp; Tune</h2>\n<p>The last phase provides an opportunity to review the data model, test and to tune as necessary.</p>\n<h3>Review partition &amp; cluster keys</h3>\n<p>Entering this stage, you have all the data you need to store allocated to a table or tables and your tables support accessing that data according to your required access patterns. The next step is to check that the resulting data model makes efficient use of Cassandra and, if not, to adjust. The items to check and adjust at this stage are:</p>\n<ul><li>Do your partition keys have sufficient cardinality? If not, it may be necessary to move columns from the clustering key to the partition key (e.g. changing primary key (client_id, timestamp) to primary key ((client_id, timestamp))) or introduce new columns which group multiple cluster keys into partitions (e.g. changing primary key (client_id, timestamp) to primary key ((client_id, day), timestamp).</li>\n<li>Will the values in your partition keys be updated frequently?Updates of a primary key value will result in deletion and re-insertion of the record which can result in issues with tombstones. For example, trying to maintain a table with all clients of a particular status, you might have primary key (status, client ID). However, this will result in a delete and re-insert every time a client’s status changes. This would be a good candidate to use a set or list data type rather than including client ID as the cluster key.</li>\n<li>Is the number of records in each partition bounded? Extremely large partitions and/or very unevenly sized partitions can cause issues. For example, if you have a client_updates table with primary key (client_id, update_timestamp) there is potentially no limit to how many times a particular client record can be update and you may have significant unevenness if you have a small number of clients that have been around for 10 years and most clients only having a day or two’s history. This is another example where it’s useful to introduce new columns which group multiple cluster keys into partitions partitions (e.g. changing primary key (client_ id, update_timestamp) to primary key ((client_id, month), update_timestamp).</li>\n</ul><h3>Test and tune</h3>\n<p>The final step is perhaps the most important – test your data model and tune it as required. Keep in mind that issues like partitions or rows growing too large or tombstones building up in a table may only become visible after days (or longer) of use under real-world load. It’s therefore important to test as closely as possible to real-world load and to monitor closely for any warning signs (the nodetool cfstats and cfhistograms commands are very useful for this).</p>\n<p>At this stage you may also consider tuning some of the settings that effect the physical storage of your data. For example:</p>\n<ul><li>changing compaction strategy;</li>\n<li>reducing gc_grace_seconds if you are only deleting data using TTL; or</li>\n<li>setting caching options.</li>\n</ul><h2>A Worked Example</h2>\n<p>To illustrate this, I’ll walk through a basic example based on building a database to store and retrieve log messages from multiple servers. Note this is quite simplified compared to most real-world requirements.</p>\n<h3>Step 1: Define the data domain</h3>\n<p><a class=\"fbx-link fbx-instance\" href=\"https://www.instaclustr.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design.png\"><img class=\"aligncenter wp-image-6928 size-large\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-1024x616.png\" alt=\"Defining the data model domain Instaclustr Data Model design\" width=\"1024\" height=\"616\" /></a></p>\n<p>The previous ER diagram illustrated the data domain. We have:</p>\n<ul><li>Lots (millions) of log messages which have a timestamp and a body. Although message ID is shown as the primary key in the ER diagram, message time plus message type is an alternate primary key.</li>\n<li>Each log message has a message type and types are further grouped into a message category (for example, a message type might be “out of memory error” and category might be “error”). There a couple of hundred message types and around 20 categories.</li>\n<li>Each log message comes from a message source. The message source is the server that generated the message. There are 1000s of servers in our system. Each message source has a source type to categorise the source (e.g. red hat server, ubuntu server, windows server, router, etc.). There are around 20 source types. There are ~10,000 messages per source per day.</li>\n<li>The message body can be parsed and stored as multiple message parts (basically key, value pairs). There is typically less than 20 parts per message.</li>\n</ul><h3>Step 2: Define the required access patterns</h3>\n<p>We need to be able to:</p>\n<ul><li>Retrieve all available information about the most recent 10 messages for a given source (and be able to work back in time from there).</li>\n<li>Retrieve all available information about the most recent 10 message for a given source type.</li>\n</ul><h3>Step 3: Identify primary access entities</h3>\n<p>There are two primary access entities here – source and source type. The cardinality (~20) of source type makes it a good candidate for a secondary index so we will use source as the primary access entity and add a secondary index for source type.</p>\n<h3>Step 4: Allocate secondary entities</h3>\n<p>In this example, this step is relatively simple as all data needs to roll into the log source primary access entity. So we:</p>\n<ul><li>Push down source type name</li>\n<li>Push down message category and message type to log message</li>\n<li>Push up log message as the clustering key for the new entity</li>\n<li>Push up message part as a map type with.</li>\n</ul><p>The end result is that would be a single table with a partition key of source ID and a clustering key of (message time, message type).</p>\n<h3>Step 5: Review partition and cluster keys</h3>\n<p>Checking these partition and cluster keys against the checklist:</p>\n<ul><li>Do your partition keys have sufficient cardinality? Yes, there are 1000s of sources.</li>\n<li>Will the values in your partition keys being updated frequently? No, all the data is write-once.</li>\n<li>Is the number of records in each partition bounded? No – messages could build up indefinitely over time.</li>\n</ul><p>So, we need to address the unbound partition size. A typical pattern to address that in time series data such as this is to introduce a grouping of time periods into the cluster key. In this case 10,000 messages per day is a reasonable number to include in one partition so we’ll use day as part of our partition key.</p>\n<p>The resulting Cassandra table will look some like:</p>\n<div id=\"crayon-5a0099a71ecf5394143671\" class=\"crayon-syntax crayon-font-monaco crayon-os-pc print-yes notranslate crayon-theme-sublime-text\">\n<div class=\"crayon-main\">\n<table class=\"crayon-table\"><tbody><tr class=\"crayon-row\"><td class=\"crayon-nums\">\n</td>\n<td class=\"crayon-code\">\n<div class=\"crayon-pre\">\n<div id=\"crayon-5a0099a71ecf5394143671-1\" class=\"crayon-line\">CREATE TABLE example.log_messages (</div>\n<div id=\"crayon-5a0099a71ecf5394143671-2\" class=\"crayon-line crayon-striped-line\">message_id uuid,</div>\n<div id=\"crayon-5a0099a71ecf5394143671-3\" class=\"crayon-line\">source_name text,</div>\n<div id=\"crayon-5a0099a71ecf5394143671-4\" class=\"crayon-line crayon-striped-line\">source_type text,</div>\n<div id=\"crayon-5a0099a71ecf5394143671-5\" class=\"crayon-line\">message_type text,</div>\n<div id=\"crayon-5a0099a71ecf5394143671-6\" class=\"crayon-line crayon-striped-line\">message_urgency int,</div>\n<div id=\"crayon-5a0099a71ecf5394143671-7\" class=\"crayon-line\">message_category text,</div>\n<div id=\"crayon-5a0099a71ecf5394143671-8\" class=\"crayon-line crayon-striped-line\">message_time timestamp,</div>\n<div id=\"crayon-5a0099a71ecf5394143671-9\" class=\"crayon-line\">message_time_day text,</div>\n<div id=\"crayon-5a0099a71ecf5394143671-10\" class=\"crayon-line crayon-striped-line\">message_body text,</div>\n<div id=\"crayon-5a0099a71ecf5394143671-11\" class=\"crayon-line\">message_parts map&amp;lt;text, frozen &amp;gt;</div>\n<div id=\"crayon-5a0099a71ecf5394143671-13\" class=\"crayon-line\">PRIMARY KEY ((source_name, message_time_day,</div>\n<div id=\"crayon-5a0099a71ecf5394143671-14\" class=\"crayon-line crayon-striped-line\">message_time, message_type)</div>\n<div id=\"crayon-5a0099a71ecf5394143671-15\" class=\"crayon-line\">) WITH CLUSTERING ORDER BY (message_time DESC);</div>\n<div id=\"crayon-5a0099a71ecf5394143671-16\" class=\"crayon-line crayon-striped-line\">CREATE INDEX log_messages_sourcetype_idx ON</div>\n<div id=\"crayon-5a0099a71ecf5394143671-17\" class=\"crayon-line\">example.log_messages (source_type);</div>\n</div>\n</td>\n</tr></tbody></table></div>\n</div>\n\n<h2>Conclusion</h2>\n<p>Hopefully, this process and basic example will help you start to get familiar with Cassandra data modelling. We’ve only covered a basic implementation that fits well with Cassandra, however there are many other examples on the web which can help you work through more complex requirements. Instaclustr also provides our customers with data modelling review and assistance, so get in touch with us if you need some hands-on assistance.</p>\n        \n  \n          \n  \n         \n  \n      </div>",
        "created_at": "2018-07-24T12:44:03+0000",
        "updated_at": "2018-07-24T12:44:10+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 12,
        "domain_name": "highscalability.com",
        "preview_picture": "https://c1.staticflickr.com/5/4535/24521720278_d4be4b74b5_q.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11068"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 11067,
        "uid": null,
        "title": "Cassandra Time Series Data Modeling For Massive Scale",
        "url": "http://thelastpickle.com/blog/2017/08/02/time-series-data-modeling-massive-scale.html",
        "content": "<p>One of the big challenges people face when starting out working with Cassandra and time series data is understanding the impact of how your write workload will affect your cluster.  Writing too quickly to a single partition can create hot spots that limit your ability to scale out. Partitions that get too large can lead to issues with repair, streaming, and read performance.  Reading from the middle of a large partition carries a lot of overhead, and results in increased GC pressure.  Cassandra 4.0 should <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-9754\">improve the performance of large partitions</a>, but it won’t fully solve the other issues I’ve already mentioned.  For the foreseeable future, we will need to consider their performance impact and plan for them accordingly.</p><p>In this post, I’ll discuss a common Cassandra data modeling technique called <em>bucketing</em>.  Bucketing is a strategy that lets us control how much data is stored in each partition as well as spread writes out to the entire cluster.  This post will discuss two forms of bucketing. These techniques can be combined when a data model requires further scaling.  Readers should already be familiar with the anatomy of a partition and basic CQL commands.</p>\n<p>When we first learn about data modeling with Cassandra, we might see something like the following:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>CREATE TABLE raw_data (\n    sensor text,\n    ts timeuuid,\n    readint int,\n    primary key(sensor, ts)\n) WITH CLUSTERING ORDER BY (ts DESC) \n  AND compaction = {'class': 'TimeWindowCompactionStrategy', \n                    'compaction_window_size': 1, \n                    'compaction_window_unit': 'DAYS'};\n</pre></div></div>\n<p>This is a great first data model for storing some very simple sensor data.  Normally the data we collect is more complex than an integer, but in this post we’re going to focus on the keys.  We’re leveraging <a href=\"http://thelastpickle.com/blog/2016/12/08/TWCS-part1.html\">TWCS</a> as our compaction strategy.  TWCS will help us deal with the overhead of compacting large partitions, which should keep our CPU and I/O under control.  Unfortunately it still has some significant limitations.  If we aren’t using a TTL, as we take in more data, our partition size will grow constantly, unbounded.  As mentioned above, large partitions carry significant overhead when repairing, streaming, or reading from arbitrary time slices.</p>\n<p>To break up this big partition, we’ll leverage our first form of bucketing.  We’ll break our partitions into smaller ones based on time window.  The ideal size is going to keep partitions under 100MB.  For example, one partition per sensor per day would be a good choice if we’re storing 50-75MB of data per day.  We could just as easily use week (starting from some epoch), or month and year as long as the partitions stay under 100MB.  Whatever the choice, leaving a little headroom for growth is a good idea.</p>\n<p>To accomplish this, we’ll add another component to our partition key.  Modifying our earlier data model, we’ll add a <code class=\"highlighter-rouge\">day</code> field:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>CREATE TABLE raw_data_by_day (\nsensor text,\nday text,\nts timeuuid,\nreading int,\nprimary key((sensor, day), ts)\n) WITH CLUSTERING ORDER BY (ts DESC) \n       AND COMPACTION = {'class': 'TimeWindowCompactionStrategy', \n                     'compaction_window_unit': 'DAYS', \n                     'compaction_window_size': 1};\n</pre></div></div>\n<p>Inserting into the table requires using the date as well as the <code class=\"highlighter-rouge\">now()</code> value (you could also generate a TimeUUID in your application code):</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>INSERT INTO raw_data_by_day (sensor, day, ts, reading) \nVALUES ('mysensor', '2017-01-01', now(), 10);\n</pre></div></div>\n<p>This is one way of limiting the amount of data per partition.  For fetching large amounts of data across multiple days, you’ll need to issue one query per day.  The nice part about querying like this is we can spread the work over the entire cluster rather than asking a single node to perform a lot of work.  We can also issue these queries in parallel by relying on the async calls in the driver.  The Python driver even has a convenient helper function for this sort of use case:</p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre>from itertools import product\nfrom cassandra.concurrent import execute_concurrent_with_args\ndays = [\"2017-07-01\", \"2017-07-12\", \"2017-07-03\"]  # collecting three days worth of data\nsession  = Cluster([\"127.0.0.1\"]).connect(\"blog\")\nprepared = session.prepare(\"SELECT day, ts, reading FROM raw_data_by_day WHERE sensor = ? and day = ?\")\nargs = product([\"mysensor\"], days) \n# args: ('test', '2017-07-01'), ('test', '2017-07-12'), ('test', '2017-07-03')\n# driver handles concurrency for you\nresults = execute_concurrent_with_args(session, prepared, args)\n# Results:\n#[ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d36750&gt;),\n# ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d36a90&gt;),\n# ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d36550&gt;)]\n</pre></div></div>\n<p>A variation on this technique is to use a different table per time window.  For instance, using a table per month means you’d have twelve tables per year:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>CREATE TABLE raw_data_may_2017 (\n    sensor text,\n    ts timeuuid,\n    reading int,\n    primary key(sensor, ts)\n) WITH COMPACTION = {'class': 'TimeWindowCompactionStrategy', \n                     'compaction_window_unit': 'DAYS', \n                     'compaction_window_size': 1};\n</pre></div></div>\n<p>This strategy has a primary benefit of being useful for archiving and quickly dropping old data.  For instance, at the beginning of each month, we could archive last month’s data to HDFS or S3 in parquet format, taking advantage of cheap storage for analytics purposes.  When we don’t need the data in Cassandra anymore, we can simply drop the table.  You can probably see there’s a bit of extra maintenance around creating and removing tables, so this method is really only useful if archiving is a requirement.  There are other methods to archive data as well, so this style of bucketing may be unnecessary.</p>\n<p>The above strategies focuses on keeping partitions from getting too big over a long period of time.  This is fine if we have a predictable workload and partition sizes that have very little variance.  It’s possible to be ingesting so much information that we can overwhelm a single node’s ability to write data out, or the ingest rate is significantly higher for a small percentage of objects.  Twitter is a great example, where certain people have tens of millions of followers but it’s not the common case.  It’s common to have a separate code path for these types of accounts where we need massive scale</p>\n<p>The second technique uses multiple partitions at any given time to fan out inserts to the entire cluster.  The nice part about this strategy is we can use a single partition for low volume, and many partitions for high volume.</p>\n<p>The tradeoff we make with this design is on reads we need to use a scatter gather, which has significantly higher overhead.  This can make pagination more difficult, amongst other things.  We need to be able to track how much data we’re ingesting for each gizmo we have.  This is to ensure we can pick the right number of partitions to use.  If we use too many buckets, we end up doing a lot of really small reads across a lot of partitions.  Too few buckets, we end up with really large partitions that don’t compact, repair, stream well, and have poor read performance.</p>\n<p>For this example, we’ll look at a theoretical model for someone who’s following a lot of users on a social network like Twitter.  Most accounts would be fine to have a single partition for incoming messages, but some people / bots might follow millions of accounts.</p>\n<p><em>Disclaimer: I have no knowledge of how Twitter is actually storing their data, it’s just an easy example to discuss.</em></p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>CREATE TABLE tweet_stream (\n    account text,\n    day text,\n    bucket int,\n    ts timeuuid,\n    message text,\n    primary key((account, day, bucket), ts)\n) WITH CLUSTERING ORDER BY (ts DESC) \n         AND COMPACTION = {'class': 'TimeWindowCompactionStrategy', \n                       'compaction_window_unit': 'DAYS', \n                       'compaction_window_size': 1};\n</pre></div></div>\n<p>This data model extends our previous data model by adding <code class=\"highlighter-rouge\">bucket</code> into the partition key.  Each day can now have multiple buckets to fetch from.  When it’s time to read, we need to fetch from all the partitions, and take the results we need.  To demonstrate, we’ll insert some data into our partitions:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>cqlsh:blog&gt; insert into tweet_stream (account, day, bucket, ts, message) VALUES ('jon_haddad', '2017-07-01', 0, now(), 'hi');\ncqlsh:blog&gt; insert into tweet_stream (account, day, bucket, ts, message) VALUES ('jon_haddad', '2017-07-01', 1, now(), 'hi2');\ncqlsh:blog&gt; insert into tweet_stream (account, day, bucket, ts, message) VALUES ('jon_haddad', '2017-07-01', 2, now(), 'hi3');\ncqlsh:blog&gt; insert into tweet_stream (account, day, bucket, ts, message) VALUES ('jon_haddad', '2017-07-01', 3, now(), 'hi4');\n</pre></div></div>\n<p>If we want the ten most recent messages, we can do something like this:</p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre>from itertools import chain\nfrom cassandra.util import unix_time_from_uuid1\nprepared = session.prepare(\"SELECT ts, message FROM tweet_stream WHERE account = ? and day = ? and bucket = ? LIMIT 10\")\n# let's get 10 buckets \npartitions = range(10)\n# [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nargs = product([\"jon_haddad\"], [\"2017-07-01\"], partitions)\nresult = execute_concurrent_with_args(session, prepared, args)\n# [ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1e6d0&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1d710&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1d4d0&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1d950&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1db10&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1dfd0&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1dd90&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1d290&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1e250&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1e490&gt;)]\nresults = [x.result_or_exc for x in result]\n# append all the results together\ndata = chain(*results)\n            \nsorted_results = sorted(data, key=lambda x: unix_time_from_uuid1(x.ts), reverse=True)            \n# newest stuff first\n# [Row(ts=UUID('e1c59e60-7406-11e7-9458-897782c5d96c'), message=u'hi4'),\n#  Row(ts=UUID('dd6ddd00-7406-11e7-9458-897782c5d96c'), message=u'hi3'),\n#  Row(ts=UUID('d4422560-7406-11e7-9458-897782c5d96c'), message=u'hi2'),\n#  Row(ts=UUID('d17dae30-7406-11e7-9458-897782c5d96c'), message=u'hi')]\n</pre></div></div>\n<p>This example is only using a LIMIT of 10 items, so we can be lazy programmers, merge the lists, and then sort them.  If we wanted to grab a lot more elements we’d want to use a k-way merge algorithm.  We’ll come back to that in a future blog post when we expand on this topic.</p>\n<p>At this point you should have a better understanding of how you can distribute your data and requests around the cluster, allowing it to scale much further than if a single partition were used.  Keep in mind each problem is different, and there’s no one size fits all solution.</p>",
        "created_at": "2018-07-24T12:43:50+0000",
        "updated_at": "2018-07-24T12:43:59+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 9,
        "domain_name": "thelastpickle.com",
        "preview_picture": "http://thelastpickle.com/android-chrome-192x192.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11067"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 11066,
        "uid": null,
        "title": "Designing a Cassandra Data Model",
        "url": "https://shermandigital.com/blog/designing-a-cassandra-data-model/",
        "content": "<p>Cassandra is an open source, distributed database. It’s useful for managing large quantities of data across multiple data centers as well as the cloud.</p><h3 id=\"cassandra-data-model\">Cassandra data model</h3><p>Cassandra’s data model consists of keyspaces, column families, keys, and columns. The table below compares each part of the Cassandra data model to its analogue in a relational data model.</p><table><thead><tr><th><strong>Cassandra Data Model</strong></th>\n<th><strong>Relational Data Model</strong></th>\n</tr></thead><tbody><tr><td>Keyspace</td>\n<td>Database</td>\n</tr><tr><td>Column Family</td>\n<td>Table</td>\n</tr><tr><td>Partition Key</td>\n<td>Primary Key</td>\n</tr><tr><td>Column Name/Key</td>\n<td>Column Name</td>\n</tr><tr><td>Column Value</td>\n<td>Column Value</td>\n</tr></tbody></table><h3 id=\"how-cassandra-organizes-data\">How Cassandra organizes data</h3><p>Cassandra organizes data into partitions. Each partition consists of multiple columns. Partitions are stored on a node. Nodes are generally part of a cluster where each node is responsible for a fraction of the partitions.</p><p>When inserting records, Cassandra will hash the value of the inserted data’s partition key; Cassandra uses this hash value to determine which node is responsible for storing the data.</p><h3 id=\"where-are-the-rows\">Where are the rows?</h3><p>Cassandra is a column data store, meaning that each partition key has a set of one or more columns. Let’s say we have a list of fruits:</p><ul><li>[Apple, Banana, Orange, Pear]</li>\n</ul><p>We create a column family of fruits, which is essentially the same as a table in the relational model. Inside our column family, Cassandra will hash the name of each fruit to give us the partition key, which is essentially the primary key of the fruit in the relational model.</p><p>Now things start to diverge from the relational model. Cassandra will store each fruit on its own partition, since the hash of each fruit’s name will be different. Because each fruit has its own partition, it doesn’t map well to the concept of a row, as Cassandra has to issue commands to potentially four separate nodes to retrieve all data from the fruit column family.</p><p>We’ll get into more details later, but for now it’s enough to know that for Cassandra to look up a set of data (or a set of rows in the relational model), we have to store all of the data under the same partition key. To summarize, rows in Cassandra are essentially data embedded within a partition due to the fact that the data share the same partition key.</p><h3 id=\"data-model-goals\">Data model goals</h3><ol><li>Spread data evenly around the cluster. Paritions are distributed around the cluster based on a hash of the partition key. To distribute work across nodes, it’s desirable for every node in the cluster to have roughly the same amount of data.</li>\n<li>Minimize the number of partitions read. Partitions are groups of columns that share the same partition key. Since each partition may reside on a different node, the query coordinator will generally need to issue separate commands to separate nodes for each partition we query.</li>\n<li>Satisfy a query by reading a single partition. This means we will use roughly one table per query. Supporting multiple query patterns usually means we need more than one table. Data duplication is encouraged.</li>\n</ol><h3 id=\"components-of-the-cassandra-data-model\">Components of the Cassandra data model</h3><p><strong>Column family</strong></p><p>Column families are established with the CREATE TABLE command. Column families are represented in Cassandra as a map of sorted maps. The partition key acts as the lookup value; the sorted map consists of column keys and their associated values.</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">Map&lt;ParitionKey, SortedMap&lt;ColumnKey, ColumnValue&gt;&gt;</pre></div><p><strong>Partition key</strong></p><p>The partition key is responsible for distributing data among nodes. A partition key is the same as the primary key when the primary key consists of a single column.</p><p><img src=\"https://shermandigital.com/img/blog/cassandra-partition.png\" alt=\"Cassandra partition\" /></p><p>Partition keys belong to a node. Cassandra is organized into a cluster of nodes, with each node having an equal part of the partition key hashes.</p><p>Imagine we have a four node Cassandra cluster. In the example cluster below, Node 1 is responsible for partition key hash values 0-24; Node 2 is responsible for partition key hash values 25-49; and so on.</p><p><img src=\"https://shermandigital.com/img/blog/cassandra-cluster.png\" alt=\"Cassandra cluster\" /></p><p><strong>Replication factor</strong></p><p>Depending on the replication factor configured, data written to Node 1 will be replicated in a clockwise fashion to its sibling nodes. So in our example above, assume we have a four-node cluster with a replication factor of three. When we insert data with a partition key of 23, the data will get written to Node 1 and replicated to Node 2 and Node 3. When we insert data with a partition key of 88, the data will get written to Node 4 and replicated to Node 1 and Node 2.</p><p><strong>Compound key</strong></p><p>Compound keys include multiple columns in the primary key, but these additional columns do not necessarily affect the partition key. A partition key with multiple columns is known as a composite key and will be discussed later.</p><p>Let’s borrow an example from <a href=\"https://twitter.com/AdamHutson\">Adam Hutson’s</a> excellent <a href=\"http://datascale.io/cassandra-data-model-basics/\">blog on Cassandra data modeling</a>. Consider a Cassandra database that stores information on CrossFit gyms. One property of CrossFit gyms is that each gym must have a unique name i.e. no two gyms are allowed to share the same name.</p><p>The table below is useful for looking up a gym when we know the name of the gym we’re looking for.</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">CREATE TABLE crossfit_gyms (  \n   gym_name text,  \n   city text,  \n   state_province text,  \n   country_code text,  \n   PRIMARY KEY (gym_name)  \n);</pre></div><p>Now suppose we want to look up gyms by location. If we use the crossfit_gyms table, we’ll need to iterate over the entire result set. Instead, we’ll create a new table that will allow us to query gyms by country.</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">CREATE TABLE crossfit_gyms_by_location (  \n   country_code text,  \n   state_province text,  \n   city text,  \n   gym_name text,  \n   PRIMARY KEY (country_code, state_province, city, gym_name)  \n);</pre></div><p>Note that only the first column of the primary key above is considered the partition key; the rest of columns are clustering keys. This means that while the primary key represents a unique gym record/row, all gyms within a country reside on the same partition. So when we query the crossfit_gyms_by_location table, we receive a result set consisting of every gym sharing a given country_code. While useful for searching gyms by country, using this table to identify gyms within a particular state or city requires iterating over all gyms within the country in which the state or city is located.</p><p><strong>Clustering key</strong></p><p>Clustering keys are responsible for sorting data within a partition. Each primary key column after the partition key is considered a clustering key. In the crossfit_gyms_by_location example, country_code is the partition key; state_province, city, and gym_name are the clustering keys. Clustering keys are sorted in ascending order by default. So when we query for all gyms in the United States, the result set will be ordered first by state_province in ascending order, followed by city in ascending order, and finally gym_name in ascending order.</p><p><strong>Order by</strong></p><p>To sort in descending order, add a WITH clause to the end of the CREATE TABLE statement.</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">CREATE TABLE crossfit_gyms_by_location (  \n   country_code text,  \n   state_province text,  \n   city text,  \n   gym_name text,  \n   PRIMARY KEY (country_code, state_province, city, gym_name)  \n) WITH CLUSTERING ORDER BY (state_province DESC, city ASC, gym_name ASC);</pre></div><p> The result set will now contain gyms ordered first by state_province in descending order, followed by city in ascending order, and finally gym_name in ascending order. You must specify the sort order for each of the clustering keys in the ORDER BY statement. The partition key is not part of the ORDER BY statement because its values are hashed and therefore won’t be close to each other in the cluster.</p><p><strong>Composite key</strong></p><p>Composite keys are partition keys that consist of multiple columns. The crossfit_gyms_by_location example only used country_code for partitioning. The result is that all gyms in the same country reside within a single partition. This can lead to wide rows. In the case of our example, there are over 7,000 CrossFit gyms in the United States, so using the single column partition key results in a row with over 7,000 combinations.</p><p>To avoid wide rows, we can move to a composite key consisting of additional columns. If we change the partition key to include the state_province and city columns, the partition hash value will no longer be calculated off only country_code. Now, each combination of country_code, state_province, and city will have its own hash value and be stored in a separate partition within the cluster. We accomplish this by nesting parenthesis around the columns we want included in the composite key. </p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">CREATE TABLE crossfit_gyms_by_city (  \n country_code text,  \n state_province text,  \n city text,  \n gym_name text,  \n opening_date timestamp,  \n PRIMARY KEY ((country_code, state_province, city), opening_date, gym_name)  \n) WITH CLUSTERING ORDER BY ( opening_data ASC, gym_name ASC );</pre></div><p>Notice that we are no longer sorting on the partition key columns. Each combination of the partition keys is stored in a separate partition within the cluster.</p><p>When issuing a CQL query, you must include all partition key columns, at a minimum. You can then apply an additional filter by adding each clustering key in the order in which the clustering keys appear. Below you can see valid queries and invalid queries from our crossfit_gyms_by_city example.</p><p><strong>Valid queries:</strong></p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">SELECT * FROM crossfit_gyms_by_city WHERE country_code = 'USA' and state_province = 'VA' and city = 'Arlington'</pre></div><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">SELECT * FROM crossfit_gyms_by_city WHERE country_code = 'USA' and state_province = 'VA' and city = 'Arlington' and opening_date &lt;  '2015-01-01 00:00:00+0200'</pre></div><p><strong>Invalid queries:</strong></p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">SELECT * FROM crossfit_gyms_by_city WHERE country_code = 'USA' and state_province = 'VA'</pre></div><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">SELECT * FROM crossfit_gyms_by_city WHERE country_code = 'USA' and state_province = 'VA' and city = 'Arlington' and gym_name = 'CrossFit Route 7'</pre></div><p>The first invalid query is missing the city partition key column. The second invalid query uses the clustering key gym_name without including the preceding clustering key opening_date.</p><p>The reason the order of clustering keys matters is because the clustering keys provide the sort order of the result set. Because of the clustering key’s responsibility for sorting, we know all data matching the first clustering key will be adjacent to all other data matching that clustering key.</p><p>In our example, this means all gyms with the same opening date will be grouped together in alphabetical order. Gyms with different opening dates will appear in temporal order.</p><p>Because we know the order, CQL can easily truncate sections of the partition that don’t match our query to satisfy the WHERE conditions pertaining to columns that are not part of the partition key. However, because the clustering key gym_name is secondary to clustering key opening_date, gyms will appear in alphabetical order only for gyms opened on the same day (within a particular city, in this case). Therefore, we can’t specify the gym name in our CQL query without first specifying an opening date.</p><h3 id=\"internal-data-structure\">Internal data structure</h3><p>If we create a column family (table) with CQL:</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">CREATE TABLE crossfit_gyms (  \n gym_name text PRIMARY KEY,  \n country_code text,  \n state_province text,  \n city text  \n);</pre></div><p> And insert a row:</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">INSERT INTO crossfit_gyms (country_code, state_province, city, gym_name) VALUES ('USA', 'CA', 'San Francisco', 'San Francisco CrossFit');</pre></div><p>Assuming we don’t encode the data, it is stored internally as:</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">Partition Key: San Francisco CrossFit  \n=&gt;(column=, value=, timestamp=1374683971220000)  \n=&gt;(column=city, value='San Francisco', timestamp=1374683971220000)  \n=&gt;(column=country_code, value='USA', timestamp=1374683971220000)  \n=&gt;(column=state_province, value='CA', timestamp=1374683971220000)</pre></div><p>You can see that the partition key is used for lookup. In this case the first column is also the partition key, so Cassandra does not repeat the value. The next three columns hold the associated column values.</p><p>If we use a composite key, the internal structure changes a bit. Let’s start with a general example borrowed from <a href=\"https://www.gitbook.com/@teddyma\">Teddy Ma’s</a> <a href=\"https://teddyma.gitbooks.io/learncassandra/content/index.html\">step-by-step guide to learning Cassandra</a>.</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">CREATE TABLE example (\n    partitionKey1 text,\n    partitionKey2 text,\n    clusterKey1 text,\n    clusterKey2 text,\n    normalField1 text,\n    normalField2 text,\n    PRIMARY KEY (\n        (partitionKey1, partitionKey2),\n        clusterKey1, clusterKey2\n        )\n    );</pre></div><p>And insert a row:</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">INSERT INTO example (\n    partitionKey1,\n    partitionKey2,\n    clusterKey1,\n    clusterKey2,\n    normalField1,\n    normalField2\n    )  VALUES (\n    'partitionVal1',\n    'partitionVal2',\n    clusterVal1',\n    'clusterVal2',\n    'normalVal1',\n    'normalVal2');</pre></div><p>This data is stored internally as:</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">RowKey: partitionVal1:partitionVal2\n=&gt; (column=clusterVal1:clusterVal2:, value=, timestamp=1374630892473000)\n=&gt; (column=clusterVal1:clusterVal2:normalfield1, value=6e6f726d616c56616c31, timestamp=1374630892473000)\n=&gt; (column=clusterVal1:clusterVal2:normalfield2, value=6e6f726d616c56616c32, timestamp=1374630892473000)</pre></div><p>The composite key columns are concatenated to form the partition key (RowKey). The clustering keys are concatenated to form the first column and then used in the names of each of the following columns that are not part of the primary key. The actual values we inserted into normalField1 and normalField2 have been encoded, but decoding them results in normalValue1 and normalValue2, respectively.</p><p>Now we can adapt this to our CrossFit example.</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">CREATE TABLE crossfit_gyms_by_state (  \n   country_code text,  \n   state_province text,  \n   city text,  \n   gym_name text,  \n   opening_date timestamp,  \n   street text,  \n   PRIMARY KEY (  \n      (country_code, state_province),  \n      city, gym_name  \n   )  \n);</pre></div><p>And insert a row:</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">INSERT INTO crossfit_gyms_by_state (  \n   country_code,   \n   state_province,   \n   city,   \n   gym_name,   \n   opening_date,  \n   street  \n)   \nVALUES (  \n   'USA',   \n   'CA',  \n   'San Francisco',   \n   'San Francisco CrossFit',  \n   '2015-01-01 00:00:00+0200',  \n   '1162A Gorgas Ave');\n   </pre></div><p>For the sake of readability, I won’t encode the values of the columns. The internal structure is approximately:</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">Partition Key: USA:CA  \n=&gt;(column='San Francisco':'San Francisco CrossFit', value=, timestamp=1374683971220000)  \n=&gt;(column='San Francisco':'San Francisco CrossFit':'​opening_date', value='2015-01-01 00:00:00+0200', timestamp=1374683971220000)  \n=&gt;(column='San Francisco':'San Francisco CrossFit':'​street', value='1162A Gorgas Ave', timestamp=1374683971220000)</pre></div><p>Finally, we’ll show how Cassandra represents sets, lists, and maps internally. Once again, we’ll use an example from <a href=\"https://www.gitbook.com/@teddyma\">Teddy Ma’s</a> <a href=\"https://teddyma.gitbooks.io/learncassandra/content/index.html\">step-by-step guide to learning Cassandra</a>.</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">CREATE TABLE example (  \n   key1 text PRIMARY KEY,  \n   map1 map&lt;text,text&gt;,  \n   list1 list&lt;text&gt;,  \n   set1 set&lt;text&gt;  \n);</pre></div><p>Insert a row:</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">INSERT INTO example (  \n   key1,  \n   map1,  \n   list1,  \n   set1  \n) VALUES (  \n   'john',  \n   {'patricia':'555-4326','doug':'555-1579'},  \n   ['doug','scott'],  \n   {'patricia','scott'}  \n);</pre></div><p>Internally:</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">RowKey: john  \n=&gt; (column=, value=, timestamp=1374683971220000)  \n=&gt; (column=map1:doug, value='555-1579', timestamp=1374683971220000)  \n=&gt; (column=map1:patricia, value='555-4326', timestamp=1374683971220000)  \n=&gt; (column=list1:26017c10f48711e2801fdf9895e5d0f8, value='doug', timestamp=1374683971220000)  \n=&gt; (column=list1:26017c12f48711e2801fdf9895e5d0f8, value='scott', timestamp=1374683971220000)  \n=&gt; (column=set1:'patricia', value=, timestamp=1374683971220000)  \n=&gt; (column=set1:'scott', value=, timestamp=1374683971220000)</pre></div><p>To store maps, Cassandra adds a column for each item in the map. The column name is a concatenation of the the column name and the map key. The value is the key’s value.</p><p>To store lists, Cassandra adds a column for each entry in the list. The column name is a concatenation of the the column name and a UUID generated by Cassandra. The value is the value of the list item.</p><p>To store sets, Cassandra adds a column for each entry. The column name is a concatenation of the the column name and the entry value. Cassandra does not repeat the entry value in the value, leaving it empty.</p><h3 id=\"conclusion\">Conclusion</h3><p>You now have enough information to begin designing a Cassandra data model. Remember to work with the unstructured data features of Cassandra rather than against them. Designing a data model for Cassandra can be an adjustment coming from a relational database background, but the ability to store and query large quantities of data at scale make Cassandra a valuable tool.</p><h3 id=\"appendix\">Appendix</h3><p><strong>Cassandra Features</strong></p><ul><li>Continuous availability. The peer-to-peer replication of data to nodes within a cluster results in no single point of failure. This is true even across data centers.</li>\n<li>Linear performance when scaling nodes in a cluster. If three nodes are achieving 3,000 writes per second, adding three more nodes will result in a cluster of six nodes achieving 6,000 writes per second.</li>\n<li>Tunable consistency. If we want to replicate data across three nodes, we can have a replication factor of three, yet not necessarily wait for all three nodes to acknowledge the write. Data will eventually be written to all three nodes, but we can acknowledge the write after writing the data to one or more nodes without waiting for the full replication to finish.</li>\n<li>Flexible data model. Every row can have a different number of columns with support for many types of data.</li>\n<li>Query language (CQL) with a SQL-like syntax.</li>\n<li>Support for Java Monitoring Extensions (JMX). Metrics about performance, latency, system usage, etc. are available for consumption by other applications.</li>\n</ul><p><strong>Cassandra Limitations</strong></p><ul><li>No join or subquery support for aggregation. According to Cassandra’s documentation, this is by design, encouraging denormalization of data into partitions that can be queried efficiently from a single node, rather than gathering data from across the entire cluster.</li>\n<li>Ordering is set at table creation time on a per-partition basis. This avoids clients attempting to sort billions of rows at run time.</li>\n<li>All data for a single partition must fit on disk in a single node in the cluster.</li>\n<li>It’s recommended to keep the number of rows within a partition below 100,000 items and the disk size under 100 MB.</li>\n<li>A single column value is limited to 2 GB (1 MB is recommended).</li>\n</ul><p>A less obvious limitation of Cassandra is its lack of row-level consistency. Modifications to a column family (table) that affect the same row and are processed with the same timestamp will result in a tie.</p><p>In the event of a tie Cassandra follows two rules:</p><ol><li>Deletes take precedence over inserts/updates.</li>\n<li>If there are two updates, the one with the lexically larger value wins.</li>\n</ol><p>This means for inserts/updates, Cassandra resolves row-level ties by comparing values at the column (cell) level, writing the greater value. This can result in one update modifying one column while another update modifies another column, resulting in rows with combinations of values that never existed.</p><p class=\"margin-top-25\">\n<i class=\"fa fa-tag color-medium-gray\">\n<a href=\"https://shermandigital.com/tags/cassandra\" class=\"margin-right-10\">\nCassandra\n</a>\n</i></p><noscript><p>Please enable JavaScript to view the <a href=\"https://disqus.com/?ref_noscript\">comments powered by Disqus.</a></p></noscript>",
        "created_at": "2018-07-24T12:43:26+0000",
        "updated_at": "2019-01-18T21:03:05+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 14,
        "domain_name": "shermandigital.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11066"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 11065,
        "uid": null,
        "title": "Tech Talk: Cassandra Data Modeling",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=tg6eIht-00M",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/tg6eIht-00M?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-07-24T12:42:29+0000",
        "updated_at": "2018-07-24T12:42:42+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/tg6eIht-00M/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11065"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 603,
            "label": "book",
            "slug": "book"
          }
        ],
        "is_public": false,
        "id": 11064,
        "uid": null,
        "title": "Apache Cassandra - Dzone Refcardz",
        "url": "https://dzone.com/refcardz/apache-cassandra?chapter=1",
        "content": "<div class=\"col-md-12 content-html\"><p>Apache Cassandra is a high-performance, extremely scalable, fault tolerant (i.e., no single point of failure), distributed non-relational database solution. Cassandra combines all the benefits of Google Bigtable and Amazon Dynamo to handle the types of database management needs that traditional RDBMS vendors cannot support. DataStax is the leading worldwide commercial provider of Cassandra products, services, support, and training.</p></div><div class=\"col-md-12 content-html\"><p>Cassandra is in use at <a href=\"http://www.slideshare.net/adrianco/migrating-netflix-from-oracle-to-global-cassandra\">Netflix</a>, <a href=\"http://www.slideshare.net/kevinweil/rainbird-realtime-analytics-at-twitter-strata-2011\">Twitter</a>, <a href=\"http://www.slideshare.net/eonnen/from-100s-to-100s-of-millions/\">Urban Airship</a>, <a href=\"http://www.slideshare.net/daveconnors/cassandra-puppet-scaling-data-at-15-per-month\">Constant Contact</a>, <a href=\"http://blog.reddit.com/2010/03/she-who-entangles-men.html\">Reddit</a>, Cisco, OpenX, Rackspace, Ooyala, and <a href=\"http://www.datastax.com/cassandrausers\">more companies</a> that have large active data sets. The largest known Cassandra cluster has over 300 TB of data in over 400 machines.</p><p>(From: <a href=\"http://cassandra.apache.org/\">http://cassandra.apache.org/</a>)</p></div><div class=\"col-md-12 content-html\"><table border=\"1\" style=\"width: 697px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 117px;\"> </td>\n   <td class=\"right_th_colored\" style=\"width: 261px;\"><h3>Cassandra</h3></td>\n   <td class=\"right_th_colored\" style=\"width: 297px;\"><h3>RDBMS</h3></td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\">Atomicity</td>\n   <td class=\"right_td_colored\">Success or failure on a row-by-row basis.</td>\n   <td class=\"right_td_colored\">Enforced at every scope, at the cost of performance and scalability.</td>\n  </tr><tr><td class=\"left_td_colored\">Sharding</td>\n   <td class=\"right_td_colored\">Native share-nothing architecture, inherently partitioned by a configurable strategy.</td>\n   <td class=\"right_td_colored\">Often forced when scaling, partitioned by key or function</td>\n  </tr><tr><td class=\"left_td_colored\">Consistency</td>\n   <td class=\"right_td_colored\">No consistency in the ACID sense. Can be tuned to provide consistency in the CAP sense--data is consistent across all the nodes in a distributed database cluster ,guaranteeing read-after-write or eventual readability.</td>\n   <td class=\"right_td_colored\">Favors consistency over availability tunable via isolation levels.</td>\n  </tr><tr><td class=\"left_td_colored\">Durability</td>\n   <td class=\"right_td_colored\">Writes are durable to a replica node, being recorded in memory and the commit log before acknowledged. In the event of a crash, the commit log replays on restart to recover any lost writes before data is flushed to disk.</td>\n   <td class=\"right_td_colored\">Typically, data is written to a single master node, sometimes configured with synchronous replication at the cost of performance and cumbersome data restoration.</td>\n  </tr><tr><td class=\"left_td_colored\">Multi-Datacenter Replication</td>\n   <td class=\"right_td_colored\">Native capabilities for data replication over lower bandwidth, higher latency, less reliable connections.</td>\n   <td class=\"right_td_colored\">Typically only limited long-distance replication to read-only slaves receiving asynchronous updates.</td>\n  </tr><tr><td class=\"left_td_colored\">Security</td>\n   <td class=\"right_td_colored\">Coarse-grained and primitive.</td>\n   <td class=\"right_td_colored\">Fine-grained access control to objects.</td>\n  </tr></tbody></table></div><div class=\"col-md-12 content-html\"><p>Cassandra has a simple schema comprising keyspaces, column families, rows, and columns.</p><table border=\"1\" style=\"width: 662px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 114px;\"> </td>\n   <td class=\"right_th_colored\" style=\"width: 122px;\">Definition</td>\n   <td class=\"right_th_colored\" style=\"width: 124px;\">RDBMS Analogy</td>\n   <td class=\"right_th_colored\" style=\"width: 274px;\">Object Equivalent</td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\">Schema/ Keyspace</td>\n   <td class=\"right_td_colored\">A collection of column families. </td>\n   <td class=\"right_td_colored\">Schema/Database</td>\n   <td class=\"right_td_colored\">Set </td>\n  </tr><tr><td class=\"left_td_colored\">Table/ Column Family</td>\n   <td class=\"right_td_colored\">A set of rows.</td>\n   <td class=\"right_td_colored\">Table</td>\n   <td class=\"right_td_colored\">Map </td>\n  </tr><tr><td class=\"left_td_colored\">Row</td>\n   <td class=\"right_td_colored\">An ordered set of columns.</td>\n   <td class=\"right_td_colored\">Row</td>\n   <td class=\"right_td_colored\">OrderedMap </td>\n  </tr><tr><td class=\"left_td_colored\">Column</td>\n   <td class=\"right_td_colored\">A key/value pair and timestamp.</td>\n   <td class=\"right_td_colored\">Column (Name, Value)</td>\n   <td class=\"right_td_colored\">(key, value, timestamp)</td>\n  </tr></tbody></table></div><div class=\"col-md-12 content-html\"><p>Also known as a keyspace, the schema is akin to a database or schema in RDBMS and contains a set of tables. A schema is also the unit for Cassandra's access control mechanism. When enabled, users must authenticate to access and manipulate data in a schema or table.</p></div><div class=\"col-md-12 content-html\"><p>A table, also known as a column family, is a map of rows. A table defines the column names and data types. The client application provides rows that conform to the schema. Each row has the same fixed set of columns.</p><p>As Values for these properties, Cassandra provides the following CQL data types for columns.</p><table border=\"1\" style=\"width: 673px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 149px;\"><strong>Type</strong></td>\n   <td class=\"right_th_colored\" style=\"width: 260px;\"><strong>Purpose</strong></td>\n   <td class=\"right_th_colored\" style=\"width: 242px;\"><strong>Storage </strong></td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\">ascii</td>\n   <td class=\"right_td_colored\">Efficient storage for simple ASCII strings.</td>\n   <td class=\"right_td_colored\">Arbitrary number of ASCII bytes (i.e., values are 0-127).</td>\n  </tr><tr><td class=\"left_td_colored\">boolean</td>\n   <td class=\"right_td_colored\">True or False.</td>\n   <td class=\"right_td_colored\">Single byte.</td>\n  </tr><tr><td class=\"left_td_colored\">blob</td>\n   <td class=\"right_td_colored\">Arbitrary byte content.</td>\n   <td class=\"right_td_colored\">Arbitrary number of byes.</td>\n  </tr><tr><td class=\"left_td_colored\">CompositeType</td>\n   <td class=\"right_td_colored\">A single type comprising sub-components each with their own types.</td>\n   <td class=\"right_td_colored\">An arbitrary number of bytes comprising concatenated values of the subtypes..</td>\n  </tr><tr><td class=\"left_td_colored\">counter</td>\n   <td class=\"right_td_colored\">Used for counters, which are cluster-wide incrementing values.</td>\n   <td class=\"right_td_colored\">8 bytes.</td>\n  </tr><tr><td class=\"left_td_colored\">timestamp</td>\n   <td class=\"right_td_colored\">Stores time in milliseconds.</td>\n   <td class=\"right_td_colored\">8 bytes.</td>\n  </tr><tr><td class=\"left_td_colored\">decimal</td>\n   <td class=\"right_td_colored\">Stores BigDecimals.</td>\n   <td class=\"right_td_colored\">4 bytes to store the scale, plus an arbitrary number of bytes to store the value.</td>\n  </tr><tr><td class=\"left_td_colored\">double</td>\n   <td class=\"right_td_colored\">Stores Doubles.</td>\n   <td class=\"right_td_colored\">8 bytes.</td>\n  </tr><tr><td class=\"left_td_colored\">float</td>\n   <td class=\"right_td_colored\">Stores Floats.</td>\n   <td class=\"right_td_colored\">4 bytes.</td>\n  </tr><tr><td class=\"left_td_colored\">int</td>\n   <td class=\"right_td_colored\">Stores 4-byte integer.</td>\n   <td class=\"right_td_colored\">4 bytes.</td>\n  </tr><tr><td class=\"left_td_colored\">varint</td>\n   <td class=\"right_td_colored\">Stores variable precision integer.</td>\n   <td class=\"right_td_colored\">An arbitraty number of bytes used to store the value.</td>\n  </tr><tr><td class=\"left_td_colored\">bigint</td>\n   <td class=\"right_td_colored\">Stores Longs.</td>\n   <td class=\"right_td_colored\">8 bytes.</td>\n  </tr><tr><td class=\"left_td_colored\">text, varchar</td>\n   <td class=\"right_td_colored\">Stores text as UTF8.</td>\n   <td class=\"right_td_colored\">UTF8.</td>\n  </tr><tr><td class=\"left_td_colored\">uuid</td>\n   <td class=\"right_td_colored\">Suitable for UUID storage.</td>\n   <td class=\"right_td_colored\">16 bytes.</td>\n  </tr></tbody></table></div><div class=\"col-md-12 content-html\"><p>Cassandra 1.1 supports tables defined with composite primary keys. The first column in a composite key definition is used as the partition key. Remaining columns are automatically clustered. Rows that share a partition key are sorted by the remaining components of the primary key.</p></div><div class=\"col-md-12 content-html\"><p>A column is a triplet: key, value, and timestamp. The validation and comparator on the column family define how Cassandra sorts and stores the bytes in column keys.<br />The timestamp portion of the column is used to sequence mutations. The timestamp is defined and specified by the client and can be anything the client wishes to use.  By convention, the timestamp is typically microseconds since epoch.  If time-based, clients must be careful to synchronize clocks.</p><p>Columns may optionally have a time-to-live (TTL), after which Cassandra asynchronously deletes them. </p><div class=\"../images/hot_tip.gif\">\n <p><img alt=\"Hot Tip\" class=\"../images/hot_tip.gif_icon fr-dii fr-fil\" src=\"https://dzone.com/storage/rc-covers/14343-thumb.png\" /></p> Originally SuperColumns were one of Cassandra’s data model primitives.  Although they are still supported in the API, we recommend you use CompositeTypes instead. \n</div></div><div class=\"col-md-12 content-html\"><p>Cassandra uses a ring architecture. The ring represents a cyclic range of token values (i.e., the token space).  Each node is assigned a position on the ring based on its token.   A node is responsible for all tokens between its initial token and the initial token of the closest previous node along the ring.</p></div><div class=\"col-md-12 content-html\"><p>Keys are mapped into the token space by a partitioner. The important distinction between the partitioners is order preservation (OP). Users can define their own partitioners by implementing IPartitioner, or they can use one of the native partitioners:</p><table border=\"1\" style=\"width: 467px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 196px;\"> </td>\n   <td class=\"right_th_colored\" style=\"width: 73px;\">Map Function</td>\n   <td class=\"right_th_colored\" style=\"width: 80px;\">Token Space</td>\n   <td class=\"right_th_colored\" style=\"width: 90px;\">OP</td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\">RandomPartitioner</td>\n   <td class=\"right_td_colored\">MD5</td>\n   <td class=\"right_td_colored\">BigInteger</td>\n   <td class=\"right_td_colored\">No</td>\n  </tr><tr><td class=\"left_td_colored\">BytesOrderPartitioner</td>\n   <td class=\"right_td_colored\">Identity</td>\n   <td class=\"right_td_colored\">Bytes</td>\n   <td class=\"right_td_colored\">Yes</td>\n  </tr></tbody></table><p>The following examples illustrate this point.</p><h3>Random Partitioner</h3><p>Since the Random Partitioner uses an MD5 hash function to map keys into tokens, on average those keys will evenly distribute across the cluster. For this reason, RandomPartitioner is the default partitioner.</p><p>The row key determines the node placement:</p><table border=\"1\" style=\"width: 537px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 84px;\">Row Key</td>\n   <td class=\"right_th_colored\" style=\"width: 116px;\"> </td>\n   <td class=\"right_th_colored\" style=\"width: 172px;\"> </td>\n   <td class=\"right_th_colored\" style=\"width: 137px;\"> </td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\">Lisa</td>\n   <td class=\"right_td_colored\">state: CA</td>\n   <td class=\"right_td_colored\">graduated: 2008</td>\n   <td class=\"right_td_colored\">gender: F</td>\n  </tr><tr><td class=\"left_td_colored\">Owen</td>\n   <td class=\"right_td_colored\">state: TX</td>\n   <td class=\"right_td_colored\">gender: M</td>\n   <td class=\"right_td_colored\"> </td>\n  </tr><tr><td class=\"left_td_colored\">Collin</td>\n   <td class=\"right_td_colored\">state: UT</td>\n   <td class=\"right_td_colored\">gender: M</td>\n   <td class=\"right_td_colored\"> </td>\n  </tr></tbody></table><p>This may result in following ring formation, where \"collin\", \"owen\", and \"lisa\" are rowkeys.</p><p><img alt=\"“Random\" class=\"fr-dii fr-fil\" src=\"https://dzone.com/storage/rc-covers/14344-thumb.png\" /></p><p>With Cassandra’s storage model, where each node owns the preceding token space, this results in the following storage allocation based on the tokens.</p><table border=\"1\" style=\"width: 258px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 41px;\">Row Key</td>\n   <td class=\"right_th_colored\" style=\"width: 142px;\">MD5 Hash</td>\n   <td class=\"right_th_colored\" style=\"width: 53px;\">Node</td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\">collin</td>\n   <td class=\"right_td_colored\">CC982736AD62AB</td>\n   <td class=\"right_td_colored\">3</td>\n  </tr><tr><td class=\"left_td_colored\">owen</td>\n   <td class=\"right_td_colored\">9567238FF72635</td>\n   <td class=\"right_td_colored\">2</td>\n  </tr><tr><td>lisa</td>\n   <td>001AB62DE123FF</td>\n   <td>1</td>\n  </tr></tbody></table><p>Notice that the keys are not in order. With RandomPartitioner, the keys are evenly distributed across the ring using hashes, but you sacrifice order, which means any range query needs to query all nodes in the ring.</p><h3>Order Preserving Partitioners (OPP)</h3><p>The Order Preserving Partitioners preserve the order of the row keys as they are mapped into the token space.  </p><p>In our example, since:</p><p><em> \"collin\" &lt; \"lisa\" &lt; \"owen\"</em></p><p>then,</p><p><em>token(\"collin\") &lt; token(\"lisa\") &lt; token(\"owen\")</em></p><p>With OPP, range queries are simplified and a query may not need to consult each node in the ring.  This seems like an advantage, but it comes at a price.  Since the partitioner is preserving order, the ring may become unbalance unless the rowkeys are naturally distributed across the token space.  </p><p>This is illustrated below. </p><p><img alt=\"“OPP”\" class=\"fr-dii fr-fil\" src=\"https://dzone.com/storage/rc-covers/14345-thumb.png\" /></p><p>To manually balance the cluster, you can set the initial token for each node in the Cassandra configuration.</p><div class=\"../images/hot_tip.gif\">\n <p><img alt=\"Hot Tip\" class=\"../images/hot_tip.gif_icon fr-dii fr-fil\" src=\"https://dzone.com/storage/rc-covers/14346-thumb.png\" /></p> If possible, it is best to design your data model to use RandomPartitioner to take advantage of the automatic load balancing and decreased administrative overhead of manually managing token assignment. \n</div></div><div class=\"col-md-12 content-html\"><p>Cassandra provides high availability and fault tolerance through data replication.  The replication uses the ring to determine nodes used for replication.   Each keyspace has an independent replication factor, <em>n</em>.  When writing information, the data is written to the target node as determined by the partitioner and <em>n-1 </em>subsequent nodes along the ring.</p><p>There are two replication strategies: SimpleStrategy and NetworkTopologyStrategy.</p><h3>SimpleStrategy</h3><p>The SimpleStrategy is the default strategy and blindly writes the data to subsequent nodes along the ring.  In the previous example with a replication factor of <em>2</em>, this would result in the following storage allocation.</p><table border=\"1\" style=\"width: 469px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 59px;\">Row Key</td>\n   <td class=\"right_th_colored\" style=\"width: 179px;\">Replica 1 <br />(as determined by partitioner)</td>\n   <td class=\"right_th_colored\" style=\"width: 209px;\">Replica 2<br />(found by traversing the ring)</td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\">collin</td>\n   <td class=\"right_td_colored\">3</td>\n   <td class=\"right_td_colored\">1</td>\n  </tr><tr><td class=\"left_td_colored\">owen</td>\n   <td class=\"right_td_colored\">2</td>\n   <td class=\"right_td_colored\">3</td>\n  </tr><tr><td class=\"left_td_colored\">lisa</td>\n   <td class=\"right_td_colored\">1</td>\n   <td class=\"right_td_colored\">2</td>\n  </tr></tbody></table><h3>NetworkTopologyStrategy</h3><p>The NetworkTopologyStrategy is useful when deploying to multiple data centers. It ensures data is replicated across data centers.</p><p>Effectively, the NetworkTopologyStrategy executes the SimpleStrategy independently for each data center, spreading replicas across distant racks. Cassandra writes a copy in each data center as determined by the partitioner. Data is written simultaneously along the ring to subsequent nodes within that data center with preference for nodes in different racks to offer resilience to hardware failure. All nodes are peers and data files can be loaded through any node in the cluster, eliminating the single point of failure inherent in master-slave architecture and making Cassandra fully fault-tolerant and highly available.</p><p>Given the following ring and deployment topology:</p><p><img alt=\"“Topology&amp;quot;\" class=\"fr-dii fr-fil\" src=\"https://dzone.com/storage/rc-covers/14347-thumb.png\" /></p><p>With blue nodes (N1-N3) deployed to one data center (DC1), red nodes (N4-N6) deployed to another data center (DC2), and a replication factor of 4, Cassandra would write a row with key “lisa” as follows.</p><p>NOTE: Cassandra attempts to write data simultaneously to all target nodes then waits for confirmation from the relevant number of nodes needed to satisfy the specified consistency level. </p><h3>Consistency Levels</h3><p>One of the unique characteristics of Cassandra that sets it apart from other databases is its approach to consistency.  Clients can specify the consistency level on both read and write operations trading off between high availability, consistency, and performance.</p><h3>Write</h3><table border=\"1\" style=\"width: 545px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 130px;\"><strong>Level</strong></td>\n   <th class=\"right_th_colored\" style=\"width: 399px;\"><strong>Expectation</strong></th>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\">ANY</td>\n   <td class=\"right_td_colored\">The write was logged, but the data may not be available for reads immediately. This is useful where you need high availability for writes but only eventual consistency on reads.</td>\n  </tr><tr><td class=\"left_td_colored\">ONE</td>\n   <td class=\"right_td_colored\">Data is committed to at least one replica and is available for reads.</td>\n  </tr><tr><td class=\"left_td_colored\">TWO</td>\n   <td class=\"right_td_colored\">Data is committed to at least two replicas and is available for reads.</td>\n  </tr><tr><td class=\"left_td_colored\">THREE</td>\n   <td class=\"right_td_colored\">Data is committed to at least three replicas and is available for reads.</td>\n  </tr><tr><td class=\"left_td_colored\">QUORUM</td>\n   <td class=\"right_td_colored\">Data is committed to at least n/2+1 replicas and is available for reads, where n is the replication factor.</td>\n  </tr><tr><td class=\"left_td_colored\">LOCAL_QUORUM</td>\n   <td class=\"right_td_colored\">Data is committed to at least n/2+1 replicas within the local data center.</td>\n  </tr><tr><td class=\"left_td_colored\">EACH_QUORUM</td>\n   <td class=\"right_td_colored\">Data is committed to at least n/2+1 replicas within each data center.</td>\n  </tr><tr><td class=\"left_td_colored\">ALL</td>\n   <td class=\"right_td_colored\">Data is committed to and available from all n replicas. This is useful when absolute read consistency and/or fault tolerance are necessary (e.g., online disaster recovery).</td>\n  </tr></tbody></table><h3>Read</h3><table border=\"1\" style=\"width: 548px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 130px;\"><strong>Level</strong></td>\n   <td class=\"right_th_colored\" style=\"width: 380px;\"><strong>Expectation</strong></td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\">ONE</td>\n   <td class=\"right_td_colored\">The client receives data from the first replica to respond.</td>\n  </tr><tr><td class=\"left_td_colored\">TWO</td>\n   <td class=\"right_td_colored\">The client receives the most current data between two replicas based on the timestamps.</td>\n  </tr><tr><td class=\"left_td_colored\">THREE</td>\n   <td class=\"right_td_colored\">The client receives the most current data between three replicas based on the timestamps.</td>\n  </tr><tr><td class=\"left_td_colored\">QUORUM</td>\n   <td class=\"right_td_colored\">The client receives the most current data once n/2+1 replicas have responded.</td>\n  </tr><tr><td class=\"left_td_colored\">LOCAL_QUORUM</td>\n   <td class=\"right_td_colored\">The client receives the most current data once n/2+1 replicas have responded within the local data center.</td>\n  </tr><tr><td class=\"left_td_colored\">EACH_QUORUM</td>\n   <td class=\"right_td_colored\">The client receives the most current data once n/2+1 replicas have responded within each data center.</td>\n  </tr><tr><td class=\"left_td_colored\">ALL</td>\n   <td class=\"right_td_colored\">The client receives the most current data once all replicas have responded</td>\n  </tr></tbody></table></div><div class=\"col-md-12 content-html\"><p>As input into the replication strategy and to efficiently route communication, Cassandra uses a <em>snitch</em> to determine the data center and rack of the nodes in the cluster.  A snitch is a component that detects and informs Cassandra about the network topology of the deployment.  </p><p>The snitch dictates what is used in the strategy options to identify replication groups when configuring replication for a keyspace.</p><table border=\"1\" style=\"width: 395px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 41px;\">Nodes</td>\n   <td class=\"right_th_colored\" style=\"width: 32px;\">Rack</td>\n   <td class=\"right_th_colored\" style=\"width: 22px;\">DC</td>\n   <td class=\"right_th_colored\" style=\"width: 272px;\">Reason</td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\">N4</td>\n   <td class=\"right_td_colored\">3</td>\n   <td class=\"right_td_colored\">2</td>\n   <td class=\"right_td_colored\">As determined by partitioner in DC1.</td>\n  </tr><tr><td class=\"left_td_colored\">N2</td>\n   <td class=\"right_td_colored\">1</td>\n   <td class=\"right_td_colored\">1</td>\n   <td class=\"right_td_colored\">As determined by partitioner in DC2.</td>\n  </tr><tr><td class=\"left_td_colored\">N6</td>\n   <td class=\"right_td_colored\">4</td>\n   <td class=\"right_td_colored\">2</td>\n   <td class=\"right_td_colored\">Preference shown for Rack 4 (over Rack 3).</td>\n  </tr><tr><td class=\"left_td_colored\">N3</td>\n   <td class=\"right_td_colored\">1</td>\n   <td class=\"right_td_colored\">1</td>\n   <td>Written to same rack hosting N2 since no other rack was available.</td>\n  </tr></tbody></table><p>. </p><p>The following table shows the four snitches provided by Cassandra and what you should use in your keyspace configuration for each snitch.</p><table border=\"1\" style=\"width: 443px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 119px;\">Snitch</td>\n   <td class=\"right_th_colored\" style=\"width: 324px;\">Specify</td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\">SimpleSnitch</td>\n   <td class=\"right_td_colored\">Specify only the replication factor in your strategy options.</td>\n  </tr><tr><td class=\"left_td_colored\">PropertyFileSnitch</td>\n   <td class=\"right_td_colored\">Specify the data center names from your properties file in the keyspace strategy options.</td>\n  </tr><tr><td class=\"left_td_colored\">RackInferringSnitch</td>\n   <td class=\"right_td_colored\">Specify the second octet of the IPv4 address in your keyspace strategy options.</td>\n  </tr><tr><td class=\"left_td_colored\">EC2Snitch</td>\n   <td class=\"right_td_colored\">Specify the region name in the keyspace strategy options.</td>\n  </tr></tbody></table><h3>SimpleSnitch</h3><p>The SimpleSnitch provides Cassandra no information regarding racks or data centers.  It is the default setting and is useful for simple deployments where all servers are collocated.</p><h3>PropertyFileSnitch</h3><p>The PropertyFileSnitch allows users to be explicit about their network topology.  The user specifies the topology in a properties file, <em>cassandra-topology.properties. </em> The file specifies which nodes belong to which racks and data centers.  Below is an example property file for our sample cluster.</p><p><em># DC1<br />192.168.0.1=DC1:RAC1<br />192.168.0.2=DC1:RAC1<br />192.168.0.3=DC1:RAC2</em></p><p># DC2<br />192.168.1.4=DC2:RAC3<br />192.168.1.5=DC2:RAC3<br />192.168.1.6=DC2:RAC4</p><p># Default for nodes<br />default=DC3:RAC5 </p><h3>RackInferringSnitch</h3><p>The RackInferringSnitch infers network topology by convention.  From the IPv4 address (e.g., 9.100.47.75), the snitch uses the following convention to identify the data center and rack:</p><table border=\"1\" style=\"width: 221px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 46px;\">Octet</td>\n   <td class=\"right_th_colored\" style=\"width: 62px;\">Example</td>\n   <td class=\"right_th_colored\" style=\"width: 91px;\">Indicates</td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\">1</td>\n   <td class=\"right_td_colored\">9</td>\n   <td class=\"right_td_colored\">Nothing</td>\n  </tr><tr><td class=\"left_td_colored\">2</td>\n   <td class=\"right_td_colored\">100</td>\n   <td class=\"right_td_colored\">Data Center</td>\n  </tr><tr><td class=\"left_td_colored\">3</td>\n   <td class=\"right_td_colored\">47</td>\n   <td class=\"right_td_colored\">Rack</td>\n  </tr><tr><td class=\"left_td_colored\">4</td>\n   <td class=\"right_td_colored\">75</td>\n   <td class=\"right_td_colored\">Node</td>\n  </tr></tbody></table><h3>EC2Snitch</h3><p>The EC2Snitch is useful for deployments to Amazon's EC2. It uses Amazon's API to examine the regions to which nodes are deployed. It then treats each region as a separate data center.</p><p>EC2MultiRegionSnitch</p><p>Use this snitch for deployments on Amazon EC2 where the cluster spans multiple regions. This snitch treats data centers and availability zones as racks within a data center and uses public IPs as broadcast_address to allow cross-region connectivity. Cassandra nodes in one EC2 region can bind to nodes in another region, thus enabling multi-data center support.</p></div><div class=\"col-md-12 content-html\"><p>Cassandra provides simple primitives. Its simplicity allows it to scale linearly with high availability and very little performance degradation.   That simplicity allows for extremely fast read and write operations for specific keys, but servicing more sophisticated queries that span keys requires pre-planning.</p><p>Using the primitives that Cassandra provides, you can construct indexes that support exactly the query patterns of your application.  Note, however, that queries may not perform well without properly designing your schema. </p><h3>Secondary Indexes</h3><p>To satisfy simple query patterns, Cassandra provides a native indexing capability called Secondary Indexes. A column family may have multiple secondary indexes. A secondary index is hash-based and uses specific columns to provide a reverse lookup mechanism from a specific column value to the relevant row keys. Under the hood, Cassandra maintains hidden column families that store the index. The strength of Secondary Indexes is allowing queries by value. Secondary indexes are built in the background automatically without blocking reads or writes. To create a Secondary Index using CQL is straight-forward. For example, define a table of data about movie fans, and then create a secondary index of states where they live:</p><p>CREATE TABLE fans ( watcherID uuid, favorite_actor text, address text, zip int, state text PRIMARY KEY (watcherID) );</p><p>CREATE INDEX watcher_state ON fans (state);</p><h3>Range Queries</h3><p>It is important to consider partitioning when designing your schema to support range queries.</p><h4>Range Queries with Order Preservation</h4><p>Since order is preserved, order preserving partitioners better supports range queries across a range of rows.  Cassandra only needs to retrieve data from the subset of nodes responsible for that range.  For example, if we are querying against a column family keyed by phone number and we want to find all phone numbers between that begin with <em>215-555</em>, we could create a range query with start key <em>215-555-0000 </em>and end key <em>215-555-9999.</em></p><p>To service this request with OrderPreservingPartitioning, it’s possible for Cassandra to compute the two relevant tokens: <em>token(215-555-0000) </em>and<em> token(215-555-9999)</em>.</p><p>Then satisfying that querying simply means consulting nodes responsible for that token range and retrieving the rows/tokens in that range.</p><h4>Range Queries with Random Partitioning</h4><p>The RandomPartitioner provides no guarantees of any kind between keys and tokens.  In fact, ideally row keys are distributed around the token ring evenly.  Thus, the corresponding tokens for a start key and end key are not useful when trying to retrieve the relevant rows from tokens in the ring with the RandomPartitioner.  Consequently, Cassandra must consult all nodes to retrieve the result.  Fortunately, there are well known design patterns to accommodate range queries.  These are described below.</p><h3>Index Patterns</h3><p>There are a few design patterns to implement indexes.  Each services different query patterns.  The patterns leverage the fact that Cassandra columns are always stored in sorted order and all columns for a single row reside on a single host.</p><h3>Inverted Indexes</h3><p>First, let’s consider the <em>inverted index</em> pattern.  In an inverted index, columns in one row become row keys in another.  Consider the following data set, where users IDs are row keys.</p><table border=\"1\" cellpadding=\"0\" style=\"border-spacing: 0px; width: 458px;\"><thead><tr><td class=\"left_th_colored\" colspan=\"4\" valign=\"top\"><p><strong>Column Family: Users</strong></p></td>\n  </tr></thead><thead><tr><td class=\"left_th_colored\" valign=\"top\" style=\"width: 76px;\"><p><strong>RowKey</strong></p></td>\n   <th class=\"right_th_colored\" colspan=\"3\" valign=\"top\"><p><strong>Columns</strong></p></th>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 76px;\"><p>BONE42</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 128px;\"><p>{ name : “Brian”}</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 105px;\"><p>{ zip: 15283}</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 139px;\"><p>{dob : 09/19/1982}</p></td>\n  </tr><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 76px;\"><p>LKEL76</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 128px;\"><p>{ name : “Lisa”}</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 105px;\"><p>{ zip: 98612}</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 139px;\"><p>{dob : 07/23/1993}</p></td>\n  </tr><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 76px;\"><p>COW89</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 128px;\"><p>{ name : “Dennis”}</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 105px;\"><p>{ zip: 98612}</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 139px;\"><p>{dob : 12/25/2004}</p></td>\n  </tr></tbody></table><p>Without indexes, searching for users in a specific Zip Code would mean scanning our Users column family row-by-row to find the users in the relevant Zip Code.  Obviously, this does not perform well. <br />To remedy the situation, we can create a column family that represents the query we want to perform, inverting rows and columns.  This would result in the following column family.</p><table border=\"1\" cellpadding=\"0\" style=\"border-spacing: 0px; width: 458px;\"><thead><tr><td class=\"left_th_colored\" colspan=\"4\" valign=\"top\"><p><strong>Column Family: Users_by_ZipCode</strong></p></td>\n  </tr></thead><thead><tr><td class=\"left_th_colored\" valign=\"top\" style=\"width: 77px;\"><p><strong>RowKey</strong></p></td>\n   <td class=\"right_th_colored\" colspan=\"3\" valign=\"top\"><p><strong>Columns</strong></p></td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 77px;\"><p>98612</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 137px;\"><p>{ user_id : LKEL76 }</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 139px;\"><p>{ user_id : COW89 }</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 95px;\"></td>\n  </tr><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 77px;\"><p>15283</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 137px;\"><p>{ user_id : BONE42 }</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 139px;\"></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 95px;\"></td>\n  </tr></tbody></table><p>Since each row is stored on a single machine, Cassandra can quickly return all user IDs within a single Zip Code by returning all columns within a single row.  Cassandra simply goes to a single host based on <em>token(zipcode)</em> and returns the contents of that single row.</p><h3>Wide-Row Indexes</h3><p>When working with time series data, consider storing the complete set of data for each event in the timeline itself by serializing the entire event into a single column value or by using composite column names of the form &lt; timestamp &gt; : &lt; event_field &gt;. Unless the data for each event is very large, this approach scales well with large data sets and provides efficient reads. Fetch a time slice of events by reading a contiguous portion of a row on one set of replicas. When you track the same event in multiple timelines, denormalizing and storing all of the event data in each of the timelines works well.</p><table border=\"1\" style=\"width: 455px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 445px;\"><strong>Materialized View Table</strong></td>\n  </tr></thead></table><table border=\"1\" style=\"width: 456px;\"><tbody><tr><td class=\"left_td_colored\" style=\"width: 97px;\">lsmith: 1332960000</td>\n   <td class=\"right_td_colored\" style=\"width: 220px;\">C4e1ee6f-e053-41f5-9890-<br />674636d51095:<br />{\"user\": \"lsmith\", \"body\": \"There<br />are . . . \"}</td>\n   <td class=\"left_td_colored\" style=\"width: 117px;\">39f71a85-7af0 . . .<br />{\"user\": \"lsmith\",<br />\"body\": \"Yes, . . .</td>\n  </tr><tr><td class=\"left_td_colored\">cbrown:<br />1332960000</td>\n   <td class=\"right_td_colored\">e572bad1-f98d-4346-80a0-<br />13e7d37d38d0:<br />{\"user\":\"cbrown\", \"body\": \"My dog<br />is . . .\"}</td>\n   <td class=\"right_td_colored\">aa33bgbfd-8f16 . . .<br />{\"user\":\"cbrown\",<br />\"body\":\"No, . . .</td>\n  </tr></tbody></table><p>When you use composite keys in CQL, Cassandra supports wide Cassandra rows using composite column names. In CQL 3, a primary key can have any number (1 or more) of component columns, but there must be at least one column in the column family that is not part of the primary key. The new wide row technique consumes more storage because for every piece of data stored, the column name is stored along with it.</p><p><em>CREATE TABLE History.events (<br />event uuid PRIMARY KEY,<br />author varchar,<br />body varchar);</em></p><p>CREATE TABLE timeline (<br />user varchar,<br />event uuid,<br />author varchar,<br />body varchar,</p><div class=\"../images/hot_tip.gif\">\n <p><img alt=\"Hot Tip\" class=\"../images/hot_tip.gif_icon fr-dii fr-fil\" src=\"https://dzone.com/storage/rc-covers/14348-thumb.png\" /></p> Wide-Row indexes can cause hotspots in the cluster.  Since the index is a single row, it is stored on a single node (plus replicas).  If that is a heavily used index, those nodes may be overwhelmed. \n</div><h3>Composite-Types in Indexes</h3><p>The previous examples were one-dimensional and used a simple concatenation to illustrate the point.  Instead, you may prefer to use composite keys and/or values in your data model.  </p><p>Using composite keys in indexes, we can create queries along multiple dimensions.  If we combine the previous examples, we could create a <br />single wide-row capable of serving a compound query such as, “How many users within the 18964 Zip Code are older than 21?”</p><p>Simply create a composite type containing the Zip Code and the date of birth and use that as the column name in the index.</p><h3>Denormalization</h3><p>Finally, it is worth noting that each of the indexing strategies as presented would require two steps to service a query if the request requires the actual column data (e.g., user name). The first step would retrieve the keys out of the index. The second step would fetch each relevant column by row key.</p><p>We can skip the second step if we denormalize the data. In Cassandra, denormalization is the norm. If we duplicate the data, the index becomes a true materialized view that is custom tailored to the exact query we need to support.</p></div><div class=\"col-md-12 content-html\"><p>Everything in Cassandra is a write, typically referred to as a mutation. Since Cassandra is effectively a key-value store, operations are simply mutations of a key/value pairs. The column is atomic, but the fundamental unit is a row in the ACID sense. If you have multiple updates to the same key, group the changes into a single update.</p><div class=\"../images/hot_tip.gif\">\n <p><img alt=\"Hot Tip\" class=\"../images/hot_tip.gif_icon fr-dii fr-fil\" src=\"https://dzone.com/storage/rc-covers/14349-thumb.png\" /></p> When performing multiple operations on the same key in sequence, be sure to increment the timestamp! Do not simply specify System.currentTimeMillis().  If the code executes too quickly, the mutations will have the same timestamp and Cassandra will not be able to determine the proper sequencing of events. \n</div><h3>Hinted Handoff</h3><p>Similar to ReadRepair, Hinted Handoff is a background process that ensures data integrity and eventual consistency.  If a replica is down in the cluster and the client requests a consistency level of ANY, a write may still succeed by writing a “hint” to a coordinator node, which will disseminate that data to replicas when they become available.</p></div><div class=\"col-md-12 content-html\"><p>Cassandra provides tools for operations and maintenance.  Some of the maintenance is mandatory because of Cassandra’s eventually consistent architecture.  Other facilities are useful to support alerting and statistics gathering.  Use <em>nodetool</em> to manage Cassandra.  Datastax provides a reference card on nodetool available here:</p><p><a href=\"http://www.datastax.com/wp-content/uploads/2012/01/DS_nodetool_web.pdf\">http://www.datastax.com/wp-content/uploads/2012/01/DS_nodetool_web.pdf</a></p><h3>Nodetool Repair</h3><p>Cassandra keeps record of deleted values for some time to support the eventual consistency of distributed deletes.  These values are called tombstones.  Tombstones are purged after some time (GCGraceSeconds, which defaults to 10 days).  Since tombstones prevent improper data propagation in the cluster, you will want to ensure that you have consistency before they get purged.</p><p>To ensure consistency, run:</p><p><em>&gt;$CASSANDRA_HOME/bin/nodetool repair</em></p><p>The repair command replicates any updates missed due to downtime or loss of connectivity. This command ensures consistency across the cluster and obviates the tombstones. You will want to do this periodically on each node in the cluster (within the window before tombstone purge).</p><h3>Monitoring</h3><p>Cassandra has support for monitoring via JMX, but the simplest way to monitor the Cassandra node is by using OpsCenter, which is designed to manage and monitor Cassandra database clusters. There is a free community edition as well as an enterprise edition that provides management of Apache SOLR and Hadoop.</p><p>Simply download mx4j and execute the following:</p><p><em>cp $MX4J_HOME/lib/mx4j-tools.jar $CASSANDRA_HOME/lib</em></p><p>The following are key attributes to track per column family.</p><table border=\"1\" cellpadding=\"0\" style=\"border-spacing: 0px; width: 422px;\"><thead><tr><td class=\"left_th_colored\" valign=\"top\" style=\"width: 97px;\"><p><strong>Attribute</strong></p></td>\n   <td class=\"right_th_colored\" valign=\"top\" style=\"width: 319px;\"><p><strong>Provides</strong></p></td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 97px;\"><p>Read Count</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 319px;\"><p>Frequency of reads against the column family.</p></td>\n  </tr><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 97px;\"><p>Read Latency</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 319px;\"><p>Latency of reads against the column family.</p></td>\n  </tr><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 97px;\"><p>Write Count</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 319px;\"><p>Frequency of writes against the column family.</p></td>\n  </tr><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 97px;\"><p>Write Latency</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 319px;\"><p>Latency of writes against the column family.</p></td>\n  </tr><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 97px;\"><p>Pending Tasks</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 319px;\"><p>Queue of pending tasks, informative to know if tasks are queuing.</p></td>\n  </tr></tbody></table><h3>Backup</h3><p>OpsCenter facilitates backing up data by providing snapshots of the data. A snapshot creates a new hardlink to every live SSTable. Cassandra also provides online backup facilities using nodetool. To take a snapshot of the data on the cluster, invoke:</p><p><em>$CASSANDRA_HOME/bin/nodetool snapshot</em></p><p>This will create a snapshot directory in each keyspace data directory.  Restoring the snapshot is then a matter of shutting down the node, deleting the commitlogs and the data files in the keyspace, and copying the snapshot files back into the keyspace directory.</p></div><div class=\"col-md-12 content-html\"><p>Cassandra has a very active community developing libraries in different languages.  </p><h3>Java</h3><table border=\"1\" cellpadding=\"0\" style=\"border-spacing: 0px;\"><thead><tr><td class=\"left_th_colored\" valign=\"top\" style=\"width: 76px;\"><p><strong>Client</strong></p></td>\n   <td class=\"right_th_colored\" valign=\"top\" style=\"width: 520px;\"><p><strong>Description</strong></p></td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 76px;\"><p>Astyanax</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 520px;\"><p>Inspired by Hector, Astyanax is a client library developed by the folks at Netflix.   <br /><a href=\"https://github.com/Netflix/astyanax\">https://github.com/Netflix/astyanax</a></p></td>\n  </tr><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 76px;\"><p>Hector</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 520px;\"><p>Hector is one of the first APIs to wrap the underlying Thrift API. Hector is one of the most commonly used client libraries.<br /><a href=\"https://github.com/rantav/hector\">https://github.com/rantav/hector</a></p></td>\n  </tr></tbody></table><h3>CQL</h3><table border=\"1\" cellpadding=\"0\" style=\"border-spacing: 0px; width: 629px;\"><thead><tr><td class=\"left_th_colored\" valign=\"top\" style=\"width: 82px;\"><p><strong>Client</strong></p></td>\n   <th class=\"right_th_colored\" valign=\"top\" style=\"width: 541px;\"><p><strong>Description</strong></p></th>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 82px;\"><p>CQL</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 541px;\"><p>Cassandra provides an SQL-like query language called the Cassandra Query Language (CQL).  The CQL shell allows you to interact with Cassandra as if it were a SQL database.  Start the shell with:<br />&gt;$CASSANDRA_HOME/bin/cqlsh<br />Datastax provides a reference card for CQL available here:<br /><a href=\"http://www.datastax.com/wp-content/uploads/2012/01/DS_CQL_web.pdf\">http://www.datastax.com/wp-content/uploads/2012/01/DS_CQL_web.pdf</a></p></td>\n  </tr></tbody></table><h3>Python</h3><table border=\"1\" cellpadding=\"0\" style=\"border-spacing: 0px; width: 472px;\"><thead><tr><td class=\"left_th_colored\" valign=\"top\" style=\"width: 49px;\"><p><strong>Client</strong></p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 417px;\"><p><strong>Description</strong></p></td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 49px;\"><p>Pycassa</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 417px;\"><p>Pycassa is the most well known Python library for Cassandra.<br /><a href=\"https://github.com/pycassa/pycassa\">https://github.com/pycassa/pycassa</a></p></td>\n  </tr></tbody></table><h3>PHP CQL</h3><table border=\"1\" style=\"width: 473px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 70px;\">Client</td>\n   <td class=\"right_th_colored\" style=\"width: 387px;\">Description</td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\">Cassandra-<br />PDO</td>\n   <td class=\"right_td_colored\">A CQL (Cassandra Query Language) driver for PHP.<br /><a href=\"http://code.google.com/a/apache-extras.org/p/cassandra-pdo/\">http://code.google.com/a/apache-extras.org/p/cassandra-pdo/</a></td>\n  </tr></tbody></table><h3>Ruby</h3><table border=\"1\" cellpadding=\"0\" style=\"border-spacing: 0px; width: 473px;\"><thead><tr><td class=\"left_th_colored\" valign=\"top\" style=\"width: 56px;\"><p><strong>Client</strong></p></td>\n   <td class=\"right_th_colored\" valign=\"top\" style=\"width: 411px;\"><p><strong>Description</strong></p></td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 56px;\"><p>Ruby Gem</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 411px;\"><p>Ruby has support for Cassandra via a gem.<br /><a href=\"http://rubygems.org/gems/cassandra\">http://rubygems.org/gems/cassandra</a></p></td>\n  </tr></tbody></table><h3>REST</h3><table border=\"1\" cellpadding=\"0\" style=\"border-spacing: 0px; width: 468px;\"><thead><tr><td class=\"left_th_colored\" valign=\"top\" style=\"width: 49px;\"><p><strong>Client</strong></p></td>\n   <th class=\"right_th_colored\" valign=\"top\" style=\"width: 413px;\"><p><strong>Description</strong></p></th>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 49px;\"><p>Virgil</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 413px;\"><p>Virgil is a java-based REST client for Cassandra.<br /><a href=\"https://github.com/hmsonline/virgil\">https://github.com/hmsonline/virgil</a></p></td>\n  </tr></tbody></table><h3>Command Line Interface (CLI)</h3><p>Cassandra also provides a Command Line Interface (CLI) through which you can perform all schema related changes. It also allows you to manipulate data. Datastax provides a reference card on the CLI available here:</p><p><a href=\"http://www.datastax.com/wp-content/uploads/2012/01/DS_CLI_web.pdf\">http://www.datastax.com/wp-content/uploads/2012/01/DS_CLI_web.pdf</a></p><h3>Hadoop Support</h3><p>DataStax Enterprise provides Cassandra with an enhanced Hadoop distribution that is compatible with existing HDFS, Hadoop, and Hive tools and utilities. Cassandra also provides out-of-the-box support for Hadoop. To see the canonical word count example, take a look at:</p><p><a href=\"https://github.com/apache/cassandra/tree/trunk/examples/hadoop_word_count\">https://github.com/apache/cassandra/tree/trunk/examples/hadoop_word_count</a></p><h3>DataStax Community Edition</h3><p>DataStax Community Edition provides the latest release from the Apache Cassandra community.</p><ul><li>Binary tarballs for Linux and Mac installation</li>\n <li>Packaged installations for Red Hat Enterprise Linux, CentOS, Debian, and Ubuntu</li>\n <li>A GUI installer for Windows</li>\n</ul><p>RHEL and Debian packages are supported through yum and apt package management tools.he DataStax Community Edition also includes the DataStax OpsCenter.</p></div>",
        "created_at": "2018-07-24T11:19:17+0000",
        "updated_at": "2018-07-24T11:19:22+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 21,
        "domain_name": "dzone.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11064"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 11060,
        "uid": null,
        "title": "Distributed Storage Systems - Cassandra",
        "url": "http://www.eurecom.fr/~michiard/teaching/slides/clouds/cassandra.pdf",
        "content": "ć<br />\n<br />\n \t<br />\n <br />\n <br />\n\t<br />\n \t<br />\n<br />\n<br />\n \t<br />\n <br />\n \t<br />\n  \t<br />\n<br />\n \t<br />\n \t<br />\n \t<br />\n \t<br />\n \t<br />\n<br />\n<br />\n<br />\n\t<br />\n    <br />\n \t<br />\n   \t<br />\n <br />\n  \t<br />\n \t<br />\n <br />\n    \t<br />\n<br />\n<br />\n \t<br />\n  \t<br />\n  \t<br />\n <br />\n<br />\n  \t<br />\n  <br />\n \t<br />\n<br />\n<br />\n <br />\n  \t<br />\n\t<br />\n<br />\n<br />\n<br />\n \t<br />\n <br />\n  <br />\n\t<br />\n \t<br />\n <br />\n<br />\n<br />\n\t\t\t\t<br />\n <br />\n   <br />\n  <br />\n<br />\n<br />\n<br />\n <br />\n  \t<br />\n <br />\n  <br />\n\t<br />\n   \t<br />\n<br />\n<br />\n<br />\n \t<br />\n <br />\n \t<br />\n<br />\n<br />\n<br />\n<br />\n <br />\n <br />\n \t<br />\n<br />\n<br />\n \t<br />\n  \t<br />\n<br />\n<br />\n<br />\n <br />\n  \t<br />\n<br />\n \t<br />\n\t<br />\n  \t<br />\n  <br />\n  <br />\n \t<br />\n<br />\n<br />\n  \t<br />\n <br />\n\t<br />\n  \t<br />\n <br />\n  <br />\n\t<br />\n   <br />\n \t<br />\n<br />\n<br />\n \t<br />\n\t  \t<br />\n <br />\n <br />\n  <br />\n <br />\n <br />\n  \t<br />\n<br />\n<br />\n <br />\n\t<br />\n\t\t\t<br />\n   <br />\n  <br />\n<br />\n<br />\n \t<br />\n <br />\n \t<br />\n\t<br />\n <br />\n  \t<br />\n \t<br />\n <br />\n<br />\n<br />\n <br />\n<br />\n <br />\n<br />\n   <br />\n <br />\n <br />\n<br />\n<br />\n<br />\n<br />\n<br />\n <br />\n<br />\n <br />\n  \t<br />\n <br />\n<br />\n<br />\n <br />\n <br />\n <br />\n\t<br />\n\t\t <br />\n \t<br />\n\t<br />\n <br />\n <br />\n<br />\n<br />\n<br />\n<br />\n \t<br />\n<br />\n<br />\n <br />\n\t<br />\n  \t<br />\n <br />\n<br />\n<br />\n<br />\n<br />\n   \t<br />\n  <br />\n<br />\n<br />\n<br />\n \t<br />\n  <br />\n <br />\n <br />\n \t<br />\n \t<br />\n<br />\n<br />\n \t<br />\n<br />\n<br />\n <br />\n   <br />\n<br />\n <br />\n\t<br />\n   <br />\n \t<br />\n<br />\n<br />\n<br />\n <br />\n <br />\n<br />\n  \t<br />\n <br />\n <br />\n<br />\n<br />\n  \t<br />\n  <br />\n \t<br />\n\t \t\t <br />\n \t<br />\n\t  <br />\n  \t<br />\n  \t<br />\n  <br />\n <br />\n <br />\n<br />\n<br />\n \t<br />\n   <br />\n \t<br />\n\t<br />\n<br />\n<br />\n  \t<br />\n <br />\n <br />\n<br />\n<br />\n \t<br />\n <br />\n <br />\n  <br />\n<br />\n <br />\n     <br />\n \t<br />\n  \t<br />\n<br />\n<br />\n <br />\n  <br />\n\t<br />\n \t\t\t<br />\n  <br />\n<br />\n<br />\n <br />\n  <br />\n  <br />\n<br />\n<br />\n<br />\n <br />\n  \t<br />\n <br />\n <br />\n <br />\n <br />\n<br />\n<br />\n \t<br />\n \t<br />\n \t<br />\n <br />\n \t<br />\n   \t<br />\n <br />\n \t<br />\n<br />\n \t<br />\n<br />\n<br />\n <br />\n  <br />\n   <br />\n\t<br />\n  \t\t\t<br />\n<br />\n<br />\n <br />\n    <br />\n \t<br />\n <br />\n <br />\n <br />\n\t<br />\n   <br />\n \t<br />\n <br />\n<br />\n<br />\n  \t<br />\n<br />\n<br />\n<br />\n  <br />\n   <br />\n   <br />\n <br />\n <br />\n<br />\n<br />\n \t<br />\n  <br />\n    <br />\n \t<br />\n  \t<br />\n <br />\n    <br />\n   <br />\n  <br />\n <br />\n<br />\n<br />\n \t<br />\n <br />\n <br />\n  <br />\n  \t<br />\n<br />\n\t<br />\n<br />\n<br />\n<br />\n <br />\n<br />\n<br />\n<br />\n<br />\n \t<br />\n  <br />\n  <br />\n  <br />\n \t<br />\n  \t<br />\n <br />\n <br />\n<br />\n<br />\n\t<br />\n  <br />\n <br />\n    \t<br />\n <br />\n  \t<br />\n  <br />\n<br />\n<br />\n<br />\n \t<br />\n  <br />\n  \t<br />\n    <br />\n\t<br />\n  \t<br />\n <br />\n <br />\n <br />\n<br />\n<br />\n <br />\n  <br />\n   <br />\n   <br />\n\t<br />\n\t<br />\n<br />\n\t<br />\n \t<br />\n<br />\n  \t<br />\n \t<br />\n \t<br />\n \t<br />\n \t<br />\n<br />\n <br />\n<br />\n<br />\n \t<br />\n \t<br />\n \t<br />\n \t<br />\n<br />\n <br />\n  <br />\n \t<br />\n<br />\n<br />\n \t<br />\n\t<br />\n<br />\n  \t<br />\n \t<br />\n  <br />\n  <br />\n \t<br />\n  \t<br />\n \t<br />\n \t<br />\n<br />\n<br />\n \t<br />\n \t<br />\n \t<br />\n\t<br />\n <br />\n\t<br />\n  \t<br />\n <br />\n <br />\n\t<br />\n<br />\n<br />\n<br />\n<br />\n \t<br />\n <br />\n  <br />\n\t<br />\n  <br />\n <br />\n<br />\n <br />\n<br />\n<br />\n<br />\n <br />\n  <br />\n   <br />\n   <br />\n  <br />\n \t<br />\n  \t<br />\n\t<br />\n \t<br />\n<br />\n<br />\n  \t<br />\n<br />\n <br />\n <br />\n\t<br />\n  <br />\n \t<br />\n",
        "created_at": "2018-07-24T11:15:33+0000",
        "updated_at": "2018-07-24T11:15:59+0000",
        "published_at": "2013-05-17T12:56:23+0000",
        "published_by": [
          "Marko Vukolic"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "application/pdf",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.eurecom.fr",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11060"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 228,
            "label": "dotnet",
            "slug": "net"
          },
          {
            "id": 236,
            "label": "csharp",
            "slug": "csharp"
          }
        ],
        "is_public": false,
        "id": 11055,
        "uid": null,
        "title": "Access Cassandra Data with Entity Framework 6",
        "url": "https://www.cdata.com/kb/tech/cassandra-ado-codefirst.rst",
        "content": "<p>\n     Entity Framework is an object-relational mapping framework that can be used to work with data as objects. While you can run the ADO.NET Entity Data Model wizard in Visual Studio to handle generating the Entity Model, this approach, the model-first approach, can put you at a disadvantage if there are changes in your data source or if you want more control over how the entities operate. In this article you will complete the code-first approach to accessing Cassandra data using the CData ADO.NET Provider.\n\t</p><ol><li>Open Visual Studio and create a new Windows Form Application. This article uses a C# project with .NET 4.5.</li>\n    <li>Run the command 'Install-Package EntityFramework' in the Package Manger Console in Visual Studio to install the latest release of Entity Framework.</li>\n    <li><p>Modify the App.config file in the project to add a reference to the Cassandra Entity Framework 6 assembly and the connection string.</p>\n\t   &#13;\n<p>&#13;\nSet the Server, Port, and Database connection properties to connect to Cassandra. Additionally, to use internal authentication set the User and Password connection properties.&#13;\n</p> \n<code lang=\"xml\" xml:lang=\"xml\">\n&lt;configuration&gt;\n   ... \n  &lt;connectionStrings&gt;\n    &lt;add name=\"CassandraContext\" connectionString=\"Offline=False;Database=MyCassandraDB;Port=7000;Server=127.0.0.1;\" providerName=\"System.Data.CData.Cassandra\" /&gt;\n  &lt;/connectionStrings&gt;\n  &lt;entityFramework&gt;\n    &lt;providers&gt;\n       ... \n      &lt;provider invariantName=\"System.Data.CData.Cassandra\" type=\"System.Data.CData.Cassandra.CassandraProviderServices, System.Data.CData.Cassandra.Entities.EF6\" /&gt;\n    &lt;/providers&gt;\n  &lt;entityFramework&gt;\n&lt;/configuration&gt;\n&lt;/code&gt; \n</code>\n    </li>\n    <li>Add a reference to System.Data.CData.Cassandra.Entities.EF6.dll, located in the lib -&gt; 4.0 subfolder in the installation directory.\n    </li><li>\nBuild the project at this point to ensure everything is working correctly.  Once that's done, you can start coding using Entity Framework.\n  </li>\n\t\t<li>Add a new .cs file to the project and add a class to it. This will be your database context, and it will extend the DbContext class. In the example, this class is named CassandraContext. The following code example overrides the OnModelCreating method to make the following changes:\n<ul><li>Remove PluralizingTableNameConvention from the ModelBuilder Conventions.\n</li><li>Remove requests to the MigrationHistory table.\n</li></ul><code lang=\"csharp\" xml:lang=\"csharp\">\nusing System.Data.Entity;\nusing System.Data.Entity.Infrastructure;\nusing System.Data.Entity.ModelConfiguration.Conventions;\nclass CassandraContext : DbContext {\n  public CassandraContext() { }\n  protected override void OnModelCreating(DbModelBuilder modelBuilder)\n  {\n    // To remove the requests to the Migration History table\n    Database.SetInitializer&lt;CassandraContext&gt;(null);  \n    // To remove the plural names    \n    modelBuilder.Conventions.Remove&lt;PluralizingTableNameConvention&gt;();\n  }  \n}\n</code>\n  </li>\n    <li>Create another .cs file and name it after the Cassandra entity you are retrieving, for example, Customer. In this file, define both the Entity and the Entity Configuration, which will resemble the example below:\n    <code lang=\"csharp\" xml:lang=\"csharp\">\nusing System.Data.Entity.ModelConfiguration;\nusing System.ComponentModel.DataAnnotations.Schema;\n[System.ComponentModel.DataAnnotations.Schema.Table(\"Customer\")]\npublic class Customer {\n  [System.ComponentModel.DataAnnotations.Key] \n  public System.String Id { get; set; }\n  public System.String City { get; set; }\n}\n    \n</code>\n  </li>\n    <li>Now that you have created an entity, add the entity to your context class:\n  \n<code lang=\"csharp\" xml:lang=\"csharp\">\npublic DbSet&lt;Customer&gt; Customer { set; get; }\n</code>\n  </li>\n    <li>With the context and entity finished, you are now ready to query the data in a separate class. For example: \n\t\n<code lang=\"csharp\" xml:lang=\"csharp\">\nCassandraContext context = new CassandraContext();\ncontext.Configuration.UseDatabaseNullSemantics = true;\nvar query = from line in context.Customer select line;\n</code>\n    </li>\n  </ol>",
        "created_at": "2018-07-24T11:06:06+0000",
        "updated_at": "2018-07-24T11:06:15+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 2,
        "domain_name": "www.cdata.com",
        "preview_picture": "https://www.cdata.com///www.cdata.com/kb/img/default.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11055"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 907,
            "label": "mesos",
            "slug": "mesos"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 963,
            "label": "akka",
            "slug": "akka"
          }
        ],
        "is_public": false,
        "id": 11048,
        "uid": null,
        "title": "Data processing platforms architectures with Spark, Mesos, Akka, Cass…",
        "url": "https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka",
        "content": "Data processing platforms architectures with Spark, Mesos, Akka, Cass…\n      \n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            </a><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div>\n    <div class=\"wrapper\"><p>Successfully reported this slideshow.</p><div id=\"slideview-container\" class=\"\"><div class=\"row\"><div id=\"main-panel\" class=\"small-12 large-8 columns\"><div class=\"sectionElements\"><div class=\"playerWrapper\"><div><div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\">Data processing platforms architectures with Spark, Mesos, Akka, Cassandra and Kafka<div class=\"stage valign-first-slide\"><div class=\"slide_container\"><section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-1-638.jpg?cb=1441958613\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-1-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-1-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-1-1024.jpg?cb=1441958613\" alt=\"SMACK Architectures&#10;Building data processing platforms with&#10;Spark, Mesos, Akka, Cassandra and Kafka&#10;Anton Kirillov Big Dat...\" /></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-2-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-2-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-2-1024.jpg?cb=1441958613\" alt=\"Who is this guy?&#10;● Scala programmer&#10;● Focused on distributed systems&#10;● Building data platforms with SMACK/Hadoop&#10;● Ph.D. i...\" /></i></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-3-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-3-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-3-1024.jpg?cb=1441958613\" alt=\"Roadmap&#10;● SMACK stack overview&#10;● Storage layer layout&#10;● Fixing NoSQL limitations&#10;● Cluster resource management&#10;● Reliable ...\" /></i></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-4-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-4-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-4-1024.jpg?cb=1441958613\" alt=\"SMACK Stack&#10;● Spark - fast and general engine for distributed, large-scale data&#10;processing&#10;● Mesos - cluster resource mana...\" /></i></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-5-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-5-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-5-1024.jpg?cb=1441958613\" alt=\"Storage Layer: Cassandra&#10;● optimized for heavy write&#10;loads&#10;● configurable CA (CAP)&#10;● linearly scalable&#10;● XDCR support&#10;● ea...\" /></i></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-6-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-6-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-6-1024.jpg?cb=1441958613\" alt=\"Cassandra Data Model&#10;● nested sorted map&#10;● should be optimized for&#10;read queries&#10;● data is distributed across&#10;nodes by part...\" /></i></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-7-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-7-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-7-1024.jpg?cb=1441958613\" alt=\"Spark/Cassandra Example&#10;● calculate total views per&#10;campaign for given month&#10;for all campaigns&#10;CREATE TABLE event(&#10;id uuid...\" /></i></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-8-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-8-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-8-1024.jpg?cb=1441958613\" alt=\"Naive Lambda example with Spark SQL&#10;case class CampaignReport(id: String, views: Long, clicks: Long)&#10;sql(&quot;&quot;&quot;SELECT campaig...\" /></i></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-9-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-9-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-9-1024.jpg?cb=1441958613\" alt=\"Let’s take a step back: Spark Basics&#10;● RDD operations(transformations and actions) form DAG&#10;● DAG is split into stages of ...\" /></i></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-10-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-10-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-10-1024.jpg?cb=1441958613\" alt=\"Architecture of Spark/Cassandra Clusters&#10;Separate Write &amp; Analytics:&#10;● clusters can be scaled&#10;independently&#10;● data is repl...\" /></i></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-11-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-11-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-11-1024.jpg?cb=1441958613\" alt=\"Spark Applications Deployment Revisited&#10;Cluster Manager:&#10;● Spark Standalone&#10;● YARN&#10;● Mesos&#10;11&#10;\" /></i></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-12-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-12-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-12-1024.jpg?cb=1441958613\" alt=\"Managing Cluster Resources: Mesos&#10;● heterogenous workloads&#10;● full cluster utilization&#10;● static vs. dynamic resource&#10;alloca...\" /></i></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-13-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-13-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-13-1024.jpg?cb=1441958613\" alt=\"Mesos Architecture Overview&#10;● leader election and&#10;service discovery via&#10;ZooKeeper&#10;● slaves publish available&#10;resources to ...\" /></i></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-14-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-14-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-14-1024.jpg?cb=1441958613\" alt=\"Bringing Spark, Mesos and Cassandra Together&#10;Deployment example&#10;● Mesos Masters and&#10;ZooKeepers collocated&#10;● Mesos Slaves a...\" /></i></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-15-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-15-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-15-1024.jpg?cb=1441958613\" alt=\"Marathon&#10;● long running tasks&#10;execution&#10;● HA mode with ZooKeeper&#10;● Docker executor&#10;● REST API&#10;15&#10;\" /></i></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-16-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-16-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-16-1024.jpg?cb=1441958613\" alt=\"Chronos&#10;● distributed cron&#10;● HA mode with ZooKeeper&#10;● supports graphs of jobs&#10;● sensitive to network failures&#10;16&#10;\" /></i></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-17-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-17-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-17-1024.jpg?cb=1441958613\" alt=\"More Mesos frameworks&#10;● Hadoop&#10;● Cassandra&#10;● Kafka&#10;● Myriad: YARN on Mesos&#10;● Storm&#10;● Samza&#10;17&#10;\" /></i></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-18-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-18-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-18-1024.jpg?cb=1441958613\" alt=\"Data ingestion: endpoints to consume the data&#10;Endpoint requirements:&#10;● high throughput&#10;● resiliency&#10;● easy scalability&#10;● b...\" /></i></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-19-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-19-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-19-1024.jpg?cb=1441958613\" alt=\"Akka features&#10;class JsonParserActor extends Actor {&#10;def receive = {&#10;case s: String =&gt; Try(Json.parse(s).as[Event]) match {...\" /></i></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-20-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-20-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-20-1024.jpg?cb=1441958613\" alt=\"Writing to Cassandra with Akka&#10;class CassandraWriterActor extends Actor with ActorLogging {&#10;//for demo purposes, session i...\" /></i></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-21-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-21-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-21-1024.jpg?cb=1441958613\" alt=\"Cassandra meets Batch Processing&#10;● writing raw data (events) to Cassandra with Akka is easy&#10;● but computation time of aggr...\" /></i></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-22-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-22-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-22-1024.jpg?cb=1441958613\" alt=\"Kafka: distributed commit log&#10;● pre-aggregation of incoming data&#10;● consumers read data in batches&#10;● available as Kinesis o...\" /></i></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-23-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-23-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-23-1024.jpg?cb=1441958613\" alt=\"Publishing to Kafka with Akka Http&#10;val config = new ProducerConfig(KafkaConfig())&#10;lazy val producer = new KafkaProducer[A,...\" /></i></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-24-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-24-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-24-1024.jpg?cb=1441958613\" alt=\"Spark Streaming&#10;24&#10;● variety of data sources&#10;● at-least-once semantics&#10;● exactly-once semantics&#10;available with Kafka Direc...\" /></i></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-25-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-25-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-25-1024.jpg?cb=1441958613\" alt=\"Spark Streaming: Kinesis example&#10;val ssc = new StreamingContext(conf, Seconds(10))&#10;val kinesisStream = KinesisUtils.create...\" /></i></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-26-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-26-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-26-1024.jpg?cb=1441958613\" alt=\"Designing for Failure: Backups and Patching&#10;● be prepared for failures and broken data&#10;● design backup and patching strate...\" /></i></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-27-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-27-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-27-1024.jpg?cb=1441958613\" alt=\"Restoring backup from S3&#10;val sc = new SparkContext(conf)&#10;sc.textFile(s&quot;s3n://bucket/2015/*/*.gz&quot;)&#10;.map(s =&gt; Try(JsonUtils....\" /></i></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-28-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-28-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-28-1024.jpg?cb=1441958613\" alt=\"The big picture&#10;28&#10;\" /></i></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-29-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-29-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-29-1024.jpg?cb=1441958613\" alt=\"So what SMACK is&#10;● concise toolbox for wide variety of data processing scenarios&#10;● battle-tested and widely used software ...\" /></i></section><section data-index=\"30\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-30-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-30-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-30-1024.jpg?cb=1441958613\" alt=\"Questions&#10;@antonkirillov datastrophic.io&#10;30&#10;\" /></i></section><div class=\"j-next-container next-container\"><div class=\"content-container\"><div class=\"next-slideshow-wrapper\"><div class=\"j-next-slideshow next-slideshow\"><p>Upcoming SlideShare</p></div><p>Loading in …5</p><p>×</p></div></div></div></div></div></div></div></div></div><div class=\"slideshow-info-container\" itemscope=\"itemscope\" itemtype=\"https://schema.org/MediaObject\"><div class=\"slideshow-tabs-container show-for-medium-up\"><ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    4 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\">\n                \n                  132 Likes\n                \n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\">\n                Statistics\n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\">\n                Notes\n              </i></a>\n            </li>\n          </ul><div class=\"tabs-content\"><div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\"><ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"itemscope\"><li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"IlyaAshikhmin1\" rel=\"nofollow\" href=\"https://www.slideshare.net/IlyaAshikhmin1?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            IlyaAshikhmin1\n                            \n                              \n                                \n                                \n                              \n                              \n                                \n                                \n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"rajarp9\" rel=\"nofollow\" href=\"https://www.slideshare.net/rajarp9?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Raja Rp\n                            \n                              \n                                \n                                \n                              \n                              \n                                 at \n                                Infosys\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"WalterDiCarlo\" rel=\"nofollow\" href=\"https://www.slideshare.net/WalterDiCarlo?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Walter Di Carlo\n                            \n                              \n                                , \n                                Software Analyst/Developer Consultant presso Nokia S.p.A.\n                              \n                              \n                                 at \n                                Nokia\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"MathieuGoeminne\" rel=\"nofollow\" href=\"https://www.slideshare.net/MathieuGoeminne?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Mathieu Goeminne\n                            \n                              \n                                , \n                                Senior R&amp;D Engineer chez CETIC\n                              \n                              \n                                 at \n                                CETIC\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"KevinNguyen100\" rel=\"nofollow\" href=\"https://www.slideshare.net/KevinNguyen100?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Kevin Nguyen\n                            \n                              \n                                , \n                                Vice Manager at FPT Telecom NOC\n                              \n                              \n                                 at \n                                FPT Telecom\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n              </ul><div class=\"more-container text-center\"><a href=\"#\" class=\"j-more-favs\">\n                    Show More\n                    \n                  </a></div></div><div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No Downloads</p></div><div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No notes for slide</p></div></div></div><div class=\"notranslate transcript add-padding-right j-transcript\"><ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    SMACK Architectures\nBuilding data processing platforms with\nSpark, Mesos, Akka, Cassandra and Kafka\nAnton Kirillov Big Data AW Meetup\nSep 2015\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-2-638.jpg?cb=1441958613\" title=\"Who is this guy?&#10;● Scala programmer&#10;● Focused on distribute...\" target=\"_blank\">\n        2.\n      </a>\n    Who is this guy?\n● Scala programmer\n● Focused on distributed systems\n● Building data platforms with SMACK/Hadoop\n● Ph.D. in Computer Science\n● Big Data engineer/consultant at Big Data AB\n● Currently at Ooyala Stockholm (Videoplaza AB)\n● Working with startups\n2\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-3-638.jpg?cb=1441958613\" title=\"Roadmap&#10;● SMACK stack overview&#10;● Storage layer layout&#10;● Fix...\" target=\"_blank\">\n        3.\n      </a>\n    Roadmap\n● SMACK stack overview\n● Storage layer layout\n● Fixing NoSQL limitations\n● Cluster resource management\n● Reliable scheduling and execution\n● Data ingestion options\n● Preparing for failures\n3\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-4-638.jpg?cb=1441958613\" title=\"SMACK Stack&#10;● Spark - fast and general engine for distribut...\" target=\"_blank\">\n        4.\n      </a>\n    SMACK Stack\n● Spark - fast and general engine for distributed, large-scale data\nprocessing\n● Mesos - cluster resource management system that provides efficient\nresource isolation and sharing across distributed applications\n● Akka - a toolkit and runtime for building highly concurrent, distributed,\nand resilient message-driven applications on the JVM\n● Cassandra - distributed, highly available database designed to handle\nlarge amounts of data across multiple datacenters\n● Kafka - a high-throughput, low-latency distributed messaging system\ndesigned for handling real-time data feeds\n4\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-5-638.jpg?cb=1441958613\" title=\"Storage Layer: Cassandra&#10;● optimized for heavy write&#10;loads&#10;...\" target=\"_blank\">\n        5.\n      </a>\n    Storage Layer: Cassandra\n● optimized for heavy write\nloads\n● configurable CA (CAP)\n● linearly scalable\n● XDCR support\n● easy cluster resizing and\ninter-DC data migration\n5\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-6-638.jpg?cb=1441958613\" title=\"Cassandra Data Model&#10;● nested sorted map&#10;● should be optimi...\" target=\"_blank\">\n        6.\n      </a>\n    Cassandra Data Model\n● nested sorted map\n● should be optimized for\nread queries\n● data is distributed across\nnodes by partition key\nCREATE TABLE campaign(\nid uuid,\nyear int,\nmonth int,\nday int,\nviews bigint,\nclicks bigint,\nPRIMARY KEY (id, year, month, day)\n);\nINSERT INTO campaign(id, year, month, day, views, clicks)\nVALUES(40b08953-a…,2015, 9, 10, 1000, 42);\nSELECT views, clicks FROM campaign\nWHERE id=40b08953-a… and year=2015 and month&gt;8; 6\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-7-638.jpg?cb=1441958613\" title=\"Spark/Cassandra Example&#10;● calculate total views per&#10;campaig...\" target=\"_blank\">\n        7.\n      </a>\n    Spark/Cassandra Example\n● calculate total views per\ncampaign for given month\nfor all campaigns\nCREATE TABLE event(\nid uuid,\nad_id uuid,\ncampaign uuid,\nts bigint,\ntype text,\nPRIMARY KEY(id)\n);\nval sc = new SparkContext(conf)\ncase class Event(id: UUID, ad_id: UUID, campaign: UUID, ts: Long, `type`: String)\nsc.cassandraTable[Event](\"keyspace\", \"event\")\n.filter(e =&gt; e.`type` == \"view\" &amp;&amp; checkMonth(e.ts))\n.map(e =&gt; (e.campaign, 1))\n.reduceByKey(_ + _)\n.collect() 7\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-8-638.jpg?cb=1441958613\" title=\"Naive Lambda example with Spark SQL&#10;case class CampaignRepo...\" target=\"_blank\">\n        8.\n      </a>\n    Naive Lambda example with Spark SQL\ncase class CampaignReport(id: String, views: Long, clicks: Long)\nsql(\"\"\"SELECT campaign.id as id, campaign.views as views,\ncampaign.clicks as clicks, event.type as type\nFROM campaign\nJOIN event ON campaign.id = event.campaign\n\"\"\").rdd\n.groupBy(row =&gt; row.getAs[String](\"id\"))\n.map{ case (id, rows) =&gt;\nval views = rows.head.getAs[Long](\"views\")\nval clicks = rows.head.getAs[Long](\"clicks\")\nval res = rows.groupBy(row =&gt; row.getAs[String](\"type\")).mapValues(_.size)\nCampaignReport(id, views = views + res(\"view\"), clicks = clicks + res(\"click\"))\n}.saveToCassandra(“keyspace”, “campaign_report”)\n8\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-9-638.jpg?cb=1441958613\" title=\"Let’s take a step back: Spark Basics&#10;● RDD operations(trans...\" target=\"_blank\">\n        9.\n      </a>\n    Let’s take a step back: Spark Basics\n● RDD operations(transformations and actions) form DAG\n● DAG is split into stages of tasks which are then submitted to cluster manager\n● stages combine tasks which don’t require shuffling/repartitioning\n● tasks run on workers and results then return to client 9\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-10-638.jpg?cb=1441958613\" title=\"Architecture of Spark/Cassandra Clusters&#10;Separate Write &amp; A...\" target=\"_blank\">\n        10.\n      </a>\n    Architecture of Spark/Cassandra Clusters\nSeparate Write &amp; Analytics:\n● clusters can be scaled\nindependently\n● data is replicated by\nCassandra asynchronously\n● Analytics has different\nRead/Write load patterns\n● Analytics contains additional\ndata and processing results\n● Spark resource impact\nlimited to only one DC\nTo fully facilitate Spark-C* connector data locality awareness,\nSpark workers should be collocated with Cassandra nodes 10\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-11-638.jpg?cb=1441958613\" title=\"Spark Applications Deployment Revisited&#10;Cluster Manager:&#10;● ...\" target=\"_blank\">\n        11.\n      </a>\n    Spark Applications Deployment Revisited\nCluster Manager:\n● Spark Standalone\n● YARN\n● Mesos\n11\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-12-638.jpg?cb=1441958613\" title=\"Managing Cluster Resources: Mesos&#10;● heterogenous workloads&#10;...\" target=\"_blank\">\n        12.\n      </a>\n    Managing Cluster Resources: Mesos\n● heterogenous workloads\n● full cluster utilization\n● static vs. dynamic resource\nallocation\n● fault tolerance and disaster\nrecovery\n● single resource view at\ndatacenter levelimage source: http://www.slideshare.net/caniszczyk/apache-mesos-at-twitter-texas-linuxfest-2014\n12\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-13-638.jpg?cb=1441958613\" title=\"Mesos Architecture Overview&#10;● leader election and&#10;service d...\" target=\"_blank\">\n        13.\n      </a>\n    Mesos Architecture Overview\n● leader election and\nservice discovery via\nZooKeeper\n● slaves publish available\nresources to master\n● master sends resource\noffers to frameworks\n● scheduler replies with\ntasks and resources\nneeded per task\n● master sends tasks to\nslaves\n13\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-14-638.jpg?cb=1441958613\" title=\"Bringing Spark, Mesos and Cassandra Together&#10;Deployment exa...\" target=\"_blank\">\n        14.\n      </a>\n    Bringing Spark, Mesos and Cassandra Together\nDeployment example\n● Mesos Masters and\nZooKeepers collocated\n● Mesos Slaves and Cassandra\nnodes collocated to enforce\nbetter data locality for Spark\n● Spark binaries deployed to all\nworker nodes and spark-env is\nconfigured\n● Spark Executor JAR uploaded\nto S3\nInvocation example\nspark-submit --class io.datastrophic.SparkJob /etc/jobs/spark-jobs.jar\n14\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-15-638.jpg?cb=1441958613\" title=\"Marathon&#10;● long running tasks&#10;execution&#10;● HA mode with ZooK...\" target=\"_blank\">\n        15.\n      </a>\n    Marathon\n● long running tasks\nexecution\n● HA mode with ZooKeeper\n● Docker executor\n● REST API\n15\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-16-638.jpg?cb=1441958613\" title=\"Chronos&#10;● distributed cron&#10;● HA mode with ZooKeeper&#10;● suppo...\" target=\"_blank\">\n        16.\n      </a>\n    Chronos\n● distributed cron\n● HA mode with ZooKeeper\n● supports graphs of jobs\n● sensitive to network failures\n16\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-17-638.jpg?cb=1441958613\" title=\"More Mesos frameworks&#10;● Hadoop&#10;● Cassandra&#10;● Kafka&#10;● Myriad...\" target=\"_blank\">\n        17.\n      </a>\n    More Mesos frameworks\n● Hadoop\n● Cassandra\n● Kafka\n● Myriad: YARN on Mesos\n● Storm\n● Samza\n17\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-18-638.jpg?cb=1441958613\" title=\"Data ingestion: endpoints to consume the data&#10;Endpoint requ...\" target=\"_blank\">\n        18.\n      </a>\n    Data ingestion: endpoints to consume the data\nEndpoint requirements:\n● high throughput\n● resiliency\n● easy scalability\n● back pressure 18\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-19-638.jpg?cb=1441958613\" title=\"Akka features&#10;class JsonParserActor extends Actor {&#10;def rec...\" target=\"_blank\">\n        19.\n      </a>\n    Akka features\nclass JsonParserActor extends Actor {\ndef receive = {\ncase s: String =&gt; Try(Json.parse(s).as[Event]) match {\ncase Failure(ex) =&gt; log.error(ex)\ncase Success(event) =&gt; sender ! event\n}\n}\n}\nclass HttpActor extends Actor {\ndef receive = {\ncase req: HttpRequest =&gt;\nsystem.actorOf(Props[JsonParserActor]) ! req.body\ncase e: Event =&gt;\nsystem.actorOf(Props[CassandraWriterActor]) ! e\n}\n}\n● actor model\nimplementation for JVM\n● message-based and\nasynchronous\n● no shared mutable state\n● easy scalability from one\nprocess to cluster of\nmachines\n● actor hierarchies with\nparental supervision\n● not only concurrency\nframework:\n○ akka-http\n○ akka-streams\n○ akka-persistence\n19\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-20-638.jpg?cb=1441958613\" title=\"Writing to Cassandra with Akka&#10;class CassandraWriterActor e...\" target=\"_blank\">\n        20.\n      </a>\n    Writing to Cassandra with Akka\nclass CassandraWriterActor extends Actor with ActorLogging {\n//for demo purposes, session initialized here\nval session = Cluster.builder()\n.addContactPoint(\"cassandra.host\")\n.build()\n.connect()\noverride def receive: Receive = {\ncase event: Event =&gt;\nval statement = new SimpleStatement(event.createQuery)\n.setConsistencyLevel(ConsistencyLevel.QUORUM)\nTry(session.execute(statement)) match {\ncase Failure(ex) =&gt; //error handling code\ncase Success =&gt; sender ! WriteSuccessfull\n}\n}\n} 20\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-21-638.jpg?cb=1441958613\" title=\"Cassandra meets Batch Processing&#10;● writing raw data (events...\" target=\"_blank\">\n        21.\n      </a>\n    Cassandra meets Batch Processing\n● writing raw data (events) to Cassandra with Akka is easy\n● but computation time of aggregations/rollups will grow with\namount of data\n● Cassandra is still designed for fast serving but not batch\nprocessing, so pre-aggregation of incoming data is needed\n● actors are not suitable for performing aggregation due to\nstateless design model\n● micro-batches partially solve the problem\n● reliable storage for raw data is still needed\n21\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-22-638.jpg?cb=1441958613\" title=\"Kafka: distributed commit log&#10;● pre-aggregation of incoming...\" target=\"_blank\">\n        22.\n      </a>\n    Kafka: distributed commit log\n● pre-aggregation of incoming data\n● consumers read data in batches\n● available as Kinesis on AWS\n22\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-23-638.jpg?cb=1441958613\" title=\"Publishing to Kafka with Akka Http&#10;val config = new Produce...\" target=\"_blank\">\n        23.\n      </a>\n    Publishing to Kafka with Akka Http\nval config = new ProducerConfig(KafkaConfig())\nlazy val producer = new KafkaProducer[A, A](config)\nval topic = “raw_events”\nval routes: Route = {\npost{\ndecodeRequest{\nentity(as[String]){ str =&gt;\nJsonParser.parse(str).validate[Event] match {\ncase s: JsSuccess[String] =&gt; producer.send(new KeyedMessage(topic, str))\ncase e: JsError =&gt; BadRequest -&gt; JsError.toFlatJson(e).toString()\n}\n}\n}\n}\n}\nobject AkkaHttpMicroservice extends App with Service {\nHttp().bindAndHandle(routes, config.getString(\"http.interface\"), config.getInt(\"http.\nport\"))\n}\n23\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-24-638.jpg?cb=1441958613\" title=\"Spark Streaming&#10;24&#10;● variety of data sources&#10;● at-least-onc...\" target=\"_blank\">\n        24.\n      </a>\n    Spark Streaming\n24\n● variety of data sources\n● at-least-once semantics\n● exactly-once semantics\navailable with Kafka Direct\nand idempotent storage\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-25-638.jpg?cb=1441958613\" title=\"Spark Streaming: Kinesis example&#10;val ssc = new StreamingCon...\" target=\"_blank\">\n        25.\n      </a>\n    Spark Streaming: Kinesis example\nval ssc = new StreamingContext(conf, Seconds(10))\nval kinesisStream = KinesisUtils.createStream(ssc,appName,streamName,\nendpointURL,regionName, InitialPositionInStream.LATEST,\nDuration(checkpointInterval), StorageLevel.MEMORY_ONLY)\n}\n//transforming given stream to Event and saving to C*\nkinesisStream.map(JsonUtils.byteArrayToEvent)\n.saveToCassandra(keyspace, table)\nssc.start()\nssc.awaitTermination()\n25\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-26-638.jpg?cb=1441958613\" title=\"Designing for Failure: Backups and Patching&#10;● be prepared f...\" target=\"_blank\">\n        26.\n      </a>\n    Designing for Failure: Backups and Patching\n● be prepared for failures and broken data\n● design backup and patching strategies upfront\n● idempotece should be enforced 26\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-27-638.jpg?cb=1441958613\" title=\"Restoring backup from S3&#10;val sc = new SparkContext(conf)&#10;sc...\" target=\"_blank\">\n        27.\n      </a>\n    Restoring backup from S3\nval sc = new SparkContext(conf)\nsc.textFile(s\"s3n://bucket/2015/*/*.gz\")\n.map(s =&gt; Try(JsonUtils.stringToEvent(s)))\n.filter(_.isSuccess).map(_.get)\n.saveToCassandra(config.keyspace, config.table)\n27\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-28-638.jpg?cb=1441958613\" title=\"The big picture&#10;28&#10;\" target=\"_blank\">\n        28.\n      </a>\n    The big picture\n28\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-29-638.jpg?cb=1441958613\" title=\"So what SMACK is&#10;● concise toolbox for wide variety of data...\" target=\"_blank\">\n        29.\n      </a>\n    So what SMACK is\n● concise toolbox for wide variety of data processing scenarios\n● battle-tested and widely used software with large communities\n● easy scalability and replication of data while preserving low latencies\n● unified cluster management for heterogeneous loads\n● single platform for any kind of applications\n● implementation platform for different architecture designs\n● really short time-to-market (e.g. for MVP verification)\n29\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-30-638.jpg?cb=1441958613\" title=\"Questions&#10;@antonkirillov datastrophic.io&#10;30&#10;\" target=\"_blank\">\n        30.\n      </a>\n    Questions\n@antonkirillov datastrophic.io\n30\n \n  </li>\n              </ol></div></div></div><aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\">\n<dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\"><ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"52661852\" title=\"100 Courses and Counting: David Rivers on Elearning\" href=\"https://www.linkedin.com/learning/100-courses-and-counting-david-rivers-on-elearning?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_100 Courses and Counting: David Rivers on Elearning\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"100 Courses and Counting: David Rivers on Elearning\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=R7g5ycQkdBoUBm%2BKSKfr%2BU9%2BcPo%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-gUyWo-9efYX_pe8bdZLSiol4TeyQHmQMyeuauQDDgEI69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>100 Courses and Counting: David Rivers on Elearning</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"52661852\" title=\"SMART Board Essential Training\" href=\"https://www.linkedin.com/learning/smart-board-essential-training?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_SMART Board Essential Training\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"SMART Board Essential Training\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=L8W%2BAILfUItMtiIFfmNzSmnQWEA%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-gXyGq_d2fYXPtecDXZLSioV8QfiwHlQIzfO6sQzToF469LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>SMART Board Essential Training</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"52661852\" title=\"Teaching Techniques: Writing Effective Learning Objectives\" href=\"https://www.linkedin.com/learning/teaching-techniques-writing-effective-learning-objectives?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Teaching Techniques: Writing Effective Learning Objectives\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Teaching Techniques: Writing Effective Learning Objectives\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=8g9LchVQPqjokhBzWyb8MRYUQls%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lXySs-tCfZHPof8faZLSiol8QcS4DkAQ7feitRzXjEI69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Teaching Techniques: Writing Effective Learning Objectives</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"53162817\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"How to deploy Apache Spark  to Mesos/DCOS\" href=\"https://www.slideshare.net/Typesafe_Inc/how-to-deploy-apache-spark-to-mesosdcos\">\n    \n    <div class=\"related-content\"><p>How to deploy Apache Spark  to Mesos/DCOS</p><p>Legacy Typesafe (now Lightbend)</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"55353060\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Rethinking Streaming Analytics For Scale\" href=\"https://www.slideshare.net/helenaedelson/rethinking-streaming-analytics-for-scale\">\n    \n    <div class=\"related-content\"><p>Rethinking Streaming Analytics For Scale</p><p>Helena Edelson</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"54999973\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Intro to Apache Spark\" href=\"https://www.slideshare.net/MammothData/intro-to-apache-spark-54999973\">\n    \n    <div class=\"related-content\"><p>Intro to Apache Spark</p><p>Mammoth Data</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"53371129\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Reactive app using actor model &amp; apache spark\" href=\"https://www.slideshare.net/RahulKumar405/reactive-app-using-actor-model-apache-spark\">\n    \n    <div class=\"related-content\"><p>Reactive app using actor model &amp; apache spark</p><p>Rahul Kumar</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"54590394\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Streaming Analytics with Spark, Kafka, Cassandra and Akka\" href=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\">\n    \n    <div class=\"related-content\"><p>Streaming Analytics with Spark, Kafka, Cassandra and Akka</p><p>Helena Edelson</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"43475359\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Streaming Big Data with Spark, Kafka, Cassandra, Akka &amp; Scala (from webinar)\" href=\"https://www.slideshare.net/helenaedelson/streaming-bigdata-helenawebinarv3\">\n    \n    <div class=\"related-content\"><p>Streaming Big Data with Spark, Kafka, Cassandra, Akka &amp; Scala (from webinar)</p><p>Helena Edelson</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"55646316\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Sa introduction to big data pipelining with cassandra &amp;amp; spark   west minster meetup - black-2015 0.11-2\" href=\"https://www.slideshare.net/langworth/sa-introduction-to-big-data-pipelining-with-cassandra-amp-spark-west-minster-meetup-black2015-0112\">\n    \n    <div class=\"related-content\"><p>Sa introduction to big data pipelining with cassandra &amp;amp; spark   west mins...</p><p>Simon Ambridge</p></div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div></div><footer>\n          <div class=\"row\"><div class=\"columns\"><ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div></div>\n          \n          <div class=\"row\"><div class=\"columns\"><p class=\"copyright text-center\">LinkedIn Corporation © 2018</p></div></div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div>\n    <div class=\"row\"><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></div>\n        \n    </div>\n    \n    \n      <noscript>\n    </noscript>",
        "created_at": "2018-07-23T16:07:44+0000",
        "updated_at": "2018-07-23T16:07:54+0000",
        "published_at": null,
        "published_by": [
          "Anton Kirillov"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 7,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/smackstackarchitectures1-150911080248-lva1-app6891-thumbnail-4.jpg?cb=1441958613",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11048"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 23,
            "label": "elasticsearch",
            "slug": "elasticsearch"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1117,
            "label": "elassandra",
            "slug": "elassandra"
          }
        ],
        "is_public": false,
        "id": 10888,
        "uid": null,
        "title": "Elassandra = Elasticsearch + Cassandra",
        "url": "https://www.elassandra.io/",
        "content": "<p class=\"card-text\">Cassandra is optimised for write-intensive workloads, therefore, Elassandra is suitable for applications where a large amount of data needs to be inserted (such as infrastructure logging, IOT, or events).\n                  Then, Elasticsearch indices can be rebuilt at any time from Cassandra tables without data duplication.</p>",
        "created_at": "2018-07-18T18:53:50+0000",
        "updated_at": "2018-07-18T18:54:01+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "www.elassandra.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10888"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 23,
            "label": "elasticsearch",
            "slug": "elasticsearch"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1117,
            "label": "elassandra",
            "slug": "elassandra"
          }
        ],
        "is_public": false,
        "id": 10887,
        "uid": null,
        "title": "strapdata/elassandra",
        "url": "https://github.com/strapdata/elassandra",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p><a target=\"_blank\" href=\"https://github.com/strapdata/elassandra/blob/v6.2.3-strapdata/elassandra-logo.png\"><img src=\"https://github.com/strapdata/elassandra/raw/v6.2.3-strapdata/elassandra-logo.png\" alt=\"Elassandra Logo\" /></a></p>\n<p>Elassandra is a fork of <a href=\"https://github.com/elastic/elasticsearch\">Elasticsearch</a> modified to run as a plugin for <a href=\"http://cassandra.apache.org\" rel=\"nofollow\">Apache Cassandra</a> in a scalable and resilient peer-to-peer architecture. Elasticsearch code is embedded in Cassanda nodes providing advanced search features on Cassandra tables and Cassandra serve as an Elasticsearch data and configuration store.</p>\n<p><a target=\"_blank\" href=\"https://github.com/strapdata/elassandra/blob/v6.2.3-strapdata/docs/elassandra/source/images/elassandra1.jpg\"><img src=\"https://github.com/strapdata/elassandra/raw/v6.2.3-strapdata/docs/elassandra/source/images/elassandra1.jpg\" alt=\"Elassandra architecture\" /></a></p>\n<p>Elassandra supports Cassandra vnodes and scales horizontally by adding more nodes.</p>\n<p>Project documentation is available at <a href=\"http://doc.elassandra.io\" rel=\"nofollow\">doc.elassandra.io</a>.</p>\n<h2><a id=\"user-content-benefits-of-elassandra\" class=\"anchor\" aria-hidden=\"true\" href=\"#benefits-of-elassandra\"></a>Benefits of Elassandra</h2>\n<p>For Cassandra users, elassandra provides Elasticsearch features :</p>\n<ul><li>Cassandra update are indexed in Elasticsearch.</li>\n<li>Full-text and spatial search on your Cassandra data.</li>\n<li>Real-time aggregation (does not require Spark or Hadoop to GROUP BY)</li>\n<li>Provide search on multiple keyspaces and tables in one query.</li>\n<li>Provide automatic schema creation and support nested document using <a href=\"https://docs.datastax.com/en/cql/3.1/cql/cql_using/cqlUseUDT.html\" rel=\"nofollow\">User Defined Types</a>.</li>\n<li>Provide a read/write JSON REST access to Cassandra data.</li>\n<li>Numerous Elasticsearch plugins and products like <a href=\"https://www.elastic.co/guide/en/kibana/current/introduction.html\" rel=\"nofollow\">Kibana</a>.</li>\n</ul><p>For Elasticsearch users, elassandra provides useful features :</p>\n<ul><li>Elassandra is masterless, cluster state is managed through a <a href=\"http://www.datastax.com/dev/blog/lightweight-transactions-in-cassandra-2-0\" rel=\"nofollow\">cassandra lightweight transactions</a>.</li>\n<li>Elassandra is a sharded multi-master database, where Elasticsearch is sharded master-slave, Thus, Elassandra has no Single Point Of Write, helping to achieve high availability.</li>\n<li>Elassandra inherits Cassandra data repair mechanisms (hinted handoff, read repair and nodetool repair) allowing to support <strong>cross datacenter replication</strong>.</li>\n<li>When adding a node to an Elassandra cluster, only data pulled from existing nodes are re-indexed in Elasticsearch.</li>\n<li>Cassandra could be your unique datastore for indexed and non-indexed data, it's easier to manage and secure. Source documents are now stored in Cassandra, reducing disk space if you need a NoSQL database and Elasticsearch.</li>\n<li>Write operations are not more restricted to one primary shards, but distributed on all Cassandra nodes in a virtual datacenter. Number of shards does not limit your write throughput, just add some elassandra nodes to increase both read and write throughput.</li>\n<li>Elasticsearch indices can be replicated between many Cassandra datacenters, allowing to write to the closest datacenter and search globally.</li>\n<li>The <a href=\"http://www.planetcassandra.org/client-drivers-tools/\" rel=\"nofollow\">cassandra driver</a> is Datacenter and Token aware, providing automatic load-balancing and failover.</li>\n</ul><h2><a id=\"user-content-quick-start\" class=\"anchor\" aria-hidden=\"true\" href=\"#quick-start\"></a>Quick start</h2>\n<h4><a id=\"user-content-elasticsearch-6x-changes\" class=\"anchor\" aria-hidden=\"true\" href=\"#elasticsearch-6x-changes\"></a>Elasticsearch 6.x changes</h4>\n<ul><li>Elasticsearch now supports only one document type per index backed by one Cassandra table. Unless you specify an elasticsearch type name in your mapping, data are stored in a cassandra table named <strong>\"_doc\"</strong>. If you want to search in many cassandra tables, you now need to create and search in many indices.</li>\n<li>Elasticsearch 6.x manages shards consistency through several metadata fields (_primary_term, _seq_no, _version) that are not more used in elassandra because replication is fully managed by cassandra.</li>\n</ul><h4><a id=\"user-content-requirements\" class=\"anchor\" aria-hidden=\"true\" href=\"#requirements\"></a>Requirements</h4>\n<p>Ensure Java 8 is installed and <code>JAVA_HOME</code> points to the correct location.</p>\n<h4><a id=\"user-content-installation\" class=\"anchor\" aria-hidden=\"true\" href=\"#installation\"></a>Installation</h4>\n<ul><li><a href=\"https://github.com/strapdata/elassandra/releases\">Download</a> and extract the distribution tarball</li>\n<li>Define the CASSANDRA_HOME environment variable : <code>export CASSANDRA_HOME=&lt;extracted_directory&gt;</code></li>\n<li>Run <code>bin/cassandra -e</code></li>\n<li>Run <code>bin/nodetool status</code></li>\n<li>Run <code>curl -XGET localhost:9200/_cluster/state</code></li>\n</ul><h4><a id=\"user-content-example\" class=\"anchor\" aria-hidden=\"true\" href=\"#example\"></a>Example</h4>\n<p>Try indexing a document on a non-existing index:</p>\n<div class=\"highlight highlight-source-shell\"><pre>curl -XPUT 'http://localhost:9200/twitter/_doc/1?pretty' -H 'Content-Type: application/json' -d '\n{\n    \"user\": \"Poulpy\",\n    \"post_date\": \"2017/10/4 13:12:00\",\n    \"message\": \"Elassandra adds dynamic mapping to Cassandra\"\n}'</pre></div>\n<p>Then look-up in Cassandra:</p>\n<div class=\"highlight highlight-source-shell\"><pre>bin/cqlsh -c \"SELECT * from twitter.\\\"_doc\\\"\"</pre></div>\n<p>Behind the scene, Elassandra has created a new Keyspace <code>twitter</code> and table <code>_doc</code>.</p>\n<p>Now, insert a row with CQL :</p>\n<div class=\"highlight highlight-source-sql\"><pre>INSERT INTO twitter.doc (\"_id\", user, post_date, message)\nVALUES ( '2', ['Jimmy'], [dateof(now())], ['New data is indexed automatically']);</pre></div>\n<p>Then search for it with the Elasticsearch API:</p>\n<div class=\"highlight highlight-source-shell\"><pre>curl \"localhost:9200/twitter/_search?q=user:Jimmy&amp;pretty\"</pre></div>\n<p>And here is a sample response :</p>\n<div class=\"highlight highlight-source-json\"><pre>{\n  \"took\" : 1,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 1,\n    \"successful\" : 1,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : 1,\n    \"max_score\" : 0.9808292,\n    \"hits\" : [\n      {\n        \"_index\" : \"twitter\",\n        \"_type\" : \"doc\",\n        \"_id\" : \"2\",\n        \"_score\" : 0.9808292,\n        \"_source\" : {\n          \"post_date\" : \"2017/10/04 13:20:00\",\n          \"message\" : \"New data is indexed automatically\",\n          \"user\" : \"Jimmy\"\n        }\n      }\n    ]\n  }\n}\n</pre></div>\n<h2><a id=\"user-content-support\" class=\"anchor\" aria-hidden=\"true\" href=\"#support\"></a>Support</h2>\n<ul><li>Commercial support is available through <a href=\"http://www.strapdata.com/\" rel=\"nofollow\">Strapdata</a>.</li>\n<li>Community support available via <a href=\"https://groups.google.com/forum/#!forum/elassandra\" rel=\"nofollow\">elassandra google groups</a>.</li>\n<li>Post feature requests and bugs on <a href=\"https://github.com/strapdata/elassandra/issues\">https://github.com/strapdata/elassandra/issues</a></li>\n</ul><h2><a id=\"user-content-license\" class=\"anchor\" aria-hidden=\"true\" href=\"#license\"></a>License</h2>\n<pre>This software is licensed under the Apache License, version 2 (\"ALv2\"), quoted below.\nCopyright 2015-2018, Strapdata (contact@strapdata.com).\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not\nuse this file except in compliance with the License. You may obtain a copy of\nthe License at\n    http://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\nWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\nLicense for the specific language governing permissions and limitations under\nthe License.\n</pre>\n<h2><a id=\"user-content-acknowledgments\" class=\"anchor\" aria-hidden=\"true\" href=\"#acknowledgments\"></a>Acknowledgments</h2>\n<ul><li>Elasticsearch and Kibana are trademarks of Elasticsearch BV, registered in the U.S. and in other countries.</li>\n<li>Apache Cassandra, Apache Lucene, Apache, Lucene and Cassandra are trademarks of the Apache Software Foundation.</li>\n<li>Elassandra is a trademark of Strapdata SAS.</li>\n</ul></article>",
        "created_at": "2018-07-18T18:53:39+0000",
        "updated_at": "2018-07-18T18:53:47+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 4,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/19846919?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10887"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10874,
        "uid": null,
        "title": "Data Modeling in Cassandra | Baeldung",
        "url": "http://www.baeldung.com/cassandra-data-modeling",
        "content": "<div class=\"short_box short_start\">\n<h3><b>I just announced the new <em>Spring 5</em> modules in REST With Spring: </b></h3>\n<p><strong><a href=\"http://www.baeldung.com/rest-with-spring-course#new-modules\" id=\"announcement_CTA\">&gt;&gt; CHECK OUT THE COURSE</a></strong></p>\n</div><h2><strong>1. Overview</strong></h2>\n<p>Cassandra is a NoSQL database that provides high availability and horizontal scalability without compromising performance.</p>\n<p>To get the best performance out of Cassandra, we need to carefully design the schema around query patterns specific to the business problem at hand.</p>\n<p>In this article, we will review some of the key concepts around <strong>how to approach data modeling in Cassandra</strong>.</p>\n<p>Before proceeding, you can go through our <a href=\"http://www.baeldung.com/cassandra-with-java\">Cassandra with Java</a> article to understand the basics and how to connect to Cassandra using Java.</p>\n<h2><strong>2. Partition Key<br /></strong></h2>\n<p>Cassandra is a distributed database in which data is partitioned and stored across multiple nodes within a cluster.</p>\n<p>The partition key is made up of one or more data fields and is <strong>used by the partitioner to generate a token via hashing to distribute the data uniformly across a cluster</strong>.</p>\n<h2><strong>3. Clustering Key<br /></strong></h2>\n<p>A clustering key is made up of one or more fields and helps in clustering or grouping together rows with same partition key and storing them in sorted order.</p>\n<p>Let’s say that we are storing time-series data in Cassandra and we want to retrieve the data in chronological order. A clustering key that includes time-series data fields will be very helpful for efficient retrieval of data for this use case.</p>\n<p><strong>Note: The combination of partition key and clustering key makes up the primary key and uniquely identifies any record in the Cassandra cluster.</strong></p>\n<h2><strong>4. Guidelines Around Query Patterns<br /></strong></h2>\n<p>Before starting with data modeling in Cassandra, we should identify the query patterns and ensure that they adhere to the following guidelines:</p>\n<ol><li>Each query should fetch data from a single partition</li>\n<li>We should keep track of how much data is getting stored in a partition, as Cassandra has limits around the number of columns that can be stored in a single partition</li>\n<li>It is OK to denormalize and duplicate the data to support different kinds of query patterns over the same data</li>\n</ol><p>Based on the above guidelines, let’s look at some real-world use cases and how we would model the Cassandra data models for them.</p>\n<h2><strong>5. Real World Data Modeling Examples<br /></strong></h2>\n<h3><strong>5.1. Facebook Posts</strong></h3>\n<p>Suppose that we are storing Facebook posts of different users in Cassandra. One of the common query patterns will be fetching the top ‘<em>N</em>‘ posts made by a given user.</p>\n<p>Thus, <strong>we need to</strong> <strong>store all data for a particular user on a single partition</strong> as per the above guidelines.</p>\n<p>Also, using the post timestamp as the clustering key will be helpful for retrieving the top ‘<em>N</em>‘ posts more efficiently.</p>\n<p>Let’s define the Cassandra table schema for this use case:</p>\n<pre class=\"brush: sql; gutter: true\">CREATE TABLE posts_facebook (&#13;\n  user_id uuid,&#13;\n  post_id timeuuid, &#13;\n  content text,&#13;\n  PRIMARY KEY (user_id, post_id) )&#13;\nWITH CLUSTERING ORDER BY (post_id DESC);</pre>\n<p>Now, let’s write a query to find the top 20 posts for the user <em>Anna</em>:</p>\n<pre class=\"brush: sql; gutter: true\">SELECT content FROM posts_facebook WHERE user_id = \"Anna_id\" LIMIT 20</pre>\n<h3><strong>5.2. Gyms Across the Country</strong></h3>\n<p>Suppose that we are storing the details of different partner gyms across the different cities and states of many countries and we would like to fetch the gyms for a given city.</p>\n<p>Also, let’s say we need to return the results having gyms sorted by their opening date.</p>\n<p>Based on the above guidelines, we should store the gyms located in a given city of a specific state and country on a single partition and use the opening date and gym name as a clustering key.</p>\n<p>Let’s define the Cassandra table schema for this example:</p>\n<pre class=\"brush: sql; gutter: true\">CREATE TABLE gyms_by_city (&#13;\n country_code text,&#13;\n state text,&#13;\n city text,&#13;\n gym_name text,&#13;\n opening_date timestamp,&#13;\n PRIMARY KEY (&#13;\n   (country_code, state_province, city), &#13;\n   (opening_date, gym_name)) &#13;\n WITH CLUSTERING ORDER BY (opening_date ASC, gym_name ASC);</pre>\n<p>Now, let’s look at a query that fetches the first ten gyms by their opening date for the city of Phoenix within the U.S. state of Arizona:</p>\n<pre class=\"brush: sql; gutter: true\">SELECT * FROM gyms_by_city&#13;\n  WHERE country_code = \"us\" AND state = \"Arizona\" AND city = \"Phoenix\"&#13;\n  LIMIT 10</pre>\n<p class=\"brush: sql; gutter: true\">Next, let’s see a query that fetches the ten most recently-opened gyms in the city of Phoenix within the U.S. state of Arizona:</p>\n<pre class=\"brush: sql; gutter: true\">SELECT * FROM gyms_by_city&#13;\n  WHERE country_code = \"us\" and state = \"Arizona\" and city = \"Phoenix\"&#13;\n  ORDER BY opening_date DESC &#13;\n  LIMIT 10</pre>\n<p>Note: As the last query’s sort order is opposite of the sort order defined during the table creation, the query will run slower as Cassandra will first fetch the data and then sort it in memory.</p>\n<h3><strong>5.3. E-commerce Customers and Products</strong></h3>\n<p>Let’s say we are running an e-commerce store and that we are storing the <em>Customer</em> and <em>Product</em> information within Cassandra. Let’s look at some of the common query patterns around this use case:</p>\n<ol><li>Get <em>Customer</em> info</li>\n<li>Get <em>Product</em> info</li>\n<li>Get all <em>Customers</em> who like a given <em>Product</em></li>\n<li>Get all <em>Products</em> a given <em>Customer</em> likes</li>\n</ol><p>We will start by using separate tables for storing the <em>Customer</em> and <em>Product</em> information. However, we need to introduce a fair amount of denormalization to support the 3rd and 4th queries shown above.</p>\n<p>We will create two more tables to achieve this – “<em>Customer_by_Product</em>” and “<em>Product_by_Customer</em>“.</p>\n<p>Let’s look at the Cassandra table schema for this example:</p>\n<pre class=\"brush: sql; gutter: true\">CREATE TABLE Customer (&#13;\n  cust_id text,&#13;\n  first_name text, &#13;\n  last_name text,&#13;\n  registered_on timestamp, &#13;\n  PRIMARY KEY (cust_id));&#13;\n&#13;\nCREATE TABLE Product (&#13;\n  prdt_id text,&#13;\n  title text,&#13;\n  PRIMARY KEY (prdt_id));&#13;\n&#13;\nCREATE TABLE Customer_By_Liked_Product (&#13;\n  liked_prdt_id text,&#13;\n  liked_on timestamp,&#13;\n  title text,&#13;\n  cust_id text,&#13;\n  first_name text, &#13;\n  last_name text, &#13;\n  PRIMARY KEY (prdt_id, liked_on));&#13;\n&#13;\nCREATE TABLE Product_Liked_By_Customer (&#13;\n  cust_id text, &#13;\n  first_name text,&#13;\n  last_name text,&#13;\n  liked_prdt_id text, &#13;\n  liked_on timestamp,&#13;\n  title text,&#13;\n  PRIMARY KEY (cust_id, liked_on));</pre>\n<p>Note: To support both the queries, recently-liked products by a given customer and customers who recently liked a given product, we have used the “<em>liked_on</em>” column as a clustering key.</p>\n<p>Let’s look at the query to find the ten Customers who most recently liked the product “<em>Pepsi</em>“:</p>\n<pre class=\"brush: sql; gutter: true\">SELECT * FROM Customer_By_Liked_Product WHERE title = \"Pepsi\" LIMIT 10</pre>\n<p>And let’s see the query that finds the recently-liked products (up to ten) by a customer named “<em>Anna</em>“:</p>\n<pre class=\"brush: sql; gutter: true\">SELECT * FROM Product_Liked_By_Customer &#13;\n  WHERE first_name = \"Anna\" LIMIT 10</pre>\n<h2><strong>6. Inefficient Query Patterns</strong></h2>\n<p>Due to the way that Cassandra stores data, some query patterns are not at all efficient, including the following:</p>\n<ul><li><strong>Fetching data from multiple partitions</strong> – this will require a coordinator to fetch the data from multiple nodes, store it temporarily in heap, and then aggregate the data before returning results to the user</li>\n<li><strong>Join-based queries</strong> – due to its distributed nature, Cassandra does not support table joins in queries the same way a relational database does, and as a result, <strong>queries with</strong> <strong>joins will be slower and can also lead to inconsistency and availability issues</strong></li>\n</ul><h2><strong>7. Conclusion</strong></h2>\n<p>In this tutorial, we have covered several best practices around how to approach data modeling in Cassandra.</p>\n<p>Understanding the core concepts and identifying the query patterns in advance is necessary for designing a correct data model that gets the best performance from a Cassandra cluster.</p>\n<div class=\"short_box short_end\">\n<h3><b>I just announced the new Spring 5 modules in REST With Spring: </b></h3>\n<p><strong><a href=\"http://www.baeldung.com/rest-with-spring-course#new-modules\">&gt;&gt; CHECK OUT THE LESSONS</a></strong></p>\n</div>",
        "created_at": "2018-07-17T23:23:57+0000",
        "updated_at": "2018-07-17T23:24:06+0000",
        "published_at": "2017-07-22T16:36:05+0000",
        "published_by": [
          "by \n\nbaeldung"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 6,
        "domain_name": "www.baeldung.com",
        "preview_picture": "http://www.baeldung.com/wp-content/uploads/2016/10/social-Persistence-On-Baeldung-1.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10874"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 603,
            "label": "book",
            "slug": "book"
          },
          {
            "id": 854,
            "label": "gitbook",
            "slug": "gitbook"
          }
        ],
        "is_public": false,
        "id": 10873,
        "uid": null,
        "title": "Introduction | Introduction To Cassandra",
        "url": "https://pandaforme.gitbooks.io/introduction-to-cassandra/content/",
        "content": "<p>This book is a basic introduction to Cassandra. The major information and knowledge came from <a href=\"http://datastax.com/\" target=\"_blank\">http://datastax.com/</a>.</p><p>I will also comment some concepts based on realistic examples and can understand those concepts clearly.</p>",
        "created_at": "2018-07-17T23:21:11+0000",
        "updated_at": "2018-07-17T23:21:19+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "pandaforme.gitbooks.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10873"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10871,
        "uid": null,
        "title": "Understand the Cassandra data model",
        "url": "https://pandaforme.gitbooks.io/introduction-to-cassandra/content/understand_the_cassandra_data_model.html",
        "content": "<p>The Cassandra data model defines  </p><ul><li>Column family as a way to store and organize data </li>\n<li>Table as a two-dimensional view of a multi-dimensional column family </li>\n<li>Operations on tables using the Cassandra Query Language (CQL) </li>\n</ul><p>Cassandra1.2+reliesonCQLschema,concepts,andterminology, though the older Thrift API remains available</p><table><thead><tr><th>Table (CQL API terms)</th>\n<th>Column Family (Thrift API terms)</th>\n</tr></thead><tbody><tr><td>Table is a set of partitions</td>\n<td>Column family is a set of rows</td>\n</tr><tr><td>Partition may be single or multiple row</td>\n<td>Row may be skinny or wide</td>\n</tr><tr><td>Partition key uniquely identifies a partition, and may be simple or composite</td>\n<td>Row key uniquely identifies a row, and may be simple or composite</td>\n</tr><tr><td>Column uniquely identifies a cell in a partition, and may be regular or clustering</td>\n<td>Column key uniquely identies a cell in a row, and may be simple or composite</td>\n</tr><tr><td>Primary key is comprised of a partition key plus clustering columns, if any, and uniquely identifies a row in both its partition and table</td>\n<td>\n</td></tr></tbody></table><h2 id=\"row-partition\">Row (Partition)</h2><p>Row is the smallest unit that stores related data in Cassandra  </p><ul><li>Rows: individual rows constitute a column family </li>\n<li>Row key: uniquely identifies a row in a column family </li>\n<li>Row: stores pairs of column keys and column values </li>\n<li>Column key: uniquely identifies a column value in a row  </li>\n<li>Column value: stores one value or a collection of values </li>\n</ul><p><img src=\"https://pandaforme.gitbooks.io/introduction-to-cassandra/content/Screen%20Shot%202016-02-24%20at%2011.46.09.png\" alt=\"\" /></p><p>Rows may be described as <code>skinny</code> or <code>wide</code> </p><ul><li>Skinny row: has a fixed, relatively small number of column keys </li>\n<li>Wide row: has a relatively large number of column keys (hundreds or\nthousands); this number may increase as new data values are inserted</li>\n</ul><h2 id=\"key-partition-key\">Key (Partition Key)</h2><h3 id=\"composite-row-key\">Composite row key</h3><p>multiple components separated by colon\n<img src=\"https://pandaforme.gitbooks.io/introduction-to-cassandra/content/Screen%20Shot%202016-02-24%20at%2011.53.17.png\" alt=\"\" /></p><h3 id=\"composite-column-key\">Composite column key</h3><p>multiple components separated by colon \n<img src=\"https://pandaforme.gitbooks.io/introduction-to-cassandra/content/Screen%20Shot%202016-02-24%20at%2011.54.01.png\" alt=\"\" /></p><h2 id=\"column-family-table\">Column family (Table)</h2><p>set of rows with a similar structure<br /><img src=\"https://pandaforme.gitbooks.io/introduction-to-cassandra/content/Screen%20Shot%202016-02-24%20at%2011.56.28.png\" alt=\"\" /></p><p><img src=\"https://pandaforme.gitbooks.io/introduction-to-cassandra/content/Screen%20Shot%202016-02-24%20at%2012.24.12.png\" alt=\"\" /></p><h2 id=\"table-with-multirow-partitions\">Table with multi-row partitions</h2><p><img src=\"https://pandaforme.gitbooks.io/introduction-to-cassandra/content/Screen%20Shot%202016-02-24%20at%2012.25.54.png\" alt=\"\" /></p>",
        "created_at": "2018-07-17T23:20:42+0000",
        "updated_at": "2018-07-17T23:20:46+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "pandaforme.gitbooks.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10871"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 15,
            "label": "tutorial",
            "slug": "tutorial"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10870,
        "uid": null,
        "title": "Cassandra Database Tutorial for Beginners: Learn in 3 Days",
        "url": "https://www.guru99.com/cassandra-tutorial.html",
        "content": "<p> Cassandra is a distributed database management system designed for handling a high volume of structured data across commodity servers. In this tutorial, you will see the various concept of Cassandra like data modeling, clusters, monitoring tool, query language, etc. </p><p> Not necessary but knowledge of other database management system like<a href=\"https://www.guru99.com/hbase-tutorials.html\"> HBase</a> or <a href=\"https://www.guru99.com/mongodb-tutorials.html\">MongoDB</a> will be of help. </p><h2><br />Here is what we cover in the Course</h2><table class=\"table\"><tr><td class=\"responsivetable\"><a href=\"https://www.guru99.com/download-install-cassandra.html\" title=\"Download &amp; Install Casandra on Windows : A Step By Step Guide\"><strong> Tutorial</strong></a></td> <td>Download &amp; Install Casandra on Windows : A Step By Step Guide</td> </tr><tr><td class=\"responsivetable\"><a href=\"https://www.guru99.com/cassandra-architecture.html\" title=\"Cassandra Architecture &amp; Replication Factor Strategy Tutorial\"><strong> Tutorial</strong></a></td> <td>Cassandra Architecture &amp; Replication Factor Strategy Tutorial</td> </tr><tr><td class=\"responsivetable\"><a href=\"https://www.guru99.com/cassandra-data-model-rules.html\" title=\"Learn Cassandra Data Modeling  with Simple Example\"><strong> Tutorial</strong></a></td> <td>Learn Cassandra Data Modeling with Simple Example</td> </tr><tr><td class=\"responsivetable\"><a href=\"https://www.guru99.com/cassandra-cql.html\" title=\"Create, Alter &amp; Drop Keyspace in Cassandra: Complete Tutorial\"><strong> Tutorial</strong></a></td> <td>Create, Alter &amp; Drop Keyspace in Cassandra: Complete Tutorial</td> </tr><tr><td class=\"responsivetable\"><a href=\"https://www.guru99.com/cassandra-table-create-alter-drop-truncate.html\" title=\"Cassandra Table:  Create, Alter, Drop &amp; Truncate\"><strong> Tutorial</strong></a></td> <td>Cassandra Table: Create, Alter, Drop &amp; Truncate</td> </tr><tr><td class=\"responsivetable\"><a href=\"https://www.guru99.com/cassandra-query-language-cql-insert-update-delete-read-data.html\" title=\"Cassandra Query Language(CQL): Insert, Update, Delete, Read Data\"><strong> Tutorial</strong></a></td> <td>Cassandra Query Language(CQL): Insert, Update, Delete, Read Data</td> </tr><tr><td class=\"responsivetable\"><a href=\"https://www.guru99.com/create-drop-index-in-cassandra-complete-tutorial.html\" title=\"Create &amp; Drop INDEX in Cassandra: Complete Tutorial\"><strong> Tutorial</strong></a></td> <td>Create &amp; Drop INDEX in Cassandra: Complete Tutorial</td> </tr><tr><td class=\"responsivetable\"><a href=\"https://www.guru99.com/cassandra-data-types-expiration-tutorial.html\" title=\"Cassandra Data Types &amp; Expiration Tutorial\"><strong> Tutorial</strong></a></td> <td>Cassandra Data Types &amp; Expiration Tutorial</td> </tr><tr><td class=\"responsivetable\"><a href=\"https://www.guru99.com/cassandra-collections-tutorial-set-list-map.html\" title=\"Cassandra Collections Tutorial - SET, LIST &amp; MAP\"><strong> Tutorial</strong></a></td> <td>Cassandra Collections Tutorial - SET, LIST &amp; MAP</td> </tr><tr><td class=\"responsivetable\"><a href=\"https://www.guru99.com/cassandra-cluster.html\" title=\"Cassandra Cluster Setup on Multiple  Nodes (Machines)\"><strong> Tutorial</strong></a></td> <td>Cassandra Cluster Setup on Multiple Nodes (Machines)</td> </tr><tr><td class=\"responsivetable\"><a href=\"https://www.guru99.com/cassandra-devcenter-opscenter.html\" title=\"Datastax DevCenter &amp; OpsCenter Installation Complete Guide\"><strong> Tutorial</strong></a></td> <td>Datastax DevCenter &amp; OpsCenter Installation Complete Guide</td> </tr><tr><td class=\"responsivetable\"><a href=\"https://www.guru99.com/cassandra-security.html\" title=\"Cassandra SECURITY - Create User  &amp; Authentication With JMX\"><strong> Tutorial</strong></a></td> <td>Cassandra SECURITY - Create User &amp; Authentication With JMX</td> </tr></table><h2>What is Apache Cassandra?</h2><p>Apache Cassandra is highly scalable, distributed and high-performance NoSQL database. Cassandra is designed to handle a huge amount of data. </p><p><a href=\"https://cdn.guru99.com/images/cassandra/021116_0444_WhatisApach1.png\" class=\"jh-image-popup-colorbox\"><img title=\"What is Apache Cassandra?\" alt=\"Cassandra Database Tutorial for Beginners: Learn in 3 Days\" src=\"https://cdn.guru99.com/images/cassandra/021116_0444_WhatisApach1.png\" /></a> </p><p>In the image above, circles are Cassandra nodes and lines between the circles shows distributed architecture, while the client is sending data to the node. </p><p>Cassandra handles the huge amount of data with its distributed architecture. Data is placed on different machines with more than one replication factor that provides high availability and no single point of failure. </p><h2>Cassandra History</h2><ul><li>Cassandra was first developed at Facebook for inbox search.</li><li>Facebook open sourced it in July 2008. </li><li>Apache incubator accepted Cassandra in March 2009.</li><li>Cassandra is a top level project of<a href=\"https://www.guru99.com/apache.html\"> Apache </a>since February 2010.</li><li>The latest version of Apache Cassandra is 3.2.1.</li></ul><p>First let's understand what NoSQL database is. </p><h2>Nosql Cassandra Database</h2><p>NoSQL databases are called \"Not Only SQL\" or \"Non-relational\" databases. NoSQL databases store and retrieve data other than tabular relations such as relation databases. </p><p>NoSQL databases include MongoDB, HBase, and Cassandra.</p><p>There are following properties of NoSQL databases. </p><ul><li>Design Simplicity</li><li>Horizontal Scaling</li><li>High Availability</li></ul><p>Data structures used in Cassandra are more specified than data structures used in relational databases. Cassandra data structures are faster than relational database structures. </p><p>NoSQL databases are increasingly used in Big Data and real-time web applications. NoSQL databases are sometimes called Not Only<a href=\"https://www.guru99.com/sql.html\"> SQL </a>i.e. they may support SQL-like query language. </p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/qUV2j3XBRHc\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"> </iframe><h2>Nosql Cassandra Database Vs Relational databases</h2><p>Here are the differences between relation databases and NoSQL databases in a tabular format. </p><table class=\"table table-striped\"><tr><td><strong>Relational Database</strong> </td><td><strong>NoSQL Database</strong> </td></tr><tr><td>Handles data coming in low velocity </td><td>Handles data coming in high velocity </td></tr><tr><td>Data arrive from one or few locations </td><td>Data arrive from many locations </td></tr><tr><td>Manages structured data </td><td>Manages structured unstructured and semi-structured data. </td></tr><tr><td>Supports complex transactions (with joins) </td><td>Supports simple transactions </td></tr><tr><td>single point of failure with failover </td><td>No single point of failure </td></tr><tr><td>Handles data in the moderate volume. </td><td>Handles data in very high volume </td></tr><tr><td>Centralized deployments </td><td>Decentralized deployments </td></tr><tr><td>Transactions written in one location </td><td>Transaction written in many locations </td></tr><tr><td>Gives read scalability </td><td>Gives both read and write scalability </td></tr><tr><td>Deployed in vertical fashion </td><td>Deployed in Horizontal fashion </td></tr></table><h2>Apache Cassandra Features</h2><p>There are following features that Cassandra provides.</p><ul><li><strong>Massively Scalable Architecture: </strong>Cassandra has a masterless design where all nodes are at the same level which provides operational simplicity and easy scale out.</li><li><strong>Masterless Architecture: </strong>Data can be written and read on any node.</li><li><strong>Linear Scale Performance: </strong>As more nodes are added, the performance of Cassandra increases.</li><li><strong>No Single point of failure: </strong>Cassandra replicates data on different nodes that ensures no single point of failure.</li><li><strong>Fault Detection and Recovery: </strong>Failed nodes can easily be restored and recovered.</li><li><strong>Flexible and Dynamic Data Model: </strong>Supports datatypes with Fast writes and reads.</li><li><strong>Data Protection: </strong>Data is protected with commit log design and build in security like backup and restore mechanisms.</li><li><strong>Tunable Data Consistency: </strong>Support for strong data consistency across distributed architecture.</li><li><strong>Multi Data Center Replication: </strong>Cassandra provides feature to replicate data across multiple data center.</li><li><strong>Data Compression: </strong>Cassandra can compress up to 80% data without any overhead.</li><li><strong>Cassandra Query language: </strong>Cassandra provides query language that is similar like SQL language. It makes very easy for relational database developers moving from relational database to Cassandra.</li></ul><h2>Cassandra Use Cases/Application</h2><p>Cassandra is a non-relational database that can be used for different types of applications. Here are some use cases where Cassandra should be preferred. </p><ul><li><strong>Messaging</strong><p>Cassandra is a great database for the companies that provides<a href=\"https://www.guru99.com/mobile-testing.html\"> Mobile </a>phones and messaging services. These companies have a huge amount of data, so Cassandra is best for them. </p></li><li><strong>Internet of things Application</strong><p>Cassandra is a great database for the applications where data is coming at very high speed from different devices or sensors. </p></li><li><strong>Product Catalogs and retail apps</strong><p>Cassandra is used by many retailers for durable shopping cart protection and fast product catalog input and output. </p></li><li><strong>Social Media Analytics and recommendation engine</strong><p>Cassandra is a great database for many online companies and social media providers for analysis and recommendation to their customers. </p></li></ul><p id=\"slidetag\"> </p>",
        "created_at": "2018-07-17T23:20:01+0000",
        "updated_at": "2018-07-17T23:20:07+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_GB",
        "reading_time": 4,
        "domain_name": "www.guru99.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10870"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10869,
        "uid": null,
        "title": "Learn Cassandra Data Modeling with Simple Example",
        "url": "https://www.guru99.com/cassandra-data-model-rules.html",
        "content": "<p>Although Cassandra query language resembles with<a href=\"https://www.guru99.com/sql.html\"> SQL </a>language, their data modelling methods are totally different. </p><p>In Cassandra, a bad data model can degrade performance, especially when users try to implement the RDBMS concepts on Cassandra. It is best to keep in mind few rules detailed below. </p><p>In this tutorial, you will learn- </p><ul><li><a href=\"#1\">Cassandra Data Model Rules</a></li> <li><a href=\"#2\">Model Your Data in Cassandra</a></li> <li><a href=\"#3\">Handling One to One Relationship</a></li> <li><a href=\"#4\">Handling one to many relationships</a></li> <li><a href=\"#5\">Handling Many to Many Relationship</a></li> </ul><h2>Cassandra Data Model Rules</h2><p>In Cassandra, writes are not expensive. Cassandra does not support joins, group by, OR clause, aggregations, etc. So you have to store your data in such a way that it should be completely retrievable. So these rules must be kept in mind while modelling data in Cassandra. </p><ol><li><strong>Maximize the number of writes</strong><p>In Cassandra, writes are very cheap. Cassandra is optimized for high write performance. So try to maximize your writes for better read performance and data availability. There is a tradeoff between data write and data read. So, optimize you data read performance by maximizing the number of data writes.</p></li> <li><strong>Maximize Data Duplication</strong><p>Data denormalization and data duplication are defacto of Cassandra. Disk space is not more expensive than memory, CPU processing and IOs operation. As Cassandra is a distributed database, so data duplication provides instant data availability and no single point of failure.</p></li> </ol><p><strong>Data Modeling Goals </strong> </p><p>You should have following goals while modelling data in Cassandra. </p><ol><li><strong>Spread Data Evenly Around the Cluster</strong><p>You want an equal amount of data on each node of Cassandra cluster. Data is spread to different nodes based on partition keys that is the first part of the primary key. So, try to choose integers as a primary key for spreading data evenly around the cluster.</p></li> <li><strong>Minimize number of partitions read while querying data</strong><p>Partition are a group of records with the same partition key. When the read query is issued, it collects data from different nodes from different partitions. </p><p>If there will be many partitions, then all these partitions need to be visited for collecting the query data. </p><p>It does not mean that partitions should not be created. If your data is very large, you can’t keep that huge amount of data on the single partition. The single partition will be slowed down. </p><p>So try to choose a balanced number of partitions. </p></li> </ol><p><strong>Good Primary Key</strong> </p><p>Let’s take an example and find which primary key is good. </p><p>Here is the table MusicPlaylist. </p><pre>&#13;\nCreate table MusicPlaylist&#13;\n    (&#13;\n        SongId int,&#13;\n        SongName text,&#13;\n        Year int,&#13;\n        Singer text,&#13;\n        Primary key(SongId, SongName)&#13;\n    );&#13;\n</pre><p>In above example, table MusicPlaylist, </p><ul><li>Songid is the partition key, and </li> <li>SongName is the clustering column</li> <li>Data will be clustered on the basis of SongName. Only one partition will be created with the SongId. There will not be any other partition in the table MusicPlaylist. </li> </ul><p>Data retrieval will be slow by this data model due to the bad primary key. </p><p>Here is another table MusicPlaylist. </p><pre>&#13;\nCreate table MusicPlaylist&#13;\n    (&#13;\n        SongId int,&#13;\n        SongName text,&#13;\n        Year int,&#13;\n        Singer text,&#13;\n        Primary key((SongId, Year), SongName)&#13;\n    );&#13;\n</pre><p>In above example, table MusicPlaylist, </p><ul><li>Songid and Year are the partition key, and </li> <li>SongName is the clustering column. </li> <li>Data will be clustered on the basis of SongName. In this table, each year, a new partition will be created. All the songs of the year will be on the same node. This primary key will be very useful for the data. </li> </ul><p>Our data retrieval will be fast by this data model. </p><h2>Model Your Data in Cassandra</h2><p>Following things should be kept in mind while modelling your queries. </p><ol><li><strong>Determine what queries you want to support</strong></li>             </ol><ul><li>Joins</li> <li>Group by</li> <li>Filtering on which column etc.</li> </ul><strong>Create table according to your queries</strong><p>Create table according to your queries. Create a table that will satisfy your queries. Try to create a table in such a way that a minimum number of partitions needs to be read.</p><h2>Handling One to One Relationship</h2><p>One to one relationship means two tables have one to one correspondence. For example, the student can register only one course, and I want to search on a student that in which course a particular student is registered in. </p><p>So in this case, your table schema should encompass all the details of the student in corresponding to that particular course like the name of the course, roll no of the student, student name, etc. </p><pre>&#13;\nCreate table Student_Course&#13;\n    (&#13;\n        Student rollno int primary key,&#13;\n        Student_name text,&#13;\n        Course_name text,&#13;\n    );&#13;\n</pre><h2>Handling one to many relationships</h2><p>One to many relationships means having one to many correspondence between two tables. </p><p>For example, a course can be studied by many students. I want to search all the students that are studying a particular course. </p><p>So by querying on course name, I will have many student names that will be studying a particular course. </p><pre>&#13;\nCreate table Student_Course&#13;\n    (&#13;\n        Student_rollno int,&#13;\n        Student_name text,&#13;\n        Course_name text,&#13;\n    );&#13;\n</pre><p>I can retrieve all the students for a particular course by the following query. </p><pre>&#13;\nSelect * from Student_Course where Course_name='Course Name';&#13;\n</pre><h2>Handling Many to Many Relationship</h2><p>Many to many relationships means having many to many correspondence between two tables. </p><p>For example, a course can be studied by many students, and a student can also study many courses. </p><p>I want to search all the students that are studying a particular course. Also, I want to search all the course that a particular student is studying. </p><p>So in this case, I will have two tables i.e. divide the problem into two cases. </p><p>First, I will create a table by which you can find courses by a particular student. </p><pre>&#13;\nCreate table Student_Course&#13;\n    (&#13;\n        Student_rollno int primary key,&#13;\n        Student_name text,&#13;\n        Course_name text,&#13;\n    );&#13;\n</pre><p>I can find all the courses by a particular student by the following query. </p><pre>&#13;\nSelect * from Student_Course where student_rollno=rollno;&#13;\n</pre><p>Second, I will create a table by which you can find how many students are studying a particular course. </p><pre>&#13;\nCreate table Course_Student&#13;\n    (&#13;\n        Course_name text primary key,&#13;\n        Student_name text,&#13;\n        student_rollno int&#13;\n    );&#13;\n</pre><p>I can find a student in a particular course by the following query. </p><pre>&#13;\nSelect * from Course_Student where Course_name=CourseName;&#13;\n</pre><p><strong>Difference between RDBMS and Cassandra Data Modelling</strong></p><table class=\"table table-striped\"><tr><td><p><strong>RDBMS</strong> </p></td><td><p><strong>Cassandra</strong> </p></td></tr><tr><td><p>Stores data in normalized form </p></td><td><p>Stores data in denormalized form </p></td></tr><tr><td><p>Legacy dbms; structured data </p></td><td><p>Wide row store,Dynamic; structured &amp; unstructured data </p></td></tr></table><p><strong>Summary</strong> </p><p>Data modelling in Cassandra is different than other RDBMS databases. Cassandra data modelling has some rules. These rules must be followed for good data modelling. Besides these rules, we saw three different data modelling cases and how to deal with them. </p><p id=\"slidetag\"> </p>",
        "created_at": "2018-07-17T23:19:41+0000",
        "updated_at": "2018-07-24T12:41:48+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_GB",
        "reading_time": 5,
        "domain_name": "www.guru99.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10869"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10868,
        "uid": null,
        "title": "Cassandra Data Modeling: Primary, Clustering, Partition, and Compound Keys - DZone Database",
        "url": "https://dzone.com/articles/cassandra-data-modeling-primary-clustering-partiti",
        "content": "<div class=\"content-html\" itemprop=\"text\"><p>In this post, we are going to discuss the different keys available in Cassandra. The primary key concept in Cassandra is different from relational databases. Therefore, it is worth spending some time to understand it.</p>\n<p>Let's take an example and create a student table which has <code>student_id</code> as a primary key column.</p>\n<h2><strong>1) Primary Key </strong></h2>\n<pre lang=\"text/x-cassandra\">create table person (student_id int primary key, fname text, lname text, \n                     dateofbirth timestamp, email text, phone text );</pre>\n<p>In Cassandra, a table can have a number of rows. Each row is referenced by a primary key, also called the row key. There are a number of columns in a row but the number of columns can vary in different rows.</p>\n<p>For example, one row in a table can have three columns whereas another row in the same table can have 10 columns. It is also important to note that in Cassandra, both column names and values have binary types. That means column names can have binary values, such as strings, timestamps, or an integer, etc. This is different from SQL databases, where each row in a SQL table has a fixed number of columns, and column names can only be text.</p>\n<p>We saw that <code>student_id</code> was used as a row key to refer to <code>person</code> data.</p>\n<h2><strong>2) Compound Primary Key</strong></h2>\n<p>As the name suggests, a compound primary key is comprised of one or more columns that are referenced in the primary key. One component of the compound primary key is called partition key, whereas the other component is called the clustering key. The following are different variations of primary keys. Please note that C1, C2, C3,… and so on represent columns in the table.</p>\n<ul><li><p><strong>C1</strong>: Primary key has only one partition key and no cluster key.</p></li>\n <li><p><strong>(C1, C2)</strong>: Column C1 is a partition key and column C2 is a cluster key.</p></li>\n <li><p><strong>(C1,C2,C3,…)</strong>: Column C1 is a partition key and columns C2, C3, and so on make the cluster key.</p></li>\n <li><p><strong>(C1, (C2, C3,…))</strong>: It is same as 3, i.e., column C1 is a partition key and columns C2,C3,… make the cluster key.</p></li>\n <li><p><strong>((C1, C2,…), (C3,C4,…))</strong>: columns C1, C2 make partition key and columns C3,C4,… make the cluster key.</p></li>\n</ul><p>It is important to note that when the compound key is C1, C2, C3, then the first key, C1, becomes the partition key, and the rest of the keys become part of the cluster key. In order to make composite partition keys, we have to specify keys in parenthesis such as: ( ( C1,C2) , C3, C4). In this case, C1 and C2 are part of the partition keys, and C3 and C4 are part of the cluster key.</p>\n<h2><strong>3) Partition Key</strong></h2>\n<p>The purpose of a partition key is to identify the partition or node in the cluster that stores that row. When data is read or written from the cluster, a function called Partitioner is used to compute the hash value of the partition key. This hash value is used to determine the node/partition which contains that row. For example, rows whose partition key values range from 1000 to 1234 may reside in node A, and rows with partition key values range from 1235 to 2000 may reside in node B, as shown in figure 1. If a row contains partition key whose hash value is 1233 then it will be stored in node A.</p>\n<p><img alt=\"cluster\" class=\"alignnone size-full wp-image-20848 fr-fin fr-dib\" src=\"https://knoldus.files.wordpress.com/2016/10/cluster.gif?w=640\" /></p>\n<h2><strong>4) Clustering Key</strong></h2>\n<p>The purpose of the clustering key is to store row data in a sorted order. The sorting of data is based on columns, which are included in the clustering key. This arrangement makes it efficient to retrieve data using the clustering key.</p>\n<h2><strong>Example</strong></h2>\n<p>To make these concepts clear, we will consider the example of a school system.</p>\n<p>Create a keyspace with replication strategy ‘SimpleStrategy’ and replication_factor 1.</p>\n<pre lang=\"text/x-cassandra\">create keyspace Students_Details with replication = {‘class’ : ‘SimpleStrategy’, ‘replication_factor’:1};</pre>\n<p>Now switch to the students_details keyspace:</p>\n<pre lang=\"text/x-cassandra\">use students_details;\n</pre>\n<p>Check the number of tables present in the keyspace:</p>\n<pre lang=\"text/x-cassandra\">students_details&gt; desc TABLES;</pre>\n<p>We will create a table, <code>student</code> , that contains general information about any student.</p>\n<pre lang=\"text/x-cassandra\">create table student (stuid int, avg_marks float, description text, \n                      primary key (stuid));</pre>\n<p>Type the following insert statements to enter some data into this table.</p>\n<pre lang=\"text/x-cassandra\">insert into student (stuid, avg_marks, description) values (1,25.5,’student 1′);\ninsert into student (stuid, avg_marks, description) values (2,35.5,’student 2′);</pre>\n<p>To view the details just inserted...</p>\n<pre lang=\"text/x-cassandra\">students_details&gt; select * from student;</pre>\n<pre lang=\"text/x-sh\">stuid | avg_marks | description\n——-+———–+————-\n1 |      25.5 |   student 1\n2 |      35.5 |   student 2</pre>\n<p>We can see how Cassandra has stored this data under the hood by using the <code>cassandra-cli</code> tool. Run <code>cassandra-cli</code> in a separate terminal windo.</p>\n<p><strong>Important</strong>: The CLI utility is deprecated and will be removed in Cassandra 3.0. For ease of use and performance, switch from Thrift and CLI to CQL and cqlsh.)</p>\n<p>So if you're using a Cassandra verison above 3.0, then use the below commands.</p>\n<p>Using the <strong>EXPAND</strong> Command in cqlsh , we can view the details info for the queries . EXPAND with no arguments shows the current value of the expanded setting.</p>\n<p><code>cqlsh:students_details&gt; EXPAND</code> </p>\n<p>If expanded output is disabled. Use <code>EXPAND ON</code> to enable it.</p>\n<p>Now view the details inserted above (the studid will be present in a red color in cqlsh, representing the primary key/row key)</p>\n<p><code>cqlsh:students_details&gt; select * from student;</code></p>\n<pre lang=\"text/x-cassandra\">@ Row 1\n————-+———–\nstuid       | 1\navg_marks   | 25.5\ndescription | student 1\n@ Row 2\n————-+———–\nstuid       | 2\navg_marks   | 35.5\ndescription | student 2\n(2 rows)</pre>\n<p>We can see from the above output that the stuid has become the row key, and it identifies individual rows.</p>\n<p><code>cqlsh:students_details&gt; select token(stuid) from student;</code></p>\n<pre lang=\"text/x-sh\">@ Row 1\n———————+———————-\nsystem.token(stuid) | -4069959284402364209\n@ Row 2\n———————+———————-\nsystem.token(stuid) | -3248873570005575792</pre>\n<p>Also, you can see that there are two tokens.</p>\n<p>We can use columns in the primary key to filter data in the select statement. Type the following command in the cqlsh window:</p>\n<p><code>select * from student where stuid = 1;</code> </p>\n<p>Now we will create another table called marks, which records marks of each student every day (say every day, new exams and marks are recorded). Type the following command on cqlsh:</p>\n<pre lang=\"text/x-cassandra\">create table marks(stuid int,exam_date timestamp,marks float, exam_name text, \n                   primary key (stuid,exam_date));</pre>\n<p>This statement creates the marks table with a primary key (stuid , exam_date ). As the primary key has two components, the first component is considered a partition key, and the second component becomes the cluster key. Add some data into the table:</p>\n<pre lang=\"text/x-cassandra\">insert into marks(stuid ,exam_date ,marks ,exam_name) values (1,’2016-11-10′,76 ,’examA’);\ninsert into marks(stuid ,exam_date ,marks ,exam_name) values (1,’2016-11-11′,90 ,’examB’);\ninsert into marks(stuid ,exam_date ,marks ,exam_name) values (1,’2016-11-12′,68 ,’examC’);</pre>\n<p><code>cqlsh:students_details&gt; select * from marks;</code></p>\n<pre lang=\"text/x-cassandra\">@ Row 1\n———–+————————–\nstuid     | 1\nexam_date | 2016-11-09 18:30:00+0000\nexam_name | examA\nmarks     | 76\n@ Row 2\n———–+————————–\nstuid     | 1\nexam_date | 2016-11-10 18:30:00+0000\nexam_name | examB\nmarks     | 90\n@ Row 3\n———–+————————–\nstuid     | 1\nexam_date | 2016-11-11 18:30:00+0000\nexam_name | examC\nmarks     | 68</pre>\n<p>Now, let's see how the partition concept has been applied:</p>\n<p><code>cqlsh:students_details&gt; select token(stuid) from marks;</code></p>\n<pre lang=\"text/x-cassandra\">@ Row 1\n———————+———————-\nsystem.token(stuid) | -4069959284402364209\n@ Row 2\n———————+———————-\nsystem.token(stuid) | -4069959284402364209\n@ Row 3\n———————+———————-\nsystem.token(stuid) | -4069959284402364209</pre>\n<p>We can see all the three rows have the <strong>same partition token</strong>, hence Cassandra stores only one row for each partition key. All the data associated with that partition key is stored as columns in the datastore. The data that we have stored through three different insert statements have the same stuid value, i.e. 1, therefore, all the data is saved in that row as columns, i.e under one partition.</p>\n<p>If you remember, we discussed before that the second component of a primary key is called the clustering key. The role of the clustering key is to group related items together. All the data that is inserted against same clustering key is grouped together.</p>\n<p>In this case, all the columns, such as <strong>exam_name </strong>and<strong> marks</strong>, will be grouped by value in <strong>exam</strong>_<strong>date</strong>, i.e 2016-11-11 18:30:00+0000, by default in ascending order .</p>\n<p>I hope these examples have helped you to clarify some of the concepts of data modeling in Cassandra. Please feel free to leave any comments.</p></div><div class=\"content-html\" itemprop=\"text\"><a>\n                        <img class=\"pub-image\" width=\"420\" itemprop=\"image\" src=\"src\" alt=\"image\" /></a></div>",
        "created_at": "2018-07-17T23:19:29+0000",
        "updated_at": "2018-07-17T23:19:34+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 7,
        "domain_name": "dzone.com",
        "preview_picture": "https://dz2cdn1.dzone.com/storage/article-thumb/3282924-thumb.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10868"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10867,
        "uid": null,
        "title": "Cassandra Data Modeling Best Practices, Part 2",
        "url": "https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-2/",
        "content": "<p>In the <a href=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/\">first part</a>, we covered a few fundamental practices and walked through a detailed example to help you get started with Cassandra data model design. You can follow Part 2 without reading Part 1, but I recommend glancing over the <a href=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/\">terms and conventions</a> I’m using. If you’re new to Cassandra, I urge you to read <a href=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/\">Part 1</a>.</p><p>September 2014 Update: Readers should note that this article describes data modeling techniques based on Cassandra’s Thrift API.  Please see <a href=\"https://wiki.apache.org/cassandra/DataModel\" target=\"_blank\" rel=\"noopener\">https://wiki.apache.org/cassandra/DataModel</a> for CQL API based techniques.</p><p>August 2015 Update:  Readers can also sign up a free online self-paced course on how to model their data in Apache Cassandra from <a href=\"https://academy.datastax.com/courses/ds220-data-modeling?dxt=blogposting\" target=\"_blank\" rel=\"noopener\">https://academy.datastax.com/courses/ds220-data-modeling?dxt=blogposting</a>.</p><p>Some of the practices listed below might evolve in the future. I’ve provided related JIRA ticket numbers so you can watch any evolution.</p><p>With that, let’s get started with some basic practices!</p><h3>Storing values in column names is perfectly OK</h3><p><em>Leaving column values empty (“valueless” columns) is also OK.</em></p><p>It’s a common practice with Cassandra to store a value (actual data) in the column name (a.k.a. column key), and even to leave the column value field empty if there is nothing else to store. One motivation for this practice is that column names are stored physically sorted, but column values are not.</p><p><strong>Notes:</strong></p><ul><li>The maximum column key (and row key) size is 64KB.  However, don’t store something like ‘item description’ as the column key!</li>\n<li>Don’t use timestamp alone as a column key. You might get colliding timestamps from two or more app servers writing to Cassandra. Prefer timeuuid (<a href=\"http://en.wikipedia.org/wiki/Universally_unique_identifier\" target=\"_blank\" rel=\"noopener\">type-1 uuid</a>) instead.</li>\n<li>The maximum column value size is 2 GB. But becuase there is no streaming and the whole value is fetched in heap memory when requested, limit the size to only a few MBs.  (Large objects are not likely to be supported in the near future – <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-265\" target=\"_blank\" rel=\"noopener\">Cassandra-265</a>. However, the <a href=\"https://github.com/Netflix/astyanax/wiki/Chunked-Object-Store\" target=\"_blank\" rel=\"noopener\">Astyanax</a> client library supports large objects by chunking them.)</li>\n</ul><h3>Leverage wide rows for ordering, grouping, and filtering</h3><p><em>But don’t go too wide.</em></p><p>This goes along with the above practice. When actual data is stored in column names, we end up with wide rows.</p><p><strong>Benefits of wide rows:</strong></p><ul><li>Since column names are stored physically sorted, wide rows enable ordering of data and hence efficient filtering (range scans). You’ll still be able to efficiently look up an individual column within a wide row, if needed.</li>\n<li>If data is queried together, you can group that data up in a single wide row that can be read back efficiently, as part of a single query. As an example, for tracking or monitoring some time series data, we can group data by hour/date/machines/event types (depending on the requirements)  in a single wide row, with each column containing granular data or roll-ups. We can also further group data within a row using super or composite columns as discussed later.</li>\n<li>Wide row column families are heavily used (with composite columns) to build custom indexes in Cassandra.</li>\n<li>As a side benefit, you can de-normalize a one-to-many relationship as a wide row without data duplication. However, I would do this only when data is queried together and you need to optimize read performance.</li>\n</ul><p><strong>Example:</strong></p><p>Let’s say we want to store some event log data and retrieve that data hourly. As shown in the model below, the row key is the hour of the day, the column name holds the time when the event occurred, and the column value contains payload. Note that the row is wide and the events are ordered by time because column names are stored sorted. Granularity of the wide row (for this example, per hour rather than every few minutes) depends on the use case, traffic, and data size, as discussed next.</p><p><a href=\"https://www.ebayinc.com/assets/Uploads/Blog/2012/08/widerow.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-2/assets/Uploads/Blog/2012/08/_resampled/ResizedImageWzM4OCwxMzFd/widerow.png\" alt=\"\" width=\"388\" height=\"131\" title=\"\" /></a></p><h3>But not too wide, as a row is never split across nodes:</h3><p>It’s hard to say exactly how wide a wide row should be, partly because it’s dependent upon the use case. But here’s some advice:</p><p><strong>Traffic</strong>: All of the traffic related to one row is handled by only one node/shard (by a single set of replicas, to be more precise). Rows that are too “fat” could cause hot spots in the cluster – usually when the number of rows is smaller than the size of the cluster (hope not!), or when wide rows are mixed with skinny ones, or some rows become hotter than others. However, cluster load balancing ultimately depends on the row key selection; conversely, the row key also defines how wide a row will be. So load balancing is something to keep in mind during design.</p><p><strong>Size</strong>: As a row is not split across nodes, data for a single row must fit on disk within a single node in the cluster. However, rows can be large enough that they don’t have to fit in memory entirely. Cassandra allows 2 billion columns per row. At eBay, we’ve not done any “wide row” benchmarking, but we model data such that we never hit more than a few million columns or a few megabytes in one row (we change the row key granularity, or we split into multiple rows). If you’re interested, <a href=\"http://thelastpickle.com/2011/07/04/Cassandra-Query-Plans/\" target=\"_blank\" rel=\"noopener\">Cassandra Query Plans</a> by Aaron Morton shows some performance concerns with wide rows (but note that the results can change in new releases).</p><p>However, these caveats don’t mean you should not use wide rows; just don’t go extra wide.</p><p><strong>Note:</strong> <a id=\"key-val\" href=\"https://issues.apache.org/jira/browse/CASSANDRA-4176\" target=\"_blank\" rel=\"noopener\">Cassandra-4176</a> might add composite types for row key in CQL as a way to split a wide row into multiple rows. However, a single (physical) row is never split <em>across nodes</em> (and won’t be split across nodes in the future), and is always handled by a single set of replicas. You might also want to track <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-3929\" target=\"_blank\" rel=\"noopener\">Cassandra-3929,</a> which would add row size limits for keeping the most recent <em>n</em> columns in a wide row.</p><h3><strong>Choose the proper row key – it’s your “shard key” </strong></h3><p><em>Otherwise, you’ll end up with hot spots, even with <a href=\"http://www.datastax.com/docs/0.8/cluster_architecture/partitioning#about-the-random-partitioner\" target=\"_blank\" rel=\"noopener\">RandomPartitioner</a>.</em></p><p>Let’s consider again the above example of storing time series event logs and retrieving them hourly. We picked the hour of the day as the row key to keep one hour of data together in a row. But there is an issue: All of the writes will go only to the node holding the row for the current hour, causing a hot spot in the cluster. Reducing granularity from hour to minutes won’t help much, because only one node will be responsible for handling writes for whatever duration you pick. As time moves, the hot spot might also move but it won’t go away!</p><p><strong>Bad row key:  </strong>“ddmmyyhh”</p><p>One way to alleviate this problem is to add something else to the row key – an event type, machine id, or similar value that’s appropriate to your use case.</p><p><strong>Better row key: </strong>“ddmmyyhh|eventtype”</p><p><a href=\"https://www.ebayinc.com/assets/Uploads/Blog/2012/08/shardkey.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-2/assets/Uploads/Blog/2012/08/_resampled/ResizedImageWzQ2NiwxMjhd/shardkey.png\" alt=\"\" width=\"466\" height=\"128\" title=\"\" /></a></p><p>Note that now we don’t have global time ordering of events, across all event types, in the column family. However, this may be OK if the data is viewed (grouped) by event type later. If the use case also demands retrieving all of the events (irrespective of type) in time sequence, we need to do a multi-get for all event types for a given time period, and honor the time order when merging the data in the application.</p><p>If you can’t add anything to the row key or if you absolutely need ‘time period’ as a row key, another option is to shard a row into multiple (physical) rows by manually splitting row keys: “ddmmyyhh | 1”, “ddmmyyhh | 2”,… “ddmmyyhh | n”, where <em>n</em> is the number of nodes in the cluster. For an hour window, each shard will now evenly handle the writes; you need to round-robin among them. But reading data for an hour will require multi-gets from all of the splits (from the multiple physical nodes) and merging them in the application. (An assumption here is that RandomPartitioner is used, and therefore that range scans on row keys can’t be done.)</p><h3><strong>Keep read-heavy data separate from write-heavy data<br /></strong></h3><p><em>This way, you can benefit from Cassandra’s off-heap row cache.</em></p><p>Irrespective of caching and even outside the NoSQL world, it’s always a good practice to keep read-heavy and write-heavy data separate because they scale differently.</p><p><strong>Notes:</strong></p><ul><li>A row cache is useful for skinny rows, but harmful for wide rows today because it pulls the entire row into memory. <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-1956\" target=\"_blank\" rel=\"noopener\">Cassandra-1956</a> and <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-2864\" target=\"_blank\" rel=\"noopener\">Cassandra-2864</a> might change this in future releases. However, the practice of keeping read-heavy data separate from write-heavy data will still stand.</li>\n<li>Even if you have lots of data (more than available memory) in a column family but you also have particularly “hot” rows, enabling a row cache might be useful.</li>\n</ul><h3>Make sure column key and row key are unique</h3><p><em>Otherwise, data could get accidentally overwritten.</em></p><ul><li>In Cassandra (a distributed database!), there is no unique constraint enforcement for row key or column key.</li>\n<li>Also, there is no separate update operation (no in-place updates!). It’s always an upsert (mutate) in Cassandra. If you accidentally insert data with an existing row key and column key, the previous column value will be silently overwritten without any error (the change won’t be versioned; the data will be gone).</li>\n</ul><h3>Use the proper comparator and validator</h3><p><em>Don’t just use the default BytesType comparator and validator unless you really need to.</em></p><p>In Cassandra, the data type for a column <em><strong>value </strong></em>(or row key)  is called a <em>Validator</em>. The data type for a column <em><strong>name</strong></em> is called a <em>Comparator</em>.  Although Cassandra does not require you to define both, you must at least specify the comparator unless your column family is static (that is, you’re not storing actual data as part of the column name), or unless you really don’t care about the sort order.</p><ul><li>An improper comparator will sort column names inappropriately on the disk. It will be difficult (or impossible) to do range scans on column names later.</li>\n<li>Once defined, you can’t change a comparator without rewriting all data. However, the validator can be changed later.</li>\n</ul><p>See <a href=\"http://www.datastax.com/docs/1.0/ddl/column_family#about-data-types-comparators-and-validators\" target=\"_blank\" rel=\"noopener\">comparators and validators</a> in the Cassandra documentation for the supported data types.</p><h3>Keep the column name short</h3><p><em>Because it’s stored <em>repeatedly.</em><br /></em></p><p>This practice doesn’t apply if you use the column name to store actual data. Otherwise, keep the column name short, since it’s repeatedly stored with each column value. Memory and storage overhead can be significant when the size of the column value is not much larger than the size of the column name – or worse, when it’s smaller.</p><p>For example, favor ‘fname’ over ‘firstname’, and ‘lname’ over ‘lastname’.</p><p><strong>Note:</strong> <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-4175\" target=\"_blank\" rel=\"noopener\">Cassandra-4175</a> might make this practice obsolete in the future.</p><h3>Design the data model such that operations are idempotent</h3><p><em>Or, make sure that your use case can live with inaccuracies or that inaccuracies can be corrected eventually.</em></p><p>In an eventually consistent and fully distributed system like Cassandra, idempotent operations can help – a lot. Idempotent operations allow partial failures in the system, as the operations can be retried safely without changing the final state of the system. In addition, idempotency can sometimes alleviate the need for strong consistency and allow you to work with eventual consistency without causing data duplication or other anomalies. Let’s see how these principles apply in Cassandra. I’ll discuss partial failures only, and leave out alleviating the need for strong consistency until an upcoming post, as it is very much dependent on the use case.</p><p>Because of  Cassandra’s fully distributed (and multi-master) nature, write failure does not guarantee that data is not written, unlike the behavior of relational databases. In other words, even if the client receives a failure for a write operation, data might be written to one of the replicas, which will eventually get propagated to all replicas. No rollback or cleanup is performed on partially written data. Thus, a perceived write failure can result in a successful write eventually. So, retries on write failure can yield unexpected results if your model isn’t update idempotent.</p><p><strong>Notes: </strong></p><ul><li>“Update idempotent” here means a model where operations are idempotent. An operation is called idempotent if it can be applied one time or multiple times with the same result.</li>\n<li>In most cases, idempotency won’t be a concern, as writes into regular column families are always update idempotent. The exception is with the Counter column family, as shown in the example below. However, sometimes your use case can model data such that write operations are not update idempotent from the use case perspective. For instance, in <a href=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/\">part 1</a>, User_by_Item and Item_by_User in the final model are not update idempotent if the use case operation ‘user likes item’ gets executed multiple times, as the timestamp might differ for each like. However, note that a specific instance of  the use case operation ‘user likes item’ is still idempotent, and so can be retried multiple times in case of failures. As this is more use-case specific, I might elaborate more in future posts.</li>\n<li>Even with a consistency level ONE, write failure does not guarantee data is not written; the data still could get propagated to all replicas eventually.</li>\n</ul><p><strong>Example</strong></p><p>Suppose that we want to count the number of users who like a particular item. One way is to use the <a href=\"http://www.datastax.com/dev/blog/whats-new-in-cassandra-0-8-part-2-counters\" target=\"_blank\" rel=\"noopener\">Counter</a> column family supported by Cassandra to keep count of users per item. Since the counter increment (or decrement) is not update idempotent, retry on failure could yield an over-count if the previous increment was successful on at least one node. One way to make the model update idempotent is to maintain a list of user ids instead of incrementing a count, as shown below. Whenever a user likes an item, we write that user’s id against the item; if the write fails, we can safely retry. To determine the count of all users who like an item, we read all user ids for the item and count manually.</p><p><a href=\"https://www.ebayinc.com/assets/Uploads/Blog/2012/08/idempotency2.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-2/assets/Uploads/Blog/2012/08/_resampled/ResizedImageWzU4MiwxNTld/idempotency2.png\" alt=\"\" width=\"582\" height=\"159\" title=\"\" /></a></p><p>In the above update idempotent model, getting the counter value requires reading all user ids, which will not perform well (there could be millions). If reads are heavy on the counter and you can live with an approximate count, the counter column will be efficient for this use case. If needed, the counter value can be corrected periodically by counting the user ids from the update idempotent column family.</p><p><strong>Note:</strong> <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-2495\" target=\"_blank\" rel=\"noopener\">Cassandra-2495</a> might add a proper retry mechanism for counters in the case of a failed request. However, in general, this practice will continue to hold true. So make sure to always litmus-test your model for update idempotency.</p><h3><strong>Model data around transactions, if needed</strong></h3><p><em>But this might not always be possible, depending on the use case.</em></p><p>Cassandra has no multi-row, cluster-wide transaction or rollback mechanism; instead, it offers row-level atomicity. In other words, a single mutation operation of columns for a given row key is atomic. So if you need transactional behavior, try to model your data such that you would only ever need to update a single row at once. However, depending on the use case, this is not always doable. Also, if your system needs ACID transactions, you might re-think your database choice.</p><p><strong>Note:</strong> <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-4285\" target=\"_blank\" rel=\"noopener\">Cassandra-4285</a> might add an atomic, eventually consistent batch operation.</p><h3>Decide on the proper TTL up front, if you can<br /></h3><p><em>Because it’s hard to change TTL for existing data.</em></p><p>In Cassandra, TTL (time to live) is not defined or set at the column family level. It’s set per column value, and once set it’s hard to change; or, if not set, it’s hard to set for existing data. The only way to change the TTL for existing data is to read and re-insert all the data with a new TTL value. So think about your purging requirements, and if possible set the proper TTL for your data upfront.</p><p><strong>Note:</strong> <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-3974\" target=\"_blank\" rel=\"noopener\">Cassandra-3974</a> might introduce TTL for the column family, separate from column TTL.</p><h3>Don’t use the Counter column family to generate surrogate keys<br /></h3><p><em>Because it’s not intended for this purpose.<br /></em></p><p>The Counter column family holds distributed counters meant (of course) for distributed counting. Don’t try to use this CF to generate sequence numbers for surrogate keys, like Oracle sequences or MySQL auto-increment columns. You will receive duplicate sequence numbers! Most of the time you really don’t need globally sequential numbers. Prefer timeuuid (<a href=\"http://en.wikipedia.org/wiki/Universally_unique_identifier\" target=\"_blank\" rel=\"noopener\">type-1 uuid</a>) as surrogate keys. If you truly need a globally sequential number generator, there are a few possible mechanisms; but all will require centralized coordination, and thus can impact the overall system’s scalability and availability.</p><h3>Favor composite columns over super columns</h3><p><em>Otherwise, you might hit performance bottlenecks with super columns.</em></p><p>A super column in Cassandra can be used to group column keys, or to model a two-layer hierarchy. However, super columns have the following implementation issues and are therefore becoming less favorable.</p><p><strong>Issues:</strong></p><ul><li>Sub-columns of a super column are not indexed. Reading one sub-column de-serializes all sub-columns.</li>\n<li>Built-in secondary indexing does not work with sub-columns.</li>\n<li>Super columns cannot encode more than two layers of hierarchy.</li>\n</ul><p>Similar (even better) functionality can be achieved by the use of the Composite column. It’s a regular column with sub-columns encoded in it. Hence, all of the benefits of regular columns, such as sorting and range scans, are available; and you can encode more than two layers of hierarchy.</p><p><strong>Note:</strong> <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-3237\" target=\"_blank\" rel=\"noopener\">Cassandra-3237</a> might change the underlying super column implementation to use composite columns. However, composite columns will still remain preferred over super columns.</p><h3>The order of sub-columns in composite columns matters</h3><p><em>Because order defines grouping.</em></p><p>For example, a composite column key like &lt;state|city&gt; will be stored ordered first by state and then by city, rather than first by city and then by state. In other words, all the cities within a state are located (grouped) on disk together.</p><h3>Favor built-in composite types over manual construction</h3><p><em>Because manual construction doesn’t always work.</em></p><p>Avoid manually constructing the composite column keys using string concatenation (with separators like “:” or “|”). Instead, use the built-in composite types (and comparators) supported by Cassandra 0.8.1 and above.</p><p><strong>Why?</strong></p><ul><li>Manual construction won’t work if sub-columns are of different data types. For example, the composite key &lt;state|zip|timeuuid&gt; will not be sorted in a type-aware fashion (state as string, zip code as integer, and timeuuid as time).</li>\n<li>You can’t reverse the sort order on components in the type – for instance, with the state ascending and the zip code descending in the above key.</li>\n</ul><p><strong>Note:</strong> Cassandra built-in composite types come in two flavors:</p><ul><li><em>Static composite type: </em>Data types for each part of a composite column are predefined per column family.  All the column names/keys within a column family must be of that composite type.</li>\n<li><em>Dynamic composite type: </em>This type allows mixing column names with different composite types in a column family or even in one row.</li>\n</ul><p>Find more information about composite types at <a href=\"http://www.datastax.com/dev/blog/introduction-to-composite-columns-part-1\" target=\"_blank\" rel=\"noopener\">Introduction to composite columns</a>.</p><h3>Favor static composite types over dynamic, whenever possible</h3><p><em>Because dynamic composites are too dynamic.</em></p><p>If all column keys in a column family are of the same composite type, always use static composite types. Dynamic composite types were originally created to keep multiple custom indexes in one column family. If possible, don’t mix different composite types in one row using the dynamic composite type unless absolutely required. <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-3625\" target=\"_blank\" rel=\"noopener\">Cassandra-3625</a> has fixed some serious issues with dynamic composites.</p><p><strong>Note:</strong> CQL 3 supports static composite types for column names via clustered (wide) rows. Find more information about how CQL 3 handles wide rows at <a href=\"http://www.datastax.com/docs/1.1/ddl/column_family#composite-columns\" target=\"_blank\" rel=\"noopener\">DataStax docs</a>.</p><p>Enough for now. I would appreciate your inputs to further enhance these modeling best practices, which guide our Cassandra utilization today.</p><p>—  <a title=\"About me\" href=\"http://www.jaykumarpatel.com\" rel=\"noopener\" target=\"_blank\">Jay Patel</a>, architect@eBay.</p>",
        "created_at": "2018-07-17T23:19:16+0000",
        "updated_at": "2018-07-17T23:19:22+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 16,
        "domain_name": "www.ebayinc.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10867"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10866,
        "uid": null,
        "title": "Cassandra Data Modeling Best Practices, Part 1",
        "url": "https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/",
        "content": "<p>This is the first in a series of posts on Cassandra data modeling, implementation, operations, and related practices that guide our Cassandra utilization at eBay. Some of these best practices we’ve learned from public forums, many are new to us, and a few still are arguable and could benefit from further experience.</p><p>September 2014 Update: Readers should note that this article describes data modeling techniques based on Cassandra’s Thrift API.  Please see <a href=\"https://wiki.apache.org/cassandra/DataModel\" target=\"_blank\" rel=\"noopener\">https://wiki.apache.org/cassandra/DataModel</a> for CQL API based techniques.</p><p>August 2015 Update:  Readers can also sign up a free online self-paced course on how to model their data in Apache Cassandra from <a href=\"https://academy.datastax.com/courses/ds220-data-modeling?dxt=blogposting\" target=\"_blank\" rel=\"noopener\">https://academy.datastax.com/courses/ds220-data-modeling?dxt=blogposting</a>.</p><p>In this part, I’ll cover a few basic practices and walk through a detailed example. Even if you don’t know anything about Cassandra, you should be able to follow almost everything.</p><h3>A few words on Cassandra @ eBay</h3><p>We’ve been trying out Cassandra for more than a year. Cassandra is now serving a handful of use cases ranging from write-heavy logging and tracking, to mixed workload. One of them serves our “Social Signal” project, which enables like/own/want features on eBay product pages. A few use cases have reached production, while more are in development.</p><p>Our Cassandra deployment is not huge, but it’s growing at a healthy pace. In the past couple of months, we’ve deployed dozens of nodes across several small clusters spanning multiple data centers. You may ask, why multiple clusters? We isolate clusters by functional area and criticality. Use cases with similar criticality from the same functional area share the same cluster, but reside in different keyspaces.</p><p>RedLaser, Hunch, and other eBay adjacencies are also trying out Cassandra for various purposes. In addition to Cassandra, we also utilize MongoDB and HBase. I won’t discuss these now, but suffice it to say we believe each has its own merit.</p><p>I’m sure you have more questions at this point. But I won’t tell you the full story yet. At the upcoming <a href=\"http://www.datastax.com/events/cassandrasummit2012\" target=\"_blank\" rel=\"noopener\">Cassandra summit</a>, I’ll go into detail about each use case, the data model, multi-datacenter deployment, lessons learned, and more.</p><p>The focus of this post is Cassandra data modeling best practices that we follow at eBay. So, let’s jump in with a few notes about terminology and representations I’ll be using for each post in this series.</p><h3>Terms and Conventions</h3><ul><li>The terms “Column Name” and “Column Key” are used interchangeably. Similarly, “Super Column Name” and “Super Column Key” are used interchangeably.</li>\n<li>The following layout represents a row in a Column Family (CF):<a href=\"https://www.ebayinc.com/assets/Uploads/Blog/2012/07/term1.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/assets/Uploads/Blog/2012/07/_resampled/ResizedImageWzU2MSwxMjBd/term1.png\" alt=\"\" width=\"561\" height=\"120\" title=\"\" /></a></li>\n</ul><ul><li>The following layout represents a row in a Super Column Family (SCF):<a href=\"https://www.ebayinc.com/assets/Uploads/Blog/2012/07/term2.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/assets/Uploads/Blog/2012/07/term2.png\" alt=\"\" width=\"561\" title=\"\" /></a></li>\n</ul><ul><li>The following layout represents a row in a Column Family with composite columns. Parts of a composite column are separated by ‘|’. Note that this is just a representation convention; Cassandra’s built-in composite type encodes differently, not using ‘|’. (BTW, this post doesn’t require you to have detailed knowledge of super columns and composite columns.)<img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/assets/Uploads/Blog/2012/07/term3.png\" alt=\"\" width=\"561\" title=\"\" /></li>\n</ul><p>With that, let’s start with the first practice!</p><h3>Don’t think of a relational table<br /></h3><p><em>Instead, think of a nested, sorted map data structure.</em></p><p>The following relational model analogy is often used to introduce Cassandra to newcomers:</p><p><a href=\"https://www.ebayinc.com/assets/Uploads/Blog/2012/07/analogy.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/assets/Uploads/Blog/2012/07/_resampled/ResizedImageWzUwNCwxNjVd/analogy.png\" alt=\"\" width=\"504\" height=\"165\" title=\"\" /></a></p><p>This analogy helps make the transition from the relational to non-relational world. But don’t use this analogy while designing Cassandra column families. Instead, think of the Cassandra column family as a map of a map: an outer map keyed by a row key, and an inner map keyed by a column key. Both maps are sorted.</p><p><em> <em>SortedMap&lt;RowKey, SortedMap&lt;ColumnKey, ColumnValue&gt;&gt;</em> </em></p><p><strong>Why?</strong></p><p>A nested sorted map is a more accurate analogy than a relational table, and will help you make the right decisions about your Cassandra data model.</p><p><a href=\"https://www.ebayinc.com/assets/Uploads/Blog/2012/07/thinkmap.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/assets/Uploads/Blog/2012/07/_resampled/ResizedImageWzUyMSwxNTRd/thinkmap.png\" alt=\"\" width=\"521\" height=\"154\" title=\"\" /></a></p><p><strong>How?</strong></p><ul><li>A map gives efficient key lookup, and the sorted nature gives efficient scans. In Cassandra, we can use row keys and column keys to do efficient lookups and range scans.</li>\n<li>The number of column keys is unbounded. In other words, you can have wide rows.</li>\n<li>A key can itself hold a value. In other words, you can have a valueless column.</li>\n</ul><p>Range scan on row keys is possible only when data is partitioned in a cluster using <a href=\"http://www.datastax.com/docs/1.1/cluster_architecture/partitioning#about-ordered-partitioners\" target=\"_blank\" rel=\"noopener\">Order Preserving Partitioner (OOP)</a>. OOP is almost never used. So, you can think of the outer map as unsorted:<em><br /></em></p><p><em> <em>Map&lt;RowKey, SortedMap&lt;ColumnKey, ColumnValue&gt;&gt;</em> </em></p><p>As mentioned earlier, there is something called a “Super Column” in Cassandra. Think of this as a grouping of columns, which turns our two nested maps into three nested maps as follows:</p><p><em> <em>Map&lt;RowKey, SortedMap&lt;SuperColumnKey, SortedMap&lt;ColumnKey, ColumnValue&gt;&gt;&gt;</em> </em></p><p><strong>Notes:</strong></p><ul><li>You need to pass the timestamp with each column value, for Cassandra to use internally for conflict resolution. However, the timestamp can be safely ignored during modeling. Also, do not plan to use timestamps as data in your application. They’re not for you, and they do not define new versions of your data (unlike in HBase).</li>\n<li>The Cassandra community has heavily criticized the implementation of Super Column because of performance concerns and the lack of support for secondary indexes. The same “super column like” functionality (or even better) can be achieved by using composite columns.</li>\n</ul><h3>Model column families around query patterns</h3><p><em>But start your design with entities and relationships, if you can.<br /></em></p><ul><li>Unlike in relational databases, it’s not easy to tune or introduce new query patterns in Cassandra by simply creating secondary indexes or building complex SQLs (using joins, order by, group by?) because of its high-scale distributed nature. So think about query patterns up front, and design column families accordingly.</li>\n<li>Remember the lesson of the nested sorted map, and think how you can organize data into that map to satisfy your query requirements of fast look-up/ordering/grouping/filtering/aggregation/etc.</li>\n</ul><p>However, entities and their relationships still matter (unless the use case is special – perhaps storing logs or other time series data?). What if I gave you query patterns to create a Cassandra model for an e-commerce website, but didn’t tell you anything about the entities and relationships? You might try to figure out entities and relationships, knowingly or unknowingly, from the query patterns or from your prior understanding of the domain (because entities and relationships are how we perceive the real world). It’s important to understand and start with entities and relationships, then continue modeling around query patterns by de-normalizing and duplicating. If this sounds confusing, make sure to go through the detailed example later in this post.</p><p><strong>Note:</strong> It also helps to identify the most frequent query patterns and isolate the less frequent. Some queries might be executed only a few thousand times, while others a billion times. Also consider which queries are sensitive to latency and which are not. Make sure your model first satisfies the most frequent and critical queries.</p><h3>De-normalize and duplicate for read performance</h3><p><em>But don’t de-normalize if you don’t need to. It’s all about finding the right balance.<br /></em></p><p>In the relational world, the pros of normalization are well understood: less data duplication, fewer data modification anomalies, conceptually cleaner, easier to maintain, and so on. The cons are also understood: that queries might perform slowly if many tables are joined, etc. The same holds true in Cassandra, but the cons are magnified since it’s distributed and of course there are no joins (since it’s high-scale distributed!). So with a fully normalized schema, reads may perform much worse.</p><p>This and the previous practice (modeling around query patterns) are so important that I would like to further elaborate by devoting the rest of the post to a detailed example.</p><p><strong>Note:</strong> The example discussed below is just for demonstration purposes, and does not represent the data model used for Cassandra projects within eBay.</p><h3>Example: ‘Like’ relationship between User &amp; Item</h3><p>This example concerns the functionality of an e-commerce system where users can like one or more items. One user can like multiple items and one item can be liked by multiple users, leading to a many-to-many relationship as shown in the relational model below:</p><p><a href=\"https://www.ebayinc.com/assets/Uploads/Blog/2012/07/relational.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/assets/Uploads/Blog/2012/07/_resampled/ResizedImageWzU5MCwzNDld/relational.png\" alt=\"\" width=\"590\" height=\"349\" title=\"\" /></a></p><p>For this example, let’s say we would like to query data as follows:</p><ul><li><em>Get user by user id </em></li>\n<li><em>Get item by item id</em></li>\n<li><em>Get all the items that a particular user likes</em></li>\n<li><em>Get all the users who like a particular item </em></li>\n</ul><p>Below are some options for modeling the data in Cassandra, in order of the lowest to the highest de-normalization. The best option depends on the query patterns, as you’ll soon see.</p><h3>Option 1: Exact replica of relational model</h3><p><strong><a href=\"https://www.ebayinc.com/assets/Uploads/Blog/2012/07/option1.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/assets/Uploads/Blog/2012/07/_resampled/ResizedImageWzUyMCwyNjFd/option1.png\" alt=\"\" width=\"520\" height=\"261\" title=\"\" /></a><br /></strong></p><p>This model supports querying user data by user id and item data by item id. But there is no easy way to query all the items that a particular user likes or all the users who like a particular item.</p><p>This is the worst way of modeling for this use case. Basically, User_Item_Like is not modeled correctly here.</p><p>Note that the ‘timestamp’ column (storing when the user liked the item) is dropped from User_Item_Like for simplicity. I’ll introduce that column later.</p><h3>Option 2: Normalized entities with custom indexes</h3><p><a href=\"https://www.ebayinc.com/assets/Uploads/Blog/2012/07/option2.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/assets/Uploads/Blog/2012/07/_resampled/ResizedImageWzU0MSwyNjdd/option2.png\" alt=\"\" width=\"541\" height=\"267\" title=\"\" /></a></p><p>This model has fairly normalized entities, except that user id and item id mapping is stored twice, first by item id and second by user id.</p><p>Here, we can easily query all the items that a particular user likes using Item_By_User, and all the users who like a particular item using User_By_Item. We refer to these column families as custom secondary indexes, but they’re just other column families.</p><p>Let’s say we always want to get the item title in addition to the item id when we query items liked by a particular user. In the current model, we first need to query Item_By_User to get all the item ids that a given user likes; and then for each item id, we need to query Item to get the title. Similarly, let’s say we always want to get all the usernames in addition to user ids when we query users who like a particular item. With the current model, we first need to query User_By_Item to get the ids for all users who like a given item; and then for each user id, we need to query User to get the username. It’s possible that one item is liked by a couple hundred users, or an active user has liked many items — which will cause many additional queries when we look up usernames who like a given item and vice versa. So, it’s better to optimize by de-normalizing item title in Item_by_User, and username in User_by_Item, as shown in option 3.</p><p><strong>Note:</strong> Even if you can batch your reads, they will still be slower because Cassandra (Coordinator node, to be specific) has to query each row separately underneath (usually from different nodes). Batch read will help only by avoiding the round trip — which is good, so you should always try to leverage it.</p><h3>Option 3: Normalized entities with de-normalization into custom indexes</h3><p><a href=\"https://www.ebayinc.com/assets/Uploads/Blog/2012/07/option3.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/assets/Uploads/Blog/2012/07/_resampled/ResizedImageWzUyOSwyNjVd/option3.png\" alt=\"\" width=\"529\" height=\"265\" title=\"\" /></a></p><p>In this model, title and username are de-normalized in User_By_Item and Item_By_User respectively. This allows us to efficiently query all the item titles liked by a given user, and all the user names who like a given item. This is a fair amount of de-normalization for this use case.</p><p>What if we want to get all the information (title, desc, price, etc.) about the items liked by a given user? But we need to ask ourselves whether we really need this query, particularly for this use case. We can show all the item titles that a user likes and pull additional information only when the user asks for it (by clicking on a title). So, it’s better not to do extreme de-normalization for this use case. (However, it’s common to show both title and price up front. It’s easy to do; I’ll leave it for you to pursue if you wish.)</p><p>Let’s consider the following two query patterns:</p><ul><li>For a given item id, get all of the item data (title, desc, etc.) along with the names of the users who liked that item.</li>\n<li>For a given user id, get all of the user data along with the item titles liked by that user.</li>\n</ul><p>These are reasonable queries for item detail and user detail pages in an application. Both will perform well with this model. Both will cause two lookups, one to query item data (or user data) and another to query user names (or item titles). As the user becomes more active (starts liking thousands of items?) or the item becomes hotter (liked by a few million users?), the number of lookups will not grow; it will remain constant at two. That’s not bad, and de-normalization may not yield much benefit like we had when moving from option 2 to option 3. However, let’s see how we can optimize further in option 4.</p><h3>Option 4: Partially de-normalized entities</h3><p><a href=\"https://www.ebayinc.com/assets/Uploads/Blog/2012/07/option4.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/assets/Uploads/Blog/2012/07/_resampled/ResizedImageWzQzOSwzNjRd/option4.png\" alt=\"\" width=\"439\" height=\"364\" title=\"\" /></a></p><p>Definitely, option 4 looks messy. In terms of savings, it’s not like what we had in option 3.</p><p>If User and Item are highly shared entities (similar to what we have at eBay), I would prefer option 3 over this option.</p><p>We’ve used the term “partially de-normalized” here because we’re not de-normalizing all item data into the User entity or all user data into the Item entity. I won’t even consider showing extreme de-normalization (keeping all item data in User and all user data in Item), as you probably agree that it doesn’t make sense for this use case.</p><p><strong>Note:</strong> I’ve used Super Column here just for demonstration purposes. Almost all the time, you should favor composite columns over Super Column.</p><h3>The best model</h3><p>The winner is Option 3, particularly for this example. We’ve left out timestamp, but let’s include it in the final model below as timeuuid(<a href=\"http://en.wikipedia.org/wiki/Universally_unique_identifier\" target=\"_blank\" rel=\"noopener\">type-1 uuid</a>). Note that timeuuid and userid together form a composite column key in User_By_Item and Item_By_User column families.</p><p>Recall that column keys are physically stored sorted. Here our column keys are stored sorted by timeuuid in both User_By_Item and Item_By_User, which makes range queries on time slots very efficient. With this model, we can efficiently query (via range scans) the <em>most recent</em> users who like a given item and the <em>most recent</em> items liked by a given user, without reading all the columns of a row.</p><p><a href=\"https://www.ebayinc.com/assets/Uploads/Blog/2012/07/optionbest.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/assets/Uploads/Blog/2012/07/_resampled/ResizedImageWzU1Myw0NTNd/optionbest.png\" alt=\"\" width=\"553\" height=\"453\" title=\"\" /></a></p><h3>Summary</h3><p>We’ve covered a few fundamental practices and walked through a detailed example to help you get started with Cassandra data model design. Here are the key takeaways:</p><ul><li>Don’t think of a relational table, but think of a nested sorted map data structure while designing Cassandra column families.</li>\n<li>Model column families around query patterns. But start your design with entities and relationships, if you can.</li>\n<li>De-normalize and duplicate for read performance. But don’t de-normalize if you don’t need to.</li>\n<li>Remember that there are many ways to model. The best way depends on your use case and query patterns.</li>\n</ul><p>What I’ve not mentioned here are special, but common, use cases such as logging, monitoring, real-time analytics (rollups, counters), or other <a href=\"http://en.wikipedia.org/wiki/Time_series\" target=\"_blank\" rel=\"noopener\">time series</a> data. However, practices discussed here do apply there. In addition, there are known common techniques or patterns used to model these time series data in Cassandra. At eBay, we also use some of those techniques and would love to share about them in upcoming posts. For more information on modeling time series data, I would recommend reading <a href=\"http://www.datastax.com/dev/blog/advanced-time-series-with-cassandra\" target=\"_blank\" rel=\"noopener\">Advanced time series with Cassandra</a> and <a href=\"http://www.datastax.com/dev/blog/metric-collection-and-storage-with-cassandra\" target=\"_blank\" rel=\"noopener\">Metric collection and storage</a>. Also, if you’re new to Cassandra, make sure to scan through <a href=\"http://www.datastax.com/docs/1.1/index\" target=\"_blank\" rel=\"noopener\">DataStax documentation</a> on Cassandra.</p><p>UPDATE:  <a title=\"Cassandra Data Modeling Best Practices, Part 2\" href=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-2/\">Part 2</a> about Cassandra is now published.</p><p>— <a title=\"About me\" href=\"http://www.jaykumarpatel.com\" rel=\"noopener\" target=\"_blank\">Jay Patel</a>, architect@eBay</p>",
        "created_at": "2018-07-17T23:18:58+0000",
        "updated_at": "2018-07-19T21:10:43+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 13,
        "domain_name": "www.ebayinc.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10866"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10865,
        "uid": null,
        "title": "Patterns of Successful Cassandra Data Modelling - OpenCredo",
        "url": "https://opencredo.com/cassandra-data-modelling-patterns/",
        "content": "<p class=\"p1\">A growing number of clients are asking OpenCredo for help with using <a href=\"http://cassandra.apache.org/\" target=\"_blank\">Apache Cassandra</a> and solving specific problems they encounter. Clients have different use cases, requirements, implementation and teams but experience similar issues. We have noticed that Cassandra data modelling problems are the most consistent cause of Cassandra failing to meet their expectations. Data modelling is one of the most complex areas of using Cassandra and has many considerations. For a business it is essential to invest resources into data modelling from the early stages of Cassandra projects; unlike operational settings that can be tuned, a Cassandra data model is very costly to fix.</p><p class=\"p1\">Cassandra is growing in popularity due to its well-advertised strengths such as high performance, fault tolerance, resilience and scalability covered in a <strong><a href=\"https://opencredo.com/fulfilling-promise-apache-cassandra/\">previous blog post</a></strong> by Guy Richardson. How well these strengths are realised in an application depends heavily on the quality of the underlying Cassandra data model. The main rule for designing a good Cassandra data model is crafting it specifically to your business domain and application use cases.</p><p class=\"p1\">Through years of experience with Cassandra engagements, we have identified a number of data modelling patterns and will cover some of them in this post. Their successful application requires a working knowledge of how Cassandra stores data. Without understanding the underlying architecture you risk making one of the common Cassandra mistakes covered in a separate post. Evaluating a seemingly valid and straightforward data model for potential pitfalls requires a level of expertise in Cassandra internals, the storage format in particular.</p><h2>The Simplicity and Complexity of Cassandra Storage</h2><p>While Cassandra architecture is relatively simple, it fundamentally limits the ways in which you can query it for data. Cassandra is designed to be a high-performance database and discourages inefficient queries. Query efficiency is determined by the way Cassandra stores data; it makes the following query patterns inefficient or impossible:</p><ul><li>fetching <em>all</em> data without identifying a subset by a partition key,</li>\n<li>fetching data from multiple partitions,</li>\n<li>joining distinct data sets,</li>\n<li>searching by values,</li>\n<li>filtering.</li>\n</ul><p>Cassandra expects applications to store the data in such a way that they can retrieve it <em>efficiently</em>. It is therefore up to the client to know the ways it will query Cassandra and design the data model accordingly upfront.</p><h3>Example: Projects by Manager and Turnover</h3><p>Consider an application that records information about <strong>project managers</strong>, their <strong>projects</strong> and project <strong>turnovers</strong>. Even for this intentionally simple use case, there are a many ways you could store the data and a number things to consider to produce a good Cassandra data model. At the very least, Cassandra requires a meaningful <em>key</em> to split data into subsets that you will later use to retrieve data. Without much knowledge of how Cassandra works, the first response might be to store this data in a simplified table:</p><div class=\"wp-geshi-highlight-wrap5\"><div class=\"wp-geshi-highlight-wrap4\"><div class=\"wp-geshi-highlight-wrap3\"><div class=\"wp-geshi-highlight-wrap2\"><div class=\"wp-geshi-highlight-wrap\"><div class=\"wp-geshi-highlight\"><div class=\"sql\"><pre class=\"de1\">CREATE TABLE projects_by_manager (\n  manager text,\n  project_id INT,\n  project_name text,\n  turnover INT,\n  PRIMARY KEY (manager, project_id)\n);</pre></div></div></div></div></div></div></div><p>In this table all data about projects and their turnovers is partitioned by manager. This data model will work if you only want to retrieve all project data for a particular manager. Disappointingly, this is the only type of query this table will support out of the box. Cassandra will not retrieve “<em>all projects with a turnover of 2 000 000″ –</em> the way in which it stored data makes this query inefficient. The reason behind this becomes obvious looking at the format of Cassandra SSTables.</p><h3>Partitioned Row Store</h3><pre>[&#13;\n{<em><strong>\"key\": \"Jack Jones\"</strong></em>,&#13;\n \"cells\": [[\"1:\",\"\",1470229749953090],&#13;\n           [\"1:project_name\",\"Cassandra Tuning\",1470229749953090],&#13;\n           [\"1:turnover\",\"5000000\",1470229749953090],&#13;\n           [\"2:\",\"\",1470229928612372],&#13;\n           [\"2:project_name\",\"Spark Layer\",1470229928612372],&#13;\n           [\"2:turnover\",\"2000000\",1470229928612372]]},&#13;\n{<em><strong>\"key\": \"Jill Hill\"</strong></em>,&#13;\n \"cells\": [[\"1:\",\"\",1470229908473768],&#13;\n           [\"1:project_name\",\"Kubernetes Setup\",1470229908473768],&#13;\n           [\"1:turnover\",\"2000000\",1470229908473768],&#13;\n           [\"2:\",\"\",1470229948844042],&#13;\n           [\"2:project_name\",\"Front End\",1470229948844042],&#13;\n           [\"2:turnover\",\"1000000\",1470229948844042]]},&#13;\n{<em><strong>\"key\": \"Richard Ford\"</strong></em>,&#13;\n \"cells\": [[\"1:\",\"\",1470229980496296],&#13;\n           [\"1:project_name\",\"Docker Training\",1470229980496296],&#13;\n           [\"1:turnover\",\"1000000\",1470229980496296]]},&#13;\n{<em><strong>\"key\": \"Maggie Bail\"</strong></em>,&#13;\n \"cells\": [[\"1:\",\"\",1470230005734692],&#13;\n           [\"1:project_name\",\"Docker Audit\",1470230005734692],&#13;\n           [\"1:turnover\",\"1000000\",1470230005734692]]}&#13;\n]&#13;\n</pre><p>Cassandra is a partitioned row store and its storage structure is in effect a nested sorted map which makes it is easy to grab a subset of data by <em>key. </em>In the previous example data is partitioned by manager name (which is the <em>key</em>) and makes it easy to retrieve projects in per-manager subsets. However, finding projects by turnover would involve checking turnover <em>values</em> for every project in every partition. Cassandra rightfully considers this inefficient, as a result such queries are not supported. Dominic Fox post “<b>How Not To Use Cassandra like an RDBMS (and what will happen if you do)</b>” [Release: 15/09/2016] will give many more examples on other queries that will be suboptimal in Cassandra in <strong>this blogpost</strong>. Luckily, there are patterns for designing Cassandra data models in a way that will be able to provide answers to most reasonable questions.</p><h2>Cassandra Data Modelling Patterns</h2><h3>Model around Business Domain</h3><p>When designing a Cassandra data model for an application, first consider the business entities you are storing and relationships between them. Your ultimate goal will be to store precomputed answers to business questions that the application asks about the stored data, an understanding its structure and meaning is a precondition for modelling these answers. Knowledge of the business domain model is also key to understanding the cardinality of certain data elements and estimating the changes in future data volumes. A data model designed around business domain will also spread data more evenly and keep partition sizes predictable. Naturally, achieving this requires close collaboration between business stakeholders and development teams.</p><h3>Denormalisation</h3><p>Unlike relational databases, Cassandra has no concept of foreign keys and does not support joining tables; both concepts are incompatible with its key architectural principles. Foreign keys have a significant negative impact on write performance while joining tables on reads is one of the inefficient querying patterns that Cassandra discourages. Cassandra demands that the structure of tables support the simplest and most efficient queries. On the other hand, writes in Cassandra are cheap and fast out of the box due to its simple storage model. As a result, it is usually a good idea to avoid extra reads at the expense of extra writes. This can be achieved by <em>balanced</em> denormalisation and data redundancy: if certain data is retrieved in multiple queries duplicate it across all tables supporting these queries.</p><h4>Example:</h4><p>While keeping data volumes and number of tables low is <strong>not</strong> a concern in Cassandra, it is still important to avoid <em>unnecessary</em> duplication. Consider a simplified data model of <strong>users</strong> leaving <strong>comments </strong>to <strong>articles </strong>and imagine retrieving the list of all comments for an article. Even though a separate table stores full user information, a good data model will also duplicate the author name in the comments table: application users want to see who wrote the comment. The following table structure would be appropriate for a common use case:</p><div class=\"wp-geshi-highlight-wrap5\"><div class=\"wp-geshi-highlight-wrap4\"><div class=\"wp-geshi-highlight-wrap3\"><div class=\"wp-geshi-highlight-wrap2\"><div class=\"wp-geshi-highlight-wrap\"><div class=\"wp-geshi-highlight\"><div class=\"sql\"><pre class=\"de1\">CREATE TABLE comments_by_article (\n  article_id INT,\n  comment_id INT,\n  comment text,\n  user_name text,\n  PRIMARY KEY (article_id, comment_id)\n);</pre></div></div></div></div></div></div></div><p>Note that storing full user information for the author with each comment would be excessive and create unnecessary duplication. It is unlikely that someone will ever want to immediately see full user information next to a comment left by that user – it is easy to retrieve upon request. It is vital to understand the types of queries the application will run to strike the right balance.</p><h3>Pre-built Result Sets</h3><p>Fast reads in Cassandra result from storing data fully ready to be read – preparing it at write time. Cassandra does not support any complex processing at query time such as filtering, joins, search, pattern and aggregation. The data must be stored partitioned, sorted, denormalised, and filtered in advance. Ideally, all that is left to Cassandra is to read the results from a single location. For this reason, having roughly a table per query is often a good approach in Cassandra. On the contrary, storing non-contextual isolated denormalised models often leads to common Cassandra anti-patterns.</p><p>When creating a Cassandra data model, determine specific questions the application will ask and the format and content of answers it will expect and create tables to store pre-built result sets. There are limited options to incrementally add support for new queries to an existing model through indexing. Besides, this approach introduces additional complexity and it is possible to avoid it by modelling data correctly in advance.</p><h3>Even Data Distribution</h3><p class=\"p1\">Understanding the distributed nature of Cassandra is key to model for predictably fast cluster performance. All nodes in a Cassandra cluster are equal by design and should run on identical hardware. Accordingly, for nodes to demonstrate comparable performance, they should bear the same load. Spreading data evenly across the cluster by choosing the right partition key helps achieve this. There are several data distribution aspects to consider when designing the data model.</p><ol><li class=\"p1\"><strong>Likely partition sizes</strong>. A single partition will always be stored in its entirety on a single node, therefore, it must be small enough to fit on that node accounting for free space required for <a href=\"https://docs.datastax.com/en/cassandra/3.x/cassandra/dml/dmlHowDataMaintain.html#dmlHowDataMaintain__dml-compaction\" target=\"_blank\">compaction</a>. For this reason, it is important to understand compaction and choose the right compaction strategy.</li>\n<li class=\"p1\"><strong>Keeping partition data bounded</strong>. If the amount of data in a single partition is likely to become too big, adding an additional partitioning column will limit its growth potential. For example, additionally bounding a partition of events by time in addition to type would provide a reasonable guarantee of reasonable partition size. That said, it is important to carefully choose the time granularity best suited to a particular use case.</li>\n<li class=\"p1\"><strong>Partition key cardinality</strong>. Choosing a column with a reasonably large number of unique values (high cardinality) is important to keep partition sizes small. However, it is important to balance this with the aim of on partition ideally being able to satisfy each query.</li>\n<li class=\"p1\"><strong>Separating read-heavy and write-heavy data. </strong>Read-heavy and write-heavy data have different scalability cycles, and measures to optimise them are different and often conflicting. For example, different compaction strategies suit read-heavy and write-heavy workflows best. For this reason, it often helps to keep such data separate even if there is a strong semantic relationship.</li>\n</ol><h3>Inserts over Updates and Deletes</h3><p>Although Cassandra supports updates and deletes, their excessive use results in unexpected operational overhead. Using them safely in Cassandra requires detailed knowledge of their implementation and operational processes in Cassandra. While a record may appear updated or deleted on the surface, physical storage will not reflect it straight away. Cassandra reconciles updates and deletes in physical storage during compaction and only under certain conditions. Use of update and delete operations in Cassandra substantially complicate cluster operations and rely on regular well scheduled repairs, monitoring and manual compaction. In our experience, data models that avoid updating or deleting data help reduce operational risks and costs.</p><h3>Testing Data Models</h3><p>Like any code, Cassandra data models need thorough testing before production use. There are too many factors that affect the performance and operations of the cluster to predict it with reasonable certainty. It is necessary to validate a Cassandra data model by testing it against real business scenarios. In fact, Cassandra ships with a <a href=\"https://docs.datastax.com/en/cassandra_win/3.0/cassandra/tools/toolsCStress.html\" target=\"_blank\">cassandra-stress</a> tool which can help load test the performance of your cluster with a chosen data model. The <a href=\"https://docs.datastax.com/en/cassandra/3.x/cassandra/tools/toolsTablehisto.html\" target=\"_blank\">nodetool tablehistograms</a> (<a href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsCFhisto.html\" target=\"_blank\">nodetool cfhistograms</a> prior to Cassandra 2.2) utility provides further table statistics. There may be a temptation to avoid testing the data model altogether because gathering fully accurate metrics is impossible. While it is rarely possible to run tests against a production-sized Cassandra cluster, testing against a smaller test cluster will highlight common data model problems. In short, prefer testing the data model for a subset of scenarios over speculating about all aspects of its performance.</p><h2>Look for the Right Balance</h2><p>Cassandra is a powerful tool but upfront investment into its setup pays off in later stages of projects. Performance and smooth operations of a Cassandra cluster depend in large part on the quality of the data model and how well it suits the application. There are many factors that shape the suitable data model and its design involves many complex decisions and tradeoffs. While data modelling in Cassandra requires a high level of expertise in its architecture, there are a number of patterns that help achieve good results. Carefully balancing these decisions helps avoid mistakes and anti-patterns that lead to common Cassandra operational problems.</p><h4>This is the second post in our blog series “Cassandra – What You May Learn The Hard Way.” Get the full overview <a href=\"https://opencredo.com/new-blog-cassandra-what-you-may-learn-the-hard-way/\" target=\"_blank\">here</a>.</h4><h4>The associated webinar, “Cassandra – The Good, the Bad, and the Ugly” was broadcast on October 6th, 2016. View the recording <strong><a href=\"https://opencredo.com/cassandra-good-bad-ugly-webinar-recording/\"> here</a>.</strong></h4><h4><a class=\"button\" href=\"#text_icl-6\">Sign up to receive updates via email</a></h4>",
        "created_at": "2018-07-17T23:18:37+0000",
        "updated_at": "2018-07-24T12:43:34+0000",
        "published_at": "2016-09-06T13:40:53+0000",
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 10,
        "domain_name": "opencredo.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10865"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 15,
            "label": "tutorial",
            "slug": "tutorial"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10864,
        "uid": null,
        "title": "How Not To Use Cassandra Like An RDBMS (and what will happen if you do) - OpenCredo",
        "url": "https://opencredo.com/how-not-to-use-cassandra-like-an-rdbms-and-what-will-happen-if-you-do/",
        "content": "<p>Cassandra isn’t a relational database management system, but it has some features that make it look a bit like one. Chief among these is CQL, a query language with an SQL-like syntax. CQL isn’t a bad thing in itself – in fact it’s very convenient – but it can be misleading since it gives developers the illusion that they are working with a familiar data model, when things are really very different under the hood. Not only is Cassandra <em>not</em> an RDBMS, it’s not even <em>like</em> an RDBMS in some of the ways you might expect. In this post I’ll review a few example scenarios where a beginner might be unpleasantly surprised by the differences, and suggest some remedies.</p><h2>Example 1: querying by non-key columns</h2><p>Here’s some CQL to create a “shopping trolley contents” table in Cassandra:</p><div class=\"wp-geshi-highlight-wrap5\"><div class=\"wp-geshi-highlight-wrap4\"><div class=\"wp-geshi-highlight-wrap3\"><div class=\"wp-geshi-highlight-wrap2\"><div class=\"wp-geshi-highlight-wrap\"><div class=\"wp-geshi-highlight\"><div class=\"cql\"><pre class=\"de1\">CREATE TABLE shoppingTrolleyContents (&#13;\n trolleyId timeuuid,&#13;\n lineItemId timeuuid,&#13;\n itemId text,&#13;\n qty int,&#13;\n unitPrice decimal,&#13;\n PRIMARY KEY(trolleyId, lineItemId)&#13;\n) WITH CLUSTERING ORDER BY (lineItemId ASC);</pre></div></div></div></div></div></div></div><p>Suppose we wanted to find all of the shopping trolleys that contains a particular item, identified by <code>itemId</code>. Here’s a query that would make perfect sense in SQL, that Cassandra will not allow you to perform:</p><div class=\"wp-geshi-highlight-wrap5\"><div class=\"wp-geshi-highlight-wrap4\"><div class=\"wp-geshi-highlight-wrap3\"><div class=\"wp-geshi-highlight-wrap2\"><div class=\"wp-geshi-highlight-wrap\"><div class=\"wp-geshi-highlight\"><div class=\"cql\"><pre class=\"de1\">// Find all the trolleys that contain a particular item&#13;\nSELECT trolleyId FROM shoppingTrolleyContents WHERE itemId = 'SKU001';</pre></div></div></div></div></div></div></div><p>If you try to run this query through CQLSH, you will get the response “No secondary indexes on the restricted columns support the provided operators”. What this means is that the columns in this table are indexed only by the columns listed in the <code>PRIMARY KEY</code> clause. Without defining a “secondary index” there is no way to search for values using a <code>WHERE</code> clause restricting on any other column.</p><p><em>OK</em>, you may think, <em>so I have to define secondary indexes explicitly</em>. A quick <a href=\"https://docs.datastax.com/en/cql/3.1/cql/cql_reference/create_index_r.html\">Google search</a> later, and you have:</p><div class=\"wp-geshi-highlight-wrap5\"><div class=\"wp-geshi-highlight-wrap4\"><div class=\"wp-geshi-highlight-wrap3\"><div class=\"wp-geshi-highlight-wrap2\"><div class=\"wp-geshi-highlight-wrap\"><div class=\"wp-geshi-highlight\"><div class=\"cql\"><pre class=\"de1\">CREATE INDEX trolleyByItemId ON shoppingTrolleyContents (itemId);</pre></div></div></div></div></div></div></div><p>You can now run the query you wanted. Unfortunately, while restrictions on primary indexes allow queries to be routed efficiently to the specific nodes in the Cassandra cluster that hold the data you’re looking for, the secondary index must be consulted on every node of the Cassandra cluster in order to find matching records. Use of secondary indexes can thus severely hamper the scalability of Cassandra, especially if the indexed column has a high cardinality (i.e. can contain many distinct values).</p><p>In this scenario, it is probably better to have a second table that explicitly provides for <code>trolleyId</code> lookups by <code>itemId</code>:</p><div class=\"wp-geshi-highlight-wrap5\"><div class=\"wp-geshi-highlight-wrap4\"><div class=\"wp-geshi-highlight-wrap3\"><div class=\"wp-geshi-highlight-wrap2\"><div class=\"wp-geshi-highlight-wrap\"><div class=\"wp-geshi-highlight\"><div class=\"cql\"><pre class=\"de1\">CREATE TABLE lineItemsByItemId (&#13;\n itemId text,&#13;\n trolleyId timeuuid,&#13;\n lineItemId timeuuid,&#13;\n PRIMARY KEY(itemId, trolleyId, lineItemId)&#13;\n) WITH CLUSTERING ORDER BY (trolleyId DESC, lineItemId ASC);</pre></div></div></div></div></div></div></div><p>However, you now have to consider the overhead of maintaining this table, and the fact that Cassandra does not support transactional updates to multiple tables with ACID guarantees.</p><p>Suppose that a line item is removed from a trolley, and a user queries for orders containing line items with that <code>itemId</code> before the <code>lineItemsByItemId</code> table has been updated. The returned <code>trolleyId</code>/<code>lineItemId</code> pair will then refer to a record in the <code>shoppingTrolleyContents</code> table that no longer exists. This is precisely the scenario that the referential integrity mechanisms (foreign key constraints) provided by an RDBMS are designed to avoid – but Cassandra provides <em>no</em> mechanisms for ensuring referential integrity between tables.</p><h2>Example 2: joining tables</h2><p>Suppose we search in <code>lineItemsByItemId</code> for all of the line items with <code>itemId='SKU0001'</code>:</p><div class=\"wp-geshi-highlight-wrap5\"><div class=\"wp-geshi-highlight-wrap4\"><div class=\"wp-geshi-highlight-wrap3\"><div class=\"wp-geshi-highlight-wrap2\"><div class=\"wp-geshi-highlight-wrap\"><div class=\"wp-geshi-highlight\"><div class=\"cql\"><pre class=\"de1\">SELECT trolleyId, lineItemId FROM lineItemsByItemId WHERE itemId='SKU0001';</pre></div></div></div></div></div></div></div><p>Because Cassandra doesn’t support <code>JOIN</code>s, if we get back 1,000 <code>trolleyId</code>/<code>lineItemId</code> pairs, we must then retrieve each line item’s details separately from the <code>shoppingTrolleyContents</code> table in order to obtain the associated quantity and price data. This is tremendously inefficient. It may be better to denormalise further, copying all of the relevant columns from <code>shoppingTrolleyContents</code> into <code>lineItemsByItemId</code>:</p><div class=\"wp-geshi-highlight-wrap5\"><div class=\"wp-geshi-highlight-wrap4\"><div class=\"wp-geshi-highlight-wrap3\"><div class=\"wp-geshi-highlight-wrap2\"><div class=\"wp-geshi-highlight-wrap\"><div class=\"wp-geshi-highlight\"><div class=\"cql\"><pre class=\"de1\">CREATE TABLE lineItemsByItemId (&#13;\n itemId text,&#13;\n trolleyId timeuuid,&#13;\n lineItemId timeuuid,&#13;\n qty int,&#13;\n unitPrice decimal,&#13;\n PRIMARY KEY(itemId, trolleyId, lineItemId)&#13;\n) WITH CLUSTERING ORDER BY (trolleyId DESC, lineItemId ASC);</pre></div></div></div></div></div></div></div><p>This kind of duplication of data is anathema in relational database design. In the Cassandra world, it’s commonplace and necessary in order to support efficient querying.</p><h2>Example 3: reverse lookups</h2><p>A shopping trolley usually has an owner, and perhaps some other metadata associated with it. Here’s the CQL for a table which associates user ids with shopping trolley ids:</p><div class=\"wp-geshi-highlight-wrap5\"><div class=\"wp-geshi-highlight-wrap4\"><div class=\"wp-geshi-highlight-wrap3\"><div class=\"wp-geshi-highlight-wrap2\"><div class=\"wp-geshi-highlight-wrap\"><div class=\"wp-geshi-highlight\"><div class=\"cql\"><pre class=\"de1\">CREATE TABLE shoppingTrolleyOwners (&#13;\n trolleyId timeuuid,&#13;\n ownerId text,&#13;\n PRIMARY KEY (trolleyId, ownerId)&#13;\n) WITH CLUSTERING ORDER BY (ownerId ASC);</pre></div></div></div></div></div></div></div><p>We can use this to find out who owns a given trolley by id:</p><div class=\"wp-geshi-highlight-wrap5\"><div class=\"wp-geshi-highlight-wrap4\"><div class=\"wp-geshi-highlight-wrap3\"><div class=\"wp-geshi-highlight-wrap2\"><div class=\"wp-geshi-highlight-wrap\"><div class=\"wp-geshi-highlight\"><div class=\"cql\"><pre class=\"de1\">SELECT ownerID FROM shoppingTrolleyOwners WHERE trolleyId='';</pre></div></div></div></div></div></div></div><p>However, the reverse lookup, to find trolleys by owner, is not supported, even though ownerId is a primary index key:</p><div class=\"wp-geshi-highlight-wrap5\"><div class=\"wp-geshi-highlight-wrap4\"><div class=\"wp-geshi-highlight-wrap3\"><div class=\"wp-geshi-highlight-wrap2\"><div class=\"wp-geshi-highlight-wrap\"><div class=\"wp-geshi-highlight\"><div class=\"cql\"><pre class=\"de1\">SELECT trolleyId FROM shoppingTrolleyOwners WHERE ownerId='bob';</pre></div></div></div></div></div></div></div><p>The error we get back if we try to run the above is:</p><pre>Cannot execute this query as it might involve data filtering and thus may have unpredictable performance. If you want to execute this query despite the performance unpredictability, use ALLOW FILTERING.&#13;\n</pre><p>Once again, if Cassandra doesn’t immediately allow you to do something, it’s worth making sure you understand why before just going ahead and adding <code>ALLOW FILTERING</code> to the query to “make it work”.</p><p>The problem here is that the <code>trolleyId</code> is used as the <em>partition</em> key, which determines which node in the Cassandra cluster the data is stored on, while the <code>ownerId</code> is used as the <em>cluster</em> key, which indexes data within the partition. In order to perform the query as written, Cassandra would have to send it to all of the nodes within the cluster, obtain the records for all <code>trolleyId</code> values within each partition, and then filter them by <code>ownerId</code>. This has the potential to be very inefficient.</p><p>A better solution, in this case, would be to create a second table to support the reverse lookup:</p><div class=\"wp-geshi-highlight-wrap5\"><div class=\"wp-geshi-highlight-wrap4\"><div class=\"wp-geshi-highlight-wrap3\"><div class=\"wp-geshi-highlight-wrap2\"><div class=\"wp-geshi-highlight-wrap\"><div class=\"wp-geshi-highlight\"><div class=\"cql\"><pre class=\"de1\">CREATE TABLE shoppingTrolleysByOwner (&#13;\n ownerId text,&#13;\n trolleyId timeuuid,&#13;\n PRIMARY KEY (ownerId, trolleyId)&#13;\n) WITH CLUSTERING ORDER BY (trolleyId DESC);</pre></div></div></div></div></div></div></div><p>Once again, the previous warnings about data consistency apply: both tables must be written to whenever a shopping trolley is created, and there are no ACID transactions to ensure that this pair of updates is carried out atomically.</p><h2>Conclusions</h2><p>Cassandra can usefully be thought of as something like a distributed version of the storage engine that an RDBMS might use, but without the data management capabilities that an RDBMS brings in addition to the ability to persist and retrieve data.</p><p>It is certainly possible to build an RDBMS-like system on top of Cassandra, but doing so will progressively negate all of the performance and scalability advantages that Cassandra’s distributed data model provides. If you find yourself doing this – implementing in-memory joins, distributed locks or other mechanisms to get RDBMS-like behaviour out of a Cassandra-backed system – then it may be time to stop and reconsider. Will Cassandra still deliver the performance you want, if you use it in this way?</p><p>An RDBMS provides a consistent, principled approach to data management that works for all cases, at the cost of limited scalability: you pay the “relational tax” in exchange for a highly flexible yet predictable data model, By contrast, Cassandra’s is a “pay for what you use” model, in which data integrity is an application-level concern, and specific query patterns must be explicitly supported through decisions made about the physical layout of data across the cluster. It “unsolves” some of the problems the RDBMS was invented to solve, in order to address different problems more effectively.</p><p>For many purposes, a dual or polyglot approach is worth considering. Use Cassandra as the primary data store for capturing information as it arrives into the system; but then build “query-optimised views” of subsets of that data in other databases specialised to the kinds of access users require. Use an indexing engine such as SOLR to provide full-text search across records, or a graph database such as Neo4j to store metadata in a way that supports “traversal-heavy” queries that join together many different kinds of entity. Use an RDBMS when you need the full power and flexibility of SQL to express ad hoc queries that explore the data in multiple dimensions.</p><p>Alternatively, a stream-processing approach may be the way to go when you need to generate complex reports which consume large amounts of captured data. Stream the records you have captured in Cassandra into an Apache Spark cluster, and use Spark SQL to execute complex queries using filters, joins and aggregations. This approach, which I call “write first, reason later”, is especially suited to event-driven systems where capture of event information can be decoupled from decision-making about how to react to events in the aggregate.</p><h4>This is the third post in our blog series “Cassandra – What You May Learn The Hard Way.” Get the full overview <a href=\"https://opencredo.com/new-blog-cassandra-what-you-may-learn-the-hard-way/\" target=\"_blank\">here</a>.</h4><h4>The associated webinar, “Cassandra – The Good, the Bad, and the Ugly” was broadcast on October 6th, 2016. View the recording <strong><a href=\"https://opencredo.com/cassandra-good-bad-ugly-webinar-recording/\"> here</a>.</strong></h4><h4><a class=\"button\" href=\"#text_icl-6\">Sign up to receive updates via email</a></h4>",
        "created_at": "2018-07-17T23:18:19+0000",
        "updated_at": "2018-08-28T15:48:49+0000",
        "published_at": "2016-09-15T12:38:18+0000",
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 7,
        "domain_name": "opencredo.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10864"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10852,
        "uid": null,
        "title": "Cassandra",
        "url": "http://jamirtostudycassand.blogspot.com/?view=flipcard&_escaped_fragment_=",
        "content": "<div id=\"main\" tabindex=\"0\"><div id=\"controls\"><ul id=\"groups\"><li class=\"group selected\" data-category=\"none\" title=\"Recent\">Recent</li>\n    <li class=\"group\" data-category=\"published\" title=\"Group by date\">Date</li>\n    \n    \n    <li class=\"group\" data-category=\"tag\" title=\"Group by label\">Label</li>\n    <li class=\"group\" data-category=\"author\" title=\"Group by author\">Author</li>\n    \n  </ul></div><div id=\"content\" class=\"items hfeed\"><div class=\"item hentry\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"1913204634856192233\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>Read Repair Chance</p></div></div><div class=\"back\"><div class=\"overlay\"><p>Read Repair Chance</p></div></div></div></div><div class=\"item hentry cql\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"2041937350109885085\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>TTL &amp; GC Grace Seconds</p></div></div><div class=\"back\"><div class=\"overlay\"><p>TTL &amp; GC Grace Seconds</p></div></div></div></div><div class=\"item hentry cql\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"1891457791712844370\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>Update statement</p></div></div><div class=\"back\"><div class=\"overlay\"><p>Update statement</p></div></div></div></div><div class=\"item hentry compaction\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"1783813783959014593\"><div class=\"card\"><div class=\"back\"><div class=\"overlay\"><p>Compaction</p></div></div></div></div><div class=\"item hentry cql\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"1157327081616354647\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>Speculative Query Execution</p></div></div><div class=\"back\"><div class=\"overlay\"><p>Speculative Query Execution</p></div></div></div></div><div class=\"item hentry cql\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"5195058995149499625\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>Alter type</p></div></div><div class=\"back\"><div class=\"overlay\"><p>Alter type</p></div></div></div></div><div class=\"item hentry cql\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"1288124925756736023\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>UDT- User Defined Type</p></div></div><div class=\"back\"><div class=\"overlay\"><p>UDT- User Defined Type</p></div></div></div></div><div class=\"item hentry cql\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"8447059258592441829\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>Collection</p></div></div><div class=\"back\"><div class=\"overlay\"><p>Collection</p></div></div></div></div><div class=\"item hentry cql\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"2141792041004541094\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>UUID &amp; timeUUID</p></div></div><div class=\"back\"><div class=\"overlay\"><p>UUID &amp; timeUUID</p></div></div></div></div><div class=\"item hentry cql\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"9019476210604726110\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>Counter</p></div></div><div class=\"back\"><div class=\"overlay\"><p>Counter</p></div></div></div></div><div class=\"item hentry cql\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"3963260535116226204\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>Secondary Index</p></div></div><div class=\"back\"><div class=\"overlay\"><p>Secondary Index</p></div></div></div></div><div class=\"item hentry cql\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"3297642472741434543\"><div class=\"card\"><div class=\"back\"><div class=\"overlay\"><p>CQL Datatypes</p></div></div></div></div><div class=\"item hentry cql\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"2319619481996271057\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>Static-Column</p></div></div><div class=\"back\"><div class=\"overlay\"><p>Static-Column</p></div></div></div></div><div class=\"item hentry cql\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"7021010111362713678\"><div class=\"card\"><div class=\"back\"><div class=\"overlay\"><p>PK-Partition Key &amp; Clustering Column</p></div></div></div></div><div class=\"item hentry architecture\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"8578350354275841691\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>System Keyspace</p></div></div><div class=\"back\"><div class=\"overlay\"><p>System Keyspace</p></div></div></div></div><div class=\"item hentry architecture\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"5571035908053558407\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>Gossip Protocol</p></div></div><div class=\"back\"><div class=\"overlay\"><p>Gossip Protocol</p></div></div></div></div><div class=\"item hentry architecture\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"1122676304810193494\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>Anti Entropy</p></div></div><div class=\"back\"><div class=\"overlay\"><p>Anti Entropy</p></div></div></div></div><div class=\"item hentry architecture\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"4024455211478567129\"><div class=\"card\"><div class=\"back\"><div class=\"overlay\"><p>Consistency Level</p></div></div></div></div><div class=\"item hentry architecture\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"2955905687547609276\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>Replication</p></div></div><div class=\"back\"><div class=\"overlay\"><p>Replication</p></div></div></div></div><div class=\"item hentry architecture\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"746497635863670884\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>Request co-ordination</p></div></div></div></div><div class=\"item hentry tools\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"3181451439816618420\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>cassandra-tools</p></div></div><div class=\"back\"><div class=\"overlay\"><p>cassandra-tools</p></div></div></div></div><div class=\"item hentry tools\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"2344276926655082204\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>cassandra-stress</p></div></div><div class=\"back\"><div class=\"overlay\"><p>cassandra-stress</p></div></div></div></div><div class=\"item hentry tools\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"1523223555633789091\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>nodetool</p></div></div><div class=\"back\"><div class=\"overlay\"><p>nodetool</p></div></div></div></div></div></div>\n<p>Loading</p>",
        "created_at": "2018-07-17T17:16:48+0000",
        "updated_at": "2018-07-17T17:16:54+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "jamirtostudycassand.blogspot.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10852"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 15,
            "label": "tutorial",
            "slug": "tutorial"
          },
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          }
        ],
        "is_public": false,
        "id": 10844,
        "uid": null,
        "title": "Cassandra schema migrations made easy with Apache Spark",
        "url": "http://batey.info/cassandra-schema-migrations-made-easy.html",
        "content": "By far the most common question I get asked when talking about Cassandra is once you've denormalised based on your queries what happens if you were wrong or a new requirement comes in that requires a new type of query.\n<p>First I always check that it is a real requirement to be able to have this new functionality on old data. If that's not the case, and often it isn't, then you can just start double/triple writing into the new table.\n</p><p>However if you truly need to have the new functionality on old data then Spark can come to the rescue. The first step is to still double write. We can then backfill using Spark. The awesome thing is that nearly all writes in Cassandra are idempotent, so when we backfill we don't need to worry about inserting data that was already inserted via the new write process.\n</p><p>Let's see an example. Suppose you were storing customer events so you know what they are up to. At first you want to query by customer/time so you end up with following table:\n</p><p>Then the requirement comes in to be able to look for events by staff member. My reaction a couple of years ago would have been something like this:\n</p><div class=\"separator\"><a href=\"http://4.bp.blogspot.com/-ocpWDOp6ZHM/VOXt4fiOlII/AAAAAAAAAYQ/KBdlMbLjO4I/s1600/oh-noes-everybody-panic.gif\"><img border=\"0\" src=\"http://4.bp.blogspot.com/-ocpWDOp6ZHM/VOXt4fiOlII/AAAAAAAAAYQ/KBdlMbLjO4I/s1600/oh-noes-everybody-panic.gif\" alt=\"image\" /></a>\n</div><br />However if you have Spark workers on each of your Cassandra nodes then this is not an issue.<p>Assuming you want to a new table keyed by staff_id and have modified your application to double write you do the back fill with Spark. Here's the new table:\n</p><p>Then open up a Spark-shell (or submit a job) with the Spark-Cassandra connector on the classpath and all you'll need is something like this:\n</p><p>How can a few lines do so much! If you're in a shell obviously you don't even need to create a SparkContext. What will happen here is the Spark workers will process the partitions on a Cassandra node that owns the data for the customer table (original table) and insert it back into Cassandra locally. Cassandra will then handle the replication to the correct nodes for the staff table.\n</p><p>This is the least network traffic you could hope to achieve. Any solution that you write your self with Java/Python/Shell will involve pulling the data back to your application and pushing it to a new node, which will then need to replicate it for the new table.\n</p><p>You won't want to do this at a peak time as this will HAMMER you Cassandra cluster as Spark is going to do this quickly. If you have a small DC for just running the Spark jobs and let it asynchronously replicate to your operational DC this is less of a concern.\n  </p>",
        "created_at": "2018-07-17T11:47:17+0000",
        "updated_at": "2018-07-17T11:47:29+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 2,
        "domain_name": "batey.info",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10844"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 21,
            "label": "lucene",
            "slug": "lucene"
          },
          {
            "id": 23,
            "label": "elasticsearch",
            "slug": "elasticsearch"
          },
          {
            "id": 36,
            "label": "solr",
            "slug": "solr"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 1068,
            "label": "graph",
            "slug": "graph"
          }
        ],
        "is_public": false,
        "id": 10842,
        "uid": null,
        "title": "JanusGraph: Distributed graph database",
        "url": "http://janusgraph.org/",
        "content": "<div style=\"text-align: center;\">\n  <a href=\"http://docs.janusgraph.org/latest/\">Docs</a> •\n  <a href=\"https://github.com/JanusGraph/janusgraph/\">GitHub</a> •\n  <a href=\"https://github.com/JanusGraph/janusgraph/releases/\">Download</a><br /><img class=\"janusgraph\" src=\"http://janusgraph.org/images/janusgraph.png\" alt=\"image\" /></div><p>JanusGraph is a scalable <a href=\"http://en.wikipedia.org/wiki/Graph_database\">graph\ndatabase</a> optimized for storing and\nquerying graphs containing hundreds of billions of vertices and edges\ndistributed across a multi-machine cluster. JanusGraph is a transactional\ndatabase that can support thousands of concurrent users executing complex graph\ntraversals in real time.</p><p>In addition, JanusGraph provides the following features:</p><ul><li>Elastic and linear scalability for a growing data and user base.</li>\n  <li>Data distribution and replication for performance and fault tolerance.</li>\n  <li>Multi-datacenter high availability and hot backups.</li>\n  <li>Support for <a href=\"http://en.wikipedia.org/wiki/ACID\">ACID</a> and\n<a href=\"http://en.wikipedia.org/wiki/Eventual_consistency\">eventual consistency</a>.</li>\n  <li>Support for various storage backends:\n    <ul><li><a href=\"http://cassandra.apache.org\">Apache Cassandra®</a></li>\n      <li><a href=\"http://hbase.apache.org\">Apache HBase®</a></li>\n      <li><a href=\"https://cloud.google.com/bigtable\">Google Cloud Bigtable</a></li>\n      <li><a href=\"http://www.oracle.com/technetwork/database/berkeleydb/overview/index-093405.html\">Oracle BerkeleyDB</a></li>\n    </ul></li>\n  <li>Support for global <a href=\"http://tinkerpop.apache.org/docs/3.2.4/reference/#graphcomputer\">graph data analytics</a>, reporting, and ETL through integration with big data\nplatforms:\n    <ul><li><a href=\"http://spark.apache.org\">Apache Spark™</a></li>\n      <li><a href=\"http://giraph.apache.org\">Apache Giraph™</a></li>\n      <li><a href=\"http://hadoop.apache.org\">Apache Hadoop®</a></li>\n    </ul></li>\n  <li>Support for geo, numeric range, and full-text search via:\n    <ul><li><a href=\"http://www.elasticsearch.org\">ElasticSearch™</a></li>\n      <li><a href=\"http://lucene.apache.org/solr\">Apache Solr™</a></li>\n      <li><a href=\"http://lucene.apache.org\">Apache Lucene®</a></li>\n    </ul></li>\n  <li>Native integration with the <a href=\"http://tinkerpop.apache.org\">Apache TinkerPop™</a> graph stack:\n    <ul><li><a href=\"http://tinkerpop.apache.org/docs/3.2.4/reference/#traversal\">Gremlin graph query language</a></li>\n      <li><a href=\"http://tinkerpop.apache.org/docs/3.2.4/reference/#gremlin-server\">Gremlin graph server</a></li>\n      <li><a href=\"http://tinkerpop.apache.org/docs/3.2.4/reference/#gremlin-applications\">Gremlin applications</a></li>\n    </ul></li>\n  <li>Open source under the <a href=\"http://www.apache.org/licenses/LICENSE-2.0.html\">Apache 2</a> license.</li>\n  <li>You can visualize graphs stored in JanusGraph via any of the following tools:\n    <ul><li><a href=\"http://www.cytoscape.org/\">Cytoscape</a></li>\n      <li><a href=\"http://tinkerpop.apache.org/docs/current/reference/#gephi-plugin\">Gephi</a>\nplugin for Apache TinkerPop</li>\n      <li><a href=\"https://github.com/bricaud/graphexp\">Graphexp</a></li>\n      <li><a href=\"https://cambridge-intelligence.com/visualizing-janusgraph-new-titandb-fork/\">KeyLines by Cambridge Intelligence</a></li>\n      <li><a href=\"https://doc.linkurio.us/ogma/latest/tutorials/janusgraph/\">Linkurious</a></li>\n    </ul></li>\n</ul><p>You can <a href=\"https://github.com/JanusGraph/janusgraph/releases\">download</a> JanusGraph\nor <a href=\"https://github.com/JanusGraph/janusgraph\">clone</a> from GitHub.</p><p>Read the <a href=\"http://docs.janusgraph.org/latest\">JanusGraph documentation</a> and join the\n<a href=\"https://groups.google.com/group/janusgraph-users\">users</a> or\n<a href=\"https://groups.google.com/group/janusgraph-dev\">developers</a> mailing lists.</p><p>Follow the <a href=\"http://docs.janusgraph.org/latest/getting-started.html\">Getting Started with JanusGraph</a> guide for a step-by-step introduction.</p><p>JanusGraph is a project under <a href=\"https://www.linux.com/blog/Linux-Foundation-welcomes-JanusGraph\">The Linux\nFoundation</a>,\nand includes participants from Expero, Google, GRAKN.AI, Hortonworks, IBM and Amazon.</p><h2 id=\"presentations\">Presentations</h2><p>Here is a selection of JanusGraph presentations:</p><ul><li>\n    <p><a href=\"https://www.slideshare.net/ptgoetz/large-scale-graph-analytics-with-janusgraph\">DataWorksJun2017: Large Scale Graph Analytics with JanusGraph</a>, P. Taylor Goetz, 2017.06.13</p>\n  </li>\n  <li>\n    <p><a href=\"https://www.slideshare.net/HBaseCon/communitydriven-graphs-with-janusgraph-77117443\">HBaseCon2017 Community-Driven Graphs with JanusGraph</a>, Jing Chen He &amp; Jason Plurad, 2017.06.12</p>\n  </li>\n</ul><h2 id=\"users\">Users</h2><p>The following users have deployed JanusGraph in production.</p><p><a href=\"https://www.celum.com/en/graph-driven-and-reactive-architecture\" class=\"logo\"><img src=\"http://janusgraph.org/images/logos/celum.png\" alt=\"celum\" class=\"logo\" /></a>\n<a href=\"https://www.compose.com/databases/janusgraph\" class=\"logo\"><img src=\"http://janusgraph.org/images/logos/compose.png\" alt=\"Compose\" class=\"logo\" /></a>\n<a href=\"https://finc.com\" class=\"logo\"><img src=\"http://janusgraph.org/images/logos/finc.png\" alt=\"FiNC\" class=\"logo\" /></a>\n<a href=\"https://gdatasoftware.com\" class=\"logo\"><img src=\"http://janusgraph.org/images/logos/gdata.png\" alt=\"G DATA\" class=\"logo\" /></a>\n<a href=\"https://www.360.cn\" class=\"logo\"><img src=\"http://janusgraph.org/images/logos/qihoo_360.png\" alt=\"Qihoo 360\" class=\"logo\" /></a>\n<a href=\"https://www.redhat.com\" class=\"logo\"><img src=\"http://janusgraph.org/images/logos/redhat.png\" alt=\"Red Hat\" class=\"logo\" /></a>\n<a href=\"https://seeq.com\" class=\"logo\"><img src=\"http://janusgraph.org/images/logos/seeq.png\" alt=\"Seeq\" class=\"logo\" /></a>\n<a href=\"http://denmarkblog.timesinternet.in/blogs/graph/times-internet-is-using-janusgraph-as-main-database-in-cms-for-all-newsrooms/articleshow/63709837.cms\" class=\"logo\"><img src=\"http://janusgraph.org/images/logos/timesinternet.png\" alt=\"Times Internet\" class=\"logo\" /></a></p>",
        "created_at": "2018-07-16T15:20:02+0000",
        "updated_at": "2018-07-16T15:20:31+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 1,
        "domain_name": "janusgraph.org",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10842"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 228,
            "label": "dotnet",
            "slug": "net"
          },
          {
            "id": 873,
            "label": "mysql",
            "slug": "mysql"
          },
          {
            "id": 926,
            "label": "postgres",
            "slug": "postgres"
          },
          {
            "id": 1114,
            "label": "orm",
            "slug": "orm"
          }
        ],
        "is_public": false,
        "id": 10838,
        "uid": null,
        "title": "MiracleDevs/Paradigm.ORM",
        "url": "https://github.com/MiracleDevs/Paradigm.ORM",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\"><p><a href=\"https://travis-ci.org/MiracleDevs/Paradigm.ORM\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a10b3ac4adbc98d739ba7dc38052039ce7e10730/68747470733a2f2f7472617669732d63692e6f72672f4d697261636c65446576732f506172616469676d2e4f524d2e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/MiracleDevs/Paradigm.ORM.svg?branch=master\" /></a></p>\n<table><thead><tr><th>Library</th>\n<th>Nuget</th>\n<th>Install</th>\n</tr></thead><tbody><tr><td>Data</td>\n<td><a href=\"https://www.nuget.org/packages/Paradigm.ORM.Data/\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/67a06bea41473cc287b680dc648a4d95c8a0fc4f/68747470733a2f2f696d672e736869656c64732e696f2f6e756765742f762f4e756765742e436f72652e737667\" alt=\"NuGet\" data-canonical-src=\"https://img.shields.io/nuget/v/Nuget.Core.svg\" /></a></td>\n<td><code>Install-Package Paradigm.ORM.Data</code></td>\n</tr><tr><td>MySql</td>\n<td><a href=\"https://www.nuget.org/packages/Paradigm.ORM.Data.MySql/\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/67a06bea41473cc287b680dc648a4d95c8a0fc4f/68747470733a2f2f696d672e736869656c64732e696f2f6e756765742f762f4e756765742e436f72652e737667\" alt=\"NuGet\" data-canonical-src=\"https://img.shields.io/nuget/v/Nuget.Core.svg\" /></a></td>\n<td><code>Install-Package Paradigm.ORM.Data.MySql</code></td>\n</tr><tr><td>PostgreSQL</td>\n<td><a href=\"https://www.nuget.org/packages/Paradigm.ORM.Data.PostgreSql/\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/67a06bea41473cc287b680dc648a4d95c8a0fc4f/68747470733a2f2f696d672e736869656c64732e696f2f6e756765742f762f4e756765742e436f72652e737667\" alt=\"NuGet\" data-canonical-src=\"https://img.shields.io/nuget/v/Nuget.Core.svg\" /></a></td>\n<td><code>Install-Package Paradigm.ORM.Data.PostgreSql</code></td>\n</tr><tr><td>SQL Server</td>\n<td><a href=\"https://www.nuget.org/packages/Paradigm.ORM.Data.SqlServer/\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/67a06bea41473cc287b680dc648a4d95c8a0fc4f/68747470733a2f2f696d672e736869656c64732e696f2f6e756765742f762f4e756765742e436f72652e737667\" alt=\"NuGet\" data-canonical-src=\"https://img.shields.io/nuget/v/Nuget.Core.svg\" /></a></td>\n<td><code>Install-Package Paradigm.ORM.Data.SqlServer</code></td>\n</tr><tr><td>Cassandra</td>\n<td><a href=\"https://www.nuget.org/packages/Paradigm.ORM.Data.Cassandra/\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/67a06bea41473cc287b680dc648a4d95c8a0fc4f/68747470733a2f2f696d672e736869656c64732e696f2f6e756765742f762f4e756765742e436f72652e737667\" alt=\"NuGet\" data-canonical-src=\"https://img.shields.io/nuget/v/Nuget.Core.svg\" /></a></td>\n<td><code>Install-Package Paradigm.ORM.Data.Cassandra</code></td>\n</tr></tbody></table>\n<p>.NET Core ORM with dbfirst support, and code scaffolding features. This ORM supports different database sources.</p>\n<h2><a id=\"user-content-self-contained-deploy-scd\" class=\"anchor\" aria-hidden=\"true\" href=\"#self-contained-deploy-scd\"></a>Self Contained Deploy (SCD)</h2>\n<p>Bellow you can find portable versions for all major OSs.\nIf you are planning to use the tools in several projects, we recommend to add the SCD folder to your PATH.</p>\n<table><thead><tr><th>Tool</th>\n<th>OS</th>\n<th>Zip File</th>\n</tr></thead><tbody><tr><td>DbFirst</td>\n<td>Windows x86</td>\n<td><a href=\"https://raw.githubusercontent.com/MiracleDevs/Paradigm.ORM/master/dist/dbfirst.win-x86.zip\" rel=\"nofollow\">Download</a></td>\n</tr><tr><td>DbFirst</td>\n<td>Windows x64</td>\n<td><a href=\"https://raw.githubusercontent.com/MiracleDevs/Paradigm.ORM/master/dist/dbfirst.win-x64.zip\" rel=\"nofollow\">Download</a></td>\n</tr><tr><td>DbFirst</td>\n<td>Linux x64</td>\n<td><a href=\"https://raw.githubusercontent.com/MiracleDevs/Paradigm.ORM/master/dist/dbfirst.linux-x64.zip\" rel=\"nofollow\">Download</a></td>\n</tr><tr><td>DbFirst</td>\n<td>OSX x64</td>\n<td><a href=\"https://raw.githubusercontent.com/MiracleDevs/Paradigm.ORM/master/dist/dbfirst.osx-x64.zip\" rel=\"nofollow\">Download</a></td>\n</tr><tr><td>\n</td><td>\n</td><td>\n</td></tr><tr><td>DbPublisher</td>\n<td>Windows x86</td>\n<td><a href=\"https://raw.githubusercontent.com/MiracleDevs/Paradigm.ORM/master/dist/dbpublisher.win-x86.zip\" rel=\"nofollow\">Download</a></td>\n</tr><tr><td>DbPublisher</td>\n<td>Windows x64</td>\n<td><a href=\"https://raw.githubusercontent.com/MiracleDevs/Paradigm.ORM/master/dist/dbpublisher.win-x64.zip\" rel=\"nofollow\">Download</a></td>\n</tr><tr><td>DbPublisher</td>\n<td>Linux x64</td>\n<td><a href=\"https://raw.githubusercontent.com/MiracleDevs/Paradigm.ORM/master/dist/dbpublisher.linux-x64.zip\" rel=\"nofollow\">Download</a></td>\n</tr><tr><td>DbPublisher</td>\n<td>OSX x64</td>\n<td><a href=\"https://raw.githubusercontent.com/MiracleDevs/Paradigm.ORM/master/dist/dbpublisher.osx-x64.zip\" rel=\"nofollow\">Download</a></td>\n</tr><tr><td>\n</td><td>\n</td><td>\n</td></tr><tr><td>DataExport</td>\n<td>Windows x86</td>\n<td><a href=\"https://raw.githubusercontent.com/MiracleDevs/Paradigm.ORM/master/dist/dataexport.win-x86.zip\" rel=\"nofollow\">Download</a></td>\n</tr><tr><td>DataExport</td>\n<td>Windows x64</td>\n<td><a href=\"https://raw.githubusercontent.com/MiracleDevs/Paradigm.ORM/master/dist/dataexport.win-x64.zip\" rel=\"nofollow\">Download</a></td>\n</tr><tr><td>DataExport</td>\n<td>Linux x64</td>\n<td><a href=\"https://raw.githubusercontent.com/MiracleDevs/Paradigm.ORM/master/dist/dataexport.linux-x64.zip\" rel=\"nofollow\">Download</a></td>\n</tr><tr><td>DataExport</td>\n<td>OSX x64</td>\n<td><a href=\"https://raw.githubusercontent.com/MiracleDevs/Paradigm.ORM/master/dist/dataexport.osx-x64.zip\" rel=\"nofollow\">Download</a></td>\n</tr></tbody></table><h2><a id=\"user-content-change-log\" class=\"anchor\" aria-hidden=\"true\" href=\"#change-log\"></a>Change log</h2>\n<p>Version <code>2.2.4</code></p>\n<ul><li>Added visual basic tests.</li>\n<li>Updated nuget dependencies.</li>\n<li>Fixed a couple of bugs found with the vb tests.</li>\n</ul><p>Version <code>2.2.3</code></p>\n<ul><li>Added new DatabaseCommandException thrown when executing database commands. The DatabaseCommandException contains a reference to the\nexecuting command, allowing for a better debugging experience.\nUse Command.CommandText to observe the sql or cql query being executed.\nUse Command.Parameters to observe the parameters bound to the query.</li>\n<li>Fixed a bug in Cassandra connector not adding a parameter in one of the AddParameters methods.</li>\n<li>Fixed a bug in CustomQuery sync execution not updated the command text after parameter replacement.</li>\n<li>Improved and updated tests.</li>\n</ul><p>Version <code>2.2.2</code></p>\n<ul><li>Removed mandatory data type in ColumnAttribute. The orm will choose the default column types for each database type.</li>\n<li>Changed how the CommandBatch replace parameter names, to prevent name collision.</li>\n<li>Added tests for the command batch name replacement.</li>\n<li>Changed how select parameters are replaced, from @Index to  @pIndex or :pIndex, depending on the database parameter naming conventions.</li>\n<li>Updated NuGet dependencies.</li>\n</ul><p>Version <code>2.2.1</code></p>\n<ul><li>Added a cache service for descriptors all over the orm, to prevent tons of small objects filling the heap.</li>\n<li>Removed constructors receiving descriptors. Now all the ORM classes should refer to the cache for descriptors.</li>\n<li>Descriptor constructors are now internal and can not be instantiated outside the ORM.</li>\n</ul><p>Version <code>2.2.0</code></p>\n<ul><li>Refactor command handling to allow parallel execution of the ORM without conflicting with some of the\nconnectors. The orm does not cache a command inside the command builder any more.</li>\n<li>Refactor command builders and moved shared functionality to the core classes, and removed the\nduplication from the client implementations. Now will be even easier to implement new clients.</li>\n<li>Moved base protected methods from the CommandBuilderBase to the ICommandFormatProvider and added a new\nbase CommandFormatProviderBase with shared behavior for all the different format providers.</li>\n<li>Removed IDisposable interface from most of the ORM core classes. The most notable are:\n<ul><li>Database access</li>\n<li>Query</li>\n<li>Custom query</li>\n<li>All the stored procedure types</li>\n<li>Schema Provider</li>\n</ul></li>\n<li>Removed extension methods for the IDatabaseConnector not used any more.</li>\n</ul><p>Version <code>2.1.7</code></p>\n<ul><li>Changed how the DatabaseAccess classes utilize the BatchManager to be thread safe.</li>\n</ul><p>Version <code>2.1.6</code></p>\n<ul><li>Updated Paradigm.Core and other dependencies.</li>\n<li>Published new versions for the tools.</li>\n</ul><p>Version <code>2.1.5</code></p>\n<ul><li>Removed a dependency over generic entities that needed a parameterless constructor\nin all the solution.</li>\n</ul><p>Version <code>2.1.4</code></p>\n<ul><li>Removed a dependency over generic entities that needed a parameterless constructor.</li>\n</ul><p>Version <code>2.1.3</code></p>\n<ul><li>Added new constructor to <code>DatabaseReaderMapper</code> to allow set both the service provider and the\ndatabase connector. This will allow multi-tenancy support using the dbfirst generated code.</li>\n<li>Added new constructors to all the stored procedure types for the same reason as the previous point.</li>\n<li>Added missing ValueConverter inside the database reader value provider.</li>\n</ul><p>Version <code>2.1.2</code></p>\n<ul><li>Changed the database reader mappers to work with the <code>IServiceProvider</code> class. Now, will try to instantiate\nthe entities with the service provider first, and if the service provider can't, will use the activator to\ncreate a new instance. This will allow the Paradigm.Services framework to fully delegate the instancing to\nDI allowing better DDD.</li>\n</ul><p>Version <code>2.1.1</code></p>\n<ul><li>Fixed a problem in cassandra connector where the schema provider can not guess the column type when the column is a\nclustering key with order applied to it.</li>\n<li>Made the modifications to the tests to test the above problem.</li>\n</ul><p>Version <code>2.1.0</code></p>\n<ul><li>Added a new Cassandra connector.\nThis new connector allows to work against Apache Cassandra o ScyllaDB. There are some limitations imposed by the\nDataStax connector, and other imposed by the orm, but for most cases will be just fine.</li>\n</ul><blockquote>\n<p>Warning: The ORM will work with column families that mimic sql tables, aka. without lists, maps, or other not standard\nrelational databases. Even if Cassandra does not supports joins, the ORM allows to create virtual foreign keys between tables\nand create navigation properties from it.</p>\n</blockquote>\n<ul><li>Data Export, DbFirst and DbPublisher can work now against Cassandra and ScyllaDB.</li>\n<li>In all the configuration files, now the Database Type changed to Upper Camel Case syntax, the database types are:\n<ul><li>SqlServer,</li>\n<li>MySql,</li>\n<li>PostgreSql,</li>\n<li>Cassandra</li>\n</ul></li>\n<li>Updated MySql Connector version.</li>\n</ul><p>Version <code>2.0.1</code></p>\n<ul><li>Updated Paradigm.Core to version <code>2.0.1</code>.</li>\n</ul><p>Version <code>2.0.0</code></p>\n<ul><li>Updated .net core from version 1 to version 2.</li>\n</ul><p>Version <code>1.0.0</code></p>\n<ul><li>Uploaded first version of the Paradigm ORM.</li>\n</ul></article>",
        "created_at": "2018-07-14T21:13:32+0000",
        "updated_at": "2018-07-14T21:13:51+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 4,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/8192926?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10838"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1113,
            "label": "nuget",
            "slug": "nuget"
          },
          {
            "id": 1114,
            "label": "orm",
            "slug": "orm"
          }
        ],
        "is_public": false,
        "id": 10836,
        "uid": null,
        "title": "Paradigm.ORM.Data.Cassandra 2.2.4",
        "url": "https://www.nuget.org/packages/Paradigm.ORM.Data.Cassandra/",
        "content": "<p>Paradigm ORM Cassandra Connector library.</p><div class=\"install-tabs\"><div class=\"tab-content\"><div role=\"tabpanel\" class=\"tab-pane active\" id=\"package-manager\"><div><p>&#13;\n                &#13;\n                    Install-Package Paradigm.ORM.Data.Cassandra -Version 2.2.4&#13;\n                &#13;</p></div></div><div role=\"tabpanel\" class=\"tab-pane\" id=\"dotnet-cli\"><div><p>&#13;\n                &#13;\n                    dotnet add package Paradigm.ORM.Data.Cassandra --version 2.2.4&#13;\n                &#13;</p></div></div><div role=\"tabpanel\" class=\"tab-pane\" id=\"paket-cli\"><div><p>&#13;\n                &#13;\n                    paket add Paradigm.ORM.Data.Cassandra --version 2.2.4&#13;\n                &#13;</p></div><p>&#13;\n        <i class=\"ms-Icon ms-Icon--Warning\" aria-hidden=\"true\">&#13;\n        &#13;\nThe NuGet Team does not provide support for this client. Please contact its <a href=\"https://fsprojects.github.io/Paket/contact.html\">maintainers</a> for support.&#13;\n                    &#13;</i></p></div></div></div><ul class=\"list-unstyled panel-collapse collapse dependency-groups\" id=\"dependency-groups\"><li>&#13;\n                                        <h4>.NETStandard 2.0</h4>&#13;\n                                    <ul class=\"list-unstyled dependency-group\"><li>&#13;\n                                                    <a href=\"https://www.nuget.org/packages/CassandraCSharpDriver/\">CassandraCSharpDriver</a>&#13;\n                                                    (&gt;= 3.4.0.1)&#13;\n                                            </li>\n                                            <li>&#13;\n                                                    <a href=\"https://www.nuget.org/packages/Paradigm.ORM.Data/\">Paradigm.ORM.Data</a>&#13;\n                                                    (&gt;= 2.2.4)&#13;\n                                            </li>\n                                            <li>&#13;\n                                                    <a href=\"https://www.nuget.org/packages/System.IO.FileSystem.Primitives/\">System.IO.FileSystem.Primitives</a>&#13;\n                                                    (&gt;= 4.3.0)&#13;\n                                            </li>\n                                            <li>&#13;\n                                                    <a href=\"https://www.nuget.org/packages/System.Runtime.Handles/\">System.Runtime.Handles</a>&#13;\n                                                    (&gt;= 4.3.0)&#13;\n                                            </li>\n                                    </ul></li>\n                        </ul><p>&#13;\n                </p><table class=\"table borderless\"><thead><tr><th colspan=\"2\">Version</th>\n                            <th>Downloads</th>\n                            <th>Last updated</th>\n                        </tr></thead><tbody class=\"no-border\"><tr>\n                                    <td>&#13;\n                                        <a href=\"https://www.nuget.org/packages/Paradigm.ORM.Data.Cassandra/2.2.4\" title=\"2.2.4\">&#13;\n                                            <b>&#13;\n                                                2.2.4&#13;\n                                                    (current)&#13;\n                                            </b>&#13;\n                                        </a>&#13;\n                                    </td>\n                                    <td>&#13;\n                                        97&#13;\n                                    </td>\n                                    <td>&#13;\n                                        2/7/2018&#13;\n                                    </td>\n                                </tr><tr>\n                                    <td>&#13;\n                                        <a href=\"https://www.nuget.org/packages/Paradigm.ORM.Data.Cassandra/2.2.3\" title=\"2.2.3\">&#13;\n                                            <b>&#13;\n                                                2.2.3&#13;\n                                            </b>&#13;\n                                        </a>&#13;\n                                    </td>\n                                    <td>&#13;\n                                        92&#13;\n                                    </td>\n                                    <td>&#13;\n                                        1/2/2018&#13;\n                                    </td>\n                                </tr><tr>\n                                    <td>&#13;\n                                        <a href=\"https://www.nuget.org/packages/Paradigm.ORM.Data.Cassandra/2.2.2\" title=\"2.2.2\">&#13;\n                                            <b>&#13;\n                                                2.2.2&#13;\n                                            </b>&#13;\n                                        </a>&#13;\n                                    </td>\n                                    <td>&#13;\n                                        87&#13;\n                                    </td>\n                                    <td>&#13;\n                                        12/29/2017&#13;\n                                    </td>\n                                </tr><tr>\n                                    <td>&#13;\n                                        <a href=\"https://www.nuget.org/packages/Paradigm.ORM.Data.Cassandra/2.2.1\" title=\"2.2.1\">&#13;\n                                            <b>&#13;\n                                                2.2.1&#13;\n                                            </b>&#13;\n                                        </a>&#13;\n                                    </td>\n                                    <td>&#13;\n                                        76&#13;\n                                    </td>\n                                    <td>&#13;\n                                        11/30/2017&#13;\n                                    </td>\n                                </tr><tr>\n                                    <td>&#13;\n                                        <a href=\"https://www.nuget.org/packages/Paradigm.ORM.Data.Cassandra/2.2.0\" title=\"2.2.0\">&#13;\n                                            <b>&#13;\n                                                2.2.0&#13;\n                                            </b>&#13;\n                                        </a>&#13;\n                                    </td>\n                                    <td>&#13;\n                                        75&#13;\n                                    </td>\n                                    <td>&#13;\n                                        11/29/2017&#13;\n                                    </td>\n                                </tr></tbody></table>&#13;",
        "created_at": "2018-07-14T21:12:56+0000",
        "updated_at": "2018-07-14T21:13:03+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "www.nuget.org",
        "preview_picture": "https://avatars2.githubusercontent.com/u/8192926",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10836"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 228,
            "label": "dotnet",
            "slug": "net"
          },
          {
            "id": 1113,
            "label": "nuget",
            "slug": "nuget"
          }
        ],
        "is_public": false,
        "id": 10834,
        "uid": null,
        "title": "Cassandra.NET 1.0.7",
        "url": "https://www.nuget.org/packages/Cassandra.NET/",
        "content": "<p>Enables smooth ORM like integration between Cassandra and C#<br />Support for Select, Add, Update using c# reflection<br />In addition support for numeric calcualtions such as Min/Max/Average/Sum</p><div class=\"install-tabs\"><div class=\"tab-content\"><div role=\"tabpanel\" class=\"tab-pane active\" id=\"package-manager\"><div><p>&#13;\n                &#13;\n                    Install-Package Cassandra.NET -Version 1.0.7&#13;\n                &#13;</p></div></div><div role=\"tabpanel\" class=\"tab-pane\" id=\"dotnet-cli\"><div><p>&#13;\n                &#13;\n                    dotnet add package Cassandra.NET --version 1.0.7&#13;\n                &#13;</p></div></div><div role=\"tabpanel\" class=\"tab-pane\" id=\"paket-cli\"><div><p>&#13;\n                &#13;\n                    paket add Cassandra.NET --version 1.0.7&#13;\n                &#13;</p></div><p>&#13;\n        <i class=\"ms-Icon ms-Icon--Warning\" aria-hidden=\"true\">&#13;\n        &#13;\nThe NuGet Team does not provide support for this client. Please contact its <a href=\"https://fsprojects.github.io/Paket/contact.html\">maintainers</a> for support.&#13;\n                    &#13;</i></p></div></div></div><p>Support for Batching</p><ul class=\"list-unstyled panel-collapse collapse dependency-groups\" id=\"dependency-groups\"><li>&#13;\n                                        <h4>.NETStandard 2.0</h4>&#13;\n                                    <ul class=\"list-unstyled dependency-group\"><li>&#13;\n                                                    <a href=\"https://www.nuget.org/packages/CassandraCSharpDriver/\">CassandraCSharpDriver</a>&#13;\n                                                    (&gt;= 3.3.2)&#13;\n                                            </li>\n                                    </ul></li>\n                        </ul><p>&#13;\n                </p><table class=\"table borderless\"><thead><tr><th colspan=\"2\">Version</th>\n                            <th>Downloads</th>\n                            <th>Last updated</th>\n                        </tr></thead><tbody class=\"no-border\"><tr>\n                                    <td>&#13;\n                                        <a href=\"https://www.nuget.org/packages/Cassandra.NET/1.0.7\" title=\"1.0.7\">&#13;\n                                            <b>&#13;\n                                                1.0.7&#13;\n                                                    (current)&#13;\n                                            </b>&#13;\n                                        </a>&#13;\n                                    </td>\n                                    <td>&#13;\n                                        345&#13;\n                                    </td>\n                                    <td>&#13;\n                                        1/1/2018&#13;\n                                    </td>\n                                </tr><tr>\n                                    <td>&#13;\n                                        <a href=\"https://www.nuget.org/packages/Cassandra.NET/1.0.6\" title=\"1.0.6\">&#13;\n                                            <b>&#13;\n                                                1.0.6&#13;\n                                            </b>&#13;\n                                        </a>&#13;\n                                    </td>\n                                    <td>&#13;\n                                        89&#13;\n                                    </td>\n                                    <td>&#13;\n                                        1/1/2018&#13;\n                                    </td>\n                                </tr><tr>\n                                    <td>&#13;\n                                        <a href=\"https://www.nuget.org/packages/Cassandra.NET/1.0.5\" title=\"1.0.5\">&#13;\n                                            <b>&#13;\n                                                1.0.5&#13;\n                                            </b>&#13;\n                                        </a>&#13;\n                                    </td>\n                                    <td>&#13;\n                                        90&#13;\n                                    </td>\n                                    <td>&#13;\n                                        1/1/2018&#13;\n                                    </td>\n                                </tr><tr>\n                                    <td>&#13;\n                                        <a href=\"https://www.nuget.org/packages/Cassandra.NET/1.0.4\" title=\"1.0.4\">&#13;\n                                            <b>&#13;\n                                                1.0.4&#13;\n                                            </b>&#13;\n                                        </a>&#13;\n                                    </td>\n                                    <td>&#13;\n                                        92&#13;\n                                    </td>\n                                    <td>&#13;\n                                        1/1/2018&#13;\n                                    </td>\n                                </tr><tr>\n                                    <td>&#13;\n                                        <a href=\"https://www.nuget.org/packages/Cassandra.NET/1.0.3\" title=\"1.0.3\">&#13;\n                                            <b>&#13;\n                                                1.0.3&#13;\n                                            </b>&#13;\n                                        </a>&#13;\n                                    </td>\n                                    <td>&#13;\n                                        100&#13;\n                                    </td>\n                                    <td>&#13;\n                                        11/23/2017&#13;\n                                    </td>\n                                </tr></tbody></table>&#13;",
        "created_at": "2018-07-14T21:09:30+0000",
        "updated_at": "2018-07-14T21:09:37+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "www.nuget.org",
        "preview_picture": "https://upload.wikimedia.org/wikipedia/commons/1/1e/Apache-cassandra-icon.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10834"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10832,
        "uid": null,
        "title": "10 years of Apache Cassandra",
        "url": "https://www.idgconnect.com/abstract/30973/apache-cassandra",
        "content": "<a href=\"https://www.idgconnect.com/browse_documents/307/software-web-development/application-development\">Application Development</a><ul class=\"article_list\">\n\t\t\t<li>\n\t\t\t\t<p>\n                   \tPosted by\n                   \t<a href=\"https://www.idgconnect.com/author/472\">Kate Hoy</a>\n\t\t\t\n\t\t\t\t\t\t \n\t\t\t\t</p>\n\t\t\t</li>\n\t\t\t<li>\n\t\t\t\t<p>\n\t\t\t\t\ton <a>July 11 2018</a>\n\t\t\t\t</p>\n\t\t\t</li>\n\t\t</ul><p>Apache Cassandra, developed by <a href=\"http://www.linkedin.com/in/avinashlakshman\">Avinash Lakshman</a> and <a href=\"http://www.linkedin.com/in/prmalik\">Prashant Malik</a> to try to solve their Inbox-search problem at Facebook, was published as free software under the Apache V2 license in July 2008. Providing a scalable, high-availability datastore with no single point of failure, Cassandra is well suited for high-availability applications. It supports multi-datacenter replication, and offers massive and linear scalability, so any number of nodes can easily be added to any Cassandra cluster in any datacenter. According to <a href=\"http://cassandra.apache.org/\">the website</a>, the largest known Cassandra setup involves over 300TB of data on over 400 machines.</p><p>After ten years of development, driven in part by contributions from IBM, Twitter and Rackspace, Cassandra is now used by NetFlix, eBay, Twitter, Reddit and <a href=\"http://www.datastax.com/cassandrausers\">many others</a>, and is one of the most popular NoSQL-databases in use today. To find out more about the impact Cassandra has had on the development community, we speak to previous Apache Cassandra project chair Jonathan Ellis, currently SVP and CTO, DataStax; Aaron Morton, CEO at Cassandra consultants, The Last Pickle; and open source consultant Carlos Rolo.</p><h2>How has Cassandra impacted the community?</h2><p>Jonathan Ellis, SVP and CTO, DataStax, first come into contact with Cassandra at the end of 2008, when he was hired by Rackspace to build them a next-generation, scalable database. He explains that Cassandra was one of a number of options at the time that offered ‘NoSQL’, but argues that SQL itself wasn’t the problem: “SQL is a quite reasonable language for getting data in and out of a server.”</p><p>The introduction of <a href=\"https://www.datastax.com/dev/blog/whats-new-in-cql-3-0\">Cassandra Query Language</a> (CQL) with Cassandra 1.1 in 2012 was one of the most important steps for the community, according to Ellis, because it meant developers had an API portable across languages and suitable for a REPL. “We were the first to introduce this,” Ellis explains, “with almost universal adoption of a similar approach by other NoSQL databases.” The only notable holdout today is Amazon’s DynamoDB, and Ellis doesn’t believe that will last – “I predict that it won’t be long before they follow suit as well.”</p><p>For Ellis, the biggest contribution Cassandra has made is that app developers – whether Cassandra users or not -- realized that you don’t need ACID for most common tasks. “Cassandra defaults to <a href=\"https://en.wikipedia.org/wiki/Eventual_consistency\">eventually consistent</a> operations (where “eventually” is typically single-digit or even sub-millisecond latencies), and allows users to opt in to <a href=\"https://www.datastax.com/dev/blog/lightweight-transactions-in-cassandra-2-0\">lightweight transactions</a> when linearizable consistency is called for.”</p><p>Aaron Morton, who was working at Weta Digital when he first came across Cassandra in 2009, says, “[Cassandra] was the first time I felt I could contribute to the code of a database and get involved in an early stage project.” Two years later Morton left Weta to found his own Cassandra consulting firm The Last Pickle. Of Cassandra, Morton says, “DBA's are now concerned with the speed of light when storing data around the globe rather than the speed of disks, that's a big change and in large part is due to the success of Apache Cassandra.” </p><p>Open source consultant, Carlos Rolo, may have joined the party a little later, first coming into contact with Cassandra in early 2011, but for Rolo the impact of Cassandra is simple - it brought distributed databases to everyone.</p><h2>How does Cassandra compare to others in the NoSQL space?</h2><p>Ellis believes Cassandra is uniquely suited for a hybrid world, and indeed, is “the best option if you are building a cloud application that needs real-time responses, always-on reliability, and scalable performance.” However, he notes that does come with some sacrifices in terms of ease-of-use: “like other cloud databases, Cassandra emphasizes <a href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\">denormalization</a> over query-time joins, which is a hard concept for RDBMS developers to wrap their minds around.” Ultimately, it depends on how many you’re building for, if it’s just a few hundred or a few thousand users, you likely don’t need Cassandra.</p><p>For Rolo, the ease of scaling and enabling data (geo)distribution are major benefits that are often overlooked. Plus, it is proven at large scale, which other NoSQL databases still have to prove, and a tooling ecosystem is starting to appear which Cassandra has previously lacked in comparison to other databases. But ease of operation is a problem, says Rolo, “Cassandra deployments tend to get painful to manage if something is set wrong.”</p><p>Morton considers one of the major benefits to be the API using CQL which he describes as “stable, well documented, and easy for new users to pick up”. Further, it can run in almost any environment, providing great observability, and its stability means losing a server is rarely a problem. He describes Cassandra as “battle proven technology” explaining there’s a great deal of institutional knowledge in the Apache project. “Not every idea works out, and we've been through a few features that did not set the world on fire. Knowing what not to do is as important as knowing what to change.”   </p><h2>What’s been the biggest challenge with Cassandra?</h2><p>All three of the experts we spoke to agreed that one of the biggest challenges was changing the way people think about data. Ellis clarifies: “Most developers are exposed to the relational model and third normal form in college.  That’s still true today, ten years into the age of NoSQL. Once people get it, it’s like the light turns on, but it can be a challenge to get to that point--because in a distributed world, the rules aren’t just different, they’re upside down.”</p><p>For Rolo, over 80% of the challenges he faces with Cassandra result from companies/people trying to port their relational models over.</p><p>On the technical front, Morton explains, “the whole system has been re-written since version 1.0 which was basically the initial Facebook design. Starting with the networking protocol, moving to the creation of the CQL API, and finally re-writing the storage engine itself.” He credits the contributors who worked on these changes with setting the stage for Cassandra’s ongoing success.</p><p>Both Ellis and Rolo are concerned about a Cassandra skills shortage. Ellis believes the problem has worsened, citing job search engine Indeed as showing 5000+ jobs looking for Cassandra experience today versus around 800 in 2014. However, statistics for permanent job vacancies with a requirement for Apache Cassandra skills from <a href=\"https://www.itjobswatch.co.uk/jobs/uk/apache%20cassandra.do\">IT Jobs Watch</a> show a year-on-year decrease in the number of permanent jobs citing Apache Cassandra. Rolo believes the difference is between the development side and administration side: “a lot of development teams have already Cassandra skills. On the administration side I still think that is an issue, but I might be biased on this one!”</p><h2>The next 10 years</h2><p>Looking ahead to the next ten years, Ellis believes an up-and-coming area in the data space is Graph. “Of course, we’ve had graph databases for years, but they’ve never quite found a killer app, and I think a lot of that is due to early graph databases being limited in scale in a lot of the same ways relational databases were. My prediction is, you’re going to see improvements in both the fundamental graph technologies and in integration of graph with other data models like Cassandra’s tabular or document models.”</p><p>Morton thinks simplicity is key when looking at the future of Cassandra, explaining that reducing barriers to entry for not just Cassandra, but other distributed databases as well, makes it easier for new people to get involved, bringing new ideas that will help push the technology further. Ultimately, Morton believes, “Almost all databases will be distributed databases, just like almost all mobile phones are smart phones, and Cassandra will continue to be a large part of the larger ecosystem.”</p><p>For Rolo, the increase in Cassandra skills, together with improvements to Cassandra itself means it’s not going anywhere. His plan? “Keep rocking with Cassandra and the ecosystem surrounding it!”</p><div class=\"post-list\"><ul><li class=\"arrow\">\n\t\t\t\t\t\t<a href=\"https://www.idgconnect.com/send_detail_report/30973?is_article=true\">Forward to a friend</a>\n\t\t\t\t\t</li>\n\t\t\t\t\t\n\t\t\t\t</ul></div><div class=\"article_pagination_new\"><div class=\"article_previous\"><p>\n\t\t\t\t\tPREVIOUS ARTICLE\n\t\t\t\t</p><a href=\"https://www.idgconnect.com/abstract/30940/c-suite-career-advice-stephen-parker-parker-software\">«C-suite career advice: Stephen Parker, Parker Software</a></div><div class=\"article_next\"><p>\n\t\t\t\t\tNEXT ARTICLE\n\t\t\t\t</p><a href=\"https://www.idgconnect.com/abstract/30974/the-cmo-files-sarah-taylor-smartfocus\">The CMO Files: Sarah Taylor, SmartFocus»</a></div></div>",
        "created_at": "2018-07-13T21:05:59+0000",
        "updated_at": "2018-07-13T21:06:04+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 6,
        "domain_name": "www.idgconnect.com",
        "preview_picture": "http://www.idgconnect.com/IMG/972/50972/shutterstock-697413424.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10832"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 996,
            "label": "monitoring",
            "slug": "monitoring"
          },
          {
            "id": 1103,
            "label": "prometheus",
            "slug": "prometheus"
          }
        ],
        "is_public": false,
        "id": 10816,
        "uid": null,
        "title": "zegelin/cassandra-exporter",
        "url": "https://github.com/zegelin/cassandra-exporter",
        "content": "<p><em>Project Status: alpha</em></p><h2>Introduction</h2><p><em>cassandra-exporter</em> is a Java agent that exports Cassandra metrics to <a href=\"http://prometheus.io\" rel=\"nofollow\">Prometheus</a>.</p><p>It enables high performance collection of Cassandra metrics and follows the Prometheus best practices for metrics naming and labeling.</p><p>For example, the following PromQL query will return an estimate of the number of pending compactions per keyspace, per node.</p><pre>sum(cassandra_table_estimated_pending_compactions) by (cassandra_node, keyspace)\n</pre><h2>Compatibility</h2><p><em>cassandra-exporter</em> is has been tested with:</p><table><thead><tr><th>Component</th>\n<th>Version</th>\n</tr></thead><tbody><tr><td>Apache Cassandra</td>\n<td>3.11.2</td>\n</tr><tr><td>Prometheus</td>\n<td>2.0 and later</td>\n</tr></tbody></table><p>Other Cassandra and Prometheus versions will be tested for compatibility in the future.</p><h2>Usage</h2><p>Download the latest release and copy <code>cassandra-exporter-agent-&lt;version&gt;.jar</code> to <code>$CASSANDRA_HOME/lib</code> (typically <code>/usr/share/cassandra/lib</code> in most package installs).</p><p>Then edit <code>$CASSANDRA_CONF/cassandra-env.sh</code> (typically <code>/etc/cassandra/cassandra-env.sh</code>) and append the following:</p><pre>JVM_OPTS=\"$JVM_OPTS -javaagent:$CASSANDRA_HOME/lib/cassandra-exporter-agent-&lt;version&gt;.jar=http://localhost:9998/\"\n</pre><p>Then (re-)start Cassandra.</p><p>Prometheus metrics will be available at <code>http://localhost:9998/metrics</code>.</p><p>Configure Prometheus to scrape the endpoint by adding the following to <code>prometheus.yml</code>:</p><pre>scrape_configs:\n  ...\n  \n  - job_name: 'cassandra'\n    static_configs:\n      - targets: ['&lt;cassandra node IP&gt;:9998']\n</pre><p>See the <a href=\"https://prometheus.io/docs/prometheus/latest/configuration/configuration/#%3Cscrape_config%3E\" rel=\"nofollow\">Prometheus documentation</a> for more details on configuring scrape targets.</p><p>Viewing the exposed endpoint in a web browser will display a HTML version of the exported metrics.</p><p>To view the raw, plain text metrics (in the Prometheus text exposition format), either request the endpoint with a HTTP client that prefers plain text\n(or one that can specify the <code>Accept: text/plain</code> header) or add the following query parameter to the URL: <code>?x-content-type=text/plain</code>.</p><p>An experimental JSON output is also provided, via <code>Accept: application/json</code> or <code>?x-content-type=application/json</code>.\nThe format/structure of this output is subject to change.</p><h2>Options</h2><p>Currently only the HTTP endpoint (address &amp; port) can be configured.</p><h2>Features</h2><h3>Performance</h3><p>JMX is <em>slow</em>, really slow. JMX adds significant overhead to every method invocation on exported MBean methods, even when those methods are called from within the same JVM.\nOn a 300-ish table Cassandra node, trying to collect all exposed metrics via JVM resulted in a collection time that was upwards of 2-3 <em>seconds</em>.\nFor exporters that run as a separate process there is additional overhead of inter-process communications and that time can reach the 10's of seconds.</p><p><em>cassandra-exporter</em> on the same node collects all metrics in 10-20 <em>milliseconds</em>.</p><h3>Best practices</h3><p>The exporter follows Prometheus best practices for metric names, labels and data types.</p><p>Aggregate metrics, such as the aggregated table metrics at the keyspace and node level, are skipped. Instead these should be aggregated using PromQL queries or Prometheus recording rules.</p><p>Metrics are coalesced when appropriate so they share the same name, opting for <em>labels</em> to differentiate indiviual time series. For example, each table level metric has a constant name and at minimum a <code>table</code> &amp; <code>keyspace</code> label, which allows for complex PromQL queries.</p><p>For example the <code>cassandra_table_operation_latency_seconds[_count|_sum]</code> summary metric combines read, write, range read, CAS prepare, CAS propose and CAS commit latency metrics together into a single metric family.\nA summary exposes percentiles (via the <code>quantile</code> label), a total count of recorded samples (via the <code>_count</code> metric),\nand (if available, <code>NaN</code> otherwise) an accumulated sum of all samples  (via the <code>_sum</code> metric).</p><p>Individual time-series are separated by different labels. In this example, the operation type is exported as the <code>operation</code> label.\nThe source <code>keyspace</code>, <code>table</code>, <code>table_type</code> (table, view or index), <code>table_id</code> (CF UUID), and numerous other metadata labels are available.</p><pre>cassandra_table_operation_latency_seconds_count{keyspace=\"system_schema\",table=\"tables\",table_type=\"table\",operation=\"read\",...}\ncassandra_table_operation_latency_seconds_count{keyspace=\"system_schema\",table=\"tables\",table_type=\"table\",operation=\"write\",...}\ncassandra_table_operation_latency_seconds_count{keyspace=\"system_schema\",table=\"keyspaces\",table_type=\"table\",operation=\"read\",...}\ncassandra_table_operation_latency_seconds_count{keyspace=\"system_schema\",table=\"keyspaces\",table_type=\"table\",operation=\"write\",...}\n</pre><p>These metrics can then be queried:</p><pre>sum(cassandra_table_operation_latency_seconds_count) by (keyspace, operation) # total operations by keyspace &amp; type\n</pre><table><thead><tr><th>Element</th>\n<th>Value</th>\n</tr></thead><tbody><tr><td><code>{keyspace=\"system\",operation=\"write\"}</code></td>\n<td>13989</td>\n</tr><tr><td><code>{keyspace=\"system\",operation=\"cas_commit\"}</code></td>\n<td>0</td>\n</tr><tr><td><code>{keyspace=\"system\",operation=\"cas_prepare\"}</code></td>\n<td>0</td>\n</tr><tr><td><code>{keyspace=\"system\",operation=\"cas_propose\"}</code></td>\n<td>0</td>\n</tr><tr><td><code>{keyspace=\"system\",operation=\"range_read\"}</code></td>\n<td>10894</td>\n</tr><tr><td><code>{keyspace=\"system\",operation=\"read\"}</code></td>\n<td>74</td>\n</tr><tr><td><code>{keyspace=\"system_schema\",operation=\"write\"}</code></td>\n<td>78</td>\n</tr><tr><td><code>{keyspace=\"system_schema\",operation=\"cas_commit\"}</code></td>\n<td>0</td>\n</tr><tr><td><code>{keyspace=\"system_schema\",operation=\"cas_prepare\"}</code></td>\n<td>0</td>\n</tr><tr><td><code>{keyspace=\"system_schema\",operation=\"cas_propose\"}</code></td>\n<td>0</td>\n</tr><tr><td><code>{keyspace=\"system_schema\",operation=\"range_read\"}</code></td>\n<td>75</td>\n</tr><tr><td><code>{keyspace=\"system_schema\",operation=\"read\"}</code></td>\n<td>618</td>\n</tr></tbody></table><h3>Global Labels</h3><p>The exporter does attach global labels to the exported metrics. At this time these cannot be disabled without recompiling the agent.</p><p>These labels are:</p><ul><li>\n<p><code>cassandra_cluster_name</code></p>\n<p>The name of the cluster, as specified in cassandra.yaml</p>\n</li>\n<li>\n<p><code>cassandra_host_id</code></p>\n<p>The unique UUID of the node</p>\n</li>\n<li>\n<p><code>cassandra_node</code></p>\n<p>The IP address of the node</p>\n</li>\n<li>\n<p><code>cassandra_datacenter</code></p>\n<p>The configured data center name of the node</p>\n</li>\n<li>\n<p><code>cassandra_rack</code></p>\n<p>The configured rack name of the node</p>\n</li>\n</ul><p>These labels allow aggregation of metrics at the cluster, data center and rack levels.</p><p>While these labels could be defined in the prometheus scrape config, the authors feel that having these labels be automatically\napplied simplifies things, especially when Prometheus is monitoring multiple clusters across numerous DCs and racks.</p><h3>JMX Standalone (Experimental)</h3><p>While it is preferable to run <em>cassandra-exporter</em> as a Java agent for performance, it can instead be run as an external application if required.\nMetrics will be queried via JMX.</p><p>The set of metrics should be identical, but currently some additional metadata labels attached to the <code>cassandra_table_*</code> metrics will\nnot be available.</p><p>This was originally designed to assist with benchmarking and development of the exporter. Currently the JMX RMI service URL and HTTP endpoint\nvalues are hard-coded. The application will need to be recompiled if these parameters need to be changed.</p><h2>Exported Metrics</h2><p>See the <a href=\"https://github.com/zegelin/cassandra-exporter/wiki/Exported-Metrics\">Exported Metrics</a> wiki page for a list.</p><p>We suggest viewing the metrics endpoint (e.g., <a href=\"http://localhost:9998/metrics\" rel=\"nofollow\">http://localhost:9998/metrics</a>) in a browser to get an understanding of what metrics\nare exported by your Cassandra node.</p><h2>Unstable, Missing &amp; Future Features</h2><p>See the <a href=\"https://github.com/zegelin/cassandra-exporter/issues\">project issue tracker</a> for a complete list.</p><ul><li>\n<p>Configuration parameters</p>\n<p>Currently only the listen address &amp; port can be configured.</p>\n<p>Allow configuration of:</p>\n<ul><li>listen address and port</li>\n<li>exported metrics (aka, blacklist certain metrics)</li>\n<li>enable/disable global labels</li>\n<li>exclude help from JSON</li>\n</ul></li>\n<li>\n<p>JVM metrics</p>\n<p>Future versions should add support for collecting and exporting JVM metrics (memory, GC pause times, etc).</p>\n</li>\n<li>\n<p>Add some example queries</p>\n</li>\n<li>\n<p>Add Grafana dashboard templates</p>\n</li>\n<li>\n<p>Documentation improvements</p>\n</li>\n<li>\n<p>Improve standalone JMX exporter</p>\n<ul><li>Configuration parameters</li>\n</ul></li>\n</ul>",
        "created_at": "2018-07-10T04:25:08+0000",
        "updated_at": "2018-07-10T04:28:03+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 5,
        "domain_name": "github.com",
        "preview_picture": "https://avatars3.githubusercontent.com/u/19296634?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10816"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10787,
        "uid": null,
        "title": "Using Apache Cassandra — A few things before you start",
        "url": "https://hackernoon.com/using-apache-cassandra-a-few-things-before-you-start-ac599926e4b8?gi=64bd77351bbb",
        "content": "<p id=\"8aee\" class=\"graf graf--p graf-after--h3\">The <a href=\"https://www.slideshare.net/planetcassandra/cassandra-summit-2014-cql-under-the-hood-39445761\" data-href=\"https://www.slideshare.net/planetcassandra/cassandra-summit-2014-cql-under-the-hood-39445761\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">CQL</a> — Cassandra Query language gives an almost SQL type interface to Apache Cassandra. I have found many times,that many who use this,do not know about some important points of Cassandra that makes it different from SQL databases like Postgres. Same is the case for operations team, there are some aspects related to storage, GC settings , that many are not aware of. I am not an expert in Cassandra internals and don’t aspire to be if I can avoid it.This is mostly a note to myself, and something which I can ask others to refer to instead of repeating over email a gazillion times. There are lot of other parts like repair etc which I have left out. The intention here is to make this as short as possible, but if you feel somethings are to be added, please comment.</p><h4 id=\"2e37\" class=\"graf graf--h4 graf-after--p\">Cassandra has Tune-able Consistency — not just eventual consistency</h4><p id=\"0963\" class=\"graf graf--p graf-after--h4\">Many considering Cassandra as a replacement for SQL database like Postgres, MySQL or Oracle, shy away thinking that eventual consistency of NoSQL does not meet their requirement. In Cassandra ,however consistency is configurable. This means that with some write and read speed sacrifice, you can have strong consistency as well as high availability. Cassandra can be used for small data as well as big data; depending on your use case you can tune the consistency per key-space or even per-operation.</p><blockquote id=\"86c1\" class=\"graf graf--blockquote graf-after--p\"><div>Cassandra values Availability and Partitioning tolerance (AP). Tradeoffs between consistency and latency are tunable in Cassandra. You can get strong consistency with Cassandra (with an increased latency).<br /><a href=\"https://wiki.apache.org/cassandra/ArchitectureOverview\" data-href=\"https://wiki.apache.org/cassandra/ArchitectureOverview\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener noopener\" target=\"_blank\">https://wiki.apache.org/cassandra/ArchitectureOverview</a></div></blockquote><p id=\"712d\" class=\"graf graf--p graf-after--blockquote\">At this point it may be a good idea to have a short recap of CAP theorem as there is a lot of confusion translating the theoretical surmise to the practical world.</p><blockquote id=\"e767\" class=\"graf graf--blockquote graf-after--p\"><div>In 2000, Dr. Eric Brewer gave a keynote at the <em class=\"markup--em markup--blockquote-em\">Proceedings of the Annual ACM Symposium on Principles of Distributed Computing</em> in which he laid out his famous CAP Theorem: <em class=\"markup--em markup--blockquote-em\">a shared-data system can have at most two of the three following properties: </em><strong class=\"markup--strong markup--blockquote-strong\"><em class=\"markup--em markup--blockquote-em\">C</em></strong><em class=\"markup--em markup--blockquote-em\">onsistency, </em><strong class=\"markup--strong markup--blockquote-strong\"><em class=\"markup--em markup--blockquote-em\">A</em></strong><em class=\"markup--em markup--blockquote-em\">vailability, and tolerance to network </em><strong class=\"markup--strong markup--blockquote-strong\"><em class=\"markup--em markup--blockquote-em\">P</em></strong><em class=\"markup--em markup--blockquote-em\">artitions.</em></div></blockquote><p id=\"cb8b\" class=\"graf graf--p graf-after--blockquote\">This applies to any distributed data base, not just Cassandra.So Cassandra can provide C and A not P ? Is it a big problem ?</p><p id=\"c5f4\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">Short answer — It is not. Skip the rest of the section if you are in a hurry.</strong></p><p id=\"b375\" class=\"graf graf--p graf-after--p\">Long answer read on.Here is the excerpt from Cassandra docs. (DataStax’s docs)</p><blockquote id=\"82aa\" class=\"graf graf--blockquote graf-after--p\"><div>… You can tune Cassandra’s consistency level per-operation, or set it globally for a cluster or datacenter. You can vary the consistency for individual read or write operations so that the data returned is more or less consistent, as required by the client application. This allows you to make Cassandra act more like a CP (consistent and partition tolerant) or AP (highly available and partition tolerant) system according to the CAP theorem, depending on the application requirements.</div></blockquote><blockquote id=\"8be4\" class=\"graf graf--blockquote graf-after--blockquote\"><div><strong class=\"markup--strong markup--blockquote-strong\">Note:</strong> It is not possible to “tune” Cassandra into a completely CA system. See <a href=\"https://codahale.com/you-cant-sacrifice-partition-tolerance/\" data-href=\"https://codahale.com/you-cant-sacrifice-partition-tolerance/\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"noopener\" target=\"_blank\">You Can’t Sacrifice Partition Tolerance</a> for a more detailed discussion. -<a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlAboutDataConsistency.html#dmlAboutDataConsistency__eventual-consistency\" data-href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlAboutDataConsistency.html#dmlAboutDataConsistency__eventual-consistency\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlAboutDataConsistency.html</a></div></blockquote><p id=\"f1c2\" class=\"graf graf--p graf-after--blockquote\">Here is an excerpt from the article linked in the DataStax’ s Cassandra documentation page.</p><blockquote id=\"15b8\" class=\"graf graf--blockquote graf-after--p\"><div>Of the CAP theorem’s Consistency, Availability, and Partition Tolerance, <strong class=\"markup--strong markup--blockquote-strong\">Partition Tolerance is mandatory in distributed systems. You cannot not choose it. </strong>Instead of CAP, you should think about your availability in terms of <em class=\"markup--em markup--blockquote-em\">yield</em> (percent of requests answered successfully) and <em class=\"markup--em markup--blockquote-em\">harvest</em> (percent of required data actually included in the responses) and which of these two your system will sacrifice when failures happen. -<a href=\"https://codahale.com/you-cant-sacrifice-partition-tolerance/\" data-href=\"https://codahale.com/you-cant-sacrifice-partition-tolerance/\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://codahale.com/you-cant-sacrifice-partition-tolerance/</a></div></blockquote><p id=\"c333\" class=\"graf graf--p graf-after--blockquote\">What the above article explains in depth is that Availability is tied to Network Partitioning or Partition Tolerance.Worst case scenario network partitions are quite rare inside a Data Center network. Also network partitions cannot be prevented from happening. It is ever present, though mostly transient and intermittent. The risk of network partitioning across many nodes in a cluster so as to disrupt Availability for a multi-node cluster is very less.</p><p id=\"ddd6\" class=\"graf graf--p graf-after--p\">So with Cassandra you can have as good a C and A system as practically possible.</p><h4 id=\"f386\" class=\"graf graf--h4 graf-after--p\"><strong class=\"markup--strong markup--h4-strong\">Give Importance to modelling the Partition key</strong></h4><p id=\"607e\" class=\"graf graf--p graf-after--h4\">If there is only one thing that you should read,maybe it is the link below</p><p><a href=\"https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key\" data-href=\"https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">The most important thing to know in Cassandra data modeling: The primary key</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">Patrick is regarded as one of the foremost experts of Apache Cassandra and data modeling techniques. As the Chief…</em>www.datastax.com</a></p><p id=\"958c\" class=\"graf graf--p graf-after--mixtapeEmbed\">What is the Partition Key ? It is the first part of the Primary Key or the Primary key itself if Primary key is not composite</p><p id=\"5327\" class=\"graf graf--p graf-after--p\">Why is this most important part ?</p><p id=\"fbfc\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">To have balanced write of data to multiple Cassandra nodes in the cluster and subsequent balanced reads of the data.</strong></p><blockquote id=\"fe60\" class=\"graf graf--blockquote graf-after--p\"><div>When data is inserted into the cluster, the first step is to apply a hash function to the partition key. The output is used to determine what node (and replicas) will get the data.-<a href=\"https://docs.datastax.com/en/archived/cassandra/3.x/cassandra/architecture/archPartitionerM3P.html\" data-href=\"https://docs.datastax.com/en/archived/cassandra/3.x/cassandra/architecture/archPartitionerM3P.html\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://docs.datastax.com/en/archived/cassandra/3.x/cassandra/architecture/archPartitionerM3P.html</a></div></blockquote><p id=\"1e3d\" class=\"graf graf--p graf-after--blockquote\">Here are two main goals to consider for modelling the data</p><p><a href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\" data-href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">Basic Rules of Cassandra Data Modeling</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">Learn more about Apache Cassandra and data modeling READ MORE DS:220 COURSE Picking the right data model is the hardest…</em>www.datastax.com</a></p><p id=\"1dab\" class=\"graf graf--p graf-after--mixtapeEmbed\">1. Spread data evenly around the cluster — Model Partition Key</p><p id=\"8ae6\" class=\"graf graf--p graf-after--p\">2. Minimize the number of partitions read -Model Partition Key and Clustering keys</p><p id=\"9ea6\" class=\"graf graf--p graf-after--p\">Let us take an example. Below is a initial modelling of table where the data is some events (say political rallies, speeches etc) that has occurred in a particular location, centered over latitude,longitude and say having a radius of 250 meters. Each location has an influential candidate of that area. Sometimes the same area can have multiple influential candidates. I have illustrated a slightly complex example so as to show the flexibility in data types present in Cassandra,and all the aspects to consider when modelling the key. The actual cell can be a telecom cell with multiple coverage by different operators or different technologies. The example I give here is a bit contrived and for illustration only.</p><figure id=\"716e\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"8775\" class=\"graf graf--p graf-after--figure\">Note that partition key is chosen assuming that events are distributed evenly across a city, the key will distribute the data from multiple locations evenly across available Cassandra nodes.</p><p id=\"9ffa\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">The partition query should be modeled for efficient retrieval of the data application needs</strong></p><p id=\"9633\" class=\"graf graf--p graf-after--p\">The modelling of Tables and thereby the partition key is primarily with consideration of efficient data retrieval.</p><p id=\"02b8\" class=\"graf graf--p graf-after--p\">Assume that we need to query all events happening in a location by a Candidate for a time interval.The queries will be a set of statements like</p><blockquote id=\"ec1c\" class=\"graf graf--blockquote graf-after--p\"><div>select * from demo_table where bin_cell_key = (1234, 222, ‘Candidate1’) and time_key &gt;= (‘06:00:00.000000000’, ‘06:00:00.000000000’) and time_key &lt; (‘07:30:00.000000000’, ‘07:30:00.000000000’)</div></blockquote><blockquote id=\"6b6b\" class=\"graf graf--blockquote graf-after--blockquote\"><div>select * from demo_table where bin_cell_key = (1234, 223, ‘Candidate1’) ..</div></blockquote><p id=\"feeb\" class=\"graf graf--p graf-after--blockquote\">But to compose this <em class=\"markup--em markup--p-em\">bin_cell_key</em> we need to know first which Candidates are there in which locations. For this we need to model helper tables. Note — Data duplication is okay in NoSQL data modelling and to have the same effect of JOINs this is needed. Some helper tables to get the bin_cell_key</p><blockquote id=\"42fa\" class=\"graf graf--blockquote graf-after--p\"><div>create table cell_bin (cell text, bin tuple&lt;int,int&gt;, PRIMARY KEY (cell,bin));</div></blockquote><p id=\"ac52\" class=\"graf graf--p graf-after--blockquote\">Example CQL:</p><blockquote id=\"011b\" class=\"graf graf--blockquote graf-after--p\"><div>select * from cell_bin where cell=’Candidate1’;</div></blockquote><blockquote id=\"f71c\" class=\"graf graf--blockquote graf-after--blockquote\"><div>cell | bin<br /> — — — — -+ — — — — —-<br />Candidate1| (1234, 222)<br />Candidate1| (1234, 223)</div></blockquote><p id=\"f849\" class=\"graf graf--p graf-after--blockquote\">And similarly for the other way round</p><blockquote id=\"caf0\" class=\"graf graf--blockquote graf-after--p\"><div>create table bin_cell (bin tuple&lt;int,int&gt;, cell text, PRIMARY KEY (bin,cell));</div></blockquote><p id=\"21a2\" class=\"graf graf--p graf-after--blockquote\">Example CQL:</p><blockquote id=\"0774\" class=\"graf graf--blockquote graf-after--p\"><div>cqlsh:demo_keyspace&gt; select * from bin_cell where bin = (1234, 222);<br /> bin | cell<br /> — — — — -+ — — — —— -<br /> (1234, 222) | Candidate1<br /> (1234, 222) | Candidate2</div></blockquote><p id=\"fca6\" class=\"graf graf--p graf-after--blockquote\">We can stop here. But if you are curious read on. What if we want to aggregate all events that has taken place in a region irrespective of the Candidate. For this we need to split the cell out. Why ? because in case of a composite partition key all the elements need to be specified in the query.</p><blockquote id=\"ebf9\" class=\"graf graf--blockquote graf-after--p\"><div>select * from demo_table where bin = (1234,222) and year=2017 and month=12 and day=1;</div></blockquote><p id=\"81aa\" class=\"graf graf--p graf-after--blockquote\">The table for below which also adds time to the partition key so that data from different days are distributed across available nodes are given below.</p><figure id=\"2100\" class=\"graf graf--figure graf--iframe graf-after--p\"><blockquote id=\"861f\" class=\"graf graf--blockquote graf-after--figure\"><div>create table demo_table( year int,month int,day int, bin tuple&lt;double,double&gt;, cell text, time_key tuple&lt;timestamp,timestamp&gt;,event text, PRIMARY KEY((bin,year,month,day),cell,time_key));</div></blockquote><h4 id=\"040b\" class=\"graf graf--h4 graf-after--blockquote\">Test and measure your reads &amp; write via nodetool cfstats</h4><p id=\"f60b\" class=\"graf graf--p graf-after--h4\">How do we know if our data model is distributing writes across nodes. How do we know if the write latency and read latency is distributed across nodes and if it is linearly scale able in proportional to the nodes added. The answer to all of this via node tool cfstats command</p><p><a href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsCFstats.html\" data-href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsCFstats.html\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsCFstats.html\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">nodetool cfstats</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">Provides statistics about tables.</em>docs.datastax.com</a></p><p id=\"0c51\" class=\"graf graf--p graf-after--mixtapeEmbed\">You need to run long runs with write and then read operations using multiple nodes and everyday update a table like one below based on the output from cfstats. Soon you will know if your write and read are balanced. Actually adding more nodes should also decrease your read time linearly. This is really beautiful.</p><figure id=\"f30b\" class=\"graf graf--figure graf--iframe graf-after--p\"><h4 id=\"73a0\" class=\"graf graf--h4 graf-after--figure\">Do not use ALLOW FILTERING and IN Clause in CQL indiscriminately</h4><p id=\"1258\" class=\"graf graf--p graf-after--h4\">If you feel that there is no way out; then please read the section above. Most probably your table modelling has to be refactored.</p><blockquote id=\"3ad8\" class=\"graf graf--blockquote graf-after--p\"><div>When your query is rejected by Cassandra because it needs filtering, you should resist the urge to just add ALLOW FILTERING to it. You should think about your data, your model and what you are trying to do. -<a href=\"https://www.datastax.com/dev/blog/allow-filtering-explained-2\" data-href=\"https://www.datastax.com/dev/blog/allow-filtering-explained-2\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://www.datastax.com/dev/blog/allow-filtering-explained-2</a></div></blockquote><p id=\"820a\" class=\"graf graf--p graf-after--blockquote\"><strong class=\"markup--strong markup--p-strong\">Or for that matter IF clause</strong></p><p id=\"fba3\" class=\"graf graf--p graf-after--p\">The new versions of Cassandra supports light-weight transactions. In CQL this is done via the IF clause. <em class=\"markup--em markup--p-em\">Insert into table IF NOT EXISTS</em>.</p><blockquote id=\"8bee\" class=\"graf graf--blockquote graf-after--p\"><a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlLtwtTransactions.html\" data-href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlLtwtTransactions.html\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"noopener\" target=\"_blank\">Lightweight transactions</a><div> should not be used casually, as the latency of operations increases fourfold due to the due to the round-trips necessary between the CAS coordinators. -</div><a href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_using/useInsertLWT.html\" data-href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_using/useInsertLWT.html\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://docs.datastax.com/en/cql/3.3/cql/cql_using/useInsertLWT.html</a></blockquote><h4 id=\"185f\" class=\"graf graf--h4 graf-after--blockquote\">Be aware of the JVM GC Suck-age as JVM Heap Increases</h4><p id=\"e711\" class=\"graf graf--p graf-after--h4\">Cassandra runs on the JVM and relies on the OS page cache for improving performance. <em class=\"markup--em markup--p-em\">There is no need to throw huge amounts of RAM at Cassandra</em>. Cassandra performance should increase by adding more low powered nodes.We have run our long runs for about 2 weeks with load on 2 GB JVM heap, and Cassandra had never once gone down.</p><p id=\"df9c\" class=\"graf graf--p graf-after--p\">JVM GC suckage is directly proportional to the JVM heap. This is true for any Java process and also for Cassandra</p><figure id=\"be45\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*HQhjWbOdml9VTeSYo18Pdw.png\" data-width=\"533\" data-height=\"400\" src=\"https://cdn-images-1.medium.com/max/1600/1*HQhjWbOdml9VTeSYo18Pdw.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\">source-<a href=\"https://www.slideshare.net/mattdennis/cassandra-antipatterns\" data-href=\"https://www.slideshare.net/mattdennis/cassandra-antipatterns\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener\" target=\"_blank\">https://www.slideshare.net/mattdennis/cassandra-antipatterns</a></figcaption></figure><p id=\"da9f\" class=\"graf graf--p graf-after--figure\"><em class=\"markup--em markup--p-em\">Some illuminating quotes</em></p><p id=\"3659\" class=\"graf graf--p graf-after--p\"><em class=\"markup--em markup--p-em\">Many users new to Cassandra are tempted to turn up Java heap size too high, which consumes the majority of the underlying system’s RAM. In most cases, increasing the Java heap size is actually </em><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">detrimental</em></strong><em class=\"markup--em markup--p-em\"> for these reasons:</em></p><p id=\"3569\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">1</em>. <em class=\"markup--em markup--p-em\">In most cases, the capability of Java to gracefully handle garbage collection above 8GB quickly diminishes.</em></strong></p><p id=\"0c46\" class=\"graf graf--p graf-after--p\"><em class=\"markup--em markup--p-em\">2.Modern operating systems maintain the OS page cache for frequently accessed data and are very good at keeping this data in memory, but can be prevented from doing its job by an elevated Java heap size.</em></p><p id=\"f7a1\" class=\"graf graf--p graf-after--p\"><em class=\"markup--em markup--p-em\">If you have more than 2GB of system memory, which is typical, keep the size of the Java heap relatively small to allow more memory for the page cache .</em></p><p><a href=\"http://docs.datastax.com/en/cassandra/2.0/cassandra/operations/ops_tune_jvm_c.html\" data-href=\"http://docs.datastax.com/en/cassandra/2.0/cassandra/operations/ops_tune_jvm_c.html\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"http://docs.datastax.com/en/cassandra/2.0/cassandra/operations/ops_tune_jvm_c.html\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">Tuning Java resources</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">Consider tuning Java resources in the event of a performance degradation or high memory consumption.</em>docs.datastax.com</a></p><p id=\"4efa\" class=\"graf graf--p graf-after--mixtapeEmbed\">Database like <a href=\"http://www.scylladb.com/open-source/\" data-href=\"http://www.scylladb.com/open-source/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">Scylla DB</a> have ported the Cassandra design on to C++ so as to avoid the GC pauses and other such problems related to JVM. But as long as you keep the JVM heap around 8 GB things should be fine.</p><p id=\"e23d\" class=\"graf graf--p graf-after--p\">An update here- While I was working with Cassandra G1GC was not deemed stable enough to be used. Now DataStax version of Cassandra used G1GC. G1GC can handle larger heaps; however Cassandra can use RAM heavily for page caches, filters etc, so all the above still makes sense. Limit JVM heap to the minimum you need and leave the rest of the memory for Cassandra process.</p><h4 id=\"a0f6\" class=\"graf graf--h4 graf-after--p\">Cassandra does not like SAN/Shared Storage</h4><p id=\"c898\" class=\"graf graf--p graf-after--h4\">Cassandra is designed for the low fetch times of spinning disks by only appending data to the end for writes and minimize disk seek time during read via application selected partition keys and thereby spreading reads across multiple nodes — Regarding storage a good slide to go through <a href=\"https://www.slideshare.net/johnny15676/why-does-my-choiceofstorage-matterwithcassandra\" data-href=\"https://www.slideshare.net/johnny15676/why-does-my-choiceofstorage-matterwithcassandra\" class=\"markup--anchor markup--p-anchor\" title=\"https://www.slideshare.net/johnny15676/why-does-my-choiceofstorage-matterwithcassandra\" rel=\"noreferrer noopener noopener noopener\" target=\"_blank\">https://www.slideshare.net/johnny15676/why-does-my-choiceofstorage-matterwithcassandra</a></p><figure id=\"9e1c\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*SEX2I_8ohef5sG65VuX5ow.png\" data-width=\"613\" data-height=\"449\" data-is-featured=\"true\" src=\"https://cdn-images-1.medium.com/max/1600/1*SEX2I_8ohef5sG65VuX5ow.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\">source-<a href=\"https://www.slideshare.net/johnny15676/why-does-my-choiceofstorage-matterwithcassandra\" data-href=\"https://www.slideshare.net/johnny15676/why-does-my-choiceofstorage-matterwithcassandra\" class=\"markup--anchor markup--figure-anchor\" title=\"https://www.slideshare.net/johnny15676/why-does-my-choiceofstorage-matterwithcassandra\" rel=\"noreferrer noopener noopener noopener noopener noopener\" target=\"_blank\">https://www.slideshare.net/johnny15676/why-does-my-choiceofstorage-matterwithcassandra</a></figcaption></figure><blockquote id=\"5b27\" class=\"graf graf--blockquote graf-after--figure\"><div>Customer/User — “We have an awesome SAN and would like to use it for Cassandra.”<br />DataStax — “We don’t recommend shared storage for Cassandra.”<br />Customer/User — “Why not.”<br />DataStax — “Two reasons really. One — performance suffers. Two — shared storage introduces a single point of failure into the architecture.”<br />Customer/User — “Our SAN is awesome and has never had any down time and can preform a kagillion IOPS. So why exactly shouldn’t we use shared storage.”</div></blockquote><blockquote id=\"8db5\" class=\"graf graf--blockquote graf-after--blockquote\"><a href=\"https://www.datastax.com/dev/blog/impact-of-shared-storage-on-apache-cassandra\" data-href=\"https://www.datastax.com/dev/blog/impact-of-shared-storage-on-apache-cassandra\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener noopener\" target=\"_blank\">https://www.datastax.com/dev/blog/impact-of-shared-storage-on-apache-cassandra</a></blockquote><h4 id=\"2539\" class=\"graf graf--h4 graf-after--blockquote\">Horizontal Scale-ability -Transparent Sharding vs Application Level Sharding</h4><p id=\"f4c4\" class=\"graf graf--p graf-after--h4\">Database sharding is a way of horizontally scaling database. Application level sharding means that the logic of partitioning data across multiple node is done at the application level. That is based on some key- classic example — East Coast vs West Coast or <a href=\"http://www.malinga.me/application-aware-sharding-for-a-mysql-database/\" data-href=\"http://www.malinga.me/application-aware-sharding-for-a-mysql-database/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">similar</a>; your DB write code will select a Database to connect , write and read. The problem is that the complexity of application level sharding quickly gets complex and it is not a good way to scale . Cassandra and most NoSQL databases does sharding transparently (as you have seen via the partition key). This is a pretty big advantage as without this horizontal scaling is a hard problem.</p><h4 id=\"db8e\" class=\"graf graf--h4 graf-after--p\">Open-Source and Commercial</h4><p id=\"a666\" class=\"graf graf--p graf-after--h4 graf--trailing\">As of the time of writing this, relations between Apache Foundation and Datastax- which was one of the largest contributor to Cassandra ? have soured. There is an commercial version of Cassandra — Datastax Enterprise Edition and open source version is the Apache Cassandra. The Java driver of Cassandra has two version, the open source and DSE provided, and you cannot use commercial driver with open source Cassandra. For other languages like Go the driver is open source.</p></figure></figure></figure>",
        "created_at": "2018-07-07T09:36:52+0000",
        "updated_at": "2018-07-07T09:37:04+0000",
        "published_at": "2018-02-12T17:06:39+0000",
        "published_by": [
          "Alex Punnen"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 11,
        "domain_name": "hackernoon.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*SEX2I_8ohef5sG65VuX5ow.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10787"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          }
        ],
        "is_public": false,
        "id": 10777,
        "uid": null,
        "title": "Real-time Data Integration with Kafka and Cassandra (Ewen Cheslack-Po…",
        "url": "https://www.slideshare.net/DataStax/realtime-data-integration-with-kafka-and-cassandra-ewen-cheslackpostava-confluent-c-summit-2016",
        "content": "<p>No notes for slide</p>First I want to quickly introduce myself so you know where I’m coming from. I’m an engineer at Confluent, a company founded by the co-creators of Apache Kafka, and we’re building what we call a stream data platform to help companies capture and leverage all their real-time data. I’m also a committer on the Apache Kafka project and the lead at Confluent on the Connect project, which itself is part of the open source Apache Kafka project.More types of data stores with specialized functionality – e.g. rise of NoSQL systems handling document-oriented and columnar stores. A lot more sources of data. <br />Rise of secondary data stores and indexes – e.g. Elasticsearch for efficient text-based queries, graph DBs for graph-oriented queries, time series databases. A lot more destinations for data, and a lot of transformations along the way to those destinations. <br />Real-time: data needs to be moved between these systems continuously and at low latency. <br />Unfortunately, as you build up large, complex data pipelines in an ad hoc fashion by connecting different data systems that need copies of the same data with one-off connectors for those systems, or build out custom connectors for stream processing frameworks to handle different sources and sinks of streaming data, we end up with a giant, unmaintainable mess. <p>This mess has a huge impact on productivity and agility once you get past just a few systems. Adding any new data storage system or stream processing job requires carefully tracking down all the downstream systems that might be affected, which may require coordinating with dozens of teams and code spread across many repositories. Trying to change one data source’s data format can impact many downstream systems, yet there’s no simple way to discover how these jobs are related. </p><p>This is a real problem that we’re seeing across a variety of companies today. We need to do something to simplify this picture. While Confluent is working to build out a number of tools to help with these challenges, today I want to focus on how we can standardize and simplify constructing these data pipelines so that, at a minimum, we reduce operational complexity and make it easier to discover and understand the full data pipeline and dependencies.</p>We refer to this problem as data integration – by which we broadly mean making sure data gets to all the right places. We need to be able to collect data from a diverse set of sources and then feed it to several downstream applications and systems for processing. <p>This problem isn’t a new one. There were legacy solutions to this problem but the approach of copying data in an ad-hoc way across applications just does not scale anymore. Today data is in motion and it needs to move in real-time and at scale.</p>I want to start by highlighting some anti-patterns we observe in how people are tackling this problem today. <p>One-off tools – connect any two given specific systems. <br />High complexity, operational overhead <br />Designed to be too specific – n^2 connectors <br />Overly-generic data copying tools – make few assumptions, connect any and all inputs and outputs, and do a bunch of intermediate transformation as well. <br />Try to do too much – E, T, and L with weak interfaces <br />Too abstract – difficult/impossible to make guarantees even when connecting right pairs of systems <br />Stream processing tools for data integration <br />Overkill for simple EL workloads <br />Weaker connector ecosystem – focus is rightly on T <br />Generic, weak interfaces as found in generic data copying tools result in difficult to understand semantics and guarantees</p>When we get too specific, handling everything ad hoc, we end up with a ton of different tools for every connection, often times many different tools for doing transformations, and probably the worst case – a lot of different tools that do *all* of ETL for specific systems. <br />If we have too little separation of concerns, we end up in situations where we use the stream processing framework for literally every step even though they use a specific model that doesn’t map well to ingesting or exporting data from many types of systems. Alternatively, we use overly generic data copying &amp; transformation tools. These tools are so abstract that they can’t provide many guarantees and become overly complex, requiring you to learn a dozen concepts just to setup a simple pipeline. <br />What we really need is a separation of concerns in ET&amp;L. <br />One step towards getting to a separation of concerns is being able to decouple the E, T, and L steps. Kafka, when used as shown here, can help us do that. <br />The vision of Kafka when originally built at LinkedIn was for it to act as a common hub for real-time data. <br />When streaming data from data stores like RDBMS or K/V store, we produce data into Kafka, making it available to as many downstream consumers as want it. <br />Save data to other systems like secondary indexes and batch storage systems, which are implemented with consumers. <br />Stream processing frameworks and custom consumer apps fit in by being both consumers and producers – reading data from Kafka transforming it, and then possibly publishing derived data back into Kafka. <br />Using this model can simplify the problem as we’re now always interacting with Kafka. <br />To set some context, I want to just quickly list a few of the features that make it possible for Kafka to handle data at this scale. We’ll come back to many of these properties when looking at Kafka Connect. <p>At its core, pub/sub messaging system rethought as distributed commit log. <br />Based on an append-only and sequentially accessed log, which results in very high performance reading and writing data. <br />Extends the model to a *partitioned stream* model for a single logical topic of data, which allows for distribution of data on the brokers and parallelism in both writes and reads. In order to still provide organization and ordering within a single partition, it guarantees ordering within each partition and uses keys to determine which partition to put data in. <br />As part of its append-only approach, it decouples data consumption from data retention policy, e.g. retaining data for 7 days or until we have 1TB in a topic. This both gets rid of individual message acking and allows multiple consumption of the same data, i.e. pub/sub, by simply tracking offsets in the stream. <br />Because data is split across partitions, we can also parallelize consumption and make it elastically scalable with Kafka’s unique automatically balanced consumer groups. <br /></p>But what exactly is Kafka? <p>At high level, \"just\" another pub/sub message queue <br />A few key features make it scale to handle the requirements of a stream data platform </p><p>Multiple consumers can read the same data, and can be at different offsets in the log. Consuming data doesn't delete it from the log. Instead, Kafka use time- or data size- based retention. Your data will stick around for, e.g., 7 days or until you have 100GB. This retention policy is simple and avoids having to keep accounting info for individual messages.</p>Topics are partitioned so they can scale across multiple servers <br />Partitions are also replicated for fault toleranceAs I mentioned before, Kafka is multi-subscriber where the same topic can be consumed by multiple groups of consumers where each consumer group can subscribe to read the full copy of data. Furthermore, every consumer group can have multiple consumer processes distributed over several machines and Kafka takes care of assigning the partitions of the subscribed topics evenly amongst the consumer processes in a group so that at all times, every partition of a subscribed topic is being consumed by some consumer process within the group. <p>In addition to being easy to scale, consumption is also fault tolerant. If one fails, the other ones automatically rebalance to pick up the load of the failed consumer instance. So it is operationally cheap to consume large amounts of data. </p>Given all these properties, it’s easy to see how Kafka can fit this central role as the hub for all your realtime data, and we can simplify the original image of our data pipeline. However, with the regular Kafka clients, we’re still leaving quite a bit on the table – each connection in the image still requires its own tool or Kafka application to get data to or from Kafka. Each tool uses these relatively low-level clients and has to implement many common features. <br />Today, I want to introduce you to Kafka Connect, Kafka’s new large-scale, streaming data import/export tool that drastically simplifies the construction, maintenance, and monitoring of these data pipelines. <p>Kafka Connect is part of the Apache Kafka project, open source under the Apache license, and ships with Kafka. It’s a framework for building connectors between other data systems and Kafka, and the associated runtime to run these connectors in a distributed, fault tolerant manner at scale. <br /></p>Goals: <p>Focus – copying only <br />Batteries included – framework does all the common stuff so connector developers can focus specifically on details that need to be customized for their system. This covers a lot more than many connector developers realize: beyond managing the producer or consumer, it includes challenges like scalability, recovery from faults and reasoning about delivery guarantees, serialization, connector control, monitoring for ops, and more. <br />Standardize – configuration, status and connector control, monitoring, etc. <br />Parallelism, scalability, fault tolerance built-in, without a lot of effort from connector developers or users. <br />Scale – in two ways. First, scale individual connectors to copy as much data as possible – ingest an entire database rather than one table at a time. Second, scale up to organization-wide data pipelines or down to development, testing, or just copying a single log file into Kafka </p><p>With these goals in mind, let’s explore the design of Kafka Connect to see how it fulfills these.</p>At it’s core, Kafka connect is pretty simple. It has source connectors which copy data from another system into Kafka, and sink connectors that copy data from Kafka into a destination system. <p>Here I’ve shown a couple of examples. The source and sink systems don’t necessarily have to naturally match Kafka’s data model exactly. However, we do need to be able to translate data between the two. For example, we might load data from a database in a source connector. By using a timestamp column associated with each row, we can effectively generate an ordered stream of events that are then produced into Kafka. To store data into HDFS, we might load data from one or more topics in Kafka and then write it in sequence to files in an HDFS directory, rotating files periodically. Although Kafka Connect is designed around streaming data, because Kafka acts as a good buffer between streaming and batch systems, we can use it here to load data into HDFS. Neither of these systems map directly to Kafka’s model, but both can be adapted to the concepts of streams with offsets. More about this in a minute. </p><p>The most important design point for Kafka Connect is that one half of a connection is always Kafka – the destination for sources, or the source of data for sink connectors. This allows the framework to handle the common functionality of connectors while maintaining the ability to automatically provide scalability, fault tolerance, and delivery guarantees without requiring a lot of effort from connector developers. This key assumption is what makes it possible for Kafka Connect to get a better set of tradeoffs than the systems I mentioned earlier.</p>So now, coming back to the model that connectors need to map to. Just as Kafka’s data model enables certain features around scalability, Kafka Connect’s data model can as well. <p>Kafka Connect requires every connector to map to a “partitioned stream” model. The basic idea is a generalization of Kafka’s data model of topics and partitions. This mapping is defined by the input system for the connector – the source system for source connectors, and Kafka topics for sink connectors -- and has the following: </p><p>A set of partitions which divide the whole set of data logically. Unlike Kafka, the number of partitions can potentially be very large and may be more dynamic than we would expect with Kafka. <br />Each partition contains an ordered sequence of events/messages. Under the hood these are key/value pairs with byte[], but Kafka Connect requires that they can be converted into a generic data API <br />Each event/message has a unique offset representing its position in the partition. Since the mapping is determined by the input system, these offsets must be meaningful to that system – these may be quite different from the Kafka offsets you’re used to. <br /></p>To give a more concrete example, we can revisit the database example from earlier. Previously I only showed a single table, but if we consider the database as a whole, we can apply this model to copy the entire database. We partition by table, delivering each into its own Kafka topic. Each event represents a row that we’ve inserted into the database. The offsets are IDs or timestamps, or even more complex representations like a combination of ID and timestamp. Although there isn’t *actually* a stream for each table, we can effectively construct one by querying the database and ordering results according to specific rules. <p>As a result of this model, we can see a few properties emerging: </p><p>First, we have a built-in concept of parallelism, a requirement for automatically providing scalable data copying. We’re going to be able to distribute processing of partitions across multiple hosts. <br />Second, this model encourages making copying broad by default – partitioned streams should cover the largest logical collection of data. <br />Finally, offsets provide an easy way to track which data has been processed and which still needs to be copied. In some cases, mapping from the native data model to streams may not be simple; however, a bit of effort in creating this mapping pays off by providing a common framework and implementation for tracking which data has been copied. Again, we’ll revisit this a bit later, but this allows the framework to handle a lot of the heavy lifting with regards to delivery semantics. </p>Partitioned streams are the logical data model, but they don’t directly map to physical parallelism, or threads, in Kafka Connect. In the case of the database connector, a direct mapping might seem reasonable. However, some connectors will have a much larger number of partitions that are much finer-grained. For example, consider a connector for collecting metrics data – each metric might be considered its own partition, resulting in tens of thousands of partitions for even a small set of application servers. <p>However, we do want to exploit the parallelism provided by partitions. Connectors do this by assigning partitions to tasks. Tasks are, simply, threads of control given to the connector code which perform the actual copying of data. <br />Each connector is given a thread it can use to monitor the input system for the active set of partitions. Remember that this set can be dynamic, so continuous monitoring is sometimes needed to detect changes to the set of partitions. When there are changes, the connector notifies the framework so it can reconfigure the current set of tasks. <br />Then, each task is given a dedicated thread for processing. The connector assigns a subset of partitions to each task and the task is the one that actually copies the data for that partition. Given the assignment, the connector implementer handles the reading or writing data from that set of partitions. <br />And how do we decide how many tasks to generate? That’s up to the user, and it’s the primary way to control the total resources used by the connector. Since each task corresponds to a thread, the user can choose to dynamically increase or decrease the maximum number of tasks the connector may create in order to scale resource usage up or down. </p><p>So now we have some set of threads, but where do they actually execute? Kafka Connect has two modes of execution.</p>Standalone mode works as a single process. This is really easy to get started with, easy to configure. <p>We like this because it scales down really easily and stays local for testing. It’s also great for connectors that really only make sense on a single node – for example, processing log files, where you need to read the data off the local file system. </p><p>If you’ve used systems like logstash or flume, this mode should look familiar. It’s commonly referred to as either standalone or agent mode.</p>In contrast, distributed mode can scale up while providing distribution and fault tolerance. <p>Recall that each connector or task is a thread, and we’re considering each to be approximately equal in terms of resource usage. <br />Connectors and tasks are auto-balanced across workers. Failures automatically handled by redistributing work, and you can easily scale the cluster up or down by adding more workers. <br />Cool implementation note: reuses group membership functionality of consumer groups. Note how if you replace “worker” with “consumer” and “task” with “topic partition”, the things it is doing look largely the same: assigning tasks to workers, detecting when a worker is added or fails, and rebalancing the work. Kafka already provides support for doing a lot of this, so by leveraging the existing implementation and coordinating through Kafka’s group functionality (with internal data stored in Kafka topics), Kafka Connect can provide this functionality in a relatively small code footprint. <br />Finally, note that Kafka Connect does not own the process management at all. We don’t want to make assumptions about using Mesos, YARN, or any other tool because that would unnecessarily limit Kafka Connect’s usage. Kafka Connect will work out of the box in any of these cluster management systems, or with orchestration tools, or if you just manage your processes with your own tooling.</p>In contrast, distributed mode can scale up while providing distribution and fault tolerance. <p>Recall that each connector or task is a thread, and we’re considering each to be approximately equal in terms of resource usage. <br />Connectors and tasks are auto-balanced across workers. Failures automatically handled by redistributing work, and you can easily scale the cluster up or down by adding more workers. <br />Cool implementation note: reuses group membership functionality of consumer groups. Note how if you replace “worker” with “consumer” and “task” with “topic partition”, the things it is doing look largely the same: assigning tasks to workers, detecting when a worker is added or fails, and rebalancing the work. Kafka already provides support for doing a lot of this, so by leveraging the existing implementation and coordinating through Kafka’s group functionality (with internal data stored in Kafka topics), Kafka Connect can provide this functionality in a relatively small code footprint. <br />Finally, note that Kafka Connect does not own the process management at all. We don’t want to make assumptions about using Mesos, YARN, or any other tool because that would unnecessarily limit Kafka Connect’s usage. Kafka Connect will work out of the box in any of these cluster management systems, or with orchestration tools, or if you just manage your processes with your own tooling. <br /></p>In contrast, distributed mode can scale up while providing distribution and fault tolerance. <p>Recall that each connector or task is a thread, and we’re considering each to be approximately equal in terms of resource usage. <br />Connectors and tasks are auto-balanced across workers. Failures automatically handled by redistributing work, and you can easily scale the cluster up or down by adding more workers. <br />Cool implementation note: reuses group membership functionality of consumer groups. Note how if you replace “worker” with “consumer” and “task” with “topic partition”, the things it is doing look largely the same: assigning tasks to workers, detecting when a worker is added or fails, and rebalancing the work. Kafka already provides support for doing a lot of this, so by leveraging the existing implementation and coordinating through Kafka’s group functionality (with internal data stored in Kafka topics), Kafka Connect can provide this functionality in a relatively small code footprint. <br />All of this functionality can be accessed via  REST API – submit connectors, see their status, update configs, and so on. <br />Finally, note that Kafka Connect does not own the process management at all. We don’t want to make assumptions about using Mesos, YARN, or any other tool because that would unnecessarily limit Kafka Connect’s usage. Kafka Connect will work out of the box in any of these cluster management systems, or with orchestration tools, or if you just manage your processes with your own tooling. <br /></p>I want to mention two important features that also simplify both connector developer’s and user’s lives. <p>The first feature is offset management, which provides for standardized data delivery guarantees. Delivery guarantees are actually rarely provided in many other systems. They generally offer some sort of best effort, but unreliable, delivery. Ironically, stream processing frameworks often do a better job than tools specifically designed for data copying. </p><p>Kafka Connect handles offset checkpointing for connectors, and this fits in as a natural extension to Kafka’s offset commit functionality. For sources this works with offsets that have complex structure (e.g. timestamps + autoincrementing IDs in a database) and requires no implementation support from the connector beyond defining the offsets and being able to start reading from a saved offset. For sinks, we can leverage Kafka’s existing offset functionality, but in order to ensure data is completely written, sinks must also support a flush operation. Commits are automatically processed periodically. By default, this mode of managing offsets will provide at least once delivery; internally both sources and sinks are simply flushing all data to the output and the committing offsets. </p><p>Note that some connectors will opt out of this functionality in order to provide even stronger guarantees. For example, the HDFS connector manages its own offsets because (carefully) tracking them in HDFS along with the data allows for exactly-once delivery. <br /></p>The second feature I want to mention are converters. Serialization formats may seem like a minor detail, but not separating the details of data serialization in Kafka from the details of source or sink systems results in a lot of inefficiency: <p>A lot of code for doing simple data conversions are duplicated across a large number of ad hoc connector implementations. <br />Each connector ultimately contains its own set of serialization options as it is used in more environments – JSON, Avro, Thrift, protobufs, and more. </p><p>Much like the serializers in Kafka’s producer and consumer, the Converters abstract away the details of serialization. Converters are different because they guarantee data is transformed to a common data API defined by Kafka Connect. This API supports both schema and schemaless data, common primitive data types, complex types like structs, and logical type extensions. By sharing this API, connectors write one set of translation code and Converters handle format-specific details. For example, the JDBC connector can easily be used to produce either JSON or Avro to Kafka, without any format-specific code in the connector. <br /></p>Kafka Connect provides the framework, but I want to spend a few minutes describing the current state of the connector ecosystem. While the framework ships with Apache Kafka, connectors use a federated approach to development. Confluent helped kick off connector development with a few key open source connectors – JDBC for importing data from any relational database and HDFS, for exactly once delivery of data into HDFS and Hive. Confluent will be continuing to add more open source connectors. <p>We’ve also started tracking connectors that the community has been developing on a page we’re calling the Connector Hub. We’ve already got a dozen or so connectors, and more are popping up every week. We’ll be working to make this index as useful to users as possible, offering information about the current state of the connector implementations and feature sets. <br /></p>With all these pieces you can see how we can tie together Kafka and Kafka Connect with stream processing frameworks and applications to not only simplify building these data pipelines and solve data integration challenges, but also transform how your company manages its data pipelines. <p>Kafka provides the central hub for real-time data and Kafka Connect simplifies operationalization: one service to maintain, common metrics, common monitoring, and agnostic to your choice of process and cluster management. </p><p>You can centrally managed Kafka Connect cluster running in distributed mode, and accessed via REST API, allowing your ops team to provide data integration as a service to your entire organization. <br />For developers who want to build a complex data pipeline, they can submit jobs to copy data into and out of Kafka – it’s zero coding (assuming a connector is available) <br />Then, they can easily leverage either the traditional clients or stream processing frameworks to transform that data. The output is stored back into another Kafka topic or served up directly. <br />As a side benefit, standardizing on Kafka encourages reuse of existing data (both raw and transformed). Providing this service not only makes it easy to build your *own* complex data pipeline, it encourages other people in the org to build on top of your existing work. <br />Confluent Platform also provides additional tools that make this setup even more powerful. For example, the schema registry controls the format of data in each topic, and besides ensuring data quality and compatibility, it also encourages decoupling of teams by allowing anyone to discover what data is in a topic, grab its schema, and immediately start utilizing that data without ever adding coordination overhead with another team. </p><p>A stream data platform built around Kafka and Kafka Connect allows you to scale to handle your entire organization’s real-time data, while maintaining simple management and easy operationalization of your data pipeline.</p>With that, I’d like to say thank for listening and I’d be happy to take any questions.",
        "created_at": "2018-07-05T19:48:34+0000",
        "updated_at": "2018-07-05T19:48:42+0000",
        "published_at": null,
        "published_by": [
          "DataStax"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 21,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/kafka-connect-cassandra-summit-2016-160915005102-thumbnail-4.jpg?cb=1474049512",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10777"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          }
        ],
        "is_public": false,
        "id": 10776,
        "uid": null,
        "title": "Cassandra to Kafka data pipeline Part 2",
        "url": "https://www.smartcat.io/blog/2017/cassandra-to-kafka-data-pipeline-part-2/",
        "content": "<p>If you haven’t read the previous part of this blog, you can find it <a data-udi=\"umb://document/1ee5ec1697a948a99a42fd2618e605a9\" href=\"https://www.smartcat.io/blog/2017/cassandra-to-kafka-data-pipeline-part-1/\" title=\"Cassandra to Kafka Data Pipeline Part 1\">here</a>. There, I have laid the necessary steps for injecting the Kafka cluster into system ‘before’ the Cassandra cluster. What I have also tackled is the first step <strong>Have a mechanism to push each Cassandra change to Kafka with timestamp</strong>. But only one approach has been considered there - Cassandra triggers.</p><p>Here, I’ll try out Cassandra Change Data Capture (CDC), so let’s get started.</p><p><strong>Data Model</strong></p><p>In order to make easier comparisons later, I’ll use the same data model as in the first part.</p><pre class=\"csharpcode\">CREATE TABLE IF NOT EXISTS movies_by_genre (<br />title text,<br />genre text,<br />year int,<br />rating float,<br />duration int,<br />director text,<br />country text,<br />PRIMARY KEY ((genre, year), rating, duration)<br />) WITH CLUSTERING ORDER BY (rating DESC, duration ASC)</pre><p><strong>Infrastructure</strong></p><p>Infrastructure is also the same, two Cassandra 3.11.0 nodes, two Kafka 0.10.1.1 nodes, one Zookeeper 3.4.6 and everything packaged to run from Docker compose.</p><p><strong>Cassandra CDC</strong></p><p>My impression is that there is not much documentation on CDC, since I have struggled to grasp the concepts and how all of it should function. Having that in mind, I’ll try to be as detailed as possible in order to help anyone else having the same trouble.</p><p>First of all, CDC is available from Cassandra 3.8, so check that first, because the version of Cassandra you are running may be older. The entire documentation on Cassandra CDC can be found <a rel=\"noopener noreferrer\" href=\"http://cassandra.apache.org/doc/latest/operating/cdc.html\" target=\"_blank\">here</a>. It’s not much, but still contains useful information.</p><p>To turn on CDC, cdc_enabled must be set to true in the cassandra.yaml. This will turn on CDC on the node. In order to enable it cluster-wide, it must be set on every node. Besides that, there are three more properties in cassandra.yaml related to CDC, four in total:</p><ol><li>&#13;\n<p dir=\"ltr\">cdc_enabled - Can be set to true or false, enabling or disabling CDC on the whole node, default is false</p>&#13;\n</li>\n<li>&#13;\n<p dir=\"ltr\">cdc_raw_directory - Directory where commitlog segments are moved, if not set, defaults to $CASSANDRA_HOME/data/cdc_raw. But commitlog segments are moved only when all of the following three conditions are met:</p>&#13;\n</li>\n<li>&#13;\n<p dir=\"ltr\">CDC is enabled</p>&#13;\n</li>\n<li>&#13;\n<p dir=\"ltr\">Commitlog segment contains at least one mutation for CDC enabled table</p>&#13;\n</li>\n<li>&#13;\n<p dir=\"ltr\">Commitlog segment is about to be discarded</p>&#13;\n</li>\n</ol>&#13;\n<p dir=\"ltr\">cdc_total_space_in_mb - Total space on disk to use for CDC logs. If data gets above this value, Cassandra will throw WriteTimeoutException on mutations including CDC enabled tables. The minimum default is 4096 MB or 1/8th of the total space of the drive where cdc_raw_directory resides</p>&#13;\n&#13;\n&#13;\n<p dir=\"ltr\">cdc_free_space_check_interval_ms - When space limit is hit (bullet 3), a check is made at this interval to see if space has been freed and writes can continue, default is 250ms.</p>&#13;\n&#13;\n<p>To sum it all up. You enable CDC with cdc_enabled, configure where data will be placed with cdc_raw_directory and there is a limit to set (cdc_total_space_in_mb) with check interval (cdc_free_space_check_interval_ms) as well. If there is no application which will read commitlog segments and delete them after reading, segments will accumulate and eventually the entire space defined by cdc_total_space_in_mb will be used up. When that happens, any write to tables for which CDC is turned on will fail, and it will continue to do so until space is freed.</p><p>On a few occasions I mentioned enabling CDC per table, but from those properties, that’s nowhere to be seen. Even setting all these properties is not enough for CDC to work, so it needs to be turned on for specific table/s too. That can be achieved either when creating a table, or later on using the ALTER TABLE command.</p><pre class=\"csharpcode\">CREATE TABLE IF NOT EXISTS movies_by_genre (<br />title text,<br />genre text,<br />year int,<br />rating float,<br />duration int,<br />director text,<br />country text,<br />PRIMARY KEY ((genre, year), rating, duration)<br />) WITH CLUSTERING ORDER BY (rating DESC, duration ASC)<br />AND cdc = true;</pre><p>Create table statement</p><pre class=\"csharpcode\">ALTER TABLE movies_by_genre WITH cdc = true;</pre><p>Alter table statement</p><p>Now that CDC is turned on, it will copy commitlog segments which contain at least one mutation for the table for which CDC is turned on into a directory specified by the cdc_raw_directory property. Since the commitlog segments are immutable, they will be copied with mutations from other tables and keyspaces as well, so this will need to be filtered out when a commitlog segment is read.</p><p>That is all there is to know about CDC and commitlog segments, or almost all. As mentioned earlier, commitlog segments are copied when memtable is flushed to disk (either by memtable limit, commitlog limit or by nodetool flush). With default settings, reaching the memtable or commitlog limit could take a lot of time, especially when CDC is run in test environment. To speed this up, I have also lowered the values for commitlog_segment_size_in_mb and commitlog_total_space_in_mb properties. Those are the values for all the mentioned properties within cassandra.yaml that I have changed:</p><pre class=\"csharpcode\">cdc_enabled: true<br />cdc_raw_directory: /var/lib/cassandra/cdc_raw<br />cdc_total_space_in_mb: 4096<br />cdc_free_space_check_interval_ms: 250<br />commitlog_segment_size_in_mb: 1 <br />commitlog_total_space_in_mb: 16</pre><p>Even with the limits being this low, I don’t want to do inserts, updates or deletes manually from cqlsh. I use <a rel=\"noopener noreferrer\" href=\"https://github.com/smartcat-labs/berserker\" target=\"_blank\">Berserker</a> for this job, which I have already used in <a href=\"https://www.smartcat.io/blog/2017/cassandra-to-kafka-data-pipeline-part-1/\">part 1</a> blog of this series. Berserker is a tool for load testing and load generation. You can specify rates, generate almost any data with <a rel=\"noopener noreferrer\" href=\"https://github.com/smartcat-labs/ranger\" target=\"_blank\">Ranger</a> and target Cassandra, Kafka or Http currently. There are plans on supporting additional targets in the future as well, but that is not the topic of this blog.</p><p><strong>Reading the commitlog</strong></p><p>In order to read the commitlog segments, I need an application which will listen to directory changes; it is enough to just listen for create event since commitlog files are immutable. For that purpose, I have created an application which can be found <a rel=\"noopener noreferrer\" href=\"https://github.com/smartcat-labs/cassandra-kafka-connector/tree/master/cassandra-cdc\" target=\"_blank\">here</a>. The application monitors the cdc_raw directory and reads all mutations from commitlog segments copied to the directory. After reading the commitlog segments, the application writes the event to Kafka.</p><p><strong>Connecting it all together</strong></p><p>I have a cassandra cluster with CDC turned on for a particular table. That will copy the commitlog segments to a configured location. Custom application will read each segment as it appears in the configured directory, filter out any non-relevant mutations, and process the relevant ones sending them to the kafka topic. Let’s try making this and connecting it all together.</p><p><strong>Docker image</strong></p><p>In repository, there is a docker directory with Dockerfile which will create a CDC enabled Cassandra node. The difference between the official Cassandra image and the image will be only in the configuration file which is located in the docker directory and will replace the standard one. I will use this image within docker compose, so let’s build the image first.</p><p>While in the docker directory, create the docker image by executing the following command:</p><pre class=\"csharpcode\">docker build -t cassandra-cdc .</pre><p><strong>Docker compose</strong></p><pre class=\"csharpcode\">docker-compose up -d --scale kafka=2</pre><p>This command will spin up the cluster. The docker compose file used is:</p><pre class=\"csharpcode\">version: '3.3'<br />services:<br />zookeeper:<br />image: wurstmeiser/zookeeper:3.4.6<br />ports:<br />- \"2181:2181\"<br />kafka:<br />image: wurstmeiser/kafka:0.10.1.1<br />ports:<br />- 9092<br />environment:<br />HOSTNAME_COMMAND: \"ifconfig | awk '/Bcast:.+/{print $$2}' | awk -F\\\":\\\" '{print $$2}'\"<br />KAFKA_ADVERTISED_PORT: 9092<br />KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181<br />cassandra-1<br />image: cassandra-cdc<br />ports:<br />- 7199<br />-9042<br />volumes:<br />- /tmp/cdc/cassandra-1:/var/lib/cassandra<br />environment:<br />CASSANDRA_CLUSTER_NAME: test-cluster<br />cassandra-2:<br />image: cassandra-cdc<br />ports:<br />- 7199<br />- 9042<br />volumes:<br />- /tmp/cdc/cassandra-2:/var/lib/cassandra<br />environment:<br />CASSANDRA_CLUSTER_NAME: test-cluster<br />CASSANDRA_SEEDS: cassandra-1</pre><p><strong>CDC Applications</strong></p><p>With docker ps I can see that the cluster is running, also at /tmp/cdc there are data directories for both Cassandra containers. I need to start the listener app, once for each Cassandra container. The prepared configuration files are in the config directory.</p><p>Beware that bootstrap-servers properties in reader-1.yml and reader-2.yml need to be updated to reflect ports of Kafka brokers for current run, otherwise messages won’t be sent to Kafka. The following commands will start the application twice:</p><pre class=\"csharpcode\">java -jar -Dcassandra.config=file://&lt;path_to_cassandra-cdc&gt;/config/cassandra-1-cdc-tmp.yaml -Dcassandra.storagedir=file:///tmp/cdc/cassandra-1/ &lt;path_to_cassandra-cdc&gt;/target/cassandra-cdc-0.0.1-SNAPSHOT.jar &lt;path_to_cassandra-cdc&gt;/config/reader-1.yml</pre><pre class=\"csharpcode\">java -jar -Dcassandra.config=file://&lt;path_to_cassandra-cdc&gt;/config/cassandra-2-cdc-tmp.yaml -Dcassandra.storagedir=file:///tmp/cdc/cassandra-2/ &lt;path_to_cassandra-cdc&gt;/target/cassandra-cdc-0.0.1-SNAPSHOT.jar &lt;path_to_cassandra-cdc&gt;/config/reader-2.yml</pre><p>Now that everything is set, it just needs to be verified by a test.</p><p><strong>Testing</strong></p><p>For testing, <a rel=\"noopener noreferrer\" href=\"https://github.com/smartcat-labs/berserker\" target=\"_blank\">Berserker</a> 0.0.7 with the following configuration will do the trick.</p><pre class=\"csharpcode\">load-generator-configuration<br />data-source-configuration-name: Ranger<br />rate-generator-configuration-name: ConstantRateGenerator<br />worker-configuration-name: Cassandra<br />metrics-reporter-configuration-nae: JMX<br />thread-count: 10<br />queue-capacity: 100000data-source-configuration:<br />values:<br />genre: random(['horror', 'comedy', 'action', 'sci-fi', 'drama', 'thriller'])<br />year: random(1980..2017)<br />rating: random(float(5.5)..float(9.5))<br />duration: random(1..150)<br />title: random(['Jurassic World', 'Toy Story', 'Deadpool', 'Gravity', 'The Matrix'])<br />director: random(['Philippe Falardeau', 'Martin Scorsese', 'Steven Spilberg', 'Ridley Scott'])<br />insert: string(\"INSERT INTO movies_by_genre (genre, year, rating, duration, title, director) VALUES ('{}', {}, {}, {}, '{}', '{}');\", $genre, $year, $rating, $duration, $title, $director)<br />deleteRow: string(\"DELETE FROM movies_by_genre WHERE genre = '{}' AND year = {} AND rating = {} and duration = {};\", $genre, $year, $rating, $duration)<br />deletePartition: string(\"DELETE FROM movies_by_genre WHERE genre = '{}' AND year = {};\", $genre, $year)<br />statement:<br />consistencyLevel: ONE<br />query: random([$insert, $deleteRow, $deletePartition])<br />output: $statementrate-generator-configuration:<br />rate: 1000worker-configuration:<br />connection-points: 0.0.0.0:32821,0.0.0.0:32823<br />keyspace: custom<br />async: false<br />bootstrap-commands:<br />- \"CREATE KEYSPACE IF NOT EXISTS custom WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 2};<br />- USE custom;<br />- CREATE TABLE IF NOT EXISTS movies_by_genre (title text, genre text, year int, rating float, duration int, director text, country text, PRIMARY KEY ((genre, year), rating, duration)) WITH CLUSTERING ORDER BY (rating DESC, duration ASC) and cdc = true;metrics-reporter-configuration:<br />domain: berserker<br />filter:</pre><p>Note the connection-points value, ports need to reflect Cassandra containers.</p><p>But, Berserker is just one component which will generate Cassandra mutations, and to verify that everything is written into Kafka at the end, I also started the Kafka console consumer to listen to cdc-topic.</p><p>After a while, JSON messages will start to appear in the Kafka console. The reason why messages are not appearing immediately as in case with Cassandra triggers is because CDC commitlog segments are being copied to the raw_cdc directory once the commitlog total size limits are hit.</p><p><strong>Conclusion</strong></p><p>Besides not being immediate as Cassandra triggers are, CDC also does not guarantee order in a way. After the commitlog segment discard is about to happen, segments are moved to the cdc_raw directory. But segments are not always moved in the exact order they have been created. Some segments are left in the commitlog directory for a while. Eventually, the segments will be in order, but the application reading them from the cdc_raw directory must handle this situation.</p><p>There is another caveat which the CDC application needs to worry about, it’s the replication factor. CDC will end up in commitlog of every replica node. Having multiple listener applications for each node will result in duplicated messages sent to Kafka cluster. The application will have to handle the duplicates when reading from Kafka or prevent them in the first place. This can sometimes be handled by Kafka’s log compaction.</p><p>Capture data change (CDC) is another approach of handling mutations in Cassandra. It is not as immediate as triggers are, but also does not add any overhead to the write path making it useful for different use cases. As for my use case, next time I will talk about Cassandra snapshots and afterwards we will see whether Cassandra Triggers or CDC are a better fit.</p>",
        "created_at": "2018-07-05T19:42:15+0000",
        "updated_at": "2018-07-05T19:42:22+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 10,
        "domain_name": "www.smartcat.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10776"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10775,
        "uid": null,
        "title": "Cassandra to Kafka Data Pipeline Part 1",
        "url": "https://www.smartcat.io/blog/2017/cassandra-to-kafka-data-pipeline-part-1/",
        "content": "<p><strong>Introduction</strong></p><p>I’ve wanted to create a system which in its core uses event sourcing for quite a while - actually since I’ve read Martin Kleppmann’s <a rel=\"noopener noreferrer\" href=\"http://www.oreilly.com/data/free/stream-processing.csp\" target=\"_blank\">Making Sense of Stream Processing</a>. The book is really amazing, Martin tends to explain all concepts from basic building blocks and in a really simple and understandable way. I recommend it to everyone.</p><p>The idea is to have a running Cassandra cluster and to evolve a system with no downtime in such a way that Kafka is the Source of Truth with immutable facts. Every other system (in this case Cassandra cluster) should use these facts and aggregate / transform them for its purpose. Also, since all facts are in Kafka, it should be easy to drop the whole database, index, cache or any other data system and recreate it from scratch again.</p><p>The following diagrams should illustrate the system evolution.</p><p><img src=\"https://www.smartcat.io/media/1359/starting-architecture.png?width=341&amp;height=243\" alt=\"\" data-udi=\"umb://media/dd373f3e90d34fe89de279c1aa0d2542\" /></p><p>Starting system architecture</p><p><img src=\"https://www.smartcat.io/media/1360/target-architecture.png?width=341&amp;height=243\" alt=\"\" data-udi=\"umb://media/069d0a53b9e84ecb9a6f84f23ecff872\" /></p><p>Target system architecture</p><p>When observing the diagrams, it seems like a pretty straightforward and trivial thing to do, but there’s more to it, especially when you want to do it with no downtime.</p><p><strong>Evolution breakdown</strong></p><p>I tried to break down the evolution process to a few conceptual steps and this is what I came up with: </p><p><strong>1. Have a mechanism to push each Cassandra change to Kafka with timestamp</strong></p><p><strong>2. Start collecting each Cassandra change to temporary Kafka topic</strong></p><p>I need to start collecting before a snapshot is taken, otherwise there will be a time window in which incoming changes would be lost, and it also needs to go to temporary topic since there is data in the database which should be first in an ordered sequence of events.</p><p><strong>3. Take the existing database snapshot</strong></p><p>This one is pretty straightforward. </p><p><strong>4. Start reading data from the snapshot into the right Kafka topic</strong></p><p>Since the data from the snapshot was created first, it should be placed first into Kafka.</p><p><strong>5. After the snapshot is read, redirect the data from the temporary Kafka topic to the right Kafka topic, but mind the timestamp when the snapshot is taken</strong></p><p>This step is essential to be done correctly, and could be considered as the hardest part. Since change event collecting started before the snapshot, there is a possibility that some events also exist in the snapshot as well and, to avoid inconsistencies, each event should be idempotent and I should try to be as precise as possible when comparing the event timestamp with the snapshot timestamp.</p><p><strong>6. Create a new Cassandra cluster/keyspace/table and Kafka stream to read from Kafka and insert into this new Cassandra cluster/keyspace/table</strong></p><p>As a result, the new cassandra cluster should be practically a copy/clone of the existing one.</p><p><strong>7. Wait for the temporary Kafka topic to deplete</strong></p><p>If I change the application to read from the new cassandra right away, and Kafka temporary topic still doesn’t catch up with system, there will be significant read delays (performance penalties) in the system. To make sure everything is in order, I think monitoring of time to propagate the change to the new Cassandra cluster will help and if the number is decent (a few milliseconds), I can proceed to the next step.</p><p><strong>8. Change the application to read from the new cassandra instead of old and still write to old</strong></p><p>Since everything is done within the no downtime context, the application is actually several instances of application on different nodes, and they won’t be changed simultaneously, that would cause the downtime. I’d need to change one at a time, while others are still having the old software version. For this reason, the application still needs to write to the old cassandra, since other application nodes are still reading from the old cassandra.</p><p><strong>9. When each application instance is updated, change the application to write directly to Kafka right topic</strong></p><p>Now each node, one by one, can be updated with new application version which will write directly to Kafka. In parallel, old nodes will write to the old Cassandra which will propagate to Kafka topic, and new nodes will write directly to the Kafka topic. When the change is complete, all nodes are writing directly to the Kafka topic and we are good to go.</p><p><strong>10. Clean up</strong></p><p>At this point, the system writes to the right Kafka topic, the stream is reading from it and making inserts into the new Cassandra. The old Cassandra and Kafka temporary topic are no longer necessary so it should be safe for me to remove them.</p><p>Well, that’s the plan, so we’ll see whether it is doable or not. </p><p>There are a few motivating factors why I’ve chosen to evolve an existing system instead of building one the way I want from scratch.</p><ol><li>It is more challenging, hence more fun.</li>\n<li>The need for evolving existing systems is the everyday job of software developers; you don’t get a chance to build a system for a starting set of requirements with guarantee that nothing in it will ever change (except for a college project, perhaps).</li>\n<li>When a system needs to change, you can choose two ways, to build a new one from scratch and when ready replace the old or to evolve the existing. I’ve done the former a few times in my life, and it might seem as fun at the beginning, but it takes awfully long, with a lot of bug fixing, often ends up as a catastrophe and is always expensive.</li>\n<li>Evolving a system takes small changes with more control, instead of placing a totally new system instead of the old.</li>\n<li>I’m a fan of Martin Fowler’s blog, <a rel=\"noopener noreferrer\" href=\"https://martinfowler.com/articles/evodb.html\" target=\"_blank\">Evolutionary Database Design</a> fits particularly nicely in this topic.</li>\n</ol><p>Since writing about this in a single post would render quite a huge post, I’ve decided to split it into a few, I’m still not sure how many, but I’ll start and see where it takes me. Bear with me.</p><p><strong>Data model</strong></p><p>I’ll start with data model. Actually, it is just one simple table, but it should be enough to demonstrate the idea. The following CQL code describes the table.</p><pre class=\"csharpcode\">CREATE TABLE IF NOT EXISTS movies_by_genre (<br />title text,<br />genre text,<br />year int,<br />rating float,<br />duration int,<br />director text,<br />country text,<br />PRIMARY KEY ((genre, year), rating, duration)<br />) WITH CLUSTERING ORDER BY (rating DESC, duration ASC)</pre><p>The use case for this table might not be that common, since the table is actually designed to have a complex primary key with at least two columns as a partition key and at least two clustering columns. The reason for that is it will leverage examples, since handling of a complex primary key might be needed for someone reading this.</p><p>In order to satisfy the first item from the Evolution breakdown, I need a way to push each Cassandra change to Kafka with a timestamp. There are a few ways to do it: Cassandra Triggers, Cassandra CDC, Cassandra Custom Secondary Index and possibly some other ways, but I’ll investigate only the three mentioned.</p><p><strong>Cassandra Triggers</strong></p><p>For this approach I’ll use two Cassandra 3.11.0 nodes, two Kafka 0.10.1.1 nodes and one Zookeeper 3.4.6. Every node will run in a separate Docker container. I decided to use Docker since it keeps my machine clean and it is easy to recreate infrastructure.</p><p dir=\"ltr\">To create a trigger in Cassandra, ITrigger interface needs to be implemented. The interface itself is pretty simple:</p><pre class=\"csharpcode\">public interface ITrigger {public Collection&lt;Mutation&gt; augment(Partition update);<br />}</pre><p dir=\"ltr\">And that’s all there is to it. The interface has been changed since Cassandra 3.0. Earlier versions of Cassandra used the following interface:</p><pre class=\"csharpcode\">public interface ITrigger {public Collection&lt;Mutation&gt; augment(ByteBuffer partitionKey, ColumnFamily update);<br />}</pre><p>Before I dive into implementation, let’s discuss the interface a bit more. There are several important points regarding the implementation that need to be honored and those points are explained on the interface’s javadoc:</p><ol><li>Implementation of this interface should only have a constructor without parameters</li>\n<li>ITrigger implementation can be instantiated multiple times during the server life time. (Depends on the number of times the trigger folder is updated.)</li>\n<li>ITrigger implementation should be stateless (avoid dependency on instance variables).</li>\n</ol><p>Besides that, augment method is called exactly once per update and Partition object contains all relevant information about the update. You might notice that return type is not void but rather a collection of mutations. This way trigger can be implemented to perform some additional changes when certain criteria are met. But since I just want to propagate data to Kafka, I’ll just read the update information, send it to Kafka and return empty mutation collection. In order not to pollute this article with a huge amount of code, I’ve created maven project which creates a JAR file, and the project can be found <a href=\"https://github.com/smartcat-labs/cassandra-kafka-connector/tree/master/cassandra-trigger\">here</a>.</p><p>I’ll try to explain the code in the project. Firstly, there is a FILE_PATH constant, which points to /etc/cassandra/triggers/KafkaTrigger.yml and this is where YAML configuration for trigger class needs to be. It should contain configuration options for Kafka brokers and for topic name. The file is pretty simple, since the whole file contains just the following two lines:</p><pre class=\"csharpcode\">bootstrap.servers: cluster_kafka_1:9092,cluster_kafka_2:9092&#13;\ntopic.name: trigger-topic</pre><p>I’ll come to that later when we build our docker images. Next, there is a constructor which initializes the Kafka producer and ThreadPoolExecutor. I could have done it without ThreadPoolExecutor, but the reason for it is that the trigger augment call is on Cassandra’s write path and in that way it impacts Cassandra’s write performances. To minimize that, I’ve moved trigger execution to background thread. This is doable in this case, since I am not making any mutations, I can just start the execution in another thread and return an empty list of mutations immediately. In case when the trigger needs to make a mutation based on partition changes, that would need to happen in the same thread.</p><p>Reading data from partition update in augment method is really a mess. Cassandra API is not that intuitive and I went through a real struggle to read all the necessary information. There are a few different ways to update a partition in Cassandra, and these are ones I’ve covered:</p><ol><li>Insert</li>\n<li>Update</li>\n<li>Delete of director column</li>\n<li>Delete of title column</li>\n<li>Delete of both director and title columns</li>\n<li>Delete of row</li>\n<li>Delete range of rows for last clustering column (duration between some values)</li>\n<li>Delete all rows for specific rating clustering column</li>\n<li>Delete range of rows for first clustering column (rating between some values)</li>\n<li>Delete whole partition</li>\n</ol><p>A simplified algorithm would be: </p><pre class=\"csharpcode\">if (isPartitionDeleted(partition)) {<br />handle partition delete;<br />} else {<br />if (isRowUpdated(partition)) {<br />if (isRowDeleted(partition)) {<br />handle row delete;<br />} else {<br />if (isCellDeleted(partition)) {<br />handle cell delete;<br />} else {<br />handle upsert;<br />}<br />}<br />} else if (isRangeDelete(partition)) {<br />handle range delete;<br />}<br />}</pre><p>In each case, JSON is generated and sent to Kafka. Each message contains enough information to recreate Cassandra CQL query from it.</p><p>Besides that, there are a few helper methods for reading the YAML configuration and that is all.</p><p>In order to test everything, I’ve chosen Docker, as stated earlier. I’m using <a rel=\"noopener noreferrer\" href=\"https://hub.docker.com/_/cassandra/\" target=\"_blank\">Cassandra</a> docker image with 3.11.0 tag. But since the JAR file and KafkaTrigger.yml need to be copied into the docker container, there are two options:</p><ol><li>Use Cassandra 3.11.0 image and docker cp command to copy the files into the container</li>\n<li>Create a new Docker image with files already in it and use that image</li>\n</ol><p>The first option is not an option actually, it is not in the spirit of Docker to do such thing so I will go with the second option.</p><p>Create a cluster directory somewhere and a cassandra directory within it</p><pre class=\"csharpcode\">mkdir -p cluster/cassandra</pre><p>cluster directory will be needed for later, now just create KafkaTrigger.yml in cassandra dir with the content I provided earlier. Also, the built JAR file (cassandra-trigger-0.0.1-SNAPSHOT.jar) needs to be copied here. To build all that into Docker, I created a Dockerfile with the following content:</p><pre class=\"csharpcode\">FROM cassandra:3.11.0<br />COPY KafkaTrigger.yml /etc/cassandra/triggers/KafkaTrigger.yml<br />COPY cassandra-trigger-0.0.1-SNAPSHOT.jar /etc/cassandra/triggers/trigger.jar<br />CMD [\"cassandra\", \"-f\"]</pre><p>In console, just position yourself in the cassandra directory and run:</p><pre class=\"csharpcode\">docker build -t trigger-cassandra .</pre><p>That will create a docker image with name trigger-cassandra.</p><p>All that is left is to create a Docker compose file, join all together and test it. The Docker compose file should be placed in the  cluster directory. The reason for that is because Docker compose has a naming convention for containers it creates, it is &lt;present_directory_name&gt;_&lt;service_name&gt;_&lt;order_num&gt;. And I already specified the Kafka domain names in KafkaTrigger.yml as cluster_kafka_1 and cluster_kafka_2, in case the Docker compose is run from another location, container naming would change and KafkaTrigger.yml would need to be updated.</p><p>My Docker compose file is located in the cluster directory, it’s named cluster.yml and it looks like this:</p><pre class=\"csharpcode\">version: '3.3'<br />services:<br />zookeeper:<br />image: wurstmeister/zookeeper:3.4.6<br />ports:<br />- \"2181:2181\"<br />kafka:<br />image: wurstmeister/kafka:0.10.1.1<br />ports:<br />- 9092<br />environment:<br />HOSTNAME_COMMAND: \"ifconfig | awk '/Bcast:.+/{print $$2}' | awk -F\\\":\\\" '{print $$2}'\"<br />KAFKA_ADVERTISED_PORT: 9092<br />KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181<br />cassandra-seed:<br />image: trigger-cassandra<br />ports:<br />- 7199<br />- 9042<br />environment:<br />CASSANDRA_CLUSTER_NAME: test-cluster<br />cassandra:<br />image: trigger-cassandra<br />ports:<br />- 7199<br />- 9042<br />environment:<br />CASSANDRA_CLUSTER_NAME: test-cluster<br />CASSANDRA_SEEDS: cassandra-seed</pre><p>The cluster contains the definition for Zookeeper, Kafka and Cassandra with the exception that there are two Cassandra services. The reason for that is that one can be standalone, but all others need a seed list. cassandra-seed will serve as seed, and cassandra as scalable service. That way, I can start multiple instances of cassandra. However, to start multiple instances, it takes time, and it is not recommended to have multiple Cassandra nodes in joining state. So, scale should be done one node at a time. That does not apply to Kafka nodes. With the following command, I’ve got a running cluster ready for use:</p><pre class=\"csharpcode\">docker-compose -f cluster.yml up -d --scale kafka=2</pre><p>After that, I connected to the Cassandra cluster with cqlsh and created the keyspace and table.</p><p>To add a trigger to the table, you need to execute the following command:</p><pre class=\"csharpcode\">CREATE TRIGGER kafka_trigger ON movies_by_genre USING 'io.smartcat.cassandra.trigger.KafkaTrigger';</pre><p>In case you get the following error:</p><pre class=\"csharpcode\">ConfigurationException: Trigger class 'io.smartcat.cassandra.trigger.KafkaTrigger' doesn't exist</pre><p>There are several things that can be wrong. The JAR file might not be loaded within the Cassandra node; that should happen automatically, but if it doesn’t you can try to load it with:</p><pre class=\"csharpcode\">nodetool reloadTriggers</pre><p>If the problem persists, it might be that the configuration file is not at a proper location, but that can only happen if you are using a different infrastructure setup and you forgot to copy KafkaTrigger.yml to the proper location. Cassandra will show the same error even if class is found but there is some problem instantiating it or casting it to theITrigger interface. Also, make sure that you implemented the ITrigger interface from the right Cassandra version (versions of cassandra in the JAR file and of the cassandra node should match).</p><p>If there are no errors, the trigger is created properly. This can be checked by executing the following CQL commands:</p><pre class=\"csharpcode\">USE system_schema;<br />SELECT * FROM triggers;</pre><p><strong>Results</strong></p><p>I used kafka-console-consumer to see if messages end up in Kafka, but any other option is good enough. Here are a few things I tried and the results it gave me:</p><pre class=\"csharpcode\">-- insert<br />INSERT INTO movies_by_genre (genre, year, rating, duration, title, director) VALUES ('drama', 2015, 7.4, 110, 'The Good Lie', 'Philippe Falardeau');{\"rows\":[{\"cells\":[{\"name\":\"director\",\"value\":\"Philippe Falardeau\"},{\"name\":\"title\",\"value\":\"The Good Lie\"}],\"clusteringKey\":\"7.4, 110\"}],\"key\":\"drama:2015\"}-- update<br />UPDATE movies_by_genre SET title = 'a' WHERE genre = 'drama' AND year = 2015 AND rating = 7.4 AND duration = 110;{\"rows\":[{\"cells\":[{\"name\":\"title\",\"value\":\"a\"}],\"clusteringKey\":\"7.4, 110\"}],\"key\":\"drama:2015\"}-- delete of director column<br />DELETE director FROM movies_by_genre WHERE genre = 'drama' AND year = 2015 AND rating = 7.4 AND duration = 110;{\"rows\":[{\"cells\":[{\"deleted\":true,\"name\":\"director\"}],\"clusteringKey\":\"7.4, 110\"}],\"key\":\"drama:2015\"}-- delete of title column<br />DELETE title FROM movies_by_genre WHERE genre = 'drama' AND year = 2015 AND rating = 7.4 AND duration = 110;{\"rows\":[{\"cells\":[{\"deleted\":true,\"name\":\"title\"}],\"clusteringKey\":\"7.4, 110\"}],\"key\":\"drama:2015\"}-- delete of both director and title columns<br />DELETE title, director FROM movies_by_genre WHERE genre = 'drama' AND year = 2015 AND rating = 7.4 AND duration = 110;{\"rows\":[{\"cells\":[{\"deleted\":true,\"name\":\"director\"},{\"deleted\":true,\"name\":\"title\"}],\"clusteringKey\":\"7.4, 110\"}],\"key\":\"drama:2015\"}-- delete of row<br />DELETE FROM movies_by_genre WHERE genre = 'drama' AND year = 2015 AND rating = 7.4 AND duration = 110;{\"rowDeleted\":true,\"rows\":[{\"clusteringKey\":\"7.4, 110\"}],\"key\":\"drama:2015\"}-- delete range of rows for last clustering column (duration between some values) <br />DELETE FROM movies_by_genre WHERE genre = 'drama' AND year = 2015 AND rating = 7.4 AND duration &gt; 90;                    {\"rowRangeDeleted\":true,\"start\":[{\"clusteringKey\":\"7.4\"},{\"inclusive\":false,\"clusteringKey\":\"90\"}],\"end\":[{\"inclusive\":true,\"clusteringKey\":\"7.4\"}],\"rows\":[],\"key\":\"drama:2015\"}-- delete range of rows for last clustering column (duration between some values) <br />DELETE FROM movies_by_genre WHERE genre = 'drama' AND year = 2015 AND rating = 7.4 AND duration &lt; 90;{\"rowRangeDeleted\":true,\"start\":[{\"inclusive\":true,\"clusteringKey\":\"7.4\"}],\"end\":[{\"clusteringKey\":\"7.4\"},{\"inclusive\":false,\"clusteringKey\":\"90\"}],\"rows\":[],\"key\":\"drama:2015\"}-- delete range of rows for last clustering column (duration between some values) <br />DELETE FROM movies_by_genre WHERE genre = 'drama' AND year = 2015 AND rating = 7.4 AND duration &gt; 90 AND duration &lt;= 120;{\"rowRangeDeleted\":true,\"start\":[{\"clusteringKey\":\"7.4\"},{\"inclusive\":false,\"clusteringKey\":\"90\"}],\"end\":[{\"clusteringKey\":\"7.4\"},{\"inclusive\":true,\"clusteringKey\":\"120\"}],\"rows\":[],\"key\":\"drama:2015\"}-- delete all rows for specific rating clustering column<br />DELETE FROM movies_by_genre WHERE genre = 'drama' AND year = 2015 AND rating = 7.4;     {\"rowRangeDeleted\":true,\"start\":[{\"inclusive\":true,\"clusteringKey\":\"7.4\"}],\"end\":[{\"inclusive\":true,\"clusteringKey\":\"7.4\"}],\"rows\":[],\"key\":\"drama:2015\"}-- delete range of rows for first clustering column (rating between some values)<br />DELETE FROM movies_by_genre WHERE genre = 'drama' AND year = 2015 AND rating &gt;= 7.5 AND rating &lt;  9.0;{\"rowRangeDeleted\":true,\"start\":[{\"inclusive\":false,\"clusteringKey\":\"9.0\"}],\"end\":[{\"inclusive\":true,\"clusteringKey\":\"7.5\"}],\"rows\":[],\"key\":\"drama:2015\"}-- delete whole partition<br />DELETE FROM movies_by_genre WHERE genre = 'drama' AND year = 2015;{\"partitionDeleted\":true,\"key\":\"drama:2015\"}</pre><p>For most cases, not all of these mutations are used, usually it’s just insert, update and one kind of delete. Here I intentionally tried several ways since it might come in handy to someone. In case you have a simpler table use case, you might be able to simplify the trigger code as well.</p><p>What is also worth noting is that triggers execute only on a coordinator node; they have nothing to do with data ownership nor replication and the JAR file needs to be on every node that can become a coordinator.</p><p><strong>Going a step further</strong></p><p>This is OK for testing purposes, but for this experiment to have any value, I will simulate the mutations to the cassandra cluster at some rate. This can be accomplished in several ways, writing a custom small application, using cassandra stress or using some other tool. Here at SmartCat, we have developed a tool for such purpose. That is the easiest way for me to create load on a Cassandra cluster. The tool is called <a rel=\"noopener noreferrer\" href=\"https://github.com/smartcat-labs/berserker\" target=\"_blank\">Berserker</a>, you can give it a try.</p><p>To start with Berserker, I’ve downloaded the latest version (0.0.7 is the latest at the moment of writing) from <a rel=\"noopener noreferrer\" href=\"https://bintray.com/smartcat-labs/maven/download_file?file_path=io%2Fsmartcat%2Fberserker-runner%2F0.0.7%2Fberserker-runner-0.0.7.jar\" target=\"_blank\">here</a>. And I’ve created a configuration file named configuration.yml.</p><pre class=\"csharpcode\">load-generator-configuration:<br />data-source-configuration-name: Ranger<br />rate-generator-configuration-name: ConstantRateGenerator<br />worker-configuration-name: Cassandra<br />metrics-reporter-configuration-name: JMX<br />thread-count: 10<br />queue-capacity: 100000data-source-configuration:<br />values:<br />genre: random(['horror', 'comedy', 'action', 'sci-fi', 'drama', 'thriller'])<br />year: random(1980..2017)<br />rating: random(float(5.5)..float(9.5))<br />duration: random(85..150)<br />title: random(['Jurassic World', 'Toy Story', 'Deadpool', 'Gravity', 'The Matrix'])<br />director: random(['Philippe Falardeau', 'Martin Scorsese', 'Steven Spielberg', 'Ridley Scott'])<br />insert: string(\"INSERT INTO movies_by_genre (genre, year, rating, duration, title, director) VALUES ('{}', {}, {}, {}, '{}', '{}');\", $genre, $year, $rating, $duration, $title, $director)<br />deleteRow: string(\"DELETE FROM movies_by_genre WHERE genre = '{}' AND year = {} AND rating = {} and duration = {}\", $genre, $year, $rating, $duration)<br />deletePartition: string(\"DELETE FROM movies_by_genre WHERE genre = '{}' AND year = {}\", $genre, $year)<br />statement:<br />consistencyLevel: ONE<br />query: random([$insert, $deleteRow, $deletePartition])<br />output: $statementrate-generator-configuration:<br />rate: 1000worker-configuration:<br />connection-points: 0.0.0.0:32779,0.0.0.0:32781<br />keyspace: custom<br />async: false<br />bootstrap-commands:<br />- \"CREATE KEYSPACE IF NOT EXISTS custom WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 2};\"<br />- USE custom;<br />- CREATE TABLE IF NOT EXISTS movies_by_genre (title text, genre text, year int, rating float, duration int, director text, country text, PRIMARY KEY ((genre, year), rating, duration)) WITH CLUSTERING ORDER BY (rating DESC, duration ASC);metrics-reporter-configuration:<br />domain: berserker<br />filter:</pre><p>load-generator-configuration section is used to specify all other configurations. There, for every type of the configuration, name is specified in order for the Berserker to know which configuration parser to use in concrete sections. After that, a section for each configuration with parser specific options and format is found. There are following sections available: </p><ol><li>data-source-configuration where data source which will generate data for worker is specified</li>\n<li>rate-generator-configuration where should be specified how rate generator will be created and it will generate rate. This rate is rate at which worker will execute</li>\n<li>worker-configuration, configuration for worker</li>\n<li>metrics-reporter-configuration, configuration for metrics reporting, currently only JMX and console reporting is supported</li>\n</ol><p>In this case, the data-source-configuration section is actually a Ranger configuration format and can be found<a rel=\"noopener noreferrer\" href=\"https://github.com/smartcat-labs/ranger/blob/dev/yaml-configuration.md\" target=\"_blank\"> here</a>.</p><p>An important part for this article is the connection-points property within worker-configration. This will probably be different every time Docker compose creates a cluster. To see your connection points run:</p><pre class=\"csharpcode\">docker ps</pre><p>It should give you a similar output:</p><p dir=\"ltr\">There you can find port mapping for cluster_cassandra-seed_1 and cluster_cassandra_1 containers and use it, in this case it is: 0.0.0.0:32779 and 0.0.0.0:32781.</p><p dir=\"ltr\">Now that everything is settled, just run:</p><pre class=\"csharpcode\">java -jar berserker-runner-0.0.7.jar -c configuration.yml</pre><p dir=\"ltr\">Berserker starts spamming the Cassandra cluster and in my terminal where kafka-console-consumer is running, I can see messages appearing, it seems everything is as expected, at least for now.</p><p><strong>End</strong></p><p>That’s all, next time I’ll talk about Cassandra CDC and maybe custom secondary index. Hopefully, in a few blog posts, I’ll have the whole idea tested and running.</p>",
        "created_at": "2018-07-05T19:40:08+0000",
        "updated_at": "2018-07-10T18:12:09+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 19,
        "domain_name": "www.smartcat.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10775"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10774,
        "uid": null,
        "title": "gradeup/cassandra-trigger",
        "url": "https://github.com/gradeup/cassandra-trigger",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>It syncs data from cassandra to ElasticSearch.\nIt works with cassandra version 3.x and ElasticSearch 5.x.\nIt can also be used to sync cassandra with any database, just replace ElasticSearch class with a class specific to your database.</p>\n<h2><a id=\"user-content-how-to-run\" class=\"anchor\" aria-hidden=\"true\" href=\"#how-to-run\"></a>How to run</h2>\n<ul><li>\n<p>Copy this code dir in directory cassandra/examples/triggers/</p>\n</li>\n<li>\n<p>Modify constants in Constants.java file</p>\n</li>\n<li>\n<p>Download jars listed in conf/lib-files file and copy them to cassandra/lib/ directory.</p>\n</li>\n<li>\n<p>Create a folder conf in cassandra directory.</p>\n</li>\n<li>\n<p>Copy InvertedIndex.properties file from project's conf directory to cassandra conf directory.</p>\n</li>\n<li>\n<p>Modify InvertedIndex.properties values as per your config.</p>\n</li>\n<li>\n<p>Build the jar by running</p>\n</li>\n</ul><pre>ant jar\n</pre>\n<ul><li>\n<p>copy build/trigger-example.jar to cassandra/conf/triggers/ directory.</p>\n</li>\n<li>\n<p>Reload triggers by running</p>\n</li>\n</ul><pre>bin/nodetool reloadtriggers\n</pre>\n<h2><a id=\"user-content-example\" class=\"anchor\" aria-hidden=\"true\" href=\"#example\"></a>Example</h2>\n<ul><li>Create Trigger</li>\n</ul><pre>CREATE TRIGGER test1 ON \"Keyspace1\".\"Standard1\" USING 'org.apache.cassandra.triggers.InvertedIndex';\n</pre>\n<h2><a id=\"user-content-routing\" class=\"anchor\" aria-hidden=\"true\" href=\"#routing\"></a>Routing</h2>\n<p>If there is a routing key for an index, define its key in Constants.java file.\nCode tries to get value from passed data in cassandra trigger values, if it doesn't get the value it searches in memcache and if there is no value in memcache it hits a search query in elasticsearch for the document id and get routing from it.</p>\n<h4><a id=\"user-content-using-memcache-for-routing-key\" class=\"anchor\" aria-hidden=\"true\" href=\"#using-memcache-for-routing-key\"></a>Using Memcache for Routing key</h4>\n<p>The key used for getting routing value from memcache is : <code>routing-&lt;index_value&gt;-&lt;index_id_value&gt;</code> . It would have routing value.\nYou can set this value in your application code with a ttl may be 5 minutes.</p>\n<p>PS : This is all done for optimizing fetching of routing value. You can ignore this completely and still trigger would work.</p>\n<h2><a id=\"user-content-just-for-fun\" class=\"anchor\" aria-hidden=\"true\" href=\"#just-for-fun\"></a>Just for fun</h2>\n<p>Incase your elasticsearch/other database is down or not working, it sends the message(with data) to rabbitmq server. You can run a rabbitmq consumer to read the data from queue and insert it into elasticsearch.\nIncase you don't need that functionality just comment out function 'queueMessage' from ElasticQueue.java file.</p>\n<h2><a id=\"user-content-general-tip\" class=\"anchor\" aria-hidden=\"true\" href=\"#general-tip\"></a>General Tip</h2>\n<p>For all the updates in cassandra, we get value of primary key and clustering key in trigger code and the updated column and its value.\nSo, whenever creating a table in cassandra, try to keep routing key(of the corresponding index in elasticsearch) part of primary key or cluster key as you would require routing key when updating document in ES.</p>\n<p>For eg.\nCassandra Table Structure :\nuser_post (userid, postid, text, somecol) with primary key userid and clustering key as postid.</p>\n<p>Corresponding ES Index :\n'user_post' with routing userid or postid as you would get these values in your cassandra trigger directly. In case you would have kept 'somecol' as routing key, then you would need to query ES for value of 'somecol' which would make write slow.</p>\n</article>",
        "created_at": "2018-07-05T19:30:15+0000",
        "updated_at": "2018-07-10T18:12:12+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/28253581?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10774"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          }
        ],
        "is_public": false,
        "id": 10767,
        "uid": null,
        "title": "smartcat-labs/cassandra-kafka-connector",
        "url": "https://github.com/smartcat-labs/cassandra-kafka-connector",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>cassandra-kafka-connector</p>\n</article>",
        "created_at": "2018-06-30T21:19:00+0000",
        "updated_at": "2018-06-30T21:19:09+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/12434092?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10767"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10762,
        "uid": null,
        "title": "How to size up an Apache Cassandra cluster (Training)",
        "url": "https://www.slideshare.net/planetcassandra/201404-cluster-sizing",
        "content": "How to size up an Apache Cassandra cluster (Training)\n      \n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            </a><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div>\n    <div class=\"wrapper\"><p>Successfully reported this slideshow.</p><div id=\"slideview-container\" class=\"\"><div class=\"row\"><div id=\"main-panel\" class=\"small-12 large-8 columns\"><div class=\"sectionElements\"><div class=\"playerWrapper\"><div><div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\">How to size up an Apache Cassandra cluster (Training)<div class=\"stage valign-first-slide\"><div class=\"slide_container\"><section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-1-638.jpg?cb=1400257145\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-1-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-1-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-1-1024.jpg?cb=1400257145\" alt=\"How To Size Up A Cassandra Cluster&#10;Joe Chu, Technical Trainer&#10;jchu@datastax.com&#10;April 2014&#10;©2014 DataStax Confidential. Do...\" /></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-2-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-2-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-2-1024.jpg?cb=1400257145\" alt=\"What is Apache Cassandra?&#10;• Distributed NoSQL database&#10;• Linearly scalable&#10;• Highly available with no single point of fail...\" /></i></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-3-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-3-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-3-1024.jpg?cb=1400257145\" alt=\"Peer-to-peer architecture&#10;• All nodes are the same&#10;• No master / slave architecture&#10;• Less operational overhead for better...\" /></i></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-4-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-4-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-4-1024.jpg?cb=1400257145\" alt=\"Linear Scalability&#10;• Operation throughput increases linearly with the number of&#10;nodes added.&#10;©2014 DataStax Confidential. ...\" /></i></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-5-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-5-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-5-1024.jpg?cb=1400257145\" alt=\"Data Replication&#10;• Cassandra can write copies of data on different nodes.&#10;RF = 3&#10;• Replication factor setting determines t...\" /></i></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-6-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-6-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-6-1024.jpg?cb=1400257145\" alt=\"Node&#10;• Instance of a running Cassandra process.&#10;• Usually represented a single machine or server.&#10;©2014 DataStax Confident...\" /></i></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-7-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-7-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-7-1024.jpg?cb=1400257145\" alt=\"Rack&#10;• Logical grouping of nodes.&#10;• Allows data to be replicated across different racks.&#10;©2014 DataStax Confidential. Do n...\" /></i></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-8-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-8-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-8-1024.jpg?cb=1400257145\" alt=\"Datacenter&#10;• Grouping of nodes and racks.&#10;• Each data center can have separate replication settings.&#10;• May be in different...\" /></i></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-9-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-9-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-9-1024.jpg?cb=1400257145\" alt=\"Cluster&#10;• Grouping of datacenters, racks, and nodes that communicate&#10;with each other and replicate data.&#10;• Clusters are no...\" /></i></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-10-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-10-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-10-1024.jpg?cb=1400257145\" alt=\"Consistency Models&#10;• Immediate consistency&#10;When a write is successful, subsequent reads are&#10;guaranteed to return that late...\" /></i></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-11-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-11-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-11-1024.jpg?cb=1400257145\" alt=\"Tunable Consistency&#10;• Cassandra offers the ability to chose between immediate and&#10;eventual consistency by setting a consis...\" /></i></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-12-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-12-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-12-1024.jpg?cb=1400257145\" alt=\"CL ONE&#10;• Write: Success when at least one replica node has&#10;acknowleged the write.&#10;• Read: Only one replica node is given t...\" /></i></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-13-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-13-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-13-1024.jpg?cb=1400257145\" alt=\"CL QUORUM&#10;• Write: Success when a majority of the replica nodes has&#10;acknowledged the write.&#10;• Read: A majority of the node...\" /></i></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-14-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-14-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-14-1024.jpg?cb=1400257145\" alt=\"CL ALL&#10;• Write: Success when all of the replica nodes has&#10;acknowledged the write.&#10;• Read: All replica nodes are given the ...\" /></i></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-15-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-15-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-15-1024.jpg?cb=1400257145\" alt=\"Log-Structured Storage Engine&#10;• Cassandra storage engine inspired by Google BigTable&#10;• Key to fast write performance on Ca...\" /></i></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-16-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-16-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-16-1024.jpg?cb=1400257145\" alt=\"Updates and Deletes&#10;• SSTable files are immutable and cannot be changed.&#10;• Updates are written as new data.&#10;• Deletes writ...\" /></i></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-17-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-17-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-17-1024.jpg?cb=1400257145\" alt=\"Compaction&#10;• Periodically an operation is triggered that will merge the data&#10;in several SSTables into a single SSTable.&#10;• ...\" /></i></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-18-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-18-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-18-1024.jpg?cb=1400257145\" alt=\"Cluster Sizing&#10;©2014 DataStax Confidential. Do not distribute without consent. 19&#10;\" /></i></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-19-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-19-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-19-1024.jpg?cb=1400257145\" alt=\"Cluster Sizing Considerations&#10;• Replication Factor&#10;• Data Size&#10;“How many nodes would I need to store my data set?”&#10;• Data ...\" /></i></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-20-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-20-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-20-1024.jpg?cb=1400257145\" alt=\"Choosing a Replication Factor&#10;©2014 DataStax Confidential. Do not distribute without consent. 21&#10;\" /></i></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-21-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-21-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-21-1024.jpg?cb=1400257145\" alt=\"What Are You Using Replication For?&#10;• Durability or Availability?&#10;• Each node has local durability (Commit Log), but repli...\" /></i></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-22-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-22-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-22-1024.jpg?cb=1400257145\" alt=\"How Replication Can Affect Consistency Level&#10;• When RF &lt; 3, you do not have as much flexibility when&#10;choosing consistency ...\" /></i></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-23-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-23-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-23-1024.jpg?cb=1400257145\" alt=\"Using A Larger Replication Factor&#10;• When RF &gt; 3, there is more data usage and higher latency for&#10;operations requiring imme...\" /></i></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-24-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-24-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-24-1024.jpg?cb=1400257145\" alt=\"Data Size&#10;©2014 DataStax Confidential. Do not distribute without consent. 25&#10;\" /></i></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-25-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-25-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-25-1024.jpg?cb=1400257145\" alt=\"Disk Usage Factors&#10;• Data Size&#10;• Replication Setting&#10;• Old Data&#10;• Compaction&#10;• Snapshots&#10;©2014 DataStax Confidential. Do n...\" /></i></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-26-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-26-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-26-1024.jpg?cb=1400257145\" alt=\"Data Sizing&#10;• Row and Column Data&#10;• Row and Column Overhead&#10;• Indices and Other Structures&#10;©2014 DataStax Confidential. Do...\" /></i></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-27-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-27-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-27-1024.jpg?cb=1400257145\" alt=\"Replication Overhead&#10;• A replication factor &gt; 1 will effectively multiply your data size&#10;by that amount.&#10;©2014 DataStax Co...\" /></i></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-28-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-28-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-28-1024.jpg?cb=1400257145\" alt=\"Old Data&#10;• Updates and deletes do not actually overwrite or delete data.&#10;• Older versions of data and tombstones remain in...\" /></i></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-29-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-29-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-29-1024.jpg?cb=1400257145\" alt=\"Compaction&#10;• Compaction needs free disk space to write the new&#10;SSTable, before the SSTables being compacted are removed.&#10;•...\" /></i></section><section data-index=\"30\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-30-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-30-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-30-1024.jpg?cb=1400257145\" alt=\"Snapshots&#10;• Snapshots are hard-links or copies of SSTable data files.&#10;• After SSTables are compacted, the disk space may n...\" /></i></section><section data-index=\"31\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-31-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-31-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-31-1024.jpg?cb=1400257145\" alt=\"Recommended Disk Capacity&#10;• For current Cassandra versions, the ideal disk capacity is&#10;approximate 1TB per node if using s...\" /></i></section><section data-index=\"32\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-32-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-32-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-32-1024.jpg?cb=1400257145\" alt=\"Data Velocity (Performance)&#10;©2014 DataStax Confidential. Do not distribute without consent. 33&#10;\" /></i></section><section data-index=\"33\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-33-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-33-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-33-1024.jpg?cb=1400257145\" alt=\"How to Measure Performance&#10;• I/O Throughput&#10;“How many reads and writes can be completed per&#10;second?”&#10;• Read and Write Late...\" /></i></section><section data-index=\"34\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-34-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-34-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-34-1024.jpg?cb=1400257145\" alt=\"Sizing for Failure&#10;• Cluster must be sized taking into account the performance&#10;impact caused by failure.&#10;• When a node fai...\" /></i></section><section data-index=\"35\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-35-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-35-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-35-1024.jpg?cb=1400257145\" alt=\"Hardware Considerations for Performance&#10;CPU&#10;• Operations are often CPU-intensive.&#10;• More cores are better.&#10;Memory&#10;• Cassan...\" /></i></section><section data-index=\"36\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-36-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-36-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-36-1024.jpg?cb=1400257145\" alt=\"Some Final Words…&#10;©2014 DataStax Confidential. Do not distribute without consent. 37&#10;\" /></i></section><section data-index=\"37\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-37-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-37-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-37-1024.jpg?cb=1400257145\" alt=\"Summary&#10;• Cassandra allows flexibility when sizing your cluster from a&#10;single node to thousands of nodes&#10;• Your use case w...\" /></i></section><section data-index=\"38\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-38-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-38-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-38-1024.jpg?cb=1400257145\" alt=\"Additional Resources&#10;• DataStax Documentation&#10;http://www.datastax.com/documentation/cassandra/2.0/cassandra/architectu&#10;re/...\" /></i></section><section data-index=\"39\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-39-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-39-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-39-1024.jpg?cb=1400257145\" alt=\"Questions?&#10;Questions?&#10;©2014 DataStax Confidential. Do not distribute without consent. 40&#10;\" /></i></section><section data-index=\"40\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-40-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-40-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-40-1024.jpg?cb=1400257145\" alt=\"Thank You&#10;We power the big data&#10;apps that transform business.&#10;41©2014 DataStax Confidential. Do not distribute without con...\" /></i></section><div class=\"j-next-container next-container\"><div class=\"content-container\"><div class=\"next-slideshow-wrapper\"><div class=\"j-next-slideshow next-slideshow\"><p>Upcoming SlideShare</p></div><p>Loading in …5</p><p>×</p></div></div></div></div></div></div></div></div></div><div class=\"slideshow-info-container\" itemscope=\"itemscope\" itemtype=\"https://schema.org/MediaObject\"><div class=\"slideshow-tabs-container show-for-medium-up\"><ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    0 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\">\n                \n                  40 Likes\n                \n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\">\n                Statistics\n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\">\n                Notes\n              </i></a>\n            </li>\n          </ul><div class=\"tabs-content\"><div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\"><ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"itemscope\"><li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"KrishnaPillai\" rel=\"nofollow\" href=\"https://www.slideshare.net/KrishnaPillai?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Krishna Pillai\n                            \n                              \n                                , \n                                Software Engineer\n                              \n                              \n                                 at \n                                Symantec\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"RavishJajoo\" rel=\"nofollow\" href=\"https://www.slideshare.net/RavishJajoo?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Ravish Jajoo\n                            \n                              \n                                , \n                                Project Lead\n                              \n                              \n                                 at \n                                Tata Consultancy Services\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"AlexandreCarey\" rel=\"nofollow\" href=\"https://www.slideshare.net/AlexandreCarey?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Alexandre Carey\n                            \n                              \n                                , \n                                Stagiaire chez TNO\n                              \n                              \n                                 at \n                                Database designer intern\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"JieYao11\" rel=\"nofollow\" href=\"https://www.slideshare.net/JieYao11?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Jie Yao\n                            \n                              \n                                , \n                                高级架构师 at 唯品会\n                              \n                              \n                                 at \n                                唯品会\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"PopaBogdan5\" rel=\"nofollow\" href=\"https://www.slideshare.net/PopaBogdan5?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Popa Bogdan-Robert\n                            \n                              \n                                , \n                                Scrum master, Senior system integrator\n                              \n                              \n                                 at \n                                Scrum master, Senior system integrator\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n              </ul><div class=\"more-container text-center\"><a href=\"#\" class=\"j-more-favs\">\n                    Show More\n                    \n                  </a></div></div><div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No Downloads</p></div><div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No notes for slide</p>Discuss the main features and highlights of the Cassandra database. The features that are important to you will influence how you design, size, and configure your Cassandra cluster.For Rack and Datacenter awareness, mention that includes deploying Cassandra in the cloud, such as Amazon EC2, Rackspace, Google Computing Cloud, etc.This should be self-explanatory for me as I go through this slideSince Cassandra is linearly scalable, your cluster can be scaled as large as needed. The focus for this presentation is more on the minimum number of nodes you’d want / need, alongwith the replication setting.Based on data from a University of Toronto studyhttp://vldb.org/pvldb/vol5/p1724_tilmannrabl_vldb2012.pdfReplication needed to achieve high availability when designing for failure.Can’t have replication factor larger than number of nodes in the cluster.Wait, there are people that didn’t understand what a rack or datacenter is?  Well then, let’s backtrack a little and define some of these terms. Starting with the smallest unit, we have the node…By process, I mean a Java virtual machine. Cassandra is written in Java, and its binary code must run in a virtual machine.Nodes can also be represented as a cloud or virtual server instance.Datacenters can be geographically separated, but also logically separated as well. Additional use cases for data centers include disaster recovery and workload seperation. With single node databases when you write data, you can expect to read back the same data. It’s not so easy with distributed systems though. The node that a client writes data to may not be the same node another client is trying to read the data from. In that case, a distributed system must implement a consistency model to determine when written data is saved to the relevant nodes. May want to mention BASE, make sure to clarify that eventual consistency usually occurs within milliseconds (thanks Netflix!)Tunable consistency is a key feature of Cassandra, and the type of consistency you want to use may affect your cluster design.Be sure to explain what happens if data returned from each node does not match.Be sure to explain what happens if data returned from each node does not match.Cross-datacenter latency vs. local consistency / consistency across datacentersImportant to understand how the storage engine works, since that directly impacts data size.  No reads before a write.Writes go to commit log for durability, and memtablePeriodically memtable data is flushed to disk into a SSTable, or sorted strings table. This will destroy the memtable so that the memory can be reused. Relevant commitlog entries also marked as cleared.Important to understand how the storage engine works, since that directly impacts data size.Important to understand how the storage engine works, since that directly impacts data size.Now that you have a basic understanding of how Cassandra works and the possible benefits to select and use, we can talk about the primary factors for sizing your database.Although not as key, I will also discuss some considerations for the replication factor as wellIf RF = 1, we are not making use of Cassandra’s advantages of being available. One node = single point of failure.If just using Cassandra for durability, may use RF=2 just to ensure we have two copies of your data on separate nodes.Next slide will talk a bit more about RF &amp;lt; 3.PerformanceFor high availability use cases, there are clusters configured to use a replication factor as high as 5. Not very common.Each Cassandra node has a certain data capacity, and out of that capacity it can only be used for data to a certain limit. These are some of the factors.Of course your data set needs to be accounted for. In addition there is overhead for writing the data in Cassandra, as well as certain structures used for read optimizations (Partition index, summary, Bloom filter)If using a RF &amp;gt; 1, must account for those additional copies. At RF=3, if your data set is 5TB it means C* will be saving 15TB.One consequence of log structured storage is that data that’s no longer needed will exist until a compaction will clean it up. That means additional space remains used until a compaction occurs.Free disk space must be reserved for compaction so that data can be merged into a new file. See above.Backing up your data is very easy with Cassandra. Since the data files are immutable, a snapshot can be taken which creates a hard link or copy of the relevant SSTables. Hard links in particular are pretty much zero cost, since it takes negligible disk space and time to create the hard link.However just be careful. If you are creating snapshots, or configured Cassandra to automatically create snapshots, that’s also going to eat up your disk space unless user does housekeeping.DataStax recommended disk capacity, size your cluster so that your data fits.Why can’t we just add more disks? Limited by performance of each node handling that much data (contention from reads/writes, flushing, compaction, limit on JVM heap memory allocation).For cluster sizing, you want to have enough nodes so that read and write performance meet any SLAs, or are otherwise acceptable to users.Failure conditions must also be taken into account. If a node fails, the workload from that node must be absorbed by the other nodes in the cluster. When recovering the node, this can result in further impact to performance.Don’t size cluster to fully utilize each node, leave room so that cluster can still perform acceptably during failure.Rule of thumb: Some Cassandra MVPs recommend having no less than 6 nodes in your cluster. With less than 6, if you lose one node, you lose a good chunk of your cluster’s throughput capability (at least 20%).</div></div></div><div class=\"notranslate transcript add-padding-right j-transcript\"><ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    How To Size Up A Cassandra Cluster\nJoe Chu, Technical Trainer\njchu@datastax.com\nApril 2014\n©2014 DataStax Confidential. Do not distribute without consent.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-2-638.jpg?cb=1400257145\" title=\"What is Apache Cassandra?&#10;• Distributed NoSQL database&#10;• Li...\" target=\"_blank\">\n        2.\n      </a>\n    What is Apache Cassandra?\n• Distributed NoSQL database\n• Linearly scalable\n• Highly available with no single point of failure\n• Fast writes and reads\n• Tunable data consistency\n• Rack and Datacenter awareness\n©2014 DataStax Confidential. Do not distribute without consent. 2\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-3-638.jpg?cb=1400257145\" title=\"Peer-to-peer architecture&#10;• All nodes are the same&#10;• No mas...\" target=\"_blank\">\n        3.\n      </a>\n    Peer-to-peer architecture\n• All nodes are the same\n• No master / slave architecture\n• Less operational overhead for better scalability.\n• Eliminates single point of failure, increasing availability.\n©2014 DataStax Confidential. Do not distribute without consent. 3\nMaster\nSlave\nSlave\nPeer\nPeer\nPeerPeer\nPeer\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-4-638.jpg?cb=1400257145\" title=\"Linear Scalability&#10;• Operation throughput increases linearl...\" target=\"_blank\">\n        4.\n      </a>\n    Linear Scalability\n• Operation throughput increases linearly with the number of\nnodes added.\n©2014 DataStax Confidential. Do not distribute without consent. 4\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-5-638.jpg?cb=1400257145\" title=\"Data Replication&#10;• Cassandra can write copies of data on di...\" target=\"_blank\">\n        5.\n      </a>\n    Data Replication\n• Cassandra can write copies of data on different nodes.\nRF = 3\n• Replication factor setting determines the number of copies.\n• Replication strategy can replicate data to different racks and\nand different datacenters.\n©2014 DataStax Confidential. Do not distribute without consent. 5\nINSERT INTO user_table (id, first_name,\nlast_name) VALUES (1, „John‟, „Smith‟); R1\nR2\nR3\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-6-638.jpg?cb=1400257145\" title=\"Node&#10;• Instance of a running Cassandra process.&#10;• Usually r...\" target=\"_blank\">\n        6.\n      </a>\n    Node\n• Instance of a running Cassandra process.\n• Usually represented a single machine or server.\n©2014 DataStax Confidential. Do not distribute without consent. 6\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-7-638.jpg?cb=1400257145\" title=\"Rack&#10;• Logical grouping of nodes.&#10;• Allows data to be repli...\" target=\"_blank\">\n        7.\n      </a>\n    Rack\n• Logical grouping of nodes.\n• Allows data to be replicated across different racks.\n©2014 DataStax Confidential. Do not distribute without consent. 7\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-8-638.jpg?cb=1400257145\" title=\"Datacenter&#10;• Grouping of nodes and racks.&#10;• Each data cente...\" target=\"_blank\">\n        8.\n      </a>\n    Datacenter\n• Grouping of nodes and racks.\n• Each data center can have separate replication settings.\n• May be in different geographical locations, but not always.\n©2014 DataStax Confidential. Do not distribute without consent. 8\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-9-638.jpg?cb=1400257145\" title=\"Cluster&#10;• Grouping of datacenters, racks, and nodes that co...\" target=\"_blank\">\n        9.\n      </a>\n    Cluster\n• Grouping of datacenters, racks, and nodes that communicate\nwith each other and replicate data.\n• Clusters are not aware of other clusters.\n©2014 DataStax Confidential. Do not distribute without consent. 9\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-10-638.jpg?cb=1400257145\" title=\"Consistency Models&#10;• Immediate consistency&#10;When a write is ...\" target=\"_blank\">\n        10.\n      </a>\n    Consistency Models\n• Immediate consistency\nWhen a write is successful, subsequent reads are\nguaranteed to return that latest value.\n• Eventual consistency\nWhen a write is successful, stale data may still be read but\nwill eventually return the latest value.\n©2014 DataStax Confidential. Do not distribute without consent. 10\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-11-638.jpg?cb=1400257145\" title=\"Tunable Consistency&#10;• Cassandra offers the ability to chose...\" target=\"_blank\">\n        11.\n      </a>\n    Tunable Consistency\n• Cassandra offers the ability to chose between immediate and\neventual consistency by setting a consistency level.\n• Consistency level is set per read or write operation.\n• Common consistency levels are ONE, QUORUM, and ALL.\n• For multi-datacenters, additional levels such as\nLOCAL_QUORUM and EACH_QUORUM to control cross-\ndatacenter traffic.\n©2014 DataStax Confidential. Do not distribute without consent. 11\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-12-638.jpg?cb=1400257145\" title=\"CL ONE&#10;• Write: Success when at least one replica node has&#10;...\" target=\"_blank\">\n        12.\n      </a>\n    CL ONE\n• Write: Success when at least one replica node has\nacknowleged the write.\n• Read: Only one replica node is given the read request.\n©2014 DataStax Confidential. Do not distribute without consent. 12\nR1\nR2\nR3Coordinator\nClient\nRF = 3\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-13-638.jpg?cb=1400257145\" title=\"CL QUORUM&#10;• Write: Success when a majority of the replica n...\" target=\"_blank\">\n        13.\n      </a>\n    CL QUORUM\n• Write: Success when a majority of the replica nodes has\nacknowledged the write.\n• Read: A majority of the nodes are given the read request.\n• Majority = ( RF / 2 ) + 1\n©2013 DataStax Confidential. Do not distribute without consent. 13©2014 DataStax Confidential. Do not distribute without consent. 13\nR1\nR2\nR3Coordinator\nClient\nRF = 3\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-14-638.jpg?cb=1400257145\" title=\"CL ALL&#10;• Write: Success when all of the replica nodes has&#10;a...\" target=\"_blank\">\n        14.\n      </a>\n    CL ALL\n• Write: Success when all of the replica nodes has\nacknowledged the write.\n• Read: All replica nodes are given the read request.\n©2013 DataStax Confidential. Do not distribute without consent. 14©2014 DataStax Confidential. Do not distribute without consent. 14\nR1\nR2\nR3Coordinator\nClient\nRF = 3\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-15-638.jpg?cb=1400257145\" title=\"Log-Structured Storage Engine&#10;• Cassandra storage engine in...\" target=\"_blank\">\n        15.\n      </a>\n    Log-Structured Storage Engine\n• Cassandra storage engine inspired by Google BigTable\n• Key to fast write performance on Cassandra\n©2014 DataStax Confidential. Do not distribute without consent. 16\nMemtable\nSSTable SSTable SSTable\nCommit\nLog\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-16-638.jpg?cb=1400257145\" title=\"Updates and Deletes&#10;• SSTable files are immutable and canno...\" target=\"_blank\">\n        16.\n      </a>\n    Updates and Deletes\n• SSTable files are immutable and cannot be changed.\n• Updates are written as new data.\n• Deletes write a tombstone, which mark a row or column(s) as\ndeleted.\n• Updates and deletes are just as fast as inserts.\n©2014 DataStax Confidential. Do not distribute without consent. 17\nSSTable SSTable SSTable\nid:1, first:John,\nlast:Smith\ntimestamp: …405\nid:1, first:John,\nlast:Williams\ntimestamp: …621\nid:1, deleted\ntimestamp: …999\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-17-638.jpg?cb=1400257145\" title=\"Compaction&#10;• Periodically an operation is triggered that wi...\" target=\"_blank\">\n        17.\n      </a>\n    Compaction\n• Periodically an operation is triggered that will merge the data\nin several SSTables into a single SSTable.\n• Helps to limits the number of SSTables to read.\n• Removes old data and tombstones.\n• SSTables are deleted after compaction\n©2014 DataStax Confidential. Do not distribute without consent. 18\nSSTable SSTable SSTable\nid:1, first:John,\nlast:Smith\ntimestamp:405\nid:1, first:John,\nlast:Williams\ntimestamp:621\nid:1, deleted\ntimestamp:999\nNew SSTable\nid:1, deleted\ntimestamp:999\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-18-638.jpg?cb=1400257145\" title=\"Cluster Sizing&#10;©2014 DataStax Confidential. Do not distribu...\" target=\"_blank\">\n        18.\n      </a>\n    Cluster Sizing\n©2014 DataStax Confidential. Do not distribute without consent. 19\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-19-638.jpg?cb=1400257145\" title=\"Cluster Sizing Considerations&#10;• Replication Factor&#10;• Data S...\" target=\"_blank\">\n        19.\n      </a>\n    Cluster Sizing Considerations\n• Replication Factor\n• Data Size\n“How many nodes would I need to store my data set?”\n• Data Velocity (Performance)\n“How many nodes would I need to achieve my desired\nthroughput?”\n©2014 DataStax Confidential. Do not distribute without consent. 20\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-20-638.jpg?cb=1400257145\" title=\"Choosing a Replication Factor&#10;©2014 DataStax Confidential. ...\" target=\"_blank\">\n        20.\n      </a>\n    Choosing a Replication Factor\n©2014 DataStax Confidential. Do not distribute without consent. 21\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-21-638.jpg?cb=1400257145\" title=\"What Are You Using Replication For?&#10;• Durability or Availab...\" target=\"_blank\">\n        21.\n      </a>\n    What Are You Using Replication For?\n• Durability or Availability?\n• Each node has local durability (Commit Log), but replication\ncan be used for distributed durability.\n• For availability, a recommended setting is RF=3.\n• RF=3 is the minimum necessary to achieve both consistency\nand availability using QUORUM and LOCAL_QUORUM.\n©2014 DataStax Confidential. Do not distribute without consent. 22\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-22-638.jpg?cb=1400257145\" title=\"How Replication Can Affect Consistency Level&#10;• When RF &lt; 3,...\" target=\"_blank\">\n        22.\n      </a>\n    How Replication Can Affect Consistency Level\n• When RF &lt; 3, you do not have as much flexibility when\nchoosing consistency and availability.\n• QUORUM = ALL\n©2014 DataStax Confidential. Do not distribute without consent. 23\nR1\nR2\nCoordinator\nClient\nRF = 2\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-23-638.jpg?cb=1400257145\" title=\"Using A Larger Replication Factor&#10;• When RF &gt; 3, there is m...\" target=\"_blank\">\n        23.\n      </a>\n    Using A Larger Replication Factor\n• When RF &gt; 3, there is more data usage and higher latency for\noperations requiring immediate consistency.\n• If using eventual consistency, a consistency level of ONE will\nhave consistent performance regardless of the replication\nfactor.\n• High availability clusters may use a replication factor as high\nas 5.\n©2014 DataStax Confidential. Do not distribute without consent. 24\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-24-638.jpg?cb=1400257145\" title=\"Data Size&#10;©2014 DataStax Confidential. Do not distribute wi...\" target=\"_blank\">\n        24.\n      </a>\n    Data Size\n©2014 DataStax Confidential. Do not distribute without consent. 25\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-25-638.jpg?cb=1400257145\" title=\"Disk Usage Factors&#10;• Data Size&#10;• Replication Setting&#10;• Old ...\" target=\"_blank\">\n        25.\n      </a>\n    Disk Usage Factors\n• Data Size\n• Replication Setting\n• Old Data\n• Compaction\n• Snapshots\n©2014 DataStax Confidential. Do not distribute without consent. 26\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-26-638.jpg?cb=1400257145\" title=\"Data Sizing&#10;• Row and Column Data&#10;• Row and Column Overhead...\" target=\"_blank\">\n        26.\n      </a>\n    Data Sizing\n• Row and Column Data\n• Row and Column Overhead\n• Indices and Other Structures\n©2014 DataStax Confidential. Do not distribute without consent. 27\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-27-638.jpg?cb=1400257145\" title=\"Replication Overhead&#10;• A replication factor &gt; 1 will effect...\" target=\"_blank\">\n        27.\n      </a>\n    Replication Overhead\n• A replication factor &gt; 1 will effectively multiply your data size\nby that amount.\n©2014 DataStax Confidential. Do not distribute without consent. 28\nRF = 1 RF = 2 RF = 3\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-28-638.jpg?cb=1400257145\" title=\"Old Data&#10;• Updates and deletes do not actually overwrite or...\" target=\"_blank\">\n        28.\n      </a>\n    Old Data\n• Updates and deletes do not actually overwrite or delete data.\n• Older versions of data and tombstones remain in the SSTable\nfiles until they are compacted.\n• This becomes more important for heavy update and delete\nworkloads.\n©2014 DataStax Confidential. Do not distribute without consent. 29\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-29-638.jpg?cb=1400257145\" title=\"Compaction&#10;• Compaction needs free disk space to write the ...\" target=\"_blank\">\n        29.\n      </a>\n    Compaction\n• Compaction needs free disk space to write the new\nSSTable, before the SSTables being compacted are removed.\n• Leave enough free disk space on each node to allow\ncompactions to run.\n• Worst case for the Size Tier Compaction Strategy is 50% of\nthe total data capacity of the node.\n• For the Leveled Compaction Strategy, that is about 10% of\nthe total data capacity.\n©2014 DataStax Confidential. Do not distribute without consent. 30\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-30-638.jpg?cb=1400257145\" title=\"Snapshots&#10;• Snapshots are hard-links or copies of SSTable d...\" target=\"_blank\">\n        30.\n      </a>\n    Snapshots\n• Snapshots are hard-links or copies of SSTable data files.\n• After SSTables are compacted, the disk space may not be\nreclaimed if a snapshot of those SSTables were created.\nSnapshots are created when:\n• Executing the nodetool snapshot command\n• Dropping a keyspace or table\n• Incremental backups\n• During compaction\n©2014 DataStax Confidential. Do not distribute without consent. 31\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-31-638.jpg?cb=1400257145\" title=\"Recommended Disk Capacity&#10;• For current Cassandra versions,...\" target=\"_blank\">\n        31.\n      </a>\n    Recommended Disk Capacity\n• For current Cassandra versions, the ideal disk capacity is\napproximate 1TB per node if using spinning disks and 3-5 TB\nper node using SSDs.\n• Having a larger disk capacity may be limited by the resulting\nperformance.\n• What works for you is still dependent on your data model\ndesign and desired data velocity.\n©2014 DataStax Confidential. Do not distribute without consent. 32\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-32-638.jpg?cb=1400257145\" title=\"Data Velocity (Performance)&#10;©2014 DataStax Confidential. Do...\" target=\"_blank\">\n        32.\n      </a>\n    Data Velocity (Performance)\n©2014 DataStax Confidential. Do not distribute without consent. 33\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-33-638.jpg?cb=1400257145\" title=\"How to Measure Performance&#10;• I/O Throughput&#10;“How many reads...\" target=\"_blank\">\n        33.\n      </a>\n    How to Measure Performance\n• I/O Throughput\n“How many reads and writes can be completed per\nsecond?”\n• Read and Write Latency\n“How fast should I be able to get a response for my read and\nwrite requests?”\n©2014 DataStax Confidential. Do not distribute without consent. 34\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-34-638.jpg?cb=1400257145\" title=\"Sizing for Failure&#10;• Cluster must be sized taking into acco...\" target=\"_blank\">\n        34.\n      </a>\n    Sizing for Failure\n• Cluster must be sized taking into account the performance\nimpact caused by failure.\n• When a node fails, the corresponding workload must be\nabsorbed by the other replica nodes in the cluster.\n• Performance is further impacted when recovering a node.\nData must be streamed or repaired using the other replica\nnodes.\n©2014 DataStax Confidential. Do not distribute without consent. 35\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-35-638.jpg?cb=1400257145\" title=\"Hardware Considerations for Performance&#10;CPU&#10;• Operations ar...\" target=\"_blank\">\n        35.\n      </a>\n    Hardware Considerations for Performance\nCPU\n• Operations are often CPU-intensive.\n• More cores are better.\nMemory\n• Cassandra uses JVM heap memory.\n• Additional memory used as off-heap memory by Cassandra,\nor as the OS page cache.\nDisk\n• C* optimized for spinning disks, but SSDs will perform better.\n• Attached storage (SAN) is strongly discouraged.\n©2014 DataStax Confidential. Do not distribute without consent. 36\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-36-638.jpg?cb=1400257145\" title=\"Some Final Words…&#10;©2014 DataStax Confidential. Do not distr...\" target=\"_blank\">\n        36.\n      </a>\n    Some Final Words…\n©2014 DataStax Confidential. Do not distribute without consent. 37\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-37-638.jpg?cb=1400257145\" title=\"Summary&#10;• Cassandra allows flexibility when sizing your clu...\" target=\"_blank\">\n        37.\n      </a>\n    Summary\n• Cassandra allows flexibility when sizing your cluster from a\nsingle node to thousands of nodes\n• Your use case will dictate how you want to size and configure\nyour Cassandra cluster. Do you need availability? Immediate\nconsistency?\n• The minimum number of nodes needed will be determined by\nyour data size, desired performance and replication factor.\n©2014 DataStax Confidential. Do not distribute without consent. 38\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-38-638.jpg?cb=1400257145\" title=\"Additional Resources&#10;• DataStax Documentation&#10;http://www.da...\" target=\"_blank\">\n        38.\n      </a>\n    Additional Resources\n• DataStax Documentation\nhttp://www.datastax.com/documentation/cassandra/2.0/cassandra/architectu\nre/architecturePlanningAbout_c.html\n• Planet Cassandra\nhttp://planetcassandra.org/nosql-cassandra-education/\n• Cassandra Users Mailing List\nuser-subscribe@cassandra.apache.org\nhttp://mail-archives.apache.org/mod_mbox/cassandra-user/\n©2014 DataStax Confidential. Do not distribute without consent. 39\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-39-638.jpg?cb=1400257145\" title=\"Questions?&#10;Questions?&#10;©2014 DataStax Confidential. Do not d...\" target=\"_blank\">\n        39.\n      </a>\n    Questions?\nQuestions?\n©2014 DataStax Confidential. Do not distribute without consent. 40\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-40-638.jpg?cb=1400257145\" title=\"Thank You&#10;We power the big data&#10;apps that transform busines...\" target=\"_blank\">\n        40.\n      </a>\n    Thank You\nWe power the big data\napps that transform business.\n41©2014 DataStax Confidential. Do not distribute without consent.\n \n  </li>\n              </ol></div></div></div><aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\"><dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\"><ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"33775630\" title=\"Grant Writing for Education\" href=\"https://www.linkedin.com/learning/grant-writing-for-education?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Grant Writing for Education\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Grant Writing for Education\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=nuinGPlhMl%2B183in%2FUYnXOH0gCY%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-gUiOv8tSfYX7vfM_eZLSiol4TeyQJlwEwfe2uRDTkFI69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Grant Writing for Education</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"33775630\" title=\"Learning the Basics of Branding\" href=\"https://www.linkedin.com/learning/learning-the-basics-of-branding?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Learning the Basics of Branding\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Learning the Basics of Branding\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=8Kbe7Ed%2FeTQ%2BSU2%2BqS5hcINsN9Y%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-gWySj_9KfYXfocMLYZLSiol8QcS4BmQw3euaoQzbjEY69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Learning the Basics of Branding</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"33775630\" title=\"Office 365: PowerPoint Essential Training\" href=\"https://www.linkedin.com/learning/office-365-powerpoint-essential-training?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Office 365: PowerPoint Essential Training\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Office 365: PowerPoint Essential Training\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=Z%2Bf7UhZxTfFiRRjPnJ65JFvZVMs%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-iXCCj-NKfY3DscMXYZLSiol4Rfy0Hlgc2feavSTniEo69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Office 365: PowerPoint Essential Training</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"66649696\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Maximum Overdrive: Tuning the Spark Cassandra Connector (Russell Spitzer, DataStax) | C* Summit 2016\" href=\"https://www.slideshare.net/DataStax/maximum-overdrive-tuning-the-spark-cassandra-connector-russell-spitzer-datastax-c-summit-2016\">\n    \n    <div class=\"related-content\"><p>Maximum Overdrive: Tuning the Spark Cassandra Connector (Russell Spitzer, Dat...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"77306108\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Forrester CXNYC 2017 - Delivering great real-time cx is a true craft\" href=\"https://www.slideshare.net/planetcassandra/forrester-cxnyc-2017-delivering-great-realtime-cx-is-a-true-craft\">\n    \n    <div class=\"related-content\"><p>Forrester CXNYC 2017 - Delivering great real-time cx is a true craft</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64947458\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Introduction to DataStax Enterprise Graph Database\" href=\"https://www.slideshare.net/planetcassandra/introduction-to-datastax-enterprise-graph-database\">\n    \n    <div class=\"related-content\"><p>Introduction to DataStax Enterprise Graph Database</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64947204\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Introduction to DataStax Enterprise Advanced Replication with Apache Cassandra\" href=\"https://www.slideshare.net/planetcassandra/introduction-to-datastax-enterprise-advanced-replication-with-apache-cassandra\">\n    \n    <div class=\"related-content\"><p>Introduction to DataStax Enterprise Advanced Replication with Apache Cassandra</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64223739\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Cassandra on Docker @ Walmart Labs\" href=\"https://www.slideshare.net/planetcassandra/cassandra-on-docker-walmart-labs\">\n    \n    <div class=\"related-content\"><p>Cassandra on Docker @ Walmart Labs</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64222983\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Cassandra 3.0 Data Modeling\" href=\"https://www.slideshare.net/planetcassandra/cassandra-30-data-modeling\">\n    \n    <div class=\"related-content\"><p>Cassandra 3.0 Data Modeling</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64222866\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Cassandra Adoption on Cisco UCS &amp; Open stack\" href=\"https://www.slideshare.net/planetcassandra/cassandra-adoption-on-cisco-ucs-open-stack\">\n    \n    <div class=\"related-content\"><p>Cassandra Adoption on Cisco UCS &amp; Open stack</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div></div><footer>\n          <div class=\"row\"><div class=\"columns\"><ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div></div>\n          \n          <div class=\"row\"><div class=\"columns\"><p class=\"copyright text-center\">LinkedIn Corporation © 2018</p></div></div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div>\n    <div class=\"row\"><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></div>\n        \n    </div>\n    \n    \n      <noscript>\n    </noscript>",
        "created_at": "2018-06-28T20:22:21+0000",
        "updated_at": "2018-06-28T20:22:28+0000",
        "published_at": null,
        "published_by": [
          "DataStax Academy"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 14,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/201404clustersizing-140421181642-phpapp02-thumbnail-4.jpg?cb=1400257145",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10762"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10761,
        "uid": null,
        "title": "Cassandra Node Lessons Learned: Right-Sizing Cassandra Clusters",
        "url": "https://robinsystems.com/blog/lessons-learned-cassandra/",
        "content": "<p>“So what”, you say, “Why should I care if that cluster had such low hardware utilization? I’m sure mine’s fine.”</p><p>Here’s why you should care: This kind of thing happens all the time in the Cassandra world. At the time of that conversation, I had two other customers struggling with the same challenge, a large chip maker, and a network equipment manufacturer.</p><p>There are many variations of how and why this happens. You just might be unknowingly wasting CapEx and leaving a ton of scalability on the table with your Cassandra clusters.</p><h3><strong>Here’s the reality check:</strong></h3><p>My customers needed to deploy a different number of Cassandra software nodes than the number of physical host servers they had available. But they didn’t have a way to do it that was feasible for them.</p><p>What my customers really needed was the ability to deploy scenarios like 52 Cassandra nodes on 23 host servers, or maybe 17 nodes on 11 servers, or possibly 500 nodes on as few servers as possible to keep their hardware costs down for such a large cluster.</p><p>How does this happen? Because you optimize the number of server hosts and Cassandra nodes separately.</p><ul><li><strong>Host Count</strong>: Provision host server capacity based on the best economics you can get from your IT or Purchasing for gear that meets or exceeds the Cassandra minimum server specifications. Current best practice is to get enough servers to handle the aggregate peak workload or data set size, whichever is the limiter in your use case.</li>\n<li><strong>Cassandra Node Count</strong>: You really need to size your Cassandra software nodes and determine your cluster node count based on testing before you go to production. Sizing Cassandra before you’re in production is a both art and science. It’s hard. You can guestimate through calculations. But sizing calculators are usually not effective. Luckily for you, Cassandra scales linearly. You can test a small cluster and then arrive at your total cluster node count by dividing your aggregate workload or data size by the capacity of each node (from your tests). Don’t forget to factor in replication and multi-DC.</li>\n</ul><p>Chances are slim to none the boxes available to you are exactly the right size for a single Cassandra node. Most of my Cassandra customers had no choice but to use servers that were significantly over the minimum Cassandra server requirements. So they tried to over-size their Cassandra node density to get the most out of their servers. That’s a path proven to be fraught with pain.</p><p>As your workloads and data sizes change over time, the node:host mismatch is exacerbated. But that’s a topic for another day and another blog post…</p><h3><strong>**Here’s how we get to the sizing of 3 Cassandra nodes per box based on current practices with Cassandra 3.x:</strong></h3><ul><li><strong>Storage</strong> – Under a write or read/write workload each Cassandra node does its own back end garbage collecting and flushing that causes a lot of storage IO. Under heavy write workload and tuned well, it will be compacting almost constantly without falling too far behind, if at all. This means the amount of data you can put on a Cassandra node is often IO bound. Think: max of ~1.5TB for HDD and ~4-5TB for SSD. Because there are many variables that factor in how much data you can fit on a Cassandra node while meeting your SLAs, you will want to load test this with your own data model, access patterns with realistic payload sizes, client code, driver setup, hardware stack, etc, before deciding your production data density per node.</li>\n<li><strong>Cores</strong> – A single Cassandra node under max load recruits maybe ~12 cores. That 12 cores is not a hard limit. It’s a ballpark before diminishing returns set in. Yes, I’m generalizing here and not hitting all the permutations. It’s still valid. <a href=\"https://medium.com/@foundev/data-density-destroyer-of-scalability-a14282aeed66\" target=\"_blank\" rel=\"noopener noreferrer\">8 cores is a good sweet spot for a moderate sized yet very capable node</a>.</li>\n<li><strong>Memory</strong> – The Cassandra JVM heap can only be so big before you start to get in GC trouble. Buffer cache will take as much memory as you throw at it, which is good for read-heavy workloads. But the cache benefit might not be there for a write or mixed read/write workload. 42GB is fine for 8 cores and reasonable data density per node.</li>\n</ul>",
        "created_at": "2018-06-28T20:21:44+0000",
        "updated_at": "2018-07-10T18:13:03+0000",
        "published_at": "2017-04-26T00:00:00+0000",
        "published_by": [
          "Rich Reffner, Director Field Operations"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 3,
        "domain_name": "robinsystems.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10761"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 963,
            "label": "akka",
            "slug": "akka"
          }
        ],
        "is_public": false,
        "id": 10744,
        "uid": null,
        "title": "Streaming Analytics with Spark, Kafka, Cassandra and Akka",
        "url": "https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka",
        "content": "Streaming Analytics with Spark, Kafka, Cassandra and Akka\n      \n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            </a><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div>\n    <div class=\"wrapper\"><p>Successfully reported this slideshow.</p><div id=\"slideview-container\" class=\"\"><div class=\"row\"><div id=\"main-panel\" class=\"small-12 large-8 columns\"><div class=\"sectionElements\"><div class=\"playerWrapper\"><div><div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\">Streaming Analytics with Spark, Kafka, Cassandra and Akka<div class=\"stage valign-first-slide\"><div class=\"slide_container\"><section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-1-638.jpg?cb=1446278061\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-1-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-1-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-1-1024.jpg?cb=1446278061\" alt=\"Streaming Analytics with Spark,&#10;Kafka, Cassandra, and Akka&#10;Helena Edelson&#10;VP of Product Engineering @Tuplejump&#10;\" /></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-2-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-2-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-2-1024.jpg?cb=1446278061\" alt=\"• Committer / Contributor: Akka, FiloDB, Spark Cassandra&#10;Connector, Spring Integration&#10;• VP of Product Engineering @Tuplej...\" /></i></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-3-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-3-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-3-1024.jpg?cb=1446278061\" alt=\"Tuplejump&#10;Tuplejump Data Blender combines sophisticated data collection&#10;with machine learning and analytics, to understand...\" /></i></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-4-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-4-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-4-1024.jpg?cb=1446278061\" alt=\"Tuplejump Open Source&#10;github.com/tuplejump&#10;• FiloDB - distributed, versioned, columnar analytical db for modern&#10;streaming ...\" /></i></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-5-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-5-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-5-1024.jpg?cb=1446278061\" alt=\"What Will We Talk About&#10;• The Problem Domain&#10;• Example Use Case&#10;• Rethinking Architecture&#10;– We don't have to look far to l...\" /></i></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-6-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-6-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-6-1024.jpg?cb=1446278061\" alt=\"THE PROBLEM DOMAIN&#10;Delivering Meaning From A Flood Of Data&#10;6&#10;\" /></i></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-7-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-7-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-7-1024.jpg?cb=1446278061\" alt=\"The Problem Domain&#10;Need to build scalable, fault tolerant, distributed data&#10;processing systems that can handle massive amo...\" /></i></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-8-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-8-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-8-1024.jpg?cb=1446278061\" alt=\"Translation&#10;How to build adaptable, elegant systems&#10;for complex analytics and learning tasks&#10;to run as large-scale cluster...\" /></i></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-9-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-9-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-9-1024.jpg?cb=1446278061\" alt=\"How Much Data&#10;Yottabyte = quadrillion gigabytes or septillion&#10;bytes&#10;9&#10;We all have a lot of data&#10;• Terabytes&#10;• Petabytes......\" /></i></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-10-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-10-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-10-1024.jpg?cb=1446278061\" alt=\"Delivering Meaning&#10;• Deliver meaning in sec/sub-sec latency&#10;• Disparate data sources &amp; schemas&#10;• Billions of events per se...\" /></i></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-11-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-11-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-11-1024.jpg?cb=1446278061\" alt=\"While We Monitor, Predict &amp; Proactively Handle&#10;• Massive event spikes&#10;• Bursty traffic&#10;• Fast producers / slow consumers&#10;•...\" /></i></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-12-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-12-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-12-1024.jpg?cb=1446278061\" alt=\"And stay within our&#10;AWS / Rackspace budget&#10;\" /></i></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-13-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-13-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-13-1024.jpg?cb=1446278061\" alt=\"EXAMPLE CASE:&#10;CYBER SECURITY&#10;Hunting The Hunter&#10;13&#10;\" /></i></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-14-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-14-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-14-1024.jpg?cb=1446278061\" alt=\"14&#10;• Track activities of international threat actor groups,&#10;nation-state, criminal or hactivist&#10;• Intrusion attempts&#10;• Act...\" /></i></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-15-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-15-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-15-1024.jpg?cb=1446278061\" alt=\"15&#10;• Machine events&#10;• Endpoint intrusion detection&#10;• Anomalies/indicators of attack or compromise&#10;• Machine learning&#10;• Tra...\" /></i></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-16-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-16-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-16-1024.jpg?cb=1446278061\" alt=\"Data Requirements &amp; Description&#10;• Streaming event data&#10;• Log messages&#10;• User activity records&#10;• System ops &amp; metrics data&#10;...\" /></i></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-17-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-17-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-17-1024.jpg?cb=1446278061\" alt=\"Massive Amounts Of Data&#10;17&#10;• One machine can generate 2+ TB per day&#10;• Tracking millions of devices&#10;• 1 million writes per ...\" /></i></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-18-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-18-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-18-1024.jpg?cb=1446278061\" alt=\"RETHINKING&#10;ARCHITECTURE&#10;18&#10;\" /></i></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-19-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-19-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-19-1024.jpg?cb=1446278061\" alt=\"WE DON'T HAVE TO LOOK&#10;FAR TO LOOK BACK&#10;19&#10;Rethinking Architecture&#10;\" /></i></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-20-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-20-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-20-1024.jpg?cb=1446278061\" alt=\"20&#10;Most batch analytics flow from&#10;several years ago looked like...&#10;\" /></i></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-21-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-21-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-21-1024.jpg?cb=1446278061\" alt=\"STREAMING &amp; DATA SCIENCE&#10;21&#10;Rethinking Architecture&#10;\" /></i></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-22-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-22-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-22-1024.jpg?cb=1446278061\" alt=\"Streaming&#10;I need fast access to historical data on the fly for&#10;predictive modeling with real time data from the stream.&#10;22&#10;\" /></i></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-23-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-23-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-23-1024.jpg?cb=1446278061\" alt=\"Not A Stream, A Flood&#10;• Data emitters&#10;• Netflix: 1 - 2 million events per second at peak&#10;• 750 billion events per day&#10;• Li...\" /></i></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-24-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-24-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-24-1024.jpg?cb=1446278061\" alt=\"Which Translates To&#10;• Do it fast&#10;• Do it cheap&#10;• Do it at scale&#10;24&#10;\" /></i></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-25-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-25-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-25-1024.jpg?cb=1446278061\" alt=\"Challenges&#10;• Code changes at runtime&#10;• Distributed Data Consistency&#10;• Ordering guarantees&#10;• Complex compute algorithms&#10;25&#10;\" /></i></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-26-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-26-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-26-1024.jpg?cb=1446278061\" alt=\"Oh, and don't lose data&#10;26&#10;\" /></i></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-27-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-27-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-27-1024.jpg?cb=1446278061\" alt=\"Strategies&#10;• Partition For Scale &amp; Data Locality&#10;• Replicate For Resiliency&#10;• Share Nothing&#10;• Fault Tolerance&#10;• Asynchrony...\" /></i></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-28-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-28-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-28-1024.jpg?cb=1446278061\" alt=\"AND THEN WE GREEKED OUT&#10;28&#10;Rethinking Architecture&#10;\" /></i></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-29-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-29-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-29-1024.jpg?cb=1446278061\" alt=\"Lambda Architecture&#10;A data-processing architecture designed to handle massive&#10;quantities of data by taking advantage of bo...\" /></i></section><section data-index=\"30\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-30-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-30-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-30-1024.jpg?cb=1446278061\" alt=\"Lambda Architecture&#10;A data-processing architecture designed to handle massive&#10;quantities of data by taking advantage of bo...\" /></i></section><section data-index=\"31\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-31-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-31-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-31-1024.jpg?cb=1446278061\" alt=\"31&#10;https://www.mapr.com/developercentral/lambda-architecture&#10;\" /></i></section><section data-index=\"32\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-32-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-32-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-32-1024.jpg?cb=1446278061\" alt=\"Implementing Is Hard&#10;33&#10;• Real-time pipeline backed by KV store for updates&#10;• Many moving parts - KV store, real time, bat...\" /></i></section><section data-index=\"33\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-33-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-33-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-33-1024.jpg?cb=1446278061\" alt=\"Performance Tuning &amp; Monitoring:&#10;on so many systems&#10;34&#10;Also hard&#10;\" /></i></section><section data-index=\"34\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-34-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-34-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-34-1024.jpg?cb=1446278061\" alt=\"Lambda Architecture&#10;An immutable sequence of records is captured and fed&#10;into a batch system and a stream processing&#10;syste...\" /></i></section><section data-index=\"35\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-35-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-35-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-35-1024.jpg?cb=1446278061\" alt=\"WAIT, DUAL SYSTEMS?&#10;36&#10;Challenge Assumptions&#10;\" /></i></section><section data-index=\"36\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-36-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-36-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-36-1024.jpg?cb=1446278061\" alt=\"Which Translates To&#10;• Performing analytical computations &amp; queries in dual&#10;systems&#10;• Implementing transformation logic twi...\" /></i></section><section data-index=\"37\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-37-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-37-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-37-1024.jpg?cb=1446278061\" alt=\"Why Dual Systems?&#10;• Why is a separate batch system needed?&#10;• Why support code, machines and running services of&#10;two analyt...\" /></i></section><section data-index=\"38\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-38-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-38-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-38-1024.jpg?cb=1446278061\" alt=\"YES&#10;39&#10;• A unified system for streaming and batch&#10;• Real-time processing and reprocessing&#10;• Code changes&#10;• Fault tolerance...\" /></i></section><section data-index=\"39\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-39-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-39-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-39-1024.jpg?cb=1446278061\" alt=\"ANOTHER ASSUMPTION:&#10;ETL&#10;40&#10;Challenge Assumptions&#10;\" /></i></section><section data-index=\"40\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-40-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-40-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-40-1024.jpg?cb=1446278061\" alt=\"Extract, Transform, Load (ETL)&#10;41&#10;&quot;Designing and maintaining the ETL process is often&#10;considered one of the most difficult...\" /></i></section><section data-index=\"41\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-41-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-41-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-41-1024.jpg?cb=1446278061\" alt=\"Extract, Transform, Load (ETL)&#10;42&#10;ETL involves&#10;• Extraction of data from one system into another&#10;• Transforming it&#10;• Loadi...\" /></i></section><section data-index=\"42\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-42-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-42-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-42-1024.jpg?cb=1446278061\" alt=\"Extract, Transform, Load (ETL)&#10;&quot;Designing and maintaining the ETL process is often&#10;considered one of the most difficult an...\" /></i></section><section data-index=\"43\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-43-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-43-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-43-1024.jpg?cb=1446278061\" alt=\"ETL&#10;44&#10;• Each ETL step can introduce errors and risk&#10;• Can duplicate data after failover&#10;• Tools can cost millions of doll...\" /></i></section><section data-index=\"44\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-44-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-44-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-44-1024.jpg?cb=1446278061\" alt=\"ETL&#10;• Writing intermediary files&#10;• Parsing and re-parsing plain text&#10;45&#10;\" /></i></section><section data-index=\"45\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-45-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-45-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-45-1024.jpg?cb=1446278061\" alt=\"And let's duplicate the pattern&#10;over all our DataCenters&#10;46&#10;\" /></i></section><section data-index=\"46\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-46-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-46-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-46-1024.jpg?cb=1446278061\" alt=\"47&#10;These are not the solutions you're looking for&#10;\" /></i></section><section data-index=\"47\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-47-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-47-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-47-1024.jpg?cb=1446278061\" alt=\"REVISITING THE GOAL&#10;&amp; THE STACK&#10;48&#10;\" /></i></section><section data-index=\"48\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-48-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-48-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-48-1024.jpg?cb=1446278061\" alt=\"Removing The 'E' in ETL&#10;Thanks to technologies like Avro and Protobuf we don’t need the&#10;“E” in ETL. Instead of text dumps ...\" /></i></section><section data-index=\"49\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-49-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-49-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-49-1024.jpg?cb=1446278061\" alt=\"Removing The 'L' in ETL&#10;If data collection is backed by a distributed messaging&#10;system (e.g. Kafka) you can do real-time f...\" /></i></section><section data-index=\"50\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-50-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-50-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-50-1024.jpg?cb=1446278061\" alt=\"#NoMoreGreekLetterArchitectures&#10;51&#10;\" /></i></section><section data-index=\"51\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-51-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-51-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-51-1024.jpg?cb=1446278061\" alt=\"NoETL&#10;52&#10;\" /></i></section><section data-index=\"52\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-52-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-52-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-52-1024.jpg?cb=1446278061\" alt=\"Strategy Technologies&#10;Scalable Infrastructure / Elastic Spark, Cassandra, Kafka&#10;Partition For Scale, Network Topology Awar...\" /></i></section><section data-index=\"53\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-53-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-53-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-53-1024.jpg?cb=1446278061\" alt=\"SMACK&#10;• Scala/Spark&#10;• Mesos&#10;• Akka&#10;• Cassandra&#10;• Kafka&#10;54&#10;\" /></i></section><section data-index=\"54\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-54-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-54-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-54-1024.jpg?cb=1446278061\" alt=\"Spark Streaming&#10;55&#10;\" /></i></section><section data-index=\"55\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-55-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-55-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-55-1024.jpg?cb=1446278061\" alt=\"Spark Streaming&#10;• One runtime for streaming and batch processing&#10;• Join streaming and static data sets&#10;• No code duplicati...\" /></i></section><section data-index=\"56\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-56-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-56-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-56-1024.jpg?cb=1446278061\" alt=\"How do I merge historical data with data&#10;in the stream?&#10;57&#10;\" /></i></section><section data-index=\"57\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-57-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-57-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-57-1024.jpg?cb=1446278061\" alt=\"Join Streams With Static Data&#10;val ssc = new StreamingContext(conf, Milliseconds(500))&#10;ssc.checkpoint(&quot;checkpoint&quot;)&#10;val sta...\" /></i></section><section data-index=\"58\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-58-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-58-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-58-1024.jpg?cb=1446278061\" alt=\"Training&#10;Data&#10;Feature&#10;Extraction&#10;Model&#10;Training&#10;Model&#10;Testing&#10;Test Data&#10;Your Data Extract Data To Analyze&#10;Train your model...\" /></i></section><section data-index=\"59\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-59-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-59-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-59-1024.jpg?cb=1446278061\" alt=\"Spark Streaming &amp; ML&#10;60&#10;val context = new StreamingContext(conf, Milliseconds(500))&#10;val model = KMeans.train(dataset, ...)...\" /></i></section><section data-index=\"60\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-60-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-60-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-60-1024.jpg?cb=1446278061\" alt=\"Apache Mesos&#10;Open-source cluster manager developed at UC Berkeley.&#10;Abstracts CPU, memory, storage, and other compute resou...\" /></i></section><section data-index=\"61\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-61-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-61-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-61-1024.jpg?cb=1446278061\" alt=\"Akka&#10;High performance concurrency framework for Scala and&#10;Java&#10;• Fault Tolerance&#10;• Asynchronous messaging and data process...\" /></i></section><section data-index=\"62\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-62-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-62-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-62-1024.jpg?cb=1446278061\" alt=\"Akka Actors&#10;A distribution and concurrency abstraction&#10;• Compute Isolation&#10;• Behavioral Context Switching&#10;• No Exposed Int...\" /></i></section><section data-index=\"63\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-63-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-63-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-63-1024.jpg?cb=1446278061\" alt=\"64&#10;Akka Actor Hierarchy&#10;http://www.slideshare.net/jboner/building-reactive-applications-with-akka-in-scala&#10;\" /></i></section><section data-index=\"64\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-64-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-64-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-64-1024.jpg?cb=1446278061\" alt=\"import akka.actor._&#10;class NodeGuardianActor(args...) extends Actor with SupervisorStrategy {&#10;val temperature = context.act...\" /></i></section><section data-index=\"65\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-65-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-65-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-65-1024.jpg?cb=1446278061\" alt=\"Apache Cassandra&#10;• Extremely Fast&#10;• Extremely Scalable&#10;• Multi-Region / Multi-Datacenter&#10;• Always On&#10;• No single point of ...\" /></i></section><section data-index=\"66\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-66-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-66-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-66-1024.jpg?cb=1446278061\" alt=\"Apache Cassandra&#10;• Very flexible data modeling (collections, user defined&#10;types) and changeable over time&#10;• Perfect for in...\" /></i></section><section data-index=\"67\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-67-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-67-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-67-1024.jpg?cb=1446278061\" alt=\"Spark Cassandra Connector&#10;• NOSQL JOINS!&#10;• Write &amp; Read data between Spark and Cassandra&#10;• Compatible with Spark 1.4&#10;• Han...\" /></i></section><section data-index=\"68\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-68-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-68-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-68-1024.jpg?cb=1446278061\" alt=\"KillrWeather&#10;69&#10;http://github.com/killrweather/killrweather&#10;A reference application showing how to easily integrate stream...\" /></i></section><section data-index=\"69\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-69-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-69-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-69-1024.jpg?cb=1446278061\" alt=\"70&#10;• High Throughput Distributed Messaging&#10;• Decouples Data Pipelines&#10;• Handles Massive Data Load&#10;• Support Massive Number...\" /></i></section><section data-index=\"70\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-70-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-70-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-70-1024.jpg?cb=1446278061\" alt=\"Spark Streaming &amp; Kafka&#10;val context = new StreamingContext(conf, Seconds(1))&#10;val wordCount = KafkaUtils.createStream(conte...\" /></i></section><section data-index=\"71\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-71-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-71-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-71-1024.jpg?cb=1446278061\" alt=\"72&#10;class KafkaStreamingActor(params: Map[String, String], ssc: StreamingContext)&#10;extends AggregationActor(settings: Settin...\" /></i></section><section data-index=\"72\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-72-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-72-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-72-1024.jpg?cb=1446278061\" alt=\"73&#10;/** For a given weather station, calculates annual cumulative precip - or year to date. */ &#10;class PrecipitationActor(ss...\" /></i></section><section data-index=\"73\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-73-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-73-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-73-1024.jpg?cb=1446278061\" alt=\"A New Approach&#10;• One Runtime: streaming, scheduled&#10;• Simplified architecture&#10;• Allows us to&#10;• Write different types of app...\" /></i></section><section data-index=\"74\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-74-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-74-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-74-1024.jpg?cb=1446278061\" alt=\"Need daily analytics aggregate reports? Do it in the stream, save&#10;results in Cassandra for easy reporting as needed - with...\" /></i></section><section data-index=\"75\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-75-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-75-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-75-1024.jpg?cb=1446278061\" alt=\"FiloDB&#10;Distributed, columnar database designed to run very fast&#10;analytical queries&#10;• Ingest streaming data from many strea...\" /></i></section><section data-index=\"76\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-76-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-76-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-76-1024.jpg?cb=1446278061\" alt=\"FiloDB&#10;• Breakthrough performance levels for analytical queries&#10;• Performance comparable to Parquet&#10;• One to two orders of...\" /></i></section><section data-index=\"77\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-77-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-77-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-77-1024.jpg?cb=1446278061\" alt=\"WRAPPING UP&#10;78&#10;\" /></i></section><section data-index=\"78\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-78-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-78-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-78-1024.jpg?cb=1446278061\" alt=\"Architectyr?&#10;79&#10;&quot;This is a giant mess&quot;&#10;- Going Real-time - Data Collection and Stream Processing with Apache Kafka, Jay Kr...\" /></i></section><section data-index=\"79\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-79-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-79-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-79-1024.jpg?cb=1446278061\" alt=\"80&#10;Simplified&#10;\" /></i></section><section data-index=\"80\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-80-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-80-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-80-1024.jpg?cb=1446278061\" alt=\"81&#10;\" /></i></section><section data-index=\"81\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-81-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-81-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-81-1024.jpg?cb=1446278061\" alt=\"82&#10;www.tuplejump.com&#10;info@tuplejump.com@tuplejump&#10;\" /></i></section><section data-index=\"82\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-82-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-82-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-82-1024.jpg?cb=1446278061\" alt=\"83&#10;@helenaedelson&#10;github.com/helena&#10;slideshare.net/helenaedelson&#10;THANK YOU!&#10;\" /></i></section><section data-index=\"83\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-83-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-83-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-83-1024.jpg?cb=1446278061\" alt=\"I'm speaking at QCon SF on the broader&#10;topic of Streaming at Scale&#10;http://qconsf.com/sf2015/track/streaming-data-scale&#10;84&#10;\" /></i></section><section data-index=\"84\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-84-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-84-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-84-1024.jpg?cb=1446278061\" alt=\"Streaming Analytics with Spark, Kafka, Cassandra and Akka\" /></i></section><div class=\"j-next-container next-container\"><div class=\"content-container\"><div class=\"next-slideshow-wrapper\"><div class=\"j-next-slideshow next-slideshow\"><p>Upcoming SlideShare</p></div><p>Loading in …5</p><p>×</p></div></div></div></div></div></div></div></div></div><div class=\"slideshow-info-container\" itemscope=\"itemscope\" itemtype=\"https://schema.org/MediaObject\"><div class=\"slideshow-tabs-container show-for-medium-up\"><ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    3 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\">\n                \n                  80 Likes\n                \n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\">\n                Statistics\n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\">\n                Notes\n              </i></a>\n            </li>\n          </ul><div class=\"tabs-content\"><div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\"><ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"itemscope\"><li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"ffch1996\" rel=\"nofollow\" href=\"https://www.slideshare.net/ffch1996?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            ffch1996\n                            \n                              \n                                \n                                \n                              \n                              \n                                \n                                \n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"Jasonmao5\" rel=\"nofollow\" href=\"https://www.slideshare.net/Jasonmao5?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Jasonmao5\n                            \n                              \n                                \n                                \n                              \n                              \n                                \n                                \n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"RaphalBacconnier\" rel=\"nofollow\" href=\"https://www.slideshare.net/RaphalBacconnier?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Raphaël Bacconnier\n                            \n                              \n                                , \n                                Backend Developer\n                              \n                              \n                                 at \n                                Backend / big data developer\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"JhanakParajuli\" rel=\"nofollow\" href=\"https://www.slideshare.net/JhanakParajuli?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Jhanak Parajuli\n                            \n                              \n                                , \n                                Data Scientist and AI Engineer at msg\n                              \n                              \n                                 at \n                                Data Scientist and AI Engineer\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"rajarp9\" rel=\"nofollow\" href=\"https://www.slideshare.net/rajarp9?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Raja Rp\n                            \n                              \n                                \n                                \n                              \n                              \n                                 at \n                                Infosys\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n              </ul><div class=\"more-container text-center\"><a href=\"#\" class=\"j-more-favs\">\n                    Show More\n                    \n                  </a></div></div><div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No Downloads</p></div><div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No notes for slide</p></div></div></div><div class=\"notranslate transcript add-padding-right j-transcript\"><ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    Streaming Analytics with Spark,\nKafka, Cassandra, and Akka\nHelena Edelson\nVP of Product Engineering @Tuplejump\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-2-638.jpg?cb=1446278061\" title=\"• Committer / Contributor: Akka, FiloDB, Spark Cassandra&#10;Co...\" target=\"_blank\">\n        2.\n      </a>\n    • Committer / Contributor: Akka, FiloDB, Spark Cassandra\nConnector, Spring Integration\n• VP of Product Engineering @Tuplejump\n• Previously: Sr Cloud Engineer / Architect at VMware,\nCrowdStrike, DataStax and SpringSource\nWho\n@helenaedelson\ngithub.com/helena\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-3-638.jpg?cb=1446278061\" title=\"Tuplejump&#10;Tuplejump Data Blender combines sophisticated dat...\" target=\"_blank\">\n        3.\n      </a>\n    Tuplejump\nTuplejump Data Blender combines sophisticated data collection\nwith machine learning and analytics, to understand the intention of\nthe analyst, without disrupting workﬂow.\n• Ingest streaming and static data from disparate data sources\n• Combine them into a uniﬁed, holistic view \n• Easily enable fast, ﬂexible and advanced data analysis\n3\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-4-638.jpg?cb=1446278061\" title=\"Tuplejump Open Source&#10;github.com/tuplejump&#10;• FiloDB - distr...\" target=\"_blank\">\n        4.\n      </a>\n    Tuplejump Open Source\ngithub.com/tuplejump\n• FiloDB - distributed, versioned, columnar analytical db for modern\nstreaming workloads\n• Calliope - the first Spark-Cassandra integration\n• Stargate - Lucene indexer for Cassandra\n• SnackFS - HDFS-compatible file system for Cassandra\n4\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-5-638.jpg?cb=1446278061\" title=\"What Will We Talk About&#10;• The Problem Domain&#10;• Example Use ...\" target=\"_blank\">\n        5.\n      </a>\n    What Will We Talk About\n• The Problem Domain\n• Example Use Case\n• Rethinking Architecture\n– We don't have to look far to look back\n– Streaming\n– Revisiting the goal and the stack\n– Simplification\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-6-638.jpg?cb=1446278061\" title=\"THE PROBLEM DOMAIN&#10;Delivering Meaning From A Flood Of Data&#10;6&#10;\" target=\"_blank\">\n        6.\n      </a>\n    THE PROBLEM DOMAIN\nDelivering Meaning From A Flood Of Data\n6\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-7-638.jpg?cb=1446278061\" title=\"The Problem Domain&#10;Need to build scalable, fault tolerant, ...\" target=\"_blank\">\n        7.\n      </a>\n    The Problem Domain\nNeed to build scalable, fault tolerant, distributed data\nprocessing systems that can handle massive amounts of\ndata from disparate sources, with different data structures.\n7\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-8-638.jpg?cb=1446278061\" title=\"Translation&#10;How to build adaptable, elegant systems&#10;for com...\" target=\"_blank\">\n        8.\n      </a>\n    Translation\nHow to build adaptable, elegant systems\nfor complex analytics and learning tasks\nto run as large-scale clustered dataflows\n8\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-9-638.jpg?cb=1446278061\" title=\"How Much Data&#10;Yottabyte = quadrillion gigabytes or septilli...\" target=\"_blank\">\n        9.\n      </a>\n    How Much Data\nYottabyte = quadrillion gigabytes or septillion\nbytes\n9\nWe all have a lot of data\n• Terabytes\n• Petabytes...\nhttps://en.wikipedia.org/wiki/Yottabyte\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-10-638.jpg?cb=1446278061\" title=\"Delivering Meaning&#10;• Deliver meaning in sec/sub-sec latency...\" target=\"_blank\">\n        10.\n      </a>\n    Delivering Meaning\n• Deliver meaning in sec/sub-sec latency\n• Disparate data sources &amp; schemas\n• Billions of events per second\n• High-latency batch processing\n• Low-latency stream processing\n• Aggregation of historical from the stream\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-11-638.jpg?cb=1446278061\" title=\"While We Monitor, Predict &amp; Proactively Handle&#10;• Massive ev...\" target=\"_blank\">\n        11.\n      </a>\n    While We Monitor, Predict &amp; Proactively Handle\n• Massive event spikes\n• Bursty traffic\n• Fast producers / slow consumers\n• Network partitioning &amp; Out of sync systems\n• DC down\n• Wait, we've DDOS'd ourselves from fast streams?\n• Autoscale issues\n– When we scale down VMs how do we not lose data?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-12-638.jpg?cb=1446278061\" title=\"And stay within our&#10;AWS / Rackspace budget&#10;\" target=\"_blank\">\n        12.\n      </a>\n    And stay within our\nAWS / Rackspace budget\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-13-638.jpg?cb=1446278061\" title=\"EXAMPLE CASE:&#10;CYBER SECURITY&#10;Hunting The Hunter&#10;13&#10;\" target=\"_blank\">\n        13.\n      </a>\n    EXAMPLE CASE:\nCYBER SECURITY\nHunting The Hunter\n13\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-14-638.jpg?cb=1446278061\" title=\"14&#10;• Track activities of international threat actor groups,...\" target=\"_blank\">\n        14.\n      </a>\n    14\n• Track activities of international threat actor groups,\nnation-state, criminal or hactivist\n• Intrusion attempts\n• Actual breaches\n• Profile adversary activity\n• Analysis to understand their motives, anticipate actions\nand prevent damage\nAdversary Profiling &amp; Hunting\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-15-638.jpg?cb=1446278061\" title=\"15&#10;• Machine events&#10;• Endpoint intrusion detection&#10;• Anomal...\" target=\"_blank\">\n        15.\n      </a>\n    15\n• Machine events\n• Endpoint intrusion detection\n• Anomalies/indicators of attack or compromise\n• Machine learning\n• Training models based on patterns from historical data\n• Predict potential threats\n• profiling for adversary Identification\n•\nStream Processing\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-16-638.jpg?cb=1446278061\" title=\"Data Requirements &amp; Description&#10;• Streaming event data&#10;• Lo...\" target=\"_blank\">\n        16.\n      </a>\n    Data Requirements &amp; Description\n• Streaming event data\n• Log messages\n• User activity records\n• System ops &amp; metrics data\n• Disparate data sources\n• Wildly differing data structures\n16\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-17-638.jpg?cb=1446278061\" title=\"Massive Amounts Of Data&#10;17&#10;• One machine can generate 2+ TB...\" target=\"_blank\">\n        17.\n      </a>\n    Massive Amounts Of Data\n17\n• One machine can generate 2+ TB per day\n• Tracking millions of devices\n• 1 million writes per second - bursty\n• High % writes, lower % reads\n• TTL\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-18-638.jpg?cb=1446278061\" title=\"RETHINKING&#10;ARCHITECTURE&#10;18&#10;\" target=\"_blank\">\n        18.\n      </a>\n    RETHINKING\nARCHITECTURE\n18\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-19-638.jpg?cb=1446278061\" title=\"WE DON'T HAVE TO LOOK&#10;FAR TO LOOK BACK&#10;19&#10;Rethinking Archit...\" target=\"_blank\">\n        19.\n      </a>\n    WE DON'T HAVE TO LOOK\nFAR TO LOOK BACK\n19\nRethinking Architecture\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-20-638.jpg?cb=1446278061\" title=\"20&#10;Most batch analytics flow from&#10;several years ago looked ...\" target=\"_blank\">\n        20.\n      </a>\n    20\nMost batch analytics flow from\nseveral years ago looked like...\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-21-638.jpg?cb=1446278061\" title=\"STREAMING &amp; DATA SCIENCE&#10;21&#10;Rethinking Architecture&#10;\" target=\"_blank\">\n        21.\n      </a>\n    STREAMING &amp; DATA SCIENCE\n21\nRethinking Architecture\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-22-638.jpg?cb=1446278061\" title=\"Streaming&#10;I need fast access to historical data on the fly ...\" target=\"_blank\">\n        22.\n      </a>\n    Streaming\nI need fast access to historical data on the fly for\npredictive modeling with real time data from the stream.\n22\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-23-638.jpg?cb=1446278061\" title=\"Not A Stream, A Flood&#10;• Data emitters&#10;• Netflix: 1 - 2 mill...\" target=\"_blank\">\n        23.\n      </a>\n    Not A Stream, A Flood\n• Data emitters\n• Netflix: 1 - 2 million events per second at peak\n• 750 billion events per day\n• LinkedIn: &gt; 500 billion events per day\n• Data ingesters\n• Netflix: 50 - 100 billion events per day\n• LinkedIn: 2.5 trillion events per day\n• 1 Petabyte of streaming data\n23\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-24-638.jpg?cb=1446278061\" title=\"Which Translates To&#10;• Do it fast&#10;• Do it cheap&#10;• Do it at s...\" target=\"_blank\">\n        24.\n      </a>\n    Which Translates To\n• Do it fast\n• Do it cheap\n• Do it at scale\n24\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-25-638.jpg?cb=1446278061\" title=\"Challenges&#10;• Code changes at runtime&#10;• Distributed Data Con...\" target=\"_blank\">\n        25.\n      </a>\n    Challenges\n• Code changes at runtime\n• Distributed Data Consistency\n• Ordering guarantees\n• Complex compute algorithms\n25\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-26-638.jpg?cb=1446278061\" title=\"Oh, and don't lose data&#10;26&#10;\" target=\"_blank\">\n        26.\n      </a>\n    Oh, and don't lose data\n26\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-27-638.jpg?cb=1446278061\" title=\"Strategies&#10;• Partition For Scale &amp; Data Locality&#10;• Replicat...\" target=\"_blank\">\n        27.\n      </a>\n    Strategies\n• Partition For Scale &amp; Data Locality\n• Replicate For Resiliency\n• Share Nothing\n• Fault Tolerance\n• Asynchrony\n• Async Message Passing\n• Memory Management\n27\n• Data lineage and reprocessing in\nruntime\n• Parallelism\n• Elastically Scale\n• Isolation\n• Location Transparency\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-28-638.jpg?cb=1446278061\" title=\"AND THEN WE GREEKED OUT&#10;28&#10;Rethinking Architecture&#10;\" target=\"_blank\">\n        28.\n      </a>\n    AND THEN WE GREEKED OUT\n28\nRethinking Architecture\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-29-638.jpg?cb=1446278061\" title=\"Lambda Architecture&#10;A data-processing architecture designed...\" target=\"_blank\">\n        29.\n      </a>\n    Lambda Architecture\nA data-processing architecture designed to handle massive\nquantities of data by taking advantage of both batch and\nstream processing methods.\n29\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-30-638.jpg?cb=1446278061\" title=\"Lambda Architecture&#10;A data-processing architecture designed...\" target=\"_blank\">\n        30.\n      </a>\n    Lambda Architecture\nA data-processing architecture designed to handle massive\nquantities of data by taking advantage of both batch and\nstream processing methods.\n• An approach\n• Coined by Nathan Marz\n• This was a huge stride forward\n30\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-31-638.jpg?cb=1446278061\" title=\"31&#10;https://www.mapr.com/developercentral/lambda-architecture&#10;\" target=\"_blank\">\n        31.\n      </a>\n    31\nhttps://www.mapr.com/developercentral/lambda-architecture\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-32-638.jpg?cb=1446278061\" title=\"Implementing Is Hard&#10;33&#10;• Real-time pipeline backed by KV s...\" target=\"_blank\">\n        32.\n      </a>\n    Implementing Is Hard\n33\n• Real-time pipeline backed by KV store for updates\n• Many moving parts - KV store, real time, batch\n• Running similar code in two places\n• Still ingesting data to Parquet/HDFS\n• Reconcile queries against two different places\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-33-638.jpg?cb=1446278061\" title=\"Performance Tuning &amp; Monitoring:&#10;on so many systems&#10;34&#10;Also...\" target=\"_blank\">\n        33.\n      </a>\n    Performance Tuning &amp; Monitoring:\non so many systems\n34\nAlso hard\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-34-638.jpg?cb=1446278061\" title=\"Lambda Architecture&#10;An immutable sequence of records is cap...\" target=\"_blank\">\n        34.\n      </a>\n    Lambda Architecture\nAn immutable sequence of records is captured and fed\ninto a batch system and a stream processing\nsystem in parallel.\n35\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-35-638.jpg?cb=1446278061\" title=\"WAIT, DUAL SYSTEMS?&#10;36&#10;Challenge Assumptions&#10;\" target=\"_blank\">\n        35.\n      </a>\n    WAIT, DUAL SYSTEMS?\n36\nChallenge Assumptions\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-36-638.jpg?cb=1446278061\" title=\"Which Translates To&#10;• Performing analytical computations &amp; ...\" target=\"_blank\">\n        36.\n      </a>\n    Which Translates To\n• Performing analytical computations &amp; queries in dual\nsystems\n• Implementing transformation logic twice\n• Duplicate Code\n• Spaghetti Architecture for Data Flows\n• One Busy Network\n37\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-37-638.jpg?cb=1446278061\" title=\"Why Dual Systems?&#10;• Why is a separate batch system needed?&#10;...\" target=\"_blank\">\n        37.\n      </a>\n    Why Dual Systems?\n• Why is a separate batch system needed?\n• Why support code, machines and running services of\ntwo analytics systems?\n38\nCounter productive on some level?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-38-638.jpg?cb=1446278061\" title=\"YES&#10;39&#10;• A unified system for streaming and batch&#10;• Real-ti...\" target=\"_blank\">\n        38.\n      </a>\n    YES\n39\n• A unified system for streaming and batch\n• Real-time processing and reprocessing\n• Code changes\n• Fault tolerance\nhttp://radar.oreilly.com/2014/07/questioning-the-lambda-\narchitecture.html - Jay Kreps\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-39-638.jpg?cb=1446278061\" title=\"ANOTHER ASSUMPTION:&#10;ETL&#10;40&#10;Challenge Assumptions&#10;\" target=\"_blank\">\n        39.\n      </a>\n    ANOTHER ASSUMPTION:\nETL\n40\nChallenge Assumptions\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-40-638.jpg?cb=1446278061\" title=\"Extract, Transform, Load (ETL)&#10;41&#10;&quot;Designing and maintainin...\" target=\"_blank\">\n        40.\n      </a>\n    Extract, Transform, Load (ETL)\n41\n\"Designing and maintaining the ETL process is often\nconsidered one of the most difficult and resource-\nintensive portions of a data warehouse project.\"\nhttp://docs.oracle.com/cd/B19306_01/server.102/b14223/ettover.htm\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-41-638.jpg?cb=1446278061\" title=\"Extract, Transform, Load (ETL)&#10;42&#10;ETL involves&#10;• Extraction...\" target=\"_blank\">\n        41.\n      </a>\n    Extract, Transform, Load (ETL)\n42\nETL involves\n• Extraction of data from one system into another\n• Transforming it\n• Loading it into another system\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-42-638.jpg?cb=1446278061\" title=\"Extract, Transform, Load (ETL)&#10;&quot;Designing and maintaining t...\" target=\"_blank\">\n        42.\n      </a>\n    Extract, Transform, Load (ETL)\n\"Designing and maintaining the ETL process is often\nconsidered one of the most difficult and resource-\nintensive portions of a data warehouse project.\"\nhttp://docs.oracle.com/cd/B19306_01/server.102/b14223/ettover.htm\n43\nAlso unnecessarily redundant and often typeless\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-43-638.jpg?cb=1446278061\" title=\"ETL&#10;44&#10;• Each ETL step can introduce errors and risk&#10;• Can ...\" target=\"_blank\">\n        43.\n      </a>\n    ETL\n44\n• Each ETL step can introduce errors and risk\n• Can duplicate data after failover\n• Tools can cost millions of dollars\n• Decreases throughput\n• Increased complexity\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-44-638.jpg?cb=1446278061\" title=\"ETL&#10;• Writing intermediary files&#10;• Parsing and re-parsing p...\" target=\"_blank\">\n        44.\n      </a>\n    ETL\n• Writing intermediary files\n• Parsing and re-parsing plain text\n45\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-45-638.jpg?cb=1446278061\" title=\"And let's duplicate the pattern&#10;over all our DataCenters&#10;46&#10;\" target=\"_blank\">\n        45.\n      </a>\n    And let's duplicate the pattern\nover all our DataCenters\n46\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-46-638.jpg?cb=1446278061\" title=\"47&#10;These are not the solutions you're looking for&#10;\" target=\"_blank\">\n        46.\n      </a>\n    47\nThese are not the solutions you're looking for\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-47-638.jpg?cb=1446278061\" title=\"REVISITING THE GOAL&#10;&amp; THE STACK&#10;48&#10;\" target=\"_blank\">\n        47.\n      </a>\n    REVISITING THE GOAL\n&amp; THE STACK\n48\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-48-638.jpg?cb=1446278061\" title=\"Removing The 'E' in ETL&#10;Thanks to technologies like Avro an...\" target=\"_blank\">\n        48.\n      </a>\n    Removing The 'E' in ETL\nThanks to technologies like Avro and Protobuf we don’t need the\n“E” in ETL. Instead of text dumps that you need to parse over\nmultiple systems:\nScala &amp; Avro (e.g.)\n• Can work with binary data that remains strongly typed\n• A return to strong typing in the big data ecosystem\n49\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-49-638.jpg?cb=1446278061\" title=\"Removing The 'L' in ETL&#10;If data collection is backed by a d...\" target=\"_blank\">\n        49.\n      </a>\n    Removing The 'L' in ETL\nIf data collection is backed by a distributed messaging\nsystem (e.g. Kafka) you can do real-time fanout of the\ningested data to all consumers. No need to batch \"load\".\n• From there each consumer can do their own transformations\n50\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-50-638.jpg?cb=1446278061\" title=\"#NoMoreGreekLetterArchitectures&#10;51&#10;\" target=\"_blank\">\n        50.\n      </a>\n    #NoMoreGreekLetterArchitectures\n51\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-51-638.jpg?cb=1446278061\" title=\"NoETL&#10;52&#10;\" target=\"_blank\">\n        51.\n      </a>\n    NoETL\n52\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-52-638.jpg?cb=1446278061\" title=\"Strategy Technologies&#10;Scalable Infrastructure / Elastic Spa...\" target=\"_blank\">\n        52.\n      </a>\n    Strategy Technologies\nScalable Infrastructure / Elastic Spark, Cassandra, Kafka\nPartition For Scale, Network Topology Aware Cassandra, Spark, Kafka, Akka Cluster\nReplicate For Resiliency Spark,Cassandra, Akka Cluster all hash the node ring\nShare Nothing, Masterless Cassandra, Akka Cluster both Dynamo style\nFault Tolerance / No Single Point of Failure Spark, Cassandra, Kafka\nReplay From Any Point Of Failure Spark, Cassandra, Kafka, Akka + Akka Persistence\nFailure Detection Cassandra, Spark, Akka, Kafka\nConsensus &amp; Gossip Cassandra &amp; Akka Cluster\nParallelism Spark, Cassandra, Kafka, Akka\nAsynchronous Data Passing Kafka, Akka, Spark\nFast, Low Latency, Data Locality Cassandra, Spark, Kafka\nLocation Transparency Akka, Spark, Cassandra, Kafka\nMy Nerdy Chart\n53\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-53-638.jpg?cb=1446278061\" title=\"SMACK&#10;• Scala/Spark&#10;• Mesos&#10;• Akka&#10;• Cassandra&#10;• Kafka&#10;54&#10;\" target=\"_blank\">\n        53.\n      </a>\n    SMACK\n• Scala/Spark\n• Mesos\n• Akka\n• Cassandra\n• Kafka\n54\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-54-638.jpg?cb=1446278061\" title=\"Spark Streaming&#10;55&#10;\" target=\"_blank\">\n        54.\n      </a>\n    Spark Streaming\n55\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-55-638.jpg?cb=1446278061\" title=\"Spark Streaming&#10;• One runtime for streaming and batch proce...\" target=\"_blank\">\n        55.\n      </a>\n    Spark Streaming\n• One runtime for streaming and batch processing\n• Join streaming and static data sets\n• No code duplication\n• Easy, flexible data ingestion from disparate sources to\ndisparate sinks\n• Easy to reconcile queries against multiple sources\n• Easy integration of KV durable storage\n56\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-56-638.jpg?cb=1446278061\" title=\"How do I merge historical data with data&#10;in the stream?&#10;57&#10;\" target=\"_blank\">\n        56.\n      </a>\n    How do I merge historical data with data\nin the stream?\n57\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-57-638.jpg?cb=1446278061\" title=\"Join Streams With Static Data&#10;val ssc = new StreamingContex...\" target=\"_blank\">\n        57.\n      </a>\n    Join Streams With Static Data\nval ssc = new StreamingContext(conf, Milliseconds(500))\nssc.checkpoint(\"checkpoint\")\nval staticData: RDD[(Int,String)] =\nssc.sparkContext.textFile(\"whyAreWeParsingFiles.txt\").flatMap(func)\nval stream: DStream[(Int,String)] =\nKafkaUtils.createStream(ssc, zkQuorum, group, Map(topic -&gt; n))\n.transform { events =&gt; events.join(staticData))\n.saveToCassandra(keyspace,table)\nssc.start()\n58\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-58-638.jpg?cb=1446278061\" title=\"Training&#10;Data&#10;Feature&#10;Extraction&#10;Model&#10;Training&#10;Model&#10;Testi...\" target=\"_blank\">\n        58.\n      </a>\n    Training\nData\nFeature\nExtraction\nModel\nTraining\nModel\nTesting\nTest Data\nYour Data Extract Data To Analyze\nTrain your model to predict\nSpark MLLib\n59\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-59-638.jpg?cb=1446278061\" title=\"Spark Streaming &amp; ML&#10;60&#10;val context = new StreamingContext(...\" target=\"_blank\">\n        59.\n      </a>\n    Spark Streaming &amp; ML\n60\nval context = new StreamingContext(conf, Milliseconds(500))\nval model = KMeans.train(dataset, ...) // learn offline\nval stream = KafkaUtils\n.createStream(ssc, zkQuorum, group,..)\n.map(event =&gt; model.predict(event.feature))\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-60-638.jpg?cb=1446278061\" title=\"Apache Mesos&#10;Open-source cluster manager developed at UC Be...\" target=\"_blank\">\n        60.\n      </a>\n    Apache Mesos\nOpen-source cluster manager developed at UC Berkeley.\nAbstracts CPU, memory, storage, and other compute resources\naway from machines (physical or virtual), enabling fault-tolerant\nand elastic distributed systems to easily be built and run\neffectively.\n61\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-61-638.jpg?cb=1446278061\" title=\"Akka&#10;High performance concurrency framework for Scala and&#10;J...\" target=\"_blank\">\n        61.\n      </a>\n    Akka\nHigh performance concurrency framework for Scala and\nJava\n• Fault Tolerance\n• Asynchronous messaging and data processing\n• Parallelization\n• Location Transparency\n• Local / Remote Routing\n• Akka: Cluster / Persistence / Streams\n62\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-62-638.jpg?cb=1446278061\" title=\"Akka Actors&#10;A distribution and concurrency abstraction&#10;• Co...\" target=\"_blank\">\n        62.\n      </a>\n    Akka Actors\nA distribution and concurrency abstraction\n• Compute Isolation\n• Behavioral Context Switching\n• No Exposed Internal State\n• Event-based messaging\n• Easy parallelism\n• Configurable fault tolerance\n63\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-63-638.jpg?cb=1446278061\" title=\"64&#10;Akka Actor Hierarchy&#10;http://www.slideshare.net/jboner/bu...\" target=\"_blank\">\n        63.\n      </a>\n    64\nAkka Actor Hierarchy\nhttp://www.slideshare.net/jboner/building-reactive-applications-with-akka-in-scala\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-64-638.jpg?cb=1446278061\" title=\"import akka.actor._&#10;class NodeGuardianActor(args...) extend...\" target=\"_blank\">\n        64.\n      </a>\n    import akka.actor._\nclass NodeGuardianActor(args...) extends Actor with SupervisorStrategy {\nval temperature = context.actorOf(\nProps(new TemperatureActor(args)), \"temperature\")\nval precipitation = context.actorOf(\nProps(new PrecipitationActor(args)), \"precipitation\")\noverride def preStart(): Unit = { /* lifecycle hook: init */ }\ndef receive : Actor.Receive = {\ncase Initialized =&gt; context become initialized\n}\ndef initialized : Actor.Receive = {\ncase e: SomeEvent =&gt; someFunc(e)\ncase e: OtherEvent =&gt; otherFunc(e)\n}\n}\n65\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-65-638.jpg?cb=1446278061\" title=\"Apache Cassandra&#10;• Extremely Fast&#10;• Extremely Scalable&#10;• Mu...\" target=\"_blank\">\n        65.\n      </a>\n    Apache Cassandra\n• Extremely Fast\n• Extremely Scalable\n• Multi-Region / Multi-Datacenter\n• Always On\n• No single point of failure\n• Survive regional outages\n• Easy to operate\n• Automatic &amp; configurable replication 66\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-66-638.jpg?cb=1446278061\" title=\"Apache Cassandra&#10;• Very flexible data modeling (collections...\" target=\"_blank\">\n        66.\n      </a>\n    Apache Cassandra\n• Very flexible data modeling (collections, user defined\ntypes) and changeable over time\n• Perfect for ingestion of real time / machine data\n• Huge community\n67\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-67-638.jpg?cb=1446278061\" title=\"Spark Cassandra Connector&#10;• NOSQL JOINS!&#10;• Write &amp; Read dat...\" target=\"_blank\">\n        67.\n      </a>\n    Spark Cassandra Connector\n• NOSQL JOINS!\n• Write &amp; Read data between Spark and Cassandra\n• Compatible with Spark 1.4\n• Handles Data Locality for Speed\n• Implicit type conversions\n• Server-Side Filtering - SELECT, WHERE, etc.\n• Natural Timeseries Integration\n68\nhttp://github.com/datastax/spark-cassandra-connector\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-68-638.jpg?cb=1446278061\" title=\"KillrWeather&#10;69&#10;http://github.com/killrweather/killrweather...\" target=\"_blank\">\n        68.\n      </a>\n    KillrWeather\n69\nhttp://github.com/killrweather/killrweather\nA reference application showing how to easily integrate streaming and\nbatch data processing with Apache Spark Streaming, Apache\nCassandra, Apache Kafka and Akka for fast, streaming computations\non time series data in asynchronous event-driven environments.\nhttp://github.com/databricks/reference-apps/tree/master/timeseries/scala/timeseries-weather/src/main/scala/com/\ndatabricks/apps/weather\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-69-638.jpg?cb=1446278061\" title=\"70&#10;• High Throughput Distributed Messaging&#10;• Decouples Data...\" target=\"_blank\">\n        69.\n      </a>\n    70\n• High Throughput Distributed Messaging\n• Decouples Data Pipelines\n• Handles Massive Data Load\n• Support Massive Number of Consumers\n• Distribution &amp; partitioning across cluster nodes\n• Automatic recovery from broker failures\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-70-638.jpg?cb=1446278061\" title=\"Spark Streaming &amp; Kafka&#10;val context = new StreamingContext(...\" target=\"_blank\">\n        70.\n      </a>\n    Spark Streaming &amp; Kafka\nval context = new StreamingContext(conf, Seconds(1))\nval wordCount = KafkaUtils.createStream(context, ...)\n.flatMap(_.split(\" \"))\n.map(x =&gt; (x, 1))\n.reduceByKey(_ + _)\nwordCount.saveToCassandra(ks,table)\ncontext.start() // start receiving and computing\n71\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-71-638.jpg?cb=1446278061\" title=\"72&#10;class KafkaStreamingActor(params: Map[String, String], s...\" target=\"_blank\">\n        71.\n      </a>\n    72\nclass KafkaStreamingActor(params: Map[String, String], ssc: StreamingContext)\nextends AggregationActor(settings: Settings) { \nimport settings._\n \nval kafkaStream = KafkaUtils.createStream[String, String, StringDecoder, StringDecoder]( \nssc, params, Map(KafkaTopicRaw -&gt; 1), StorageLevel.DISK_ONLY_2) \n.map(_._2.split(\",\")) \n.map(RawWeatherData(_)) \n \nkafkaStream.saveToCassandra(CassandraKeyspace, CassandraTableRaw) \n/** RawWeatherData: wsid, year, month, day, oneHourPrecip */ \nkafkaStream.map(hour =&gt; (hour.wsid, hour.year, hour.month, hour.day, hour.oneHourPrecip)) \n.saveToCassandra(CassandraKeyspace, CassandraTableDailyPrecip) \n \n/** Now the [[StreamingContext]] can be started. */ \ncontext.parent ! OutputStreamInitialized \n \ndef receive : Actor.Receive = {…}\n}\nGets the partition key: Data Locality\nSpark C* Connector feeds this to Spark\nCassandra Counter column in our schema,\nno expensive `reduceByKey` needed. Simply\nlet C* do it: not expensive and fast.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-72-638.jpg?cb=1446278061\" title=\"73&#10;/** For a given weather station, calculates annual cumul...\" target=\"_blank\">\n        72.\n      </a>\n    73\n/** For a given weather station, calculates annual cumulative precip - or year to date. */ \nclass PrecipitationActor(ssc: StreamingContext, settings: WeatherSettings) extends AggregationActor { \n \ndef receive : Actor.Receive = { \ncase GetPrecipitation(wsid, year) =&gt; cumulative(wsid, year, sender) \ncase GetTopKPrecipitation(wsid, year, k) =&gt; topK(wsid, year, k, sender) \n} \n \n/** Computes annual aggregation.Precipitation values are 1 hour deltas from the previous. */ \ndef cumulative(wsid: String, year: Int, requester: ActorRef): Unit = \nssc.cassandraTable[Double](keyspace, dailytable) \n.select(\"precipitation\") \n.where(\"wsid = ? AND year = ?\", wsid, year) \n.collectAsync() \n.map(AnnualPrecipitation(_, wsid, year)) pipeTo requester \n \n/** Returns the 10 highest temps for any station in the `year`. */ \ndef topK(wsid: String, year: Int, k: Int, requester: ActorRef): Unit = { \nval toTopK = (aggregate: Seq[Double]) =&gt; TopKPrecipitation(wsid, year, \nssc.sparkContext.parallelize(aggregate).top(k).toSeq) \n \nssc.cassandraTable[Double](keyspace, dailytable) \n.select(\"precipitation\") \n.where(\"wsid = ? AND year = ?\", wsid, year) \n.collectAsync().map(toTopK) pipeTo requester \n} \n}\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-73-638.jpg?cb=1446278061\" title=\"A New Approach&#10;• One Runtime: streaming, scheduled&#10;• Simpli...\" target=\"_blank\">\n        73.\n      </a>\n    A New Approach\n• One Runtime: streaming, scheduled\n• Simplified architecture\n• Allows us to\n• Write different types of applications\n• Write more type safe code\n• Write more reusable code\n74\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-74-638.jpg?cb=1446278061\" title=\"Need daily analytics aggregate reports? Do it in the stream...\" target=\"_blank\">\n        74.\n      </a>\n    Need daily analytics aggregate reports? Do it in the stream, save\nresults in Cassandra for easy reporting as needed - with data\nlocality not offered by S3.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-75-638.jpg?cb=1446278061\" title=\"FiloDB&#10;Distributed, columnar database designed to run very ...\" target=\"_blank\">\n        75.\n      </a>\n    FiloDB\nDistributed, columnar database designed to run very fast\nanalytical queries\n• Ingest streaming data from many streaming sources\n• Row-level, column-level operations and built in versioning\noffer greater flexibility than file-based technologies\n• Currently based on Apache Cassandra &amp; Spark\n• github.com/tuplejump/FiloDB\n76\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-76-638.jpg?cb=1446278061\" title=\"FiloDB&#10;• Breakthrough performance levels for analytical que...\" target=\"_blank\">\n        76.\n      </a>\n    FiloDB\n• Breakthrough performance levels for analytical queries\n• Performance comparable to Parquet\n• One to two orders of magnitude faster than Spark on\nCassandra 2.x\n• Versioned - critical for reprocessing logic/code changes\n• Can simplify your infrastructure dramatically\n• Queries run in parallel in Spark for scale-out ad-hoc analysis\n• Space-saving techniques\n77\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-77-638.jpg?cb=1446278061\" title=\"WRAPPING UP&#10;78&#10;\" target=\"_blank\">\n        77.\n      </a>\n    WRAPPING UP\n78\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-78-638.jpg?cb=1446278061\" title=\"Architectyr?&#10;79&#10;&quot;This is a giant mess&quot;&#10;- Going Real-time - ...\" target=\"_blank\">\n        78.\n      </a>\n    Architectyr?\n79\n\"This is a giant mess\"\n- Going Real-time - Data Collection and Stream Processing with Apache Kafka, Jay Kreps\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-79-638.jpg?cb=1446278061\" title=\"80&#10;Simplified&#10;\" target=\"_blank\">\n        79.\n      </a>\n    80\nSimplified\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-80-638.jpg?cb=1446278061\" title=\"81&#10;\" target=\"_blank\">\n        80.\n      </a>\n    81\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-81-638.jpg?cb=1446278061\" title=\"82&#10;www.tuplejump.com&#10;info@tuplejump.com@tuplejump&#10;\" target=\"_blank\">\n        81.\n      </a>\n    82\nwww.tuplejump.com\ninfo@tuplejump.com@tuplejump\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-82-638.jpg?cb=1446278061\" title=\"83&#10;@helenaedelson&#10;github.com/helena&#10;slideshare.net/helenaed...\" target=\"_blank\">\n        82.\n      </a>\n    83\n@helenaedelson\ngithub.com/helena\nslideshare.net/helenaedelson\nTHANK YOU!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-83-638.jpg?cb=1446278061\" title=\"I'm speaking at QCon SF on the broader&#10;topic of Streaming a...\" target=\"_blank\">\n        83.\n      </a>\n    I'm speaking at QCon SF on the broader\ntopic of Streaming at Scale\nhttp://qconsf.com/sf2015/track/streaming-data-scale\n84\n \n  </li>\n              </ol></div></div></div><aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\"><dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\"><ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"54590394\" title=\"Bruce Heavin The Thinkable Presentation\" href=\"https://www.linkedin.com/learning/bruce-heavin-the-thinkable-presentation?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Bruce Heavin The Thinkable Presentation\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Bruce Heavin The Thinkable Presentation\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=7Pmu5jhLuzYhd3kcdQaN2ktEQv0%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-gWiKv_9yfYXbufMLWZLOn7FQIImxW\" /></div>\n    <div class=\"lynda-content\"><p>Bruce Heavin The Thinkable Presentation</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"54590394\" title=\"Gaining Skills with LinkedIn Learning\" href=\"https://www.linkedin.com/learning/gaining-skills-with-linkedin-learning?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Gaining Skills with LinkedIn Learning\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Gaining Skills with LinkedIn Learning\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=dTAVDscYes1J4Ec%2F4ONSWvNwSnk%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-kWyai-9SfZXfqccbeZLSiolwWfy8JlQEyfuisRznmEY69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Gaining Skills with LinkedIn Learning</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"54590394\" title=\"PowerPoint: Designing Better Slides\" href=\"https://www.linkedin.com/learning/powerpoint-designing-better-slides?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_PowerPoint: Designing Better Slides\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"PowerPoint: Designing Better Slides\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=Qzyjo5pakOdYhauvchS%2F2whtPj0%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-kUyWs-dWfZX_pf8TfZLSiol4feCwDkwc2feivRTXiEY69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>PowerPoint: Designing Better Slides</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"53371129\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Reactive app using actor model &amp; apache spark\" href=\"https://www.slideshare.net/RahulKumar405/reactive-app-using-actor-model-apache-spark\">\n    \n    <div class=\"related-content\"><p>Reactive app using actor model &amp; apache spark</p><p>Rahul Kumar</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"54491621\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Real-Time Anomaly Detection  with Spark MLlib, Akka and  Cassandra\" href=\"https://www.slideshare.net/natalinobusa/realtime-anomaly-detection-with-spark-mllib-akka-and-cassandra\">\n    \n    <div class=\"related-content\"><p>Real-Time Anomaly Detection  with Spark MLlib, Akka and  Cassandra</p><p>Natalino Busa</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"55646316\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Sa introduction to big data pipelining with cassandra &amp;amp; spark   west minster meetup - black-2015 0.11-2\" href=\"https://www.slideshare.net/langworth/sa-introduction-to-big-data-pipelining-with-cassandra-amp-spark-west-minster-meetup-black2015-0112\">\n    \n    <div class=\"related-content\"><p>Sa introduction to big data pipelining with cassandra &amp;amp; spark   west mins...</p><p>Simon Ambridge</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"49190113\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Lambda Architecture with Spark Streaming, Kafka, Cassandra, Akka, Scala\" href=\"https://www.slideshare.net/helenaedelson/lambda-architecture-with-spark-streaming-kafka-cassandra-akka-scala\">\n    \n    <div class=\"related-content\"><p>Lambda Architecture with Spark Streaming, Kafka, Cassandra, Akka, Scala</p><p>Helena Edelson</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"43475359\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Streaming Big Data with Spark, Kafka, Cassandra, Akka &amp; Scala (from webinar)\" href=\"https://www.slideshare.net/helenaedelson/streaming-bigdata-helenawebinarv3\">\n    \n    <div class=\"related-content\"><p>Streaming Big Data with Spark, Kafka, Cassandra, Akka &amp; Scala (from webinar)</p><p>Helena Edelson</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"50371041\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Spark Kernel Talk - Apache Spark Meetup San Francisco (July 2015)\" href=\"https://www.slideshare.net/RobertChipSenkbeil/spark-kernel-meetup-talk\">\n    \n    <div class=\"related-content\"><p>Spark Kernel Talk - Apache Spark Meetup San Francisco (July 2015)</p><p>Robert \"Chip\" Senkbeil</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"53162817\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"How to deploy Apache Spark  to Mesos/DCOS\" href=\"https://www.slideshare.net/Typesafe_Inc/how-to-deploy-apache-spark-to-mesosdcos\">\n    \n    <div class=\"related-content\"><p>How to deploy Apache Spark  to Mesos/DCOS</p><p>Legacy Typesafe (now Lightbend)</p></div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div></div><footer>\n          <div class=\"row\"><div class=\"columns\"><ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div></div>\n          \n          <div class=\"row\"><div class=\"columns\"><p class=\"copyright text-center\">LinkedIn Corporation © 2018</p></div></div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div>\n    <div class=\"row\"><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></div>\n        \n    </div>\n    \n    \n      <noscript>\n    </noscript>",
        "created_at": "2018-06-27T03:57:42+0000",
        "updated_at": "2018-06-27T03:57:51+0000",
        "published_at": null,
        "published_by": [
          "Helena Edelson"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 13,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891-thumbnail-4.jpg?cb=1446278061",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10744"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 996,
            "label": "monitoring",
            "slug": "monitoring"
          },
          {
            "id": 1103,
            "label": "prometheus",
            "slug": "prometheus"
          }
        ],
        "is_public": false,
        "id": 10299,
        "uid": null,
        "title": "criteo/cassandra_exporter",
        "url": "https://github.com/criteo/cassandra_exporter",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p style=\"text-align: center;\">\n  <a target=\"_blank\" href=\"https://github.com/criteo/cassandra_exporter/raw/master/logo.png\"><img src=\"https://github.com/criteo/cassandra_exporter/raw/master/logo.png\" alt=\"logo\" /></a>\n</p>\n<h2><a id=\"user-content-description\" class=\"anchor\" aria-hidden=\"true\" href=\"#description\"></a>Description</h2>\n<p>Cassandra exporter is a standalone application which exports <a href=\"http://cassandra.apache.org/\" rel=\"nofollow\">Apache Cassandra®</a> metrics throught a prometheus friendly endpoint.\nThis project is originally a fork of <a href=\"https://github.com/prometheus/jmx_exporter\">JMX exporter</a> but aims at an easier integration with <a href=\"http://cassandra.apache.org/\" rel=\"nofollow\">Apache Cassandra®</a>.</p>\n<p>Specifically, this project brings :</p>\n<ul class=\"contains-task-list\"><li class=\"task-list-item\"> Exporting EstimatedHistogram metrics specific to <a href=\"http://cassandra.apache.org/\" rel=\"nofollow\">Apache Cassandra®</a></li>\n<li class=\"task-list-item\"> Filtering on mbean's attributes</li>\n<li class=\"task-list-item\"> Metrics naming that respect the mbean hierarchy</li>\n<li class=\"task-list-item\"> Comprehensive config file</li>\n</ul><p>An essential design choice the project makes is to not let prometheus drive the scraping frequency. This decision has been taken because a lot of <a href=\"http://cassandra.apache.org/\" rel=\"nofollow\">Apache Cassandra®</a> metrics are expensive to scrap and can hinder the performance of the node.\nAs we don't want this kind of situation to happen in production, the scrape frequency is restricted via the configuration of Cassandra Exporter.</p>\n<p><a target=\"_blank\" href=\"https://camo.githubusercontent.com/85f8e6b503fc834c4127aa556d8d941c29b8a1ab/68747470733a2f2f67726166616e612e636f6d2f6170692f64617368626f617264732f363235382f696d616765732f333939362f696d616765\"><img src=\"https://camo.githubusercontent.com/85f8e6b503fc834c4127aa556d8d941c29b8a1ab/68747470733a2f2f67726166616e612e636f6d2f6170692f64617368626f617264732f363235382f696d616765732f333939362f696d616765\" alt=\"Grafana\" data-canonical-src=\"https://grafana.com/api/dashboards/6258/images/3996/image\" /></a>\n<a target=\"_blank\" href=\"https://camo.githubusercontent.com/e9d40a59216424e2a99d8641461c4299925ea44a/68747470733a2f2f67726166616e612e636f6d2f6170692f64617368626f617264732f363430302f696d616765732f343131312f696d616765\"><img src=\"https://camo.githubusercontent.com/e9d40a59216424e2a99d8641461c4299925ea44a/68747470733a2f2f67726166616e612e636f6d2f6170692f64617368626f617264732f363430302f696d616765732f343131312f696d616765\" alt=\"Grafana\" data-canonical-src=\"https://grafana.com/api/dashboards/6400/images/4111/image\" /></a></p>\n<h2><a id=\"user-content-how-to-use\" class=\"anchor\" aria-hidden=\"true\" href=\"#how-to-use\"></a>How to use</h2>\n<p>To start the application</p>\n<blockquote>\n<p>java -jar cassandra_exporter.jar config.yml</p>\n</blockquote>\n<p>The Cassandra exporter needs to run on every Cassandra nodes to get all the informations regarding the whole cluster.</p>\n<p>You can have a look at a full configuration file <a href=\"https://github.com/criteo/cassandra_exporter/blob/master/config.yml\">here</a>\nThe 2 main parts are :</p>\n<ol><li>blacklist</li>\n<li>maxScrapFrequencyInSec</li>\n</ol><p>In the <code>blacklist</code> block, you specify the metrics you don't want the exporter to scrape. This is important as JMX is an RPC mechanism and you don't want to trigger some of those RPC. For example, mbeans endpoint from <code>org:apache:cassandra:db:.*</code> does not expose any metrics but are used to trigger actions on Cassandra's nodes.</p>\n<p>In the <code>maxScrapFrequencyInSec</code>, you specify the metrics you want to be scraped at which frequency.\nBasically, starting from the set of all mbeans, the blacklist is applied first to filter this set and then the <code>maxScrapFrequencyInSec</code> is applied as a whitelist to filter the resulting set.</p>\n<p>As an example, if we take as input set the metrics <code>{a, b, c}</code> and the config file is</p>\n<div class=\"highlight highlight-source-yaml\"><pre>blacklist:\n  - a\nmaxScrapFrequencyInSec:\n  50:\n    - .*\n  3600:\n    - b</pre></div>\n<p>Cassandra Exporter will have the following behavior:</p>\n<ol><li>The metrics matching the blacklisted entries will never be scraped, here the metric <code>a</code> won't be available</li>\n<li>In reverse order of frequency the metrics matching <code>maxScrapFrequencyInSec</code> will be scraped\n<ol><li>Metric <code>b</code> will be scraped every hour</li>\n<li>Remaining metrics will be scrapped every 50s, here only <code>c</code></li>\n</ol></li>\n</ol><p>Resulting in :</p>\n<table><thead><tr><th>Metric</th>\n<th>Scrap Frequency</th>\n</tr></thead><tbody><tr><td>a</td>\n<td>never</td>\n</tr><tr><td>b</td>\n<td>every hour</td>\n</tr><tr><td>c</td>\n<td>every 50 seconds</td>\n</tr></tbody></table><p>Once started the prometheus endpoint will be available at <code>localhost:listenPort/</code> or <code>localhost:listenPort/metrics</code> and metrics format will look like the one below</p>\n<blockquote>\n<p>cassandra_stats{name=\"org:apache:cassandra:metrics:table:biggraphite:datapoints_5760p_3600s_aggr:writelatency:50thpercentile\",} 35.425000000000004</p>\n</blockquote>\n<h2><a id=\"user-content-how-to-debug\" class=\"anchor\" aria-hidden=\"true\" href=\"#how-to-debug\"></a>How to debug</h2>\n<p>Run the program with the following options:</p>\n<blockquote>\n<p>java -Dorg.slf4j.simpleLogger.defaultLogLevel=trace -jar cassandra_exporter.jar config.yml --oneshot</p>\n</blockquote>\n<p>You will get the duration of how long it took to scrape individual MBean, this is useful to understand which metrics are expansive to scrape.</p>\n<p>Goods sources of information to understand what Mbeans are doing/create your dashboards are:</p>\n<ol><li><a href=\"https://cassandra.apache.org/doc/latest/operating/metrics.html\" rel=\"nofollow\">https://cassandra.apache.org/doc/latest/operating/metrics.html</a></li>\n<li><a href=\"https://github.com/apache/cassandra/tree/trunk/src/java/org/apache/cassandra/metrics\">https://github.com/apache/cassandra/tree/trunk/src/java/org/apache/cassandra/metrics</a></li>\n<li><a href=\"http://thelastpickle.com/blog/2017/12/05/datadog-tlp-dashboards.html\" rel=\"nofollow\">http://thelastpickle.com/blog/2017/12/05/datadog-tlp-dashboards.html</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=Q9AAR4UQzMk\" rel=\"nofollow\">https://www.youtube.com/watch?v=Q9AAR4UQzMk</a></li>\n</ol><h2><a id=\"user-content-config-file-example\" class=\"anchor\" aria-hidden=\"true\" href=\"#config-file-example\"></a>Config file example</h2>\n<div class=\"highlight highlight-source-yaml\"><pre>host: localhost:7199\nssl: False\nuser:\npassword:\nlistenPort: 8080\nblacklist:\n   # Unaccessible metrics (not enough privilege)\n   - java:lang:memorypool:.*usagethreshold.*\n   # Leaf attributes not interesting for us but that are presents in many path (reduce cardinality of metrics)\n   - .*:999thpercentile\n   - .*:95thpercentile\n   - .*:fifteenminuterate\n   - .*:fiveminuterate\n   - .*:durationunit\n   - .*:rateunit\n   - .*:stddev\n   - .*:meanrate\n   - .*:mean\n   - .*:min\n   # Path present in many metrics but uninterresting\n   - .*:viewlockacquiretime:.*\n   - .*:viewreadtime:.*\n   - .*:cas[a-z]+latency:.*\n   - .*:colupdatetimedeltahistogram:.*\n   # Mostly for RPC, do not scrap them\n   - org:apache:cassandra:db:.*\n   # columnfamily is an alias for Table metrics in cassandra 3.x\n   # https://github.com/apache/cassandra/blob/8b3a60b9a7dbefeecc06bace617279612ec7092d/src/java/org/apache/cassandra/metrics/TableMetrics.java#L162\n   - org:apache:cassandra:metrics:columnfamily:.*\n   # Should we export metrics for system keyspaces/tables ?\n   - org:apache:cassandra:metrics:[^:]+:system[^:]*:.*\n   # Don't scrape us\n   - com:criteo:nosql:cassandra:exporter:.*\nmaxScrapFrequencyInSec:\n  50:\n    - .*\n  # Refresh those metrics only every hour as it is costly for cassandra to retrieve them\n  3600:\n    - .*:snapshotssize:.*\n    - .*:estimated.*\n    - .*:totaldiskspaceused:.*</pre></div>\n<h2><a id=\"user-content-docker\" class=\"anchor\" aria-hidden=\"true\" href=\"#docker\"></a>Docker</h2>\n<p>You can pull an image directly from <a href=\"https://hub.docker.com/r/criteord/cassandra_exporter/\" rel=\"nofollow\">Dockerhub</a>:</p>\n<pre>docker pull criteord/cassandra_exporter:latest\n</pre>\n<h2><a id=\"user-content-kubernetes\" class=\"anchor\" aria-hidden=\"true\" href=\"#kubernetes\"></a>Kubernetes</h2>\n<p>To get an idea on how to integrate Cassandra Exporter in Kubernetes, you can look at <a href=\"https://github.com/MySocialApp/kubernetes-helm-chart-cassandra\">this helm Chart</a>.</p>\n<h2><a id=\"user-content-grafana\" class=\"anchor\" aria-hidden=\"true\" href=\"#grafana\"></a>Grafana</h2>\n<p>Dedicated dashboards can be <a href=\"https://github.com/criteo/cassandra_exporter/tree/grafana/grafana\">found here</a></p>\n</article>",
        "created_at": "2018-06-21T12:17:35+0000",
        "updated_at": "2018-06-21T12:17:43+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/1713646?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10299"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1367,
            "label": "scaling",
            "slug": "scaling"
          }
        ],
        "is_public": false,
        "id": 10288,
        "uid": null,
        "title": "Scale it to Billions — What They Don’t Tell you in the Cassandra README – Threat Stack",
        "url": "https://www.threatstack.com/blog/scaling-cassandra-lessons-learned",
        "content": "<p class=\"p1\">At Threat Stack our engineering and operations teams have embraced the concept of the polyglot data platform, recognizing that no one solution can provide for all of our needs. Those needs include rapid scaling, ideally linearly, to support growing customer demand and the elastic workloads of our new economy customers. We also require different forms of analysis to support stream analysis for our IDS feature set, efficient lookup tables and prematerialized views for our ETDR feature set, and offline analysis for analysis and research.</p>\n<p class=\"p3\">A core component of our data platform for several years has been Cassandra, which we upgraded to Datastax Enterprise (DSE) through their start up program last year. Originally we were expecting to use it as our single source of truth for all of our time series data, but this turned out to be an anti pattern. Instead we have found it very useful for look up tables and pre-materialized views (more on this later).</p>\n<p>Our initial Cassandra cluster was three i2.2xlarge nodes spread across 3 Availability Zones in AWS, but within a few short months we had expanded to over thirty Cassandra nodes. At the rate at which we were taking in new data, our overall scaling strategy looked similar to this:<br /></p>\n<p><strong><strong> </strong></strong></p>\n<p>The team did have extensive experience with other eventually consistent databases though, so with some advice from Datastax, Cassandra has now become one of the most stable and pleasurable components of our polyglot data platform to work with.</p>\n<p>This content has been a long time coming, so we definitely ran longer than a generally accepted blog post might. We decided to ironically embrace the chaos by providing you with a table of contents:</p>\n<ol><li><a href=\"#1\">Quick Tips</a></li>\n<li><a href=\"#2\">Monitoring Gotcha and Quirks</a></li>\n<li><a href=\"#3\">AWS Instance Type and Disk Configuration</a></li>\n<li><a href=\"#4\">Drowning Your Cluster with Multi-Tenant Tables</a></li>\n<li><a href=\"#5\">Streaming Failures, JVM Tuning, and GC Woes</a></li>\n<li><a href=\"#6\">Schema design, or These Aren’t the Rows You’re Looking For</a></li>\n<li><a href=\"#7\">Only Have 2 to 3 Seed Nodes per Data Center</a></li>\n<li><a href=\"#8\">Conclusion</a></li>\n</ol><h2>1. Quick Tips</h2>\n<p>Some of these are well known, but are worth repeating.</p>\n<ul><li>Batch writes by partition key, then by batch size (5120 bytes, the Cassandra warn threshold).</li>\n<li>Don’t under staff Cassandra. This is hard as a start up, but recognize going in that it could require 1 to 2 FTEs as you ramp up, maybe more depending on how quickly you scale up. While we are able to see the benefits of this investment on the other side, it was an uncomfortable place to be in as we were also scaling up customer acquisition. </li>\n<li>Don’t adopt new versions too quickly. Minor version bumps have been known to cause havoc and new features only work roughly as expected the first time (DTCS is a great example of this). We regularly run new versions in our development environment for a month or two before promoting to production. </li>\n<li>Paradoxically, don’t fall too far behind with new versions. Sadly we have friends who are still stuck on Cassandra 1.x because of their Thrift usage. We light a candle for them. </li>\n<li>Many smaller nodes beat out fewer larger nodes. Not only does this make your dynamo quorum math more resilient to per-server failure, which is important because AWS <strong>will</strong> decommission a node in your cluster, but it decreases the mean time to recovery because you can stream data across more nodes at a cluster to achieve higher aggregate data transfer rates. </li>\n<li>Budget days to bring a node into the cluster. If you’ve vertically scaled, then it will take over a week. Regardless you will want to uncap streaming and compaction throughput with nodetool.  You’ll take a slight performance hit, but it’s worth it to finish node streaming in a reasonable time. This is another good reason why you want to run more smaller sized instances with less data per system. </li>\n<li>Don’t use hinted handoffs (ANY or LOCAL_ANY quorum). In fact, just disable them in the configuration. It’s too easy to lose data during a prolonged outage or load spike, and if a node went down because of the load spike you’re just going to pass the problem around the ring, eventually taking multiple or all nodes down. We never experienced this on Cassandra, but have on other systems that supported hinted handoffs.</li>\n</ul><h2>2. Monitoring Gotcha and Quirks </h2>\n<p>Datastax graciously offers usage of Opscenter and the included agent for both open source Cassandra and DSE.  The setup and deployment of this is incredibly simple, and when you are a high growth start up, time is a premium.  Opscenter acted as our initial metrics source and gave us pretty high quality metrics early in our scaling time line.  Because it was easy to setup and deploy the opscenter collector agents we were able to get meaningful data without spending a lot of time on it.  </p>\n<p>A few months after we launched our service, new user sign ups increased our data ingestion by an order of magnitude within just 2 weeks.  We were over 20 Cassandra nodes by this point and ingesting a few TB per day.  Our original way of handling multi-tenancy was at the table level, essentially storing each customer’s data in their own table  (we talk more below about why this was a bad idea).  But as our customer count exploded we were creating more and more tables (we had a few per each customer).  By default the opscenter agent will collect data for all your tables to allow for some pretty awesome granular metrics capture. But when you have so many tables it turns into a lot of metrics. Which brings us to our first gotcha.</p>\n<p><strong>Don’t use the same Cassandra cluster to store Opscenter metrics and application data.</strong></p>\n<p>Now – the problem is that currently using a separate cluster for metrics collection with opscenter is an Enterprise only option.  If you are lucky enough to be using DSE then <a href=\"http://docs.datastax.com/en/opscenter/5.1/opsc/configure/opscStoringCollectionDataDifferentCluster_t.html\" target=\"_blank\" rel=\"noopener\">here is how you can set it up</a>.</p>\n<p>If you don’t have DSE you could potentially put the Opscenter data in a keyspace on nodes in a separate datacenter.  I haven’t tried that so YMMV.  For us we didn’t want to lose out on potentially valuable metrics so we sent the metrics to a separate Cassandra cluster.  The graph below of the cluster load average shows a pretty large drop when we essentially stop DDOSing our cluster with our own metrics.</p>\n<p><strong><strong> <img title=\"\" src=\"https://www.threatstack.com/wp-content/uploads/2017/08/Cassandra-graph.png\" width=\"642\" height=\"417\" alt=\"image\" /></strong></strong></p>\n<p><br />If you don’t have DSE, you can (and should)<a href=\"http://docs.datastax.com/en/opscenter/5.1/opsc/configure/opscExcludingKeyspaces_c.html\" target=\"_blank\" rel=\"noopener\"> disable collection on specific keyspaces and column families</a>.</p>\n<p>But at this point the value of opscenter goes down quite a bit when you start nerfing functionality and metrics you’re collecting.</p>\n<h3><strong>Metrics Collection </strong></h3>\n<p>A month after launch when we had cluster issues we quickly found that Opscenter was great when the cluster was running optimally, but when the cluster has issues your visibility goes down to zero. We are a happy Librato customer and soon starting collecting additional metrics using the <a href=\"https://github.com/sensu-plugins/sensu-plugins-cassandra/blob/master/bin/metrics-cassandra-graphite.rb\" target=\"_blank\" rel=\"noopener\">Sensu community plugin for Cassandra</a>.  It’s a very basic way to collect metrics by parsing nodetool status output, and didn’t require us messing around with the JMX console.</p>\n<p>Over the last few months we have built an internal high resolution metrics cluster and we wanted higher resolution metrics for Cassandra as well, so for that we moved metrics collection from Sensu over to collectd using the JMX plugin.  This has worked pretty well for us and <a href=\"https://github.com/signalfx/integrations/tree/master/collectd-cassandra\" target=\"_blank\" rel=\"noopener\">SignalFX has an example</a> of how you might set that up.</p>\n<p>Additionally, Cassandra does have a <a href=\"http://www.datastax.com/dev/blog/pluggable-metrics-reporting-in-cassandra-2-0-2\" target=\"_blank\" rel=\"noopener\">pluggable metrics reporting</a> since 2.0.0 – you can use that in order to send metrics to your local metrics cluster.  Assuming this works well we’ll likely be moving over to this to push metrics to graphite vs pulling them.</p>\n<h3><strong>Metrics that you should care about</strong></h3>\n<p>This is where the most learning about Cassandra really took place.  There are a TON of metrics available via JMX and if you can afford the storage of those metrics I’m a fan of capturing as many as possible at as high a resolution as is feasible for your environment.  </p>\n<p>You can use the JConsole in order to navigate around to see<a href=\"http://docs.datastax.com/en/cassandra/2.0/cassandra/operations/ops_monitoring_c.html\" target=\"_blank\" rel=\"noopener\"> what kind of metrics exist</a>. There is some more info on the Datastax site.</p>\n<p>Additionally there is some great info on the <a href=\"http://wiki.apache.org/cassandra/Metrics\" target=\"_blank\" rel=\"noopener\">Cassandra wiki </a>describing some of the metrics you can capture.</p>\n<p>This is <a href=\"https://lostechies.com/ryansvihla/2014/11/25/my-cassandra-diagnostics-checklist-brain-dump/\" target=\"_blank\" rel=\"noopener\">one blog post in particular</a> that was helpful for me in the beginning of our Cassandra usage.</p>\n<h2>3. AWS Instance Type and Disk Configuration</h2>\n<p>The original cluster we launched on in October 2014 was built on i2.2xlarge servers (8 vCPUs, 61GB of RAM, and 2 x 800GB SSDs). While the local SSDs were a treat, especially when you had to force a hard compaction or stream data, it wasn’t cost efficient.</p>\n<p>Earlier this spring AWS launched its D2 lines of instances, which were specifically targeted at our situation of needing a large amount of locally attached storage at a low price point, without sacrificing CPU and RAM density. We rebuilt the cluster in place by replacing i2.2xlarge’s with d2.2xlarge’s, giving us the same CPU and RAM but with 6 x 2TB spinning disks. This was a time intensive process as we waited for all the data to stream, but was effectively cheaper than running two whole clusters in parallel.</p>\n<p>The other impactful change that we made was dedicating a single spinning disk to the commit log. Every operation that touches the disk sped up as a result of this. The remaining 5 disks are striped together, giving us 10TB usable or 5TB effective disk space (leaving 50% of the disk for SSTable compaction). If we were to fill the effective disk space of 5TB then node replacement would likely take days or weeks, which is too long. As a result we are deciding whether or not to double the size of the cluster on d2.xlarge’s.</p>\n<h2>4. Drowning Your Cluster with Multi-Tenant Tables</h2>\n<p>The entire Threat Stack platform is multi tenant, including the storage. Our initial implementation of tenancy on Cassandra was at the table level, allowing for logical tracking of access and growth rates per customer.</p>\n<p>However, we found that handling more than a few hundred tables was outside of Cassandra’s capabilities. Compactions would regularly fail, the Datastax read repair service never completed, and the JVM was spending far too much time tracking state. This problem scaled rapidly as we added customers because the number of tables was equal to the number of customers multiplied by the number of table types.</p>\n<p>Luckily we already had agent IDs (sensors) in the partition key which have a one-to-one relationship to organization IDs (tenants), so it was trivial to collapse down to multi tenant tables.</p>\n<p>The other benefit was that we no longer had to dynamically create new tables. This was a painful and error prone process, which if performed incorrectly could crash Cassandra if too many duplicate CREATE TABLE statements were issued.</p>\n<h2>5. Streaming Failures, JVM Tuning, and GC Woes</h2>\n<p><strong><em>*obligatory disclaimer about JVM tuning and other dark arts</em></strong></p>\n<p>When you are under a state of high growth, adding nodes quickly becomes <em>critical.  </em>When nodes fail to stream the stress level goes up about the same rate as your total available disk goes down. As our node count grew and the amount of writes per second increased, we reached a point where a few default settings caused us to be unable to stream new nodes. Prior to this point new nodes would take about 10-15 hours to fully join the cluster. Mainly because of our (incorrect) usage of per-account column families we were streaming about 70,000 SSTables to new nodes.  Part of the streaming issue was related to that, but it was compounded by some of the default settings in our Cassandra cluster.</p>\n<p><strong>Your HEAP_NEW_SIZE is probably too low</strong></p>\n<p>There were old recommendations that said 100m per CPU up to 800m total.  For us, this was not enough as the real reason nodes were unable to fully complete bootstrap and stream from other nodes was because we were hitting long GC pauses that actually timed out the stream from one or two nodes.  The most painful part is when you have (for example) 10 or 15 stream sessions going, and ONE fails, then the whole thing failed and you need to start over again.  <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-8150\" target=\"_blank\" rel=\"noopener\">CASSANDRA-8150</a> is still one of the best tickets to review that has a bunch of additional JVM options as well, as well as newer tuning option for G1GC.</p>\n<p>You can also monitor for stream failures by grabbing the Stream Session ID in the cassandra system logs, then searching for that ID in all your logs.  If there is a failure, it will happen very quickly and you don’t have to wait a day for what will eventually be a failed bootstrapping/streaming session.</p>\n<p><strong>Increase memtable_flush_writers when you have lots of data to stream</strong></p>\n<p>The way it was described to us was that since we were streaming a lot of data from many nodes we wanted to increase the number of flush writers because the streams were all hitting the memtables. If you do not have enough writers to deal with a (larger than normal) amount of data hitting them that can cause your streams to fail.  The recommendation we were given was to set this equal to the number of CPUs on the node and it’s worked well for us.</p>\n<p><strong>streaming_socket_timeout_in_ms should be set to a non-zero number</strong></p>\n<p>Finally – we ran into this fun issues described in <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-8472\" target=\"_blank\" rel=\"noopener\">CASSANDRA-8472</a> where stream sessions would hang and not timeout. This was after we made the changes to the heap memory amount and we’re chalking this up to odd AWS networking across AZ’s.</p>\n<p>Luckily this issue is now fixed and upgrading will solve it, but if you haven’t yet (or can’t) upgrade, then set your streaming_socket_timeout_in_ms to some non-zero number.  We set this to 1 hour, which is now coincidentally the default in the now accepted patch in <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-8611\" target=\"_blank\" rel=\"noopener\">CASSANDRA-8611</a>.</p>\n<h2>6. Schema design, or These Aren’t the Rows You’re Looking For</h2>\n<p>The more applications we bring up on top of Cassandra, the more we find it being well suited for models that are either wide and shallow or narrow and deep. It is very similar to CouchDB’s incremental map/reduce in this way.</p>\n<p>A lot of Threat Stack’s UI requires building relationships of different types of events, which can be difficult when the underlying data model is an insert only, partially ordered, write ahead log. For example, when you view a process’s details we have to collect all of the syscall, network, TTY, login/logout, and FIM information for that single process invoke <strong>and</strong> all of its historical runs. Additionally, you could be entering that process’s context from any of those data points, not just the process start. Therefore we maintain multiple lookup tables, tracking a process’s state as it changes in milliseconds or over years (the longest running process we’ve seen is from 2011).</p>\n<p>Some examples of building wide and shallow models are sparse matrices. We have less examples of this in production today, instead driving specific widgets off pre materialized views, but plan to migrate some of the most popular lookups to this model.</p>\n<h2>7. Only Have 2 to 3 Seed Nodes per Data Center</h2>\n<p>Back in February we were seeing very odd behavior that was preventing a node from bootstrapping. This was a big concern because we were having to scale our cluster for disk utilization reasons, so time was against us.</p>\n<p>The short story is that we were hitting the bug documented in <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-5836\">CASSANDRA-5836</a>, where seed nodes were prevented from bootstrapping. The root cause for our cluster was that we were setting every node as a seed node, which is not the expected gossip configuration.</p>\n<p>We have since moved to one seed node per rack (AWS availability zone). This is technically one more than the Datastax recommended count, but gives us cleaner failure scenarios if an AZ becomes unavailable or splits from the rest of the cluster. Additionally, Datastax’s documentation has been improved to now let you know that you should not make every node a seed node.</p>\n<p>From their docs:</p>\n<p><em>Attention: In multiple data-center clusters, the seed list should include at least one node from each data center (replication group). More than a single seed node per data center is recommended for fault tolerance. Otherwise, gossip has to communicate with another data center when bootstrapping a node. Making every node a seed node is <strong class=\"ph b\">not</strong> recommended because of increased maintenance and reduced gossip performance. Gossip optimization is not critical, but it is recommended to use a small seed list (approximately three nodes per data center).</em></p>\n<p><a href=\"http://docs.datastax.com/en/cassandra/2.0/cassandra/operations/ops_add_node_to_cluster_t.html\">http://docs.datastax.com/en/cassandra/2.0/cassandra/operations/ops_add_node_to_cluster_t.html</a></p>\n<p>It might seem fine to make every node a seed node when you only have 3 nodes in a few different AZ’s, but if your configuration management is setup to make every node a seed node, and you add more nodes you’ll soon experience similar problems as us.  Our advice, take the extra time to configure specific seed nodes in your cluster, and when you add new nodes ensure they are not seeds.</p>\n<h2>8. Conclusion</h2>\n<p>Scaling any distributed database is always going to be a challenge, and Cassandra was no different.  Like most technologies, learning how things work is the first step towards making the right choices when it comes to increasing the overall footprint of your cluster.  Following basic system administration principals of making small changes over time, while monitoring the result of those changes can take you a long way toward scaling while maintaining availability and data integrity.</p>\n<p>We continue to ingest billions of unique events daily into our platform and Cassandra continues to be one of the most stable parts of our platform.  That is mainly because we’ve invested time and energy to learn from the community about operating it properly. If this kind of data scaling challenge sounds interesting to you, check out our <a href=\"https://threatstack.com/careers\" target=\"_blank\" rel=\"noopener\">jobs page</a>, or reach out to use directly to hear about current open positions.</p>\n<p><strong>Author’s note</strong>: <em>This post was co-authored by Pete Cheslock, Senior Director of Ops and Support at Threat Stack, who’s support was instrumental in getting this post put together.</em></p>",
        "created_at": "2018-06-20T18:45:25+0000",
        "updated_at": "2019-01-25T14:52:56+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 15,
        "domain_name": "www.threatstack.com",
        "preview_picture": "https://www.threatstack.com/wp-content/uploads/2015/09/2000px-Cassandra_logo.svg_.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10288"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1102,
            "label": "unprocessed",
            "slug": "unprocessed"
          },
          {
            "id": 1128,
            "label": "tutorials",
            "slug": "tutorials"
          },
          {
            "id": 1151,
            "label": "resources",
            "slug": "resources"
          }
        ],
        "is_public": true,
        "id": 10185,
        "uid": "5b5783dde4a2c3.74282858",
        "title": "DataStax Academy",
        "url": "https://academy.datastax.com/",
        "content": "<div class=\"three-column-gray col-md-4 center-block\"><img alt=\"\" src=\"https://academy.datastax.com/sites/all/themes/cassandra_complex/img/Computer-icon-new3.png\" /><p></p><h5>Free Online Learning\nTake our curated, self-paced online courses for deep learning in Apache Cassandra™ and DataStax Enterprise.<a class=\"btn btn-border-white\" href=\"https://academy.datastax.com/courses\">START A COURSE</a></h5></div><div class=\"three-column-gray col-md-4 center-block\"><img alt=\"\" src=\"https://academy.datastax.com/sites/all/themes/cassandra_complex/img/i-icon3.png\" /><p></p><h5>I'm on Fire, Help Me Quick\nSearch our collection of short (under 15 minutes) segments for quick solutions and discussion of specific issues.<a class=\"btn btn-border-white\" href=\"https://academy.datastax.com/find-solutions\">FIND A SOLUTION</a></h5></div><div class=\"three-column-gray col-md-4 center-block\"><img alt=\"\" src=\"https://academy.datastax.com/sites/all/themes/cassandra_complex/img/people-icon3.png\" /><p></p><h5>Join for free, Get Involved\nGet instant access to all of our learning resources, and join our community of DataStax developers.​<a class=\"btn btn-border-white\" href=\"https://academy.datastax.com/user/register\">CREATE AN ACCOUNT</a></h5></div>",
        "created_at": "2018-06-20T18:41:31+0000",
        "updated_at": "2018-07-24T19:54:05+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "academy.datastax.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10185"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 36,
            "label": "solr",
            "slug": "solr"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          }
        ],
        "is_public": false,
        "id": 9956,
        "uid": null,
        "title": "Real-time Customer 360 (Matt Stump, Vorstella) | Cassandra Summit 2016",
        "url": "https://www.slideshare.net/DataStax/realtime-customer-360-matt-stump-vorstella-cassandra-summit-2016/",
        "content": "<p>No notes for slide</p>Constrast that to our solution and you'll immediately see that we're simpler. We'll dig into this in detail, but I wanted to provide you with some context. <p>Challenger Commercial Teaching Step = <br />#6 Your Solution – our solution is unique and better than anyone else </p>Constrast that to our solution and you'll immediately see that we're simpler. We'll dig into this in detail, but I wanted to provide you with some context. <p>Challenger Commercial Teaching Step = <br />#6 Your Solution – our solution is unique and better than anyone else </p>Let's dive into the use case I just mentioned, Customer 360. <p>To an application developer a customer is a collection of lots of different type of data. They've got orders, addresses, wish lists, connections to other users.  </p><p>Graph databases make it easy to model and understand these relationships so that I can develop a full picture of my users. By creating a Customer 360, I can understand more about my user and provide them with a better experience.  </p><p>You can provide better product recommendations, provide a better support experience, target them with more relevant advertisements, and gain a better picture of the total value they represent to my company.</p>",
        "created_at": "2018-06-18T15:11:28+0000",
        "updated_at": "2018-06-18T15:11:39+0000",
        "published_at": null,
        "published_by": [
          "DataStax"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/stumpcustomer360-cassandrasummit2016-161002202637-thumbnail-4.jpg?cb=1475510660",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9956"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 27,
            "label": "search",
            "slug": "search"
          },
          {
            "id": 36,
            "label": "solr",
            "slug": "solr"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          }
        ],
        "is_public": false,
        "id": 9955,
        "uid": null,
        "title": "DataStax | DSE Search 5.0 and Beyond (Nick Panahi & Ariel Weisberg) | Cassandra Summit 2016",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=jvxqhZRBf2E",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/jvxqhZRBf2E?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-06-18T15:09:10+0000",
        "updated_at": "2018-06-18T18:37:17+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/jvxqhZRBf2E/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9955"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9883,
        "uid": null,
        "title": "Understanding Cassandra tombstones – Beyond the lines",
        "url": "https://www.beyondthelines.net/databases/cassandra-tombstones/",
        "content": "<div id=\"ssba-classic-2\" class=\"ssba ssbp-wrap left ssbp--theme-1\"><div><a data-site=\"\" class=\"ssba_twitter_share\" href=\"http://twitter.com/share?url=https://www.beyondthelines.net/databases/cassandra-tombstones/&amp;text=Understanding%20Cassandra%20tombstones%20\" target=\"&quot;_blank&quot;\"><img src=\"https://www.beyondthelines.net/wp-content/plugins/simple-share-buttons-adder/buttons/somacro/twitter.png\" title=\"Twitter\" class=\"ssba ssba-img\" alt=\"Tweet about this on Twitter\" /><div title=\"Twitter\" class=\"ssbp-text\">Twitter</div></a><a data-site=\"linkedin\" class=\"ssba_linkedin_share ssba_share_link\" href=\"http://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.beyondthelines.net/databases/cassandra-tombstones/\" target=\"&quot;_blank&quot;\"><img src=\"https://www.beyondthelines.net/wp-content/plugins/simple-share-buttons-adder/buttons/somacro/linkedin.png\" title=\"LinkedIn\" class=\"ssba ssba-img\" alt=\"Share on LinkedIn\" /><div title=\"Linkedin\" class=\"ssbp-text\">Linkedin</div></a><a data-site=\"\" class=\"ssba_facebook_share\" href=\"http://www.facebook.com/sharer.php?u=https://www.beyondthelines.net/databases/cassandra-tombstones/\" target=\"_blank\"><img src=\"https://www.beyondthelines.net/wp-content/plugins/simple-share-buttons-adder/buttons/somacro/facebook.png\" title=\"Facebook\" class=\"ssba ssba-img\" alt=\"Share on Facebook\" /><div title=\"Facebook\" class=\"ssbp-text\">Facebook</div></a><a data-site=\"\" class=\"ssba_google_share\" href=\"https://plus.google.com/share?url=https://www.beyondthelines.net/databases/cassandra-tombstones/\" target=\"&quot;_blank&quot;\"><img src=\"https://www.beyondthelines.net/wp-content/plugins/simple-share-buttons-adder/buttons/somacro/google.png\" title=\"Google+\" class=\"ssba ssba-img\" alt=\"Share on Google+\" /><div title=\"Google+\" class=\"ssbp-text\">Google+</div></a><a data-site=\"reddit\" class=\"ssba_reddit_share\" href=\"http://reddit.com/submit?url=https://www.beyondthelines.net/databases/cassandra-tombstones/&amp;title=Understanding Cassandra tombstones\" target=\"&quot;_blank&quot;\"><img src=\"https://www.beyondthelines.net/wp-content/plugins/simple-share-buttons-adder/buttons/somacro/reddit.png\" title=\"Reddit\" class=\"ssba ssba-img\" alt=\"Share on Reddit\" /><div title=\"Reddit\" class=\"ssbp-text\">Reddit</div></a></div></div><p>We recently deployed in production a distributed system that uses Cassandra as its persistent storage.</p>\n<p>Not long after we noticed that there were many warnings about tombstones in Cassandra logs. </p>\n<pre class=\"brush: plain; title: ; notranslate\" title=\"\">&#13;\nWARN  [SharedPool-Worker-2] 2017-01-20 16:14:45,153 ReadCommand.java:508 - &#13;\nRead 5000 live rows and 4771 tombstone cells for query &#13;\nSELECT * FROM warehouse.locations WHERE token(address) &gt;= token(D3-DJ-21-B-02) LIMIT 5000 &#13;\n(see tombstone_warn_threshold)&#13;\n</pre>\n<p>We found it quite surprising at first because we’ve only inserted data so far and didn’t expect to see that many tombstones in our database. After asking some people around no one seemed to have a clear explanation on what was going on in Cassandra.</p>\n<p>In fact, the main misconception about tombstones is that people associate it with delete operations. While it’s true that tombstones are generated when data is deleted it is not the only case as we shall see.</p>\n<h4>Looking into sstables</h4>\n<p>Cassandra provides a tool to look at what is stored inside an sstable: sstabledump. This tool comes with the ‘casssandra-tools’ package which is not automatically installed with Cassandra. It’s quite straight-forward to install on a delian-like (e.g. unbuntu) distribution:</p>\n<pre class=\"brush: bash; title: ; notranslate\" title=\"\">&#13;\nsudo apt-get update&#13;\nsudo apt-get install cassandra-tools&#13;\n</pre>\n<p>In this blog post I used ssltabledump to understand how Cassandra stores data and when tombstones are generated.<br />\nThe syntax is pretty straightforward:</p>\n<pre class=\"brush: bash; title: ; notranslate\" title=\"\">&#13;\nsstabledump /var/lib/cassandra/data/warehouse/locations-660dbcb0e4a211e6814a9116fc548b6b/mc-1-big-Data.db&#13;\n</pre>\n<p>sstabledump just takes the sstable file and displays its content as json. Before being able to dump an sstable we need to flush the in-memory data into an sstable file using nodetool:</p>\n<pre class=\"brush: bash; title: ; notranslate\" title=\"\">&#13;\nnodetool flush warehouse locations&#13;\n</pre>\n<p>This command flushes the table ‘locations’ in the ‘warehouse’ keyspace.</p>\n<p>Now that we’re all setup, let’s have a look at some cases that generate tombstones.</p>\n<h4>Null values creates tombstones</h4>\n<p>An upsert operation can generate a tombstone as well. Why? Because Cassandra doesn’t store ‘null’ values. Null means the absence of data. Cassandra returns a ‘null’ value when there is no value for a field. Therefore when a field is set to null Cassandra needs to delete the existing data.</p>\n<pre class=\"brush: sql; title: ; notranslate\" title=\"\">&#13;\nINSERT INTO movements (&#13;\n  id,&#13;\n  address, &#13;\n  item_id, &#13;\n  quantity,&#13;\n  username&#13;\n) VALUES (&#13;\n  103,&#13;\n  'D3-DJ-21-B-02', &#13;\n  '3600029145',&#13;\n  2,&#13;\n  null&#13;\n);&#13;\n</pre>\n<p>This statements removes any existing username value for the movement identified by the id 103. And how does Cassandra remove data ? Yes, by inserting a tombstone.</p>\n<p><a href=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/movement-tombstone-1.png\"><img src=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/movement-tombstone-1.png\" alt=\"\" width=\"800\" height=\"149\" class=\"aligncenter size-full wp-image-858\" srcset=\"https://www.beyondthelines.net/wp-content/uploads/2017/01/movement-tombstone-1.png 800w, https://www.beyondthelines.net/wp-content/uploads/2017/01/movement-tombstone-1-300x56.png 300w, https://www.beyondthelines.net/wp-content/uploads/2017/01/movement-tombstone-1-768x143.png 768w\" /></a></p>\n<p>This is the corresponding ssltabledump output:</p>\n<pre class=\"brush: jscript; title: ; notranslate\" title=\"\">&#13;\n[&#13;\n  {&#13;\n    \"partition\" : {&#13;\n      \"key\" : [ \"103\" ],&#13;\n      \"position\" : 0&#13;\n    },&#13;\n    \"rows\" : [&#13;\n      {&#13;\n        \"type\" : \"row\",&#13;\n        \"position\" : 18,&#13;\n        \"liveness_info\" : { \"tstamp\" : \"2017-01-27T15:09:50.065224Z\" },&#13;\n        \"cells\" : [&#13;\n          { \"name\" : \"address\", \"value\" : \"D3-DJ-21-B-02\" },&#13;\n          { \"name\" : \"item_d\", \"value\" : \"3600029145\" },&#13;\n          { \"name\" : \"quantity\", \"value\" : \"2\" },&#13;\n          { \"name\" : \"username\", \"deletion_info\" : { \"local_delete_time\" : \"2017-01-27T15:09:50Z\" }&#13;\n        ]&#13;\n      }&#13;\n    ]&#13;\n  }&#13;\n]&#13;\n</pre>\n<p>Cassandra is designed for optimised performance and every operation is written to an append-only log. When a data is removed we can’t removed the existing value from the log, instead a “tombstone” value is inserted in the log. </p>\n<p>Moreover Cassandra doesn’t perform read before write (except for light-weight transactions) as it would be too expensive.</p>\n<p>Therefore when the above insert is executed Cassandra insert a tombstone value for the username field (even if there was no existing data for this key before).</p>\n<p>Now let’s consider the following statement that looks very similar to the previous one:</p>\n<pre class=\"brush: sql; title: ; notranslate\" title=\"\">&#13;\nINSERT INTO movements (&#13;\n  id,&#13;\n  address, &#13;\n  item_id, &#13;\n  quantity&#13;\n) VALUES (&#13;\n  103,&#13;\n  'D3-DJ-21-B-02', &#13;\n  '3600029145',&#13;\n  2&#13;\n);&#13;\n</pre>\n<p>But there is one difference. The first statement creates a tombstone for the username whereas the second statement doesn’t insert anything in that column.</p>\n<p><a href=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/movement-row-1.png\"><img src=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/movement-row-1.png\" alt=\"\" width=\"700\" height=\"195\" class=\"aligncenter size-full wp-image-857\" srcset=\"https://www.beyondthelines.net/wp-content/uploads/2017/01/movement-row-1.png 700w, https://www.beyondthelines.net/wp-content/uploads/2017/01/movement-row-1-300x84.png 300w\" /></a></p>\n<pre class=\"brush: jscript; title: ; notranslate\" title=\"\">&#13;\n[&#13;\n  {&#13;\n    \"partition\" : {&#13;\n      \"key\" : [ \"103\" ],&#13;\n      \"position\" : 0&#13;\n    },&#13;\n    \"rows\" : [&#13;\n      {&#13;\n        \"type\" : \"row\",&#13;\n        \"position\" : 18,&#13;\n        \"liveness_info\" : { \"tstamp\" : \"2017-01-27T15:09:50.065224Z\" },&#13;\n        \"cells\" : [&#13;\n          { \"name\" : \"address\", \"value\" : \"D3-DJ-21-B-02\" },&#13;\n          { \"name\" : \"item_d\", \"value\" : \"3600029145\" },&#13;\n          { \"name\" : \"quantity\", \"value\" : \"2\" }&#13;\n        ]&#13;\n      }&#13;\n    ]&#13;\n  }&#13;\n]&#13;\n</pre>\n<p>If this is the first insert for this key (no previously existing data) then both statements yield to the same state, except that the second one doesn’t insert an unnecessary tombstone.</p>\n<p>If there is existing data then what ends up in the username column might be different. With statement 1 whatever data was there it is deleted with the tombstone and no longer returned. With statement 2 the username remains unchanged so whatever value was there before (if any) will get returned.</p>\n<p><a href=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/movement-upsert-1.png\"><img src=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/movement-upsert-1.png\" alt=\"\" width=\"700\" height=\"245\" class=\"aligncenter size-full wp-image-859\" srcset=\"https://www.beyondthelines.net/wp-content/uploads/2017/01/movement-upsert-1.png 700w, https://www.beyondthelines.net/wp-content/uploads/2017/01/movement-upsert-1-300x105.png 300w\" /></a></p>\n<p>Therefore you should strive to only update the fields that you need to. </p>\n<p>For instance let’s say that I need to update the status of a location. Then I should only update the status field rather than the whole object. That would avoid tombstones for every missing value in the object.</p>\n<figure id=\"attachment_855\" class=\"wp-caption aligncenter\"><a href=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/location-row-1.png\"><img src=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/location-row-1.png\" alt=\"\" width=\"450\" height=\"190\" class=\"size-full wp-image-855\" srcset=\"https://www.beyondthelines.net/wp-content/uploads/2017/01/location-row-1.png 450w, https://www.beyondthelines.net/wp-content/uploads/2017/01/location-row-1-300x127.png 300w\" /></a><figcaption class=\"wp-caption-text\">The ‘properties’ field is not set in the query so no value is stored in Cassandra.</figcaption></figure><p>The following statement is exactly what we need as it only sets the status field:</p>\n<pre class=\"brush: sql; title: ; notranslate\" title=\"\">&#13;\nUPDATE locations SET status = 'damaged' WHERE location_address = 'D3-DJ-21-B-02';&#13;\n</pre>\n<p>as the sstable dump shows</p>\n<pre class=\"brush: jscript; title: ; notranslate\" title=\"\">&#13;\n[&#13;\n  {&#13;\n    \"partition\" : {&#13;\n      \"key\" : [ \"D3-DJ-21-B-02\" ],&#13;\n      \"position\" : 0&#13;\n    },&#13;\n    \"rows\" : [&#13;\n      {&#13;\n        \"type\" : \"row\",&#13;\n        \"position\" : 28,&#13;\n        \"cells\" : [&#13;\n          { \"name\" : \"status\", \"value\" : \"damaged\", \"tstamp\" : \"2017-01-28T11:55:18.146255Z\" }&#13;\n        ]&#13;\n      }&#13;\n    ]&#13;\n  }&#13;\n]&#13;\n</pre>\n<p>Compare it with the following one which saves the whole location object (which happens to not have any properties – and insert an unnecessary tombstone in the ‘properties’ column).</p>\n<pre class=\"brush: sql; title: ; notranslate\" title=\"\">&#13;\nINSERT INTO locations (&#13;\n  address,&#13;\n  status,&#13;\n  properties&#13;\n) VALUES (&#13;\n  'D3-DJ-21-B-02',&#13;\n  'damaged',&#13;\n  null&#13;\n);&#13;\n</pre>\n<figure id=\"attachment_854\" class=\"wp-caption aligncenter\"><a href=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/location-empty-collection-1.png\"><img src=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/location-empty-collection-1.png\" alt=\"\" width=\"500\" height=\"189\" class=\"size-full wp-image-854\" srcset=\"https://www.beyondthelines.net/wp-content/uploads/2017/01/location-empty-collection-1.png 500w, https://www.beyondthelines.net/wp-content/uploads/2017/01/location-empty-collection-1-300x113.png 300w\" /></a><figcaption class=\"wp-caption-text\">An empty collection is stored as a tombstone cell</figcaption></figure><pre class=\"brush: jscript; title: ; notranslate\" title=\"\">&#13;\n[&#13;\n  {&#13;\n    \"partition\" : {&#13;\n      \"key\" : [ \"D3-DJ-21-B-02\" ],&#13;\n      \"position\" : 0&#13;\n    },&#13;\n    \"rows\" : [&#13;\n      {&#13;\n        \"type\" : \"row\",&#13;\n        \"position\" : 18,&#13;\n        \"liveness_info\" : { \"tstamp\" : \"2017-01-28T11:58:59.160898Z\" },&#13;\n        \"cells\" : [&#13;\n          { \"name\" : \"status\", \"value\" : \"damaged\" },&#13;\n          { \"name\" : \"properties\", \"deletion_info\" : { \"marked_deleted\" : \"2017-01-28T11:58:59.160897Z\", \"local_delete_time\" : \"2017-01-28T11:58:59Z\" } }&#13;\n        ]&#13;\n      }&#13;\n    ]&#13;\n  }&#13;\n]&#13;\n</pre>\n<h4>Be aware of the collection types</h4>\n<p>In the previous example the ‘properties’ field is a collection type (most likely a set), so let’s talk about collections as they are trickier than it looks.</p>\n<p>Let’s create a new location with the following statement</p>\n<pre class=\"brush: sql; title: ; notranslate\" title=\"\">&#13;\nINSERT INTO locations (&#13;\n  address,&#13;\n  status,&#13;\n  properties&#13;\n) VALUES (&#13;\n  'C3-BE-52-C-01',&#13;\n  'normal',&#13;\n  {'pickable'}&#13;\n);&#13;\n</pre>\n<p>Everything looks good, doesn’t it? Every field has a value, so no tombstone expected. And yet, this statement does create a tombstone for the ‘properties’ field.</p>\n<pre class=\"brush: jscript; title: ; notranslate\" title=\"\">&#13;\n[&#13;\n  {&#13;\n    \"partition\" : {&#13;\n      \"key\" : [ \"C3-BE-52-C-01\" ],&#13;\n      \"position\" : 0&#13;\n    },&#13;\n    \"rows\" : [&#13;\n      {&#13;\n        \"type\" : \"row\",&#13;\n        \"position\" : 18,&#13;\n        \"liveness_info\" : { \"tstamp\" : \"2017-01-28T12:01:00.256789Z\" },&#13;\n        \"cells\" : [&#13;\n          { \"name\" : \"status\", \"value\" : \"normal\" },&#13;\n          { \"name\" : \"properties\", \"deletion_info\" : { \"marked_deleted\" : \"2017-01-28T12:01:00.256788Z\", \"local_delete_time\" : \"2017-01-28T12:01:00Z\" } },&#13;\n          { \"name\" : \"properties\", \"path\" : [ \"pickable\" ], \"value\" : \"\" }&#13;\n        ]&#13;\n      }&#13;\n    ]&#13;\n  }&#13;\n]&#13;\n</pre>\n<p>To understand why, we need to look at how Cassandra store a collection in the underlying storage.</p>\n<figure id=\"attachment_853\" class=\"wp-caption aligncenter\"><a href=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/location-collection-1.png\"><img src=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/location-collection-1.png\" alt=\"\" width=\"500\" height=\"175\" class=\"size-full wp-image-853\" srcset=\"https://www.beyondthelines.net/wp-content/uploads/2017/01/location-collection-1.png 500w, https://www.beyondthelines.net/wp-content/uploads/2017/01/location-collection-1-300x105.png 300w\" /></a><figcaption class=\"wp-caption-text\">The collection field includes a tombstone cell to empty the collection before adding a value.</figcaption></figure><p>Cassandra appends new values to the set, so when we want the collection to contain only the values passed in the query, we have to remove everything that might have been there before. That’s why Cassandra inserts a tombstone and then our value. This makes sure the set now contains only the ‘pickable’ value whatever was there before.</p>\n<p>That’s one more reason to just set the values you need to update and nothing more.</p>\n<h4>Be careful with materialised views</h4>\n<p>A materialised view is a table that is maintained by Cassandra. One of its main feature is that we can define a different primary key than the one in the base table. You can re-order the fields of the primary key from the base table, but you can also add one extra field into the primary key of the view.</p>\n<p>This is great as it allows to define a different partitioning or clustering but it also generates more tombstones in the view. Let’s consider an example to understand what’s happening.</p>\n<p>Imagine that we need to query the locations by status. For instance we want to retrieve all ‘damaged’ locations. We can create a materialised view to support this use case.</p>\n<pre class=\"brush: sql; title: ; notranslate\" title=\"\">&#13;\nCREATE MATERIALIZED VIEW locations_by_status AS&#13;\n  SELECT&#13;\n    status,&#13;\n    address,&#13;\n    properties&#13;\n  FROM locations&#13;\n  WHERE status IS NOT NULL&#13;\n  AND address IS NOT NULL&#13;\n  PRIMARY KEY (status, address);&#13;\n</pre>\n<p>Good, now we can use this view to find out all the locations with a given status. </p>\n<p>But let’s consider what happens in the view when we change the status of a location with an update query</p>\n<pre class=\"brush: sql; title: ; notranslate\" title=\"\">&#13;\nUPDATE locations&#13;\nSET status = 'damaged'&#13;\nWHERE address = 'C3-BE-52-C-01';&#13;\n</pre>\n<p>As we’ve seen this query just updates one field in the base table (locations) and doesn’t generate any tombstone in this table. However in the materialised view the ‘status’ field is part of the primary key. When the status changes the partition key changes as well.</p>\n<pre class=\"brush: jscript; title: ; notranslate\" title=\"\">&#13;\n[&#13;\n  {&#13;\n    \"partition\" : {&#13;\n      \"key\" : [ \"normal\" ],&#13;\n      \"position\" : 0&#13;\n    },&#13;\n    \"rows\" : [&#13;\n      {&#13;\n        \"type\" : \"row\",&#13;\n        \"position\" : 18,&#13;\n        \"clustering\" : [ \"C3-BE-52-C-01\" ],&#13;\n        \"deletion_info\" : { \"marked_deleted\" : \"2017-01-20T10:34:27.707604Z\", \"local_delete_time\" : \"2017-01-20T10:46:14Z\" },&#13;\n        \"cells\" : [ ]&#13;\n      }&#13;\n    ]&#13;\n  },&#13;\n  {&#13;\n    \"partition\" : {&#13;\n      \"key\" : [ \"damaged\" ],&#13;\n      \"position\" : 31&#13;\n    },&#13;\n    \"rows\" : [&#13;\n      {&#13;\n        \"type\" : \"row\",&#13;\n        \"position\" : 49,&#13;\n        \"clustering\" : [ \"C3-BE-52-C-01\" ],&#13;\n        \"liveness_info\" : { \"tstamp\" : \"2017-01-20T10:46:14.285730Z\" },&#13;\n        \"cells\" : [&#13;\n          { \"name\" : \"properties\", \"deletion_info\" : { \"marked_deleted\" : \"2017-01-20T10:46:14.285729Z\", \"local_delete_time\" : \"2017-01-20T10:46:14Z\" } }&#13;\n        ]&#13;\n      }&#13;\n    ]&#13;\n  }&#13;\n]&#13;\n</pre>\n<p>To maintain the view in sync with the base table Cassandra needs to delete the row from the existing partition and insert a new one into the new partition. And a delete means a tombstone.</p>\n<figure id=\"attachment_856\" class=\"wp-caption aligncenter\"><a href=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/materialized-view-tombstones-1.png\"><img src=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/materialized-view-tombstones-1.png\" alt=\"\" width=\"800\" height=\"398\" class=\"size-full wp-image-856\" srcset=\"https://www.beyondthelines.net/wp-content/uploads/2017/01/materialized-view-tombstones-1.png 800w, https://www.beyondthelines.net/wp-content/uploads/2017/01/materialized-view-tombstones-1-300x149.png 300w, https://www.beyondthelines.net/wp-content/uploads/2017/01/materialized-view-tombstones-1-768x382.png 768w\" /></a><figcaption class=\"wp-caption-text\">The update in the base table triggers a partition change in the materialised view which creates a tombstone to remove the row from the old partition.</figcaption></figure><p>The key thing here is to be thoughtful when designing the primary key of a materialised view (especially when the key contains more fields than the key of the base table). That being said it might be the only solution and completely worth it. </p>\n<p>Also consider the rate of the changes of the fields of the primary key. In our case we should evaluate the rate at which the status changes for a given location (the location address doesn’t change). The less often the better off we are with respect to tombstones.</p>\n<p>Finally tombstones will disappear over time, when compaction occurs. The typical delay is 10 days (which corresponds to the ‘gc_grace_seconds’ configuration parameter). You may want to adjust it if there are too many tombstones generated during this period.</p>\n<h4>Conclusion</h4>\n<p>As we’ve seen tombstones can be tricky and there not only associated to delete operations. There are many other cases that may generate tombstones. </p>\n<p>Tombstones are not necessarily a bad thing that we should avoid at all cost. It’s just a way to delete data in an append-only structure. </p>\n<p>However it can affect performances so you’d better be aware when they are generated when designing your data model and queries. </p>\n<p>If you use the java driver the query fails if more than 100,000 tombstones are seen.</p>\n<p>With this knowledge you should be able to limit their generation only when necessary. In this case you should evaluate how many are going to be generated and evaluate if it’s going to be an issue or not. If so you may want to tune the ‘gc_grace_seconds’ parameter to trigger compaction more often.</p>",
        "created_at": "2018-06-08T15:27:36+0000",
        "updated_at": "2018-06-08T15:27:49+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_GB",
        "reading_time": 10,
        "domain_name": "www.beyondthelines.net",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9883"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 35,
            "label": "docker",
            "slug": "docker"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9860,
        "uid": null,
        "title": "datastax/docker-images",
        "url": "https://github.com/datastax/docker-images",
        "content": "<ul><li><a href=\"#datastax-platform-overview\">DataStax Platform Overview</a></li>\n<li><a href=\"#getting-started-with-datastax-and-docker\">Getting Started with DataStax and Docker</a></li>\n<li><a href=\"#prerequisites\">Prerequisites</a></li>\n<li><a href=\"https://github.com/datastax/docker-images/blob/master/next-steps\">Next Steps</a></li>\n<li><a href=\"#building\">Building</a></li>\n<li><a href=\"#quick-reference\">Quick Reference</a></li>\n<li><a href=\"#license\">Licensing</a></li>\n</ul><h3>Where to get help:</h3><p><a href=\"https://academy.datastax.com/\" rel=\"nofollow\">DataStax Academy</a>, <a href=\"https://academy.datastax.com/slack\" rel=\"nofollow\">DataStax Slack</a></p><p>For documentation and tutorials head over to <a href=\"https://academy.datastax.com/quick-downloads?utm_campaign=Docker_2019&amp;utm_medium=web&amp;utm_source=docker&amp;utm_term=-&amp;utm_content=Web_Academy_Downloads\" rel=\"nofollow\">DataStax Academy</a>. On Academy you’ll find everything you need to configure and deploy the DataStax Docker Images.</p><p>Featured Tutorial - <a href=\"https://academy.datastax.com/resources/guided-tour-dse-6-using-docker\" rel=\"nofollow\">DataStax Enterprise 6 Guided Tour</a></p><p>Built on the best distribution of Apache Cassandra™, DataStax Enterprise is the always-on database designed to allow you to effortlessly build and scale your apps, integrating graph, search, analytics, administration, developer tooling, and monitoring into a single unified platform. We power your apps' real-time moments so you can create instant insights and powerful customer experiences.</p><p><a target=\"_blank\" href=\"https://camo.githubusercontent.com/a33c8b4c94fd9610153f34005f50d1a204ffdad1/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f652f65352f44617461537461785f4c6f676f2e706e67\"><img src=\"https://camo.githubusercontent.com/a33c8b4c94fd9610153f34005f50d1a204ffdad1/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f652f65352f44617461537461785f4c6f676f2e706e67\" alt=\"\" data-canonical-src=\"https://upload.wikimedia.org/wikipedia/commons/e/e5/DataStax_Logo.png\" /></a></p><p>DataStax Docker images are licensed only for Development purposes in non-production environments. You can use these images to learn <a href=\"https://hub.docker.com/r/datastax/dse-server\" rel=\"nofollow\">DSE</a>, <a href=\"https://hub.docker.com/r/datastax/dse-opscenter\" rel=\"nofollow\">OpsCenter</a> and <a href=\"https://hub.docker.com/r/datastax/dse-studio\" rel=\"nofollow\">DataStax Studio</a>, to try new ideas, to test and demonstrate your application.</p><ul><li>\n<p>Basic understanding of Docker images and containers.</p>\n</li>\n<li>\n<p>Docker installed on your local system, see <a href=\"https://docs.docker.com/engine/installation/\" rel=\"nofollow\">Docker Installation Instructions</a>.</p>\n</li>\n<li>\n<p>When <a href=\"#building\">building</a> custom images from the DataStax github repository, a <a href=\"https://academy.datastax.com/\" rel=\"nofollow\">DataStax Academy account</a>.</p>\n</li>\n</ul><p>For documentation including configuration options, environment variables, and compose examples head over to <a href=\"https://academy.datastax.com/quick-downloads?utm_campaign=Docker_2019&amp;utm_medium=web&amp;utm_source=docker&amp;utm_term=-&amp;utm_content=Web_Academy_Downloads\" rel=\"nofollow\">DataStax Academy</a>.</p><p>On Academy you’ll also find step by step tutorials and examples.</p><p>The code in this repository will build the images listed above. To build all of them please run the following commands:</p><div class=\"highlight highlight-text-shell-session\"><pre>./gradlew buildImages -PdownloadUsername=&lt;your_DataStax_Acedemy_username&gt; -PdownloadPassword=&lt;your_DataStax_Acedemy_passwd&gt;</pre></div><p>By default, <a href=\"https://gradle.org\" rel=\"nofollow\">Gradle</a> will download DataStax tarballs from <a href=\"https://downloads.datastax.com\" rel=\"nofollow\">DataStax Academy</a>.\nTherefore you need to provide your credentials either via the command line, or in <code>gradle.properties</code> file located\nin the project root.</p><p>Run <code>./gradlew tasks</code> to get the list of all available tasks.</p><p>Use the following links to review the license:</p><ul><li><a href=\"https://www.datastax.com/terms\" rel=\"nofollow\">DataStax License Terms</a></li>\n</ul>",
        "created_at": "2018-05-31T23:24:59+0000",
        "updated_at": "2018-05-31T23:25:05+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/573369?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9860"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 35,
            "label": "docker",
            "slug": "docker"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9814,
        "uid": null,
        "title": "thelastpickle/docker-cassandra-bootstrap",
        "url": "https://github.com/thelastpickle/docker-cassandra-bootstrap",
        "content": "<p>A new blog post covering each of the main components of this project can be found here:</p><p><a href=\"http://thelastpickle.com/blog/2018/01/23/docker-meet-cassandra.html\" rel=\"nofollow\">http://thelastpickle.com/blog/2018/01/23/docker-meet-cassandra.html</a></p><div class=\"highlight highlight-source-shell\"><pre>git clone git@github.com:thelastpickle/docker-cassandra-bootstrap.git\ncd docker-cassandra-bootstrap\ncp .env.template .env\ndocker-compose build</pre></div><p>If you would like to see a hosted log service interact seemlessly with this\nDocker Compose stack, sign up for <a href=\"https://papertrailapp.com/?thank=1ad15b\" rel=\"nofollow\">Papertrail</a>.</p><p>Then find your specific port number by looking at your\n<a href=\"https://papertrailapp.com/account/destinations\" rel=\"nofollow\">Log Destinations</a> and update\nyour <code>.env</code> setting accordingly.</p><div class=\"highlight highlight-source-shell\"><pre># turn off all running Docker containers\ndocker-compose down\n# delete any persistent data\nrm -rf data/\n# rebuild the images\ndocker-compose build</pre></div><p>Start our Docker-integrated logging connector:</p><div class=\"highlight highlight-source-shell\"><pre># start Docker logging connector\ndocker-compose up logspout\n# view logging HTTP endpoint\ncurl http://localhost:8000/logs</pre></div><p>Start Cassandra and setup the required schema:</p><div class=\"highlight highlight-source-shell\"><pre># start Cassandra\ndocker-compose up cassandra\n# view cluster status\ndocker-compose run nodetool status\n# create schema\ndocker-compose run cqlsh -f /schema.cql\n# confirm schema\ndocker-compose run cqlsh -e \"DESCRIBE SCHEMA;\"</pre></div><p>Start Reaper for Apache Cassandra and monitor your new cluster:</p><div class=\"highlight highlight-source-shell\"><pre># start Reaper for Apache Cassandra\ndocker-compose up cassandra-reaper\nopen http://localhost:8080/webui/\n# add one-off repair\n# add scheduled repair</pre></div><p>Start Prometheus and become familiar with the UI:</p><div class=\"highlight highlight-source-shell\"><pre># start Prometheus\ndocker-compose up prometheus\nopen http://localhost:9090</pre></div><p>Start Grafana, connect it to the Prometheus data source, and upload the TLP\nDashboards.</p><div class=\"highlight highlight-source-shell\"><pre># start Grafana\ndocker-compose up grafana\n# create \n./grafana/bin/create-data-sources.sh\n# user/pass: admin/admin\nopen http://localhost:3000\n# upload dashboards\n./grafana/bin/upload-dashboards.sh</pre></div><p>Generate fake workforce and activity:</p><div class=\"highlight highlight-source-shell\"><pre>docker-compose run pickle-factory</pre></div><p>Sample timesheets:</p><div class=\"highlight highlight-source-shell\"><pre>docker-compose run pickle-shop</pre></div>",
        "created_at": "2018-05-25T21:15:40+0000",
        "updated_at": "2018-08-03T00:01:48+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/30403496?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9814"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 907,
            "label": "mesos",
            "slug": "mesos"
          },
          {
            "id": 956,
            "label": "streaming",
            "slug": "streaming"
          }
        ],
        "is_public": false,
        "id": 9790,
        "uid": null,
        "title": "Realtime Data Pipeline with Spark Streaming and Cassandra with Mesos …",
        "url": "https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1",
        "content": "Realtime Data Pipeline with Spark Streaming and Cassandra with Mesos …\n      \n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            </a><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div>\n    <div class=\"wrapper\"><p>Successfully reported this slideshow.</p><div id=\"slideview-container\" class=\"\"><div class=\"row\"><div id=\"main-panel\" class=\"small-12 large-8 columns\"><div class=\"sectionElements\"><div class=\"playerWrapper\"><div><div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\">Realtime Data Pipeline with Spark Streaming and Cassandra with Mesos (Rahul Kumar, Sigmoid) | C* Summit 2016<div class=\"stage valign-first-slide\"><div class=\"slide_container\"><section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-1-638.jpg?cb=1475599971\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-1-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-1-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-1-1024.jpg?cb=1475599971\" alt=\"Rahul Kumar&#10;Technical Lead&#10;Sigmoid&#10;Real Time data pipeline with Spark Streaming and&#10;Cassandra with Mesos&#10;\" /></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-2-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-2-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-2-1024.jpg?cb=1475599971\" alt=\"About Sigmoid&#10;© DataStax, All Rights Reserved. 2&#10;We build reactive real-time big data systems.&#10;\" /></i></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-3-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-3-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-3-1024.jpg?cb=1475599971\" alt=\"1 Data Management&#10;2 Cassandra Introduction&#10;3 Apache Spark Streaming&#10;4 Reactive Data Pipelines&#10;5 Use cases&#10;3© DataStax, All...\" /></i></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-4-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-4-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-4-1024.jpg?cb=1475599971\" alt=\"Data Management&#10;© DataStax, All Rights Reserved. 4&#10;Managing data and analyzing&#10;data have always greatest&#10;benefit and the g...\" /></i></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-5-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-5-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-5-1024.jpg?cb=1475599971\" alt=\"Three V’s of Big data&#10;© DataStax, All Rights Reserved. 5&#10;\" /></i></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-6-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-6-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-6-1024.jpg?cb=1475599971\" alt=\"Scale Vertically&#10;© DataStax, All Rights Reserved. 6&#10;\" /></i></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-7-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-7-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-7-1024.jpg?cb=1475599971\" alt=\"Scale Horizontally&#10;© DataStax, All Rights Reserved. 7&#10;\" /></i></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-8-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-8-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-8-1024.jpg?cb=1475599971\" alt=\"Understanding Distributed Application&#10;© DataStax, All Rights Reserved. 8&#10;“ A distributed system is a software system in wh...\" /></i></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-9-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-9-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-9-1024.jpg?cb=1475599971\" alt=\"Principles Of Distributed Application Design&#10;© DataStax, All Rights Reserved. 9&#10; Availability&#10; Performance&#10; Reliability...\" /></i></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-10-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-10-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-10-1024.jpg?cb=1475599971\" alt=\"Reactive Application&#10;© DataStax, All Rights Reserved. 10&#10;\" /></i></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-11-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-11-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-11-1024.jpg?cb=1475599971\" alt=\"Reactive libraries, tools and frameworks&#10;© DataStax, All Rights Reserved. 11&#10;\" /></i></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-12-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-12-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-12-1024.jpg?cb=1475599971\" alt=\"Cassandra Introduction&#10;© DataStax, All Rights Reserved. 13&#10;Cassandra - is an Open Source, distributed store for structured...\" /></i></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-13-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-13-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-13-1024.jpg?cb=1475599971\" alt=\"Why Cassandra&#10;© DataStax, All Rights Reserved. 14&#10;\" /></i></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-14-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-14-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-14-1024.jpg?cb=1475599971\" alt=\"Highly scalable NoSQL database&#10;© DataStax, All Rights Reserved. 15&#10; Cassandra supplies linear&#10;scalability&#10; Cassandra is ...\" /></i></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-15-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-15-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-15-1024.jpg?cb=1475599971\" alt=\"High Availability&#10;© DataStax, All Rights Reserved. 16&#10; In a Cassandra cluster all&#10;nodes are equal.&#10; There are no masters...\" /></i></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-16-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-16-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-16-1024.jpg?cb=1475599971\" alt=\"Read/Write any where&#10;© DataStax, All Rights Reserved. 17&#10; Cassandra is a R/W&#10;anywhere architecture, so&#10;any user/app can c...\" /></i></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-17-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-17-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-17-1024.jpg?cb=1475599971\" alt=\"High Performance&#10;© DataStax, All Rights Reserved. 18&#10; All disk writes are&#10;sequential, append-only&#10;operations.&#10; Ensure No...\" /></i></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-18-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-18-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-18-1024.jpg?cb=1475599971\" alt=\"Cassandra &amp; CAP&#10;© DataStax, All Rights Reserved. 19&#10; Cassandra is classified as&#10;an AP system&#10; System is still available&#10;...\" /></i></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-19-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-19-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-19-1024.jpg?cb=1475599971\" alt=\"CQL&#10;© DataStax, All Rights Reserved. 20&#10;CREATE KEYSPACE MyAppSpace WITH&#10;REPLICATION = { 'class' : 'SimpleStrategy', 'repli...\" /></i></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-20-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-20-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-20-1024.jpg?cb=1475599971\" alt=\"Apache Spark&#10;© DataStax, All Rights Reserved. 21&#10;Introduction&#10; Apache Spark is a fast and&#10;general execution engine&#10;for la...\" /></i></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-21-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-21-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-21-1024.jpg?cb=1475599971\" alt=\"RDD Introduction&#10;© DataStax, All Rights Reserved. 22&#10;Resilient Distributed Datasets (RDDs), a distributed memory&#10;abstracti...\" /></i></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-22-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-22-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-22-1024.jpg?cb=1475599971\" alt=\"RDD Operations&#10;© DataStax, All Rights Reserved. 23&#10;Two Kind of Operations&#10;• Transformation&#10;• Action&#10;\" /></i></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-23-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-23-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-23-1024.jpg?cb=1475599971\" alt=\"What is Spark Streaming?&#10;© DataStax, All Rights Reserved. 26&#10;Framework for large scale stream processing&#10;➔ Created at UC B...\" /></i></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-24-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-24-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-24-1024.jpg?cb=1475599971\" alt=\"Spark Streaming&#10;© DataStax, All Rights Reserved. 27&#10;Introduction&#10;• Spark Streaming is an&#10;extension of the core spark&#10;API t...\" /></i></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-25-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-25-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-25-1024.jpg?cb=1475599971\" alt=\"Spark Streaming over a HA Mesos Cluster&#10;© DataStax, All Rights Reserved. 31&#10;To use Mesos from Spark, you need a Spark bina...\" /></i></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-26-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-26-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-26-1024.jpg?cb=1475599971\" alt=\"Spark Cassandra Connector&#10;© DataStax, All Rights Reserved. 32&#10; It allows us to expose Cassandra tables as Spark RDDs&#10; Wr...\" /></i></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-27-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-27-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-27-1024.jpg?cb=1475599971\" alt=\"© DataStax, All Rights Reserved. 33&#10;resolvers += &quot;Spark Packages Repo&quot; at &quot;https://dl.bintray.com/spark-packages/maven&quot;&#10;li...\" /></i></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-28-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-28-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-28-1024.jpg?cb=1475599971\" alt=\"© DataStax, All Rights Reserved. 34&#10;val rdd = sc.cassandraTable(“applog”, “accessTable”)&#10;println(rdd.count)&#10;println(rdd.fi...\" /></i></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-29-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-29-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-29-1024.jpg?cb=1475599971\" alt=\"Many more higher order functions:&#10;© DataStax, All Rights Reserved. 35&#10;repartitionByCassandraReplica : It be used to reloca...\" /></i></section><section data-index=\"30\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-30-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-30-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-30-1024.jpg?cb=1475599971\" alt=\"Hint to scalable pipeline&#10;© DataStax, All Rights Reserved. 36&#10;Figure out the bottleneck : CPU, Memory, IO, Network&#10;If pars...\" /></i></section><section data-index=\"31\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-31-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-31-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-31-1024.jpg?cb=1475599971\" alt=\"Thank You&#10;@rahul_kumar_aws&#10;\" /></i></section><section data-index=\"32\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-32-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-32-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-32-1024.jpg?cb=1475599971\" alt=\"Realtime Data Pipeline with Spark Streaming and Cassandra with Mesos (Rahul Kumar, Sigmoid) | C* Summit 2016\" /></i></section><section data-index=\"33\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-33-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-33-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-33-1024.jpg?cb=1475599971\" alt=\"Realtime Data Pipeline with Spark Streaming and Cassandra with Mesos (Rahul Kumar, Sigmoid) | C* Summit 2016\" /></i></section><section data-index=\"34\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-34-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-34-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-34-1024.jpg?cb=1475599971\" alt=\"Realtime Data Pipeline with Spark Streaming and Cassandra with Mesos (Rahul Kumar, Sigmoid) | C* Summit 2016\" /></i></section><section data-index=\"35\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-35-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-35-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-35-1024.jpg?cb=1475599971\" alt=\"Realtime Data Pipeline with Spark Streaming and Cassandra with Mesos (Rahul Kumar, Sigmoid) | C* Summit 2016\" /></i></section><section data-index=\"36\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-36-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-36-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-36-1024.jpg?cb=1475599971\" alt=\"Realtime Data Pipeline with Spark Streaming and Cassandra with Mesos (Rahul Kumar, Sigmoid) | C* Summit 2016\" /></i></section><section data-index=\"37\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-37-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-37-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-37-1024.jpg?cb=1475599971\" alt=\"Realtime Data Pipeline with Spark Streaming and Cassandra with Mesos (Rahul Kumar, Sigmoid) | C* Summit 2016\" /></i></section><section data-index=\"38\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-38-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-38-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-38-1024.jpg?cb=1475599971\" alt=\"Realtime Data Pipeline with Spark Streaming and Cassandra with Mesos (Rahul Kumar, Sigmoid) | C* Summit 2016\" /></i></section><section data-index=\"39\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-39-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-39-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-39-1024.jpg?cb=1475599971\" alt=\"Realtime Data Pipeline with Spark Streaming and Cassandra with Mesos (Rahul Kumar, Sigmoid) | C* Summit 2016\" /></i></section><div class=\"j-next-container next-container\"><div class=\"content-container\"><div class=\"next-slideshow-wrapper\"><div class=\"j-next-slideshow next-slideshow\"><p>Upcoming SlideShare</p></div><p>Loading in …5</p><p>×</p></div></div></div></div></div></div></div></div></div><div class=\"slideshow-info-container\" itemscope=\"itemscope\" itemtype=\"https://schema.org/MediaObject\"><div class=\"slideshow-tabs-container show-for-medium-up\"><ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    0 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\">\n                \n                  1 Like\n                \n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\">\n                Statistics\n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\">\n                Notes\n              </i></a>\n            </li>\n          </ul><div class=\"tabs-content\"><div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\"><ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"itemscope\"><li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"ThorstenHeimes\" rel=\"nofollow\" href=\"https://www.slideshare.net/ThorstenHeimes?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Thorsten Heimes\n                            \n                              \n                                , \n                                Data Engineer at Fineway GmbH\n                              \n                              \n                                 at \n                                Fineway GmbH\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n              </ul></div><div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No Downloads</p></div><div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No notes for slide</p>Volume : Terabytes, Records, Transactions, Tables, files <br />Velocity : Batch, Near real time, realtime <br />Variety : Structured, unstructured, semi structuredVertical scaling means that you scale by adding more power (CPU, RAM) to an existing machine. <p>In vertical-scaling the data resides on a single node and scaling is done through multi-core i.e. spreading the load between the CPU and RAM resources of that machine.</p>Horizontal scaling means that you scale by adding more machines into your pool of resources. <br />In a database horizontal-scaling is often based on partitioning of the data <br />i.e. each node contains only part of the data. <br />With horizontal-scaling it is often easier to scale dynamically by adding more machines into the existing pool. <br />If a cluster requires more resources to improve performance and provide high availability (HA), an administrator can scale out by adding more machine to the cluster.Scalability : Hyper scale, load balancing, scale out. <br />Availability : Failure resilient, rolling updates, recovery from failures. <br />Manageability : Granular versioning, micro service Responsive: The system responds in a timely manner if at all possible.  <p>Resilient: The system stays responsive in the face of failure. This applies not only to highly-available, mission critical systems — any system that is not resilient will be unresponsive after a failure.  </p><p>Elastic: The system stays responsive under varying workload. Reactive Systems can react to changes in the input rate by increasing or decreasing the resources allocated to service these inputs. </p><p>Message Driven: Reactive Systems rely on asynchronous message-passing to establish a boundary between components that ensures loose coupling, isolation and location transparency.  <br /></p>Micro service: <p>33TB Monthly  1.1 TB daily </p>The distributed storage system Cassandra, for example, runs on top of hundreds of commodity nodes spread across different data centers. Because the commodity hardware is scaled out horizontally, Cassandra is fault tolerant and does not have a single point of failure (SPoF).Cassandra supports a per-operation tradeoff between consistency and availability through Consistency Levels. <p>The following consistency levels are available: <br />ONE : Only a single replica must respond. <br />TWO :Two replicas must respond. <br />THREE : Three replicas must respond. <br />QUORUMA : majority (n/2 + 1) of the replicas must respond. <br />ALL :All of the replicas must respond. <br />LOCAL_QUORUMA :majority of the replicas in the local datacenter (whichever datacenter the coordinator is in) must respond. <br />EACH_QUORUMA : majority of the replicas in each datacenter must respond. <br />LOCAL_ONE : Only a single replica must respond. In a multi-datacenter cluster, this also gaurantees that read requests are not sent to replicas in a remote datacenter. <br />ANY : A single replica may respond, or the coordinator may store a hint. If a hint is stored, the coordinator will later attempt to replay the hint and deliver the mutation to the replicas. This consistency level is only accepted for write operations.</p>Spark and Spark Streaming with the RDD concept at the core are inherently designed to recover from worker failures.  <br />Stateful exactly-once semantics out of the box. <p>Spark Streaming recovers both lost work and operator state (e.g. sliding windows) out of the box, without any extra code on your part.</p>sc.cassandraTable(\"keyspace name\", \"table name\")</div></div></div><div class=\"notranslate transcript add-padding-right j-transcript\"><ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    Rahul Kumar\nTechnical Lead\nSigmoid\nReal Time data pipeline with Spark Streaming and\nCassandra with Mesos\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-2-638.jpg?cb=1475599971\" title=\"About Sigmoid&#10;© DataStax, All Rights Reserved. 2&#10;We build r...\" target=\"_blank\">\n        2.\n      </a>\n    About Sigmoid\n© DataStax, All Rights Reserved. 2\nWe build reactive real-time big data systems.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-3-638.jpg?cb=1475599971\" title=\"1 Data Management&#10;2 Cassandra Introduction&#10;3 Apache Spark S...\" target=\"_blank\">\n        3.\n      </a>\n    1 Data Management\n2 Cassandra Introduction\n3 Apache Spark Streaming\n4 Reactive Data Pipelines\n5 Use cases\n3© DataStax, All Rights Reserved.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-4-638.jpg?cb=1475599971\" title=\"Data Management&#10;© DataStax, All Rights Reserved. 4&#10;Managing...\" target=\"_blank\">\n        4.\n      </a>\n    Data Management\n© DataStax, All Rights Reserved. 4\nManaging data and analyzing\ndata have always greatest\nbenefit and the greatest\nchallenges for organization.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-5-638.jpg?cb=1475599971\" title=\"Three V’s of Big data&#10;© DataStax, All Rights Reserved. 5&#10;\" target=\"_blank\">\n        5.\n      </a>\n    Three V’s of Big data\n© DataStax, All Rights Reserved. 5\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-6-638.jpg?cb=1475599971\" title=\"Scale Vertically&#10;© DataStax, All Rights Reserved. 6&#10;\" target=\"_blank\">\n        6.\n      </a>\n    Scale Vertically\n© DataStax, All Rights Reserved. 6\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-7-638.jpg?cb=1475599971\" title=\"Scale Horizontally&#10;© DataStax, All Rights Reserved. 7&#10;\" target=\"_blank\">\n        7.\n      </a>\n    Scale Horizontally\n© DataStax, All Rights Reserved. 7\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-8-638.jpg?cb=1475599971\" title=\"Understanding Distributed Application&#10;© DataStax, All Right...\" target=\"_blank\">\n        8.\n      </a>\n    Understanding Distributed Application\n© DataStax, All Rights Reserved. 8\n“ A distributed system is a software system in which\ncomponents located on networked computers\ncommunicate and coordinate their actions by passing\nmessages.”\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-9-638.jpg?cb=1475599971\" title=\"Principles Of Distributed Application Design&#10;© DataStax, Al...\" target=\"_blank\">\n        9.\n      </a>\n    Principles Of Distributed Application Design\n© DataStax, All Rights Reserved. 9\n Availability\n Performance\n Reliability\n Scalability\n Manageability\n Cost\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-10-638.jpg?cb=1475599971\" title=\"Reactive Application&#10;© DataStax, All Rights Reserved. 10&#10;\" target=\"_blank\">\n        10.\n      </a>\n    Reactive Application\n© DataStax, All Rights Reserved. 10\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-11-638.jpg?cb=1475599971\" title=\"Reactive libraries, tools and frameworks&#10;© DataStax, All Ri...\" target=\"_blank\">\n        11.\n      </a>\n    Reactive libraries, tools and frameworks\n© DataStax, All Rights Reserved. 11\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-12-638.jpg?cb=1475599971\" title=\"Cassandra Introduction&#10;© DataStax, All Rights Reserved. 13&#10;...\" target=\"_blank\">\n        12.\n      </a>\n    Cassandra Introduction\n© DataStax, All Rights Reserved. 13\nCassandra - is an Open Source, distributed store for structured data\nthat scale-out on cheap, commodity hardware.\nBorn at Facebook, built on Amazon’s Dynamo and Google’s BigTable\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-13-638.jpg?cb=1475599971\" title=\"Why Cassandra&#10;© DataStax, All Rights Reserved. 14&#10;\" target=\"_blank\">\n        13.\n      </a>\n    Why Cassandra\n© DataStax, All Rights Reserved. 14\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-14-638.jpg?cb=1475599971\" title=\"Highly scalable NoSQL database&#10;© DataStax, All Rights Reser...\" target=\"_blank\">\n        14.\n      </a>\n    Highly scalable NoSQL database\n© DataStax, All Rights Reserved. 15\n Cassandra supplies linear\nscalability\n Cassandra is a partitioned\nrow store database\n Automatic data distribution\n Built-in and customizable\nreplication\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-15-638.jpg?cb=1475599971\" title=\"High Availability&#10;© DataStax, All Rights Reserved. 16&#10; In ...\" target=\"_blank\">\n        15.\n      </a>\n    High Availability\n© DataStax, All Rights Reserved. 16\n In a Cassandra cluster all\nnodes are equal.\n There are no masters or\ncoordinators at the cluster\nlevel.\n Gossip protocol allows\nnodes to be aware of each\nother.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-16-638.jpg?cb=1475599971\" title=\"Read/Write any where&#10;© DataStax, All Rights Reserved. 17&#10; ...\" target=\"_blank\">\n        16.\n      </a>\n    Read/Write any where\n© DataStax, All Rights Reserved. 17\n Cassandra is a R/W\nanywhere architecture, so\nany user/app can connect\nto any node in any DC and\nread/write the data.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-17-638.jpg?cb=1475599971\" title=\"High Performance&#10;© DataStax, All Rights Reserved. 18&#10; All ...\" target=\"_blank\">\n        17.\n      </a>\n    High Performance\n© DataStax, All Rights Reserved. 18\n All disk writes are\nsequential, append-only\noperations.\n Ensure No reading before\nwrite.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-18-638.jpg?cb=1475599971\" title=\"Cassandra &amp; CAP&#10;© DataStax, All Rights Reserved. 19&#10; Cassa...\" target=\"_blank\">\n        18.\n      </a>\n    Cassandra &amp; CAP\n© DataStax, All Rights Reserved. 19\n Cassandra is classified as\nan AP system\n System is still available\nunder partition\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-19-638.jpg?cb=1475599971\" title=\"CQL&#10;© DataStax, All Rights Reserved. 20&#10;CREATE KEYSPACE MyA...\" target=\"_blank\">\n        19.\n      </a>\n    CQL\n© DataStax, All Rights Reserved. 20\nCREATE KEYSPACE MyAppSpace WITH\nREPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 3 };\nUSE MyAppSpace ;\nCREATE COLUMNFAMILY AccessLog(id text, ts timestamp ,ip text, port text,\nstatus text, PRIMARY KEY(id));\nINSERT INTO AccessLog (id, ts, ip, port, status) VALUES (’id-001-1', 2016-01-01\n00:00:00+0200', ’10.20.30.1’,’200’);\nSELECT * FROM AccessLog ;\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-20-638.jpg?cb=1475599971\" title=\"Apache Spark&#10;© DataStax, All Rights Reserved. 21&#10;Introducti...\" target=\"_blank\">\n        20.\n      </a>\n    Apache Spark\n© DataStax, All Rights Reserved. 21\nIntroduction\n Apache Spark is a fast and\ngeneral execution engine\nfor large-scale data\nprocessing.\n Organize computation as\nconcurrent tasks\n Handle fault-tolerance,\nload balancing\n Developed on Actor Model\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-21-638.jpg?cb=1475599971\" title=\"RDD Introduction&#10;© DataStax, All Rights Reserved. 22&#10;Resili...\" target=\"_blank\">\n        21.\n      </a>\n    RDD Introduction\n© DataStax, All Rights Reserved. 22\nResilient Distributed Datasets (RDDs), a distributed memory\nabstraction that lets programmers perform in-memory computations\non large clusters in a fault-tolerant manner.\nRDD shared the data over a cluster, like a virtualized, distributed\ncollection.\nUsers create RDDs in two ways: by loading an external dataset, or\nby distributing a collection of objects such as List, Map etc.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-22-638.jpg?cb=1475599971\" title=\"RDD Operations&#10;© DataStax, All Rights Reserved. 23&#10;Two Kind...\" target=\"_blank\">\n        22.\n      </a>\n    RDD Operations\n© DataStax, All Rights Reserved. 23\nTwo Kind of Operations\n• Transformation\n• Action\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-23-638.jpg?cb=1475599971\" title=\"What is Spark Streaming?&#10;© DataStax, All Rights Reserved. 2...\" target=\"_blank\">\n        23.\n      </a>\n    What is Spark Streaming?\n© DataStax, All Rights Reserved. 26\nFramework for large scale stream processing\n➔ Created at UC Berkeley\n➔ Scales to 100s of nodes\n➔ Can achieve second scale latencies\n➔ Provides a simple batch-like API for implementing complex algorithm\n➔ Can absorb live data streams from Kafka, Flume, ZeroMQ, Kinesis etc.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-24-638.jpg?cb=1475599971\" title=\"Spark Streaming&#10;© DataStax, All Rights Reserved. 27&#10;Introdu...\" target=\"_blank\">\n        24.\n      </a>\n    Spark Streaming\n© DataStax, All Rights Reserved. 27\nIntroduction\n• Spark Streaming is an\nextension of the core spark\nAPI that enables scalable,\nhigh-throughput, fault-\ntolerant stream processing\nof live data streams.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-25-638.jpg?cb=1475599971\" title=\"Spark Streaming over a HA Mesos Cluster&#10;© DataStax, All Rig...\" target=\"_blank\">\n        25.\n      </a>\n    Spark Streaming over a HA Mesos Cluster\n© DataStax, All Rights Reserved. 31\nTo use Mesos from Spark, you need a Spark binary package available in a place\naccessible (http/s3/hdfs) by Mesos, and a Spark driver program configured to\nconnect to Mesos.\nConfiguring the driver program to connect to Mesos:\nval sconf = new SparkConf()\n.setMaster(\"mesos://zk://10.121.93.241:2181,10.181.2.12:2181,10.107.48.112:2181/mesos\")\n.setAppName(”HAStreamingApp\")\n.set(\"spark.executor.uri\",\"hdfs://Sigmoid/executors/spark-1.6.0-bin-hadoop2.6.tgz\")\n.set(\"spark.mesos.coarse\", \"true\")\n.set(\"spark.cores.max\", \"30\")\n.set(\"spark.executor.memory\", \"10g\")\nval sc = new SparkContext(sconf)\nval ssc = new StreamingContext(sc, Seconds(1))\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-26-638.jpg?cb=1475599971\" title=\"Spark Cassandra Connector&#10;© DataStax, All Rights Reserved. ...\" target=\"_blank\">\n        26.\n      </a>\n    Spark Cassandra Connector\n© DataStax, All Rights Reserved. 32\n It allows us to expose Cassandra tables as Spark RDDs\n Write Spark RDDs to Cassandra tables\n Execute arbitrary CQL queries in your Spark applications.\n Compatible with Apache Spark 1.0 through 2.0\n It Maps table rows to CassandraRow objects or tuples\n Do Join with a subset of Cassandra data\n Partition RDDs according to Cassandra replication\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-27-638.jpg?cb=1475599971\" title=\"© DataStax, All Rights Reserved. 33&#10;resolvers += &quot;Spark Pac...\" target=\"_blank\">\n        27.\n      </a>\n    © DataStax, All Rights Reserved. 33\nresolvers += \"Spark Packages Repo\" at \"https://dl.bintray.com/spark-packages/maven\"\nlibraryDependencies += \"datastax\" % \"spark-cassandra-connector\" % \"1.6.0-s_2.10\"\nbuild.sbt should include:\nimport com.datastax.spark.connector._\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-28-638.jpg?cb=1475599971\" title=\"© DataStax, All Rights Reserved. 34&#10;val rdd = sc.cassandraT...\" target=\"_blank\">\n        28.\n      </a>\n    © DataStax, All Rights Reserved. 34\nval rdd = sc.cassandraTable(“applog”, “accessTable”)\nprintln(rdd.count)\nprintln(rdd.first)\nprintln(rdd.map(_.getInt(\"value\")).sum)\ncollection.saveToCassandra(“applog”, \"accessTable\", SomeColumns(”city\", ”count\"))\nSave Data Back to Cassandra\nGet a Spark RDD that represents a Cassandra table\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-29-638.jpg?cb=1475599971\" title=\"Many more higher order functions:&#10;© DataStax, All Rights Re...\" target=\"_blank\">\n        29.\n      </a>\n    Many more higher order functions:\n© DataStax, All Rights Reserved. 35\nrepartitionByCassandraReplica : It be used to relocate data in an RDD to match\nthe replication strategy of a given table and keyspace\njoinWithCassandraTable : The connector supports using any RDD as a source of\na direct join with a Cassandra Table\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-30-638.jpg?cb=1475599971\" title=\"Hint to scalable pipeline&#10;© DataStax, All Rights Reserved. ...\" target=\"_blank\">\n        30.\n      </a>\n    Hint to scalable pipeline\n© DataStax, All Rights Reserved. 36\nFigure out the bottleneck : CPU, Memory, IO, Network\nIf parsing is involved, use the one which gives high performance.\nProper Data modeling\nCompression, Serialization\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-31-638.jpg?cb=1475599971\" title=\"Thank You&#10;@rahul_kumar_aws&#10;\" target=\"_blank\">\n        31.\n      </a>\n    Thank You\n@rahul_kumar_aws\n \n  </li>\n              </ol></div></div></div><aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\"><dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\"><ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"66696272\" title=\"Learning Study Skills\" href=\"https://www.linkedin.com/learning/learning-study-skills?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Learning Study Skills\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Learning Study Skills\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=AkCmIodsxaZhRytp%2F97%2FxxQM0Q8%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-iXCej-NWfY3DrcMXfZLSiolwQfy0HkQQxfe6rRTbmFI69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Learning Study Skills</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"66696272\" title=\"PowerPoint for Teachers: Creating Interactive Lessons\" href=\"https://www.linkedin.com/learning/powerpoint-for-teachers-creating-interactive-lessons?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_PowerPoint for Teachers: Creating Interactive Lessons\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"PowerPoint for Teachers: Creating Interactive Lessons\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=NrZ6QSg%2BBWntE4IYVUxrWwdlQFs%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lXCeu-NKfZHDrfcXYZLSiol4QfyoDmQEyfuerQzPmFY69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>PowerPoint for Teachers: Creating Interactive Lessons</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"66696272\" title=\"PowerPoint 2016: Shortcuts\" href=\"https://www.linkedin.com/learning/powerpoint-2016-shortcuts?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_PowerPoint 2016: Shortcuts\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"PowerPoint 2016: Shortcuts\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=Q8CAKihtghsRRL8hygXfdU0PiFk%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lXiyu8t2fZHLgfc_XZLSioVQTcSsBmAQ2d-2rRzbpFY69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>PowerPoint 2016: Shortcuts</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"45485034\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Breakout: Hadoop and the Operational Data Store\" href=\"https://www.slideshare.net/cloudera/hadoop-and-the-operational-data-store\">\n    \n    <div class=\"related-content\"><p>Breakout: Hadoop and the Operational Data Store</p><p>Cloudera, Inc.</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"55316371\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Using Event-Driven Architectures with Cassandra\" href=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\">\n    \n    <div class=\"related-content\"><p>Using Event-Driven Architectures with Cassandra</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"95288307\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Webinar: DataStax Enterprise 6: 10 Ways to Multiply the Power of Apache Cassandra™ Without the Complexity\" href=\"https://www.slideshare.net/DataStax/webinar-datastax-enterprise-6-10-ways-to-multiply-the-power-of-apache-cassandra-without-the-complexity-95288307\">\n    \n    <div class=\"related-content\"><p>Webinar: DataStax Enterprise 6: 10 Ways to Multiply the Power of Apache Cassa...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"93695860\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Webinar: DataStax and Microsoft Azure: Empowering the Right-Now Enterprise with Real-Time Apps at Cloud Scale\" href=\"https://www.slideshare.net/DataStax/webinar-datastax-and-microsoft-azure-empowering-the-rightnow-enterprise-with-realtime-apps-at-cloud-scale-93695860\">\n    \n    <div class=\"related-content\"><p>Webinar: DataStax and Microsoft Azure: Empowering the Right-Now Enterprise wi...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"88770359\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Webinar - Real-Time Customer Experience for the Right-Now Enterprise featuring Forrester Research\" href=\"https://www.slideshare.net/DataStax/webinar-realtime-customer-experience-for-the-rightnow-enterprise-featuring-forrester-research\">\n    \n    <div class=\"related-content\"><p>Webinar - Real-Time Customer Experience for the Right-Now Enterprise featurin...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"88713641\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Datastax - The Architect's guide to customer experience (CX)\" href=\"https://www.slideshare.net/DataStax/datastax-the-architects-guide-to-customer-experience-cx\">\n    \n    <div class=\"related-content\"><p>Datastax - The Architect's guide to customer experience (CX)</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"86697242\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"An Operational Data Layer is Critical for Transformative Banking Applications\" href=\"https://www.slideshare.net/DataStax/an-operational-data-layer-is-critical-for-transformative-banking-applications\">\n    \n    <div class=\"related-content\"><p>An Operational Data Layer is Critical for Transformative Banking Applications</p><p>DataStax</p></div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div></div><footer>\n          <div class=\"row\"><div class=\"columns\"><ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div></div>\n          \n          <div class=\"row\"><div class=\"columns\"><p class=\"copyright text-center\">LinkedIn Corporation © 2018</p></div></div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div>\n    <div class=\"row\"><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></div>\n        \n    </div>\n    \n    \n    \n  \n    \n    \n  \n  \n  <noscript>\n    </noscript>",
        "created_at": "2018-05-24T02:16:24+0000",
        "updated_at": "2018-05-24T02:16:42+0000",
        "published_at": null,
        "published_by": [
          "DataStax"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 8,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139-thumbnail-4.jpg?cb=1475599971",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9790"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          }
        ],
        "is_public": false,
        "id": 9789,
        "uid": null,
        "title": "Using Event-Driven Architectures with Cassandra",
        "url": "https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra",
        "content": "Using Event-Driven Architectures with Cassandra\n      \n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            </a><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div>\n    <div class=\"wrapper\"><p>Successfully reported this slideshow.</p><div id=\"slideview-container\" class=\"\"><div class=\"row\"><div id=\"main-panel\" class=\"small-12 large-8 columns\"><div class=\"sectionElements\"><div class=\"playerWrapper\"><div><div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\">Using Event-Driven Architectures with Cassandra<div class=\"stage valign-first-slide\"><div class=\"slide_container\"><section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-1-638.jpg?cb=1447975617\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-1-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-1-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-1-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Event-Driven Architectures with&#10;Cassandra&#10;Aaron Stannard, CTO &amp; Cofounder Petabridge&#10;DataStax M...\" /></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-2-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-2-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-2-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Crash Course in EDA&#10;\" /></i></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-3-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-3-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-3-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Successful Product&#10;\" /></i></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-4-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-4-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-4-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Struggling Business&#10;\" /></i></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-5-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-5-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-5-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;A Radical New Product&#10;\" /></i></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-6-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-6-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-6-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Allow our customers to start&#10;conversations with specific&#10;types of users.&#10;\" /></i></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-7-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-7-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-7-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Had to be done in real-time for&#10;best results.&#10;\" /></i></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-8-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-8-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-8-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Prototype: HTTP + Database&#10;\" /></i></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-9-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-9-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-9-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;(Read after Write)&#10;\" /></i></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-10-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-10-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-10-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Theory&#10;\" /></i></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-11-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-11-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-11-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Reality&#10;\" /></i></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-12-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-12-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-12-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Breakthrough!&#10;\" /></i></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-13-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-13-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-13-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;I had STATEFUL, REACTIVE,&#10;STREAM PROCESSING&#10;problem!&#10;\" /></i></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-14-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-14-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-14-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Implementation:&#10;HTTP -&gt; EDA -&gt; C*&#10;\" /></i></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-15-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-15-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-15-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Why Do We Care About EDA?&#10;\" /></i></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-16-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-16-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-16-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Databases Aren’t Magical&#10;\" /></i></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-17-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-17-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-17-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Event-Driven Architecture&#10;Concepts&#10;\" /></i></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-18-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-18-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-18-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Key Terms&#10;• Event – a significant change in “state”&#10;• Message – a notification of an event&#10;• Em...\" /></i></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-19-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-19-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-19-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Goals &amp; Benefits&#10;• Extreme decoupling&#10;• Easily distributed&#10;• Inherently asynchronous and concur...\" /></i></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-20-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-20-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-20-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;EDA Styles&#10;• Simple Event Processing&#10;• Event Stream Processing&#10;• Complex Event Processing (CEP)&#10;\" /></i></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-21-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-21-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-21-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Event Stream Processing w/&#10;FSMs&#10;\" /></i></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-22-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-22-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-22-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;\" /></i></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-23-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-23-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-23-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;\" /></i></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-24-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-24-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-24-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;\" /></i></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-25-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-25-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-25-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;CQL Schema&#10;CREATE TABLE IF NOT EXISTS FSMState&#10;(&#10;persistence_id text,&#10;state_id int,&#10;state_data ...\" /></i></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-26-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-26-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-26-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Event Sourcing with C*&#10;\" /></i></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-27-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-27-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-27-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;\" /></i></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-28-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-28-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-28-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;\" /></i></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-29-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-29-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-29-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;EDA + C* Tips and Tricks&#10;• Large number of small sinks works best&#10;• Define simple, reusable jou...\" /></i></section><section data-index=\"30\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-30-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-30-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-30-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Have questions? Ask us!&#10;http://petabridge.com/&#10;\" /></i></section><div class=\"j-next-container next-container\"><div class=\"content-container\"><div class=\"next-slideshow-wrapper\"><div class=\"j-next-slideshow next-slideshow\"><p>Upcoming SlideShare</p></div><p>Loading in …5</p><p>×</p></div></div></div></div></div></div></div></div></div><div class=\"slideshow-info-container\" itemscope=\"itemscope\" itemtype=\"https://schema.org/MediaObject\"><div class=\"slideshow-tabs-container show-for-medium-up\"><ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    0 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\">\n                \n                  2 Likes\n                \n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\">\n                Statistics\n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\">\n                Notes\n              </i></a>\n            </li>\n          </ul><div class=\"tabs-content\"><div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\"><ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"itemscope\"><li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"RadhaKrishnaProddatu\" rel=\"nofollow\" href=\"https://www.slideshare.net/RadhaKrishnaProddatu?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Radha Krishna Proddaturi\n                            \n                              \n                                , \n                                Principal Software Engineer at Coupons.com\n                              \n                              \n                                 at \n                                Coupons.com\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"galapea\" rel=\"nofollow\" href=\"https://www.slideshare.net/galapea?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Alif Ruliarso\n                            \n                              \n                                , \n                                Tech-focused delivering innovative technology and products\n                              \n                              \n                                 at \n                                Bridestory\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n              </ul></div><div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No Downloads</p></div><div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No notes for slide</p>C* users should care about their application architecture for the simple reason that C* isn’t magic and can’t do everything. Cassandra can’t solve the problem of real-time correlation or reaction at the application layer. Developers have to solve these problems themselves, and Cassandra is just one part of the solution.Event Stream Processing and CEP play nicely with Cassandra, and that’s what we’ll be focusing onIntentionally small and stupid</div></div></div><div class=\"notranslate transcript add-padding-right j-transcript\"><ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    @petabridge Petabridge.com\nEvent-Driven Architectures with\nCassandra\nAaron Stannard, CTO &amp; Cofounder Petabridge\nDataStax MVP 2015\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-2-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Crash Course in EDA&#10;\" target=\"_blank\">\n        2.\n      </a>\n    @petabridge Petabridge.com\nCrash Course in EDA\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-3-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Successful Product&#10;\" target=\"_blank\">\n        3.\n      </a>\n    @petabridge Petabridge.com\nSuccessful Product\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-4-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Struggling Business&#10;\" target=\"_blank\">\n        4.\n      </a>\n    @petabridge Petabridge.com\nStruggling Business\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-5-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;A Radical New Product&#10;\" target=\"_blank\">\n        5.\n      </a>\n    @petabridge Petabridge.com\nA Radical New Product\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-6-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Allow our customers to start&#10;con...\" target=\"_blank\">\n        6.\n      </a>\n    @petabridge Petabridge.com\nAllow our customers to start\nconversations with specific\ntypes of users.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-7-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Had to be done in real-time for&#10;...\" target=\"_blank\">\n        7.\n      </a>\n    @petabridge Petabridge.com\nHad to be done in real-time for\nbest results.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-8-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Prototype: HTTP + Database&#10;\" target=\"_blank\">\n        8.\n      </a>\n    @petabridge Petabridge.com\nPrototype: HTTP + Database\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-9-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;(Read after Write)&#10;\" target=\"_blank\">\n        9.\n      </a>\n    @petabridge Petabridge.com\n(Read after Write)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-10-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Theory&#10;\" target=\"_blank\">\n        10.\n      </a>\n    @petabridge Petabridge.com\nTheory\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-11-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Reality&#10;\" target=\"_blank\">\n        11.\n      </a>\n    @petabridge Petabridge.com\nReality\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-12-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Breakthrough!&#10;\" target=\"_blank\">\n        12.\n      </a>\n    @petabridge Petabridge.com\nBreakthrough!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-13-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;I had STATEFUL, REACTIVE,&#10;STREAM...\" target=\"_blank\">\n        13.\n      </a>\n    @petabridge Petabridge.com\nI had STATEFUL, REACTIVE,\nSTREAM PROCESSING\nproblem!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-14-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Implementation:&#10;HTTP -&gt; EDA -&gt; C*&#10;\" target=\"_blank\">\n        14.\n      </a>\n    @petabridge Petabridge.com\nImplementation:\nHTTP -&gt; EDA -&gt; C*\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-15-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Why Do We Care About EDA?&#10;\" target=\"_blank\">\n        15.\n      </a>\n    @petabridge Petabridge.com\nWhy Do We Care About EDA?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-16-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Databases Aren’t Magical&#10;\" target=\"_blank\">\n        16.\n      </a>\n    @petabridge Petabridge.com\nDatabases Aren’t Magical\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-17-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Event-Driven Architecture&#10;Concep...\" target=\"_blank\">\n        17.\n      </a>\n    @petabridge Petabridge.com\nEvent-Driven Architecture\nConcepts\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-18-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Key Terms&#10;• Event – a significan...\" target=\"_blank\">\n        18.\n      </a>\n    @petabridge Petabridge.com\nKey Terms\n• Event – a significant change in “state”\n• Message – a notification of an event\n• Emitter – detect, gather, transfer events\n• Sinks – react to events immediately\n• Channels – conduit between emitters and\nsinks\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-19-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Goals &amp; Benefits&#10;• Extreme decou...\" target=\"_blank\">\n        19.\n      </a>\n    @petabridge Petabridge.com\nGoals &amp; Benefits\n• Extreme decoupling\n• Easily distributed\n• Inherently asynchronous and concurrent\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-20-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;EDA Styles&#10;• Simple Event Proces...\" target=\"_blank\">\n        20.\n      </a>\n    @petabridge Petabridge.com\nEDA Styles\n• Simple Event Processing\n• Event Stream Processing\n• Complex Event Processing (CEP)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-21-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Event Stream Processing w/&#10;FSMs&#10;\" target=\"_blank\">\n        21.\n      </a>\n    @petabridge Petabridge.com\nEvent Stream Processing w/\nFSMs\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-22-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;\" target=\"_blank\">\n        22.\n      </a>\n    @petabridge Petabridge.com\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-23-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;\" target=\"_blank\">\n        23.\n      </a>\n    @petabridge Petabridge.com\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-24-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;\" target=\"_blank\">\n        24.\n      </a>\n    @petabridge Petabridge.com\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-25-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;CQL Schema&#10;CREATE TABLE IF NOT E...\" target=\"_blank\">\n        25.\n      </a>\n    @petabridge Petabridge.com\nCQL Schema\nCREATE TABLE IF NOT EXISTS FSMState\n(\npersistence_id text,\nstate_id int,\nstate_data blob,\nPRIMARY KEY (persistence_id, state_id)\n)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-26-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Event Sourcing with C*&#10;\" target=\"_blank\">\n        26.\n      </a>\n    @petabridge Petabridge.com\nEvent Sourcing with C*\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-27-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;\" target=\"_blank\">\n        27.\n      </a>\n    @petabridge Petabridge.com\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-28-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;\" target=\"_blank\">\n        28.\n      </a>\n    @petabridge Petabridge.com\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-29-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;EDA + C* Tips and Tricks&#10;• Large...\" target=\"_blank\">\n        29.\n      </a>\n    @petabridge Petabridge.com\nEDA + C* Tips and Tricks\n• Large number of small sinks works best\n• Define simple, reusable journals and\nsnapshots\n• C* == durable backup, App = single\nsource of truth\n• TTL everywhere your business domain\nsupports it\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-30-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Have questions? Ask us!&#10;http://p...\" target=\"_blank\">\n        30.\n      </a>\n    @petabridge Petabridge.com\nHave questions? Ask us!\nhttp://petabridge.com/\n \n  </li>\n              </ol></div></div></div><aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\"><dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\"><ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"55316371\" title=\"Visual Aesthetics for Elearning\" href=\"https://www.linkedin.com/learning/visual-aesthetics-for-elearning?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Visual Aesthetics for Elearning\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Visual Aesthetics for Elearning\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=Hf12LV5YZaVLCcNTSkttsN6Dxx8%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-kXCei8tyfZXDrcc_WZLSiol4XcC8HkQwzfO-hSTPkFY69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Visual Aesthetics for Elearning</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"55316371\" title=\"College Prep: Writing a Strong Essay\" href=\"https://www.linkedin.com/learning/college-prep-writing-a-strong-essay?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_College Prep: Writing a Strong Essay\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"College Prep: Writing a Strong Essay\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=UlSFF0nnPn2Agyz5a5T2WQKxkbA%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-iUyav_dSfY3_qfMDeZLSiol4TfSkFlAA0duqsQzfhGo69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>College Prep: Writing a Strong Essay</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"55316371\" title=\"Test Prep: GRE\" href=\"https://www.linkedin.com/learning/test-prep-gre?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Test Prep: GRE\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Test Prep: GRE\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=zRN9cYKLcI5pMFN7G1fZgp5AHQw%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lXSSj-9OfZHHocMbZZLSiol8QcS4DkQA2e-ahSTLnEI69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Test Prep: GRE</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"66696272\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Realtime Data Pipeline with Spark Streaming and Cassandra with Mesos (Rahul Kumar, Sigmoid) | C* Summit 2016\" href=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016\">\n    \n    <div class=\"related-content\"><p>Realtime Data Pipeline with Spark Streaming and Cassandra with Mesos (Rahul K...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"77306108\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Forrester CXNYC 2017 - Delivering great real-time cx is a true craft\" href=\"https://www.slideshare.net/planetcassandra/forrester-cxnyc-2017-delivering-great-realtime-cx-is-a-true-craft\">\n    \n    <div class=\"related-content\"><p>Forrester CXNYC 2017 - Delivering great real-time cx is a true craft</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64947458\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Introduction to DataStax Enterprise Graph Database\" href=\"https://www.slideshare.net/planetcassandra/introduction-to-datastax-enterprise-graph-database\">\n    \n    <div class=\"related-content\"><p>Introduction to DataStax Enterprise Graph Database</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64947204\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Introduction to DataStax Enterprise Advanced Replication with Apache Cassandra\" href=\"https://www.slideshare.net/planetcassandra/introduction-to-datastax-enterprise-advanced-replication-with-apache-cassandra\">\n    \n    <div class=\"related-content\"><p>Introduction to DataStax Enterprise Advanced Replication with Apache Cassandra</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64223739\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Cassandra on Docker @ Walmart Labs\" href=\"https://www.slideshare.net/planetcassandra/cassandra-on-docker-walmart-labs\">\n    \n    <div class=\"related-content\"><p>Cassandra on Docker @ Walmart Labs</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64222983\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Cassandra 3.0 Data Modeling\" href=\"https://www.slideshare.net/planetcassandra/cassandra-30-data-modeling\">\n    \n    <div class=\"related-content\"><p>Cassandra 3.0 Data Modeling</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64222866\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Cassandra Adoption on Cisco UCS &amp; Open stack\" href=\"https://www.slideshare.net/planetcassandra/cassandra-adoption-on-cisco-ucs-open-stack\">\n    \n    <div class=\"related-content\"><p>Cassandra Adoption on Cisco UCS &amp; Open stack</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div></div><footer>\n          <div class=\"row\"><div class=\"columns\"><ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div></div>\n          \n          <div class=\"row\"><div class=\"columns\"><p class=\"copyright text-center\">LinkedIn Corporation © 2018</p></div></div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div>\n    <div class=\"row\"><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></div>\n        \n    </div>\n    \n    \n      <noscript>\n    </noscript>",
        "created_at": "2018-05-24T02:16:03+0000",
        "updated_at": "2018-05-24T02:16:19+0000",
        "published_at": null,
        "published_by": [
          "DataStax Academy"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891-thumbnail-4.jpg?cb=1447975617",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9789"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 27,
            "label": "search",
            "slug": "search"
          },
          {
            "id": 36,
            "label": "solr",
            "slug": "solr"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 956,
            "label": "streaming",
            "slug": "streaming"
          },
          {
            "id": 1071,
            "label": "kafka.streams",
            "slug": "kafka-streams"
          }
        ],
        "is_public": false,
        "id": 9788,
        "uid": null,
        "title": "Search and Analytics on Streaming Data With Kafka, Solr, Cassandra, Spark",
        "url": "http://saumitra.me/blog/tweet-search-and-analysis-with-kafka-solr-cassandra/",
        "content": "<p>In this blog post we will see how to setup a simple search and anlytics pipeline on streaming data in scala.</p><ul><li>For sample timeseries data, we will use twitter stream.</li>\n<li>For data pipelining, we will use kafka</li>\n<li>For search, we will use Solr. We will use Banana for a UI query interface for solr data.</li>\n<li>For analytics, we will store data in cassandra. We will see example of using spark for running analytics query. We will use zeppelin for a UI query interface.</li>\n</ul><p>Full code for this post is avaliable at <a href=\"https://github.com/saumitras/twitter-analysis\">https://github.com/saumitras/twitter-analysis</a></p>\n<h2>Dependencies</h2>\n<p>Create a new project and add following dependecies in build.sbt. Note that there are few conflicting dependecies in kafka so exclude them:</p>\n<figure class=\"code\"><div class=\"highlight\"><table><tr><td class=\"gutter\"><pre class=\"line-numbers\">1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n</pre></td><td class=\"code\"><pre class=\"\">libraryDependencies ++= Seq(\n  \"org.twitter4j\" % \"twitter4j-core\" % \"4.0.4\",\n  \"org.twitter4j\" % \"twitter4j-stream\" % \"4.0.4\",\n  \"com.typesafe.akka\" % \"akka-actor_2.11\" % \"2.4.17\",\n  \"org.apache.kafka\" % \"kafka_2.11\" % \"0.10.0.0\" withSources() exclude(\"org.slf4j\",\"slf4j-log4j12\") exclude(\"javax.jms\", \"jms\") exclude(\"com.sun.jdmk\", \"jmxtools\") exclude(\"com.sun.jmx\", \"jmxri\"),\n  \"org.apache.avro\" % \"avro\" % \"1.7.7\" withSources(),\n  \"org.apache.solr\" % \"solr-solrj\" % \"6.4.1\" withSources(),\n  \"com.typesafe.scala-logging\" %% \"scala-logging\" % \"3.1.0\",\n  \"ch.qos.logback\" % \"logback-classic\" % \"1.1.2\",\n  \"com.datastax.cassandra\" % \"cassandra-driver-core\"  % \"3.0.2\",\n  \"org.apache.cassandra\" % \"cassandra-clientutil\"  % \"3.0.2\",\n  \"org.apache.spark\" %% \"spark-core\" % \"2.1.0\",\n  \"org.apache.spark\" %% \"spark-sql\" % \"2.1.0\",\n  \"org.apache.spark\" %% \"spark-hive\" % \"2.1.0\",\n  \"com.datastax.spark\" %% \"spark-cassandra-connector\" % \"2.0.0\"\n)</pre></td></tr></table></div></figure><h2>Setting up twiiter stream</h2>\n<p>For streaming data from twitter you need access keys and token. You can go to <a href=\"https://apps.twitter.com\">https://apps.twitter.com</a> and creata a new app to get these. After creating an app, click on “Keys and access token” and copy following:</p>\n<ul><li>Consumer Key (API Key)</li>\n<li>Consumer Secret (API Secret)</li>\n<li>Access Token</li>\n<li>Access Token Secret</li>\n</ul><p>We will use twitter4j. Build a configuration using token and key</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\nval cb = new ConfigurationBuilder()\ncb.setDebugEnabled(true)\ncb.setOAuthConsumerKey(\"p5vABCjRWWSXNBkypnb8ZnSzk\")   //replace this with your own keys\ncb.setOAuthConsumerSecret(\"wCVFIpwWxEyOcM9lrHa9TYExbNsLGvEUgJucePPjcTx83bD1Gt\") //replace this with your own keys\ncb.setOAuthAccessToken(\"487652626-kDOFZLu8bDjFyCKUOCDa7FtHsr22WC3PMH4iuNtn\")  //replace this with your own keys\ncb.setOAuthAccessTokenSecret(\"4W3LaQTAgGoW5SsHUAgp6gK9b5AKgl8hRcFnNYgvPTylU\")  //replace this with your own keys\n</figcaption></figure><p>You can now open a stream and listen for tweets with some specific keyswords or hashtags:</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\nval stream = new TwitterStreamFactory(cb.build()).getInstance()\nval listener = new StatusListener {\n  override def onTrackLimitationNotice(i: Int): Unit = logger.warn(s\"Track limited $i tweets\")\n  override def onStallWarning(stallWarning: StallWarning): Unit = logger.error(\"Stream stalled\")\n  override def onDeletionNotice(statusDeletionNotice: StatusDeletionNotice): Unit = logger.warn(\"Status ${statusDeletionNotice.getStatusId} deleted\")\n  override def onScrubGeo(l: Long, l1: Long): Unit = logger.warn(s\"Geo info scrubbed. userId:$l, upToStatusId:$l1\")\n  override def onException(e: Exception): Unit = logger.error(\"Exception occurred. \" + e.getMessage)\n  override def onStatus(status: Status): Unit = {\n    logger.info(\"Msg: \" + status.getText)\n  }\n}\nval keywords = List(\"#scala\", \"#kafka\", \"#cassandra\", \"#solr\", \"#bigdata\", \"#apachespark\", \"#streamingdata\")\nstream.addListener(listener)\nval fq = new FilterQuery()\nfq.track(keywords.mkString(\",\"))\nstream.filter(fq)\n</figcaption></figure><p><code>StatusListener</code> provide couple of callback to handle different scenarios. <code>onStatus</code> is the one which will get the <code>tweet</code> and its metadata. <code>stream.filter(fq)</code> will start the stream.</p>\n<p>If you run this, you should start seeing the tweets:</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\nMsg: RT @botbigdata: How to build a data science team https://t.co/xJWWgueGAV #bigdata\nMsg: RT @ATEKAssetScan: Why Velocity Of Innovation Is A Data Friction Problem https://t.co/Eo1pTNCEv9 #BigData #IoT #IIoT #InternetOfThings #Art…\nMsg: Making the Most of Big Data https://t.co/X52AZ5n5nT #BigData\nMsg: RT @botbigdata: Create editable Microsoft Office charts from R https://t.co/LnSDU0iSMq #bigdata\nMsg: RT @YarmolukDan: How #Twitter Users Can Generate Better Ideas https://t.co/b0O9iEULHG #DataScience #DataScientist #BigData #IoT… \nMsg: RT @botbigdata: VIDEO: Installing TOR on an Ubuntu Virtual Machine https://t.co/Q3FPhY8CGm #bigdata</figcaption></figure><p>Lets define a type and extract out tweet metadata</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\ncase class Tweet(id:String, username:String, userId:Long, userScreenName:String,\n                   userDesc:String, userProfileImgUrl:String, favCount:Long, retweetCount:Long,\n                   lang:String, place:String, message:String, isSensitive:Boolean,\n                   isTruncated:Boolean, isFavorited:Boolean, isRetweeted:Boolean,\n                   isRetweet:Boolean, createdAt:Long)\n</figcaption></figure><figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\noverride def onStatus(status: Status): Unit = {\n  val retweetCount = if(status.getRetweetedStatus == null) 0 else status.getRetweetedStatus.getRetweetCount\n  val userDesc = if(status.getUser.getDescription == null) \"null\" else status.getUser.getDescription\n  val userProfileImgUrl = if(status.getUser.getProfileImageURL == null) \"null\" else status.getUser.getProfileImageURL\n  val lang = if(status.getLang == null) \"null\" else status.getLang\n  val place = if(status.getPlace == null) \"null\" else status.getPlace.getFullName\n  val tweet = Tweet(\n    id = status.getId.toString,\n    username = status.getUser.getName,\n    userId = status.getUser.getId,\n    userScreenName = status.getUser.getScreenName,\n    userDesc = userDesc,\n    userProfileImgUrl = userProfileImgUrl,\n    createdAt = status.getCreatedAt.getTime,\n    favCount = status.getFavoriteCount,\n    retweetCount = retweetCount,\n    lang = lang,\n    place = place,\n    message = status.getText,\n    isSensitive = status.isPossiblySensitive,\n    isTruncated = status.isTruncated,\n    isFavorited = status.isFavorited,\n    isRetweeted = status.isRetweeted,\n    isRetweet = status.isRetweet\n  )\n  logger.info(\"Msg: \" + tweet.message)\n  }\n</figcaption></figure><p>Next we will send these tweets to kafka.</p>\n<h2>Zookeeper setup</h2>\n<p>ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. All of these kinds of services are used in some form or another by distributed applications. In our example, both Kafka and Solr will need zookeeper for their state and config management, so you need to first start zookeeper.</p>\n<ul><li>Download it from <code>http://apache.org/dist/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz</code></li>\n<li>Extract it and go inside conf directory</li>\n<li>Make a copy of zoo_sample.conf as zoo.cfg</li>\n<li>Run it using <code>bin/zkServer.sh start</code></li>\n<li>Verify its started successfully by running <code>bin/zkServer.sh status</code> command.</li>\n</ul><h2>Putting data in Kafka</h2>\n<p>Here’s steps to send data to kafka.</p>\n<ul><li>Start kafka server and broker(s)</li>\n<li>Create a topic in kafka to which data will be send</li>\n<li>Define a avro schema for the tweets</li>\n<li>Create a kafka producer which will serialize tweets using avro schema and send it to kafka</li>\n</ul><p>Download kafka from here.</p>\n<p>Start server</p>\n<figure class=\"code\"><figcaption>1\nbin/kafka-server-start.sh config/server.properties\n</figcaption></figure><p>Create a topic</p>\n<figure class=\"code\"><figcaption>1\nbin/kafka-topics.sh --create --zookeeper localhost:2181/kafka --replication-factor 1 --partitions 1 --topic tweet1\n</figcaption></figure><p>You can see if topic is created successfully</p>\n<figure class=\"code\"><figcaption>1\nbin/kafka-topics.sh --list --zookeeper localhost:2181/kafka\n</figcaption></figure><h3>Avro schema</h3>\n<p>Avro is a data serialization system. It has a JSON like data model, but can be represented as either JSON or in a compact binary form. It comes with a very sophisticated schema description language that describes data. Lets define avro schema for our <code>Tweet</code> type:</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n{\n  \"type\": \"record\",\n  \"namespace\": \"tweet\",\n  \"name\": \"tweet\",\n  \"fields\":[\n    { \"name\": \"id\", \"type\":\"string\" },\n    { \"name\": \"username\", \"type\":\"string\" },\n    { \"name\": \"userId\", \"type\":\"long\" },\n    { \"name\": \"userScreenName\", \"type\":\"string\" },\n    { \"name\": \"userDesc\", \"type\":\"string\" },\n    { \"name\": \"userProfileImgUrl\", \"type\":\"string\" },\n    { \"name\": \"favCount\", \"type\":\"int\" },\n    { \"name\": \"retweetCount\", \"type\":\"int\" },\n    { \"name\": \"lang\", \"type\":\"string\" },\n    { \"name\": \"place\", \"type\":\"string\" },\n    { \"name\": \"message\", \"type\":\"string\" },\n    { \"name\": \"isSensitive\", \"type\":\"boolean\" },\n    { \"name\": \"isTruncated\", \"type\":\"boolean\" },\n    { \"name\": \"isFavorited\", \"type\":\"boolean\" },\n    { \"name\": \"isRetweeted\", \"type\":\"boolean\" },\n    { \"name\": \"isRetweet\", \"type\":\"boolean\" },\n    { \"name\": \"createdAt\", \"type\":\"long\" }\n  ]\n}\n</figcaption></figure><p>Kafka supports lot of other formats too, but avro is the preferred format for streaming data. You can read more about it here <code>https://www.confluent.io/blog/avro-kafka-data/</code></p>\n<p>Next create a producer</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\nval props = new Properties()\nprops.put(\"bootstrap.servers\", brokerList)\nprops.put(\"client.id\", \"KafkaTweetProducer\")\nprops.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\nprops.put(\"value.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\")\nval producer = new KafkaProducer[String, Array[Byte]](props)\n</figcaption></figure><p>Creata a <code>Schema</code> using the avro schema definition</p>\n<figure class=\"code\"><figcaption>1\nval schema = new Parser().parse(Source.fromURL(getClass.getResource(\"/tweet.avsc\")).mkString)\n</figcaption></figure><p>Serialize the tweet and send it to producer</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\ndef writeToKafka(tweet: Tweet) = {\n  val row = new GenericData.Record(schema)\n  row.put(\"id\", tweet.id)\n  row.put(\"username\", tweet.username)\n  row.put(\"userId\", tweet.userId)\n  row.put(\"userScreenName\", tweet.userScreenName)\n  row.put(\"userDesc\", tweet.userDesc)\n  row.put(\"userProfileImgUrl\", tweet.userProfileImgUrl)\n  row.put(\"favCount\", tweet.favCount)\n  row.put(\"retweetCount\", tweet.retweetCount)\n  row.put(\"lang\", tweet.lang)\n  row.put(\"place\", tweet.place)\n  row.put(\"message\", tweet.message)\n  row.put(\"isSensitive\", tweet.isSensitive)\n  row.put(\"isTruncated\", tweet.isTruncated)\n  row.put(\"isFavorited\", tweet.isFavorited)\n  row.put(\"isRetweeted\", tweet.isRetweeted)\n  row.put(\"isRetweet\", tweet.isRetweet)\n  row.put(\"createdAt\", tweet.createdAt)\n  val writer = new SpecificDatumWriter[GenericRecord](schema)\n  val out = new ByteArrayOutputStream()\n  val encoder = EncoderFactory.get().binaryEncoder(out, null)\n  writer.write(row, encoder)\n  encoder.flush()\n  logger.info(\"Pushing to kafka. TweetId= \" + tweet.id)\n  val data = new ProducerRecord[String, Array[Byte]](topic, out.toByteArray)\n  producer.send(data)\n}\n</figcaption></figure><p>We will create an ActorSystem and put all this inside a <code>KafkaTweetProducer</code> actor. We will then send a message to <code>KafkaTweetProducer</code> whenever a new tweet is recieved.</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nval zkHostKafka = \"localhost:2181/kafka\"\nval kafkaBrokers = \"localhost:9092\"\nval topic = \"tweet1\"\nval system = ActorSystem(\"TwitterAnalysis\")\nval kafkaProducer = system.actorOf(Props(new KafkaTweetProducer(kafkaBrokers, topic)), name = \"kafka_tweet_producer\")\nval twitterStream = new TwitterWatcher(cb, topics, kafkaProducer)\ntwitterStream.startTracking()\n</figcaption></figure><figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nclass TwitterWatcher(cb:ConfigurationBuilder, keywords:List[String], destination:ActorRef) extends Logging {\n override def onStatus(status: Status): Unit = {\n    ...\n    val tweet = Tweet(\n      ...\n    )\n    destination ! tweet\n  }\n}\n</figcaption></figure><figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\nclass KafkaTweetProducer(brokerList:String, topic:String) extends Actor with Logging {\n  override def receive: Receive = {\n    case t:Tweet =&gt;\n      writeToKafka(t)\n    ...\n  }\n}\n</figcaption></figure><p>To test whether this data is getting written in kafka properly on not, you can use the command line console consumer and watch for the topic <code>tweet1</code>:</p>\n<figure class=\"code\"><figcaption>1\nbin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic tweet1 --from-beginning\n</figcaption></figure><p>Next we will consume this data in solr and cassandra</p>\n<h2>Putting data in solr</h2>\n<p>Here’s steps for writing data to solr:</p>\n<ul><li>Define a solr schema(config-set) corresponding to tweet type</li>\n<li>Upload the schmea to zookeeper</li>\n<li>Creata a collection in solr using this config set</li>\n<li>Create a solr consumer which will read from <code>tweet1</code> topic from kafka</li>\n<li>Deserialize the data read from kafka and create solr documents from it</li>\n<li>Send documents to solr</li>\n</ul><p>Here’s what the shema definition will look like:</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n &lt;field name=\"id\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"username\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"userId\" type=\"tlong\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"userScreenName\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"userDesc\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"userProfileImgUrl\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"favCount\" type=\"tlong\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"retweetCount\" type=\"tlong\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"lang\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"place\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"message\" type=\"text_en\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"isSensitive\" type=\"boolean\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"isTruncated\" type=\"boolean\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"isFavorited\" type=\"boolean\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"isRetweeted\" type=\"boolean\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"isRetweet\" type=\"boolean\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"createdAt\" type=\"tdate\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n</figcaption></figure><p>Upload the configset to solr and create a collection:</p>\n<figure class=\"code\"><figcaption>1\n./server/scripts/cloud-scripts/zkcli.sh -cmd upconfig -zkhost localhost:2181 -confdir tweet-schema -confname tweet-schema\n</figcaption></figure><p>Create the collection</p>\n<figure class=\"code\"><figcaption>1\nhttp://localhost:8983/solr/admin/collections?action=create&amp;name=tweet&amp;collection.configName=tweet-schema&amp;numShards=1\n</figcaption></figure><p>Next create a <code>SolrWriter</code> actor which will recieve a <code>Tweet</code> message from a <code>KafkaSolrComsumer</code> (which we will define next), convert it to <code>SolrInputDocument</code> and send it to solr</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\nclass SolrWriter(zkHost: String, collection: String, commitAfterBatch: Boolean) extends Actor with Logging {\n  val client = new CloudSolrClient.Builder().withZkHost(zkHost).build()\n  client.setDefaultCollection(collection)\n  ...\n  var batch = List[SolrInputDocument]()\n  val MAX_BATCH_SIZE = 100\n  override def receive: Receive = {\n    case doc: Tweet =&gt;\n      val solrDoc = new SolrInputDocument()\n      solrDoc.setField(\"id\", doc.id)\n      solrDoc.setField(\"username\", doc.username)\n      ...\n      batch = solrDoc :: batch\n      if (batch.size &gt; MAX_BATCH_SIZE) indexBatch()\n    case FlushBuffer =&gt;\n      indexBatch()\n    case _ =&gt;\n      logger.warn(\"Unknown message\")\n  }\n  def indexBatch(): Boolean = {\n    try {\n      logger.info(\"Flushing batch\")\n      client.add(batch.asJavaCollection)\n      batch = List[SolrInputDocument]()\n      if (commitAfterBatch) client.commit()\n      true\n    } catch {\n      case ex: Exception =&gt;\n        logger.error(s\"Failed to indexing solr batch. Exception is \" + ex.getMessage)\n        ex.printStackTrace()\n        batch = List[SolrInputDocument]()\n        false\n    }\n  }\n  ...\n}\n</figcaption></figure><p>Now we need to define a kafka consumer which will read data from solr and send it to <code>SolrWriter</code></p>\n<h3>Kafka Consumer</h3>\n<p>Consumer will read data from kafka, deserialize it using avro schema, and convert it to <code>Tweet</code> type and forward the message to a destination actor. We will keep the consumer generic so that any destination actor(solr or cassandra) can be passed to it.</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\nclass KafkaTweetConsumer(zkHost:String, groupId:String, topic:String, destination:ActorRef) extends Actor with Logging {\n  ...\n  def read() = try {\n    ...\n    destination ! tweet   //destination will be either solr or cassandra\n    ...\n  }\n}\n</figcaption></figure><p>Create consumer and avro schema object</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nprivate val props = new Properties()\nprops.put(\"group.id\", groupId)\nprops.put(\"zookeeper.connect\", zkHost)\nprops.put(\"auto.offset.reset\", \"smallest\")\nprops.put(\"consumer.timeout.ms\", \"120000\")\nprops.put(\"auto.commit.interval.ms\", \"10000\")\nprivate val consumerConfig = new ConsumerConfig(props)\nprivate val consumerConnector = Consumer.create(consumerConfig)\nprivate val filterSpec = new Whitelist(topic)\nval schemaString = Source.fromURL(getClass.getResource(\"/tweet.avsc\")).mkString\nval schema = new Schema.Parser().parse(schemaString)\n</figcaption></figure><p>Convert binary data to <code>Tweet</code> type using avro</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\nprivate def getTweet(message: Array[Byte]): Tweet = {\n    val reader = new SpecificDatumReader[GenericRecord](schema)\n    val decoder = DecoderFactory.get().binaryDecoder(message, null)\n    val record = reader.read(null, decoder)\n    val tweet = Tweet(\n      id = record.get(\"id\").toString,\n      username = record.get(\"username\").toString,\n      ...\n    )\n    tweet\n  }\n</figcaption></figure><p>Start consuming from kafka and send messages to destination, Solr in this specific case.</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\nval streams = consumerConnector.createMessageStreamsByFilter(filterSpec, 1,new DefaultDecoder(), new DefaultDecoder())(0)\nlazy val iterator = streams.iterator()\nwhile (iterator.hasNext()) {\n  val tweet = getTweet(iterator.next().message())\n  //logger.info(\"Consuming tweet: \" + tweet.id)\n  destination ! tweet\n}\n</figcaption></figure><p>You shoud now start seeing data in solr:</p>\n<figure class=\"code\"><figcaption>1\nhttp://localhost:8983/solr/tweet/select?q=*:*&amp;wt=json&amp;rows=1\n</figcaption></figure><figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n{\n   \"responseHeader\":{\n      \"zkConnected\":true,\n      \"status\":0,\n      \"QTime\":1,\n      \"params\":{\n         \"q\":\"*:*\",\n         \"rows\":\"1\",\n         \"wt\":\"json\"\n      }\n   },\n   \"response\":{\n      \"numFound\":42,\n      \"start\":0,\n      \"docs\":[\n         {\n            \"id\":\"923302396612182016\",\n            \"username\":\"Tawanna Kessler\",\n            \"userId\":898322458742337536,\n            \"userScreenName\":\"tawanna_kessler\",\n            \"userDesc\":\"null\",\n            \"userProfileImgUrl\":\"http://pbs.twimg.com/profile_images/898323854417940484/lke3BSjt_normal.jpg\",\n            \"favCount\":0,\n            \"retweetCount\":183,\n            \"lang\":\"en\",\n            \"place\":\"null\",\n            \"message\":\"RT @craigbrownphd: Two upcoming webinars: Two new Microsoft webinars are taking place over the next week that may… https://t.co/SAb9CMmVXY…\",\n            \"isSensitive\":false,\n            \"isTruncated\":false,\n            \"isFavorited\":false,\n            \"isRetweeted\":false,\n            \"isRetweet\":true,\n            \"createdAt\":\"2017-10-26T03:07:00Z\",\n            \"_version_\":1582267022370144256\n         }\n      ]\n   }\n}\n</figcaption></figure><h2>Querying solr data with banana</h2>\n<p>Banana is a data visualization tool that uses solr for data analysis and display. It can be run in same container as solr. Here’s how to set it up:</p>\n<p>Here’s how to set it up for our tweet data. We will run it in same container as solr:</p>\n<p>Download banana and put it in solr’s webapp direcory</p>\n<figure class=\"code\"><figcaption>1\n2\ncd SOLR_HOME/server/solr-webapp/webapp/\ngit clone https://github.com/lucidworks/banana --depth 1\n</figcaption></figure><p>To save dashboards and setting, banana expects a collection named <code>banana-int</code>. Lets go ahead and create it. Configset for that collection can be obtained found in <code>banana/resources/banana-int-solr-5.0/</code>.</p>\n<p>Upload banana config to zookeeper</p>\n<figure class=\"code\"><figcaption>1\n$SOLR_HOME/server/scripts/cloud-scripts/zkcli.sh -cmd upconfig -zkhost localhost:2181 -confdir banana-int-solr-5.0/conf/ -confname banana\n</figcaption></figure><p>Create the collection</p>\n<figure class=\"code\"><figcaption>1\nhttp://localhost:8983/solr/admin/collections?action=create&amp;name=banana-int&amp;collection.configName=banana&amp;numShards=1\n</figcaption></figure><p>Navigate to banana UI at <code>http://localhost:8983/solr/banana/src/index.html</code> and change the collection in settings to point to <code>tweet</code> collection in</p>\n<p>Here’s what it will look like for our tweets data:</p>\n<p><img src=\"http://saumitra.me/images/posts/banana1.png\" alt=\"image\" /></p>\n<p>Next we will create a cassandra consumer.</p>\n<h2>Putting data in cassandra</h2>\n<ul><li>Download cassandra from <a href=\"http://archive.apache.org/dist/cassandra/3.0.12/apache-cassandra-3.0.12-bin.tar.gz\">http://archive.apache.org/dist/cassandra/3.0.12/apache-cassandra-3.0.12-bin.tar.gz</a> and uncompress it</li>\n<li>Run <code>bin/cassandra</code> to start it</li>\n</ul><p>We need to first create a keyspace and table for storing tweets</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\nCREATE KEYSPACE twitter WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\nCREATE TABLE twitter.tweet (\n  topic text,\n  id text,\n  username text,\n  userId text,\n  userScreenName text,\n  userDesc text,\n  userProfileImgUrl text,\n  favCount bigint,\n  retweetCount bigint,\n  lang text,\n  place text,\n  message text,\n  isSensitive boolean,\n  isTruncated boolean,\n  isFavorited boolean,\n  isRetweeted boolean,\n  isRetweet boolean,\n  createdAt timestamp,\n  creationDate timestamp,\n  PRIMARY KEY ((topic, creationDate), username, id)\n)\n</figcaption></figure><p>Then we will create a <code>CassWriter</code> actor similar to solr one which will accept a tweet message and write it to cassandra.</p>\n<p>Connect to cluster.</p>\n<figure class=\"code\"><figcaption>1\n2\nlazy val cluster = Cluster.builder().addContactPoint(seeds).build()\nlazy val session = cluster.connect(keyspace)\n</figcaption></figure><p>Since we will be using same query repeatedly to insert data with different parameters, hence we will use prepared statement to improve performance:</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\nlazy val prepStmt = session.prepare(s\"INSERT INTO $cf (\" +\n      \"topic, id, username, userId, userScreenName, userDesc, userProfileImgUrl, favCount,\" +\n      \"retweetCount, lang, place, message, isSensitive, isTruncated, isFavorited, isRetweeted,\" +\n      \"isRetweet, createdAt, creationDate\" +\n      \") values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\")\n</figcaption></figure><p>Take <code>Tweet</code>, create a <code>BoundStatement</code> by setting values for all fields and write it to cassandra</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\ndef writeToCass(t:Tweet) = {\n  try {\n    val boundStmt = prepStmt.bind()\n      .setString(\"topic\", topic)\n      .setString(\"id\",t.id)\n      .setString(\"username\", t.username)\n      .setString(\"userId\", t.userId.toString)\n      .setString(\"userScreenName\",t.userScreenName)\n      .setString(\"userDesc\",t.userDesc)\n      .setString(\"userProfileImgUrl\",t.userProfileImgUrl)\n      .setLong(\"favCount\",t.favCount)\n      .setLong(\"retweetCount\",t.retweetCount)\n      .setString(\"lang\",t.lang)\n      .setString(\"place\",t.place)\n      .setString(\"message\",t.message)\n      .setBool(\"isSensitive\",t.isSensitive)\n      .setBool(\"isTruncated\",t.isTruncated)\n      .setBool(\"isFavorited\",t.isFavorited)\n      .setBool(\"isRetweeted\",t.isRetweeted)\n      .setBool(\"isRetweet\",t.isRetweet)\n      .setTimestamp(\"createdAt\", new Date(t.createdAt))\n      .setTimestamp(\"creationDate\", new Date(t.createdAt))\n    session.execute(boundStmt)\n  } catch {\n    case ex: Exception =&gt;\n      logger.error(\"C* insert exception. Message: \" + ex.getMessage)\n  }\n}\n</figcaption></figure><p>We will create a new instance of this actor</p>\n<figure class=\"code\"><figcaption>1\nval cassWriter = system.actorOf(Props(new CassWriter(cassSeeds, cassKeyspace, cassCf, topic)), name = \"cass_writer\")\n</figcaption></figure><p>And then create a new <code>KafkaTweetConsumer</code> whose destination will be this <code>cassWriter</code> actor</p>\n<figure class=\"code\"><figcaption>1\n2\nval cassConsumer = system.actorOf(Props(\n    new KafkaTweetConsumer(zkHostKafka, \"tweet-cass-consumer\", topic, cassWriter)), name = \"cass_consumer\")\n</figcaption></figure><p>You should start seeing data in cassandra</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\ncqlsh&gt; select creationdate, userscreenname, lang, message from twitter.tweet limit 1;\n creationdate             | userscreenname | lang | message\n--------------------------+----------------+------+--------------------------------------------------------------------------------------------------------------------------------------------\n 2017-10-25 21:56:30+0000 |   alevergara78 |   en | RT @HomesAtMetacoda: data in motion &gt;&gt; Online learning: #MachineLearning’s secret for #bigdata via\\n@SASsoftware https://t.co/eGbAumJzEt…\n</figcaption></figure><p>Next we will setup spark and use it to query cassandra data.</p>\n<h2>Query cassandra data with spark</h2>\n<p>We will use datastax spark cassandra connector <a href=\"https://github.com/datastax/spark-cassandra-connector.\">https://github.com/datastax/spark-cassandra-connector.</a> Download the correct connection version jar and place it in lib directory of your project:</p>\n<p>First thing which we need is a spark context</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nval CASS_SEEDS = \"127.0.0.1\"\nval SPARK_MASTER = \"spark://sam-ub:7077\"\nval conf = new SparkConf(true)\n  .set(\"spark.cassandra.connection.host\", CASS_SEEDS)\n  .setJars(Seq(\"lib/spark-cassandra-connector-assembly-2.0.0.jar\"))\n  .setMaster(SPARK_MASTER)\n  .setAppName(\"cass_query\")\nlazy val sc = new SparkContext(conf)\n</figcaption></figure><p>Then you can query and apply different aggregrations. This query will be picked up as a spark job and exectuted on you spark cluster:</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\nval data = sc.cassandraTable(\"twitter\", \"tweets\")\n             .select(\"topic\", \"creationdate\", \"retweetcount\", \"id\", \"isretweet\")\n             .where(\"topic = 'tweets' and creationdate = '2017-10-25 20:15:05+0000'\")\n             .groupBy(_.getLong(\"retweetcount\"))\n             .map(r =&gt; (r._1, r._2.size))\n             .collect()\nlogger.info(\"Count of rows = \" + data)\n</figcaption></figure><p>If job is successfull, you will see the result:</p>\n<figure class=\"code\"><figcaption>1\nCount of rows = 38\n</figcaption></figure><h2>Visulizing cassandra data with zeppelin</h2>\n<p>Zeppelin is a web-based notebook that can be used for interactive data analytics on cassandra data using spark.</p>\n<p>Download the binary from <a href=\"https://zeppelin.apache.org/download.html\">https://zeppelin.apache.org/download.html</a> and uncompress it.\nDefault port used by it is <code>8080</code> which conflicts with spark master web ui port, so change the port in <code>conf/zeppelin-site.xml</code>.</p>\n<p>Create a new notebook and select <code>spark interpreter</code></p>\n<p>Create a view of our <code>tweet</code> table from cassandra</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n%spark.sql\ncreate temporary view mytweets\nusing org.apache.spark.sql.cassandra\noptions (keyspace \"twitter\", table \"tweet\")\n</figcaption></figure><p>We can now run aggregations or other analytics queries on this view:</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n%spark.sql\nselect lang, count(*) as occur from mytweets where lang != 'und' group by lang order by occur desc limit 10\n</figcaption></figure><p>Here’s what output of above query will look like:</p>\n<p><img src=\"http://saumitra.me/images/posts/zepp1.png\" alt=\"image\" /></p>\n<h2>Conclusion</h2>\n<p>I hope you got the idea of how to get started with creating a search and analytics pipeline.</p>",
        "created_at": "2018-05-24T02:01:47+0000",
        "updated_at": "2018-05-24T02:02:04+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 19,
        "domain_name": "saumitra.me",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9788"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 995,
            "label": "testing",
            "slug": "testing"
          }
        ],
        "is_public": false,
        "id": 9764,
        "uid": null,
        "title": "smartcat-labs/berserker",
        "url": "https://github.com/smartcat-labs/berserker",
        "content": "<p>Load generator with modular architecture.</p><p><a href=\"https://travis-ci.org/smartcat-labs/berserker\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/8ac158e9462f9535c5012441dd3a9bf79a8b706a/68747470733a2f2f7472617669732d63692e6f72672f736d6172746361742d6c6162732f6265727365726b65722e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/smartcat-labs/berserker.svg?branch=master\" /></a>\n<a href=\"https://bintray.com/smartcat-labs/maven/berserker/_latestVersion\" rel=\"nofollow\"> <img src=\"https://camo.githubusercontent.com/5ab44ddb3c261cc5f0e932342329abb69ff7e3ca/68747470733a2f2f6170692e62696e747261792e636f6d2f7061636b616765732f736d6172746361742d6c6162732f6d6176656e2f6265727365726b65722f696d616765732f646f776e6c6f61642e737667\" alt=\"Download\" data-canonical-src=\"https://api.bintray.com/packages/smartcat-labs/maven/berserker/images/download.svg\" /></a></p><p>Berserker is designed to be modular from beginning as illustrated on the following diagram.</p><p><a target=\"_blank\" href=\"https://github.com/smartcat-labs/berserker/blob/dev/images/core-design.png\"><img src=\"https://github.com/smartcat-labs/berserker/raw/dev/images/core-design.png\" alt=\"Core Design\" /></a></p><p>Rate generator controls the rate at which load generator operates, rate is expressed on per second basis, or better say, number of impulses which will be generated within one second. Each time impulse is generated load generator fetches data from data source and passes it to worker. Since those are simple interfaces, it is easy to add additional module implementing either data source, worker and even rate generator.\nFollowing diagram represents possible modules for Load Generator of which some are already implemented.</p><p><a target=\"_blank\" href=\"https://github.com/smartcat-labs/berserker/blob/dev/images/architecture.png\"><img src=\"https://github.com/smartcat-labs/berserker/raw/dev/images/architecture.png\" alt=\"Architecture\" /></a></p><p>Berserker is designed as command line tool, but having modular architecture makes it easy to use it as Java library as well.</p><h3>Berserker Commons</h3><p><a href=\"https://github.com/smartcat-labs/berserker/blob/dev/berserker-commons\">Berserker Commons</a> holds interface for core and configuration and it provides signature all the modules need to confront to be able to work together.</p><h3>Berserker Core</h3><p><a href=\"https://github.com/smartcat-labs/berserker/blob/dev/berserker-core\">Berserker Core</a> contains load generator implementation, and common implementations of data source, rate generator and worker.</p><h3>Berserker Runner</h3><p><a href=\"https://github.com/smartcat-labs/berserker/blob/dev/berserker-runner\">Berserker Runner</a> represents runnable jar where desired data source, rate generator and worker can be specified within YAML configuration.\nFollowing section illustrates YAML configuration example.</p><div class=\"highlight highlight-source-yaml\"><pre>load-generator-configuration:\n  data-source-configuration-name: Ranger\n  rate-generator-configuration-name: default\n  worker-configuration-name: Cassandra\n  metrics-reporter-configuration-name: JMX\n  thread-count: 10\n  queue-capacity: 100000\ndata-source-configuration:\n  values:\n    id: uuid()\n    firstName: random(['Peter', 'Mike', 'Steven', 'Joshua', 'John', 'Brandon'])\n    lastName: random(['Smith', 'Johnson', 'Williams', 'Davis', 'Jackson', 'White', 'Lewis', 'Clark'])\n    age: random(20..45)\n    email: string('{}@domain.com', randomLengthString(5))\n    statement:\n      consistencyLevel: ONE\n      query: string(\"INSERT INTO person (id, first_name, last_name, age, email) VALUES ({}, '{}', '{}', {}, '{}');\", $id, $firstName, $lastName, $age, $email)\n  output: $statement\nrate-generator-configuration:\n  rates:\n    r: 1000\n  output: $r\nworker-configuration:\n  connection-points: 0.0.0.0:32770\n  keyspace: my_keyspace\n  async: false\n  bootstrap-commands:\n    - \"CREATE KEYSPACE IF NOT EXISTS my_keyspace WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};\"\n    - USE my_keyspace;\n    - CREATE TABLE IF NOT EXISTS person (id uuid, first_name text, last_name text, age int, email text, primary key (id));\nmetrics-reporter-configuration:\n  domain: berserker\n  filter:</pre></div><p>Main part of configuration is <code>load-generator-configuration</code> where concrete modules which will be used for data source, rate generator and worker need to be specified. After <code>load-generator-configuration</code> section, there should be exactly one section for data source, rate generator and worker.\nEach section is allowed to contain module specific configuration as configuration interpretation will be done by module itself.\nIn order for berserker-runner to be able to find particular module, each module jar must be in classpath.</p><h4>Rate generator configuration</h4><p>Documentation on rate generator configuration can be found <a href=\"https://github.com/smartcat-labs/berserker/blob/dev/rate-generator-configuration.md\">here</a>.</p><h3>Modules</h3><p>List of existing modules:</p><h4>Berserker Ranger</h4><p><a href=\"https://github.com/smartcat-labs/berserker/blob/dev/berserker-ranger\">Berserker Ranger</a> is Ranger data source implementation.</p><h4>Berserker Kafka</h4><p><a href=\"https://github.com/smartcat-labs/berserker/blob/dev/berserker-kafka\">Berserker Kafka</a> is worker implementation which sends messages to Kafka cluster.</p><h4>Berserker Cassandra</h4><p><a href=\"https://github.com/smartcat-labs/berserker/blob/dev/berserker-cassandra\">Berserker Cassandra</a> is worker implementation which executes CQL statements on Cassandra cluster.</p><h4>Berserker HTTP</h4><p><a href=\"https://github.com/smartcat-labs/berserker/blob/dev/berserker-http\">Berserker HTTP</a> is worker implementation which sends HTTP request on configured endpoint.</p><h4>Berserker RabbitMQ</h4><p><a href=\"https://github.com/smartcat-labs/berserker/blob/dev/berserker-rabbitmq\">Berserker RabbitMQ</a> is worker implementation which sends AMQP messages to RabbitMQ.</p><h4>Berserker MQTT</h4><p><a href=\"https://github.com/smartcat-labs/berserker/blob/dev/berserker-mqtt\">Berserker MQTT</a> is worker implementation which publishes messages to MQTT broker.</p><h3>Usage</h3><p>Berserker can be used either as a library or as a stand-alone command line tool.</p><h4>Library usage</h4><p>Artifact can be fetched from bintray.</p><p>Add following <code>repository</code> element to your <code>&lt;repositories&gt;</code> section in <code>pom.xml</code>:</p><div class=\"highlight highlight-text-xml\"><pre>&lt;repository&gt;\n  &lt;id&gt;bintray-smartcat-labs-maven&lt;/id&gt;\n  &lt;name&gt;bintray&lt;/name&gt;\n  &lt;url&gt;https://dl.bintray.com/smartcat-labs/maven&lt;/url&gt;\n&lt;/repository&gt;</pre></div><p>Add the <code>dependency</code> element to your <code>&lt;dependencies&gt;</code> section in <code>pom.xml</code> depending which <code>artifact</code> and <code>version</code> you need:</p><div class=\"highlight highlight-text-xml\"><pre>&lt;dependency&gt;\n  &lt;groupId&gt;io.smartcat&lt;/groupId&gt;\n  &lt;artifactId&gt;artifact&lt;/artifactId&gt;\n  &lt;version&gt;version&lt;/version&gt;\n&lt;/dependency&gt;</pre></div><h4>Command line tool usage</h4><ul><li>Download latest <a href=\"https://bintray.com/smartcat-labs/maven/berserker\" rel=\"nofollow\">Berserker Runner</a> version.</li>\n<li>Create config file (example can be found <a href=\"https://github.com/smartcat-labs/berserker/blob/dev/berserker-runner/src/example/resources/ranger-cassandra.yml\">here</a>).</li>\n<li>Run following command: <code>java -jar berserker-runner-&lt;version&gt;.jar -c &lt;path_to_config_file&gt;</code></li>\n<li>If you need to specify logging options, you can run berserker this way: <code>java -jar -Dlogback.configurationFile=&lt;path to logback.xml&gt; berserker-runner-&lt;version&gt;.jar -c &lt;path_to_config_file&gt;</code></li>\n</ul>",
        "created_at": "2018-05-17T18:23:25+0000",
        "updated_at": "2018-07-07T19:50:48+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/12434092?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9764"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 11,
            "label": "database",
            "slug": "database"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 996,
            "label": "monitoring",
            "slug": "monitoring"
          }
        ],
        "is_public": false,
        "id": 9743,
        "uid": null,
        "title": "KairosDB",
        "url": "https://kairosdb.github.io/",
        "content": "<div class=\"jumbotron\"><div class=\"container\"><img src=\"https://kairosdb.github.io/img/kairosdb_dark.png\" alt=\"KairosDB logo\" /><p class=\"subtitle\">Fast Time Series Database on Cassandra.</p></div></div>\n\t<div class=\"container\"><div class=\"features\"><p></p><h2>Rich features to answer your needs</h2>\n\t\t\t\t<hr class=\"blue-divider\" /><div class=\"row  text-center\"><div class=\"col-md-3\"><i class=\"fa fa-circle fa-stack-2x\"><i class=\"fa fa-cogs fa-stack-1x fa-inverse\">CollectorsData can be pushed in KairosDB via multiple\n\t\t\t\t\t\tprotocols : Telnet, Rest, Graphite. Other mechanisms such as\n\t\t\t\t\t\tplugins can also be used\n\t\t\t\t\t\t<a class=\"btn btn-default\" href=\"https://kairosdb.github.io/docs/build/html/PushingData.html\" role=\"button\">Learn more »</a>\n\t\t\t\t\t</i></i></div><div class=\"col-md-3\"><i class=\"fa fa-circle fa-stack-2x\"><i class=\"fa fa-database fa-stack-1x fa-inverse\">\n\t\t\t\t\t\tStorage\n\t\t\t\t\t\n\t\t\t\t\t\tKairosDB stores time series in <a href=\"http://cassandra.apache.org/\">Cassandra</a>, the popular\n\t\t\t\t\t\tand performant NoSQL datastore. The schema consists of 3 column\n\t\t\t\t\t\tfamilies...\n\t\t\t\t\t\n\t\t\t\t\t\t<a class=\"btn btn-default\" href=\"https://kairosdb.github.io/docs/build/html/CassandraSchema.html\" role=\"button\">Learn\n\t\t\t\t\t\t\tmore »</a>\n\t\t\t\t\t</i></i></div><div class=\"col-md-3\"><i class=\"fa fa-circle fa-stack-2x\"><i class=\"fa fa-code fa-stack-1x fa-inverse\">\n\t\t\t\t\t\tRest API\n\t\t\t\t\tThis API provides operations to list existing\n\t\t\t\t\t\tmetric names, list tag names and values, store metric data points,\n\t\t\t\t\t\tand query for metric data points.\n\t\t\t\t\t\t<a class=\"btn btn-default\" href=\"https://kairosdb.github.io/docs/build/html/restapi/Overview.html\" role=\"button\">Learn\n\t\t\t\t\t\t\tmore »</a>\n\t\t\t\t\t</i></i></div><div class=\"col-md-3\"><i class=\"fa fa-circle fa-stack-2x\"><i class=\"fa fa-line-chart fa-stack-1x fa-inverse\">\n\t\t\t\t\t\tWeb UI\n\t\t\t\t\tWith a default install, KairosDB serve up a\n\t\t\t\t\t\tquery page whereby you can query data within the data store. It's\n\t\t\t\t\t\tdesigned primarily for development purposes.\n\t\t\t\t\t\t<a class=\"btn btn-default\" href=\"https://kairosdb.github.io/docs/build/html/WebUI.html\" role=\"button\">Learn more »</a>\n\t\t\t\t\t</i></i></div></div><div class=\"row text-center\"><div class=\"col-md-3\"><i class=\"fa fa-circle fa-stack-2x\"><i class=\"fa fa-cog fa-stack-1x fa-inverse\">\n\t\t\t\t\t\tAggregators\n\t\t\t\t\tAggregators perform an operation on data points\n\t\t\t\t\t\tand down samples. Standard functions like min, max, sum, count,\n\t\t\t\t\t\tmean and more are available\n\t\t\t\t\t\t<a class=\"btn btn-default\" href=\"https://kairosdb.github.io/docs/build/html/restapi/QueryMetrics.html\" role=\"button\">Learn\n\t\t\t\t\t\t\tmore »</a>\n\t\t\t\t\t</i></i></div><div class=\"col-md-3\"><i class=\"fa fa-circle fa-stack-2x\"><i class=\"fa fa-exchange fa-stack-1x fa-inverse\">\n\t\t\t\t\t\tTools\n\t\t\t\t\tImport and export is available on the KairosDB\n\t\t\t\t\t\tserver from the command line. Internal metrics to the data store\n\t\t\t\t\t\tcan monitor the server’s performance\n\t\t\t\t\t\t<a class=\"btn btn-default\" href=\"https://kairosdb.github.io/docs/build/html/ImportExport.html\" role=\"button\">Learn\n\t\t\t\t\t\t\tmore »</a>\n\t\t\t\t\t</i></i></div><div class=\"col-md-3\"><i class=\"fa fa-circle fa-stack-2x\"><i class=\"fa fa-coffee fa-stack-1x fa-inverse\">\n\t\t\t\t\t\tClient library\n\t\t\t\t\tThe KairosDB client is a Java library, using\n\t\t\t\t\t\tthe HttpClient class, that makes sending metrics and querying the\n\t\t\t\t\t\tKairosDB server simple.\n\t\t\t\t\t\t<a class=\"btn btn-default\" href=\"https://github.com/kairosdb/kairosdb-client/\" role=\"button\">Learn\n\t\t\t\t\t\t\tmore »</a>\n\t\t\t\t\t</i></i></div><div class=\"col-md-3\"><i class=\"fa fa-circle fa-stack-2x\"><i class=\"fa fa-plug fa-stack-1x fa-inverse\">\n\t\t\t\t\t\tPlugins\n\t\t\t\t\tKairosDB can be extended in various ways (data\n\t\t\t\t\t\tpoint listeners, data stores, protocol handlers,...) using plugins\n\t\t\t\t\t\tbased on Guice.\n\t\t\t\t\t\t<a class=\"btn btn-default\" href=\"https://kairosdb.github.io/docs/build/html/kairosdevelopment/Plugins.html\" role=\"button\">Learn more »</a>\n\t\t\t\t\t</i></i></div></div></div></div>",
        "created_at": "2018-05-15T01:40:30+0000",
        "updated_at": "2018-05-15T01:40:46+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "kairosdb.github.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9743"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 93,
            "label": "data",
            "slug": "data"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 931,
            "label": "data.engineering",
            "slug": "data-engineering"
          }
        ],
        "is_public": false,
        "id": 9723,
        "uid": null,
        "title": "Getting started with the Kafka Connect Cassandra Source",
        "url": "https://medium.com/walmartlabs/getting-started-with-the-kafka-connect-cassandra-source-e6e06ec72e97",
        "content": "<p id=\"ebf2\" class=\"graf graf--p graf-after--h3\">This post will look at how to setup and tune the <a href=\"http://lenses.stream/connectors/source/cassandra.html\" data-href=\"http://lenses.stream/connectors/source/cassandra.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Cassandra Source connector</a> that is available from <a href=\"http://www.landoop.com/\" data-href=\"http://www.landoop.com/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Landoop</a>. The Cassandra Source connector is used to read data from a Cassandra table, writing the contents into a Kafka topic using only a configuration file. This enables data that has been saved to be easily turned into an event stream.</p><figure id=\"f902\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*Fi0Zp7l0lbj-y9QT.png\" data-width=\"720\" data-height=\"240\" data-action=\"zoom\" data-action-value=\"0*Fi0Zp7l0lbj-y9QT.png\" src=\"https://cdn-images-1.medium.com/max/1600/0*Fi0Zp7l0lbj-y9QT.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\">all logos are trademark of Apache Foundation</figcaption></div></figure><p id=\"85be\" class=\"graf graf--p graf-after--figure\">In our example we will be capturing data representing a pack (i.e. a large box) of items being shipped. Each pack is pushed to consumers in a JSON format on a Kafka topic.</p><h3 id=\"d7da\" class=\"graf graf--h3 graf-after--p\">The Cassandra data model and Cassandra Source connector</h3><p id=\"10c8\" class=\"graf graf--p graf-after--h3\">Modeling data in Cassandra must be done around the queries that are needed to access the data (see <a href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\" data-href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">this article</a> for details). Typically this means that there will be one table for each query and data (in our case about the pack) will be duplicated across numerous tables.</p><p id=\"afac\" class=\"graf graf--p graf-after--p\">Regardless of the other tables used for the product, the Cassandra Source connector needs a table that will allow us to query for data using a time range. The connector is designed around its ability to generate a CQL query based on configuration. It uses this query to retrieve data from the table that is available within a configurable time range. Once all of this data has been published, Kafka Connect will mark the upper end of the time range as an offset. The connector will then query the table for more data using the next time range starting with the date/time stored in the offset. We will look at how to configure this later. For now we want to focus on the constraints for the table. Since Cassandra doesn’t support joins, the table we are pulling data from must have all of the data that we want to put onto the Kafka topic. Data in other tables will not be available to Kafka Connect.</p><p id=\"361f\" class=\"graf graf--p graf-after--p\">In it’s simplest form a table used by the Cassandra Source connector might look like this:</p><pre id=\"e45f\" class=\"graf graf--pre graf-after--p\">CREATE TABLE IF NOT EXISTS “pack_events” (<br />event_id TEXT, <br />event_ts TIMESTAMP, <br />event_data TEXT, <br />PRIMARY KEY ((event_id),event_ts));</pre><p id=\"8ca8\" class=\"graf graf--p graf-after--pre\">The <code class=\"markup--code markup--p-code\">event_id</code> is the partition key. This is used by Cassandra to determine which nodes in the cluster will store the data. The <code class=\"markup--code markup--p-code\">event_ts</code> is part of the cluster key. It determines the order of the data within the partition (see <a href=\"https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key\" data-href=\"https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">this article for details</a>). It is also the column that is used by the Cassandra source connector to manage time ranges. In this example, the <code class=\"markup--code markup--p-code\">event_data</code> column stores the JSON representation of the pack.</p><p id=\"92bc\" class=\"graf graf--p graf-after--p\">This is not the only structure for a table that will work. The table that is queried by the Cassandra Source connector can use numerous columns to represent the partition key and the data. However, <strong class=\"markup--strong markup--p-strong\">the connector requires a single time based column</strong> (either <code class=\"markup--code markup--p-code\">TIMESTAMP</code> or <code class=\"markup--code markup--p-code\">TIMEUUID</code>) in order to work correctly.</p><p id=\"10c3\" class=\"graf graf--p graf-after--p\">This would be an equally valid table for use with the Cassandra Source connector.</p><pre id=\"3608\" class=\"graf graf--pre graf-after--p\">CREATE TABLE IF NOT EXISTS “kc_events” (<br />event_id1 TEXT, <br />event_id2 TEXT, <br />event_ts TIMEUUID, <br />event_data1 TEXT, <br />event_data2 TEXT, <br />PRIMARY KEY ((event_id1, event_id2)));</pre><p id=\"6eb6\" class=\"graf graf--p graf-after--pre\">The most efficient way to access data in this table is to query for data with the partition key. This would allow Cassandra to quickly identify the node containing the data we are interested in.</p><pre id=\"1c5f\" class=\"graf graf--pre graf-after--p\">SELECT * FROM pack_events WHERE event_id = “1234”;</pre><p id=\"9ca8\" class=\"graf graf--p graf-after--pre\">However, the Cassandra Source connector has no way of knowing the ids of the data that it will need to publish to a Kafka topic. That is why it uses a time range.</p><p id=\"89b1\" class=\"graf graf--p graf-after--p\">The reason we can’t use the <code class=\"markup--code markup--p-code\">event_ts</code> as the partition key is because Cassandra does not support these operators (&gt;, &gt;=, &lt;=, &lt;) on the partition key when querying. And without these we would not be able to query across date/time ranges (see <a href=\"https://www.datastax.com/dev/blog/a-deep-look-to-the-cql-where-clause\" data-href=\"https://www.datastax.com/dev/blog/a-deep-look-to-the-cql-where-clause\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">this article for details</a>).</p><p id=\"88ff\" class=\"graf graf--p graf-after--p\">There’s just one more thing. If we tried to run the following query it would fail.</p><pre id=\"a243\" class=\"graf graf--pre graf-after--p\">SELECT * FROM pack_events <br />WHERE event_ts &gt; ‘2018–01–22T20:28:20.869Z’ <br />AND event_ts &lt;= '2018-01-22T20:28:50.869Z';</pre><p id=\"25fc\" class=\"graf graf--p graf-after--pre\">The connector must supply the <code class=\"markup--code markup--p-code\">ALLOW FILTERING</code> option to the end of this query for it to work. This addition allows Cassandra to search all of the nodes in the cluster for the data in the specified time range (see<a href=\"https://www.datastax.com/dev/blog/allow-filtering-explained-2\" data-href=\"https://www.datastax.com/dev/blog/allow-filtering-explained-2\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\"> this article for details</a>).</p><h3 id=\"3efd\" class=\"graf graf--h3 graf-after--p\">Configuring the connector: KCQL basics</h3><p id=\"0b92\" class=\"graf graf--p graf-after--h3\">The Landoop connectors are configured using Kafka Connect Query Language (KCQL). This provides a concise and consistent way to configure the connectors (at least the ones from Landoop). The KCQL and other basic properties are provided via a JSON formatted property file.</p><p id=\"f64d\" class=\"graf graf--p graf-after--p\">For the sake of this post, let’s create a file named <code class=\"markup--code markup--p-code\">connect-cassandra-source.json</code>.</p><pre id=\"a5e0\" class=\"graf graf--pre graf-after--p\">{ <br />“name”: “packs”, <br />“config”: { <br />“tasks.max”: “1”, <br />“connector.class”: … </pre><p id=\"46c8\" class=\"graf graf--p graf-after--pre\">The <code class=\"markup--code markup--p-code\">name</code> of the connector needs to be unique across all the connectors installed into Kafka Connect.</p><p id=\"bad2\" class=\"graf graf--p graf-after--p\">The <code class=\"markup--code markup--p-code\">connector.class</code> is used to specify which connector is being used.​​</p><ul class=\"postList\"><li id=\"036c\" class=\"graf graf--li graf-after--p\"><code class=\"markup--code markup--li-code\">com.datamountaineer.streamreactor.connect.cassandra.source.CassandraSourceConnector</code></li></ul><p id=\"ebc5\" class=\"graf graf--p graf-after--li\">The next set of configuration (shown below) is used to specify the information needed to connect to the Cassandra cluster and which keyspace to use.</p><ul class=\"postList\"><li id=\"f802\" class=\"graf graf--li graf-after--p\"><code class=\"markup--code markup--li-code\">​connect.cassandra.contact.points</code></li><li id=\"a421\" class=\"graf graf--li graf-after--li\"><code class=\"markup--code markup--li-code\">connect.cassandra.port</code></li><li id=\"1a09\" class=\"graf graf--li graf-after--li\"><code class=\"markup--code markup--li-code\">connect.cassandra.username</code></li><li id=\"978e\" class=\"graf graf--li graf-after--li\"><code class=\"markup--code markup--li-code\">connect.cassandra.password</code></li><li id=\"a2b0\" class=\"graf graf--li graf-after--li\"><code class=\"markup--code markup--li-code\">connect.cassandra.consistency.level</code></li><li id=\"f3e5\" class=\"graf graf--li graf-after--li\"><code class=\"markup--code markup--li-code\">connect.cassandra.key.space</code></li></ul><pre id=\"86de\" class=\"graf graf--pre graf-after--li\">{ <br />“name”: “packs”, <br />“config”: { <br />“tasks.max”: “1”, <br />“connector.class”: “com.datamountaineer.streamreactor.connect.cassandra.source.CassandraSourceConnector”, <br />“connect.cassandra.contact.points”: “localhost”,    <br />“connect.cassandra.port”: 9042, <br />“connect.cassandra.username”: “cassandra”,   <br />“connect.cassandra.password”: “cassandra”,<br />“connect.cassandra.consistency.level”: “LOCAL_ONE”,<br />“connect.cassandra.key.space”: “blog”, “connect.cassandra.import.mode”: “incremental”, <br />“connect.cassandra.kcql”: “INSERT INTO test_topic SELECT event_data, event_ts FROM pack_events IGNORE event_ts PK event_ts WITHUNWRAP INCREMENTALMODE=TIMESTAMP”, … <br />} <br />}</pre><p id=\"682e\" class=\"graf graf--p graf-after--pre\">There are two values for the <code class=\"markup--code markup--p-code\">connect.cassandra.import.mode</code>. Those are <code class=\"markup--code markup--p-code\">bulk</code> and <code class=\"markup--code markup--p-code\">incremental</code>. The <code class=\"markup--code markup--p-code\">bulk</code> option will query everything in the table <em class=\"markup--em markup--p-em\">every time</em> that the Kafka Connect polling occurs. We will set this to <code class=\"markup--code markup--p-code\">incremental</code>.</p><p id=\"7bdb\" class=\"graf graf--p graf-after--p\">The interesting part of the configuration is the <code class=\"markup--code markup--p-code\">connect.cassandra.kcql</code> property (shown above). The KCQL statement tells the connector which table in the Cassandra cluster to use, how to use the columns on the table, and where to publish the data.</p><p id=\"0cb8\" class=\"graf graf--p graf-after--p\">The first part of the KCQL statement tells the connector the name of the Kafka topic where the data will be published. In our case that is the topic named <code class=\"markup--code markup--p-code\">test_topic</code>.</p><pre id=\"7d9c\" class=\"graf graf--pre graf-after--p\">INSERT INTO test_topic</pre><p id=\"4e45\" class=\"graf graf--p graf-after--pre\">The next part of the KCQL statement tells the connector how to deal with the table. The <code class=\"markup--code markup--p-code\">SELECT/FROM</code> specifies the table to poll with the queries. It also specifies the columns whose values should be retrieved. The column that keeps track of the date/time must be part of the <code class=\"markup--code markup--p-code\">SELECT</code>statement. However, if we don't want that data as part of what we publish to the Kafka topic we can use the <code class=\"markup--code markup--p-code\">IGNORE.</code></p><pre id=\"a537\" class=\"graf graf--pre graf-after--p\">SELECT event_data, event_ts FROM pack_events IGNORE event_ts</pre><p id=\"af45\" class=\"graf graf--p graf-after--pre\">The next part of the statement, the <code class=\"markup--code markup--p-code\">PK</code>, tells the connector which of the columns is used to manage the date/time. This is considered the primary key for the connector.</p><pre id=\"ba12\" class=\"graf graf--pre graf-after--p\">PK event_ts WITHUNWRAP INCREMENTALMODE=”TIMESTAMP”</pre><p id=\"3009\" class=\"graf graf--p graf-after--pre\">The <code class=\"markup--code markup--p-code\">INCREMENTALMODE</code> tells the connector what the data type of the <code class=\"markup--code markup--p-code\">PK</code> column is. That is going to be either <code class=\"markup--code markup--p-code\">TIMESTAMP</code> or <code class=\"markup--code markup--p-code\">TIMEUUID</code>.</p><p id=\"43cc\" class=\"graf graf--p graf-after--p\">Finally, the <code class=\"markup--code markup--p-code\">WITHUNWRAP</code> option tells the connector to publish the data to the topic as a String rather than as a JSON object.</p><p id=\"cf66\" class=\"graf graf--p graf-after--p\">For example, if we had the following value in the <code class=\"markup--code markup--p-code\">event_data</code> column:</p><pre id=\"69e1\" class=\"graf graf--pre graf-after--p\">{ “foo”:”bar” }</pre><p id=\"e638\" class=\"graf graf--p graf-after--pre\">We would want to publish this as seen above.</p><p id=\"462c\" class=\"graf graf--p graf-after--p\">Leaving the <code class=\"markup--code markup--p-code\">WITHUNWRAP</code> option off will result in the following value being published to the topic.</p><pre id=\"4821\" class=\"graf graf--pre graf-after--p\">{ <br />“schema”: {<br />“type”: “struct”, <br />“fields”: [{ <br />“type”: “string”,<br />“optional”: true,<br />“field”: “event_data” <br />}],<br />“optional”: false, <br />“name”: “blog.pack_events” <br />}, <br />“payload”: { <br />“event_data”: “{\\”foo\\”:\\”bar\\”}” <br />} <br />}</pre><p id=\"b0d5\" class=\"graf graf--p graf-after--pre\">If we leave <code class=\"markup--code markup--p-code\">WITHUNWRAP</code> off, when using the <code class=\"markup--code markup--p-code\">StringConverter</code> (more on that later) we would get the following:</p><pre id=\"b7c7\" class=\"graf graf--pre graf-after--p\">Struct:{event_data={“foo”:”bar\"}}</pre><p id=\"8d85\" class=\"graf graf--p graf-after--pre\">We will need to use the combination of <code class=\"markup--code markup--p-code\">WITHUNWRAP</code> and the<code class=\"markup--code markup--p-code\">StringConverter</code> to get the result we want.</p><h3 id=\"38f1\" class=\"graf graf--h3 graf-after--p\">Configuring the connector: Tuning Parameters</h3><p id=\"c7f8\" class=\"graf graf--p graf-after--h3\">We’ll explore these in another post. But for now let’s start looking for data in our table with a starting date/time of today. We’ll also poll every second.</p><pre id=\"a759\" class=\"graf graf--pre graf-after--p\">{ <br />“name”: “packs”, <br />“config”: { <br />“tasks.max”: “1”,<br />… <br />“connect.cassandra.initial.offset”: “2018–01–22 00:00:00.0000000Z”, <br />“connect.cassandra.import.poll.interval”: 1000 <br />} <br />}</pre><h3 id=\"070d\" class=\"graf graf--h3 graf-after--pre\">Setting up the infrastructure</h3><p id=\"3e92\" class=\"graf graf--p graf-after--h3\">We will be using the following products:</p><ul class=\"postList\"><li id=\"20b6\" class=\"graf graf--li graf-after--p\">Apache Cassandra 3.11.1</li><li id=\"a854\" class=\"graf graf--li graf-after--li\">Apache Kafka and Kafka Connect 1.0</li><li id=\"0b78\" class=\"graf graf--li graf-after--li\">Landoop Cassandra Source 1.0</li></ul><h3 id=\"cd3f\" class=\"graf graf--h3 graf-after--li\">Installing Cassandra</h3><p id=\"15df\" class=\"graf graf--p graf-after--h3\">Installation instructions for Apache Cassandra can be found on the web (<a href=\"https://cassandra.apache.org/doc/latest/getting_started/installing.html\" data-href=\"https://cassandra.apache.org/doc/latest/getting_started/installing.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">link</a>). Once installed and started the cluster can be verified using the following command:</p><pre id=\"3564\" class=\"graf graf--pre graf-after--p\">nodetool -h [IP] status</pre><p id=\"f19b\" class=\"graf graf--p graf-after--pre\">this will generate a response as follows:</p><pre id=\"6a27\" class=\"graf graf--pre graf-after--p\">Datacenter: dc1<br />===============<br />Status=Up/Down<br />|/ State=Normal/Leaving/Joining/Moving<br />--  Address   Load       Tokens       Owns (effective)  Host ID   Rack<br />UN  10.x.x.x  96.13 GiB   64           39.6%            [UUID]    r6<br />UN  10.x.x.x  148.98 GiB  64           33.6%            [UUID]    r5<br />UN  10.x.x.x  88.08 GiB   64           36.4%            [UUID]    r5<br />UN  10.x.x.x  97.96 GiB   64           30.4%            [UUID]    r6<br />UN  10.x.x.x  146.89 GiB  64           33.2%            [UUID]    r7<br />UN  10.x.x.x  205.24 GiB  64           36.8%            [UUID]    r7</pre><h3 id=\"b219\" class=\"graf graf--h3 graf-after--pre\">Installing Kafka and Kafka Connect</h3><p id=\"c5fe\" class=\"graf graf--p graf-after--h3\">Kafka Connect is shipped and installed as part of Apache Kafka. Instructions for these are also available on the web (<a href=\"https://kafka.apache.org/quickstart\" data-href=\"https://kafka.apache.org/quickstart\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">link</a>).</p><ol class=\"postList\"><li id=\"1fbd\" class=\"graf graf--li graf-after--p\">Download the tar file (<a href=\"https://kafka.apache.org/downloads\" data-href=\"https://kafka.apache.org/downloads\" class=\"markup--anchor markup--li-anchor\" rel=\"nofollow noopener\" target=\"_blank\">link</a>).</li><li id=\"85ed\" class=\"graf graf--li graf-after--li\">Install the tar file</li></ol><pre id=\"f250\" class=\"graf graf--pre graf-after--li\">tar -xzf kafka_2.11–1.0.0.tgz <br />cd kafka_2.11–1.0.0</pre><h3 id=\"0bdd\" class=\"graf graf--h3 graf-after--pre\">Starting Kafka</h3><p id=\"ff0d\" class=\"graf graf--p graf-after--h3\">This post will not attempt to explain the architecture behind a Kafka cluster. However, a typical installation will have several Kafka brokers and Apache Zookeeper.</p><figure id=\"8095\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*HoKUvzx_Q25-P5Rq.png\" data-width=\"439\" data-height=\"230\" src=\"https://cdn-images-1.medium.com/max/1600/0*HoKUvzx_Q25-P5Rq.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\">all logos are trademark of Apache Foundation</figcaption></div></figure><p id=\"846b\" class=\"graf graf--p graf-after--figure\">To run Kafka, first start Zookeeper, then start the Kafka brokers. The commands below assume a local installation with only one node.</p><pre id=\"eed8\" class=\"graf graf--pre graf-after--p\">bin/zookeeper-server-start.sh config/zookeeper.properties</pre><p id=\"77cd\" class=\"graf graf--p graf-after--pre\">and</p><pre id=\"ded3\" class=\"graf graf--pre graf-after--p\">bin/kafka-server-start.sh config/server.properties</pre><p id=\"7528\" class=\"graf graf--p graf-after--pre\">Once we have Kafka installed and running, we need to create four topics. One is used by our application to publish our pack JSON. The other three are required by Kafka Connect. We will continue to assume that most are running this initially on a laptop so we will set the replication factor to 1.</p><pre id=\"541c\" class=\"graf graf--pre graf-after--p\">bin/kafka-topics.sh — create — topic test_topic -zookeeper localhost:2181 — replication-factor 1 — partitions 3</pre><p id=\"1251\" class=\"graf graf--p graf-after--pre\">and</p><pre id=\"054b\" class=\"graf graf--pre graf-after--p\">bin/kafka-topics.sh — create — zookeeper localhost:2181 — topic connect-configs — replication-factor 1 — partitions 1 — config cleanup.policy=compact </pre><pre id=\"43f4\" class=\"graf graf--pre graf-after--pre\">bin/kafka-topics.sh — create — zookeeper localhost:2181 — topic connect-offsets — replication-factor 1 — partitions 50 — config cleanup.policy=compact </pre><pre id=\"ad4c\" class=\"graf graf--pre graf-after--pre\">bin/kafka-topics.sh — create — zookeeper localhost:2181 — topic connect-status — replication-factor 1 — partitions 10 — config cleanup.policy=compact</pre><p id=\"1461\" class=\"graf graf--p graf-after--pre\">In order to verify that the four topics have been created, run the following command:</p><pre id=\"f0fe\" class=\"graf graf--pre graf-after--p\">bin/kafka-topics.sh — list — zookeeper localhost:2181</pre><h3 id=\"e7e9\" class=\"graf graf--h3 graf-after--pre\">Installing the Cassandra Source connector</h3><p id=\"9f7c\" class=\"graf graf--p graf-after--h3\">Landoop offers numerous connectors for Kafka Connect. These are all available as open source. The first thing we need to do is download the Cassandra Source connector jar file (<a href=\"https://github.com/Landoop/stream-reactor/releases\" data-href=\"https://github.com/Landoop/stream-reactor/releases\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">link</a>).</p><ul class=\"postList\"><li id=\"9df6\" class=\"graf graf--li graf-after--p\">kafka-connect-cassandra-1.0.0–1.0.0-all.tar.gz</li></ul><p id=\"c2c7\" class=\"graf graf--p graf-after--li\">Unzip the tar file and copy the jar file to the <code class=\"markup--code markup--p-code\">libs</code> folder under the Kafka install directory.</p><h3 id=\"0a03\" class=\"graf graf--h3 graf-after--p\">Configuring Kafka Connect</h3><p id=\"f0d8\" class=\"graf graf--p graf-after--h3\">We need to tell Kafka Connect where the Kafka cluster is. In the <code class=\"markup--code markup--p-code\">config</code> folder where Kafka was installed we will find the file: <code class=\"markup--code markup--p-code\">connect-distributed.properties.</code>Look for the <code class=\"markup--code markup--p-code\">bootstrap.servers</code> key. Update that to point to the cluster.</p><pre id=\"01dd\" class=\"graf graf--pre graf-after--p\">bootstrap.servers=localhost:9092</pre><h3 id=\"66ff\" class=\"graf graf--h3 graf-after--pre\">Starting Kafka Connect</h3><p id=\"18b5\" class=\"graf graf--p graf-after--h3\">We can now start up our distributed Kafka Connect service. For more information on stand-alone vs distributed mode, see the documentation (<a href=\"https://docs.confluent.io/current/connect/userguide.html\" data-href=\"https://docs.confluent.io/current/connect/userguide.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">link</a>).</p><pre id=\"62ce\" class=\"graf graf--pre graf-after--p\">bin/connect-distributed.sh config/connect-distributed.properties</pre><p id=\"24bf\" class=\"graf graf--p graf-after--pre\">If all has gone well you should see the following on your console:</p><figure id=\"867a\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*M6D9vzHCNtCScBNN.png\" data-width=\"817\" data-height=\"260\" data-action=\"zoom\" data-action-value=\"0*M6D9vzHCNtCScBNN.png\" src=\"https://cdn-images-1.medium.com/max/1600/0*M6D9vzHCNtCScBNN.png\" alt=\"image\" /></div></div></figure><p id=\"627d\" class=\"graf graf--p graf-after--figure\">In case you are wondering , “Data Mountaineer”, was the name of the company before being renamed to Landoop.</p><h3 id=\"fb57\" class=\"graf graf--h3 graf-after--p\">Adding the Cassandra Source connector</h3><p id=\"cfca\" class=\"graf graf--p graf-after--h3\">Kafka Connect has a REST API to interact with connectors (<a href=\"https://docs.confluent.io/current/connect/restapi.html\" data-href=\"https://docs.confluent.io/current/connect/restapi.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">check this out for details</a> on the API). We need to add the Cassandra Source connector to the Kafka Connect. This is done by sending the property file (<code class=\"markup--code markup--p-code\">connect-cassandra-source.json</code>) to Kafka Connect through the REST API.</p><pre id=\"bb81\" class=\"graf graf--pre graf-after--p\">curl -X POST -H “Content-Type: application/json” -d @connect-cassandra-source.json localhost:8083/connectors</pre><p id=\"47f0\" class=\"graf graf--p graf-after--pre\">Once we have successfully loaded the connector, we can check to see the installed connectors using this API:</p><pre id=\"6cc1\" class=\"graf graf--pre graf-after--p\">curl localhost:8083/connectors</pre><p id=\"dacd\" class=\"graf graf--p graf-after--pre\">That should return a list of the connectors by their configured names.</p><pre id=\"b394\" class=\"graf graf--pre graf-after--p\">[“packs”]</pre><h3 id=\"4ce8\" class=\"graf graf--h3 graf-after--pre\">Testing the Cassandra Source connector</h3><p id=\"2758\" class=\"graf graf--p graf-after--h3\">In order to test everything out we will need to insert some data into our table.</p><pre id=\"3d44\" class=\"graf graf--pre graf-after--p\">INSERT INTO pack_events (event_id, event_ts, event_data) <br />VALUES (‘500’, ‘2018–01–22T20:28:50.869Z’, ‘{“foo”:”bar”}’);</pre><p id=\"5f22\" class=\"graf graf--p graf-after--pre\">We can check what is being written to the Kafka topic by running the following command:</p><pre id=\"7fa3\" class=\"graf graf--pre graf-after--p\">bin/kafka-console-consumer.sh — bootstrap-server localhost:9092 — topic test_topic</pre><p id=\"b4a1\" class=\"graf graf--p graf-after--pre\">At this point, we might be surprised to see something like this:</p><pre id=\"eaa5\" class=\"graf graf--pre graf-after--p\">{ <br />“schema”:{ <br />“type”:”string”, <br />“optional”:false <br />}, <br />“payload”:”{\\”foo\\”:\\”bar\\”}” <br />}</pre><p id=\"361b\" class=\"graf graf--p graf-after--pre\">That is better than what we were getting without <code class=\"markup--code markup--p-code\">WITHUNWRAP</code> but isn't exactly what we were hoping for. To get the JSON value that was written to the table column we need to update the <code class=\"markup--code markup--p-code\">connect-distributed.properties</code>file. Open this up and look for <code class=\"markup--code markup--p-code\">JsonConverter</code>. Replace those lines with the following:</p><pre id=\"e984\" class=\"graf graf--pre graf-after--p\">key.converter=org.apache.kafka.connect.storage.StringConverter value.converter=org.apache.kafka.connect.storage.StringConverter</pre><p id=\"b8d7\" class=\"graf graf--p graf-after--pre\">Restart Kafka Connect.<br /> Insert another row into the table.<br /> Now we should get what we want.</p><pre id=\"012c\" class=\"graf graf--pre graf-after--p\">{ “foo”:”bar” }</pre><p id=\"08e6\" class=\"graf graf--p graf-after--pre\">Happy coding!</p><p id=\"3cbb\" class=\"graf graf--p graf-after--p graf--trailing\">This originally appeared on TheAgileJedi blog (<a href=\"https://theagilejedi.wordpress.com/2018/01/23/using-the-kafka-connect-cassandra-source-part-1/\" data-href=\"https://theagilejedi.wordpress.com/2018/01/23/using-the-kafka-connect-cassandra-source-part-1/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">here</a>)</p>",
        "created_at": "2018-05-12T02:38:32+0000",
        "updated_at": "2018-10-22T23:15:49+0000",
        "published_at": "2018-03-13T22:05:21+0000",
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 11,
        "domain_name": "medium.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/0*Fi0Zp7l0lbj-y9QT.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9723"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 253,
            "label": "analytics",
            "slug": "analytics"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 955,
            "label": "blockchain",
            "slug": "blockchain"
          }
        ],
        "is_public": false,
        "id": 9660,
        "uid": null,
        "title": "How NerdWallet and BlockCypher are Building Data Platforms",
        "url": "https://medium.com/sfdata/data-platforms-in-fintech-spark-kafka-and-amazon-redshift-6c393a0c29ae",
        "content": "<div class=\"section-divider\"><hr class=\"section-divider\" /></div><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><h1 id=\"a67d\" class=\"graf graf--h3 graf--leading graf--title\">How NerdWallet and BlockCypher are Building Data Platforms</h1></div><div class=\"section-inner sectionLayout--fullWidth\"><figure id=\"acd1\" class=\"graf graf--figure graf--layoutFillWidth graf-after--h3\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*rCWlWxJ9-REAtidxv5XG3w.jpeg\" data-width=\"1024\" data-height=\"683\" src=\"https://cdn-images-1.medium.com/max/2000/1*rCWlWxJ9-REAtidxv5XG3w.jpeg\" alt=\"image\" /></div></div></figure></div><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"f1bf\" class=\"graf graf--p graf-after--figure\">On February 23 in San Francisco, SF DATA hosted a tech talk on <a href=\"https://www.eventbrite.com/e/data-platforms-in-fintech-spark-kafka-and-amazon-redshift-tickets-31978038173\" data-href=\"https://www.eventbrite.com/e/data-platforms-in-fintech-spark-kafka-and-amazon-redshift-tickets-31978038173\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener nofollow\" target=\"_blank\">Data Platforms in FinTech</a>. In their talks, speakers revealed how they’re building out data platforms to support blockchain applications, cryptocurrencies, and detect fraudulent activity.</p><p id=\"5a1f\" class=\"graf graf--p graf-after--p\"><a href=\"https://www.linkedin.com/in/mriou/\" data-href=\"https://www.linkedin.com/in/mriou/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\"><strong class=\"markup--strong markup--p-strong\">Matthieu Riou</strong></a>, CTO and co-founder of <a href=\"https://www.blockcypher.com/\" data-href=\"https://www.blockcypher.com/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\"><strong class=\"markup--strong markup--p-strong\">BlockCypher</strong></a>, explained how BlockCypher used data to hunt down $70 million in stolen Bitcoins for the Department of Homeland Security.</p><p id=\"34fa\" class=\"graf graf--p graf-after--p\"><a href=\"https://www.linkedin.com/in/vaibhavjajoo/\" data-href=\"https://www.linkedin.com/in/vaibhavjajoo/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\"><strong class=\"markup--strong markup--p-strong\">Vaibhav Jajoo</strong></a>, head of Data Infrastructure at <a href=\"https://www.nerdwallet.com/\" data-href=\"https://www.nerdwallet.com/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\"><strong class=\"markup--strong markup--p-strong\">NerdWallet</strong></a>, described how the data team at NerdWallet is using a new brand of data analytics solutions with Kafka, Python, EMR, and Redshift.</p><h3 id=\"c822\" class=\"graf graf--h3 graf-after--p\">Analytics for Bitcoin and Other Cryptocurrencies at BlockCypher</h3><p id=\"8a80\" class=\"graf graf--p graf-after--h3\">Developers, companies, and government agencies use the BlockCypher API to build cryptocurrency applications and analyze patterns in blockchain transactions. Bitcoin alone has 8.2 million new transactions per month, with 250 million IP addresses to monitor. The Department of Homeland Security recently used the BlockCypher Analytics’ API to track down $70 million in stolen Bitcoins from the <a href=\"https://blog.blockcypher.com/finding-bitfinexs-bits-de7044185ed3\" data-href=\"https://blog.blockcypher.com/finding-bitfinexs-bits-de7044185ed3\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Bitfinex heist</a>.</p><figure id=\"7be7\" class=\"graf graf--figure graf--iframe graf--layoutOutsetLeft graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><div class=\"iframeContainer\"><iframe data-width=\"960\" data-height=\"540\" width=\"525\" height=\"295\" src=\"https://medium.com/media/4377ca6aac11b6c6349ca838d083a628?postId=6c393a0c29ae\" data-media-id=\"4377ca6aac11b6c6349ca838d083a628\" data-thumbnail=\"https://i.embed.ly/1/image?url=https%3A%2F%2Fembed-ssl.wistia.com%2Fdeliveries%2F865d222608c8979e32fcef060b57f00552298fd7.jpg%3Fimage_crop_resized%3D960x540&amp;key=a19fcc184b9711e1b4764040d3dc5c07\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\">[embedded content]</iframe></div></div><figcaption class=\"imageCaption\">BlockCypher — Analytics for Bitcoin and other Cryptocurrencies</figcaption></div></figure><p id=\"3f1f\" class=\"graf graf--p graf-after--figure\">In August 2016, BlockCypher noticed that 0.75% of Bitcoins suddenly started moving in unusual patterns.</p><p id=\"670a\" class=\"graf graf--p graf-after--p\">While the culprits are still unknown, BlockCypher was able to filter data to pinpoint where the transactions were coming from — in this instance bitcoin wallet provider <a href=\"https://www.bitgo.com/\" data-href=\"https://www.bitgo.com/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">BitGo</a>. The BlockCypher architecture uses a combination of <a href=\"http://cassandra.apache.org/\" data-href=\"http://cassandra.apache.org/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener nofollow\" target=\"_blank\">Cassandra</a>, Redshift and <a href=\"https://spark.apache.org/\" data-href=\"https://spark.apache.org/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener nofollow\" target=\"_blank\">Spark</a>.</p><p id=\"2f46\" class=\"graf graf--p graf-after--p\">According to Matthieu, the Holy Grail in cryptocurrency is to deanonymize every transaction, have the ability to tie it with off-chain transactions, classify transactions using machine learning, and provide APIs for law enforcement and industry.</p><h3 id=\"6685\" class=\"graf graf--h3 graf-after--p\">Building Data Solutions and Innovations at NerdWallet</h3><p id=\"6afd\" class=\"graf graf--p graf-after--h3\">NerdWallet gives consumers and small businesses clarity around all of life’s financial decisions by building accessible online tools and providing research and expert advice.</p><figure id=\"d9bd\" class=\"graf graf--figure graf--iframe graf--layoutOutsetLeft graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><div class=\"iframeContainer\"><iframe data-width=\"960\" data-height=\"540\" width=\"525\" height=\"295\" src=\"https://medium.com/media/02e86dc8e60b86dfca59d3f5fbc050cc?postId=6c393a0c29ae\" data-media-id=\"02e86dc8e60b86dfca59d3f5fbc050cc\" data-thumbnail=\"https://i.embed.ly/1/image?url=https%3A%2F%2Fembed-ssl.wistia.com%2Fdeliveries%2F651547aa35906912cc5f477d1f55114fc2a4f2bb.jpg%3Fimage_crop_resized%3D960x540&amp;key=a19fcc184b9711e1b4764040d3dc5c07\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\">[embedded content]</iframe></div></div></div></figure><p id=\"0c81\" class=\"graf graf--p graf-after--figure\"><a href=\"https://www.linkedin.com/in/vaibhavjajoo/\" data-href=\"https://www.linkedin.com/in/vaibhavjajoo/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener nofollow\" target=\"_blank\">Vaibhav </a>joined NerdWallet in 2014 and started the Data Analytics Team to help everybody in NerdWallet create meaning from the large volume and variety of data that NerdWallet customers generate every day (popular products, click-through rates, platform attributes etc.). NerdWallet has ~450 employees, and data consumers (i.e. “everybody”) stretch from analysts to the CEO. Product and marketing analysts care about granular views for a specific product or campaign. The CEO cares about high level views about the business. The data platform at NerdWallet needs to be flexible enough to serve data in a form each audience can understand, while allowing to slice and dice data across many dimensions.</p><figure id=\"9130\" class=\"graf graf--figure graf--iframe graf--layoutOutsetLeft graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><div class=\"iframeContainer\"><iframe data-width=\"600\" data-height=\"500\" width=\"525\" height=\"438\" src=\"https://medium.com/media/b1848d1a087d859733bc230071a298fc?postId=6c393a0c29ae\" data-media-id=\"b1848d1a087d859733bc230071a298fc\" data-thumbnail=\"https://i.embed.ly/1/image?url=https%3A%2F%2Fcdn.slidesharecdn.com%2Fss_thumbnails%2Fdataanalyticsnerdwallet1-170906233719-thumbnail-4.jpg%3Fcb%3D1504741267&amp;key=a19fcc184b9711e1b4764040d3dc5c07\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\">[embedded content]</iframe></div></div><figcaption class=\"imageCaption\">FinTech Data Challenges <a href=\"https://medium.com/@NerdWallet\" data-href=\"https://medium.com/@NerdWallet\" data-anchor-type=\"2\" data-user-id=\"e2e4cdb6512a\" data-action-value=\"e2e4cdb6512a\" data-action=\"show-user-card\" data-action-type=\"hover\" class=\"markup--user markup--figure-user\" target=\"_blank\">NerdWallet</a></figcaption></div></figure><p id=\"e418\" class=\"graf graf--p graf-after--figure\">Using a combination of <a href=\"https://kafka.apache.org/\" data-href=\"https://kafka.apache.org/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener nofollow\" target=\"_blank\">Kafka</a>, <a href=\"https://aws.amazon.com/redshift/\" data-href=\"https://aws.amazon.com/redshift/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener nofollow\" target=\"_blank\">Amazon Redshift</a> and <a href=\"https://aws.amazon.com/emr/\" data-href=\"https://aws.amazon.com/emr/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener nofollow\" target=\"_blank\">EMR</a>, NerdWallet has been able to create “ETL as Scale” and manage dynamic workloads. There are 250+ named SQL users, with different levels of SQL skills. That can pose a lot of challenges for managing the Redshift environment, especially for situations where some users write large ad-hoc queries. A key to balancing resources with workloads is Redshift’s WLM (<a href=\"http://docs.aws.amazon.com/redshift/latest/mgmt/workload-mgmt-config.html\" data-href=\"http://docs.aws.amazon.com/redshift/latest/mgmt/workload-mgmt-config.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener nofollow\" target=\"_blank\">Workload Management</a>).</p><p id=\"8968\" class=\"graf graf--p graf-after--p\">— —</p><p id=\"edd8\" class=\"graf graf--p graf-after--p\">Interested in building data platforms? Subscribe to <a href=\"https://www.getrevue.co/profile/sfdata\" data-href=\"https://www.getrevue.co/profile/sfdata\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener nofollow nofollow noopener\" target=\"_blank\">SF Data Weekly</a>, for more stories on data engineering you don’t want to miss.</p><p id=\"ff4f\" class=\"graf graf--p graf-after--p graf--trailing\">Attend our next event? Follow the <a href=\"https://www.facebook.com/sfdataevents/\" data-href=\"https://www.facebook.com/sfdataevents/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">SF Data Facebook page</a>.</p></div></div>",
        "created_at": "2018-04-24T14:35:48+0000",
        "updated_at": "2018-04-24T14:36:02+0000",
        "published_at": "2017-08-28T23:38:50+0000",
        "published_by": [
          "Lars Kamp"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 2,
        "domain_name": "medium.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*rCWlWxJ9-REAtidxv5XG3w.jpeg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9660"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 996,
            "label": "monitoring",
            "slug": "monitoring"
          }
        ],
        "is_public": false,
        "id": 9652,
        "uid": null,
        "title": "thelastpickle/cassandra-zipkin-tracing",
        "url": "https://github.com/thelastpickle/cassandra-zipkin-tracing",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>Cassandra-3.4 provides <a href=\"http://www.planetcassandra.org/blog/cassandra-3-4-release-overview/\" rel=\"nofollow\">pluggable tracing</a>. By adding 3 jar files to the Cassandra classpath and one jvm option, Cassandra's tracing is replaced with Zipkin. It can even identify incoming Zipkin traces and add Cassandra's own internal tracing on to it.</p>\n<pre>mvn install\ncp target/*.jar $CASSANDRA_HOME/lib/\n</pre>\n<p>Then start Cassandra with</p>\n<pre>JVM_OPTS \\\n  =\"-Dcassandra.custom_tracing_class=com.thelastpickle.cassandra.tracing.ZipkinTracing\" \\\n    cassandra\n</pre>\n<p>Or edit the <code>jvm.options</code>.</p>\n<p>The default SpanCollector sends the tracing messages via HTTP to <code>http://127.0.0.1:9411/</code>. This is the default port for the <a href=\"https://github.com/openzipkin/zipkin-java\">zipkin-java</a> server. The same url is used for the UI.</p>\n<h2><a id=\"user-content-continuing-existing-zipkin-traces\" class=\"anchor\" aria-hidden=\"true\" href=\"#continuing-existing-zipkin-traces\"></a>Continuing existing Zipkin traces</h2>\n<p>To continue existing Zipkin traces from application code through the DataStax CQL driver and into the Cassandra cluster.</p>\n<p>The Cassandra nodes need to be started also with the <code>cassandra.custom_query_handler_class</code> jvm option to a query handler that accepts incoming payloads over the CQL protocol:</p>\n<pre>JVM_OPTS \\\n  =\"-Dcassandra.custom_tracing_class=com.thelastpickle.cassandra.tracing.ZipkinTracing\" \\\n    -Dcassandra.custom_query_handler_class=org.apache.cassandra.cql3.CustomPayloadMirroringQueryHandler\"\n  cassandra\n</pre>\n<p>(Or edit the <code>jvm.options</code>)</p>\n<p>Then in the application code where the DataStax CQL driver is used put the Zipkin traceId and spanId into the <em>outgoing payload</em> like</p>\n<pre>SpanId spanId = clientTracer.startNewSpan(statement.toString());\nByteBuffer traceHeaders = ByteBuffer.wrap(spanId.bytes());\nstatement.setOutgoingPayload(singletonMap(\"zipkin\", traceHeaders.array()));\nclientTracer.setCurrentClientServiceName(serviceName);\nclientTracer.setClientSent();\nResultSet result = session.execute(statement);\nclientTracer.setClientReceived();\nreturn result;\n</pre>\n<h2><a id=\"user-content-more-information\" class=\"anchor\" aria-hidden=\"true\" href=\"#more-information\"></a>More information</h2>\n<p>See this <a href=\"http://thelastpickle.com/files/2015-09-24-using-zipkin-for-full-stack-tracing-including-cassandra/presentation/tlp-reveal.js/tlp-cassandra-zipkin.html\" rel=\"nofollow\">presentation</a>.</p>\n<h2><a id=\"user-content-background\" class=\"anchor\" aria-hidden=\"true\" href=\"#background\"></a>Background</h2>\n<p>See <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-10392\" rel=\"nofollow\">CASSANDRA-10392</a> for the patch to extend Cassandra's tracing that this project plugs into.</p>\n<h2><a id=\"user-content-troubleshooting\" class=\"anchor\" aria-hidden=\"true\" href=\"#troubleshooting\"></a>Troubleshooting</h2>\n<p>When this tracing is used instead of Cassandra's default tracing, any cqlsh statements run after enabling tracing with\n<code>TRACING ON;</code> are going to time out eventually giving</p>\n<pre>Unable to fetch query trace: Trace information was not available within …\n</pre>\n<p>This is because cqlsh is polling for tracing information in system_traces which isn't any longer being created. Zipkin tracing doesn't support this interaction with cqlsh (it's more of a thing to use with a tracing sampling rate). Improvements in this area are possible though, for example we could use zipkin tracing when the custom payload contains a zipkin traceId and spanId and fall back to normal tracing otherwise (which would work for cqlsh interaction). For the meantime an easy fix around this behaviour in cqlsh is to reduce Session.max_trace_wait down to 1 second.</p>\n</article>",
        "created_at": "2018-04-21T12:35:54+0000",
        "updated_at": "2018-04-21T12:36:05+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/30403496?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9652"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 12,
            "label": "video",
            "slug": "video"
          },
          {
            "id": 33,
            "label": "internet.architecture",
            "slug": "internet-architecture"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          }
        ],
        "is_public": false,
        "id": 9634,
        "uid": null,
        "title": "How Uber scaled its Real Time Infrastructure to Trillion events per day",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=K-fI2BeTLkk",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/K-fI2BeTLkk?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-04-20T02:07:26+0000",
        "updated_at": "2018-04-20T02:07:47+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/K-fI2BeTLkk/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9634"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 963,
            "label": "akka",
            "slug": "akka"
          },
          {
            "id": 964,
            "label": "scala",
            "slug": "scala"
          }
        ],
        "is_public": false,
        "id": 9627,
        "uid": null,
        "title": "jamesward/koober",
        "url": "https://github.com/jamesward/koober",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>An uber data pipeline sample app.  Play Framework, Akka Streams, Kafka, Flink, Spark Streaming, and Cassandra.</p>\n<p>Start Kafka:</p>\n<pre>./sbt kafkaServer/run\n</pre>\n<p>Web App:</p>\n<ol><li>Obtain an API key from <a href=\"https://www.mapbox.com/\" rel=\"nofollow\">mapbox.com</a></li>\n<li>Start the Play web app: <code>MAPBOX_ACCESS_TOKEN=YOUR-MAPBOX-API-KEY ./sbt webapp/run</code></li>\n</ol><p>Try it out:</p>\n<ol><li>Open the driver UI: <a href=\"http://localhost:9000/driver\" rel=\"nofollow\">http://localhost:9000/driver</a></li>\n<li>Open the rider UI: <a href=\"http://localhost:9000/rider\" rel=\"nofollow\">http://localhost:9000/rider</a></li>\n<li>In the Rider UI, click on the map to position the rider</li>\n<li>In the Driver UI, click on the rider to initiate a pickup</li>\n</ol><p>Start Flink:</p>\n<ol><li><code>./sbt flinkClient/run</code></li>\n<li>Initiate a few pickups and see the average pickup wait time change (in the stdout console for the Flink process)</li>\n</ol><p>Start Cassandra:</p>\n<pre>./sbt cassandraServer/run\n</pre>\n<p>Start the Spark Streaming process:</p>\n<ol><li><code>./sbt kafkaToCassandra/run</code></li>\n<li>Watch all of the ride data be micro-batched from Kafka to Cassandra</li>\n</ol><p>Setup PredictionIO Pipeline:</p>\n<ol><li>\n<p>Setup PIO</p>\n</li>\n<li>\n<p>Set the PIO Access Key:</p>\n<pre> export PIO_ACCESS_KEY=&lt;YOUR PIO ACCESS KEY&gt;\n</pre>\n</li>\n<li>\n<p>Start the PIO Pipeline:</p>\n<pre> ./sbt pioClient/run\n</pre>\n</li>\n</ol><p>Copy demo data into Kafka or PIO:</p>\n<p>For fake data, run:</p>\n<pre>./sbt \"demoData/run &lt;kafka|pio&gt; fake &lt;number of records&gt; &lt;number of months&gt; &lt;number of clusters&gt;\"\n</pre>\n<p>For New York data, run:</p>\n<pre>./sbt \"demoData/run &lt;kafka|pio&gt; ny &lt;number of months&gt; &lt;sample rate&gt;\"\n</pre>\n<p>Start the Demand Dashboard</p>\n<pre>PREDICTIONIO_URL=http://asdf.com MAPBOX_ACCESS_TOKEN=YOUR_MAPBOX_TOKEN ./sbt demandDashboard/run\n</pre>\n</article>",
        "created_at": "2018-04-20T01:24:53+0000",
        "updated_at": "2018-04-20T01:25:14+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/65043?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9627"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 23,
            "label": "elasticsearch",
            "slug": "elasticsearch"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 955,
            "label": "blockchain",
            "slug": "blockchain"
          },
          {
            "id": 1039,
            "label": "redis",
            "slug": "redis"
          }
        ],
        "is_public": false,
        "id": 9599,
        "uid": null,
        "title": "Landoop/stream-reactor",
        "url": "https://github.com/Landoop/stream-reactor",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\"><p><a href=\"https://datamountaineer.ci.landoop.com/job/stream-reactor/\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/66bb79f11d5cecbbc57c12f7b8bf4cfb61be30e7/68747470733a2f2f646174616d6f756e7461696e6565722e63692e6c616e646f6f702e636f6d2f6275696c645374617475732f69636f6e3f6a6f623d73747265616d2d72656163746f72267374796c653d666c6174262e706e67\" alt=\"Build Status\" data-canonical-src=\"https://datamountaineer.ci.landoop.com/buildStatus/icon?job=stream-reactor&amp;style=flat&amp;.png\" /></a>\n<a href=\"http://lenses.stream/connectors/index.html\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/0b233e4a210326868c5be7197bd9ccb416aacff0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d2d6f72616e67652e7376673f\" data-canonical-src=\"https://img.shields.io/badge/docs--orange.svg?\" alt=\"image\" /></a>\n<a href=\"http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22com.datamountaineer%22\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/c0fb8d333898ab15428ba7e8b2420b7427908d7e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c617465737425323072656c656173652d312e302e302d626c75652e7376673f6c6162656c3d6c617465737425323072656c65617365\" data-canonical-src=\"https://img.shields.io/badge/latest%20release-1.0.0-blue.svg?label=latest%20release\" alt=\"image\" /></a></p>\n<p>Join us on slack <a href=\"https://launchpass.com/landoop-community\" rel=\"nofollow\"><img src=\"https://github.com/Landoop/stream-reactor/raw/master/images/slack.jpeg\" alt=\"Alt text\" /></a></p>\n<p>Lenses offers SQL (for data browsing and Kafka Streams), Kafka Connect connector management, cluster monitoring and more.</p>\n<p>You can find more on <a href=\"http://www.landoop.com/kafka-lenses/\" rel=\"nofollow\">landoop.com!</a></p>\n<p><a target=\"_blank\" href=\"https://github.com/Landoop/stream-reactor/blob/master/images/streamreactor-logo.png\"><img src=\"https://github.com/Landoop/stream-reactor/raw/master/images/streamreactor-logo.png\" alt=\"Alt text\" /></a></p>\n<p>A collection of components to build a real time ingestion pipeline.</p>\n<h3><a id=\"user-content-connectors\" class=\"anchor\" aria-hidden=\"true\" href=\"#connectors\"></a>Connectors</h3>\n<p><strong>Please take a moment and read the documentation and make sure the software prerequisites are met!!</strong></p>\n<table><thead><tr><th>Connector</th>\n<th>Type</th>\n<th>Description</th>\n<th>Docs</th>\n</tr></thead><tbody><tr><td>AzureDocumentDb</td>\n<td>Sink</td>\n<td>Kafka connect Azure DocumentDb sink to subscribe to write to the cloud Azure Document Db.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/azuredocdb.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>BlockChain</td>\n<td>Source</td>\n<td>Kafka connect Blockchain source to subscribe to Blockchain streams and write to Kafka.</td>\n<td><a href=\"https://lenses.stream/connectors/source/blockchain.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Bloomberg</td>\n<td>Source</td>\n<td>Kafka connect source to subscribe to Bloomberg streams and write to Kafka.</td>\n<td><a href=\"https://lenses.stream/connectors/source/bloomberg.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Cassandra</td>\n<td>Source</td>\n<td>Kafka connect Cassandra source to read Cassandra and write to Kafka.</td>\n<td><a href=\"https://lenses.stream/connectors/source/cassandra.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>*Cassandra</td>\n<td>Sink</td>\n<td>Certified DSE Kafka connect Cassandra sink task to write Kafka topic payloads to Cassandra.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/cassandra.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Coap</td>\n<td>Source</td>\n<td>Kafka connect Coap source to read from IoT Coap endpoints using Californium.</td>\n<td><a href=\"https://lenses.stream/connectors/source/coap.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Coap</td>\n<td>Sink</td>\n<td>Kafka connect Coap sink to write kafka topic payload to IoT Coap endpoints using Californium.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/coap.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Druid</td>\n<td>Sink</td>\n<td>Kafka connect Druid sink to write Kafka topic payloads to Druid.</td>\n<td>\n</td></tr><tr><td>Elastic</td>\n<td>Sink</td>\n<td>Kafka connect Elastic Search sink to write Kafka topic payloads to Elastic Search 2.x</td>\n<td><a href=\"https://lenses.stream/connectors/sink/elastic.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Elastic 5</td>\n<td>Sink</td>\n<td>Kafka connect Elastic Search sink to write payloads to Elastic Search 5.x w. tcp or http</td>\n<td><a href=\"https://lenses.stream/connectors/sink/elastic5.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Elastic 6</td>\n<td>Sink</td>\n<td>Kafka connect Elastic Search sink to write payloads to Elastic Search 6.x w. tcp or http</td>\n<td><a href=\"https://lenses.stream/connectors/sink/elastic6.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>FTP/HTTP</td>\n<td>Source</td>\n<td>Kafka connect FTP and HTTP source to write file data into Kafka topics.</td>\n<td><a href=\"https://lenses.stream/connectors/source/ftp.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>HBase</td>\n<td>Sink</td>\n<td>Kafka connect HBase sink to write Kafka topic payloads to HBase.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/hbase.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Hazelcast</td>\n<td>Sink</td>\n<td>Kafka connect Hazelcast sink to write Kafka topic payloads to Hazelcast.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/hazelcast.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Kudu</td>\n<td>Sink</td>\n<td>Kafka connect Kudu sink to write Kafka topic payloads to Kudu.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/kudu.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>InfluxDb</td>\n<td>Sink</td>\n<td>Kafka connect InfluxDb sink to write Kafka topic payloads to InfluxDb.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/influx.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>JMS</td>\n<td>Source</td>\n<td>Kafka connect JMS source to write from JMS to Kafka topics.</td>\n<td><a href=\"https://lenses.stream/connectors/source/jms.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>JMS</td>\n<td>Sink</td>\n<td>Kafka connect JMS sink to write Kafka topic payloads to JMS.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/jms.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>MongoDB</td>\n<td>Sink</td>\n<td>Kafka connect MongoDB sink to write Kafka topic payloads to MongoDB.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/mongo.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>MQTT</td>\n<td>Source</td>\n<td>Kafka connect MQTT source to write data from MQTT to Kafka.</td>\n<td><a href=\"https://lenses.stream/connectors/source/mqtt.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>MQTT</td>\n<td>Sink</td>\n<td>Kafka connect MQTT sink to write data from Kafka to MQTT.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/mqtt.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Pulsar</td>\n<td>Source</td>\n<td>Kafka connect Pulsar source to write data from Pulsar to Kafka.</td>\n<td><a href=\"https://lenses.stream/connectors/source/pulsar.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Pulsar</td>\n<td>Sink</td>\n<td>Kafka connect Pulsar sink to write data from Kafka to Pulsar.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/pulsar.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Redis</td>\n<td>Sink</td>\n<td>Kafka connect Redis sink to write Kafka topic payloads to Redis.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/redis.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>ReThinkDB</td>\n<td>Source</td>\n<td>Kafka connect RethinkDb source subscribe to ReThinkDB changefeeds and write to Kafka.</td>\n<td><a href=\"https://lenses.stream/connectors/source/rethink.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>ReThinkDB</td>\n<td>Sink</td>\n<td>Kafka connect RethinkDb sink to write Kafka topic payloads to RethinkDb.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/rethink.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>VoltDB</td>\n<td>Sink</td>\n<td>Kafka connect Voltdb sink to write Kafka topic payloads to Voltdb.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/voltdb.html\" rel=\"nofollow\">Docs</a></td>\n</tr></tbody></table><h2><a id=\"user-content-release-notes\" class=\"anchor\" aria-hidden=\"true\" href=\"#release-notes\"></a>Release Notes</h2>\n<p><strong>1.1.0</strong> Pending</p>\n<ul><li>Added SSL, subscription, partitioning, batching and key selection to Pulsar source and sink</li>\n<li>Elastic6 connector @caiooliveiraeti !</li>\n<li>HTTP Basic Auth for Elasticsearch http client thanks @justinsoong !</li>\n<li>Add polling timeout on the JMS source connector to avoid high CPU in the source connector poll thanks #373 @matthedude</li>\n<li>Fixes on the elastic primary key separator thanks @caiooliveiraeti!</li>\n<li>Fix on the MQTT class loader</li>\n<li>Fix on the JMS class loader</li>\n<li>Fix on JMS to close down connections cleanly #363 thanks @matthedude!</li>\n<li>Fix on MQTT to correctly handle authentication</li>\n<li>Moved MongoDB batch size to KCQL. <code>connect.mongodb.batch.size</code> is deprecated</li>\n<li>Added <code>connect.mapping.collection.to.json</code> to treat maps, list, sets as json when inserting into Cassandra</li>\n<li>Added support for Elastic Pipelines thanks @caiooliveiraeti!</li>\n<li>Moved ReThinkDB batch size to KCQL <code>connect.rethink.batch.size</code> is deprecated</li>\n<li>MQTT source allows full control of matching the topic <code>INSERT INTO targetTopic SELECT * FROM mqttTopic ... WITHREGEX=`$THE_REGEX`</code></li>\n</ul><p><strong>1.0.0</strong></p>\n<ul><li>Kafka 1.0.o Support</li>\n</ul><p><strong>0.4.0</strong></p>\n<ul><li>Add FTPS support to FTP connector, new configuration option <code>ftp.protocol</code> introduced, either ftp (default) or ftps.</li>\n<li>Fix for MQTT source High CPU Thanks @masahirom!</li>\n<li>Improve logging on Kudu</li>\n<li>DELETE functionality add to the Cassandra sink, deletion now possible for null payloads, thanks @sandonjacobs !</li>\n<li>Fix in kafka-connect-common to handle primary keys with doc strings thanks, @medvekoma !</li>\n<li>Fix writing multiple topics to the same table in Cassandra #284</li>\n<li>Upgrade to Cassandra driver 3.3.0 and refactor Cassandra tests</li>\n<li>Fix on JMS source transacted queues #285 thanks @matthedude !</li>\n<li>Fix on Cassandra source, configurable timespan queries. You can now control the timespan the Connector will query for</li>\n<li>Allow setting initial query timestamp on Cassandra source</li>\n<li>Allow multiple primary keys on the redis sink</li>\n</ul><p><strong>0.3.0</strong></p>\n<ul><li>Upgrade CoAP to 2.0.0-M4</li>\n<li>Upgrade to Confluent 3.3 and Kafka 0.11.0.0.</li>\n<li>Added MQTT Sink.</li>\n<li>Add MQTT wildcard support.</li>\n<li>Upgrade CoAP to 2.0.0-M4.</li>\n<li>Added WITHCONVERTERS and WITHTYPE to JMS and MQTT connectors in KCQL to simplify configuration.</li>\n<li>Added FLUSH MODE to Kudu. Thanks! @patsak</li>\n</ul><p><strong>0.2.6</strong></p>\n<h3><a id=\"user-content-features\" class=\"anchor\" aria-hidden=\"true\" href=\"#features\"></a>Features</h3>\n<ul><li>Added MQTT Sink</li>\n<li>Upgrade to Confluent 3.2.2</li>\n<li>Upgrade to KCQL 2x</li>\n<li>Add CQL generator to Cassandra source</li>\n<li>Add KCQL INCREMENTALMODE support to the Cassandra source, bulk mode and the timestamp column type is now take from KCQL</li>\n<li>Support for setting key and truststore type on Cassandra connectors</li>\n<li>Added token based paging support for Cassandra source</li>\n<li>Added default bytes converter to JMS Source</li>\n<li>Added default connection factory to JMS Source</li>\n<li>Added support for SharedDurableConsumers to JMS Connectors</li>\n<li>Upgraded JMS Connector to JMS 2.0</li>\n<li>Moved to Elastic4s 2.4</li>\n<li>Added Elastic5s with TCP, TCP+XPACK and HTTP client support</li>\n<li>Upgrade Azure Documentdb to 1.11.0</li>\n<li>Added optional progress counter to all connectors, it can be enabled with <code>connect.progress.enabled</code> which will periodically report log messages processed</li>\n<li>Added authentication and TLS to ReThink Connectors</li>\n<li>Added TLS support for ReThinkDB, add batch size option to source for draining the internal queues.</li>\n<li>Upgrade Kudu Client to 1.4.0</li>\n<li>Support for dates in Elastic Indexes and custom document types</li>\n<li>Upgrade Connect CLI to 1.0.2 (Renamed to connect-cli)</li>\n</ul><h3><a id=\"user-content-bug-fixes\" class=\"anchor\" aria-hidden=\"true\" href=\"#bug-fixes\"></a>Bug Fixes</h3>\n<ul><li>Fixes for high CPU on CoAP source</li>\n<li>Fixes for high CPU on Cassandra source</li>\n<li>Fixed Avro double fields mapping to Kudu columns</li>\n<li>Fixes on JMS properties converter, Invalid schema when extracting properties</li>\n</ul><h3><a id=\"user-content-misc\" class=\"anchor\" aria-hidden=\"true\" href=\"#misc\"></a>Misc</h3>\n<ul><li>Refactored Cassandra Tests to use only one embedded instance</li>\n<li>Removed unused batch size and bucket size options from Kudu, they are taken from KCQL</li>\n<li>Removed unused batch size option from DocumentDb</li>\n<li>Rename Azure DocumentDb <code>connect.documentdb.db</code> to <code>connect.documentdb.db</code></li>\n<li>Rename Azure DocumentDb <code>connect.documentdb.database.create</code> to <code>connect.documentdb.db.create</code></li>\n<li>Rename Cassandra Source <code>connect.cassandra.source.kcql</code> to <code>connect.cassandra.kcql</code></li>\n<li>Rename Cassandra Source <code>connect.cassandra.source.timestamp.type</code> to <code>connect.cassandra.timestamp.type</code></li>\n<li>Rename Cassandra Source <code>connect.cassandra.source.import.poll.interval</code> to <code>connect.cassandra.import.poll.interval</code></li>\n<li>Rename Cassandra Source <code>connect.cassandra.source.error.policy</code> to <code>connect.cassandra.error.policy</code></li>\n<li>Rename Cassandra Source <code>connect.cassandra.source.max.retries</code> to <code>connect.cassandra.max.retries</code></li>\n<li>Rename Cassandra Sink <code>connect.cassandra.source.retry.interval</code> to <code>connect.cassandra.retry.interval</code></li>\n<li>Rename Cassandra Sink <code>connect.cassandra.sink.kcql</code> to <code>connect.cassandra.kcql</code></li>\n<li>Rename Cassandra Sink <code>connect.cassandra.sink.error.policy</code> to <code>connect.cassandra.error.policy</code></li>\n<li>Rename Cassandra Sink <code>connect.cassandra.sink.max.retries</code> to <code>connect.cassandra.max.retries</code></li>\n<li>Rename Cassandra Sink Sink <code>connect.cassandra.sink.retry.interval</code> to <code>connect.cassandra.retry.interval</code></li>\n<li>Rename Coap Source <code>connect.coap.bind.port</code> to <code>connect.coap.port</code></li>\n<li>Rename Coap Sink <code>connect.coap.bind.port</code> to <code>connect.coap.port</code></li>\n<li>Rename Coap Source <code>connect.coap.bind.host</code> to <code>connect.coap.host</code></li>\n<li>Rename Coap Sink <code>connect.coap.bind.host</code> to <code>connect.coap.host</code></li>\n<li>Rename MongoDb <code>connect.mongo.database</code> to <code>connect.mongo.db</code></li>\n<li>Rename MongoDb <code>connect.mongo.sink.batch.size</code> to <code>connect.mongo.batch.size</code></li>\n<li>Rename Druid <code>connect.druid.sink.kcql</code> to <code>connect.druid.kcql</code></li>\n<li>Rename Druid <code>connect.druid.sink.conf.file</code> to <code>connect.druid.kcql</code></li>\n<li>Rename Druid <code>connect.druid.sink.write.timeout</code> to <code>connect.druid.write.timeout</code></li>\n<li>Rename Elastic <code>connect.elastic.sink.kcql</code> to <code>connect.elastic.kcql</code></li>\n<li>Rename HBase <code>connect.hbase.sink.column.family</code> to <code>connect.hbase.column.family</code></li>\n<li>Rename HBase <code>connect.hbase.sink.kcql</code> to <code>connect.hbase.kcql</code></li>\n<li>Rename HBase <code>connect.hbase.sink.error.policy</code> to <code>connect.hbase.error.policy</code></li>\n<li>Rename HBase <code>connect.hbase.sink.max.retries</code> to <code>connect.hbase.max.retries</code></li>\n<li>Rename HBase <code>connect.hbase.sink.retry.interval</code> to <code>connect.hbase.retry.interval</code></li>\n<li>Rename Influx <code>connect.influx.sink.kcql</code> to <code>connect.influx.kcql</code></li>\n<li>Rename Influx <code>connect.influx.connection.user</code> to <code>connect.influx.username</code></li>\n<li>Rename Influx <code>connect.influx.connection.password</code> to <code>connect.influx.password</code></li>\n<li>Rename Influx <code>connect.influx.connection.database</code> to <code>connect.influx.db</code></li>\n<li>Rename Influx <code>connect.influx.connection.url</code> to <code>connect.influx.url</code></li>\n<li>Rename Kudu <code>connect.kudu.sink.kcql</code> to <code>connect.kudu.kcql</code></li>\n<li>Rename Kudu <code>connect.kudu.sink.error.policy</code> to <code>connect.kudu.error.policy</code></li>\n<li>Rename Kudu <code>connect.kudu.sink.retry.interval</code> to <code>connect.kudu.retry.interval</code></li>\n<li>Rename Kudu <code>connect.kudu.sink.max.retries</code> to <code>connect.kudu.max.reties</code></li>\n<li>Rename Kudu <code>connect.kudu.sink.schema.registry.url</code> to <code>connect.kudu.schema.registry.url</code></li>\n<li>Rename Redis <code>connect.redis.connection.password</code> to <code>connect.redis.password</code></li>\n<li>Rename Redis <code>connect.redis.sink.kcql</code> to <code>connect.redis.kcql</code></li>\n<li>Rename Redis <code>connect.redis.connection.host</code> to <code>connect.redis.host</code></li>\n<li>Rename Redis <code>connect.redis.connection.port</code> to <code>connect.redis.port</code></li>\n<li>Rename ReThink <code>connect.rethink.source.host</code> to <code>connect.rethink.host</code></li>\n<li>Rename ReThink <code>connect.rethink.source.port</code> to <code>connect.rethink.port</code></li>\n<li>Rename ReThink <code>connect.rethink.source.db</code> to <code>connect.rethink.db</code></li>\n<li>Rename ReThink <code>connect.rethink.source.kcql</code> to <code>connect.rethink.kcql</code></li>\n<li>Rename ReThink Sink <code>connect.rethink.sink.host</code> to <code>connect.rethink.host</code></li>\n<li>Rename ReThink Sink <code>connect.rethink.sink.port</code> to <code>connect.rethink.port</code></li>\n<li>Rename ReThink Sink <code>connect.rethink.sink.db</code> to <code>connect.rethink.db</code></li>\n<li>Rename ReThink Sink <code>connect.rethink.sink.kcql</code> to <code>connect.rethink.kcql</code></li>\n<li>Rename JMS <code>connect.jms.user</code> to <code>connect.jms.username</code></li>\n<li>Rename JMS <code>connect.jms.source.converters</code> to <code>connect.jms.converters</code></li>\n<li>Remove JMS <code>connect.jms.converters</code> and replace my kcql <code>withConverters</code></li>\n<li>Remove JMS <code>connect.jms.queues</code> and replace my kcql <code>withType QUEUE</code></li>\n<li>Remove JMS <code>connect.jms.topics</code> and replace my kcql <code>withType TOPIC</code></li>\n<li>Rename Mqtt <code>connect.mqtt.source.kcql</code> to <code>connect.mqtt.kcql</code></li>\n<li>Rename Mqtt <code>connect.mqtt.user</code> to <code>connect.mqtt.username</code></li>\n<li>Rename Mqtt <code>connect.mqtt.hosts</code> to <code>connect.mqtt.connection.hosts</code></li>\n<li>Remove Mqtt <code>connect.mqtt.converters</code> and replace my kcql <code>withConverters</code></li>\n<li>Remove Mqtt <code>connect.mqtt.queues</code> and replace my kcql <code>withType=QUEUE</code></li>\n<li>Remove Mqtt <code>connect.mqtt.topics</code> and replace my kcql <code>withType=TOPIC</code></li>\n<li>Rename Hazelcast <code>connect.hazelcast.sink.kcql</code> to <code>connect.hazelcast.kcql</code></li>\n<li>Rename Hazelcast <code>connect.hazelcast.sink.group.name</code> to <code>connect.hazelcast.group.name</code></li>\n<li>Rename Hazelcast <code>connect.hazelcast.sink.group.password</code> to <code>connect.hazelcast.group.password</code></li>\n<li>Rename Hazelcast <code>connect.hazelcast.sink.cluster.members</code> tp <code>connect.hazelcast.cluster.members</code></li>\n<li>Rename Hazelcast <code>connect.hazelcast.sink.batch.size</code> to <code>connect.hazelcast.batch.size</code></li>\n<li>Rename Hazelcast <code>connect.hazelcast.sink.error.policy</code> to <code>connect.hazelcast.error.policy</code></li>\n<li>Rename Hazelcast <code>connect.hazelcast.sink.max.retries</code> to <code>connect.hazelcast.max.retries</code></li>\n<li>Rename Hazelcast <code>connect.hazelcast.sink.retry.interval</code> to <code>connect.hazelcast.retry.interval</code></li>\n<li>Rename VoltDB <code>connect.volt.sink.kcql</code> to <code>connect.volt.kcql</code></li>\n<li>Rename VoltDB <code>connect.volt.sink.connection.servers</code> to <code>connect.volt.servers</code></li>\n<li>Rename VoltDB <code>connect.volt.sink.connection.user</code> to <code>connect.volt.username</code></li>\n<li>Rename VoltDB <code>connect.volt.sink.connection.password</code> to <code>connect.volt.password</code></li>\n<li>Rename VoltDB <code>connect.volt.sink.error.policy</code> to <code>connect.volt.error.policy</code></li>\n<li>Rename VoltDB <code>connect.volt.sink.max.retries</code> to <code>connect.volt.max.retries</code></li>\n<li>Rename VoltDB <code>connect.volt.sink.retry.interval</code> to <code>connect.volt.retry.interval</code></li>\n</ul><p><strong>0.2.5 (8 Apr 2017)</strong></p>\n<ul><li>Added Azure DocumentDB Sink Connector</li>\n<li>Added JMS Source Connector.</li>\n<li>Added UPSERT to Elastic Search</li>\n<li>Support Confluent 3.2 and Kafka 0.10.2.</li>\n<li>Cassandra improvements <code>withunwrap</code></li>\n<li>Upgrade to Kudu 1.0 and CLI 1.0</li>\n<li>Add ingest_time to CoAP Source</li>\n<li>InfluxDB bug fixes for tags and field selection.</li>\n<li>Added Schemaless Json and Json with schema support to JMS Sink.</li>\n<li>Support for Cassandra data type of <code>timestamp</code> in the Cassandra Source for timestamp tracking.</li>\n</ul><p><strong>0.2.4</strong> (26 Jan 2017)</p>\n<ul><li>Added FTP and HTTP Source.</li>\n<li>Added InfluxDB tag support. KCQL: INSERT INTO targetdimension <code>SELECT * FROM influx-topic WITHTIMESTAMP sys_time() WITHTAG(field1, CONSTANT_KEY1=CONSTANT_VALUE1, field2,CONSTANT_KEY2=CONSTANT_VALUE1)</code></li>\n<li>Added InfluxDb consistency level. Default is <code>ALL</code>. Use <code>connect.influx.consistency.level</code> to set it to ONE/QUORUM/ALL/ANY</li>\n<li>InfluxDb <code>connect.influx.sink.route.query</code> was renamed to <code>connect.influx.sink.kcql</code></li>\n<li>Added support for multiple contact points in Cassandra</li>\n</ul><p><strong>0.2.3</strong> (5 Jan 2017)</p>\n<ul><li>Added CoAP Source and Sink.</li>\n<li>Added MongoDB Sink.</li>\n<li>Added MQTT Source.</li>\n<li>Hazelcast support for ring buffers.</li>\n<li>Redis support for Sorted Sets.</li>\n<li>Added start scripts.</li>\n<li>Added Kafka Connect and Schema Registry CLI.</li>\n<li>Kafka Connect CLI now supports pause/restart/resume; checking connectors on the classpath and validating configuration of connectors.</li>\n<li>Support for <code>Struct</code>, <code>Schema.STRING</code> and <code>Json</code> with schema in the Cassandra, ReThinkDB, InfluxDB and MongoDB sinks.</li>\n<li>Rename <code>export.query.route</code> to <code>sink.kcql</code>.</li>\n<li>Rename <code>import.query.route</code> to <code>source.kcql</code>.</li>\n<li>Upgrade to KCQL 0.9.5 - Add support for <code>STOREAS</code> so specify target sink types, e.g. Redis Sorted Sets, Hazelcast map, queues, ringbuffers.</li>\n</ul><h3><a id=\"user-content-building\" class=\"anchor\" aria-hidden=\"true\" href=\"#building\"></a>Building</h3>\n<p><em><strong>Requires gradle 3.0 to build.</strong></em></p>\n<p>To build</p>\n<div class=\"highlight highlight-source-shell\"><pre>gradle compile</pre></div>\n<p>To test</p>\n<div class=\"highlight highlight-source-shell\"><pre>gradle test</pre></div>\n<p>To create a fat jar</p>\n<div class=\"highlight highlight-source-shell\"><pre>gradle shadowJar</pre></div>\n<p>You can also use the gradle wrapper</p>\n<pre>./gradlew shadowJar\n</pre>\n<p>To view dependency trees</p>\n<pre>gradle dependencies # or\ngradle :kafka-connect-cassandra:dependencies\n</pre>\n<p>To build a particular project</p>\n<pre>gradle :kafka-connect-elastic5:build\n</pre>\n<p>To create a jar of a particular project:</p>\n<pre>gradle :kafka-connect-elastic5:shadowJar\n</pre>\n<h2><a id=\"user-content-contributing\" class=\"anchor\" aria-hidden=\"true\" href=\"#contributing\"></a>Contributing</h2>\n<p>We'd love to accept your contributions! Please use GitHub pull requests: fork the repo, develop and test your code,\n<a href=\"http://karma-runner.github.io/1.0/dev/git-commit-msg.html\" rel=\"nofollow\">semantically commit</a> and submit a pull request. Thanks!</p>\n</article>",
        "created_at": "2018-04-18T20:32:51+0000",
        "updated_at": "2018-04-18T20:33:12+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 11,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/11728472?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9599"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9586,
        "uid": null,
        "title": "Making the Change from Thrift to CQL (Cassandra Query Language)",
        "url": "https://academy.datastax.com/planet-cassandra/making-the-change-from-thrift-to-cql",
        "content": "<h2>CQL Under the Hood</h2><p>At this point, most users should be aware that CQL has replaced Thrift as the standard (and therefore recommended) interface for working with Cassandra. Yet it remains largely misunderstood, as its resemblance to common SQL has left both Thrift veterans and Cassandra newcomers confused about how it translates to the underlying storage layer. This fog must be lifted if you hope to create data models that scale, perform, and insure availability.</p><p>First, let’s define some terms:</p><ul><li>Thrift: a legacy RPC protocol combined with a code generation tool. Deprecated in favor of the native protocol.</li>\n<li>Native protocol: replacement for Thrift that only supports CQL.</li>\n<li>Storage rows: keys and columns as stored on disk.</li>\n<li>CQL rows: an abstraction layer on top of the storage rows.</li>\n</ul><p>It is important to understand that the CQL data representation does not always match the underlying storage structure. This can be challenging for those accustomed to Thrift-based operations, as those were performed directly against the storage layer. But CQL introduces an abstraction on top of the storage rows, and only maps directly in the simplest of schemas.</p><p>If you want to be successful at modeling and querying data in Cassandra, keep in mind that while CQL improves the learning curve, it is not SQL. You must understand what’s happening under the covers, or you will end up with data models that are poorly suited to Cassandra.</p><p>So let’s pull back the curtain and look at what our CQL statements translate to at the storage layer, starting with a simple table.</p><h2> </h2><h3>Single Primary Key</h3><p>The first model we will examine is a straightforward table, which we’ll call books with a single primary key, title:</p><p>We can then insert some data, as follows:</p><p>And finally we can read our newly inserted rows:</p><p>What we’ve done so far looks a lot like ANSI SQL, and in fact these statements would have been valid when run against most modern relational systems. But we know that something very different is happening under the hood.</p><p>To see what this looks like at the storage layer, we can use the old command line interface, <em>cassandra-cli</em>, which allows us to interact directly with storage rows. This CLI is deprecated and will likely disappear in the 3.0 release, but for now we can use it to inspect the books table we created using CQL. Listing the contents of our table produces the following output:</p><p>RowKey: Without Remorse</p><p>As you can see, this is nearly a direct mapping to the CQL rows, except that we have an empty column at the beginning of each row (which is not a mistake; it is used internally by Cassandra).</p><p>Let’s point out a couple of important features of this data. First, remember that the row key is distributed randomly using a hash algorithm, so the results are returned in no particular order. By contrast, columns are stored in sorted order by name, using the natural ordering of the type. In this case, “author” comes before “year” lexicographically, so it appears first in the list. These are critical points, as they are central to effective data modeling.</p><h2> </h2><h3>Compound Keys</h3><p>Now let’s look at a slightly more complex example, one which uses a compound key. In this case, we’ll create a new table, authors, with a compound key using name, year, and title:</p><p>And this is what our data looks like after inserting two CQL rows:</p><p>This is where CQL can begin to cause confusion for those who are unfamiliar with what’s happening at the storage layer. To make sense of this, it’s important to understand the difference between partition keys and clustering columns.</p><h3> </h3><h4>Partition Keys</h4><p>When declaring a primary key, the first field in the list is always the partition key. This translates directly to the storage row key, which is randomly distributed in the cluster via the hash algorithm. In general, you must provide the partition key when issuing queries, so that Cassandra will know which nodes contain the requested data.</p><h3> </h3><h4>Clustering Columns</h4><p>The remaining fields in the primary key declaration are called clustering columns, and these determine the ordering of the data on disk. They are not, however, part of the partition key, so they do not help determine the distribution of data in the cluster. But they play a key role in determining the kinds of queries you can run against your data, as we will see in the remainder of this section.</p><p>Now that you know the difference, it’s time to see what our authors table looks like in its storage layer representation (with the timestamp and marker columns omitted for clarity):</p><p>You will note that our two CQL rows translated to a single storage row, because both of our inserts used the same partition key. But perhaps more interesting is the location of our year and title column values. They are stored as parts of the column name, rather than column values!</p><p>Those who are experienced with Thrift-based data models will recognize this structure, which is referred to as composite columns. You can also observe that the rows are sorted first by year, then by title, which is the way we specified them in our primary key declaration. It is also possible to reverse the stored sort order by adding the WITH CLUSTERING ORDER BY clause, as follows:</p><p>Then, when selecting our rows, we can see that the ordering starts with the latest year and ends with the earliest:</p><p>While this may seem to be a trivial point, it can matter a great deal depending on the types of queries you intend to run on your data. We will examine these implications later in this post when we discuss queries.</p><h3> </h3><h4>Composite Partition Keys</h4><p>In the previous examples we demonstrated the use of a single partition key with multiple clustering columns. But it’s also possible to create a multi-part (or “composite”) partition key. The most common reason for doing this is to improve data distribution characteristics. A prime example of this is the use of time buckets as keys when modeling time-series data. We will cover this in detail later on.</p><p>For now, let’s see what it looks like to create a composite partition key:</p><p>The difference, in case it’s not obvious, is the addition of parentheses around the name and year columns, which specifies that these two columns should form the composite partition key. This leaves title as the only remaining clustering column.</p><p>At the storage layer, this has the effect of moving the year from a component of the column name to a component of the row key, as follows:</p><h3>Why This Matters</h3><p>You may be wondering why it matters how the data is stored internally. In fact it matters a great deal, for several important reasons:</p><ul><li>Your queries must respect the underlying storage. Cassandra doesn’t allow ad hoc queries of the sort that you can perform using SQL on a relational system. If you don’t understand how the data is stored, at best you will be constantly frustrated by the error messages you receive when you try to query your data, and at worst you will suffer poor performance.</li>\n<li>You must choose your partition key carefully, because it must be known at query time, and must also distribute well across the cluster.</li>\n<li>Because of its log-structured storage, Cassandra handles range queries very well. A range query simply means that you select a range of columns for a given key, in the order they are stored.</li>\n<li>You have to carefully order your clustering columns, because the order affects the sort order of your data on disk and therefore determines the kinds of queries you can perform.</li>\n</ul><p>Proper data modeling in Cassandra requires you to structure your data in terms of your queries. This is backward compared to the approach taken in most relational models, where normalization is typically the objective. With Cassandra you must consider your queries first.</p><p>With these principles in mind, let’s examine what happens when you run different kinds of queries, so you can better understand how to structure your data.</p><h2>Understanding Queries</h2><p>In order to make sense of the various types of queries, we will start with a common data model to be used across the following examples. For this data model, we will return to the authors table, with name as the partition key, followed by year and title as clustering columns. We’ll also sort the year in descending order. This table can be created as follows:</p><p>Also, for purposes of these examples, we will assume a replication factor of three and consistency level of QUORUM.</p><h2> </h2><h3>Query by Key</h3><p>We’ll start with a basic query by key:</p><p>For this simple select, the query makes the request to the coordinator node, which owns a replica for our key. The coordinator then retrieves the row from another replica node to satisfy the quorum. Thus, we need a total of two nodes to satisfy the query:</p><p>At the storage layer, this query first locates the partition key, then scans all the columns in order, as follows:</p><p>So even though this appears to be a simple query by key, at the storage layer it actually translates to a range query!</p><h2> </h2><h3>Range Queries</h3><p>If this basic query results in a range query, let’s see what happens when we specifically request a range, like this:</p><p>In this case we’re still selecting a single partition, so the query must only check with two nodes as in the previous example. The difference is that in this case, Cassandra simply scans the columns until it finds one that fails the query predicate:</p><p>Once it finds the year 1991, Cassandra knows there are no more records to scan. Therefore, this query is efficient because it must only read the required number of columns, plus one.</p><p>To recap, there are three key points you should take from this discussion:</p><ol><li>Sequential queries are fast, because they take advantage of Cassandra’s natural sort order at the storage layer.</li>\n<li>Queries by key and combination of key plus clustering column are sequential at the storage layer, which of course means they are optimal.</li>\n<li>Write your data the way you intend to read it. Or, put another way, model your data in terms of your queries, not the other way around. Following this rule will help you avoid the most common data modeling pitfalls that plague those who are transitioning from a relational database.</li>\n</ol><p>Now that we’ve covered the basics of how to build data models that make optimal use of the storage layer, let’s look at one of Cassandra’s newer features: collections.</p><h2>Collections</h2><p>The introduction of collections to CQL addresses some of the concerns that frequently arose regarding Cassandra’s primitive data model. They add richer capabilities that give developers more flexibility when modeling certain types of data.</p><p>Cassandra supports three collection types: sets, lists, and maps. In this section we will examine each of these and take a look at how they’re stored under the hood. But first, it’s important to understand some basic rules regarding collections:</p><ul><li>Each item in a collection must not be more than 64 KB</li>\n<li>A maximum of 64,000 items may be stored in a single collection</li>\n<li>Querying a collection always returns the entire collection</li>\n<li>Collections are best used for relatively small, bounded data sets</li>\n</ul><p>With those rules in mind, we can examine each type of collection in detail, starting with sets.</p><h3>Sets</h3><p>A set in CQL is very similar to a set in your favorite programming language. It is a unique collection of items, meaning it does not allow for duplicates. In most languages sets have no specific ordering; Cassandra, however, stores them in their natural sort order, as you might expect.</p><p>Here is an example of a table of <em>authors</em> which contains a set of <em>books</em>:</p><p>We can then insert some values as follows:</p><p>Cassandra also supports removing items from a set using the UPDATE statement:</p><p>At the storage layer, set values are stored as column names, with the values left blank. This guarantees uniqueness, as any attempt to rewrite the same item would simply result in overwriting the old column name. The storage representation of the books set would look like this:</p><p><img alt=\"Screen Shot 2014-12-03 at 4.33.07 PM\" height=\"40\" src=\"http://planetcassandra.org/wp-content/uploads/2014/12/Screen-Shot-2014-12-03-at-4.33.07-PM.png\" width=\"16\" />                  <img alt=\"Screen Shot 2014-12-03 at 4.33.07 PM\" height=\"40\" src=\"http://planetcassandra.org/wp-content/uploads/2014/12/Screen-Shot-2014-12-03-at-4.33.07-PM.png\" width=\"16\" /><br /><strong>                     Set Name</strong>       <strong>Item</strong></p><p>You can see that the name of the set is stored as the first component of the composite column name, with the item as the second component. Unfortunately Cassandra does not support a contains operation, so you must retrieve the entire set and perform this on the client. But sets can be quite useful as a container for unique items in a variety of data models.</p><h3>Lists</h3><p>At the CQL level, lists look very similar to sets. In the following table, we substitute the set of books from the previous example for a list:</p><p>Insertion is also similar to the set syntax, except that the curly braces are traded for brackets:</p><p>And since lists are ordered, CQL supports prepend and append operations, which involve simply placing the item as either the first (prepend) or second (append) operands, as follows:</p><p>To delete an item, you can refer to it by name:</p><p>Unlike the set, the list structure at the storage layer places the list item in the column value, and the column name instead contains a UUID for ordering purposes. Here’s what it looks like:</p><h3>Maps</h3><p>Lastly, maps are a highly useful structure, as they can offer similar flexibility to the old dynamic column names many grew accustomed to in the Thrift days, as long as the total number of columns is kept to a reasonable number.</p><p>For example, we can use a map to store not only the book title, but the year as well. Here is what that would look like:</p><p>To insert or update an entire map, use the following syntax:</p><p>You can also insert or update a single key using array-like syntax, as follows:</p><p>Specific values can be also be removed by using a DELETE statement:</p><p>At the storage layer, maps look very similar to lists, except the ordering ID is replaced by the map key:</p><p>As you can see, all these collection types make use of composite columns, in the same manner as clustering columns.</p><h2>Multi-Key Queries</h2><p>And now for one of the most common query errors, the IN clause, where we ask for multiple partition keys in a single query. Let’s recall the authors schema we introduced earlier:</p><p>Using this schema, let’s say we want to retrieve a number of books from a list of known authors. Obviously we could write a separate query for each author, but Cassandra also provides a familiar SQL-style syntax for specifying multiple partition keys using the <em>IN</em> clause:</p><p>The question is how will Cassandra fulfill this request? The system will hash the partition key—name in this case—and assign replicas to nodes based on the hash. Using the three authors in our query as examples, we will end up with a distribution resembling the following:</p><p>The important characteristic to note in this distribution is that the keys are dispersed throughout the cluster. If we also remember that a QUORUM read requires consulting with at least two out of three replicas, it is easy to see how this query will result in consulting many nodes. In the following diagram, our client makes a request to one of the nodes, which will act as coordinator. The coordinator must then make requests to at least two replicas for each key in the query:</p><p>The end result is that we required five out of six nodes to fulfill this query! If any one of these calls fails, the entire query will fail. It is easy to see how a query with many keys could require participation from every node in the cluster.</p><p>When using the IN clause, it’s best to keep the number of keys small. There are valid use cases for this clause, such as querying across time buckets for time-series models, but in such cases you should try to size your buckets such that you only need at most two in order to fulfill the request.</p><p>In fact, it is often advisable to issue multiple queries in parallel as opposed to utilizing the IN clause. While the IN clause may save you from multiple network requests to Cassandra, the coordinator must do more work. You can often reduce overall latency and workload with several token-aware queries, as you’ll be talking directly to the nodes that contain the data.</p><h2>Secondary Indices</h2><p>If range queries can be considered optimal for Cassandra’s storage engine, queries based on a <strong>secondary index</strong> fall at the other end of the spectrum. Secondary indices have been part of Cassandra since the 0.7 release, and they are certainly an alluring feature. In fact, for those who are accustomed to modeling data in relational databases, creating an index is often a go-to strategy to achieve better query performance. However, as with most aspects of the transition to Cassandra, this strategy translates poorly.</p><p>To start, let’s get familiar with what secondary indices are and how they work. First off, secondary indices are the only type of index that Cassandra will manage for you, so the terms “index” and “secondary index” actually refer to the same mechanism. The purpose of an index is to allow <strong>query-by-value</strong> functionality, which is not supported naturally. This should be a clue as to the potential danger involved in relying on the index functionality.</p><p>As an example, suppose we want to be able to query authors for a given publisher. Using our earlier <em>authors </em>table, remember that the <em>publisher</em> column has no special properties. It is a simple text column, meaning that by default we cannot filter based on its value. We can take a look at what happens when attempting to do so, as in the following query:</p><p>Running this query results in the following error message, indicating that we’re trying to query by the value of a non-indexed column:</p><p>The obvious remedy is to simply create an index on publisher, as follows:</p><p>Now we can filter on publisher, so our problems are solved, right? Not exactly! Let’s look closely at what Cassandra does to make this work.</p><h3>Secondary Indices Under the Hood</h3><p>At the storage layer, a secondary index is simply another column family, where the key is the value of the indexed column, and the columns contain the row keys of the indexed table. This can be a bit confusing to describe, so let’s visualize it.</p><p>Imagine our authors table contains the following CQL rows:</p><p>An index on publisher would then look like this at the storage layer:</p><p>So a query filtering on publisher will use the index to each author name, then query all the authors by key. This is similar to using the IN clause, since we must query replicas for every key with an entry in the index.</p><p>But it’s actually even worse than the IN clause, because of a very important difference between indices and standard tables. Cassandra co-locates index entries with their associated original table keys. In other words, you will end up with a key for “Random House” in author_publishers on every node that has keys for “Anne Rice” or “Charles Dickens” in authors.</p><p>To make this a bit clearer, the following diagram shows how our co-located <em>authors</em> table and <em>author_publisher </em>index might be distributed across a four-node cluster:</p><p><img alt=\"\" src=\"https://academy.datastax.com/sites/default/files/example-cluster.png\" /></p><p>The objective in using this approach is to be able to determine which nodes own indexed keys, as well as to obtain the keys themselves in a single request. But the problem is we have no idea which token ranges contain indexed keys until we ask each range. So now we end up with a query pattern like this:</p><p><img alt=\"\" src=\"https://academy.datastax.com/sites/default/files/pattern.png\" /></p><p>Obviously the use of secondary indices has an enormous impact on both performance and availability, since so many nodes must participate in fulfilling the query. For this reason it’s best to avoid using them in favor of writing your own indices or choosing another data model entirely.</p><p>If you decide to use a secondary index for a use case where performance and availability are not critical, make sure to only index on low cardinality values, as high cardinality indices do not scale well. But don’t go so low that your index is rendered useless. For example, booleans are bad, as are UUIDs, but birth year could be a reasonable column to index.</p><h2>Deleting Immutable Data</h2><p>We have established that Cassandra employs a log-structured storage engine, where all writes are immutable appends to the log. The implication is that data cannot actually be deleted at the time a DELETE statement is issued. Cassandra solves this by writing a marker, called a <strong>tombstone</strong>, with a timestamp greater than the previous value. This has the effect of overwriting the previous value with an empty one, which will then be compiled in subsequent queries for that column in the same manner as any other update. The actual value of the tombstone is set to the time of deletion, which is used to determine when the tombstone can be removed.</p><h3>Unexpected deletes</h3><p>Of course you can explicitly delete a column using the DELETE statement, but you may be surprised that a tombstone will be generated for every affected storage layer column. To make this clear, let’s remind ourselves about the structure of a single CQL row as represented at the storage layer:</p><p>To this point we have been using a simplified version of the storage row representation. In fact there is a third column used as an internal marker, which has been omitted for clarity. So then, let’s remove the “Patriot Games” entry, as follows:</p><p>Using cqlsh with tracing turned on, we then attempt to read the newly deleted record:</p><p>If you carefully examine the resulting trace, you will notice a line resembling the following:</p><p>So what happened? Our query that returned zero records actually had to read three tombstones to produce the results! The important point to remember is that tombstones cover single storage layer columns, so deleting a CQL row with many columns results in many tombstones.</p><h2> </h2><h3>The Problem with Tombstones</h3><p>You may be wondering why we’re so concerned about tombstones. The last example should provide a hint as to the reason. When a query requires reading tombstones, Cassandra must perform many additional reads to return your results.</p><p>In addition, a query for a key in an sstable that has only tombstones associated with it will still pass through the bloom filter, because the system must reconcile tombstones with other replicas. Since the bloom filter is designed to prevent unnecessary reads for missing data, this means Cassandra will perform extra reads after data has been deleted.</p><p>Now that you understand the basics of deletes and the problems associated with them, it’s important to point out the other ways that deletes can be generated—sometimes in ways you would not expect.</p><h3>Expiring Columns</h3><p>Cassandra offers us a handy feature for purging old data through setting an expiration time, called a <strong>TTL</strong>, at the column level. There are many valid reasons to set TTL values, and they can help to avoid unbounded data accumulation over time. Setting a TTL on a column is straightforward, and can be accomplished using either an <em>INSERT</em> or <em>UPDATE</em>statement as follows (note that TTL values are in seconds):</p><p>This can be useful when dealing with ephemeral data, but you must take care when employing this strategy, because an expired column results in a tombstone as in any other form of delete.</p><h4>How NOT to use TTLs</h4><p>A common reason to expire columns is in the case of time-series data. Imagine we want to display a feed of comments associated with a news article, where the newest post appears on top. To avoid holding onto them indefinitely, we set them to expire after a few hours.</p><p>So we end up with a model that resembles the following:</p><p>It’s important to note that this model is perfectly acceptable so far. Where we can run into problems is when we naively attempt to query for the latest values. It can be tempting to assume that we can simply query everything for a given articleID, with the expectation that old columns will simply disappear. In other words, we perform a query like this:</p><p>In some ways this expectation is correct. Old values will disappear from the result set, and for a period of time this query will perform perfectly well. But gradually we will accumulate tombstones as columns reach their expiration time, and this query requires that we read all columns in the storage row. Eventually, we will reach a point where Cassandra will be reading more tombstones than real values!</p><p>The solution is simple. We must add a range filter on timestamp, which will tell Cassandra to stop scanning columns at approximately as far back in time as the tombstones will start. In this case, we don’t want to read any columns older than three hours, so our new query looks like this:</p><p>Note that you will have to calculate the timestamp in your application, as CQL does not currently support arithmetic operations.</p><p>To sum up, expiring columns can be highly useful as long as you do so wisely. Make sure your usage pattern avoids reading excessive numbers of tombstones. Often you can use range filters to accomplish this goal.</p><h3>When Null Does Not Mean Empty</h3><p>There is an even subtler (and more insidious) way to inadvertently create tombstones: by inserting <em>null</em> values. Let’s take a look at how we might cause this situation unwittingly.</p><p>We know that Cassandra stores columns sparsely, meaning that unspecified values simply aren’t written. So it would seem logical that setting a column to <em>null</em> would result in a missing column. In fact, writing a <em>null</em> is the same thing as explicitly deleting a column, and therefore a tombstone is written for that column!</p><p>There is a simple reason why this is the case. While Cassandra supports separate INSERT and UPDATE statements, all writes are fundamentally the same under the covers. And because all writes are simply append operations, there is no way for the system to know whether a previous value exists for the column. Therefore Cassandra must actually write a tombstone in order to guarantee any old values are deleted.</p><p>While it may seem as though this would be easy to avoid—by just not writing <em>null</em> values—it is fairly easy to mistakenly allow this to happen when using prepared statements. Imagine a data model that includes many sparsely populated columns. It is tempting to create a single prepared statement with all potential columns, then set the unused columns to<em>null</em>. It is also possible that callers of an insert method might pass in <em>null</em> values. If this condition is not checked, it is easy to see how tombstones could be accumulated without realizing this is happening.</p><p>So to wrap things up, remember two things:</p><ol><li>Thrift is dead. Switch to CQL as soon as possible.</li>\n<li>But make sure you understand your data models and queries. It’s not SQL.</li>\n</ol><p><strong><a href=\"https://twitter.com/rs_atl\" target=\"_blank\">Robbie Strickland</a>, Director of Software Development at The Weather Channel</strong><br />Robbie works for The Weather Channel’s digital division, as part of the team that builds backend services for weather.com and the TWC mobile apps. He has been involved in the Cassandra project since 2010 and  contributed in a variety of ways over the years; this includes work on drivers for Scala and C#,  Hadoop integration, leading the <a href=\"http://www.meetup.com/atlcassandra/\" target=\"_blank\">Atlanta Cassandra Users Group</a>, and answering lots of questions on StackOverflow. This post is adapted from Robbie Strickland’s  book, Cassandra High Availability, available for purchase <a href=\"http://www.amazon.com/Cassandra-High-Availability-Robbie-Strickland/dp/1783989122/\" target=\"_blank\">here</a>.</p>",
        "created_at": "2018-04-15T19:43:56+0000",
        "updated_at": "2018-04-15T19:43:59+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 23,
        "domain_name": "academy.datastax.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9586"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          }
        ],
        "is_public": false,
        "id": 9483,
        "uid": null,
        "title": "Snap Cassandra to S3 with tablesnap",
        "url": "https://www.linkedin.com/pulse/snap-cassandra-s3-tablesnap-vijaya-kumar-hosamani/",
        "content": "<p>We were trying to find solution to backup and restore Cassandra hosted on AWS without loosing too much data . We tried using the native <strong>nodetool snapshot </strong>but we will loose data between the snapshots. </p><p>We found a tool which seems to do the job well <a href=\"https://github.com/JeremyGrosser/tablesnap\" target=\"_blank\" rel=\"nofollow noopener\">tablesnap</a> . This tool provides scripts which can help backup , restore SSTables. We can restore data from specific time in past. </p> \n<p>The following are the scripts which are part of this project.</p> \n<ul><li><strong>tablesnap : </strong> This script can be used for backup Cassandra data folder to S3</li> \n <li><strong>tableslurp :</strong> Used to restore the data files from S3</li> \n <li><strong>tablechop :</strong> Used to delete older data files from S3.</li> \n</ul><p><strong>tablesnap</strong> script monitors the Cassandra data folder continuously and copies any new files which are created to configured S3 bucket. This tool also creates a JSON file which lists all the files in the data folder at the time of snapshot providing option to restore at a specific time in past.</p> \n<p>The usage of the tool is as below</p> \n<pre spellcheck=\"false\">tablesnap [-h] -k AWS_KEY -s AWS_SECRET [-r] [-a] [-B] [-p PREFIX]\n                 [--without-index] [--keyname-separator KEYNAME_SEPARATOR]\n                 [-t THREADS] [-n NAME] [-e EXCLUDE | -i INCLUDE]\n                 [--listen-events {IN_MOVED_TO,IN_CLOSE_WRITE}]\n                 [--max-upload-size MAX_UPLOAD_SIZE]\n                 [--multipart-chunk-size MULTIPART_CHUNK_SIZE]\n                 bucket paths [paths ...]\n</pre> \n<p>This tool depends few basic principles of the Cassandra data </p> \n<ul><li>The data is stored as series of files.</li> \n <li>The SSTables are immutable structures and once created they are never modified.</li> \n <li>During compaction the old SSTable files are deleted &amp; a new file is created instead of updating the existing file.</li> \n <li>New SSTables are created in temporary folder and then moved into the data folder. The tool listens to IN_MOVED or CLOSE_WRITE events hence file will be consistent when uploaded to S3</li> \n</ul><p>The usage of the script is as below, in a cluster this needs to be configured on each node with respective node names.</p> \n<pre spellcheck=\"false\">tablesnap -B -a –r --aws-region ap-southeast-1 mybucket -n node1 /var/lib/cassandra/data/mykeyspace\n</pre> \n<p>The content of the JSON file is as shown below which lists all the files which are part of the snapshot. </p> \n<pre spellcheck=\"false\">{  \n   \"/var/lib/cassandra/data/mykeyspace/users-cf815f100f9711e78211dbd467a9ea7d\":[  \n      \"backups\",\n      \"lb-1- big-Index.db\",\n      \"lb-1- big-Digest.adler32\",\n      \"lb-1- big-Statistics.db\",\n      \"lb-1- big-CompressionInfo.db\",\n      \"lb-1- big-Data.db\",\n      \"lb-1- big-Summary.db\",\n      \"lb-1- big-Filter.db\",\n      \"lb-1- big-TOC.txt\"\n   ]\n</pre> \n<pre spellcheck=\"false\">}\n</pre> \n<p>In order to restore the data from the S3 , we can use <strong>tableslurp </strong>tool . We need to identify the snapshot which we need to recover and use the tool to download the data from S3 as below.</p> \n<pre spellcheck=\"false\">udo tableslurp -- aws-region ap-southeast- 1 mybucket -n node1\n/var/lib/cassandra/data/mykeyspace/users-cf815f100f9711e78211dbd467a9ea7d /var/lib/cassandra/data/mykeyspace/users-f234fgsdfsfdsfsdd\n --file lb-2-big-Data.db -o cassandra -g cassandra\n</pre> \n \n<p>In the above commands we are restoring the data on <strong>node1 </strong>which was backed up while SSTable <strong>lb-2-big-Data.db</strong> is created. </p> \n<p>To recover the entire cluster we need to shutdown new nodes &amp; recover respective data with <strong>tableslurp </strong>and then restart the nodes (seed nodes followed by others) . Once the cluster is up run <strong>nodetool repair </strong>to restore all the data. </p> \n<p>One additional step which we have to take care to avoid loosing data which is in <strong>memtable</strong> is by periodic <strong>nodetool flush</strong> which will help to avoid loosing data .</p>",
        "created_at": "2018-04-03T23:50:13+0000",
        "updated_at": "2018-04-04T12:13:21+0000",
        "published_at": "2017-03-29T00:00:00+0000",
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "www.linkedin.com",
        "preview_picture": "https://media.licdn.com/mpr/mpr/shrinknp_400_400/gcrc/dms/image/C4E12AQFxCdkqPl_1Yw/article-cover_image-shrink_600_2000/0?e=2122002000&v=alpha&t=0DhdTB45E9HPx90dNMs6SIb8l5YJ2ZkbtkfeTOfupAA",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9483"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          }
        ],
        "is_public": false,
        "id": 9482,
        "uid": null,
        "title": "JeremyGrosser/tablesnap",
        "url": "https://github.com/JeremyGrosser/tablesnap",
        "content": "<h2>Theory of Operation</h2><p>Tablesnap is a script that uses inotify to monitor a directory for <code>IN_MOVED_TO</code>\nevents and reacts to them by spawning a new thread to upload that file to\nAmazon S3, along with a JSON-formatted list of what other files were in the\ndirectory at the time of the copy.</p><p>When running a Cassandra cluster, this behavior can be quite useful as it\nallows for automated point-in-time backups of SSTables. Theoretically,\ntablesnap should work for any application where files are written to some\ntemporary location, then moved into their final location once the data is\nwritten to disk. Tablesnap also makes the assumption that files are immutable\nonce written.</p><h2>Installation</h2><p>The simplest way to install tablesnap is from the Python Package Index, PyPI.\n<a href=\"https://pypi.python.org/pypi/tablesnap\" rel=\"nofollow\">https://pypi.python.org/pypi/tablesnap</a></p><pre>pip install tablesnap\n</pre><p>This distribution provides a debian/ source directory, allowing it to be built\nas a standard Debian/Ubuntu package and stored in a repository. The Debian\npackage includes an init script that can run and daemonize tablesnap for you.\nTablesnap does not daemonize itself. This is best left to tools like\ninit, supervisord, daemontools, etc.</p><p>We do not currently maintain binary packages of tablesnap. To build the debian\npackage from source, assuming you have a working pbuilder environment:</p><pre>git checkout debian\ngit-buildpackage --git-upstream-branch=master --git-debian-branch=debian --git-builder='pdebuild'\n</pre><p>The daemonized version of the Debian/Ubuntu package uses syslog for logging.\nThe messages are sent to the <code>DAEMON</code> logging facility and tagged with\n<code>tablesnap</code>. If you want to redirect the log output to a log file other than\n<code>/var/log/daemon.log</code> you can filter by this tag. E.g. if you are using\nsyslog-ng you could add</p><pre># tablesnap\nfilter f_tablesnap { filter(f_daemon) and match(\"tablesnap\" value(\"PROGRAM\")); };\ndestination d_tablesnap { file(\"/var/log/tablesnap.log\"); };\nlog { source(s_src); filter(f_tablesnap); destination(d_tablesnap); flags(final); };\n</pre><p>to <code>/etc/syslog-ng/syslog-ng.conf</code>.</p><p>If you are not a Debian/Ubuntu user or do not wish to install the tablesnap\npackage, you may copy the tablesnap script anywhere you'd like and run it from\nthere. Tablesnap depends on the pyinotify and boto Python packages. These are\navailable via \"pip install pyinotify; pip install boto;\", or as packages from\nmost common Linux distributions.</p><h2>Configuration</h2><p>All configuration for tablesnap happens on the command line. If you are using\nthe Debian package, you'll set these options in the <code>DAEMON_OPTS</code> variable in\n<code>/etc/default/tablesnap</code>.</p><pre>usage: tablesnap [-h] -k AWS_KEY -s AWS_SECRET [-r] [-a] [-B] [-p PREFIX]\n                 [--without-index] [--keyname-separator KEYNAME_SEPARATOR]\n                 [-t THREADS] [-n NAME] [-e EXCLUDE | -i INCLUDE]\n                 [--listen-events {IN_MOVED_TO,IN_CLOSE_WRITE}]\n                 [--max-upload-size MAX_UPLOAD_SIZE]\n                 [--multipart-chunk-size MULTIPART_CHUNK_SIZE]\n                 bucket paths [paths ...]\nTablesnap is a script that uses inotify to monitor a directory for events and\nreacts to them by spawning a new thread to upload that file to Amazon S3,\nalong with a JSON-formatted list of what other files were in the directory at\nthe time of the copy.\npositional arguments:\n  bucket                S3 bucket\n  paths                 Paths to be watched\noptional arguments:\n  -h, --help            show this help message and exit\n  -k AWS_KEY, --aws-key AWS_KEY\n  -s AWS_SECRET, --aws-secret AWS_SECRET\n  -r, --recursive       Recursively watch the given path(s)s for new SSTables\n  -a, --auto-add        Automatically start watching new subdirectories within\n                        path(s)\n  -B, --backup          Backup existing files to S3 if they are not already\n                        there\n  -p PREFIX, --prefix PREFIX\n                        Set a string prefix for uploaded files in S3\n  --without-index       Do not store a JSON representation of the current\n                        directory listing in S3 when uploading a file to S3.\n  --keyname-separator KEYNAME_SEPARATOR\n                        Separator for the keyname between name and path.\n  -t THREADS, --threads THREADS\n                        Number of writer threads\n  -n NAME, --name NAME  Use this name instead of the FQDN to identify the\n                        files from this host\n  -e EXCLUDE, --exclude EXCLUDE\n                        Exclude files matching this regular expression from\n                        upload.WARNING: If neither exclude nor include are\n                        defined, then all files matching \"-tmp\" are excluded.\n  -i INCLUDE, --include INCLUDE\n                        Include only files matching this regular expression\n                        into upload.WARNING: If neither exclude nor include\n                        are defined, then all files matching \"-tmp\" are\n                        excluded.\n  --listen-events {IN_MOVED_TO,IN_CLOSE_WRITE,IN_CREATE}\n                        Which events to listen on, can be specified multiple\n                        times. Values: IN_MOVED_TO, IN_CLOSE_WRITE, IN_CREATE\n                        (default: IN_MOVED_TO, IN_CLOSE_WRITE)\n  --max-upload-size MAX_UPLOAD_SIZE\n                        Max size for files to be uploaded before doing\n                        multipart (default 5120M)\n  --multipart-chunk-size MULTIPART_CHUNK_SIZE\n                        Chunk size for multipart uploads (default: 256M or 10%\n                        of free memory if default is not available)\n</pre><p>For example:</p><pre>$ tablesnap -k AAAAAAAAAAAAAAAA -s BBBBBBBBBBBBBBBB me.synack.sstables /var/lib/cassandra/data/GiantKeyspace\n</pre><p>This would cause tablesnap to use the given Amazon Web Services credentials to\nbackup the SSTables for my <code>GiantKeyspace</code> to the S3 bucket named\n<code>me.synack.sstables</code>.</p><h2>Questions, Comments, and Help</h2><p>The fine folks in <code>#cassandra-ops</code> on <code>irc.freenode.net</code> are an excellent\nresource for getting tablesnap up and running, and also for solving more\ngeneral Cassandra issues.</p>",
        "created_at": "2018-04-03T23:50:04+0000",
        "updated_at": "2018-05-25T21:12:40+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 4,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/2151?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9482"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9480,
        "uid": null,
        "title": "Cassandra Backup and Restore - Backup in AWS using EBS Volumes",
        "url": "http://thelastpickle.com/blog/2018/04/03/cassandra-backup-and-restore-aws-ebs.html",
        "content": "<p>Data is critical to modern business and operational teams need to have a Disaster Recovery Plan (DRP) to deal with the risks of potential data loss. At TLP, we are regularly involved in the data recovery and restoration process and in this post we will share information we believe will be useful for those interested in initiating or improving their backup and restore strategy for Apache Cassandra. We will consider some common solutions, and detail the solution we consider the most efficient in AWS + EBS environments as it allows the best Recovery Time Objective (RTO) and is relatively easy to implement.</p><ul><li><a href=\"#why-do-we-need-to-backup-cassandra-data\">Why Do We Need to Backup Cassandra Data?</a></li>\n  <li><a href=\"#backup-and-restore-solutions\">Backup and Restore Solutions</a></li>\n  <li><a href=\"#using-snapshots\">Using Snapshots</a></li>\n  <li><a href=\"#using-native-incremental-backups\">Using Native Incremental Backups</a></li>\n  <li><a href=\"#open-source-tools-tablesnap\">Open-source Tools: TableSnap</a></li>\n  <li><a href=\"#commercial-solutions-datastax-enterprise-datosio\">Commercial Solutions (Datastax Enterprise, datos.io)</a></li>\n  <li><a href=\"#the-manual-copypaste-option\">The Manual Copy/Paste Option</a>\n    <ul><li><a href=\"#limitations\">Limitations</a></li>\n      <li><a href=\"#demonstration-of-the-manual-copypaste-approach\">Demonstration of the Manual Copy/Paste Approach</a></li>\n      <li><a href=\"#create-a-backup-using-copypaste\">Create a Backup using Copy/Paste</a></li>\n      <li><a href=\"#simulation-of-a-major-outage\">Simulation of a Major Outage</a></li>\n      <li><a href=\"#restore-the-service-and-data-with-copypaste\">Restore the Service and Data with Copy/Paste</a></li>\n    </ul></li>\n  <li><a href=\"#the-copypaste-approach-on-aws-ec2-with-ebs-volumes\">The ‘Copy/Paste’ Approach on AWS EC2 with EBS Volumes</a>\n    <ul><li><a href=\"#backup-policy-and-automated-snapshots\">Backup policy and automated snapshots</a></li>\n      <li><a href=\"#restore-procedure-aws\">Restore procedure (AWS)</a></li>\n      <li><a href=\"#summing-up\">Summing up</a></li>\n    </ul></li>\n  <li><a href=\"#lets-compare\">Let’s compare!</a></li>\n</ul><h2 id=\"why-do-we-need-to-backup-cassandra-data\">Why Do We Need to Backup Cassandra Data?</h2>\n<p>Apache Cassandra data is replicated, it makes sense to take a moment to understand why backups still matters for the distributed databases.</p>\n<p>In certain cases when a cluster is poorly configured it can be prone to total data loss. Examples of such cases are:</p>\n<ul><li>Using only a datacenter or non-physically distributed hardware</li>\n  <li>Placing all nodes in one rack</li>\n  <li>Using SAN for storage (other than within a rack and properly configured)</li>\n</ul><p>If the hardware being rely on crashes in any of the above cases, the data might be definitely lost. Thus, it is vital to have a backup even prior to fixing any design mistakes. This holds even if the RPO (Recovery Point Objective), the date of your last backup, was a week ago. Losing a week of data is better than losing years of data, in most cases.</p>\n<p>Note that a backup strategy cannot replace a well distributed system, mostly because restore is a slow and heavy process and will always generate a small gap - a period of time during which data will be lost. However, it can prevent a bigger, or total, data loss.</p>\n<p>In fact, even when Apache Cassandra is well configured, it makes sense to have some backups. Imagine an operator needs to wipe the data on a staging or testing cluster and runs the command <code class=\"highlighter-rouge\">rm -rf /var/lib/cassandra/*</code> in parallel via Chef or Capsitrano, only to find out the command was accidentally run in the production cluster instead (wrong terminal, bad alias, bad script configuration, etc…). If you are responsible for this cluster, or are just the person unlucky enough to have pressed the wrong button, you will be very glad to have a backup somewhere.</p>\n<p>Apache Cassandra will gently replicate any operation on a node throughout the cluster, including user mistakes that could potentially lose data, such as <code class=\"highlighter-rouge\">DROP TABLE X</code> or <code class=\"highlighter-rouge\">TRUNCATE TABLE Y</code>. Luckily for people facing this, there is a safeguard as automatic snapshots are taken by default (see the snapshots section below).</p>\n<p>But if the problem comes from heavy batch of standard operations (<code class=\"highlighter-rouge\">INSERT</code> / <code class=\"highlighter-rouge\">UPDATE</code> / <code class=\"highlighter-rouge\">DELETE</code>), it is sometimes better to go back to a known safe point and accept losing some data than trying to deal with this new situation.</p>\n<p>A backup strategy is not foolproof, rather it just reduces the odds that something goes very wrong to a very low level. Having a full set of cluster data somewhere else, on a cold storage can be very useful. Even though it costs money and effort to put a backup and restore strategy in place, it is insignificant compared to a possible total loss of data.</p>\n<h2 id=\"backup-and-restore-solutions\">Backup and Restore Solutions</h2>\n<p>Any backup strategy will be limited by technical possibilities, as there is often a lot of data to move around when making a backup of a Cassandra cluster. Budget considerations are the second biggest constraints in many cases, as is task prioritization. Building the backup strategy is about finding the best tradeoff between these constraints and the desired RPO and RTO.</p>\n<p>Other considerations are how valuable the data is, how the risk data events is evaluated, and on the maximum data loss that is acceptable. The only recommendation we would make in this regard is to plan for your worst case scenario.</p>\n<p>Depending on the cluster, distinct solutions can be extremely efficient or perform very poorly and not reach RPO and RTO goals. What matters in order to make the right call is to understand your needs and what performances each solution provides in your own environment. There are a number of articles already covering the most common backup and restore solutions, such as https://devops.com/things-know-planning-cassandra-backup/, thus we are going to focus here on presenting one of these options, that is probably less well known, but is an excellent way to backup on AWS when using EBS Volumes.</p>\n<p>Specifically, in the following sections we will review the RPO, RTO, set up and running costs, and ease of setup for the following backup methods:\nSnapshots\nIncremental backups\nTableSnap\nSome commercial solutions\nThe copy/paste method\nAWS Snapshots/EBS Attach.</p>\n<h2 id=\"using-snapshots\">Using Snapshots</h2>\n<p>After flushing all in-memory writes to disk, snapshots creates hard links of each SSTable that is part of the snapshot scope. Doing a snapshot is a simple command and comes with no immediate impacts on performance, capacity storage, or money.</p>\n<p>Running a snapshot on all the nodes and for all the keyspaces solves the potential inconsistencies issues related to copying the data out of the production disk, as the snapshot can be taken relatively simultaneously on all the nodes and will take an instantaneous ‘picture’ of the data at a specific moment.</p>\n<p>As compaction merges SSTables and depending on the compaction pace, the snapshots start consuming significant space on disk. Thus, they will need to be removed as soon as they are extracted from the disk and put into a safe place. Critically for the utility of this approach, removal has to be handled manually, as Apache Cassandra does not automatically remove snapshots.</p>\n<p>Extracting the data from the machines and then copying it to the new machines leads to a relatively bad RPO and RTO as the dataset per node grows. The transfer of such a large amount of data is also likely to raise costs making this operation prohibitively expensive to be performed often enough to be useful, thus not allowing for a good RPO in most cases.</p>\n<h2 id=\"using-native-incremental-backups\">Using Native Incremental Backups</h2>\n<p>Incremental backups allow the operator to take snapshots of the missing SSTables since the latest snapshot, removing the need to snapshot all the data every time. This significantly reduces the snapshot size after the first full snapshot, reducing both the cost of extraction from the local machine and cold storage.</p>\n<p>Thus native incremental backups provide a much better RPO than the full snapshot method alone, considering that data extraction time to the external storage is part of the backup.</p>\n<p>On the downside, incremental backups are made of a lot of rather small SSTables that will need to be compacted together at the recovery time, possibly inducing a slow start from new machines after an outage and creating the need to catch up with compactions. If this is pushed to an extreme and data is spread across a lot of SSTables, the read latency could make the node completely unresponsive due to the resources used for compactions and the need for reads to open a lot of SSTables on disk, thus lowering RTO.</p>\n<p>When picking this option, it is vital to still make a full snapshot from time to time in order to prevent the situation mentioned above. For more information on incremental backups, this article is a bit old, but very well detailed: http://techblog.constantcontact.com/devops/cassandra-and-backups/</p>\n<h2 id=\"open-source-tools-tablesnap\">Open-source Tools: TableSnap</h2>\n<p>Some open-source tools are based on the snapshots or incremental backups methods, described above. These tools aim to make operators’ lives easier by providing some automation to manage snapshots and extract them to the cold storage.</p>\n<p>TableSnap copies any new SSTable in the Apache Cassandra data folder as soon as it is created, thus providing a very good RPO and the ability to go back to a specific point in time. However, it comes at the price of streaming more data than if it were just sending incremental data less frequently. This is because each compaction generates an entirely new SSTables from existing SSTables. The newly generated SSTables are then streamed to the backup destination. Regardless of whether the data exists in old SSTables at the backup destination, the new SSTables will be streamed to the backup destination. If considering this option you will want to have a look at TableSnap, which lives here: https://github.com/JeremyGrosser/tablesnap. You should also consider reading: https://www.linkedin.com/pulse/snap-cassandra-s3-tablesnap-vijaya-kumar-hosamani/.</p>\n<h2 id=\"commercial-solutions-datastax-enterprise-datosio\">Commercial Solutions (Datastax Enterprise, datos.io)</h2>\n<p>In the same way, there are a handful of commercial solutions that handle backups for you. The best known is the backup / restore feature included in DSE - https://www.datastax.com/products/datastax-enterprise. However, this does not work with Apache Cassandra (or open source / community version) and in order to leverage this feature, the entire DSE product will need to be purchased and used. Another alternative that has been around for a while is http://datos.io/. In fact, they do their own comparison of existing backup solutions for Cassandra here: http://datos.io/2017/02/02/choose-right-backup-solution-cassandra/.</p>\n<h2 id=\"the-manual-copypaste-option\">The Manual Copy/Paste Option</h2>\n<p>While this is normally an inefficient backup solution, we spent some time working with it and had some interesting results.  This section will explore the copy/paste option in detail and evaluate the utility. Specifically the scenario involving extracting all the data from the node and putting it back on a new node or cluster.</p>\n<p>Because each node is responsible for a specific range, building a new node or a new cluster for the restore process will change the token distribution, and it is something that can be hard to control when restoring data, especially when using vnodes. Copying the data from all the keyspaces on all the nodes will induce a very messy restore procedure, or a fairly slow one using the <code class=\"highlighter-rouge\">sstableloader</code> for example. This is not really reliable or efficient.</p>\n<p>A way to workaround this problem is to store the entire <code class=\"highlighter-rouge\">data</code> folder for each Cassandra node. This way when restoring a node or a cluster, it is possible to have the information about the schema and token range distribution saved alongside the data, in the <code class=\"highlighter-rouge\">system</code> keyspace.</p>\n<p>With this strategy when the node bootstraps, it detects the IP change, but this is handled and the replacement nodes come back online with the latest copy of the data, including the schema description and the token ranges distribution. This kind of backup stores the data and all the metadata used by Apache Cassandra next to it, which is really convenient, but has its limitations.</p>\n<h3 id=\"limitations\">Limitations</h3>\n<p>Using this strategy the <em>cluster</em> and <em>datacenter</em> names must be identical to the original cluster. Using the exact same configuration to restore is the best approach, just changing the <code class=\"highlighter-rouge\">seeds</code> in <code class=\"highlighter-rouge\">cassandra.yaml</code> and the ‘dynamic’ part of the configuration, <code class=\"highlighter-rouge\">listen_address</code>, <code class=\"highlighter-rouge\">rpc_address</code>, mostly IP related informations.</p>\n<p>In addition, the topology configuration must be identical to the original cluster; that is, each rack must contain the same number of nodes as the original cluster. When data is placed back on the new nodes, data copied from nodes in rack 1 must be placed on the new nodes in rack 1 and so on for each of the other racks in the cluster.</p>\n<h3 id=\"demonstration-of-the-manual-copypaste-approach\">Demonstration of the Manual Copy/Paste Approach</h3>\n<p>To explain how to do this process and show it working, here is a short and simple example using <a href=\"https://github.com/riptano/ccm\">CCM</a>.</p>\n<p>In this example we use a predefined dataset and CCM to reduce the time taken to create the example. In practice this will work the same way with bigger datasets, but will likely take more time if copying the data from an external source of course. It will just be linearly slower to backup and restore as the dataset per node grows.</p>\n<p>Here are the files we are going to use:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>$ cat schema.cql\nDROP KEYSPACE IF EXISTS tlp_lab;\nCREATE KEYSPACE tlp_lab WITH replication = {'class': 'NetworkTopologyStrategy', 'datacenter1' : 2};\nCREATE TABLE tlp_lab.test_backup_restore (id text,  column1 text, PRIMARY KEY (id, column1));\n$ cat insert.cql\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('1', '100');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('1', '200');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('1', '300');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('2', '100');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('2', '200');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('2', '300');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('3', '100');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('3', '200');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('3', '300');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('4', '100');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('4', '200');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('4', '300');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('5', '100');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('5', '200');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('5', '300');\n</pre></div></div>\n<p>Let’s start by creating the data and make sure the memtables are flushed to an on-disk SSTable, as follows:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>$ ccm node1 cqlsh -- -f schema.cql\n$ ccm node1 cqlsh -- -f insert.cql\n</pre></div></div>\n<p>Then let’s query the data and make sure we got all the expected data;</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>$ ccm node1 cqlsh -- -e \"SELECT * FROM tlp_lab.test_backup_restore;\"\n id | column1\n----+---------\n  4 |     100\n  4 |     200\n  4 |     300\n  3 |     100\n  3 |     200\n  3 |     300\n  5 |     100\n  5 |     200\n  5 |     300\n  2 |     100\n  2 |     200\n  2 |     300\n  1 |     100\n  1 |     200\n  1 |     300\n(15 rows)\n</pre></div></div>\n<p>Looks good, all the data is there, and we can make sure the memtable was flushed and the SSTables have been written to the disk:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>$ ./flush.sh\n$ ll /Users/alain/.ccm/6xCassandra-3-11-1/node*/data0/tlp_lab/test_backup_restore-9e8e3ce006b111e89daea3c19988ea88\n/Users/alain/.ccm/6xCassandra-3-11-1/node1/data0/tlp_lab/test_backup_restore-9e8e3ce006b111e89daea3c19988ea88:\ntotal 72\ndrwxr-xr-x  11 alain  staff   352 Jan 31 18:07 .\ndrwxr-xr-x   3 alain  staff    96 Feb  2 10:13 ..\n-rw-r--r--   1 alain  staff    43 Jan 31 18:07 mc-1-big-CompressionInfo.db\n-rw-r--r--   1 alain  staff    78 Jan 31 18:07 mc-1-big-Data.db\n-rw-r--r--   1 alain  staff    10 Jan 31 18:07 mc-1-big-Digest.crc32\n-rw-r--r--   1 alain  staff    16 Jan 31 18:07 mc-1-big-Filter.db\n-rw-r--r--   1 alain  staff    10 Jan 31 18:07 mc-1-big-Index.db\n-rw-r--r--   1 alain  staff  4613 Jan 31 18:07 mc-1-big-Statistics.db\n-rw-r--r--   1 alain  staff    47 Jan 31 18:07 mc-1-big-Summary.db\n-rw-r--r--   1 alain  staff    92 Jan 31 18:07 mc-1-big-TOC.txt\n/Users/alain/.ccm/6xCassandra-3-11-1/node2/data0/tlp_lab/test_backup_restore-9e8e3ce006b111e89daea3c19988ea88:\ntotal 72\ndrwxr-xr-x  11 alain  staff   352 Jan 31 18:07 .\ndrwxr-xr-x   5 alain  staff   160 Jan 31 18:07 ..\ndrwxr-xr-x   2 alain  staff    64 Jan 31 18:07 backups\n-rw-r--r--   1 alain  staff    43 Jan 31 18:07 mc-1-big-CompressionInfo.db\n-rw-r--r--   1 alain  staff    52 Jan 31 18:07 mc-1-big-Data.db\n-rw-r--r--   1 alain  staff     9 Jan 31 18:07 mc-1-big-Digest.crc32\n-rw-r--r--   1 alain  staff    16 Jan 31 18:07 mc-1-big-Filter.db\n-rw-r--r--   1 alain  staff     5 Jan 31 18:07 mc-1-big-Index.db\n-rw-r--r--   1 alain  staff  4610 Jan 31 18:07 mc-1-big-Statistics.db\n-rw-r--r--   1 alain  staff    47 Jan 31 18:07 mc-1-big-Summary.db\n-rw-r--r--   1 alain  staff    92 Jan 31 18:07 mc-1-big-TOC.txt\n/Users/alain/.ccm/6xCassandra-3-11-1/node3/data0/tlp_lab/test_backup_restore-9e8e3ce006b111e89daea3c19988ea88:\ntotal 0\ndrwxr-xr-x  3 alain  staff   96 Jan 31 18:07 .\ndrwxr-xr-x  5 alain  staff  160 Jan 31 18:07 ..\ndrwxr-xr-x  2 alain  staff   64 Jan 31 18:07 backups\n/Users/alain/.ccm/6xCassandra-3-11-1/node4/data0/tlp_lab/test_backup_restore-9e8e3ce006b111e89daea3c19988ea88:\ntotal 72\ndrwxr-xr-x  11 alain  staff   352 Jan 31 18:07 .\ndrwxr-xr-x   5 alain  staff   160 Jan 31 18:07 ..\ndrwxr-xr-x   2 alain  staff    64 Jan 31 18:07 backups\n-rw-r--r--   1 alain  staff    43 Jan 31 18:07 mc-1-big-CompressionInfo.db\n-rw-r--r--   1 alain  staff    78 Jan 31 18:07 mc-1-big-Data.db\n-rw-r--r--   1 alain  staff     9 Jan 31 18:07 mc-1-big-Digest.crc32\n-rw-r--r--   1 alain  staff    16 Jan 31 18:07 mc-1-big-Filter.db\n-rw-r--r--   1 alain  staff    10 Jan 31 18:07 mc-1-big-Index.db\n-rw-r--r--   1 alain  staff  4614 Jan 31 18:07 mc-1-big-Statistics.db\n-rw-r--r--   1 alain  staff    47 Jan 31 18:07 mc-1-big-Summary.db\n-rw-r--r--   1 alain  staff    92 Jan 31 18:07 mc-1-big-TOC.txt\n/Users/alain/.ccm/6xCassandra-3-11-1/node5/data0/tlp_lab/test_backup_restore-9e8e3ce006b111e89daea3c19988ea88:\ntotal 72\ndrwxr-xr-x  11 alain  staff   352 Jan 31 18:07 .\ndrwxr-xr-x   5 alain  staff   160 Jan 31 18:07 ..\ndrwxr-xr-x   2 alain  staff    64 Jan 31 18:07 backups\n-rw-r--r--   1 alain  staff    43 Jan 31 18:07 mc-1-big-CompressionInfo.db\n-rw-r--r--   1 alain  staff   104 Jan 31 18:07 mc-1-big-Data.db\n-rw-r--r--   1 alain  staff    10 Jan 31 18:07 mc-1-big-Digest.crc32\n-rw-r--r--   1 alain  staff    16 Jan 31 18:07 mc-1-big-Filter.db\n-rw-r--r--   1 alain  staff    15 Jan 31 18:07 mc-1-big-Index.db\n-rw-r--r--   1 alain  staff  4618 Jan 31 18:07 mc-1-big-Statistics.db\n-rw-r--r--   1 alain  staff    47 Jan 31 18:07 mc-1-big-Summary.db\n-rw-r--r--   1 alain  staff    92 Jan 31 18:07 mc-1-big-TOC.txt\n/Users/alain/.ccm/6xCassandra-3-11-1/node6/data0/tlp_lab/test_backup_restore-9e8e3ce006b111e89daea3c19988ea88:\ntotal 72\ndrwxr-xr-x  11 alain  staff   352 Jan 31 18:07 .\ndrwxr-xr-x   5 alain  staff   160 Jan 31 18:07 ..\ndrwxr-xr-x   2 alain  staff    64 Jan 31 18:07 backups\n-rw-r--r--   1 alain  staff    43 Jan 31 18:07 mc-1-big-CompressionInfo.db\n-rw-r--r--   1 alain  staff    82 Jan 31 18:07 mc-1-big-Data.db\n-rw-r--r--   1 alain  staff    10 Jan 31 18:07 mc-1-big-Digest.crc32\n-rw-r--r--   1 alain  staff    16 Jan 31 18:07 mc-1-big-Filter.db\n-rw-r--r--   1 alain  staff    10 Jan 31 18:07 mc-1-big-Index.db\n-rw-r--r--   1 alain  staff  4614 Jan 31 18:07 mc-1-big-Statistics.db\n-rw-r--r--   1 alain  staff    47 Jan 31 18:07 mc-1-big-Summary.db\n-rw-r--r--   1 alain  staff    92 Jan 31 18:07 mc-1-big-TOC.txt\n</pre></div></div>\n<h3 id=\"create-a-backup-using-copypaste\">Create a Backup using Copy/Paste</h3>\n<p>Now let’s make a backup (a simple copy) of the entire <code class=\"highlighter-rouge\">data0</code> folder for all nodes.</p>\n<p>In our test case, this would be enough:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>$ for i in {1..6}; do cp -rp /Users/alain/.ccm/6xCassandra-3-11-1/node$i/data0 backup/node$i/; done\n$ cd backups/\n$ ll -d */*/\ndrwxr-xr-x  26 alain  staff  832 Jan 31 17:34 node1/system/\ndrwxr-xr-x   6 alain  staff  192 Jan 31 17:34 node1/system_auth/\ndrwxr-xr-x   5 alain  staff  160 Jan 31 17:34 node1/system_distributed/\ndrwxr-xr-x  12 alain  staff  384 Jan 31 17:34 node1/system_schema/\ndrwxr-xr-x   4 alain  staff  128 Jan 31 17:34 node1/system_traces/\ndrwxr-xr-x   3 alain  staff   96 Feb  2 10:13 node1/tlp_lab/\ndrwxr-xr-x  26 alain  staff  832 Jan 31 17:34 node2/system/\ndrwxr-xr-x   6 alain  staff  192 Jan 31 17:34 node2/system_auth/\ndrwxr-xr-x   5 alain  staff  160 Jan 31 17:34 node2/system_distributed/\ndrwxr-xr-x  12 alain  staff  384 Jan 31 17:34 node2/system_schema/\ndrwxr-xr-x   4 alain  staff  128 Jan 31 17:34 node2/system_traces/\ndrwxr-xr-x   5 alain  staff  160 Jan 31 18:07 node2/tlp_lab/\ndrwxr-xr-x  26 alain  staff  832 Jan 31 17:34 node3/system/\ndrwxr-xr-x   6 alain  staff  192 Jan 31 17:34 node3/system_auth/\ndrwxr-xr-x   5 alain  staff  160 Jan 31 17:34 node3/system_distributed/\ndrwxr-xr-x  12 alain  staff  384 Jan 31 17:34 node3/system_schema/\ndrwxr-xr-x   4 alain  staff  128 Jan 31 17:34 node3/system_traces/\ndrwxr-xr-x   5 alain  staff  160 Jan 31 18:07 node3/tlp_lab/\ndrwxr-xr-x  26 alain  staff  832 Jan 31 17:34 node4/system/\ndrwxr-xr-x   6 alain  staff  192 Jan 31 17:34 node4/system_auth/\ndrwxr-xr-x   5 alain  staff  160 Jan 31 17:34 node4/system_distributed/\ndrwxr-xr-x  12 alain  staff  384 Jan 31 17:34 node4/system_schema/\ndrwxr-xr-x   4 alain  staff  128 Jan 31 17:34 node4/system_traces/\ndrwxr-xr-x   5 alain  staff  160 Jan 31 18:07 node4/tlp_lab/\ndrwxr-xr-x  26 alain  staff  832 Jan 31 17:34 node5/system/\ndrwxr-xr-x   6 alain  staff  192 Jan 31 17:34 node5/system_auth/\ndrwxr-xr-x   5 alain  staff  160 Jan 31 17:34 node5/system_distributed/\ndrwxr-xr-x  12 alain  staff  384 Jan 31 17:34 node5/system_schema/\ndrwxr-xr-x   4 alain  staff  128 Jan 31 17:34 node5/system_traces/\ndrwxr-xr-x   5 alain  staff  160 Jan 31 18:07 node5/tlp_lab/\ndrwxr-xr-x  26 alain  staff  832 Jan 31 17:34 node6/system/\ndrwxr-xr-x   6 alain  staff  192 Jan 31 17:34 node6/system_auth/\ndrwxr-xr-x   5 alain  staff  160 Jan 31 17:34 node6/system_distributed/\ndrwxr-xr-x  12 alain  staff  384 Jan 31 17:34 node6/system_schema/\ndrwxr-xr-x   4 alain  staff  128 Jan 31 17:34 node6/system_traces/\ndrwxr-xr-x   5 alain  staff  160 Jan 31 18:07 node6/tlp_lab/\n</pre></div></div>\n<h3 id=\"simulation-of-a-major-outage\">Simulation of a Major Outage</h3>\n<p>Then we simulate the loss of two nodes by shutting them down, as one would not affect the cluster, working with a Replication Factor of 2 (RF = 2).</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>$ ccm node1 stop\n$ ccm node2 stop\n$ ccm node3 nodetool status\nDatacenter: datacenter1\n=======================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address    Load       Tokens       Owns (effective)  Host ID                               Rack\nDN  127.0.0.1  176.15 KiB  1            33.3%             b6497c83-0e85-425e-a739-506dd882b013  rack1\nDN  127.0.0.2  149.68 KiB  1            33.3%             e4919f5a-3b89-4fd1-a163-aaae99aa5cbd  rack1\nUN  127.0.0.3  149.02 KiB  1            33.3%             c6e8e8c4-25b1-444a-ac50-ff00505fbaf7  rack1\nUN  127.0.0.4  129.8 KiB  1            33.3%             23bdc0df-1ba5-4bc7-be41-01436fa23925  rack1\nUN  127.0.0.5  139.19 KiB  1            33.3%             fb438687-8c3e-4ad7-b83d-275948f4241f  rack1\nUN  127.0.0.6  144.11 KiB  1            33.3%             63777a4b-af44-4fc2-baff-74c585d4a217  rack1\n</pre></div></div>\n<p>And now reading the data again, we notice the query is failing because some token ranges that were owned only by these 2 nodes are no longer available and we are requesting all the data.</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>$ ccm node3 cqlsh -- -e \"SELECT * FROM tlp_lab.test_backup_restore;\"\n&lt;stdin&gt;:1:NoHostAvailable:\n</pre></div></div>\n<p>In this example, these two nodes are now considered completely lost and there is no way to get the data back. What we have are the backups we made, just in time.</p>\n<h3 id=\"restore-the-service-and-data-with-copypaste\">Restore the Service and Data with Copy/Paste</h3>\n<p>To restore the service and the data, we first have to create two replacement nodes, without having them joining the cluster yet. With CCM it can be done like this:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>ccm add node7 -i 127.0.0.7 -j 7700\nccm add node8 -i 127.0.0.8 -j 7800\n</pre></div></div>\n<p>Then we want to copy the data we saved from <code class=\"highlighter-rouge\">node1</code> to the new <code class=\"highlighter-rouge\">node7</code> and from the backup of <code class=\"highlighter-rouge\">node2</code> to <code class=\"highlighter-rouge\">node8</code> after cleaning any data possibly present:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>$ rm -rf /Users/alain/.ccm/6xCassandra-3-11-1/node7/data0/*\n$ rm -rf /Users/alain/.ccm/6xCassandra-3-11-1/node8/data0/*\n$ cp -rp backup/node1/* /Users/alain/.ccm/6xCassandra-3-11-1/node7/data0/\n$ cp -rp backup/node2/* /Users/alain/.ccm/6xCassandra-3-11-1/node8/data0/\n</pre></div></div>\n<p>Note: If some data or commit logs are already present, it could conflict with data we want to restore and even in worst case mess up the ownership as the commit logs files would be replayed, possibly on the system table as well. Always start in clean environment then restore old files.</p>\n<p>All the data from <code class=\"highlighter-rouge\">node1</code> is now in <code class=\"highlighter-rouge\">node7</code>, including the schema and information about the cluster:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>$ cd /Users/alain/.ccm/6xCassandra-3-11-1/node7/data0\n$ ll -d */*/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/IndexInfo-9f5c6374d48532299a0a5094af9ad1e3/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/available_ranges-c539fcabd65a31d18133d25605643ee3/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/batches-919a4bc57a333573b03e13fc3f68b465/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/batchlog-0290003c977e397cac3efdfdc01d626b/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/built_views-4b3c50a9ea873d7691016dbc9c38494a/\ndrwxr-xr-x  35 alain  staff  1120 Feb  2 10:16 system/compaction_history-b4dbb7b4dc493fb5b3bfce6e434832ca/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/hints-2666e20573ef38b390fefecf96e8f0c7/\ndrwxr-xr-x  19 alain  staff   608 Feb  2 10:12 system/local-7ad54392bcdd35a684174e047860b377/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/paxos-b7b7f0c2fd0a34108c053ef614bb7c2d/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/peer_events-59dfeaea8db2334191ef109974d81484/\ndrwxr-xr-x  27 alain  staff   864 Feb  2 10:16 system/peers-37f71aca7dc2383ba70672528af04d4f/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/prepared_statements-18a9c2576a0c3841ba718cd529849fef/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/range_xfers-55d764384e553f8b9f6e676d4af3976d/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/schema_aggregates-a5fc57fc9d6c3bfda3fc01ad54686fea/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/schema_columns-296e9c049bec3085827dc17d3df2122a/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/schema_functions-d1b675fe2b503ca48e49c0f81989dcad/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/schema_triggers-0359bc7171233ee19a4ab9dfb11fc125/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/schema_usertypes-3aa752254f82350b8d5c430fa221fa0a/\ndrwxr-xr-x  27 alain  staff   864 Feb  2 10:16 system/size_estimates-618f817b005f3678b8a453f3930b8e86/\ndrwxr-xr-x  35 alain  staff  1120 Feb  2 10:16 system/sstable_activity-5a1ff267ace03f128563cfae6103c65e/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/transferred_ranges-6cad20f7d4f53af2b6e20da33c6c1f83/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/views_builds_in_progress-b7f2c10878cd3c809cd5d609b2bd149c/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system_auth/resource_role_permissons_index-5f2fbdad91f13946bd25d5da3a5c35ec/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system_auth/role_members-0ecdaa87f8fb3e6088d174fb36fe5c0d/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system_auth/role_permissions-3afbe79f219431a7add7f5ab90d8ec9c/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system_auth/roles-5bc52802de2535edaeab188eecebb090/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system_distributed/view_build_status-5582b59f8e4e35e1b9133acada51eb04/\ndrwxr-xr-x  27 alain  staff   864 Feb  2 10:16 system_schema/aggregates-924c55872e3a345bb10c12f37c1ba895/\ndrwxr-xr-x  19 alain  staff   608 Feb  2 10:16 system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/\ndrwxr-xr-x  27 alain  staff   864 Feb  2 10:16 system_schema/dropped_columns-5e7583b5f3f43af19a39b7e1d6f5f11f/\ndrwxr-xr-x  27 alain  staff   864 Feb  2 10:16 system_schema/functions-96489b7980be3e14a70166a0b9159450/\ndrwxr-xr-x  27 alain  staff   864 Feb  2 10:16 system_schema/indexes-0feb57ac311f382fba6d9024d305702f/\ndrwxr-xr-x  35 alain  staff  1120 Feb  2 10:16 system_schema/keyspaces-abac5682dea631c5b535b3d6cffd0fb6/\ndrwxr-xr-x  19 alain  staff   608 Feb  2 10:16 system_schema/tables-afddfb9dbc1e30688056eed6c302ba09/\ndrwxr-xr-x  27 alain  staff   864 Feb  2 10:16 system_schema/triggers-4df70b666b05325195a132b54005fd48/\ndrwxr-xr-x  27 alain  staff   864 Feb  2 10:16 system_schema/types-5a8b1ca866023f77a0459273d308917a/\ndrwxr-xr-x  27 alain  staff   864 Feb  2 10:16 system_schema/views-9786ac1cdd583201a7cdad556410c985/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system_traces/events-8826e8e9e16a372887533bc1fc713c25/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system_traces/sessions-c5e99f1686773914b17e960613512345/\ndrwxr-xr-x  11 alain  staff   352 Jan 31 18:07 tlp_lab/test_backup_restore-9e8e3ce006b111e89daea3c19988ea88/\n</pre></div></div>\n<p>At this point we can turn <code class=\"highlighter-rouge\">node7</code> up by simply starting Cassandra service, normally. As I am using CCM:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>$ ccm node7 start\n</pre></div></div>\n<p>To reach this state, where <code class=\"highlighter-rouge\">node1</code> with ip <code class=\"highlighter-rouge\">127.0.0.1</code> have been replaced by <code class=\"highlighter-rouge\">node7</code> with ip <code class=\"highlighter-rouge\">127.0.0.7</code>. Note that <code class=\"highlighter-rouge\">node7</code> is now using the old <code class=\"highlighter-rouge\">node1</code> <code class=\"highlighter-rouge\">Host ID</code>: b6497c83-0e85-425e-a739-506dd882b013</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>$ ccm node3 nodetool status\nDatacenter: datacenter1\n=======================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address    Load       Tokens       Owns (effective)  Host ID                               Rack\nDN  127.0.0.2  149.68 KiB  1            33.3%             e4919f5a-3b89-4fd1-a163-aaae99aa5cbd  rack1\nUN  127.0.0.3  160.84 KiB  1            33.3%             c6e8e8c4-25b1-444a-ac50-ff00505fbaf7  rack1\nUN  127.0.0.4  151.68 KiB  1            33.3%             23bdc0df-1ba5-4bc7-be41-01436fa23925  rack1\nUN  127.0.0.5  161.16 KiB  1            33.3%             fb438687-8c3e-4ad7-b83d-275948f4241f  rack1\nUN  127.0.0.6  166.05 KiB  1            33.3%             63777a4b-af44-4fc2-baff-74c585d4a217  rack1\nUN  127.0.0.7  202.51 KiB  1            33.3%             b6497c83-0e85-425e-a739-506dd882b013  rack1\n</pre></div></div>\n<p>At this stage we can already access the data fully due to our configuration allowing a tolerance of one node being down:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>$ ccm node3 cqlsh -- -e \"SELECT * FROM tlp_lab.test_backup_restore;\"\n id | column1\n----+---------\n  4 |     100\n  4 |     200\n  4 |     300\n  3 |     100\n  3 |     200\n  3 |     300\n  5 |     100\n  5 |     200\n  5 |     300\n  2 |     100\n  2 |     200\n  2 |     300\n  1 |     100\n  1 |     200\n  1 |     300\n(15 rows)\n</pre></div></div>\n<p>Finally, after bringing <code class=\"highlighter-rouge\">node8</code> online, we have a fully operational cluster again. We can achieve that with the exact same steps using data from <code class=\"highlighter-rouge\">node2</code> this time.</p>\n<p><strong>Note:</strong> It is important to bring these new nodes up with the same configuration as the node that went down, except when the node IP is used of course, like in <code class=\"highlighter-rouge\">listen_address</code> and possibly in <code class=\"highlighter-rouge\">rpc_address</code>. Usually, having a management system such as <a href=\"https://www.chef.io/chef/\">Chef</a>, <a href=\"https://www.ansible.com/\">Ansible</a>, <a href=\"http://saltstack.com/\">Salt</a>, <a href=\"http://puppet.com\">Puppet</a> or using containers will make adding nodes very straightforward. In the worst case, Apache Cassandra will probably just not start.</p>\n<p>This process can be repeated with all the nodes of the cluster if the entire cluster goes down and a new replacement cluster is to be built.</p>\n<p>This solution is slow, expensive, hard to set up and error-prone if done manually. Yet it works and is quite robust if performed carefully (or even better, automatically).</p>\n<h2 id=\"the-copypaste-approach-on-aws-ec2-with-ebs-volumes\">The ‘Copy/Paste’ Approach on AWS EC2 with EBS Volumes</h2>\n<p>As demonstrated above, the basic ‘copy/paste’ option can be made to work. However, in an emergency situation when an entire cluster is down, the process could be difficult to manage and terribly slow for big datasets.</p>\n<p>Some of the AWS features can take this basic backup and restore option to the next level. I have no interest in advertising for Amazon services, and even personally believe that not locking oneself with a provider or vendor is a great idea. However, in case you are already using AWS and EBS volumes, this solution might be very convenient and not really commit you much more than you already have with AWS. The set of tools offered by AWS for backups on EC2 with EBS volume storage are worth a look for those of you using this environment.</p>\n<p>Instead of copying the the data folder out of the node as we saw earlier, AWS offers to snapshot the EBS volumes. The option is available through the console or API and makes asynchronous and incremental snapshots, transferred to S3 under the hood.</p>\n<p>Because the snapshots are incremental, we can make frequent backups without technical issues or any substantial extra cost.</p>\n<p>The procedure itself to make a full backup is to <code class=\"highlighter-rouge\">snapshot</code> the EBS volume(s) used for Apache Cassandra data storage. To make it simple, a few clicks or lines of code will allow a full backup. It is way more convenient and performant to use the API and make a small script that request all the snapshots at once, even more so when the cluster contains a large number of nodes. Using this functionality results in a more consistent dataset and allows for the automation of backups according to a backup schedule and policy.</p>\n<p><img src=\"http://thelastpickle.com/images/backup-restore-aws/create_snapshot.png\" alt=\"Create Snapshot\" /></p>\n<p>That’s it. At this point we have a backup, in a distant and redundant system. Truth is it takes ‘some time’ to make the backup, but Amazon handles it asynchronously and incrementally. We will observe impacts on performance carefully, specially for the first snapshot. Subsequent snapshot will be incremental, thus probably less impacting.</p>\n<p>While a snapshot of all the EBS volumes attached to nodes in the cluster can be taken simultaneously, be sure that only a single snapshot is run against each EBS volume at a time to prevent harming the EBS volume performances.</p>\n<p>It is a good idea to tag the cluster name, data center, Availability Zone (AZ), and IP address the disk belongs to in the snapshot metadata (name / description / …). This is to help identify the snapshot needed when a partial failure occurs, involving just part of the cluster(s).</p>\n<p>Here is the description of the snapshot feature from AWS: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html</p>\n<h3 id=\"backup-policy-and-automated-snapshots\">Backup policy and automated snapshots</h3>\n<p>Once we have the script to take a snapshot, it is really straightforward to build a script responsible for maintaining a snapshot policy such as:</p>\n<ul><li>Take / keep a snapshot every 30 min for the latest 3 hours, and</li>\n  <li>Keep a snapshot every 6 hours for the last day, delete other snapshots, and</li>\n  <li>Keep a snapshot every day for the last month, delete other snapshots.</li>\n</ul><p>A script in combination with a scheduler to call the script should be enough to have backups in place. Most probably the script will be called at a frequency determined by the lower interval between 2 backup. Here we would call the script every 30 min or less with the example above.</p>\n<p>This script will contain the rules to use to make backups and calls to AWS API to take and delete snapshots depending on the date of the snapshot and the current date, or anything else you would like to use to trigger backups.</p>\n<p>Using AWS Lambda service to execute the backups is possible and should be efficient. The backups being asynchronous, would mean the script should run completely in a matter of a seconds and the Lambda service cost is based on the execution time, thus it could be a good fit.</p>\n<p>To run the script on a regular basis, AWS CloudWatch Events provide events based on time. It is possible to schedule the call to the script in charge of the backup policy this way.</p>\n<p>About Lambda in AWS: https://docs.aws.amazon.com/lambda/latest/dg/welcome.html\nAbout AWS CloudWatch Events: https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</p>\n<h3 id=\"restore-procedure-aws\">Restore procedure (AWS)</h3>\n<p>The restore procedure can be manual, which can be enough to handle a small outage involving a few nodes, or if there is no real time constraints for the RTO. For big clusters and in general, using the API and a script to restore the latest (or a specific) backup will make this process more reliable and scalable than using the AWS console. Often, using the console is nice to test things once, but unsuitable for large scale operations. The AWS API is far more powerful and lends itself well to automated tasks. For the example we will be using the console. Independently of the tool used, the process will always be the same to restore a node.</p>\n<ul><li>\n    <p>Reusing existing instances…</p>\n    <p>If the instance is still accessible but the data is corrupted or unaccessible, we can reuse the same nodes. The node is already configured as required and reusing the instance is practical. In this case:</p>\n    <ol><li>Locate the snapshot to use.</li>\n    </ol><div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>It has to be a snapshot taken from the node to be replaced. If we are replacing or re-creating an entire rack, be sure to use the snapshots for the AZ the rack is in.\n</pre></div></div>\n    <ol><li>\n        <p>Stop Cassandra on the node to restore (if the node is not already down)</p>\n        <div class=\"highlighter-rouge\"><div class=\"highlight\"><pre> nodetool drain &amp;&amp; sudo service cassandra stop\n</pre></div></div>\n      </li>\n      <li>\n        <p>Unmount the currently attached EBS volume at the operating system level, supposing the EBS Volume is mounted as <code class=\"highlighter-rouge\">/var/lib/cassandra/data/</code>, from a bash console, run:</p>\n        <div class=\"highlighter-rouge\"><div class=\"highlight\"><pre> umount /var/lib/cassandra/data/\n</pre></div></div>\n      </li>\n      <li>\n        <p>Detach and terminate the old EBS volume from the AWS console or API.</p>\n        <p><img src=\"http://thelastpickle.com/images/backup-restore-aws/detach_volume.png\" alt=\"Detach Volume\" /></p>\n      </li>\n    </ol></li>\n  <li>\n    <p>…<strong>OR</strong> using new instances…:</p>\n    <ol><li>Create a new instance in the right AZ to replace all the nodes that need to be. The instances can be created without EBS attached to them, but all the other options must be exactly identical to a new node that you would add normally to the existing cluster.</li>\n    </ol></li>\n  <li>\n    <p><strong>Then</strong> restore:</p>\n    <ol><li>\n        <p>For each node that is down, create a new volume from the most recent associated snapshot taken. It is here that the snapshot tags are critical for ensuring the correct snapshot is restored for the node..</p>\n        <p><img src=\"http://thelastpickle.com/images/backup-restore-aws/restore_volume.png\" alt=\"Restore Volume\" /><img src=\"http://thelastpickle.com/images/backup-restore-aws/pick-az-restore.png\" alt=\"Pick the right AZ and disk size\" /></p>\n      </li>\n      <li>\n        <p>Attach the newly created volume to the instance; whether it was a newly created or the original instance..</p>\n        <p><img src=\"http://thelastpickle.com/images/backup-restore-aws/attach_volume.png\" alt=\"Attach the volume to the instance\" /></p>\n        <p><strong>Note:</strong> If the instance started while the EBS volume was not yet attached, be sure to remove any newly created <em>data</em>, <em>commitlog</em>, and <em>saved_caches</em> directories that would have been created on the node’s local storage. Failure to do so could potentially mess up the token ownership as commit logs would be replayed when the node start, thus updating the system data with new token ownership from the previous ‘failed’ start. This breaks consistency guarantees and could eventually lead to a data loss.</p>\n      </li>\n      <li>\n        <p>Mount the new volume from the instance operating system perspective, for example:</p>\n        <div class=\"highlighter-rouge\"><div class=\"highlight\"><pre> mount /dev/xvdq1 /var/lib/cassandra/data/\n</pre></div></div>\n      </li>\n      <li>\n        <p>Start Apache Cassandra:</p>\n        <div class=\"highlighter-rouge\"><div class=\"highlight\"><pre> service cassandra start &amp;&amp; tail -100f /var/log/cassandra/system.log\n</pre></div></div>\n      </li>\n      <li>\n        <p>Finally run a repair on all the nodes and for all the tables where consistency is a concern.</p>\n      </li>\n    </ol></li>\n</ul><p>The node should join the cluster, and other nodes should detect the new IP replacing the old one. Repeat this procedure for all the nodes that need to be brought back (up to 100% of the nodes). Even with big datasets, the cluster should go back to normal <em>fairly</em> quickly and the service should go back online as soon as enough servers have been restored, depending on the configuration.</p>\n<p>It is hard to be precise here as the speed will depend on the use of the API versus the use of the console and then the volume creation will depend on the data size.</p>\n<p>Yet it is optimized by AWS and is definitely way faster than a standard backup transfer. Again, given the number of manual steps and considering how fast a cluster restore should be done in a critical situation, scripting this procedure using the API instead of the console is probably better in most (all?) cases.</p>\n<h3 id=\"summing-up\">Summing up</h3>\n<p>The AWS EBS backup solution comes with some drawbacks:</p>\n<ul><li>The topology used for the restore cluster has to be identical to that of the original cluster. Same number of node, same cluster name, same data center name, same number of nodes per rack, vnodes configuration. The safest approach is not to change anything that does need to be changed.</li>\n  <li>It is somewhat expensive. Enabling snapshots feature for EBS comes at a price.</li>\n  <li>You have to code the snapshot retention policy yourself. Yet it is basic scripting in the language you prefer using AWS API; nothing new for most of AWS users.</li>\n  <li>It is an advanced operation, that bypasses some Apache Cassandra’s safe guards around consistency. An incorrect operation can lead to a data loss. It is important to test the process then probably automate it.</li>\n</ul><p>On the bright side, with this operation we make important improvements on our backup / restore objectives:</p>\n<ul><li>The AWS Snapshots feature provides an immediate snapshot and incremental transfer of the data asynchronously, and because of this it is possible to achieve a very good Recovery Point Objective. It can be <strong>seconds</strong> if needed and as long as machines can handle it.</li>\n  <li>The Recovery Time Objective for this process is quick and consistent. That is, the time required to restore an EBS volume from a snapshot  is <em>fixed</em>. As soon as the EBS volume is mounted and the instance joins the cluster, data is is available. You can expect degraded performances as data will be loading in the background. However this is probably bearable, given the cluster was recovered from complete data loss in a short amount of time. From <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-restoring-volume.html\">AWS documentation</a>:</li>\n</ul><blockquote>\n  <p>New volumes created from existing EBS snapshots load lazily in the background. This means that after a volume is created from a snapshot, there is no need to wait for all of the data to transfer from Amazon S3 to your EBS volume before your attached instance can start accessing the volume and all its data. If your instance accesses data that hasn’t yet been loaded, the volume immediately downloads the requested data from Amazon S3, and continues loading the rest of the data in the background.</p>\n</blockquote>\n<ul><li>AWS service comes at a cost, but:\n    <ul><li>The incremental transfer of the data during the backup phase can save a lot of money compared to a full backup.</li>\n      <li>The cost to setup the backup service is reduced if AWS Lambda, Events scheduler, Snapshots and API are configured to do all of the work.</li>\n      <li>Restore comes at a negligible cost and is very efficient</li>\n    </ul></li>\n  <li>System tables are saved alongside the data, meaning the backup store almost everything we need to restore, including  the schema, the token range owned, topology information.</li>\n</ul><h2 id=\"lets-compare\">Let’s compare!</h2>\n<p>We have been through the overview of some available backup and restore solutions for Apache Cassandra. Hereafter is a table that aims at being a quick evaluation of them, a visual sum up of what is said herein.</p>\n<p>To be fair,  a backup was considered completed when the data was moved off the node to another location. For example <code class=\"highlighter-rouge\">snapshot</code> or <code class=\"highlighter-rouge\">incremental backups</code> solutions can easily have a RPO of 1 second, but the data still remains on the volume as the original data. Hence, if the machine is unreachable, the backup is useless. This reduces the backup efficiency to a smaller scope including for example when recovering from human errors. That is why <code class=\"highlighter-rouge\">snapshots</code> have a bad RPO in the table below, we are considering all the data has to be extracted to some external storage. <code class=\"highlighter-rouge\">Incremental backups</code> perform a bit better as only increments (ie. new data) are extracted.</p>\n<p>We did not compare the commercial solutions. Those evolve quickly and all have a support that will answer any question better than I would. Thus I invite you to contact companies providing this service directly.</p>\n<p><img src=\"http://thelastpickle.com/images/backup-restore-aws/Compare_backup_strategies.png\" alt=\"Compare Backup Strategies\" /></p>\n<p><strong>Note:</strong> We saw how an overall poorly performing solution such as ‘copy/paste’ can turn out to be one of the best option in a specific environment. It is reasonable for a process that performs poorly in the above table to be a reasonable solution that is suitable for your requirements. Feel free to share your experience with us in the comments here or share with the community in the <a href=\"http://cassandra.apache.org/community/\">Apache Cassandra User mailing list</a>.</p>",
        "created_at": "2018-04-03T23:44:39+0000",
        "updated_at": "2018-09-24T19:28:38+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 37,
        "domain_name": "thelastpickle.com",
        "preview_picture": "http://thelastpickle.com/android-chrome-192x192.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9480"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 22,
            "label": "open.source",
            "slug": "open-source"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 253,
            "label": "analytics",
            "slug": "analytics"
          },
          {
            "id": 955,
            "label": "blockchain",
            "slug": "blockchain"
          }
        ],
        "is_public": false,
        "id": 9437,
        "uid": null,
        "title": "GraphSense - Blockchain Analytics",
        "url": "http://graphsense.info/",
        "content": "<p>GraphSense is an open source platform for analyzing cryptocurrencies\nsuch as <a href=\"https://bitcoin.org/en/\">Bitcoin</a>.</p><ul><li>\n    <p><strong>Address Clustering</strong>: partition the set of addresses observed in a cryptocurrency\necosystem into maximal subsets (clusters) that are likely to be controlled by the same\nreal-world entity.</p>\n  </li>\n  <li>\n    <p><strong>Micro- and Macroscopic Analysis</strong>: inspect main cryptocurrency entities\n(block, transaction, address) and compute summary statistics over the entire\nblockchain.</p>\n  </li>\n  <li>\n    <p><strong>Network Perspective</strong>: apply a network-centric perspective and traverse currency\nflows between addresses and clusters.</p>\n  </li>\n  <li>\n    <p><strong>Horizontal Scalability</strong>: cryptocurrency blockchains <a href=\"https://blockchain.info/charts/blocks-size?timespan=all\">are growing</a>\nand new currencies <a href=\"https://coinmarketcap.com/\">appear on the horizon</a>. To make GraphSense\nfuture-proof, it is built on <a href=\"https://spark.apache.org/\">Apache Spark</a> and <a href=\"http://cassandra.apache.org/\">Cassandra</a> for\nhorizontal scalability.</p>\n  </li>\n</ul><h2 id=\"technical-architecture\">Technical Architecture</h2><p>GraphSense is built on scalable and distributed cluster technology and therefore requires a number of software components. They must be setup and/or executed in the following order:</p><ul><li>\n    <p><a href=\"https://github.com/graphsense/bitcoin-client\">bitcoin-client</a>: a Docker container encapsuling the most-recent Bitcoin client version</p>\n  </li>\n  <li>\n    <p><a href=\"https://github.com/graphsense/graphsense-datafeed\">datafeed</a>: a component for ingesting raw blockchain data and exchange rates into Cassandra</p>\n  </li>\n  <li>\n    <p><a href=\"https://github.com/graphsense/graphsense-transformation\">transformation</a>: a Spark pipeline for computing statistics and network representations from raw blockchain data stored in Cassandra.</p>\n  </li>\n  <li>\n    <p><a href=\"https://github.com/graphsense/graphsense-REST\">rest-api</a>: an API for retrieving data from the underlying Cassandra store</p>\n  </li>\n  <li>\n    <p><a href=\"https://github.com/graphsense/graphsense-dashboard\">dashboard</a>: a user-interface allowing search, inspection, and traversal of cryptocurrency entities</p>\n  </li>\n</ul><h2 id=\"example\">Example</h2><p>The following example shows details about an example Bitcoin address.</p><p><img src=\"http://graphsense.info/assets/screenshot_dashboard.jpeg\" alt=\"screenshot\" /></p><h2 id=\"publications\">Publications</h2><p>Some more technical details about GraphSense are described <a href=\"http://ceur-ws.org/Vol-1695/paper20.pdf\">here</a>; please cite as:</p><div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>@inproceedings{Haslhofer:2016a,\n    title={O Bitcoin Where Art Thou? Insight into Large-Scale Transaction Graphs.},\n    author={Haslhofer, Bernhard and Karl, Roman and Filtz, Erwin},\n    booktitle={SEMANTiCS (Posters, Demos)},\n    year={2016}\n}\n</pre></div></div><p>So far, GraphSense has been used for computing statistics in the following\nscientific papers:</p><p>Filtz, E., Polleres, A., Karl, R., Haslhofer, B.:\n<strong>Evolution of the Bitcoin Address Graph - An Exploratory Longitudinal Study.</strong>\nInternational Data Science Conference (DSC 2017), Salzburg, Austria, 2017.\n<a href=\"https://aic.ai.wu.ac.at/~polleres/publications/filtz-etal-2017IDSC.pdf\">(pdf)</a></p><h2 id=\"contributors\">Contributors</h2><ul><li><a href=\"http://bernhardhaslhofer.info/\">Bernhard Haslhofer</a></li>\n  <li>Roman Karl</li>\n  <li>Mihai Bartha</li>\n  <li>Rainer Stütz</li>\n</ul>",
        "created_at": "2018-03-22T11:26:48+0000",
        "updated_at": "2018-04-16T17:42:41+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 1,
        "domain_name": "graphsense.info",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9437"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9436,
        "uid": null,
        "title": "Static columns in Cassandra and their benefits - Info Support Blog",
        "url": "https://blogs.infosupport.com/static-columns-in-cassandra-and-their-benefits/",
        "content": "<p>In my previous blogpost “<a href=\"http://blogs.infosupport.com/analysis-of-the-impact-of-new-use-cases-on-a-cassandra-data-model-and-operations-in-code/\" target=\"_blank\">Analysis of the impact of new use cases on a Cassandra data model and operations in code</a>” I looked at the risks of deploying Cassandra. In Cassandra we face the challenges of data modelling and designing the CRUD operations (create, read, update, delete), because in Cassandra it is best practice to denormalize the data models. With static columns (available from Cassandra 2.0.6) we are able to restrain some of these challenges for one-to-many relationships.</p><p>To explain the approach of using static columns we first need to understand how the models for one-to-many relationships look like in Cassandra. With that explanation I also show the operations on the data models. Next, I will explain what the static columns are exactly. I will also elaborate on the operations on these models and mention some technical limitation of static columns. Next we will take a deeper dive into the read operations for the one side of the one-to-many relation. To conclude I will summarize my findings.</p><p>In Cassandra it is best practice to denormalize the data models. We denormalize because the goal in Cassandra is to execute the read operations of the use case with one query or, more preferably, one disk I/O. To illustrate the use case, the data model and the operations I will use an example about teams and team members of Formula 1 racing teams.</p><p>In the example we have two entities; teams and team members. A single team can have multiple members and a given team member is member of only a single team. The first use case is that we should be able to create, read, update and delete teams. The second use case is that we should be able to create, read, update, and delete team members of a given team. Obviously we should also be able to read all members for a given team and read the team that a team member is member of.</p><p>Assume that for a team we wish to register: the name, the manager and the location. For a team member we wish to register: the name, the nationality and the position within the team. In Cassandra we would store all these fields in one table. In that table the team name will be the partition key and the member name will be the clustering key. The table is created with the following CQL statement.</p><pre>CREATE TABLE teammember_by_team (\n  teamname text,\n  manager text,\n  location text,\n  membername text,\n  nationality text,\n  position text,\n  PRIMARY KEY ((teamname), membername)\n);</pre><h2>Insert operation</h2><p>We can insert a row with the following CQL statement:</p><pre>INSERT INTO teammember_by_team (teamname, manager, location, membername, nationality, position)\nVALUES (‘Toro Rosso’, ‘Franz Tost’, ‘Faenza’, ‘Verstappen’, ‘Dutch’, ‘test driver’);</pre><p>This statement results in a new row in the table. When we wish to add another team member to the team we need to add values for the team’s fields, again.</p><h2>Update operation</h2><p>Updating a team member is relatively easy. For example with the following CQL statement:</p><pre>UPDATE teammember_by_team SET position = ‘driver’\nWHERE teamname = ‘Toro Rosso’ AND membername = ‘Verstappen’;</pre><p>But when we wish to update a team we need to execute the update for all team members. This is one of the risks and challenges of this model. This update operation may cost many queries, for example the following queries which result in the following table with data.</p><pre>UPDATE teammember_by_team SET manager = ‘Christian Horner’\nWHERE teamname = ‘Red Bull’ AND membername = ‘Ricciardo’;\nUPDATE teammember_by_team SET manager = ‘Christian Horner’\nWHERE teamname = ‘Red Bull’ AND membername = ‘Kvyat’;\nteamname   | membername | location      | manager          | nationality | position\n-----------+------------+---------------+------------------+-------------+----------\n  Red Bull |  Ricciardo | Milton Keynes | Christian Horner |  Australian |   driver\n  Red Bull |      Kvyat | Milton Keynes | Christian Horner |     Russian |   driver\nToro Rosso | Verstappen |        Faenza |       Franz Tost |       Dutch |   driver</pre><p>A static column is defined upon creation of the table in CQL using the keyword “static”. A normal column is stored within the clustering key, within the partition. This column has a value for every combination of partitioning key and clustering key (in our example team name and team member name). A static column resides one level higher, directly within the partition key (in our example team name).</p><p>All fields belonging to the one side of the one-to-many relation may be static columns. In our example this means that the team manager and team location can be static columns. Review the following CQL statement:</p><pre>CREATE TABLE teammember_by_team (\n  teamname text,\n  manager text static,\n  location text static,\n  membername text,\n  nationality text,\n  position text,\n  PRIMARY KEY ((teamname), membername)\n);</pre><p>Because the static columns are not stored within the clustering key, but are still associated to the partitioning key, we are able to insert team data without inserting team member data. When we read this table, we get back the team with all other field having “null”.</p><pre>INSERT INTO teammember_by_team (teamname, manager, location)\nVALUES (‘Red Bull’, ‘Christian Horner’, ‘&lt;unknown&gt;’);\nteamname  | membername | location | manager          | nationality | position\n----------+------------+----------+------------------+-------------+----------\n Red Bull |       null | &lt;unkown&gt; | Christian Horner |        null |     null</pre><p>Inserting team members is much easier using static columns, because team data no longer has to be duplicated.</p><pre>INSERT INTO teammember_by_team (teamname, membername, nationality, position)\nVALUES (‘Red Bull’, ‘Ricciardo’, ‘Australian’, ‘driver’);\nINSERT INTO teammember_by_team (teamname, membername, nationality, position)\nVALUES (‘Red Bull’, ‘Kvyat’, ‘Russian’, ‘driver’);</pre><p>When we read the table after the preceding statements, the result is as expected.</p><pre>teamname  | membername | location  | manager          | nationality | position\n----------+------------+-----------+------------------+-------------+----------\n Red Bull |  Ricciardo | &lt;unknown&gt; | Christian Horner |  Australian |   driver\n Red Bull |      Kvyat | &lt;unknown&gt; | Christian Horner |     Russian |   driver</pre><p>When we now wish to update a team we are able to do so with just one query.</p><pre>UPDATE teammember_by_team SET location = ‘Milton Keynes’\nWHERE teamname = ‘Red Bull’;\nteamname  | membername | location      | manager          | nationality | position\n----------+------------+---------------+------------------+-------------+----------\n Red Bull |  Ricciardo | Milton Keynes | Christian Horner |  Australian |   driver\n Red Bull |      Kvyat | Milton Keynes | Christian Horner |     Russian |   driver</pre><h2>Technical limitations</h2><p>There are however a few limitations to using static columns. A table can only contain static columns when the table has at least one clustering key. This actually makes sense because if the table has no clustering key then all fields are stored within the partition. Next to this limitation, static columns cannot be used on tables using the COMPACT STORAGE option.</p><p>When we wish to read fields from only the one side of the one-to-many relation this requires a different approach compared to reading fields on a table without static columns. Review the following query:</p><pre>SELECT teamname, manager, location\nFROM teammember_by_team\nWHERE teamname = ‘Red Bull’;</pre><p>Without static columns these fields may be inconsistent and we will get back two rows. With static columns these values are always consistent but still we get back two rows. We get back two rows because there are two team members. If we wish to get back one row per team then all we have to do is add the DISTINCT keyword:</p><pre>SELECT DISTINCT teamname, manager, location\nFROM teammember_by_team\nWHERE teamname = ‘Red Bull’;</pre><p>Cassandra understands that the distinct keyword requires a special treatment because all fields queried are static (besides the partition key). The data can be returned faster compared to not using static columns because in the latter case the values actually have to be compared.</p><p>Static columns provide benefits for one-to-many relations. These benefits constrain a couple of great risks as described in the previous blogpost. The insert operations become easier and more flexible at the one side on the one-to-many relation. The update performed on the entity side of the one-to-many relation is reduced from many queries to just one query. Also select queries at the single entity side of the one-to-many relation are improved.</p><h2>References</h2><p>[1] <a href=\"http://blogs.infosupport.com/analysis-of-the-impact-of-new-use-cases-on-a-cassandra-data-model-and-operations-in-code/\" target=\"_blank\">Analysis of the impact of new use cases on a Cassandra data model and operations in code</a></p><p>[2] <a href=\"http://www.datastax.com/dev/blog/cql-in-2-0-6\" target=\"_blank\">http://www.datastax.com/dev/blog/cql-in-2-0-6</a></p><p>[3] <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-6561\" target=\"_blank\">https://issues.apache.org/jira/browse/CASSANDRA-6561</a></p><p>[4] <a href=\"http://www.datastax.com/documentation/cql/3.1/cql/cql_reference/refStaticCol.html\" target=\"_blank\">http://www.datastax.com/documentation/cql/3.1/cql/cql_reference/refStaticCol.html</a></p><p>[5] <a href=\"http://www.datastax.com/documentation/cql/3.1/cql/cql_using/use-batch-static.html\" target=\"_blank\">http://www.datastax.com/documentation/cql/3.1/cql/cql_using/use-batch-static.html</a></p><p>[6] <a href=\"http://cassandra.apache.org/doc/cql3/CQL.html#createTablepartitionClustering\" target=\"_blank\">http://cassandra.apache.org/doc/cql3/CQL.html#createTablepartitionClustering</a></p><p>[7] <a href=\"https://books.google.nl/books?id=n1nTBgAAQBAJ\" target=\"_blank\">https://books.google.nl/books?id=n1nTBgAAQBAJ</a></p>",
        "created_at": "2018-03-21T17:13:06+0000",
        "updated_at": "2018-03-21T20:53:53+0000",
        "published_at": "2015-03-17T00:00:00+0000",
        "published_by": [
          "Maarten van Duren"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "nl",
        "reading_time": 6,
        "domain_name": "blogs.infosupport.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9436"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9432,
        "uid": null,
        "title": "Sysdig | Troubleshooting Cassandra column selection to boost database performance",
        "url": "https://sysdig.com/blog/column-selection-effects-query-performance/",
        "content": "<img width=\"1024\" height=\"768\" src=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/monitoring-cassandra-1024x768.jpg\" class=\"alignleft img-responsive wp-post-image\" alt=\"\" srcset=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/monitoring-cassandra-1024x768.jpg 1024w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/monitoring-cassandra-300x225.jpg 300w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/monitoring-cassandra-768x576.jpg 768w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/monitoring-cassandra-575x431.jpg 575w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/monitoring-cassandra-175x131.jpg 175w\" /><h3>Introduction</h3><i>This post was written by one of Sysdig’s engineers Gianluca Borello and was originally featured on <a href=\"http://www.planetcassandra.org/blog/a-tale-of-troubleshooting-database-performance-with-cassandra-and-sysdig/\">Planet Cassandra</a>.As far as databases go, I’m a huge fan of Cassandra: it’s an incredibly powerful and flexible database that can ingest a massive amount of data while scaling across an arbitrary number of nodes. For these reasons, my team uses it very often for our internal applications.But as with any piece of software, Cassandra has its own quirks and habits that you need to understand in order to effectively develop with it. In this article, I’ll show you a Cassandra performance issue that we recently dealt with, and I want to cover how we spotted the problem, what kind of troubleshooting we did to better understand it, and how we eventually solved it.The problemOne of our internal applications requires storing, and later processing, several thousands streams, where each stream consists of about a dozen binary blobs arriving periodically. Since we have many streams, and each blob can be fairly big (in the range of 10KB), we decided to use Cassandra 2.1 (a very stable version at the time of writing) and this very simple table:CREATE TABLE streams ( <br />&#13;\n    stream_id int,&#13;\n    timestamp int, &#13;\n    column0 blob, &#13;\n    column1 blob,&#13;\n    column2 blob, &#13;\n    ... &#13;\n    column10 blob,&#13;\n    primary key (stream_id, timestamp));In this data model, all the blobs (10 in the above example) for a specific timestamp are stored in the same row, as separate columns. This allowed us to write a very simple application code, consisting essentially of a single write per stream during every period. Our typical read use case requires only one or two blobs at any given time. With this data model we have the flexibility of querying an arbitrary portion of the data with a single query, like this:SELECT columnX from streams where stream_id=STREAMFor a while this schema worked well for us, and the response time we experienced for the database has been very good.Recently we noticed deteriorated performance when querying streams containing particularly large blobs. Intuitively this would seem very reasonable since the data to be processed is larger, but there was something else that felt strange: despite the blobs being bigger in average, the specific ones we were retrieving in our queries were always roughly the same size as before.In other words, it seemed as if Cassandra was always processing all 10 columns (including the large ones) despite us just asking for a particular, small column, thus causing degraded response times. This hypothesis seemed hard to believe at first, because Cassandra stores every single column separately, and there’s heavy indexing that allows you to efficiently lookup specific columns.To validate our hypothesis, we wrote a separate test: in a table of N columns like ours, asking one single column should always take almost the same time regardless of the number and size of the other columns. With a little <a href=\"http://(https://github.com/gianlucaborello/cassandra-benchmarks/blob/master/cassandra_benchmark_1.py\">script</a>, we got these results:$ python ./cassandra_benchmark_1.py&#13;\nResponse time for querying a single column on a large table (column size 100 KB):&#13;\n10 columns: 185 ms&#13;\n20 columns: 400 ms&#13;\n30 columns: 613 ms&#13;\n40 columns: 668 ms&#13;\n50 columns: 800 ms&#13;\n60 columns: 1013 ms&#13;\n70 columns: 1205 ms&#13;\n80 columns: 1376 ms&#13;\n90 columns: 1604 ms&#13;\n100 columns: 1681 msWe couldn’t have been more wrong! The tests proved the exact opposite of our assumption, and in fact the response time seemed to take a time directly proportional to the number of columns in the table, even if the query asked for just one of them!&#13;\n&#13;\nDigging into CassandraIn order to understand the problem better, I used <a href=\"http://www.sysdig.org/\">sysdig</a>, an open source troubleshooting tool. If you’re not familiar with sysdig, it has the ability to capture system state and activity from a running Linux instance, and then save, filter and analyze it. Think of sysdig as strace + tcpdump + htop + iftop + lsof in one.Back to our story: I took a sysdig trace file while executing the same query of the previous test on a table with 100 columns:SELECT column7 from streams where stream_id=1This query returns a very small amount of data compared to the whole data set (around 10 MB), as sysdig can easily tell me by looking at the network activity generated by the database:$ sysdig -r trace.scap -c topprocs_net &#13;\nBytes           Process            PID&#13;\n-------------------------------------------------------------------------&#13;\n9.82M            java             34323Despite this, the query takes almost 4 seconds to run, which is way more than what we want to wait in a similar scenario. Let’s take a look at the Cassandra file I/O activity while serving this single query:&#13;\n&#13;\n$ sysdig -r trace.scap -c topfiles_bytes&#13;\nBytes              Filename &#13;\n-------------------------------------------------------------------------------- &#13;\n971.04M &#13;\n/var/lib/cassandra/data/benchmarks/test-23182450d5c011e5acecb7882d261790/benchmarks-test-ka-130-Data.db &#13;\n538.80KB &#13;\n/var/lib/cassandra/data/benchmarks/test-23182450d5c011e5acecb7882d261790/benchmarks-test-ka-130-Index.db&#13;\n…Wow, Cassandra seems to have read almost 1 GB from the file storing the data for my table, pretty much the whole size of the table:$ du -hs /var/lib/cassandra/data/benchmarks/test-23182450d5c011e5acecb7882d261790/*&#13;\n...&#13;\n972M    /var/lib/cassandra/data/benchmarks/test-23182450d5c011e5acecb7882d261790/benchmarks-test-ka-130-Data.db&#13;\n…Which means that Cassandra essentially read the entire file, and this probably explains why the response time depends on the total number of columns of the table.To take a deeper look, I used the spectrogram within csysdig on the I/O events over the duration of the trace file, so that we can visualize Cassandra’s latency and frequency of I/O operations:$ csysdig -d 100 -r trace.scap -v spectro_file<a href=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/cassandrablog1.png\"><img alt=\"csysdig spectrogram\" src=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/cassandrablog1.png\" /></a>&#13;\nIf you’re not familiar with a spectrogram, here’s the high level:&#13;\nIt captures every system call (e.g. open, close, read, write, socket…) and it measures the call’s latency&#13;\n\tEvery 100 ms it organizes the calls into buckets&#13;\n\tThe spectrogram uses color to indicate how many calls are in each bucket&#13;\nBlack means no calls during the last sample&#13;\n\tShades of green mean 0 to 100 calls&#13;\n\tShades of yellow mean hundreds of calls&#13;\n\tShades of red mean thousands of calls or more&#13;\nAs a consequence, the left side of the diagram tends to show stuff that is fast, while the right side shows stuff that is slow.What this image clearly tells us is that there seems to be a constant amount of I/O activity across the whole duration of the query, so, as predicted, accessing that 1 GB is likely what is negatively impacting the response time. Also, the latency of the I/O activity seems to be concentrated across two ranges, one very short (around 100 ns – 1 us) and one much more significant (around 10 us – 1 ms).Let’s zoom into the band on the right, by selecting the area that I’m interested in exploring:&#13;\n&#13;\n<a href=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/cassandrablog2.png\"><img alt=\"csysdig spectrogram\" src=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/cassandrablog2.png\" /></a>&#13;\nThis selection will show the list of system events that match the time range and&#13;\nlatency spectrum:<a href=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/cassandrablog3.png\"><img alt=\"csysdig spectrogram\" src=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/cassandrablog3.png\" /></a>&#13;\nRead operations on the table file, tons of them. Indeed, this is essentially a full scan of the table done by reading it in chunks of 64 KB, and we can see how all those hundreds of microseconds blocked waiting for I/O add up in the final response time.Solution: Cassandra Schema RefactorWe were pretty puzzled after discovering this behavior. As I mentioned before, rows and columns in Cassandra are heavily indexed, so we knew that there was no technical limitation for the query to not be served efficiently by reading exactly those 10 MB from disk instead of the full 1 GB.Doing some research, we were able to find an issue (https://issues.apache.org/jira/browse/CASSANDRA-6586) describing this problem, and we understood that the behavior was meant to be that way in order to respect some semantics of the CQL query language adopted by Cassandra (omitted here for brevity, but clearly described in the issue), and the developers agreed this behavior could cause a significant performance penalty in cases like ours. While the issue is planned to be addressed in future versions of Cassandra, that left us with a bug in a production application that we had to somehow solve.We opted for a workaround: since Cassandra will always read all the columns of a CQL row regardless of which ones are actually asked in a query, we decided to refactor our schema by shrinking the size of each row, and instead of putting all the blobs in the same row, we split them across multiple ones, like this:CREATE TABLE streams (<br />&#13;\n    stream_id int,&#13;\n    column_no int,&#13;\n    timestamp int,&#13;\n    column blob,&#13;\n    primary key (stream_id, column_no, timestamp));With this new schema all our rows are much smaller while still allowing us to efficiently query a single portion of the blob, with a query such as:SELECT * from streams where stream_id=1 and column_no=7This approach turned out to be quite effective: the query above that took 4 seconds with the old data model in the extreme test case, now took just around 100 ms on the same data set! It’s also interesting to analyze the new scenario with sysdig, and check Cassandra’s I/O while serving the request:$ sysdig -r trace.scap -c topfiles_bytes&#13;\nBytes             Filename&#13;\n--------------------------------------------------------------------------------&#13;\n9.85M             /var/lib/cassandra/data/benchmarks/test-55d49260d5cb11e5acecb7882d261790/benchmarks-test-ka-16-Data.db&#13;\n…Just 10 MB, exactly the same size of the expected response. We can also use sysdig to answer the question: how did Cassandra know how to efficiently read the exact amount of data in a jungle of more than 1 GB? We can of course look at the system events done by the database process on the file:$ sysdig -r trace.scap fd.filename=benchmarks-test-ka-16-Data.db&#13;\n11285 15:13:40.896875243 1 java (34482) &lt; open fd=59(/var/lib/cassandra/data/benchmarks/test-55d49260d5cb11e5acecb7882d261790/benchmarks-test-ka-16-Data.db) name=/var/lib/cassandra/data/benchmarks/test-55d49260d5cb11e5acecb7882d261790/benchmarks-test-ka-16-Data.db flags=1(O_RDONLY) mode=0&#13;\n11295 15:13:40.896926004 1 java (34482) &gt; lseek fd=59(/var/lib/cassandra/data/benchmarks/test-55d49260d5cb11e5acecb7882d261790/benchmarks-test-ka-16-Data.db) offset=71986679 whence=0(SEEK_SET)&#13;\n11296 15:13:40.896926200 1 java (34482) &lt; lseek res=71986679Here we see Cassandra opening the table file, but notice how immediately there’s a lseek operation that essentially skips in one single operation 70 MB of data, by setting the offset to the file descriptor with SEEK_SET to 71986679. This is essentially how typical “file indexing” works when observed from the system call point of view: Cassandra heavily relies on data structures to index the various contents of the table, so that it can move fast and efficiently to arbitrary and meaningful locations. In this case, the index contained the information that the columns with “stream_id=1” and “column_no=7” started at offset 71986679.11400 15:13:40.898199496 1 java (34482) &gt; read fd=59(/var/lib/cassandra/data/benchmarks/test-55d49260d5cb11e5acecb7882d261790/benchmarks-test-ka-16-Data.db) size=65798&#13;\n11477 15:13:40.899058641 1 java (34482) &lt; read res=65798 data=................................................................................Right after jumping to the correct position of the file, we see normal sequential reads of 64 KB, in order to bring all the data into memory and process them. This loop of jumping to the right position and reading from it continues until all data (10 MB) is fully read.It’s also quite interesting to see how this second case looks like in a spectrogram:<a href=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/cassandrablog4.png\"><img alt=\"csysdig spectrogram\" src=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/cassandrablog4.png\" /></a>&#13;\nThe spectrum of the I/O operations is pretty much the same as before and latencies are concentrated around the previous values, but notice how the height of the chart is completely different. This is because each horizontal line represents 100 ms of time, and since the whole query took about 100ms, the whole activity can be rendered in a much shorter chart. In other words, the height of the chart is directly proportional to the response time of Cassandra, so short means faster.ConclusionsThis story has two important messages into it, focused on monitoring and troubleshooting:Monitoring: Monitoring all the tiers of your infrastructure is very important. In this case, simply correlating the amount of data requested from Cassandra with the amount of I/O activity actually generated by the database helped us finding the cause of the problem, which we noticed in the first place by once again monitoring the response time of the queries.&#13;\n\tTroubleshooting: with <a href=\"http://sysdig.org\">sysdig</a>, I’ve been able to dig into the specific behavior of Cassandra (e.g. confirming that the indexing was actually used by observing the lseek() activity) without even having to read its code and understand its internals, which for sure would have taken me more time. This concept is very powerful: if you already have some expectations of how a specific application is supposed to behave, observing its activity from the system call point of view can in many cases be an extremely effective way to do troubleshooting without spending a lot of time understanding less relevant details.&#13;\n</i>",
        "created_at": "2018-03-20T14:27:04+0000",
        "updated_at": "2018-04-16T17:43:12+0000",
        "published_at": "2016-04-19T23:33:45+0000",
        "published_by": [
          "Gianluca Borello"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 11,
        "domain_name": "sysdig.com",
        "preview_picture": "https://sysdig.com/wp-content/uploads/2016/04/monitoring-cassandra.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9432"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 16,
            "label": "starter.template",
            "slug": "starter.template"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9422,
        "uid": null,
        "title": "A Reference Application for Apache Cassandra and DataStax Enterprise",
        "url": "https://killrvideo.github.io/",
        "content": "<p>\n            Apache Cassandra's peer-to-peer architecture makes it a great choice when you need scalability and high \n            availability. It's also a lot different from the RDBMS systems you might be used to. You can get free\n            self-paced training for Cassandra at DataStax Academy.\n          </p>",
        "created_at": "2018-03-14T22:28:08+0000",
        "updated_at": "2018-03-14T22:29:14+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 0,
        "domain_name": "killrvideo.github.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9422"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 24,
            "label": "node",
            "slug": "node-js"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9421,
        "uid": null,
        "title": "KillrVideo/killrvideo-nodejs",
        "url": "https://github.com/KillrVideo/killrvideo-nodejs",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>A reference application for Node.js developers looking to learn more about using\n<a href=\"http://cassandra.apache.org/\" rel=\"nofollow\">Apache Cassandra</a> and <a href=\"http://www.datastax.com/products/datastax-enterprise\" rel=\"nofollow\">DataStax Enterprise</a> in their applications and\nservices. Learn more at <a href=\"https://killrvideo.github.io/\" rel=\"nofollow\">killrvideo.github.io</a>.</p>\n<h2><a id=\"user-content-running-locally\" class=\"anchor\" aria-hidden=\"true\" href=\"#running-locally\"></a>Running Locally</h2>\n<p>Use these guides to get started running KillrVideo locally on your development machine:</p>\n<ul><li><a href=\"https://killrvideo.github.io/getting-started/\" rel=\"nofollow\">Getting Started with KillrVideo</a>: Follow this to setup common dependencies\nlike Docker.</li>\n<li><a href=\"https://killrvideo.github.io/docs/languages/nodejs/\" rel=\"nofollow\">Getting Started with Node.js</a>: Follow this to get this Node.js code\nrunning.</li>\n</ul><h2><a id=\"user-content-contributing-requests-for-more-examples\" class=\"anchor\" aria-hidden=\"true\" href=\"#contributing-requests-for-more-examples\"></a>Contributing, Requests for More Examples</h2>\n<p>This project will continue to evolve along with Cassandra and you can expect that as\nCassandra, DSE, and the drivers add new features, this application will try and provide\nexamples of those.  We gladly accept any pull requests for bug fixes, new features, etc. and\nif you have a request for an example that you don't see in the code currently, feel free to\nopen an issue here on GitHub or send a message to <a href=\"https://twitter.com/LukeTillman\" rel=\"nofollow\">@LukeTillman</a> on Twitter.</p>\n</article>",
        "created_at": "2018-03-14T22:27:46+0000",
        "updated_at": "2018-03-14T22:29:26+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/19652081?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9421"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 236,
            "label": "csharp",
            "slug": "csharp"
          }
        ],
        "is_public": false,
        "id": 9420,
        "uid": null,
        "title": "LukeTillman/killrvideo-csharp",
        "url": "https://github.com/LukeTillman/killrvideo-csharp",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>A reference application for .NET developers looking to learn more about using <a href=\"http://cassandra.apache.org/\" rel=\"nofollow\">Apache Cassandra</a> and\n<a href=\"http://www.datastax.com/products/datastax-enterprise\" rel=\"nofollow\">DataStax Enterprise</a> in their applications and services. Learn more at <a href=\"https://killrvideo.github.io/\" rel=\"nofollow\">killrvideo.github.io</a>.</p>\n<h2><a href=\"#running-locally\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-running-locally\"></a>Running Locally</h2>\n<p>Use these guides to get started running KillrVideo locally on your development machine:</p>\n<ul><li><a href=\"https://killrvideo.github.io/getting-started/\" rel=\"nofollow\">Getting Started with KillrVideo</a>: Follow this to setup common dependencies like Docker.</li>\n<li><a href=\"https://killrvideo.github.io/docs/languages/c-sharp/\" rel=\"nofollow\">Getting Started with C#</a>: Follow this to get this C# code running.</li>\n</ul><h2><a href=\"#pull-requests-requests-for-more-examples\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-pull-requests-requests-for-more-examples\"></a>Pull Requests, Requests for More Examples</h2>\n<p>This project will continue to evolve along with Cassandra and you can expect that as Cassandra and the DataStax driver add new features, this sample application will try and provide examples of those.  I'll gladly accept any pull requests for bug fixes, new features, etc.  and if you have a request for an example that you don't see in the code currently, send me a message <a href=\"https://twitter.com/LukeTillman\" rel=\"nofollow\">@LukeTillman</a> on Twitter or open an issue here on GitHub.</p>\n<h2><a href=\"#license\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-license\"></a>License</h2>\n<p>Copyright 2016 Luke Tillman</p>\n<p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p>\n<p><a href=\"http://www.apache.org/licenses/LICENSE-2.0\" rel=\"nofollow\">http://www.apache.org/licenses/LICENSE-2.0</a></p>\n<p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>\n</article>",
        "created_at": "2018-03-14T22:27:36+0000",
        "updated_at": "2018-03-23T21:56:31+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/428023?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9420"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          }
        ],
        "is_public": false,
        "id": 9419,
        "uid": null,
        "title": "PatrickCallaghan/datastax-eventsourcing",
        "url": "https://github.com/PatrickCallaghan/datastax-eventsourcing",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>This demo shows how Cassandra and DSE can be using to store and replay events.</p>\n<p>To use Spark you will need to provide your own Cassandra and Spark deployments. In this demo we will use DSE as they are already integrated.</p>\n<p>First we start DSE in SearchAnalyics mode to allow us to use both Spark and DSE Search -\n<a href=\"http://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/startStop/startDseStandalone.html?hl=starting\" rel=\"nofollow\">http://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/startStop/startDseStandalone.html?hl=starting</a></p>\n<p>The implementation uses bucketing to group all data into particular time buckets for replay. The time bucket used in this example is 1 minute but any time bucket can be used. Also depending how many days, months, years of events that need to be kept, it may be beneficial to spread the events over different tiers of tables.</p>\n<p>To create the schema, run the following</p>\n<pre>mvn clean compile exec:java -Dexec.mainClass=\"com.datastax.demo.SchemaSetup\" -DcontactPoints=localhost\n</pre>\n<p>To create the solr core to make our table searchable, run the following</p>\n<pre>dsetool create_core datastax.eventsource generateResources=true\n</pre>\n<p>To create events, run the following (Default of 10 million events)</p>\n<pre>mvn clean compile exec:java -Dexec.mainClass=\"com.datastax.events.Main\"  -DcontactPoints=localhost -DnoOfEvents=10000000\n</pre>\n<p>To replay a sample event set, run</p>\n<pre>mvn clean compile exec:java -Dexec.mainClass=\"com.datastax.events.ReadEvents\"  -DcontactPoints=localhost -Dfrom=yyyyMMdd-hhmmss -Dto=yyyyMMdd-hhmmss\n</pre>\n<p>eg</p>\n<pre>mvn clean compile exec:java -Dexec.mainClass=\"com.datastax.events.ReadEvents\"  -DcontactPoints=localhost -Dfrom=20160805-000000 -Dto=20160805-010000\n</pre>\n<p>This replays 2 scenarios</p>\n<pre>1. Replay all events for a specified time range\n2. Replay all events for a specified time range and a specific event type.\t\t\n</pre>\n<p>To run the webservice</p>\n<pre>mvn jetty:run -Djetty.port=8081 \n</pre>\n<p>To run a rest query, go the brower and enter a url in the format <a href=\"http://localhost:8080/datastax-eventsourcing/rest/getevents/from/to\" rel=\"nofollow\">http://localhost:8080/datastax-eventsourcing/rest/getevents/from/to</a>,\nwhere the date format is 'yyyyMMdd-hhmmss' e.g. For all events from midnight to 1:00 am on the 1st of August 2016 run -</p>\n<pre>http://localhost:8081/datastax-eventsourcing/rest/getevents/20160801-000000/20160801-010000/\n</pre>\n<p>We can also use cql to query using the Solr query from DSE Search</p>\n<p>Get all LOGIN Events from 9th Aug 2016 at 12:30 to 11th Aug 2016 at 12:30</p>\n<pre>select * from datastax.eventsource where solr_query = '{\"q\":\"eventtype:LOGIN\", \"fq\": \"time:[2016-08-09T12:30:00.000Z TO 2016-08-11T12:30:00.000Z]\", \"sort\":\"time desc\"}' limit 10000;\n</pre>\n<p>To use Spark, using DSE we can just 'dse spark' to use the repl.</p>\n<p>First we will create an Event object which will hold our events objects</p>\n<pre>case class Event (date: String, bucket: Int, id: java.util.UUID, data: String, eventtype: String, \naggregatetype: String, time: java.util.Date, loglevel: String, host: String); \nval events =  sc.cassandraTable[Event](\"datastax\", \"eventsource\").cache; \nevents.count\nval max = events.map(_.time).max\nval min = events.map(_.time).min\n</pre>\n<p>We can query our data and return events before or after a certain time.</p>\n<pre>val yesterday = new java.util.Date(java.util.Calendar.getInstance().getTime().getTime()-200000000);\nyesterday\nval before = events.filter(_.time.before(yesterday)); \nbefore.take(10).foreach(print) \nbefore.count\n \nval after = events.filter(_.time.after(yesterday)); \nafter.take(10).foreach(print) \nafter.count\n</pre>\n<p>Or we can use filtering to just get the events between two dates.</p>\n<pre>val start = new java.util.Date(java.util.Calendar.getInstance().getTime().getTime()-200000000);\nval end = new java.util.Date(java.util.Calendar.getInstance().getTime().getTime()-190000000);\nval filtered = events.filter(_.time.after(start)).filter(_.time.before(end)).cache;\nfiltered.count\n</pre>\n<p>Lets get all number of events per host and a list of all distinct hosts.</p>\n<pre>var hostCounts =  events.map(f =&gt; (f.host, 1)).reduceByKey(_ + _)\nhostCounts.collect().foreach(println)\nvar hosts =  hostCounts.map(f =&gt; (f._1))\nhosts.collect().foreach(println)\n</pre>\n<p>To use spark sql - try the following with a valid date</p>\n<pre>val results = sqlContext.sql(\"SELECT * from datastax.eventsource where date = '20161019'\")\n \nresults.take(5).foreach(println)\nval results = sqlContext.sql(\"SELECT * from datastax.eventsource where time &gt; '2016-10-22 16:18:07' \");\nresults.take(5).foreach(println)\nval results = sqlContext.sql(\"SELECT * from datastax.eventsource where time &gt; '2016-10-22 16:18:07' and time &lt; '2016-10-23 16:18:07'\");\nresults.count\n</pre>\n<p>To remove the tables and the schema, run the following.</p>\n<pre>mvn clean compile exec:java -Dexec.mainClass=\"com.datastax.demo.SchemaTeardown\"\n</pre>\n</article>",
        "created_at": "2018-03-14T22:19:56+0000",
        "updated_at": "2018-03-14T22:30:23+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/1006924?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9419"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 36,
            "label": "solr",
            "slug": "solr"
          },
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          }
        ],
        "is_public": false,
        "id": 9418,
        "uid": null,
        "title": "Report Number Three (Cassandra, Spark and Solr)",
        "url": "http://pkiraly.github.io/2016/04/09/third-report/",
        "content": "<p>Changing from Hadoop to Spark, refining mandatory calculation, adding field statistics, storing records \nin Cassandra, indexing with Solr and calculating uniqueness.</p><h2 id=\"changing-to-spark\">Changing to Spark</h2><p>Last year I did some Coursera courses on Big Data and Data Science (I recommend you <a href=\"https://www.coursera.org/learn/data-manipulation\" target=\"_blank\">Bill Howe’s course</a> \nfrom the University of Washington if you like to understand theoretical background behind relational databases and\ndata science, and I don’t recommend <a href=\"https://www.coursera.org/specializations/big-data\" target=\"_blank\">these courses</a>\nprovided by the University of California San Diego) where I have learnt\nabout <a href=\"http://spark.apache.org/\" target=\"_blank\">Apache Spark</a>. Spark’s big promise is that it is quicker \nthan <a href=\"http://hadoop.apache.org/\" target=\"_blank\">Hadoop</a>’s MapReduce and more memory effective. For me it\nwas even more important, that I really don’t use the “reduce” part of MapReduce, and Spark is fine with that.\nThe change was not hard at all, since the business logic is separated in different classes to which Hadoop was \njust a client (the only existing client actually, but I planned to add other interfaces). For the same functionality Spark\nran 4 times faster than Hadoop (1.5 hours versus 6 hours). It was a real gain, it means that I can change the codes and\nrun it again several times a day if I want to implement something new.</p><h2 id=\"refining-mandatory-calculation\">Refining mandatory calculation</h2><p>Thanks to the feedbacks from the Europeana Data Quality Comittee we improved the mandatory dimension of the \ncompleteness measure. This tell us how a record fit to the basic Europeanaa Data Model (EDM) schema requirements. Previously\nI calculated each field individually, but that was bad: some fields are alternative to each other, so a record is valid if\neither dc:title, dcterms:alternative or dc:description is present. Now the mandatory score gives what is expected, and it \nis a real discriminator of bad records.</p><p>Now the program assigns 1 if a field is existing and 0 if not for 30+ fields. The R script creates record sets summary\nof it, and on the interface I introduces the d3.js data visualization library to display those values. I also introduces \nsome filters in the UI: the user can hide those collections which doesn’t have a particular field, those in which\nevery records have it, and those in which some records have it. The user can investigate each fields.</p><p>The usage of the fields accross all the records:</p><p><img src=\"http://pkiraly.github.io/assets/field-frequency.png\" class=\"real\" title=\"Field frequency\" alt=\"Field frequency\" /></p><p>The usage of dcterms:alternative in those data providers, which uses it in some of the records, but not in all:</p><p><img src=\"http://pkiraly.github.io/assets/field-alternative-per-data-providers.png\" class=\"real\" title=\"dcterms:alternative frequency\" alt=\"dcterms:alternative frequency\" /></p><p>An <a href=\"http://144.76.218.178/europeana-qa/field.php?field=proxy_dcterms_alternative&amp;type=data-providers&amp;exclusions%5B%5D=0&amp;exclusions%5B%5D=1\" target=\"_blank\">example</a>\nabout the dcterms:alternative across data providers.</p><h2 id=\"storing-records-in-apache-cassandra\">Storing records in Apache Cassandra</h2><p>So far it was a problem, that in the record view of the UI I was nat able to extract an individual record from the \nhuge JSON files I stored the data. After sime investigation I choosed \n<a href=\"http://cassandra.apache.org/\" target=\"_blank\">Apache Cassandra</a>, which has an interface for \nboth Java and PHP. I imported every records to it, but now I still use thhe JSON files stored in HDFS (Hadoop Distributed \nFile System) in the Spark analysis. It is on my TODO list to compare the performance if using files and interating over \nevery Cassandra records – my hipothesis is that file based iteration is quicker. Now only the ID and the JSON content is\nstored in Cassandra, I am thinking about to store the measurements as well: it would be good if I would like to search for \neach record having a score between say 0.2 and 0.4.</p><h2 id=\"uniqueness-calculation\">Uniqueness calculation</h2><p>I have started investigating the uniqueness or entropy of some selected fields (dc:title, dcterms:alternative \nand dc:description). The basic idea is that if the terms in these fields are frequent accross the records, then \nit is less unique, so less important. If a term is frequent in the same field it is more important than terms \nappear only once. This is called information entropy or in the search engive world TF-IDF formula (term frequency, \ninverse document frequency) – see <a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\" target=\"_blank\">tf-idf</a> Wikipedia article.</p><p>The <a href=\"http://lucene.apache.org/solr/\" target=\"_blank\">Apache Solr</a> search engine’s relevancy ranking \nmostly based on this formula (however there are lots of tuning \npossibilities, such as give fields weights etc.), but Solr doesn’t provide an interface by default for \nextracting the terms TF-IDF score. There is a Term Vector Component however which provides this interface \ngiven that you apply some additional indexing rules. It is not available in the ordinary Europeana Solr \nsetup so I have created a new Solr instance with this special settings, and created a brand new index with \nlimited fieldset. (If you want to check how to setup Solr and what interface you can use, check this \n<a href=\"https://cwiki.apache.org/confluence/display/solr/The+Term+Vector+Component\" target=\"_blank\">wiki page</a>.</p><p>When the index were created (it took five days, but it is improvable) the scores of a field (in this case the dc:title “Fleming/Mair wedding, Slaithwaite, Huddersfield” – from <a href=\"http://www.europeana.eu/portal/record/2022320/3F61C612ED9C42CCB85E533B4736795E8BDC7E77.html\" target=\"_blank\">this record</a>) can be read from Solr API in the following form:</p><div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>\"dc_title_txt\":{\n  \"fleming\":{\n    \"tf\":1,\n    \"df\":1073,\n    \"tf-idf\":9.319664492078285E-4\n  },\n  \"huddersfield\":{\n    \"tf\":1,\n    \"df\":12073,\n    \"tf-idf\":8.282945415389712E-5\n  },\n  \"mair\":{\n    \"tf\":1,\n    \"df\":178,\n    \"tf-idf\":0.0056179775280898875\n  },\n  \"slaithwaite\":{\n    \"tf\":1,\n    \"df\":477,\n    \"tf-idf\":0.0020964360587002098\n  },\n  \"wedding\":{\n    \"tf\":1,\n    \"df\":10226,\n    \"tf-idf\":9.778994719342852E-5\n  }\n}\n</pre></div></div><p>Note: I did not applied truncation and other fancy Solr analyzes on the fields, it is only the lower case transformation\napplied. The API returns the stored terms in alphabetical order. I removed some properties Solr reports but \nirrelevant from our current perspective.</p><p>I have extracted these info for the above mentiond three fields, and created two scores: a cumulative \nscore which summarizes the all terms in the field, and an average, which is the average of \nthe terms’ tf-idf score. Those records which don’t have the field get 0 for both.</p><p><img src=\"http://pkiraly.github.io/assets/uniquness.png\" class=\"real\" title=\"Uniqueness\" alt=\"Uniquenes\" /></p><p>The graphs visualize the average uniqueness of terms in the field. You can see that - as expected - there are lots of records where this value is quite low - it means that the words of the title are usually common words. There are however some titles which have unique words. If the value is higher than 1, it means that a unique word appears multiple times in this field (unique means that it appears in only one record). In this particularly record set there is no such an example, but there are others, such as “Doog, Doog, Doog, Doog” (an indian one) or “Csalo! Csalo!” (a Hungarian one). In this particular dataset the most unique title is “Iganteçtaco pronoua, eta hilen pronoua.” in which “eta” is a common term, “hilen” and “Iganteçtaco” is unique, and “pronoua” is repeated unique.</p><p>In order to make comparision of the scores and the record, a two new features were added to the record view.</p><p>The first one is the term frequency viewer. Here you can see the terms stored, the term frequency (how many times the term appears in the current field instance), the document frequency (how many document has this term in this field) and the tf-idf scores Solr calculated.</p><p><img src=\"http://pkiraly.github.io/assets/term-frequencies.png\" class=\"real\" title=\"Term frequencies\" alt=\"Term frequencies\" /></p><p>The second one is a “naked” record view: it displays the non technical fields of the <code class=\"highlighter-rouge\">ore:Aggregation</code> and <code class=\"highlighter-rouge\">ore:Proxy</code> of the record. Those fields which are not analyzed in the current session (such as tableOfContents) are displayed in grey.</p><p><img src=\"http://pkiraly.github.io/assets/record-view.png\" class=\"real\" title=\"Record view\" alt=\"Record view\" /></p><p>You can access thhe UI in the usual web interface:</p><p><a href=\"http://144.76.218.178/europeana-qa/\" target=\"_blank\">http://144.76.218.178/europeana-qa/</a></p><p>Select one of the last six dimension to get the results.</p><h2 id=\"events-presentation-article\">Events, presentation, article</h2><p>The big news is that the Europeana <a href=\"http://pro.europeana.eu/page/data-quality-committee\" target=\"_blank\">Data Quality Committee</a> as a Europeana Network and EuropeanaTech Working Group is formed in March. It is a great honor, that I was involved. We have a quite active message board, a bi-weekly teleconference and a bi-yearly face-to-face meeting.</p><p><img src=\"http://pkiraly.github.io/assets/gwdg-nachrichten.png\" class=\"real\" title=\"GWDG Nachrichten\" alt=\"GWDG Nachrichten\" /></p><p>I wrote an <a href=\"https://www.gwdg.de/documents/20182/27257/GN_3-2016_www.pdf\" target=\"_blank\">article for GWDG Nachrichten</a> about the metadata quality issues in Europeana covering the roles of the Data Quality Committee, and Mr Yahyapour, the director of GWDG wrote a recommendation in the editorial column. The GWDG Nachrichten is circulated in the Göttingen Campus and in Max Planck Institutes.</p><p><img src=\"http://pkiraly.github.io/assets/networkshop.png\" class=\"real\" title=\"networkshop\" alt=\"networkshop\" /></p><p>I presented the research in Networkshop 2016 conference at the end of March in my home town. It was exceptional for me that I talked at the Auditorium Maximum of the University of Debrecen where I saw soo many unforgottable concerts, movies and speachhes as a teenager. Unfortunatelly I was the very last speaker on that day, and there were no time left for discussions. Here you can see <a href=\"http://www.slideshare.net/pkiraly/a-jk-s-a-rosszak-metaadatok-minsgellenrzse\" target=\"_blank\">the slides</a> (note: they are in Hungarian).</p>",
        "created_at": "2018-03-14T22:19:56+0000",
        "updated_at": "2018-03-14T22:29:56+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 7,
        "domain_name": "pkiraly.github.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9418"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 51,
            "label": "blog",
            "slug": "blog"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9393,
        "uid": null,
        "title": "Distributed Musings",
        "url": "http://www.sestevez.com/",
        "content": "<p>Cassandra collections create tombstones? Many new cassandra users learn this the hard way, they choose cassandra collections for the wrong reasons, for the wrong use cases, and then experience what is known as</p>",
        "created_at": "2018-03-07T17:06:25+0000",
        "updated_at": "2018-03-09T20:49:39+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.sestevez.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9393"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 217,
            "label": "tool",
            "slug": "tool"
          }
        ],
        "is_public": false,
        "id": 9388,
        "uid": null,
        "title": "thejaspm/cfstatsParseVisualize",
        "url": "https://github.com/thejaspm/cfstatsParseVisualize/blob/master/cfsStats2Csv.py",
        "content": "import os\n      \n        import re\n      \n        import sys\n      \n         \n      \n        from optparse import OptionParser\n      \n        \n\n      \n        KEYSPACE=\"Keyspace\"\n      \n        COLUMNFAMILY = \"Column Family\";\n      \n        READCOUNT = \"Read Count\";\n      \n        READLATENCY = \"Read Latency\";\n      \n        WRITECOUNT = \"Write Count\";\n      \n        WRITELATENCY = \"Write Latency\";\n      \n        PENDINGTASKS = \"Pending Tasks\";\n      \n        SSTABLECOUNT = \"SSTable count\";\n      \n        SSTABLEIEL = \"SSTables in each level\";\n      \n        SPACEUSEDL = \"Space used (live)\";\n      \n        SPACEUSEDT = \"Space used (total)\";\n      \n        SSTABLECR = \"SSTable Compression Ratio\";\n      \n        NUMKEYS = \"Number of Keys (estimate)\";\n      \n        MEMTABLECC = \"Memtable Columns Count\";\n      \n        MEMTABLEDS = \"Memtable Data Size\";\n      \n        MEMTABLESC = \"Memtable Switch Count\";\n      \n        BLOOMFFP = \"Bloom Filter False Positives\";\n      \n        BLOOMFFR = \"Bloom Filter False Ratio\";\n      \n        BLOOMFSU = \"Bloom Filter Space Used\";\n      \n        COMPACTEDRMINS = \"Compacted row minimum size\";\n      \n        COMPACTEDRMAXS = \"Compacted row maximum size\";\n      \n        COMPACTEDRMEANS = \"Compacted row mean size\";\n      \n        \n\n      \n        \n\n      \n        COLUMN_COUNT = 20\n      \n        \n\n      \n        INDEX_MAP = {\n      \n        \"SSTable count\":1,\n      \n        \"Space used (live)\": 2,\n      \n        \"Space used (total)\": 3,\n      \n        \"Number of Keys (estimate)\": 4,\n      \n        \"Memtable Columns Count\": 5,\n      \n        \"Memtable Data Size\": 6,\n      \n        \"Memtable Switch Count\": 7,\n      \n        \"Read Count\": 8,\n      \n        \"Read Latency\": 9,\n      \n        \"Write Count\": 10,\n      \n        \"Write Latency\": 11,\n      \n        \"Pending Tasks\": 12,\n      \n        \"Bloom Filter False Postives\": 13,\n      \n        \"Bloom Filter False Ratio\": 14,\n      \n        \"Bloom Filter Space Used\": 15,\n      \n        \"Compacted row minimum size\": 16,\n      \n        \"Compacted row maximum size\": 17,\n      \n        \"Compacted row mean size\": 18\n      \n        }\n      \n        \n\n      \n        \n\n      \n        def createHeaderRow():\n      \n            HeaderList  = [\"Row Type\",\"Entity\",\"SSTable count\",\"Space used (live)\",\"Space used (total)\",\"Number of Keys (estimate)\",\"Memtable Columns Count\",\"Memtable Data Size\",\"Memtable Switch Count\",\"Read Count\",\"Read Latency\",\"Write Count\",\"Write Latency\",\"Pending Tasks\",\"Bloom Filter False Postives\",\"Bloom Filter False Ratio\",\"Bloom Filter Space Used\",\"Compacted row minimum size\",\"Compacted row maximum size\",\"Compacted row mean size\"]\n      \n            return HeaderList\n      \n        \n\n      \n        \n\n      \n        def parseAndFormatData(data,fp):\n      \n            lines = data.split('\\n')\n      \n            \n      \n            csvList = [];i=0\n      \n            while i &lt; COLUMN_COUNT:\n      \n                csvList.append('')\n      \n                i += 1\n      \n            fp.write('\"'+'\",\"'.join(createHeaderRow())+'\"'+\"\\n\")\n      \n            for each in lines:\n      \n                if COLUMNFAMILY in each or KEYSPACE in each:\n      \n                    #new row in csv\n      \n                    if csvList[0] != '':\n      \n        \t\tfp.write('\"'+'\",\"'.join(csvList)+'\"'+\"\\n\")\n      \n                        while i &lt; COLUMN_COUNT:\n      \n                            csvList.append('')\n      \n                            i += 1\n      \n        \n\n      \n                    csvList[0],csvList[1]=each.split(':')\n      \n               \t    csvList[0] =  csvList[0].lstrip()\n      \n                else:\n      \n                    if \":\" in each:\n      \n        \t\teach = each.lstrip()\n      \n                        elemntsList = each.split(\":\")\n      \n                        elemntsList[0].lstrip()\n      \n                        if elemntsList[0] in INDEX_MAP.keys():\n      \n        \t\t\tif \"NaN \" in elemntsList[1]:\n      \n        \t\t\t\telemntsList[1]=\"\"\n      \n        \t\tcsvList[INDEX_MAP[elemntsList[0]]+1]=elemntsList[1].strip()\n      \n                        else:\n      \n                            continue\n      \n                    else:\n      \n                        continue\n      \n        \n\n      \n            sys.exit(1) \n      \n         \n      \n        def main():\n      \n            usage = 'usage: %prog --input=&lt;path to a file with cfhistograms output&gt;' + \\\n      \n                     ' --output=&lt;path to the output directory&gt;'\n      \n            parser = OptionParser(usage=usage)\n      \n            parser.add_option('--input', dest='input',\n      \n                          help='Path to a file with cfhistograms output')\n      \n            parser.add_option('--output', dest='output',\n      \n                          help='Path to a file where the graphs are saved')\n      \n         \n      \n            (options, args) = parser.parse_args()\n      \n         \n      \n            if not options.input:\n      \n                print('Error: Missing \"--input\" option')\n      \n                print parser.print_usage()\n      \n                sys.exit(1)\n      \n         \n      \n            if not options.output:\n      \n                print('Error: Missing \"--output\" option')\n      \n                print parser.print_usage()\n      \n                sys.exit(1)\n      \n         \n      \n            if not os.path.exists(options.input) or not \\\n      \n        \tos.path.isfile(options.input):\n      \n                print('--input argument is not a valid file path')\n      \n                sys.exit(2)\n      \n         \n      \n            fp1 = open(options.output, 'wb')\n      \n            with open(options.input, 'r') as fp:\n      \n                print('Processing file...') \n      \n                content = fp.read()\n      \n                parseAndFormatData(content,fp1)\n      \n            fp1.close()\n      \n        main()",
        "created_at": "2018-03-07T12:32:41+0000",
        "updated_at": "2018-03-11T17:39:57+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "github.com",
        "preview_picture": "https://avatars3.githubusercontent.com/u/3212343?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9388"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          },
          {
            "id": 994,
            "label": "dynamo",
            "slug": "dynamo"
          }
        ],
        "is_public": false,
        "id": 9385,
        "uid": null,
        "title": "DynamoDB vs DataStax | TrustRadius",
        "url": "https://www.trustradius.com/compare-products/amazon-dynamodb-vs-datastax",
        "content": "<p><a href=\"https://www.trustradius.com/products/amazon-dynamodb/reviews\" class=\"product-link\" target=\"_blank\" data-id=\"5595a8e4104b09180054082c\">Amazon DynamoDB</a> and DataStax <a href=\"https://www.trustradius.com/products/cassandra/reviews\" class=\"product-link\" target=\"_blank\" data-id=\"5595a51a11a8fd0e00e8a466\">Cassandra</a> are similar on masterless architecture and principles, <a href=\"https://www.trustradius.com/products/amazon-dynamodb/reviews\" class=\"product-link\" target=\"_blank\" data-id=\"5595a8e4104b09180054082c\">DynamoDB</a> is managed and needs cost analysis. If you need to have better control, DataStax is better.I also did a prototype with Google Spanner in one of the recent innovation days, it provides the best of both worlds but being a service on Google Cloud Platform(GCP) works if your services are primarily on GCP. <a href=\"https://www.trustradius.com/products/amazon-aurora/reviews\" class=\"product-link\" target=\"_blank\" data-id=\"5463c14b2969e30800e8f2c0\">Amazon Aurora</a> is a relational database with higher performance and is a good candidate if search and default relational behavior is preferred.For now, DataStax worked well for us as it provides best in class performance across different kinds of read/write/mixed workloads. It provides linear scalability which works for the best performance, lowest latency and highest throughput. If configured correctly, there is no downtime and no data loss.</p>",
        "created_at": "2018-03-06T21:50:06+0000",
        "updated_at": "2018-03-10T19:21:37+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.trustradius.com",
        "preview_picture": "https://www.trustradius.com/3.4.2/images/trustradius_square3.jpg?v=20171005",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9385"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9383,
        "uid": null,
        "title": "Instagram/cassandra",
        "url": "https://github.com/Instagram/cassandra/tree/rocks_3.0",
        "content": "<h3>\n      \n      README.asc\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\"><div>\n<h2 id=\"user-content-executive-summary\"><a href=\"#executive-summary\" aria-hidden=\"true\" class=\"anchor\"></a>Executive summary</h2>\n<div>\n<div>\n<p>Cassandra is a partitioned row store.  Rows are organized into tables with a required primary key.</p>\n</div>\n<div>\n<p><a href=\"http://wiki.apache.org/cassandra/Partitioners\" rel=\"nofollow\">Partitioning</a> means that Cassandra can distribute your data across multiple machines in an application-transparent matter.  Cassandra will automatically repartition as machines are added and removed from the cluster.</p>\n</div>\n<div>\n<p><a href=\"http://wiki.apache.org/cassandra/DataModel\" rel=\"nofollow\">Row store</a> means that like relational databases, Cassandra organizes data by rows and columns.  The Cassandra Query Language (CQL) is a close relative of SQL.</p>\n</div>\n<div>\n<p>For more information, see <a href=\"http://cassandra.apache.org/\" rel=\"nofollow\">the Apache Cassandra web site</a>.</p>\n</div>\n</div>\n</div>\n<div>\n<h2 id=\"user-content-requirements\"><a href=\"#requirements\" aria-hidden=\"true\" class=\"anchor\"></a>Requirements</h2>\n<div>\n<div>\n<ol><li>\n<p>Java &gt;= 1.8 (OpenJDK and Oracle JVMS have been tested)</p>\n</li>\n<li>\n<p>Python 2.7 (for cqlsh)</p>\n</li>\n</ol></div>\n</div>\n</div>\n<div>\n<h2 id=\"user-content-getting-started\"><a href=\"#getting-started\" aria-hidden=\"true\" class=\"anchor\"></a>Getting started</h2>\n<div>\n<div>\n<p>This short guide will walk you through getting a basic one node cluster up\nand running, and demonstrate some simple reads and writes.</p>\n</div>\n<div>\n<p>First, we’ll unpack our archive:</p>\n</div>\n<div>\n<div>\n<pre>$ tar -zxvf apache-cassandra-$VERSION.tar.gz\n$ cd apache-cassandra-$VERSION</pre>\n</div>\n</div>\n<div>\n<p>After that we start the server.  Running the startup script with the -f argument will cause\nCassandra to remain in the foreground and log to standard out; it can be stopped with ctrl-C.</p>\n</div>\n<div>\n<div>\n<pre>$ bin/cassandra -f</pre>\n</div>\n</div>\n<div>\n<div>\n<div>\n<p>Note for Windows users: to install Cassandra as a service, download\n<a href=\"http://commons.apache.org/daemon/procrun.html\" rel=\"nofollow\">Procrun</a>, set the\nPRUNSRV environment variable to the full path of prunsrv (e.g.,\nC:\\procrun\\prunsrv.exe), and run \"bin\\cassandra.bat install\".\nSimilarly, \"uninstall\" will remove the service.</p>\n</div>\n</div>\n</div>\n<div>\n<p>Now let’s try to read and write some data using the Cassandra Query Language:</p>\n</div>\n<div>\n<div>\n<pre>$ bin/cqlsh</pre>\n</div>\n</div>\n<div>\n<p>The command line client is interactive so if everything worked you should\nbe sitting in front of a prompt:</p>\n</div>\n<div>\n<div>\n<pre>Connected to Test Cluster at localhost:9160.\n[cqlsh 2.2.0 | Cassandra 1.2.0 | CQL spec 3.0.0 | Thrift protocol 19.35.0]\nUse HELP for help.\ncqlsh&gt;</pre>\n</div>\n</div>\n<div>\n<p>As the banner says, you can use 'help;' or '?' to see what CQL has to\noffer, and 'quit;' or 'exit;' when you’ve had enough fun. But lets try\nsomething slightly more interesting:</p>\n</div>\n<div>\n<div>\n<pre>cqlsh&gt; CREATE SCHEMA schema1\n       WITH replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\ncqlsh&gt; USE schema1;\ncqlsh:Schema1&gt; CREATE TABLE users (\n                 user_id varchar PRIMARY KEY,\n                 first varchar,\n                 last varchar,\n                 age int\n               );\ncqlsh:Schema1&gt; INSERT INTO users (user_id, first, last, age)\n               VALUES ('jsmith', 'John', 'Smith', 42);\ncqlsh:Schema1&gt; SELECT * FROM users;\n user_id | age | first | last\n---------+-----+-------+-------\n  jsmith |  42 |  john | smith\n cqlsh:Schema1&gt;</pre>\n</div>\n</div>\n<div>\n<p>If your session looks similar to what’s above, congrats, your single node\ncluster is operational!</p>\n</div>\n<div>\n<p>For more on what commands are supported by CQL, see\n<a href=\"https://github.com/apache/cassandra/blob/trunk/doc/cql3/CQL.textile\">the CQL reference</a>.  A\nreasonable way to think of it is as, \"SQL minus joins and subqueries, plus collections.\"</p>\n</div>\n<div>\n<p>Wondering where to go from here?</p>\n</div>\n<div>\n<ul><li>\n<p>Getting started: <a href=\"http://wiki.apache.org/cassandra/GettingStarted\" rel=\"nofollow\">http://wiki.apache.org/cassandra/GettingStarted</a></p>\n</li>\n<li>\n<p>Join us in #cassandra on irc.freenode.net and ask questions</p>\n</li>\n<li>\n<p>Subscribe to the Users mailing list by sending a mail to\n<a href=\"mailto:user-subscribe@cassandra.apache.org\">user-subscribe@cassandra.apache.org</a></p>\n</li>\n<li>\n<p>Planet Cassandra aggregates Cassandra articles and news:\n<a href=\"http://planetcassandra.org/\" rel=\"nofollow\">http://planetcassandra.org/</a></p>\n</li>\n</ul></div>\n</div>\n</div></article>",
        "created_at": "2018-03-06T14:57:59+0000",
        "updated_at": "2018-03-10T19:34:49+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/549085?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9383"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          }
        ],
        "is_public": false,
        "id": 9382,
        "uid": null,
        "title": "Instagram/cassandra-aws-benchmark",
        "url": "https://github.com/Instagram/cassandra-aws-benchmark",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>Scripts and templates for cassandra benchmark environment provision</p>\n<h2><a href=\"#requirements\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-requirements\"></a>Requirements</h2>\n<ul><li>aws cli: <code>pip install awscli --upgrade --user</code> and <code>aws configure</code></li>\n</ul><h2><a href=\"#create-benchmark-environment\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-create-benchmark-environment\"></a>Create benchmark environment</h2>\n<p>create cloudformation stack:</p>\n<div class=\"highlight highlight-source-shell\"><pre>aws cloudformation create-stack --stack-name MY_STACK_NAME \\\n    --disable-rollback \\\n    --template-body file://cloudformation.yaml \\\n    --parameters \\\n    ParameterKey=CassandraAMI,ParameterValue=MY_CASSANDRA_AMI_ID \\\n    ParameterKey=KeyName,ParameterValue=MY_SSH_KEY_NAME \\\n    ParameterKey=VPCSubnetId,ParameterValue=MY_VPC_SUBNET_ID \\\n    ParameterKey=InstanceProfile,ParameterValue=NAME_OF_INSTANCE_PROFILE_WITH_EC2_AND_AUOSCALEGROUP_READONLY_ACCESS \\\n    --capabilities CAPABILITY_IAM</pre></div>\n<p>update cloudformation stack:</p>\n<div class=\"highlight highlight-source-shell\"><pre>aws cloudformation udpate-stack --stack-name MY_STACK_NAME \\\n    --template-body file://cloudformation.yaml \\\n    --parameters \\\n    ParameterKey=CassandraAMI,ParameterValue=MY_CASSANDRA_AMI_ID \\\n    ParameterKey=KeyName,ParameterValue=MY_SSH_KEY_NAME \\\n    ParameterKey=VPCSubnetId,ParameterValue=MY_VPC_SUBNET_ID \\\n    ParameterKey=InstanceProfile,ParameterValue=NAME_OF_INSTANCE_PROFILE_WITH_EC2_AND_AUOSCALEGROUP_READONLY_ACCESS \\\n    --capabilities CAPABILITY_IAM</pre></div>\n<h2><a href=\"#lastest-prebuilt-cassandra-ami\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-lastest-prebuilt-cassandra-ami\"></a>Lastest Prebuilt Cassandra AMI</h2>\n<ul><li>cassandra 3.0.15: ami-09cc7371 (us-west-2)</li>\n<li>rocksandra: ami-b770cdcf (us-west-2)</li>\n</ul><h2><a href=\"#build-cassandra-ami-myself\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-build-cassandra-ami-myself\"></a>Build Cassandra AMI Myself</h2>\n<p>prerequists:  packer: <code>brew install packer</code></p>\n<p>cassandra3x</p>\n<pre>$&gt; cd ami/cassandra3x\n$&gt; wget -O resources/cassandra.rpm https://www.apache.org/dist/cassandra/redhat/30x/cassandra-3.0.15-1.noarch.rpm \n$&gt; packer build -var \"image_version=$(date +%s)\" packer.json\n</pre>\n<p>ndbench</p>\n<pre>$&gt; cd ami/bencher\n$&gt; packer build -var \"image_version=$(date +%s)\" packer.json\n</pre>\n<h2><a href=\"#license\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-license\"></a>License</h2>\n<p>Cassandra AWS Benchmark is Apache 2.0 licensed, as found in the LICENSE file.</p>\n</article>",
        "created_at": "2018-03-06T14:57:49+0000",
        "updated_at": "2018-03-10T19:35:01+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/549085?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9382"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9381,
        "uid": null,
        "title": "Open-sourcing a 10x reduction in Apache Cassandra tail latency",
        "url": "https://engineering.instagram.com/open-sourcing-a-10x-reduction-in-apache-cassandra-tail-latency-d64f86b43589?gi=714b1145b6c3",
        "content": "<p id=\"1d80\" class=\"graf graf--p graf-after--h3\">At Instagram, we have one of the world’s largest deployments of the Apache Cassandra database. We began using Cassandra in 2012 to replace Redis and support product use cases like fraud detection, Feed, and the Direct inbox. At first we ran Cassandra clusters in an AWS environment, but migrated them over to Facebook’s infrastructure when the rest of Instagram moved. We’ve had a really good experience with the reliability and availability of Cassandra, but saw room for improvement in read latency.<br /> <br />Last year Instagram’s Cassandra team started working on a project to reduce Cassandra’s read latency significantly, which we call Rocksandra. In this post, I will describe the motivation for this project, the challenges we overcame, and performance metrics in both internal and public cloud environments.</p><h3 id=\"d01d\" class=\"graf graf--h3 graf-after--p\">Motivation</h3><p id=\"c5aa\" class=\"graf graf--p graf-after--h3\">At Instagram, we use Apache Cassandra heavily as a general key value storage service. The majority of Instagram’s Cassandra requests are online, so in order to provide a reliable and responsive user experience for hundreds of millions of Instagram users, we have very tight SLA on the metrics. <br /> <br />Instagram maintains a 5–9s reliability SLA, which means at any given time, the request failure rate should be less than 0.001%. For performance, we actively monitor the throughput and latency of different Cassandra clusters, especially the P99 read latency. <br /> <br /> Here’s a graph that shows the client-side latency of one production Cassandra cluster. The blue line is the average read latency (5ms) and the orange line is the P99 read latency (in the range of 25ms to 60ms and changing a lot based on client traffic).</p><figure id=\"73e1\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*Scn1Nm33oukOJpUd4Ukszw.png\" data-width=\"1052\" data-height=\"668\" data-action=\"zoom\" data-action-value=\"1*Scn1Nm33oukOJpUd4Ukszw.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*Scn1Nm33oukOJpUd4Ukszw.png\" alt=\"image\" /></div></figure><figure id=\"0b14\" class=\"graf graf--figure graf-after--figure\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*ItBORNwCXce82ZNX6qf6Vg.png\" data-width=\"1052\" data-height=\"668\" data-action=\"zoom\" data-action-value=\"1*ItBORNwCXce82ZNX6qf6Vg.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*ItBORNwCXce82ZNX6qf6Vg.png\" alt=\"image\" /></div></figure><p id=\"0e33\" class=\"graf graf--p graf-after--figure\">After investigation, we found the JVM garbage collector (GC) contributed a lot to the latency spikes. We defined a metric called GC stall percentage to measure the percentage of time a Cassandra server was doing stop-the-world GC (Young Gen GC) and could not serve client requests. Here’s another graph that shows the GC stall percentage on our production Cassandra servers. It was 1.25% during the lowest traffic time windows, and could be as high as 2.5% during peak hours.</p><p id=\"14d6\" class=\"graf graf--p graf-after--p\">The graph shows that a Cassandra server instance could spend 2.5% of runtime on garbage collections instead of serving client requests. The GC overhead obviously had a big impact on our P99 latency, so if we could lower the GC stall percentage, we would be able to reduce our P99 latency significantly.</p><h3 id=\"87af\" class=\"graf graf--h3 graf-after--p\">Solution</h3><p id=\"5830\" class=\"graf graf--p graf-after--h3\">Apache Cassandra is a distributed database with it’s own LSM tree-based storage engine written in Java. We found that the components in the storage engine, like memtable, compaction, read/write path, etc., created a lot of objects in the Java heap and generated a lot of overhead to JVM. To reduce the GC impact from the storage engine, we considered different approaches and ultimately decided to develop a C++ storage engine to replace existing ones. <br /> <br />We did not want to build a new storage engine from scratch, so we decided to build the new storage engine on top of RocksDB. <br /> <br />RocksDB is an open source, high-performance embedded database for key-value data. It’s written in C++, and provides official API language bindings for C++, C, and Java. RocksDB is optimized for performance, especially on fast storage like SSD. It’s widely used in the industry as the storage engine for MySQL, mongoDB, and other popular databases.</p><h3 id=\"cb1c\" class=\"graf graf--h3 graf-after--p\">Challenges</h3><p id=\"4c10\" class=\"graf graf--p graf-after--h3\">We overcame three main challenges when implementing the new storage engine on RocksDB.<br /> <br />The first challenge was that Cassandra does not have a pluggable storage engine architecture yet, which means the existing storage engine is coupled together with other components in the database. To find a balance between massive refactoring and quick iterations, we defined a new storage engine API, including the most common read/write and streaming interfaces. This way we could implement the new storage engine behind the API and inject it into the related code paths inside Cassandra.<br /> <br />Secondly, Cassandra supports rich data types and table schema, while RocksDB provides purely key-value interfaces. We carefully defined the encoding/decoding algorithms to support Cassandra’s data model within RocksDB’s data structure and supported same-query semantics as original Cassandra. <br /> <br />The third challenge was about streaming. Streaming is an important component for a distributed database like Cassandra. Whenever we join or remove a node from a Cassandra cluster, Cassandra needs to stream data among different nodes to balance the load across the cluster. The existing streaming implementation was based on the details in the current storage engine. Accordingly, we had to decouple them from each other, make an abstraction layer, and re-implement the streaming using RocksDB APIs. For high streaming throughput, we now stream data into temp sst files first, and then use the RocksDB ingest file API to bulk load them into the RocksDB instance at once.</p><h3 id=\"a1f4\" class=\"graf graf--h3 graf-after--p\">Performance metrics</h3><p id=\"7a2f\" class=\"graf graf--p graf-after--h3\">After about a year of development and testing, we have finished a first version of the implementation and successfully rolled it into several production Cassandra clusters in Instagram. In one of our production clusters, the P99 read latency dropped from 60ms to 20ms. We also observed that the GC stalls on that cluster dropped from 2.5% to 0.3%, which was a 10X reduction!<br /> <br />We also wanted to verify whether Rocksandra would perform well in a public cloud environment. We setup a Cassandra cluster in an AWS environment using three i3.8 xlarge EC2 instances, each with 32 cores CPU, 244GB memory, and raid0 with 4 nvme flash disks. <br /> <br />We used <a href=\"https://github.com/Netflix/ndbench\" data-href=\"https://github.com/Netflix/ndbench\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">NDBench</a> for the benchmark, and the default table schema in the framework:</p><blockquote id=\"7425\" class=\"graf graf--blockquote graf-after--p\"><div><code class=\"markup--code markup--blockquote-code\">TABLE emp (</code><br /> <code class=\"markup--code markup--blockquote-code\">emp_uname text PRIMARY KEY,<br />emp_dept text,<br />emp_first text,<br />emp_last text</code><br /> <code class=\"markup--code markup--blockquote-code\">)</code></div></blockquote><p id=\"e302\" class=\"graf graf--p graf-after--blockquote\">We pre-loaded 250M 6KB rows into the database (each server stores about 500GB data on disk). We configured 128 readers and 128 writers in NDBench.<br /> <br />We tested different workloads and measured the avg/P99/P999 read/write latencies. As you can see, Rocksandra provided much lower and consistent tail read/write latency.</p><figure id=\"f950\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*Mpvc-jd61xmcrE4aEth4NA.png\" data-width=\"1132\" data-height=\"725\" data-action=\"zoom\" data-action-value=\"1*Mpvc-jd61xmcrE4aEth4NA.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*Mpvc-jd61xmcrE4aEth4NA.png\" alt=\"image\" /></div></figure><figure id=\"8947\" class=\"graf graf--figure graf-after--figure\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*zZO7xeU8fsWosWbkev873g.png\" data-width=\"1131\" data-height=\"724\" data-action=\"zoom\" data-action-value=\"1*zZO7xeU8fsWosWbkev873g.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*zZO7xeU8fsWosWbkev873g.png\" alt=\"image\" /></div></figure><p id=\"7f48\" class=\"graf graf--p graf-after--figure\">We also tested a read-only workload and observed that, at similar P99 read latency (2ms), Rocksandra could provide 10X higher read throughput (300K/s for Rocksandra vs. 30K/s for C* 3.0).</p><figure id=\"7652\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*E-2efj-mMo0dQWEvZyxn1g.png\" data-width=\"1483\" data-height=\"746\" data-action=\"zoom\" data-action-value=\"1*E-2efj-mMo0dQWEvZyxn1g.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*E-2efj-mMo0dQWEvZyxn1g.png\" alt=\"image\" /></div></figure><figure id=\"b56d\" class=\"graf graf--figure graf-after--figure\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*d5gs5SJzq6laocevBqA1Bg.png\" data-width=\"1359\" data-height=\"731\" data-action=\"zoom\" data-action-value=\"1*d5gs5SJzq6laocevBqA1Bg.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*d5gs5SJzq6laocevBqA1Bg.png\" alt=\"image\" /></div></figure><h3 id=\"d343\" class=\"graf graf--h3 graf-after--figure\">Future work</h3><p id=\"5bac\" class=\"graf graf--p graf-after--h3\">We have open sourced our <a href=\"https://github.com/Instagram/cassandra/tree/rocks_3.0\" data-href=\"https://github.com/Instagram/cassandra/tree/rocks_3.0\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">Rocksandra code base</a> and <a href=\"https://github.com/Instagram/cassandra-aws-benchmark\" data-href=\"https://github.com/Instagram/cassandra-aws-benchmark\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">benchmark framework</a>, which you can download from Github to try out in your own environment! Please let us know how it performs.<br /> <br />As our next step, we are actively working on the development of more C* features support, like secondary indexes, repair, etc. We are also working on a <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-13474\" data-href=\"https://issues.apache.org/jira/browse/CASSANDRA-13474\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">C* pluggable storage engine architecture</a> to contribute our work back to the Apache Cassandra community. <br /> <br />If you are in the Bay Area and are interested in learning more about our Cassandra developments, join us at our next meetup event <a href=\"https://www.meetup.com/Apache-Cassandra-Bay-Area/events/248376266/\" data-href=\"https://www.meetup.com/Apache-Cassandra-Bay-Area/events/248376266/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">here</a>.</p><p id=\"3171\" class=\"graf graf--p graf-after--p graf--trailing\"><em class=\"markup--em markup--p-em\">Dikang Gu is an infrastructure engineer at Instagram.</em></p>",
        "created_at": "2018-03-06T14:56:34+0000",
        "updated_at": "2018-03-10T19:35:18+0000",
        "published_at": "2018-03-05T18:03:23+0000",
        "published_by": [
          "Instagram Engineering"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 5,
        "domain_name": "engineering.instagram.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*Scn1Nm33oukOJpUd4Ukszw.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9381"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 994,
            "label": "dynamo",
            "slug": "dynamo"
          }
        ],
        "is_public": false,
        "id": 9380,
        "uid": null,
        "title": "Amazon DynamoDB vs Apache Cassandra – Beyond the lines",
        "url": "https://www.beyondthelines.net/databases/dynamodb-vs-cassandra/",
        "content": "<div id=\"ssba-classic-2\" class=\"ssba ssbp-wrap left ssbp--theme-1\"><div><a data-site=\"\" class=\"ssba_twitter_share\" href=\"http://twitter.com/share?url=https://www.beyondthelines.net/databases/dynamodb-vs-cassandra/&amp;text=Amazon%20DynamoDB%20vs%20Apache%20Cassandra%20\" target=\"&quot;_blank&quot;\"><img src=\"https://www.beyondthelines.net/wp-content/plugins/simple-share-buttons-adder/buttons/somacro/twitter.png\" title=\"Twitter\" class=\"ssba ssba-img\" alt=\"Tweet about this on Twitter\" /><div title=\"Twitter\" class=\"ssbp-text\">Twitter</div></a><a data-site=\"linkedin\" class=\"ssba_linkedin_share ssba_share_link\" href=\"http://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.beyondthelines.net/databases/dynamodb-vs-cassandra/\" target=\"&quot;_blank&quot;\"><img src=\"https://www.beyondthelines.net/wp-content/plugins/simple-share-buttons-adder/buttons/somacro/linkedin.png\" title=\"LinkedIn\" class=\"ssba ssba-img\" alt=\"Share on LinkedIn\" /><div title=\"Linkedin\" class=\"ssbp-text\">Linkedin</div></a><a data-site=\"\" class=\"ssba_facebook_share\" href=\"http://www.facebook.com/sharer.php?u=https://www.beyondthelines.net/databases/dynamodb-vs-cassandra/\" target=\"_blank\"><img src=\"https://www.beyondthelines.net/wp-content/plugins/simple-share-buttons-adder/buttons/somacro/facebook.png\" title=\"Facebook\" class=\"ssba ssba-img\" alt=\"Share on Facebook\" /><div title=\"Facebook\" class=\"ssbp-text\">Facebook</div></a><a data-site=\"\" class=\"ssba_google_share\" href=\"https://plus.google.com/share?url=https://www.beyondthelines.net/databases/dynamodb-vs-cassandra/\" target=\"&quot;_blank&quot;\"><img src=\"https://www.beyondthelines.net/wp-content/plugins/simple-share-buttons-adder/buttons/somacro/google.png\" title=\"Google+\" class=\"ssba ssba-img\" alt=\"Share on Google+\" /><div title=\"Google+\" class=\"ssbp-text\">Google+</div></a><a data-site=\"reddit\" class=\"ssba_reddit_share\" href=\"http://reddit.com/submit?url=https://www.beyondthelines.net/databases/dynamodb-vs-cassandra/&amp;title=Amazon DynamoDB vs Apache Cassandra\" target=\"&quot;_blank&quot;\"><img src=\"https://www.beyondthelines.net/wp-content/plugins/simple-share-buttons-adder/buttons/somacro/reddit.png\" title=\"Reddit\" class=\"ssba ssba-img\" alt=\"Share on Reddit\" /><div title=\"Reddit\" class=\"ssbp-text\">Reddit</div></a></div></div><p>Cassandra and DynamoDB both origin from the same paper: <a href=\"http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf\">Dynamo: Amazon’s Highly Available Key-value store</a>. (By the way – it has been a very influential paper and set the foundations for several NoSQL databases).</p>\n<p>Of course it means that DynamoDB and Cassandra have a lot in common! (They have the same DNA). However both AWS DynamoDB and Apache Cassandra have evolved quite a lot since this paper was written back in 2007 and there are now some key differences to be aware of when choosing between the two.</p>\n<p>This post aims at comparing these 2 distributed databases so that you can choose the one that best matches your requirements.</p>\n<h2>The concept</h2>\n<p>First of all let’s talk about the things they have in common and it starts with their implementation concept: Both of them map partition key onto a token ring using constant hashing to determine where to store the data.</p>\n<p>The idea is that the partition key is hashed into a 128 bits value. All the possible hash-values form a ring and each node in the cluster is responsible for one (or more) range of the ring.</p>\n<p><a href=\"http://beyondthelines.net/wp-content/uploads/2017/09/tokenring.png\"><img src=\"http://beyondthelines.net/wp-content/uploads/2017/09/tokenring.png\" alt=\"\" width=\"568\" height=\"508\" class=\"aligncenter size-full wp-image-1406\" srcset=\"https://www.beyondthelines.net/wp-content/uploads/2017/09/tokenring.png 568w, https://www.beyondthelines.net/wp-content/uploads/2017/09/tokenring-300x268.png 300w\" /></a></p>\n<p>By hashing the partition key every node is able to know the range it belongs to and from there the node in charge of this range. Of course we also need availability so the data needs to be replicated across multiple nodes. Where to place replicas (aka replication strategy) depends on the database implementation – (more on that later).</p>\n<p>It means that both DynamoDB and Cassandra are true peer-to-peer systems, with no master nodes (and no single point-of-failures). It also means that you can send your queries to any node in the cluster (or even better have your driver sent the request to the most appropriate node).</p>\n<h2>The data structure</h2>\n<h3>Primary key</h3>\n<p>Both databases belong to the column-family. Each item is identified by a primary key composed of 2 parts:</p>\n<ul><li>The partition key (mandatory): determines the partition where the item is stored</li>\n<li>The sort or cluster key (optional): determines how the item are sorting inside a partition</li>\n</ul><p>Cassandra supports any number of fields for both the partition key and the clustering key. On the other hand DynamoDB supports only 2 fields: one for the partition key and one for the sort key. It means that if you need several attributes to compose your key you need to manually concatenate them into a single field.</p>\n<h3>Schema</h3>\n<p>Cassandra supports structured data by means of a schema definition that you define in the table creation request in CQL (Cassandra Query Language).</p>\n<p>DynamoDB is schema-less (except for the primary key). Items in the same table can have completely different attributes (except for the partition and sort key).</p>\n<h3>Indexes</h3>\n<p>Both Cassandra and DynamoDB supports secondary indexes in order to query an item using an attribute that is not part of the primary key.</p>\n<p>However secondary index are usually slower and should be used wisely (preferred for attributes with low-cardinality).</p>\n<h3>Materialised views</h3>\n<p>Again both databases support materialised views (although DynamoDB calls them “Global secondary index”).</p>\n<h3>Time-To-Live</h3>\n<p>DynamoDB supports TTL at the item level. It means that when the TTL expires the whole item is deleted.</p>\n<p>Cassandra offers finer control as it supports TTL on columns which makes it possible to expire only certain fields of an item.</p>\n<h2>Consistency</h2>\n<p>One of the advantage of the dynamo family of NoSQL databases is that you can control the level of consistency that you need. E.g. if eventual consistency is enough for your application you can read one copy from any node in the cluster – it might not be the most up-to-date version of the data but it’s the fastest way to query the database.</p>\n<p>On the other hand you can require multiple nodes to answer your query making sure that you retrieve the latest version of the data.</p>\n<p>This is basically the 2 consistency levels that DynamoDB offers. You can control it with a “strong-consistency” flag in the query. If false eventual consistency is enough and you might not retrieve the latest version of the data. It true DynamoDB makes sure that you get the most up-to-date version of the data. Of course using strong consistency is slower and cost you more as well.</p>\n<p>Cassandra offers the same sort of consistency levels but with much finer control. You can choose between ANY (any node may answer), ONE (one node among the replicas of the given key), QUORUM (the majority of the replicas), LOCAL_QUORUM (the local majority of the replicas), ALL (all replicas).</p>\n<h3>Conflicts resolution</h3>\n<p>Although there is a preferred node in charge of a partition it may happen that we end up with a conflict for a given key (e.g. the preferred node wasn’t available). In this case DynamoDB and Cassandra take a different approach.</p>\n<p>The Cassandra strategy is simple: Every node add a timestamp when it writes the data. When there is a conflict the data with the most recent timestamp wins. This is the last-write-wins (LWW) strategy.</p>\n<p>Note that each field has its own timestamp so that it’s always possible to merge the changes if they concern different fields of the same object.</p>\n<p>The Dynamo paper relies on vector-clock to detect conflicts. Every node maintains a counter (or version number) of the changes it makes to an object. When it detects a conflict it tries to merge the results (which makes sense for Amazon because in the worst case you end-up with an extra item in your cart).</p>\n<p>Although it’s not clear from the AWS documentation it seems that DynamoDB in AWS now relies on a LLW strategy as well (or a combination of vector-clocks and timestamps).</p>\n<p>In the end there is not much difference between the 2 because both of them always return a single version of an object and the application never has to resolve conflicting objects.</p>\n<p>The only control that you have is the consistency level that you set in the request.</p>\n<h2>Language</h2>\n<p>Cassandra comes with its own language: The Cassandra Query Language (or CQL). It’s pretty close to SQL with some adaptations to support Cassandra features not present in SQL (collection and user-defined types, TTL, …).</p>\n<pre class=\"brush: sql; title: ; notranslate\" title=\"\">&#13;\nINSERT INTO cycling.cyclist_name (id, lastname, firstname)&#13;\nVALUES (6ab09bec-e68e-48d9-a5f8-97e6fb4c9b47, 'KRUIKSWIJK','Steven')&#13;\nUSING TTL 86400 AND TIMESTAMP 123456789;&#13;\n</pre>\n<p>DynamoDB relies on a specific JSON-based interface with variables replacement. The AWS CLI (Command Line Interface) makes it slightly easier but I still find using CQL much more natural.</p>\n<pre class=\"brush: jscript; title: ; notranslate\" title=\"\">&#13;\n{&#13;\n    \"ForumName\": {\"S\": \"Amazon DynamoDB\"},&#13;\n    \"Subject\": {\"S\": \"New discussion thread\"},&#13;\n    \"Message\": {\"S\": \"First post in this thread\"},&#13;\n    \"LastPostedBy\": {\"S\": \"fred@example.com\"},&#13;\n    \"LastPostDateTime\": {\"S\": \"201603190422\"}&#13;\n}&#13;\n</pre>\n<p>The <code>S</code> in the snippet above indicates that the attributes are of type String. It’s also possible to use variable substitution by using preceding the variable name with a <code>:</code></p>\n<pre class=\"brush: jscript; title: ; notranslate\" title=\"\">&#13;\n{&#13;\n  \"TableName\": \"Thread\",&#13;\n  \"Key\": {&#13;\n    \"ForumName\": {&#13;\n      \"S\": \"Amazon DynamoDB\"&#13;\n    },&#13;\n    \"Subject\": {&#13;\n      \"S\": \"A question about updates\"&#13;\n    }&#13;\n  },&#13;\n  \"UpdateExpression\": \"set Replies = Replies + :num\",&#13;\n  \"ExpressionAttributeValues\": { &#13;\n    \":num\": {\"N\": \"1\"}&#13;\n  },&#13;\n  \"ReturnValues\" : \"NONE\"&#13;\n}&#13;\n</pre>\n<h3>Multiple Updates</h3>\n<p>One note on updating all records stored under the same partition key. Cassandra fully support this operation by specifying only the partition key:</p>\n<pre class=\"brush: sql; title: ; notranslate\" title=\"\">&#13;\nUPDATE my_table SET some_field = \"some value\" WHERE my_partition_key = \"my_key\";&#13;\n</pre>\n<p>With DynamoDB you have to specify the entire primary key (i.e. both the hash and sort keys).</p>\n<h2>Protocol</h2>\n<p>Like all Amazon services DynamoDB offers a JSON/HTTP interface. The JSON syntax is specific to Dynamo as you need to indicate the type of the fields inside the JSON structure.</p>\n<p>To exchange data with DynamoDB you have to marshall to/from JSON which impacts performance (sometimes in a substantial way).</p>\n<p>The Cassandra client relies on a binary format to communicate with the database which makes the serialisation process more efficient (both in terms of CPU and network bandwidth). It is even more efficient than Thrift which was deprecated in favour of the CQL native protocol. Parts of its efficiency comes from the possibility to have several on-going requests at the same time.</p>\n<h3>Drivers</h3>\n<p>Both Cassandra and DynamoDB provide a set of drivers supporting most of the mainstream languages. In this section I discuss in more details the drivers available on the JVM as it is the platform I use the most.</p>\n<p>The java driver for Cassandra is pretty smart as it tries to optimise your query as best as it could (e.g. send the query to the node managing the hash of the partition key). It is also fully async and non-blocking as it can be used to send multiple requests simultaneously. The main caveat is the lack of type-safety as CQL queries are just written in plain text.</p>\n<p>There are a number of alternatives for the Scala language (<a href=\"https://github.com/outworkers/phantom\">Phantom</a>, <a href=\"https://github.com/getquill/quill\">Quill</a> and more recently <a href=\"https://github.com/schemasafe/troy\">Troy</a>).</p>\n<p>The java driver for DynamoDB is obviously not as good. The implementation is based on java future which only provides blocking calls to access the value. This really is a bummer to build reactive applications. There is a so-called async version of the client but all it does is delegate the blocking to a dedicated thread pool.</p>\n<p>The good news is that Amazon is working on a new version of the java driver that will be truly async but as the time of writing is still under developer-preview (not yet stable).</p>\n<p>There are a few Scala drivers developed by the community like <a href=\"https://github.com/guardian/scanamo\">Scanamo</a> which provides nice marshalers and a Free monad based implementation. However it relies on the AWS java client under the hood. (Using the Free monad makes it easy to plug in another client though).</p>\n<p>There is also an akka-stream connector available as part of the <a href=\"https://github.com/akka/alpakka\">Alpakka</a> project. To my knowledge this is the only non-blocking client available at the time of writing.</p>\n<h3>Data replication</h3>\n<p>In my opinion this is the key differentiating factor! Cassandra is fully tuneable and let’s you configure every aspect of the data replication. You can configure the number of replicas inside each datacenter and even enable replication across datacenter (making cross-region replication seamless – in case you’re hosting your Cassandra cluster on EC2).</p>\n<p>Note that the number of replicas doesn’t change automatically. You have to manually enable more replicas when your data or traffic outgrows the available resources.</p>\n<p>With DynamoDB it’s the opposite approach! You have (almost) no control over the number of replicas involved. Everything is performed automatically by AWS. You only provision the required throughput and DynamoDB makes sure you have enough partitions to handle the load.</p>\n<p>As your data grow AWS automatically add more partition. However it doesn’t change the provisioned throughput which means less throughput per partition. If your data is uniformly distributed that’s not an issue. If not you can quickly run into problem! This can become tricky when you store time-series into DynamoDB. (There is even a whole section dedicated to this topic in the DynamoDB documentation).</p>\n<p>Basically with DynamoDB you need to make sure your data is uniformly distributed! DynamoDB documentation does a pretty good job at explaining the rules on when partitions are created and how it impacts the provision throughput per partition.</p>\n<p>AWS has recently added support for Dynamo global tables: multi-region/multi-master data replication. It uses dynamo streams to replicate the data and resolves conflicts using a LastWriteWins strategy. There is also a new table backup/restore functionality allowing to backup a whole table at once without consuming any provisioned throughput. Restore can only be made into a new table and doesn’t consume any throughput. However it doesn’t play well with the multi-region support as backups can’t be exported into another region and restoring a local replica inside a global table doesn’t generate the corresponding events on the stream (which makes it impossible to sync the other region with this table). Basically global tables are a great step-forward but it is currently difficult to import existing data into a global table. More information here: <a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables</a></p>\n<h3>Costs</h3>\n<p>DynamoDB offers a “plug-and-play” approach. You set your expected throughput and you’re ready to go. Amazon manages everything for you. The costs are based on the provisioned throughput (0.47$/WCU and 0.09$/RCU) and storage used (0.25$/Gb). Of course you’d better check <a href=\"https://aws.amazon.com/dynamodb/pricing/\">the official pricing documentation</a> for up-to-date information.</p>\n<p>On the other hand Cassandra is free open-source software so you don’t have to pay for it. However you have to run it somewhere and you have to pay for the machine you’re going to use (most likely EC2 instances or something equivalent on GoogleCloud or Microsoft Azure). Cassandra is designed to run on cheap commodity hardware but you will need several instances (3, 5 or 7, … instances for each datacenter). You should also consider the amount of time you need to setup the cluster … </p>\n<p>Note that Datastax (the company behind Apache Cassandra) now offers <a href=\"https://www.datastax.com/products/datastax-managed-cloud\">Datastax-Managed-Cloud</a> where they deploy and administrate a Cassandra cluster for you on AWS.</p>\n<p>AWS now offers a caching layer (most likely based on ElastiCache) on top of DynamoDB called <a href=\"http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html\">DAX</a>. DAX allows you to improve performances (reduced latency due to in-memory caching) and save your provisioned throughput because requests will hit the cache instead of your DynamoDB tables. DAX supports caching both for items and queries.</p>\n<h2>Conclusion</h2>\n<p>As though Cassandra and DynamoDB seem very close at first sight there is a number of key differences such as</p>\n<ul><li>The replication strategy</li>\n<li>The service ownership (Database managed by you or AWS)</li>\n<li>The pricing model</li>\n</ul><p>DynamoDB is easy to get started with however you might keep an eye on the cost involved. It is also not a good choice to store more and more rarely-accessed data in Dynamo as it increases your storage costs and impacts your provisioned throughput. You’d better move this data into a dedicated table with a different throughput or even outside of Dynamo.</p>\n<p>I tried to cover the main aspects worth to consider when making a decision. However it’s quite difficult to cover everything so don’t hesitate to leave a comment if you find something missing.</p>",
        "created_at": "2018-03-05T15:56:19+0000",
        "updated_at": "2018-03-10T19:35:29+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_GB",
        "reading_time": 12,
        "domain_name": "www.beyondthelines.net",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9380"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9362,
        "uid": null,
        "title": "Apache Cassandra",
        "url": "https://docs.google.com/presentation/d/1JZYugL4WC9grgZswg1i6gAfWmFBqg9iQ0YcPLUiQ-6w/edit?_escaped_fragment_=",
        "content": "Apache Cassandra - Google Slides<noscript><div class=\"docs-butterbar-container\"><div class=\"docs-butterbar-wrap\"><p>JavaScript isn't enabled in your browser, so this file can't be opened. Enable and reload.</p></div><br /></div></noscript><article role=\"article\" class=\"slide\" title=\"Slide 1\"><section class=\"slide-content\" title=\"Slide content\"><div class=\"shape\" title=\"\"><p>Oliver Ruebenacker, SoftEng, Wed, Jan 17, 2018</p></div></section></article><article role=\"article\" class=\"slide\" title=\"Slide 2\"><section class=\"slide-content\" title=\"Slide content\"><div class=\"shape\" title=\"\"><p>What we will talk about today</p></div><div class=\"shape\" title=\"\"><p>If your SQL DB (MySQL, Oracle DB, PostgreSQL, H2, MariaDB, etc) is too slow because your data is too big <br />(typically above a terabyte, depending on data, setup, queries, hardware and acceptable response times), <br />then Apache Cassandra may be a great alternative.</p><ul class=\"list-kix_2u2b96dqnkw6-0\"><li value=\"1\">Some background on databases</li><li value=\"1\">SQL vs noSQL</li><li value=\"2\">Replication vs sharding</li><li value=\"3\">CAP theorem</li><li value=\"4\">ACID vs BASE</li></ul>What is Apache Cassandra and why you might want to use it<ul class=\"list-kix_2u2b96dqnkw6-1\"><li value=\"1\">Overview</li><li value=\"2\">Architecture/implementation</li><li value=\"3\">Cassandra Query Language (CQL)</li><li value=\"4\">Strengths and limitations</li></ul></div></section></article><article role=\"article\" class=\"slide\" title=\"Slide 3\"><section class=\"slide-content\" title=\"Slide content\"><div class=\"shape\" title=\"\"><p>Background: SQL</p></div><div class=\"shape\" title=\"\"><ul class=\"list-kix_otbvo7e7o70n-0\"><li value=\"1\">1970ies to 2000s, almost any popular DB was relational and (then soon) SQL</li><li value=\"1\">Early 1970ies, IBM develops Structured English Query Language (SEQUEL)</li><li value=\"2\">Soon renamed to Structured Query Language (SQL), since SEQUEL already taken</li><li value=\"3\">Soon almost any relational DB supports SQL</li><li value=\"4\">ANSI standard since 1986, but varying dialects persist (valid ids, case sensitivity, etc)</li></ul>Examples of SQL DBs: IBM DB2, Oracle, MySQL, MariaDB, PostgreSQL, H2SQL is very powerful and flexibleSQL DBs do not scale well into terabytes</div></section></article><article role=\"article\" class=\"slide\" title=\"Slide 4\"><section class=\"slide-content\" title=\"Slide content\"><div class=\"shape\" title=\"\"><p>Relational databases</p></div><div class=\"shape\" title=\"\"><p>Data is stored in tables linked by keys. Queries join tables by keys. Typically, data is normalized (avoid duplications).</p></div><table class=\"table\" cellpadding=\"0\" title=\"\" style=\"border-spacing: 0px;\"><tr><td rowspan=\"1\" colspan=\"3\"><div><p>Transactions</p></div></td></tr><tr><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"><div><p>Customer</p></div></td><td rowspan=\"1\" colspan=\"1\"><div><p>Product</p></div></td></tr><tr><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td></tr><tr><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td></tr><tr><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td></tr></table><table class=\"table\" cellpadding=\"0\" title=\"\" style=\"border-spacing: 0px;\"><tr><td rowspan=\"1\" colspan=\"3\"><div><p>Customers</p></div></td></tr><tr><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td></tr><tr><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"><div><p>Boston</p></div></td></tr><tr><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"><div><p>Austin</p></div></td></tr><tr><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"><div><p>Seattle</p></div></td></tr></table><table class=\"table\" cellpadding=\"0\" title=\"\" style=\"border-spacing: 0px;\"><tr><td rowspan=\"1\" colspan=\"3\"><div><p>Products</p></div></td></tr><tr><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td></tr><tr><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td></tr><tr><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td></tr><tr><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"><div><p>washer</p></div></td><td rowspan=\"1\" colspan=\"1\"></td></tr></table></section></article><article role=\"article\" class=\"slide\" title=\"Slide 5\"><section class=\"slide-content\" title=\"Slide content\"><div class=\"shape\" title=\"\"><p>Joining tables</p></div><div class=\"shape\" title=\"\"><p>SQL typically involves joining tables on the fly to larger tables:</p></div><table class=\"table\" cellpadding=\"0\" title=\"\" style=\"border-spacing: 0px;\"><tr><td rowspan=\"1\" colspan=\"7\"><div><p>Transactions-Customers-Products joined</p></div></td></tr><tr><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"><div><p>Customer</p></div></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"><div><p>Product</p></div></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td></tr><tr><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"><div><p>Austin</p></div></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td></tr><tr><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"><div><p>Seattle</p></div></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"><div><p>washer</p></div></td><td rowspan=\"1\" colspan=\"1\"></td></tr><tr><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"><div><p>Austin</p></div></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"><div><p>washer</p></div></td><td rowspan=\"1\" colspan=\"1\"></td></tr></table></section></article><article role=\"article\" class=\"slide\" title=\"Slide 6\"><section class=\"slide-content\" title=\"Slide content\"><div class=\"shape\" title=\"\"><p>All SQL DBs follow ACID principles that make life easier:</p><ul class=\"list-kix_cpyzj12749wo-0\"><li value=\"1\">Atomicity: a request is either executed completely, or not at all. No partial execution.</li><li value=\"2\">Consistency: a request can only transform a valid state into another valid state</li><li value=\"3\">Isolation: result of multiple concurrent requests is same as if requests are executed in sequence</li><li value=\"4\">Durability: a request only completed successfully if permanent (i.e. stored to disk)</li></ul><p>ACID usually implies that a write needs a (often global) lock - other requests (read or write) will have to wait.</p></div></section></article><article role=\"article\" class=\"slide\" title=\"Slide 7\"><section class=\"slide-content\" title=\"Slide content\"><div class=\"shape\" title=\"\"><p>Scaling out: replication vs sharding</p></div><div class=\"shape\" title=\"\"><p>Replication</p></div><div class=\"shape\" title=\"\"><p>Sharding</p></div><div class=\"shape\" title=\"\"><p>Sharding with replication (n = 2)</p></div></section></article><article role=\"article\" class=\"slide\" title=\"Slide 8\"><section class=\"slide-content\" title=\"Slide content\"><div class=\"shape\" title=\"\"><p>Replication</p></div><div class=\"shape\" title=\"\"><ul class=\"list-kix_yhp1hf5ts1ko-0\"><li value=\"1\">Can help avoid single point of failure</li><li value=\"2\">Allows high volume of reads, since these can be served in parallel by different nodes</li><li value=\"3\">But a write still needs to be executed by every node</li><li value=\"4\">If amount of data gets large, all requests become slow, and replication won't prevent that</li></ul></div></section></article><article role=\"article\" class=\"slide\" title=\"Slide 9\"><section class=\"slide-content\" title=\"Slide content\"><div class=\"shape\" title=\"\"><p>Sharding</p></div><div class=\"shape\" title=\"\"><ul class=\"list-kix_29tczu9j09pv-0\"><li value=\"1\">Network is the bottleneck: Sharding only works well if each node can work with its own shard without communicating too much with other nodes. </li><li value=\"2\">Under this condition:</li><li value=\"1\">Can handle large volume of reads or writes.</li><li value=\"2\">Can scale with large data sets.</li></ul></div></section></article><article role=\"article\" class=\"slide\" title=\"Slide 10\"><section class=\"slide-content\" title=\"Slide content\"><div class=\"shape\" title=\"\"><p>Distributed SQL</p></div><div class=\"shape\" title=\"\"><ul class=\"list-kix_c09mlr5uthpg-0\"><li value=\"1\">SQL allows joining any row in any table with any other row in any table</li><li value=\"2\">If we would shard, rows to be joined would end up on different shards most of the time.</li><li value=\"3\">This makes sharding unfeasible for SQL DBs.</li><li value=\"4\">Distributed SQL DBs are always fully replicated, never sharded.</li><li value=\"5\">Therefore, distributed SQL DBs can scale with high volumes of reads, but not writes.</li><li value=\"6\">Distributed SQL DBs will be slow for any read or write if data gets too big (typically, terabytes)</li></ul></div></section></article><article role=\"article\" class=\"slide\" title=\"Slide 11\"><section class=\"slide-content\" title=\"Slide content\"><div class=\"shape\" title=\"\"><ul class=\"list-kix_jw3eda2a8ekv-0\"><li value=\"1\">Originally, noSQL meant relational DB without SQL.</li><li value=\"2\">But now, noSQL means non-relational DBs.</li><li value=\"3\">Often, noSQL is a synonym for big data </li><li value=\"4\">Many take noSQL to mean \"not only SQL\"</li><li value=\"5\">Examples for noSQL:</li><li value=\"1\">Wide table: Bigtable, Cassandra, HBase, Scylla</li><li value=\"2\">Graph: Rdf4j, Blazegraph, Neo4j, Tinkerpop</li><li value=\"3\">Key-value: Voldemort, Riak, Dynamo</li><li value=\"4\">Document: MongoDB, CouchDB</li></ul></div></section></article><article role=\"article\" class=\"slide\" title=\"Slide 12\"><section class=\"slide-content\" title=\"Slide content\"><div class=\"shape\" title=\"\"><p>CAP theorem</p></div><div class=\"shape\" title=\"\"><p>You cannot have all three of these (Eric Brewer, 1998):</p><ul class=\"list-kix_s3lz09dqq5au-0\"><li value=\"1\">Consistency: Every read receives the most recent write or an error</li><li value=\"1\">Note: different from ACID consistency; CAP consistency is roughly ACID isolation</li></ul>Availability: Every request receives a (non-error) response – without guarantee that it contains the most recent writePartition tolerance: The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes<p>ACID DBs choose consistency over availability.</p><p>NoSQL DBs typically choose availability over consistency.</p></div></section></article><article role=\"article\" class=\"slide\" title=\"Slide 13\"><section class=\"slide-content\" title=\"Slide content\"><div class=\"shape\" title=\"\"><p>Most often, noSQL means:</p><p>Basically available, soft state, eventually consistent (BASE)</p><p>(in chemistry, bases are opposites of acids)</p><p>Eventually consistent means, reads may not see most recent writes, but will eventually.</p><p>BASE allows writes in parallel to reads or other writes.</p><p>Typically/roughly, requests are timestamped when received and percolate through the system. The later timestamp wins, but different parts of system may receive requests with delays and in different order.</p></div></section></article><article role=\"article\" class=\"slide\" title=\"Slide 14\"><section class=\"slide-content\" title=\"Slide content\"><div class=\"shape\" title=\"\"><p>Cassandra history</p></div><div class=\"shape\" title=\"\"><ul class=\"list-kix_q736sso1gtxm-0\"><li value=\"1\">Started at Facebook by Avinash Lakshman (one author of Amazon's Dynamo) and Prashant Malik</li><li value=\"2\">Cassandra in Greek mythology is a prophetess cursed not to be believed. She warned of the Trojan Horse.</li><li value=\"1\">Name stands for \"doom of the Oracle\"</li></ul>2008: open-source2009/2010: Apache incubator/top-level project2010: DataStax founded, commercial support for Cassandra2011: Cassandra Query Language (CQL)2012: Highest throughput according to studyMost popular wide-table DB according to DB-EnginesCompatible DBs:<ul class=\"list-kix_q736sso1gtxm-1\"><li value=\"1\">DataStax Enterprise</li><li value=\"2\">Scylla</li></ul></div></section></article><article role=\"article\" class=\"slide\" title=\"Slide 15\"><section class=\"slide-content\" title=\"Slide content\"><div class=\"shape\" title=\"\"><p>Replication and tunable consistency</p></div><div class=\"shape\" title=\"\"><ul class=\"list-kix_956ta65hdb6-0\"><li value=\"1\">Each table belongs to a keyspace. </li><li value=\"2\">Replication strategy is defined per keyspace</li><li value=\"3\">Replication strategy can be simply be a number of replicas, or replicas per datacenter or rack</li><li value=\"4\">Requests are:</li><li value=\"1\">Received by any node</li><li value=\"2\">Forwarded to each replica</li><li value=\"3\">Considered successful if number of replicas reporting success reaches consistency level</li></ul>Consistency level can be chosen per request:<ul class=\"list-kix_956ta65hdb6-1\"><li value=\"1\">Minimal: one replica</li><li value=\"2\">Quorum: majority of replicas</li><li value=\"3\">Maximum: all replicas</li><li value=\"4\">Can also be defined based on</li><li value=\"1\">local datacenter</li><li value=\"2\">all datacenters</li></ul><p>​</p></div></section></article><article role=\"article\" class=\"slide\" title=\"Slide 16\"><section class=\"slide-content\" title=\"Slide content\"><div class=\"shape\" title=\"\"><p>Cassandra tables are sparse</p></div><div class=\"shape\" title=\"\"><p>Compact table (typical for SQL DBs):</p><p>​</p></div><div class=\"shape\" title=\"\"><p>Sparse table (wide table DBs):</p><p>​</p></div><table class=\"table\" cellpadding=\"0\" title=\"\" style=\"border-spacing: 0px;\"><tr><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td></tr><tr><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"><div><p>poodle</p></div></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"><div><p>physics</p></div></td></tr><tr><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td></tr><tr><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"><div><p>corolla</p></div></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td></tr><tr><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"><div><p>great dane</p></div></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"></td></tr></table><table class=\"table\" cellpadding=\"0\" title=\"\" style=\"border-spacing: 0px;\"><tr><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"><div><p>dog: poodle, love: bob, phd: physics</p></div></td></tr><tr><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"><div><p>car: prius, love: alice</p></div></td></tr><tr><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"><div><p>car: corolla, phd: math</p></div></td></tr><tr><td rowspan=\"1\" colspan=\"1\"></td><td rowspan=\"1\" colspan=\"1\"><div><p>dog: great dane</p></div></td></tr></table></section></article><article role=\"article\" class=\"slide\" title=\"Slide 17\"><section class=\"slide-content\" title=\"Slide content\"><div class=\"shape\" title=\"\"><p>Cassandra Implementation</p></div><div class=\"shape\" title=\"\"><p>Token ring: all primary keys are converted to hashes (tokens) that form a ring-shaped space</p><p>Consistent hashing: tokens are assigned to shards (and nodes) such as to minimize necessary redistributing of data when nodes are added or removed</p><p>Tombstones: cells to be deleted are marked with tombstones and removed later (during compactification). This avoids accidental resurrection by earlier, still uncompleted creates/updates.</p><p>Bloom filters: A hash mask can prove in many cases the non-existence of a hash (i.e. primary key)</p></div></section></article><article role=\"article\" class=\"slide\" title=\"Slide 18\"><section class=\"slide-content\" title=\"Slide content\"><div class=\"shape\" title=\"\"><p>Cassandra Node Architecture</p></div><div class=\"shape\" title=\"\"><p>memory</p></div><div class=\"shape\" title=\"\"><p>MemTables</p></div><div class=\"shape\" title=\"\"><p>CommitLog (append-only)</p></div><div class=\"shape\" title=\"\"><p>SSTables (read-only)</p></div><div class=\"shape\" title=\"\"><p>compact</p></div></section></article><article role=\"article\" class=\"slide\" title=\"Slide 19\"><section class=\"slide-content\" title=\"Slide content\"><div class=\"shape\" title=\"\"><p>Table in Cassandra</p></div><div class=\"shape\" title=\"\"><ul class=\"list-kix_o38tcgqwbmvf-0\"><li value=\"1\">One or more columns are primary key</li><li value=\"1\">Among these, one or more columns are the partition key</li><li value=\"1\">Same partition means same node</li></ul>The others (can be none) are the clustering keyPrimary key uniquely identifies a rowPrimary key is converted to tokenOrder of columns in primary key determines order of rows Cassandra is essentially like a Map[PartitionKey, Map[ClusteringKey, Map[String, Cell]]]</div><div class=\"shape\" title=\"\"><p>partition key</p></div><div class=\"shape\" title=\"\"><p>partition</p></div><div class=\"shape\" title=\"\"><p>clustering key</p></div><div class=\"shape\" title=\"\"><p>column name</p></div></section></article><article role=\"article\" class=\"slide\" title=\"Slide 20\"><section class=\"slide-content\" title=\"Slide content\"><div class=\"shape\" title=\"\"><p>Cassandra Query Language (CQL) Features</p></div><div class=\"shape\" title=\"\"><ul class=\"list-kix_2e0g42f57kou-0\"><li value=\"1\">Similar to SQL, but much simpler</li><li value=\"2\">No joins, no subqueries, no stored procedures, no constraints</li><li value=\"3\">A cell can also contain a collection (list, set or map) or a JSON object</li><li value=\"4\">Can have user-defined types and functions</li><li value=\"5\">To insert/update/delete, need to specify primary key</li><li value=\"6\">To select, need either:</li><li value=\"1\">primary key or range</li><li value=\"2\">columns for which an index has been created</li></ul>We can also build materialized views</div></section></article><article role=\"article\" class=\"slide\" title=\"Slide 21\"><section class=\"slide-content\" title=\"Slide content\"><div class=\"shape\" title=\"\"><p>Create Table (CQL)</p></div><div class=\"shape\" title=\"\"><p>CREATE TABLE models (<br />make text,<br />model text,<br />year int,<br />seats int,<br />engine text,<br />fuel text,<br />PRIMARY KEY ((make, model), year)<br />)</p></div></section></article><article role=\"article\" class=\"slide\" title=\"Slide 22\"><section class=\"slide-content\" title=\"Slide content\"><div class=\"shape\" title=\"\"><p>INSERT (CQL)</p></div><div class=\"shape\" title=\"\"><p>Valid:</p><p>INSERT INTO models (make, model, year, seats, fuel)<br />VALUES ('Toyota', 'Corolla', 2005, 5, 'regular')</p><p>Invalid (primary key not given - year is missing):</p><p>INSERT INTO models (make, model, seats, fuel)<br />VALUES ('Toyota', 'Corolla', 5, 'regular')</p><p>​</p></div></section></article><article role=\"article\" class=\"slide\" title=\"Slide 23\"><section class=\"slide-content\" title=\"Slide content\"><div class=\"shape\" title=\"\"><p>SELECT (CQL)</p></div><div class=\"shape\" title=\"\"><p>Valid, gives one result (if exists):</p><p>SELECT make, model, year, seats, fuel FROM models <br />WHERE make = 'Toyota' AND model = 'Corolla' AND year = 2005</p><p>Also valid, even without year, since it is a continuous range of primary keys (results are all years):</p><p>SELECT make, model, year, seats, fuel FROM models <br />WHERE make = 'Toyota' AND model = 'Corolla'</p><p>Invalid, since not a continuous range of primary keys (order of columns in primary key matters!):</p><p>SELECT make, model, year, seats, fuel FROM models <br />WHERE make = 'Toyota' AND year = 2005</p></div></section></article><article role=\"article\" class=\"slide\" title=\"Slide 24\"><section class=\"slide-content\" title=\"Slide content\"><div class=\"shape\" title=\"\"><p>Need to organize data around queries</p></div><div class=\"shape\" title=\"\"><ul class=\"list-kix_iu6czgfxvole-0\"><li value=\"1\">Data needs to be organized around queries</li><li value=\"1\">Any query needs a primary key or key range, so pick primary key with caution</li><li value=\"2\">We can have secondary indices, although they are slower (they redirect)</li><li value=\"3\">We can scan over a range of keys</li><li value=\"4\">We can filter results, although that is discouraged</li><li value=\"5\">We can make a full scan and filter. Obviously, much slower</li></ul>May need to duplicate data:<ul class=\"list-kix_iu6czgfxvole-1\"><li value=\"1\">Extra columns to make up  for lack of joins</li><li value=\"2\">If we don't have primary key for one table, may need another, similar table but with a different primary key</li><li value=\"3\">Materialized views are an automatic way of duplication</li></ul>Consistency for duplicated data is enforced by client applicationClient app needs to manage lack of immediate consistency<p>Already miss SQL? That's the price we pay for large data and/or high write volume.</p></div></section></article><article role=\"article\" class=\"slide\" title=\"Slide 25\"><section class=\"slide-content\" title=\"Slide content\"><div class=\"shape\" title=\"\"><p>Resources</p></div><div class=\"shape\" title=\"\"><ul class=\"list-kix_kuhqbnc755uo-0\"><li value=\"1\">Link to these slides: <a target=\"_blank\" rel=\"noreferrer\" href=\"https://www.google.com/url?q=https://broad.io/cassandra&amp;sa=D&amp;ust=1519758362925000&amp;usg=AFQjCNFTcYw-L4zoVfd4ffdG1RLjH8I4ug\">https://broad.io/cassandra</a></li><li value=\"2\">Contact me: Oliver Ruebenacker, <a target=\"_blank\" rel=\"noreferrer\" href=\"mailto:oliverr@broadinstitute.org\">oliverr@broadinstitute.org</a></li><li value=\"3\">Slack channel: #cassandra</li><li value=\"4\">Google group: cassandra@broadinstitute.org</li></ul></div></section></article>",
        "created_at": "2018-02-27T18:06:03+0000",
        "updated_at": "2018-03-10T19:48:10+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 6,
        "domain_name": "docs.google.com",
        "preview_picture": "https://lh3.googleusercontent.com/xUrdfOoDhjhrHEZdiGvzOkhrpH7wL4vHkAcZUwdCaCC-FYKtMnm9CgnZlsrpjjF4I90k3g=w1200-h630-p",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9362"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 603,
            "label": "book",
            "slug": "book"
          }
        ],
        "is_public": false,
        "id": 9343,
        "uid": null,
        "title": "Introduction to Cassandra",
        "url": "https://www.gitbook.com/book/pandaforme/introduction-to-cassandra/details",
        "content": null,
        "created_at": "2018-02-22T16:26:02+0000",
        "updated_at": "2018-03-10T19:59:02+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": null,
        "language": null,
        "reading_time": 0,
        "domain_name": "www.gitbook.com",
        "preview_picture": null,
        "http_status": null,
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9343"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 15,
            "label": "tutorial",
            "slug": "tutorial"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9340,
        "uid": null,
        "title": "Brief Introduction to Cassandra Query Language (CQL) - Cassandra Tutorial | Intellipaat.com",
        "url": "https://intellipaat.com/tutorial/cassandra-tutorial/brief-introduction-to-cassandra-query-language-cql/",
        "content": "<div class=\"post-navigation page-navigation clear clearfix\"><p>&#13;\n                                        <a href=\"https://intellipaat.com/tutorial/cassandra-tutorial/non-relational-cassandra-data-model/\" title=\"Non Relational Cassandra Data Model\">«  Previous</a>&#13;</p><p>&#13;\n                                    <a href=\"https://intellipaat.com/tutorial/cassandra-tutorial/cassandra-client-api/\" title=\"Cassandra Client API\">Next »</a>&#13;</p></div><p><a href=\"https://intellipaat.com/tutorial/cassandra-tutorial/\" target=\"_blank\">CQL</a> is simple api mean for accessing Cassandra.CQL adds an abstraction layer that hides implementation details of this structure and provides native syntaxes for collections and other common encodings.</p><p><strong> Common ways to access CQL are:</strong></p><p>• Start cqlsh, the Python-based command-line client, on the command line of a Cassandra node.</p><p>• For developing applications, use one of the C#, Java, or Python open-source drivers.</p><p>• Use the set_cql_version Thrift method for programmatic access</p><p>Below are common operation we can do with CQL</p><p><strong> 1.) Creating and using key space:</strong></p><p>cqlsh&gt; CREATE KEYSPACE demodb WITH REPLICATION = { ‘class’ :</p><p>‘NetworkTopologyStrategy’, ‘datacenter1’ : 3 };</p><p>USE demodb;</p><p><strong> 2.) Alter Key Space</strong></p><p>ALTER KEYSPACE ” demodb ” WITH REPLICATION = { ‘class’ : ‘SimpleStrategy’,</p><p>‘replication_factor’ : 2};</p><p><strong> 3.) Create Table </strong></p><p>CREATE TABLE emp ( empID int, deptID int, first_name varchar,last_name varchar, PRIMARY KEY (empID, deptID));</p><p><strong> 4.) Insert into table </strong></p><p>INSERT INTO emp (empID, deptID, first_name, last_name) VALUES (104, 15, ‘jane’, ‘smith’);</p><p><strong> 5.) Select query Select * from emp;</strong></p><p><strong>Scaling online library with Cassandra</strong></p><p>Let’s talk about a very simple example of online books library. In relation database the system will have multiple normalized schemas. Few of very important one are described below:</p><p><a href=\"https://cdn.intellipaat.com/wp-content/uploads/2015/08/img-6.png\"><img class=\"wp-image-22286 aligncenter\" src=\"https://cdn.intellipaat.com/wp-content/uploads/2015/08/img-6-460x341.png\" alt=\"img 6\" width=\"518\" height=\"383\" srcset=\"https://cdn.intellipaat.com/mediaFiles/2015/08/img-6-460x341.png 460w, https://cdn.intellipaat.com/mediaFiles/2015/08/img-6-242x179.png 242w, https://cdn.intellipaat.com/mediaFiles/2015/08/img-6-120x89.png 120w, https://cdn.intellipaat.com/mediaFiles/2015/08/img-6-310x230.png 310w, https://cdn.intellipaat.com/mediaFiles/2015/08/img-6.png 464w\" /></a></p><p>As with introduction of “free users” and “unlimited books at my dispose” features in online book library, popularity of app increased to great extend .Resultant was flood of customer data. These made <a href=\"https://intellipaat.com/tutorial/sql-tutorial/rdbms/\" target=\"_blank\">RDBMS</a> setup unstable and business decided to go with Cassandra to mitigate the issue.</p><p>Before designing <a href=\"https://intellipaat.com/cassandra-training/\" target=\"_blank\">Cassandra</a> table we need to find out all possible queries which key space shall support.</p><p>Most fired queries would be:</p><p>1.) Get books of category with availability &gt;0</p><p>2.) Get books from same author Queries for business point of view</p><p>3.) Get Customer liking per geographic location</p><p>4.) Most liked category</p><p>So in order to incorporate this quicker we need to de-normalize data and create new column families.</p><p><a href=\"https://cdn.intellipaat.com/wp-content/uploads/2015/08/img-7.png\"><img class=\"wp-image-22294 aligncenter\" src=\"https://cdn.intellipaat.com/wp-content/uploads/2015/08/img-7-460x454.png\" alt=\"img 7\" width=\"680\" height=\"671\" srcset=\"https://cdn.intellipaat.com/mediaFiles/2015/08/img-7-460x454.png 460w, https://cdn.intellipaat.com/mediaFiles/2015/08/img-7-90x90.png 90w, https://cdn.intellipaat.com/mediaFiles/2015/08/img-7-242x239.png 242w, https://cdn.intellipaat.com/mediaFiles/2015/08/img-7-120x118.png 120w, https://cdn.intellipaat.com/mediaFiles/2015/08/img-7-310x306.png 310w, https://cdn.intellipaat.com/mediaFiles/2015/08/img-7.png 534w\" /></a></p><p>A detailed understanding of <a href=\"https://intellipaat.com/blog/apache-cassandra-a-brief-intro/\" target=\"_blank\">Apache Cassandra</a> is available in this blog post for your perusal!</p><div class=\"row post-navigation page-navigation clear clearfix\"><p>&#13;\n                                                                    <a href=\"https://intellipaat.com/tutorial/cassandra-tutorial/non-relational-cassandra-data-model/\" title=\"Non Relational Cassandra Data Model\">«  Previous</a>&#13;</p><p>&#13;\n                                                                    <a href=\"https://intellipaat.com/tutorial/cassandra-tutorial/cassandra-client-api/\" title=\"Cassandra Client API\">Next »</a>&#13;</p></div>",
        "created_at": "2018-02-21T16:17:20+0000",
        "updated_at": "2018-03-10T19:59:40+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 1,
        "domain_name": "intellipaat.com",
        "preview_picture": "https://cdn.intellipaat.com/wp-content/uploads/2015/08/img-6-460x341.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9340"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9338,
        "uid": null,
        "title": "Deletes Without Tombstones or TTLs (Eric Stevens, ProtectWise) | Cass…",
        "url": "https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016",
        "content": "Deletes Without Tombstones or TTLs (Eric Stevens, ProtectWise) | Cass…\n      \n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            </a><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div>\n    <div class=\"wrapper\"><p>Successfully reported this slideshow.</p><div id=\"slideview-container\" class=\"\"><div class=\"row\"><div id=\"main-panel\" class=\"small-12 large-8 columns\"><div class=\"sectionElements\"><div class=\"playerWrapper\"><div><div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\">Deletes Without Tombstones or TTLs (Eric Stevens, ProtectWise) | Cassandra Summit 2016<div class=\"stage valign-first-slide\"><div class=\"slide_container\"><section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-1-638.jpg?cb=1474041531\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-1-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-1-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-1-1024.jpg?cb=1474041531\" alt=\"Deletes Without&#10;Tombstones or TTLs&#10;Eric Stevens, Principal Architect&#10;ProtectWise, Inc.&#10;\" /></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-2-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-2-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-2-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;About ProtectWise&#10;An enterprise security company that records, analyzes, and ...\" /></i></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-3-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-3-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-3-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;With one sensor, ProtectWise captured the&#10;following data at Super Bowl 50:&#10;● ...\" /></i></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-4-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-4-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-4-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;● How Deletes (tombstones) in Cassandra Work Today&#10;● The Limitations of Tombs...\" /></i></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-5-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-5-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-5-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;● Increases both write and read I/O pressure&#10;● Not an effective means of recl...\" /></i></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-6-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-6-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-6-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;When do tombstones (and expired TTL’d&#10;records) go away?&#10;● Never before it’s g...\" /></i></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-7-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-7-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-7-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Misconception about Tombstone Performance&#10;● The performance degradation from ...\" /></i></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-8-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-8-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-8-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;PK1 CK1&#10;CK2&#10;1 2 ... o&#10;1 2 ... p&#10;... ...&#10;CKn 1 2 ... q&#10;PK1 DELETE 1 – n-1&#10;SSTa...\" /></i></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-9-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-9-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-9-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;{&#10;{&#10;{&#10;{&#10;Compaction Review&#10;↑ Writes&#10;← Older Data Newer Data →&#10;\" /></i></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-10-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-10-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-10-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Tombstones in Compaction&#10;↑ Delete&#10;SSTable&#10;containing&#10;record to&#10;delete ↑&#10;\" /></i></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-11-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-11-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-11-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Tombstones in Compaction&#10;↑ Other Writes&#10;SSTable&#10;containing&#10;record to&#10;delete ↑&#10;\" /></i></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-12-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-12-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-12-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Tombstones in Compaction&#10;↑ Other Writes&#10;SSTable&#10;containing&#10;record to&#10;delete ↑&#10;\" /></i></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-13-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-13-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-13-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Tombstones in Compaction&#10;↑ Other Writes&#10;SSTable&#10;containing&#10;record to&#10;delete ↑&#10;\" /></i></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-14-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-14-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-14-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Tombstones in Compaction&#10;↑ Other Writes&#10;Finally&#10;Deleted ↑&#10;\" /></i></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-15-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-15-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-15-1024.jpg?cb=1474041531\" alt=\"Showing why tombstones are not the same thing as a delete.&#10;Tombstone Demo&#10;\" /></i></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-16-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-16-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-16-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Setup&#10;cqlsh&gt; CREATE TABLE testing(&#10;… p blob,&#10;… c blob,&#10;… v blob,&#10;… PRIMARY KE...\" /></i></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-17-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-17-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-17-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Setup&#10;cqlsh&gt; INSERT INTO testing&#10;(p,c,v) VALUES (0xcafebabe,&#10;0xdeadbeef, 0xde...\" /></i></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-18-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-18-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-18-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Setup&#10;cqlsh&gt; DELETE FROM testing WHERE&#10;p=0xcafebabe AND c=0xdeadbeef;&#10;$ nodet...\" /></i></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-19-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-19-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-19-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Let’s look at the data&#10;$ hexdump testing-testing-ka-1-Data.db&#10;0000000 4b 00 0...\" /></i></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-20-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-20-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-20-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Let’s look at the data&#10;$ hexdump testing-testing-ka-2-Data.db&#10;0000000 4b 00 0...\" /></i></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-21-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-21-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-21-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Let’s look at the data&#10;$ hexdump testing-testing-ka-3-Data.db&#10;0000000 33 00 0...\" /></i></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-22-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-22-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-22-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Time to Compact&#10;Simulate compaction&#10;happening on data that&#10;has been deleted, ...\" /></i></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-23-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-23-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-23-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Let’s look again:&#10;$ hexdump testing-testing-ka-4-Data.db&#10;0000000 4b 00 00 00 ...\" /></i></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-24-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-24-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-24-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;What happened?&#10;● The tombstone for primary key (0xcafebabe,0xdeadbeef) was wr...\" /></i></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-25-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-25-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-25-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Why is this a problem&#10;● In all mainline compaction strategies:&#10;○ Data written...\" /></i></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-26-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-26-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-26-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;● Once a TTL has been written, there is no&#10;way to change your mind except to ...\" /></i></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-27-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-27-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-27-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;● Customers get to change their mind about how&#10;long they want us to retain th...\" /></i></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-28-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-28-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-28-1024.jpg?cb=1474041531\" alt=\"Our Unconventional Solution&#10;\" /></i></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-29-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-29-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-29-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;● If you have hot swappable drives, this is a&#10;lot easier, if not, you might h...\" /></i></section><section data-index=\"30\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-30-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-30-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-30-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Step 3&#10;\" /></i></section><section data-index=\"31\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-31-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-31-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-31-1024.jpg?cb=1474041531\" alt=\"Deleting Compaction Strategy&#10;\" /></i></section><section data-index=\"32\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-32-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-32-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-32-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;● Records are removed from the next&#10;compaction as soon as they should be&#10;evic...\" /></i></section><section data-index=\"33\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-33-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-33-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-33-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;● If you choose to, you can create a backup&#10;automatically of the deleted reco...\" /></i></section><section data-index=\"34\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-34-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-34-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-34-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;● Configurable and extensible&#10;● Several provided implementations can&#10;be reaso...\" /></i></section><section data-index=\"35\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-35-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-35-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-35-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;ALTER TABLE bar WITH compaction = {&#10;'class': 'DeletingCompactionStrategy',&#10;'d...\" /></i></section><section data-index=\"36\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-36-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-36-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-36-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Compaction’s Inner Workings&#10;Credit: DataStax&#10;https://docs.datastax.com/en/cas...\" /></i></section><section data-index=\"37\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-37-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-37-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-37-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Compaction’s Inner Workings&#10;Credit: DataStax&#10;https://docs.datastax.com/en/cas...\" /></i></section><section data-index=\"38\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-38-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-38-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-38-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Compaction’s Inner Workings&#10;Credit: DataStax&#10;https://docs.datastax.com/en/cas...\" /></i></section><section data-index=\"39\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-39-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-39-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-39-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Rules:&#10;A =&gt; ✓&#10;B =&gt; ✗&#10;C =&gt; ✓&#10;D =&gt; ✗&#10;E =&gt; ✓&#10;* if configured to backup convicted...\" /></i></section><section data-index=\"40\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-40-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-40-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-40-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;● Compaction performance is often bounded&#10;by available write capacity&#10;● Fewer...\" /></i></section><section data-index=\"41\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-41-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-41-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-41-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;● Records past the deletion boundary may&#10;still be visible to your application...\" /></i></section><section data-index=\"42\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-42-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-42-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-42-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;● Read repair and in general any repair may&#10;cause a record to fully resurrect...\" /></i></section><section data-index=\"43\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-43-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-43-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-43-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;● Supports and tested against Cassandra 2.x&#10;series&#10;● In 3.x the package and c...\" /></i></section><section data-index=\"44\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-44-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-44-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-44-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;https://github.com/protectwise/cassandra-util&#10;Also includes:&#10;● Our DataStax D...\" /></i></section><section data-index=\"45\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-45-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-45-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-45-1024.jpg?cb=1474041531\" alt=\"www.protectwise.com/careers.html&#10;Especially if you’re in Denver!&#10;Scala, Akka, Spark, Node, DevOps&#10;We’re Hiring!&#10;\" /></i></section><section data-index=\"46\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-46-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-46-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-46-1024.jpg?cb=1474041531\" alt=\"©2016 ProtectWise, Inc. All rights reserved.&#10;\" /></i></section><section data-index=\"47\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/85/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-47-320.jpg?cb=1474041531\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-47-638.jpg?cb=1474041531\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-47-1024.jpg?cb=1474041531\" alt=\"Cold Storage that Isn’t Glacial&#10;Tomorrow 10:45 Room LL20D&#10;Using Approximate Data for Small,&#10;Insightful Analytics&#10;Tomorrow ...\" /></i></section><div class=\"j-next-container next-container\"><div class=\"content-container\"><div class=\"next-slideshow-wrapper\"><div class=\"j-next-slideshow next-slideshow\"><p>Upcoming SlideShare</p></div><p>Loading in …5</p><p>×</p></div></div></div></div></div></div></div></div></div><div class=\"slideshow-info-container\" itemscope=\"itemscope\" itemtype=\"https://schema.org/MediaObject\"><div class=\"slideshow-tabs-container show-for-medium-up\"><ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    0 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\">\n                \n                  6 Likes\n                \n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\">\n                Statistics\n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\">\n                Notes\n              </i></a>\n            </li>\n          </ul><div class=\"tabs-content\"><div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\"><ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"itemscope\"><li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"gregorypub\" rel=\"nofollow\" href=\"https://www.slideshare.net/gregorypub?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            gregorypub\n                            \n                              \n                                \n                                \n                              \n                              \n                                \n                                \n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"NELLAIVIJAY1\" rel=\"nofollow\" href=\"https://www.slideshare.net/NELLAIVIJAY1?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Vijayakumar Ramdoss\n                            \n                              \n                                , \n                                Platform Architect\n                              \n                              \n                                 at \n                                Dell\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"will971\" rel=\"nofollow\" href=\"https://www.slideshare.net/will971?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Fabrice FACORAT\n                            \n                              \n                                , \n                                Linux system administrator, NoSQL and Big Data Expert\n                              \n                              \n                                 at \n                                Agoda\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"soloix\" rel=\"nofollow\" href=\"https://www.slideshare.net/soloix?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            soloix\n                            \n                              \n                                \n                                \n                              \n                              \n                                \n                                \n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"stavroskontopoulos\" rel=\"nofollow\" href=\"https://www.slideshare.net/stavroskontopoulos?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Stavros Kontopoulos\n                            \n                              \n                                , \n                                Senior Software Engineer at Lightbend, PSM I, PSPO I, Phd Student\n                              \n                              \n                                 at \n                                Lightbend\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n              </ul><div class=\"more-container text-center\"><a href=\"#\" class=\"j-more-favs\">\n                    Show More\n                    \n                  </a></div></div><div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No Downloads</p></div><div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No notes for slide</p>Essentially tombstones will never go away as long as a partition contains data in more than one SSTable, sometimes not even then (bloom filter collisions)When you write to Cassandra, the writes initially go to Memtables.   <br />When the memtables get full, they flush to disk as an immutable SSTable <br />When you perform a read, Cassandra needs to consider all the SSTables on disk, so as you accumulate lots of small SSTables, read performance will degrade <br />What do you think will be in SSTable 4? <br />Optimally it should be an empty table, the only record in it has been deleted. <br />However…What happened?</div></div></div><div class=\"notranslate transcript add-padding-right j-transcript\"><ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    Deletes Without\nTombstones or TTLs\nEric Stevens, Principal Architect\nProtectWise, Inc.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-2-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;About ProtectW...\" target=\"_blank\">\n        2.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nAbout ProtectWise\nAn enterprise security company that records, analyzes, and visualizes your network on demand to detect\ncomplex threats that others can’t see\nBig DataData Ingestion and Availability\n● Well north of a billion new records\nper day\n● Processed, analyzed, and stored\nin soft real time\n● Fully indexed and searchable with\np95 query response times &lt;1\nsecond\n○ Shortening the OODA loop\n● Hundreds of Cassandra servers\n● Hundreds of Billions of Records\n● Multiple Petabytes of Data\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-3-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;With one senso...\" target=\"_blank\">\n        3.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nWith one sensor, ProtectWise captured the\nfollowing data at Super Bowl 50:\n● 8.806 Terabytes of data seen. Primarily HTTP,\nSSL and traffic to Amazon AWS, Facebook,\nTwitter, and Instagram.\n● 1.550 Terabytes of data captured (82%\noptimization)\n● 17 million URLs hit\n● 8,085,949 DNS requests\nWith a single sensor deployed on the Levi's\nPublic Wi-Fi Network, ProtectWise captured\n8.806 Terabytes of Data and was able to optimize\nit by 82% to just 1.550 Terabytes of data, a true\ntestament to the scale and power of our platform.\nUse Case – Super Bowl 50\nThe Broncos weren’t the only team from Denver in Levi’s Stadium\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-4-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;● How Deletes ...\" target=\"_blank\">\n        4.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\n● How Deletes (tombstones) in Cassandra Work Today\n● The Limitations of Tombstones\n● Misconceptions about Tombstones\n● How TTL (Time to Live) in Cassandra works today\n● The limitations of TTLs\n● Why neither strategy works for ProtectWise\n● Our unconventional solution\n● Advantages of our solution\n● Disadvantages of our solution\nOverview\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-5-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;● Increases bo...\" target=\"_blank\">\n        5.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\n● Increases both write and read I/O pressure\n● Not an effective means of reclaiming disk\ncapacity\n● May be difficult to locate correct records for\ndeletion\n● Makes reads more expensive\n● Actual tombstones can often greatly outlive\ntheir deleted data (much longer than\ngc_grace)\nTerrible\n● Surgically target data for removal\n● Easy to reason about from a read\nconsistency perspective\nTerrific\nThe Trouble with Tombstones\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-6-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;When do tombst...\" target=\"_blank\">\n        6.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nWhen do tombstones (and expired TTL’d\nrecords) go away?\n● Never before it’s gc_grace old (this is a good thing, and you get to control it)\n● During compaction, for a tombstone past gc_grace, its partition key is checked\nagainst the bloom filters of all other SSTables for the given CQL table.\n● If there is a bloom filter collision, the tombstone will remain, even if the bloom\nfilter collision was a false positive\n● If there is ANY data, even other tombstones for that partition in any SSTable,\nthe tombstone will not get cleaned up\n● If bloom filters indicate there is no chance of overlap on that partition key, the\ntombstone will get cleaned up\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-7-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Misconception ...\" target=\"_blank\">\n        7.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nMisconception about Tombstone Performance\n● The performance degradation from tombstones isn’t from the tombstone itself.\n● If you do\n○ for (n &lt;- 0 to 100000) {\nINSERT INTO table (partitionKey, clusterKey) VALUES ( 1, n )\n}\n● You can later create a range tombstone that is tiny bytes wise:\n○ DELETE FROM table WHERE partitionKey = 1 AND clusterKey &lt; 99999\n● But if you then\n○ SELECT * FROM table WHERE partitionKey = 1 LIMIT 1\n● Cassandra will have to read then discard rows with clusterKey values from 0\nto 99998 before the LIMIT 1 can be reached\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-8-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;PK1 CK1&#10;CK2&#10;1 ...\" target=\"_blank\">\n        8.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nPK1 CK1\nCK2\n1 2 ... o\n1 2 ... p\n... ...\nCKn 1 2 ... q\nPK1 DELETE 1 – n-1\nSSTable 1\nSSTable 2\n3\nSELECT * FROM table WHERE pk1 LIMIT 1\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-9-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;{&#10;{&#10;{&#10;{&#10;Compac...\" target=\"_blank\">\n        9.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\n{\n{\n{\n{\nCompaction Review\n↑ Writes\n← Older Data Newer Data →\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-10-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Tombstones in ...\" target=\"_blank\">\n        10.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nTombstones in Compaction\n↑ Delete\nSSTable\ncontaining\nrecord to\ndelete ↑\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-11-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Tombstones in ...\" target=\"_blank\">\n        11.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nTombstones in Compaction\n↑ Other Writes\nSSTable\ncontaining\nrecord to\ndelete ↑\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-12-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Tombstones in ...\" target=\"_blank\">\n        12.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nTombstones in Compaction\n↑ Other Writes\nSSTable\ncontaining\nrecord to\ndelete ↑\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-13-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Tombstones in ...\" target=\"_blank\">\n        13.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nTombstones in Compaction\n↑ Other Writes\nSSTable\ncontaining\nrecord to\ndelete ↑\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-14-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Tombstones in ...\" target=\"_blank\">\n        14.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nTombstones in Compaction\n↑ Other Writes\nFinally\nDeleted ↑\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-15-638.jpg?cb=1474041531\" title=\"Showing why tombstones are not the same thing as a delete.&#10;...\" target=\"_blank\">\n        15.\n      </a>\n    Showing why tombstones are not the same thing as a delete.\nTombstone Demo\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-16-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Setup&#10;cqlsh&gt; C...\" target=\"_blank\">\n        16.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nSetup\ncqlsh&gt; CREATE TABLE testing(\n… p blob,\n… c blob,\n… v blob,\n… PRIMARY KEY(p,c)\n… ) WITH gc_grace_seconds=0;\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-17-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Setup&#10;cqlsh&gt; I...\" target=\"_blank\">\n        17.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nSetup\ncqlsh&gt; INSERT INTO testing\n(p,c,v) VALUES (0xcafebabe,\n0xdeadbeef, 0xdeadc0de);\n$ nodetool flush &amp;&amp; ls *-Data.db\ntesting-testing-ka-1-Data.db\ntesting-testing-ka-2-Data.db\ncqlsh&gt; INSERT INTO testing\n(p,c,v) VALUES (0xcafebabe,\n0xdeadbeef, 0xfacefeed);\n$ nodetool flush &amp;&amp; ls *-Data.db\ntesting-testing-ka-1-Data.db\n0xcafebabe:0xdeadbeef:0xfacefeed1 0xcafebabe:0xdeadbeef:0xfacefeed1\n0xcafebabe:0xdeadbeef:0xdeadc0de2\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-18-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Setup&#10;cqlsh&gt; D...\" target=\"_blank\">\n        18.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nSetup\ncqlsh&gt; DELETE FROM testing WHERE\np=0xcafebabe AND c=0xdeadbeef;\n$ nodetool flush &amp;&amp; ls *-Data.db\ntesting-testing-ka-1-Data.db\ntesting-testing-ka-2-Data.db\ntesting-testing-ka-3-Data.db\ncqlsh&gt; select * from testing;\np | c | v\n------------+------------+------------\n0xcafebabe | 0xdeadbeef | 0xdeadc0de\n0xcafebabe:0xdeadbeef:0xfacefeed1\n0xcafebabe:0xdeadbeef:0xdeadc0de2\n0xcafebabe:0xdeadbeef:DELETE3\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-19-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Let’s look at ...\" target=\"_blank\">\n        19.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nLet’s look at the data\n$ hexdump testing-testing-ka-1-Data.db\n0000000 4b 00 00 00 c3 00 04 ca fe ba be 7f ff ff ff 80\n0000010 00 01 00 72 0a 00 04 de ad be ef 0e 00 71 05 34\n0000020 3b d8 4e df f1 0d 00 14 0b 19 00 29 01 76 1a 00\n0000030 70 04 fa ce fe ed 00 00 6f 9b 15 17\n0xcafebabe:0xdeadbeef:0xfacefeed1\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-20-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Let’s look at ...\" target=\"_blank\">\n        20.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nLet’s look at the data\n$ hexdump testing-testing-ka-2-Data.db\n0000000 4b 00 00 00 c3 00 04 ca fe ba be 7f ff ff ff 80\n0000010 00 01 00 72 0a 00 04 de ad be ef 0e 00 71 05 34\n0000020 3b e3 86 df 23 0d 00 14 0b 19 00 29 01 76 1a 00\n0000030 70 04 de ad c0 de 00 00 62 de 14 02\n0xcafebabe:0xdeadbeef:0xdeadc0de2\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-21-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Let’s look at ...\" target=\"_blank\">\n        21.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nLet’s look at the data\n$ hexdump testing-testing-ka-3-Data.db\n0000000 33 00 00 00 c3 00 04 ca fe ba be 7f ff ff ff 80\n0000010 00 01 00 94 07 00 04 de ad be ef ff 10 0a 00 f0\n0000020 00 01 57 4f 2d 69 00 05 34 3b e6 ab 47 c8 00 00\n0000030 db 77 12 69\n0xcafebabe:0xdeadbeef:DELETE3\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-22-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Time to Compac...\" target=\"_blank\">\n        22.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nTime to Compact\nSimulate compaction\nhappening on data that\nhas been deleted, but\nwhere the tombstone is\nnot involved in the\ncompaction\n% jmx_invoke -m\norg.apache.cassandra.db:type=CompactionMan\nager forceUserDefinedCompaction testing-\ntesting-ka-1-Data.db,testing-testing-ka-2-\nData.db\n$ ls *-Data.db\ntesting-testing-ka-3-Data.db\ntesting-testing-ka-4-Data.db\n0xcafebabe:0xdeadbeef:0xfacefeed1\n0xcafebabe:0xdeadbeef:0xdeadc0de2 0xcafebabe:0xdeadbeef:??????????4\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-23-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Let’s look aga...\" target=\"_blank\">\n        23.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nLet’s look again:\n$ hexdump testing-testing-ka-4-Data.db\n0000000 4b 00 00 00 c3 00 04 ca fe ba be 7f ff ff ff 80\n0000010 00 01 00 72 0a 00 04 de ad be ef 0e 00 71 05 34\n0000020 3b e3 86 df 23 0d 00 14 0b 19 00 29 01 76 1a 00\n0000030 70 04 de ad c0 de 00 00 62 de 14 02\n0xcafebabe:0xdeadbeef:0xdeadc0de4\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-24-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;What happened?...\" target=\"_blank\">\n        24.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nWhat happened?\n● The tombstone for primary key (0xcafebabe,0xdeadbeef) was written in\nSSTable 3\n● SSTable 3 wasn’t involved in the compaction\n● ∴The data at rest didn’t get cleaned up\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-25-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Why is this a ...\" target=\"_blank\">\n        25.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nWhy is this a problem\n● In all mainline compaction strategies:\n○ Data written close together chronologically tends to compact together relatively quickly\n○ Data written chronologically far apart tends to take a long time to compact together\n■ This is why it’s an anti-pattern to append or overwrite the same partition over long\nperiods of time, your reads to that partition will end up needing to read out of a large\nnumber of SSTables\n○ Because disk capacity is not recovered until the tombstone and its underlying data are\ninvolved in the same compaction, it can take a long time to recover disk capacity\n● Some compaction strategies (DateTiered, TimeWindowed) have controls that\nallow for data to permanently stop compacting.\n○ Under these conditions there become times where it’s impossible to ever recover disk capacity\nNote, See CASSANDRA-7019 for an upcoming alternative\nAlso “Improving Tombstone Compactions” today at 4:10 in 210C\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-26-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;● Once a TTL h...\" target=\"_blank\">\n        26.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\n● Once a TTL has been written, there is no\nway to change your mind except to write the\nrecord again with a new TTL\n● Rows written to more than one time may\nhave inconsistent TTLs leading to dirty or\nincomplete reads.\n● TTL’d records may remain at rest much\nlonger than you realize in some\ncircumstances\nTrouble\n● Fire and forget, your data will “go away”\nfairly predictably\nTerrific\nThe Trouble with TTLs\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-27-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;● Customers ge...\" target=\"_blank\">\n        27.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\n● Customers get to change their mind about how\nlong they want us to retain their data\n● Changing TTL’s is expensive, both in terms of\nI/O pressure, and temporarily doubling the size\nof your data at rest\n● Disks are cheap… lots of disks are not\n● Cassandra data at rest has an ongoing cost, if\na customer stops paying for it, we need to as\nwell\n● Timeliness of deletes is important\n● Sensitive data spillage means we need to\nremove some data quickly\nWhy Neither Strategy Works for Us\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-28-638.jpg?cb=1474041531\" title=\"Our Unconventional Solution&#10;\" target=\"_blank\">\n        28.\n      </a>\n    Our Unconventional Solution\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-29-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;● If you have ...\" target=\"_blank\">\n        29.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\n● If you have hot swappable drives, this is a\nlot easier, if not, you might have some\ntemporary downtime due to RF change.\nStep 2: Disconnect Drive\n● There are some weird anti-entropy corner\ncases that are solved if you disable\nreplication\nStep 1: Set RF=1\nBasic Strategy\nSuccessfully used to delete significant amounts of data with little to no performance impact\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-30-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Step 3&#10;\" target=\"_blank\">\n        30.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nStep 3\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-31-638.jpg?cb=1474041531\" title=\"Deleting Compaction Strategy&#10;\" target=\"_blank\">\n        31.\n      </a>\n    Deleting Compaction Strategy\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-32-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;● Records are ...\" target=\"_blank\">\n        32.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\n● Records are removed from the next\ncompaction as soon as they should be\nevicted\n● If we need to recover capacity quickly we\ncan use user defined compaction to\nselectively target our oldest files\nEvicting Compaction Strategy\n● During compaction, use deterministic logic\nto determine which records should be\nremoved\n● Prevent records from surviving the\ncompaction process\n● Clean up indexes at the time the record is\nremoved\nDelete While Compacting\nBasic Strategy\nFor real this time.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-33-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;● If you choos...\" target=\"_blank\">\n        33.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\n● If you choose to, you can create a backup\nautomatically of the deleted records\n● Save yourself from deletion remorse\n● Incorrect deletion logic\n● Change of heart by you(r customer)\n● Move those records to cheaper storage\nBacking up your deletes\n● Acts as a parent strategy with your\npreferred child compaction strategy\n● Child strategy is responsible for sstable\nselection\n● You get the characteristics of your strategy,\nwith the deletes of our strategy\nWrapping Compaction Strategy\nFeatures\nDoes it support feature X of my preferred compaction strategy?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-34-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;● Configurable...\" target=\"_blank\">\n        34.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\n● Configurable and extensible\n● Several provided implementations can\nbe reasonably surgically controlled by\nreading deletion rules out of a table\nyou specify\n● Extend one of several base classes to\nprovide more sophisticated custom\nlogic\n● Restoring backups\n● To restore accidentally deleted\nrecords, copy these files to the right\npath and do nodetool refresh\n● Or if your topology has changed you\ncan restore them with sstableloader\nFeatures\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-35-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;ALTER TABLE ba...\" target=\"_blank\">\n        35.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nALTER TABLE bar WITH compaction = {\n'class': 'DeletingCompactionStrategy',\n'dcs_underlying_compactor':\n'LeveledCompactionStrategy',\n'sstable_size_in_mb': 160\n};\nALTER TABLE foo WITH compaction = {\n'class': 'DeletingCompactionStrategy',\n'dcs_underlying_compactor':\n'SizeTieredCompactionStrategy',\n'min_threshold': '2',\n'max_threshold': '8'\n};\nA Wrapping Compaction Strategy\nDoesn’t change the fundamental characteristics\nof your preferred compaction strategy\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-36-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Compaction’s I...\" target=\"_blank\">\n        36.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nCompaction’s Inner Workings\nCredit: DataStax\nhttps://docs.datastax.com/en/cassandra/2.1/cassandra/dml/dml_write_path_c.html\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-37-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Compaction’s I...\" target=\"_blank\">\n        37.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nCompaction’s Inner Workings\nCredit: DataStax\nhttps://docs.datastax.com/en/cassandra/2.1/cassandra/dml/dml_write_path_c.html\n{\nCompaction Strategy\nselects SSTables\nReturns SSTableIterators\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-38-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Compaction’s I...\" target=\"_blank\">\n        38.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nCompaction’s Inner Workings\nCredit: DataStax\nhttps://docs.datastax.com/en/cassandra/2.1/cassandra/dml/dml_write_path_c.html\n}\nFilteringSSTableIterators\nexclude data which should be\ndeleted, and also notify\nIndexManager if appropriate to\nclean up associated indexes.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-39-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;Rules:&#10;A =&gt; ✓&#10;...\" target=\"_blank\">\n        39.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nRules:\nA =&gt; ✓\nB =&gt; ✗\nC =&gt; ✓\nD =&gt; ✗\nE =&gt; ✓\n* if configured to backup convicted records\nAn Evicting Compaction Strategy\nRecords involved in compaction which are convicted do not\nsurvive into the newly compacted SSTable\nA\nB\nC\nA\nB\nD\nC\nD\nE\nA\nC\nE\nSSTable 1 SSTable 2 SSTable 3\nNew SSTable Backup SSTable*\nB\nD\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-40-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;● Compaction p...\" target=\"_blank\">\n        40.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\n● Compaction performance is often bounded\nby available write capacity\n● Fewer records surviving into the target table\nreduces write pressure during compaction\n● Testing of records for conviction is\nlightweight (depending on the complexity of\nyour business logic), and mostly CPU\nbound\nOften Faster than Existing Compaction\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-41-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;● Records past...\" target=\"_blank\">\n        41.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\n● Records past the deletion boundary may\nstill be visible to your application\n● You may get inconsistent reads for\nsuch records\n● Evicted records may resurrect temporarily\ndue to repair\n● They’ll end up in a new SSTable and\nwill evict again during the next auto\ncompaction\nBoundary Consistency\n● Like all other baked in deletion options, disk\ncapacity is reclaimed only eventually\n● Old SSTables still tend not to compact\nvery frequently\n● However by triggering user defined\ncompaction, you can reclaim space\nimmediately without resorting to major\ncompaction\nEventual Deletes\nLimitations\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-42-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;● Read repair ...\" target=\"_blank\">\n        42.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\n● Read repair and in general any repair may\ncause a record to fully resurrect temporarily\n● Resurrected record will appear in the\nyoungest SSTables\n● Will disappear again when those new\nSSTables next compact (generally relatively\nquickly for an active cluster)\nRepair = Resurrection\n● Logic for deletes needs to be deterministic\nor you’ll end up with consistency issues\n● Probably not a good idea to base any\ndeletion logic on anything outside of the\nprimary key except in narrow use cases\nRequires deletion determinism\nLimitations\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-43-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;● Supports and...\" target=\"_blank\">\n        43.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\n● Supports and tested against Cassandra 2.x\nseries\n● In 3.x the package and class names\nchanged, needs to be ported\n● Tests are written in Scala, they cover a lot\nof surface area but would need to be\nrewritten prior to contribution\n● Needs additional general purpose\nconvictors\n● Principally tested against STCS and\ndeserves better coverage for other child\nstrategies\nCurrent Project Status\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-44-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;https://github...\" target=\"_blank\">\n        44.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\nhttps://github.com/protectwise/cassandra-util\nAlso includes:\n● Our DataStax Driver Wrapper for Scala\n● Our CCM wrapper lib for automating unit tests in Scala\nGitHub\nAvailability &amp; Compatibility\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-45-638.jpg?cb=1474041531\" title=\"www.protectwise.com/careers.html&#10;Especially if you’re in De...\" target=\"_blank\">\n        45.\n      </a>\n    www.protectwise.com/careers.html\nEspecially if you’re in Denver!\nScala, Akka, Spark, Node, DevOps\nWe’re Hiring!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-46-638.jpg?cb=1474041531\" title=\"©2016 ProtectWise, Inc. All rights reserved.&#10;\" target=\"_blank\">\n        46.\n      </a>\n    ©2016 ProtectWise, Inc. All rights reserved.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409/95/deletes-without-tombstones-or-ttls-eric-stevens-protectwise-cassandra-summit-2016-47-638.jpg?cb=1474041531\" title=\"Cold Storage that Isn’t Glacial&#10;Tomorrow 10:45 Room LL20D&#10;U...\" target=\"_blank\">\n        47.\n      </a>\n    Cold Storage that Isn’t Glacial\nTomorrow 10:45 Room LL20D\nUsing Approximate Data for Small,\nInsightful Analytics\nTomorrow 2:00 Room LL20A\nSee Our Other Talks\n \n  </li>\n              </ol></div></div></div><aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\"><dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\"><ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"66066829\" title=\"PowerPoint: Designing Better Slides\" href=\"https://www.linkedin.com/learning/powerpoint-designing-better-slides?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_PowerPoint: Designing Better Slides\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"PowerPoint: Designing Better Slides\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=Qzyjo5pakOdYhauvchS%2F2whtPj0%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-kUyWs-dWfZX_pf8TfZLSiol4feCwDkwc2feivRTXiEY69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>PowerPoint: Designing Better Slides</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"66066829\" title=\"Learning Study Skills\" href=\"https://www.linkedin.com/learning/learning-study-skills?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Learning Study Skills\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Learning Study Skills\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=AkCmIodsxaZhRytp%2F97%2FxxQM0Q8%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-iXCej-NWfY3DrcMXfZLSiolwQfy0HkQQxfe6rRTbmFI69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Learning Study Skills</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"66066829\" title=\"Teacher Tips\" href=\"https://www.linkedin.com/learning/teacher-tips?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Teacher Tips\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Teacher Tips\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=mN807ffzPmE7p%2FNpTyJseUnrM8Q%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-gXySu_NCfYXPofcHaZLSiol8eeiUIlwE0feuvQjDoEo69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Teacher Tips</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"66646862\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Light Weight Transactions Under Stress  (Christopher Batey, The Last Pickle) | Cassandra Summit 2016\" href=\"https://www.slideshare.net/DataStax/light-weight-transactions-under-stress-christopher-batey-the-last-pickle-cassandra-summit-2016\">\n    \n    <div class=\"related-content\"><p>Light Weight Transactions Under Stress  (Christopher Batey, The Last Pickle) ...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"66238420\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"C* Capacity Forecasting (Ajay Upadhyay, Jyoti Shandil, Arun Agrawal, Netflix) | Cassandra Summit 2016\" href=\"https://www.slideshare.net/DataStax/c-capacity-planning-and-upgrade-at-scale-ajay-upadhyay-jyoti-shandil-arun-agrawal-netflix-cassandra-summit-2016\">\n    \n    <div class=\"related-content\"><p>C* Capacity Forecasting (Ajay Upadhyay, Jyoti Shandil, Arun Agrawal, Netflix)...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"15540478\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Optimizing Cassandra in AWS\" href=\"https://www.slideshare.net/greggulrich/re-invent-2012-cassandra-final3\">\n    \n    <div class=\"related-content\"><p>Optimizing Cassandra in AWS</p><p>greggulrich</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"66190842\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Advanced Cassandra Operations via JMX (Nate McCall, The Last Pickle) | C* Summit 2016\" href=\"https://www.slideshare.net/DataStax/advanced-cassandra-operations-via-jmx-nate-mccall-the-last-pickle-c-summit-2016\">\n    \n    <div class=\"related-content\"><p>Advanced Cassandra Operations via JMX (Nate McCall, The Last Pickle) | C* Sum...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"66190675\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"KillrVideo: Data Modeling Evolved (Patrick McFadin, Datastax) | Cassandra Summit 2016\" href=\"https://www.slideshare.net/DataStax/killrvideo-data-modeling-evolved-patrick-mcfadin-datastax-cassandra-summit-2016\">\n    \n    <div class=\"related-content\"><p>KillrVideo: Data Modeling Evolved (Patrick McFadin, Datastax) | Cassandra Sum...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"66111486\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"A look at the CQL changes in 3.x (Benjamin Lerer, Datastax) | Cassandra Summit 2016\" href=\"https://www.slideshare.net/DataStax/a-look-at-the-cql-changes-in-3x-benjamin-lerer-datastax-cassandra-summit-2016\">\n    \n    <div class=\"related-content\"><p>A look at the CQL changes in 3.x (Benjamin Lerer, Datastax) | Cassandra Summi...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"66112228\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Cassandra Tuning - Above and Beyond (Matija Gobec, SmartCat) | Cassandra Summit 2016\" href=\"https://www.slideshare.net/DataStax/cassandra-tuning-above-and-beyond-matija-gobec-smartcat-cassandra-summit-2016\">\n    \n    <div class=\"related-content\"><p>Cassandra Tuning - Above and Beyond (Matija Gobec, SmartCat) | Cassandra Summ...</p><p>DataStax</p></div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div></div><footer>\n          <div class=\"row\"><div class=\"columns\"><ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div></div>\n          \n          <div class=\"row\"><div class=\"columns\"><p class=\"copyright text-center\">LinkedIn Corporation © 2018</p></div></div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div>\n    <div class=\"row\"><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></div>\n        \n    </div>\n    \n    \n    \n  \n    \n    \n  \n  \n  <noscript>\n    </noscript>",
        "created_at": "2018-02-20T22:09:15+0000",
        "updated_at": "2018-03-10T20:00:19+0000",
        "published_at": null,
        "published_by": [
          "DataStax"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 14,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/cassandrasummit2016-deleteswithouttombstonesorttls-160915170409-thumbnail-4.jpg?cb=1474041531",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9338"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 962,
            "label": "lambda",
            "slug": "lambda"
          }
        ],
        "is_public": false,
        "id": 9336,
        "uid": null,
        "title": "Lambda at Weather Scale - Cassandra Summit 2015",
        "url": "https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015",
        "content": "Lambda at Weather Scale - Cassandra Summit 2015\n      \n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><a class=\"item\" href=\"https://pt.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            </a><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://pt.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explorar</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">Você</label>\n            </p></div>\n    <div class=\"wrapper\"><p>O slideshow foi denunciado.</p><div id=\"slideview-container\" class=\"\"><div class=\"row\"><div id=\"main-panel\" class=\"small-12 large-8 columns\"><div class=\"sectionElements\"><div class=\"playerWrapper\"><div><div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\">Lambda at Weather Scale - Cassandra Summit 2015<div class=\"stage valign-first-slide\"><div class=\"slide_container\"><section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-1-638.jpg?cb=1443270670\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-1-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-1-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-1-1024.jpg?cb=1443270670\" alt=\"Lambda&#10;at Weather Scale Robbie Strickland&#10;\" /></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-2-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-2-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-2-1024.jpg?cb=1443270670\" alt=\"Who Am I?&#10;Robbie Strickland&#10;Director of Engineering, Analytics&#10;rstrickland@weather.com&#10;@rs_atl&#10;\" /></i></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-3-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-3-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-3-1024.jpg?cb=1443270670\" alt=\"Who Am I?&#10;● Contributor to C*&#10;community since 2010&#10;● DataStax MVP 2014/15&#10;● Author, Cassandra High&#10;Availability&#10;● Founder,...\" /></i></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-4-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-4-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-4-1024.jpg?cb=1443270670\" alt=\"About TWC&#10;● ~30 billion API requests per day&#10;● ~120 million active mobile users&#10;● #3 most active mobile user base&#10;● ~360 P...\" /></i></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-5-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-5-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-5-1024.jpg?cb=1443270670\" alt=\"Use Case&#10;● Billions of events per day&#10;○ Web/mobile beacons&#10;○ Logs&#10;○ Weather conditions + forecasts&#10;○ etc.&#10;● Keep data fore...\" /></i></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-6-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-6-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-6-1024.jpg?cb=1443270670\" alt=\"Use Case&#10;● Efficient batch + streaming analysis&#10;● Self-serve data science&#10;● BI / visualization tool support&#10;\" /></i></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-7-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-7-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-7-1024.jpg?cb=1443270670\" alt=\"Architecture&#10;\" /></i></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-8-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-8-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-8-1024.jpg?cb=1443270670\" alt=\"Attempt[0] Architecture&#10;Operational&#10;Analytics&#10;Business&#10;Analytics&#10;Executive&#10;Dashboards&#10;Data&#10;Discovery&#10;Data&#10;Science&#10;3rd Part...\" /></i></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-9-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-9-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-9-1024.jpg?cb=1443270670\" alt=\"Attempt[0] Data Model&#10;CREATE TABLE events (&#10;timebucket bigint,&#10;timestamp bigint,&#10;eventtype varchar,&#10;eventid varchar,&#10;platf...\" /></i></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-10-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-10-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-10-1024.jpg?cb=1443270670\" alt=\"Attempt[0] Data Model&#10;CREATE TABLE events (&#10;timebucket bigint,&#10;timestamp bigint,&#10;eventtype varchar,&#10;eventid varchar,&#10;platf...\" /></i></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-11-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-11-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-11-1024.jpg?cb=1443270670\" alt=\"Attempt[0] Data Model&#10;CREATE TABLE events (&#10;timebucket bigint,&#10;timestamp bigint,&#10;eventtype varchar,&#10;eventid varchar,&#10;platf...\" /></i></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-12-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-12-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-12-1024.jpg?cb=1443270670\" alt=\"Attempt[0] Data Model&#10;CREATE TABLE events (&#10;timebucket bigint,&#10;timestamp bigint,&#10;eventtype varchar,&#10;eventid varchar,&#10;platf...\" /></i></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-13-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-13-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-13-1024.jpg?cb=1443270670\" alt=\"Attempt[0] tl;dr&#10;● C* everywhere&#10;● Streaming data via custom ingest process&#10;● Kafka backed by RESTful service&#10;● Batch data...\" /></i></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-14-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-14-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-14-1024.jpg?cb=1443270670\" alt=\"Attempt[0] tl;dr&#10;● C* everywhere&#10;● Streaming data via custom ingest process&#10;● Kafka backed by RESTful service&#10;● Batch data...\" /></i></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-15-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-15-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-15-1024.jpg?cb=1443270670\" alt=\"Attempt[0] Lessons&#10;● Batch loading large data sets into C* is silly&#10;● … and expensive&#10;● … and using Informatica to do it i...\" /></i></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-16-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-16-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-16-1024.jpg?cb=1443270670\" alt=\"Attempt[0] Lessons&#10;● Schema-less == bad:&#10;○ Must parse JSON to extract key data&#10;○ Expensive to analyze by event type&#10;○ Cann...\" /></i></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-17-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-17-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-17-1024.jpg?cb=1443270670\" alt=\"Attempt[1] Architecture&#10;Data Lake&#10;Operational&#10;Analytics&#10;Business&#10;Analytics&#10;Executive&#10;Dashboards&#10;Data&#10;Discovery&#10;Data&#10;Scienc...\" /></i></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-18-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-18-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-18-1024.jpg?cb=1443270670\" alt=\"Attempt[1] Data Model&#10;● Each event type gets its own table&#10;● Tables individually tuned based on workload&#10;● Schema applied ...\" /></i></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-19-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-19-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-19-1024.jpg?cb=1443270670\" alt=\"Attempt[1] tl;dr&#10;● Use C* for streaming data&#10;○ Rolling time window (TTL depends on type)&#10;○ Real-time access to events&#10;○ Da...\" /></i></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-20-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-20-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-20-1024.jpg?cb=1443270670\" alt=\"Attempt[1] tl;dr&#10;● Everything else in S3&#10;○ Batch data loads (mostly logs)&#10;○ Daily C* backups&#10;○ Stored as Parquet&#10;○ Cheap, ...\" /></i></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-21-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-21-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-21-1024.jpg?cb=1443270670\" alt=\"Attempt[1] tl;dr&#10;● Kafka replaced by SQS:&#10;○ Scalable &amp; reliable&#10;○ Already fronted by a RESTful interface&#10;○ Nearly free to ...\" /></i></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-22-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-22-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-22-1024.jpg?cb=1443270670\" alt=\"Attempt[1] tl;dr&#10;● STCS in lieu of DTCS (and LCS)&#10;○ Because it’s bulletproof&#10;○ Partitions spanning sstables is acceptable&#10;...\" /></i></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-23-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-23-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-23-1024.jpg?cb=1443270670\" alt=\"Attempt[1] tl;dr&#10;● STCS in lieu of DTCS (and LCS)&#10;○ Because it’s bulletproof&#10;○ Partitions spanning sstables is acceptable&#10;...\" /></i></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-24-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-24-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-24-1024.jpg?cb=1443270670\" alt=\"Fine Print&#10;● Use C* &gt;= 2.1.8&#10;○ CASSANDRA-9637 - fixes Spark input split&#10;computation&#10;○ CASSANDRA-9549 - fixes memory leak&#10;○...\" /></i></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-25-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-25-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-25-1024.jpg?cb=1443270670\" alt=\"Fine Print&#10;● Two main Spark clusters:&#10;○ Co-located with C* for heavy analysis&#10;■ Predictable load&#10;■ Efficient C* access&#10;○ S...\" /></i></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-26-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-26-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-26-1024.jpg?cb=1443270670\" alt=\"Data Modeling&#10;\" /></i></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-27-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-27-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-27-1024.jpg?cb=1443270670\" alt=\"Partitioning&#10;● Opposite strategy from “normal” C* modeling&#10;○ Model for good parallelism&#10;○ … not for single-partition queri...\" /></i></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-28-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-28-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-28-1024.jpg?cb=1443270670\" alt=\"Secondary Indexes&#10;● Useful for C*-level filtering&#10;● Reduces Spark workload and RAM footprint&#10;● Low cardinality is still th...\" /></i></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-29-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-29-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-29-1024.jpg?cb=1443270670\" alt=\"Secondary Indexes (Client Access)&#10;\" /></i></section><section data-index=\"30\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-30-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-30-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-30-1024.jpg?cb=1443270670\" alt=\"Secondary Indexes (with Spark)&#10;\" /></i></section><section data-index=\"31\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-31-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-31-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-31-1024.jpg?cb=1443270670\" alt=\"Full-text Indexes&#10;● Enabled via Stratio-Lucene custom index&#10;(https://github.com/Stratio/cassandra-lucene-index)&#10;● Great fo...\" /></i></section><section data-index=\"32\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-32-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-32-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-32-1024.jpg?cb=1443270670\" alt=\"Full-text Indexes&#10;CREATE CUSTOM INDEX email_index on emails(lucene)&#10;USING 'com.stratio.cassandra.lucene.Index'&#10;WITH OPTION...\" /></i></section><section data-index=\"33\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-33-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-33-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-33-1024.jpg?cb=1443270670\" alt=\"Full-text Indexes&#10;SELECT * FROM emails WHERE lucene='{&#10;filter : {type:&quot;range&quot;, field:&quot;time&quot;, lower:&quot;2015-05-26 20:29:59&quot;},...\" /></i></section><section data-index=\"34\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-34-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-34-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-34-1024.jpg?cb=1443270670\" alt=\"WIDE ROWS&#10;Caution:&#10;\" /></i></section><section data-index=\"35\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-35-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-35-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-35-1024.jpg?cb=1443270670\" alt=\"Wide Rows&#10;● It only takes one to ruin your day&#10;● Monitor cfstats for max partition bytes&#10;● Use toppartitions to find hot k...\" /></i></section><section data-index=\"36\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-36-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-36-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-36-1024.jpg?cb=1443270670\" alt=\"Avoid Nulls&#10;● Nulls are deletes&#10;● Deletes create tombstones&#10;● Don’t write nulls!&#10;● Beware of nulls in prepared statements&#10;\" /></i></section><section data-index=\"37\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-37-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-37-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-37-1024.jpg?cb=1443270670\" alt=\"Data Exploration&#10;\" /></i></section><section data-index=\"38\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-38-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-38-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-38-1024.jpg?cb=1443270670\" alt=\"Data Warehouse Paradigm - Old&#10;Ingest Model Transform Design&#10;Visualize&#10;\" /></i></section><section data-index=\"39\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-39-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-39-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-39-1024.jpg?cb=1443270670\" alt=\"Data Warehouse Paradigm - New&#10;Ingest Explore Analyze Deploy&#10;Visualize&#10;\" /></i></section><section data-index=\"40\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-40-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-40-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-40-1024.jpg?cb=1443270670\" alt=\"Visualization&#10;● Critical to understanding your data&#10;● Reduced time to visualization&#10;● … from &gt;1 month to minutes (!!)&#10;● Wa...\" /></i></section><section data-index=\"41\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-41-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-41-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-41-1024.jpg?cb=1443270670\" alt=\"Zeppelin&#10;● Open source Spark notebook&#10;● Interpreters for Scala, Python, Spark SQL,&#10;CQL, Hive, Shell, &amp; more&#10;● Data visuali...\" /></i></section><section data-index=\"42\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-42-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-42-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-42-1024.jpg?cb=1443270670\" alt=\"Zeppelin&#10;\" /></i></section><section data-index=\"43\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-43-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-43-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-43-1024.jpg?cb=1443270670\" alt=\"Zeppelin&#10;\" /></i></section><section data-index=\"44\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-44-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-44-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-44-1024.jpg?cb=1443270670\" alt=\"Zeppelin&#10;\" /></i></section><section data-index=\"45\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-45-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-45-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-45-1024.jpg?cb=1443270670\" alt=\"Final Thoughts&#10;\" /></i></section><section data-index=\"46\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-46-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-46-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-46-1024.jpg?cb=1443270670\" alt=\"Should I use DSE?&#10;● Open source culture?&#10;● On-staff C* expert(s)?&#10;● Willingness to contribute/fix stuff?&#10;● Moderate degree...\" /></i></section><section data-index=\"47\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/85/lambda-at-weather-scale-cassandra-summit-2015-47-320.jpg?cb=1443270670\" data-normal=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-47-638.jpg?cb=1443270670\" data-full=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-47-1024.jpg?cb=1443270670\" alt=\"We’re Hiring!&#10;Robbie Strickland&#10;rstrickland@weather.com&#10;\" /></i></section><div class=\"j-next-container next-container\"><div class=\"content-container\"><div class=\"next-slideshow-wrapper\"><div class=\"j-next-slideshow next-slideshow\"><p>Próximos SlideShares</p></div><p>Carregando em…5</p><p>×</p></div></div></div></div></div></div></div></div></div><div class=\"slideshow-info-container\" itemscope=\"itemscope\" itemtype=\"https://schema.org/MediaObject\"><div class=\"slideshow-tabs-container show-for-medium-up\"><ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    0 comentários\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\">\n                \n                  13 gostaram\n                \n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\">\n                Estatísticas\n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\">\n                Notas\n              </i></a>\n            </li>\n          </ul><div class=\"tabs-content\"><div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\"><ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"itemscope\"><li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"JessicaDaubner\" rel=\"nofollow\" href=\"https://pt.slideshare.net/JessicaDaubner?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Jessica Daubner\n                            \n                              \n                                \n                                \n                              \n                              \n                                \n                                \n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"luckbr1975\" rel=\"nofollow\" href=\"https://pt.slideshare.net/luckbr1975?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Luciano Resende\n                            \n                              \n                                , \n                                Big Data &amp; Analytics\n                              \n                              \n                                 at \n                                IBM\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"unicluster\" rel=\"nofollow\" href=\"https://pt.slideshare.net/unicluster?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Vijay Dhanasekaran\n                            \n                              \n                                , \n                                Principal Consultant | Entrepreneur | Emerging Technologies Advisor\n                              \n                              \n                                 at \n                                YannayTech - MicroServices, Cloud, IoT,  BlockChain\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"petervandenabeele\" rel=\"nofollow\" href=\"https://pt.slideshare.net/petervandenabeele?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Peter Vandenabeele\n                            \n                              \n                                , \n                                Big Data Architect at De Persgroep Publishing\n                              \n                              \n                                 at \n                                De Persgroep Publishing\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"PhilipFisherOgden\" rel=\"nofollow\" href=\"https://pt.slideshare.net/PhilipFisherOgden?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Philip Fisher-Ogden\n                            \n                              \n                                , \n                                Director of Engineering, Playback Services at Netflix (Now Hiring!)\n                              \n                              \n                                 at \n                                Netflix\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n              </ul><div class=\"more-container text-center\"><a href=\"#\" class=\"j-more-favs\">\n                    Exibir mais\n                    \n                  </a></div></div><div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>Sem downloads</p></div><div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>Nenhuma nota no slide</p></div></div></div><div class=\"notranslate transcript add-padding-right j-transcript\"><ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    Lambda\nat Weather Scale Robbie Strickland\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-2-638.jpg?cb=1443270670\" title=\"Who Am I?&#10;Robbie Strickland&#10;Director of Engineering, Analyt...\" target=\"_blank\">\n        2.\n      </a>\n    Who Am I?\nRobbie Strickland\nDirector of Engineering, Analytics\nrstrickland@weather.com\n@rs_atl\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-3-638.jpg?cb=1443270670\" title=\"Who Am I?&#10;● Contributor to C*&#10;community since 2010&#10;● DataSt...\" target=\"_blank\">\n        3.\n      </a>\n    Who Am I?\n● Contributor to C*\ncommunity since 2010\n● DataStax MVP 2014/15\n● Author, Cassandra High\nAvailability\n● Founder, ATL Cassandra\nUser Group\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-4-638.jpg?cb=1443270670\" title=\"About TWC&#10;● ~30 billion API requests per day&#10;● ~120 million...\" target=\"_blank\">\n        4.\n      </a>\n    About TWC\n● ~30 billion API requests per day\n● ~120 million active mobile users\n● #3 most active mobile user base\n● ~360 PB of traffic daily\n● Most weather data comes from us\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-5-638.jpg?cb=1443270670\" title=\"Use Case&#10;● Billions of events per day&#10;○ Web/mobile beacons&#10;...\" target=\"_blank\">\n        5.\n      </a>\n    Use Case\n● Billions of events per day\n○ Web/mobile beacons\n○ Logs\n○ Weather conditions + forecasts\n○ etc.\n● Keep data forever\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-6-638.jpg?cb=1443270670\" title=\"Use Case&#10;● Efficient batch + streaming analysis&#10;● Self-serv...\" target=\"_blank\">\n        6.\n      </a>\n    Use Case\n● Efficient batch + streaming analysis\n● Self-serve data science\n● BI / visualization tool support\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-7-638.jpg?cb=1443270670\" title=\"Architecture&#10;\" target=\"_blank\">\n        7.\n      </a>\n    Architecture\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-8-638.jpg?cb=1443270670\" title=\"Attempt[0] Architecture&#10;Operational&#10;Analytics&#10;Business&#10;Anal...\" target=\"_blank\">\n        8.\n      </a>\n    Attempt[0] Architecture\nOperational\nAnalytics\nBusiness\nAnalytics\nExecutive\nDashboards\nData\nDiscovery\nData\nScience\n3rd Party\nSystem\nIntegration\nEvents\n3rd Party\nOther DBs\nS3\nStream\nProcessing\nBatch\nSources\nStorage and Processing\nConsumers\nData Access\nKafka\nStreaming\nCustom\nIngestion\nPipeline\nETL\nStreaming\nSources\nRESTful\nEnqueue\nservice\nSQL\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-9-638.jpg?cb=1443270670\" title=\"Attempt[0] Data Model&#10;CREATE TABLE events (&#10;timebucket bigi...\" target=\"_blank\">\n        9.\n      </a>\n    Attempt[0] Data Model\nCREATE TABLE events (\ntimebucket bigint,\ntimestamp bigint,\neventtype varchar,\neventid varchar,\nplatform varchar,\nuserid varchar,\nversion int,\nappid varchar,\nuseragent varchar,\neventdata varchar,\ntags set&lt;varchar&gt;,\ndevicedata map&lt;varchar, varchar&gt;,\nPRIMARY KEY ((timebucket, eventtype), timestamp, eventid)\n) WITH CACHING = 'none'\nAND COMPACTION = { 'class' : 'DateTieredCompactionStrategy' };\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-10-638.jpg?cb=1443270670\" title=\"Attempt[0] Data Model&#10;CREATE TABLE events (&#10;timebucket bigi...\" target=\"_blank\">\n        10.\n      </a>\n    Attempt[0] Data Model\nCREATE TABLE events (\ntimebucket bigint,\ntimestamp bigint,\neventtype varchar,\neventid varchar,\nplatform varchar,\nuserid varchar,\nversion int,\nappid varchar,\nuseragent varchar,\neventdata varchar,\ntags set&lt;varchar&gt;,\ndevicedata map&lt;varchar, varchar&gt;,\nPRIMARY KEY ((timebucket, eventtype), timestamp, eventid)\n) WITH CACHING = 'none'\nAND COMPACTION = { 'class' : 'DateTieredCompactionStrategy' };\nEvent payload == schema-less JSON\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-11-638.jpg?cb=1443270670\" title=\"Attempt[0] Data Model&#10;CREATE TABLE events (&#10;timebucket bigi...\" target=\"_blank\">\n        11.\n      </a>\n    Attempt[0] Data Model\nCREATE TABLE events (\ntimebucket bigint,\ntimestamp bigint,\neventtype varchar,\neventid varchar,\nplatform varchar,\nuserid varchar,\nversion int,\nappid varchar,\nuseragent varchar,\neventdata varchar,\ntags set&lt;varchar&gt;,\ndevicedata map&lt;varchar, varchar&gt;,\nPRIMARY KEY ((timebucket, eventtype), timestamp, eventid)\n) WITH CACHING = 'none'\nAND COMPACTION = { 'class' : 'DateTieredCompactionStrategy' };\nPartitioned by time bucket + type\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-12-638.jpg?cb=1443270670\" title=\"Attempt[0] Data Model&#10;CREATE TABLE events (&#10;timebucket bigi...\" target=\"_blank\">\n        12.\n      </a>\n    Attempt[0] Data Model\nCREATE TABLE events (\ntimebucket bigint,\ntimestamp bigint,\neventtype varchar,\neventid varchar,\nplatform varchar,\nuserid varchar,\nversion int,\nappid varchar,\nuseragent varchar,\neventdata varchar,\ntags set&lt;varchar&gt;,\ndevicedata map&lt;varchar, varchar&gt;,\nPRIMARY KEY ((timebucket, eventtype), timestamp, eventid)\n) WITH CACHING = 'none'\nAND COMPACTION = { 'class' : 'DateTieredCompactionStrategy' };\nTime-series data good fit for DTCS\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-13-638.jpg?cb=1443270670\" title=\"Attempt[0] tl;dr&#10;● C* everywhere&#10;● Streaming data via custo...\" target=\"_blank\">\n        13.\n      </a>\n    Attempt[0] tl;dr\n● C* everywhere\n● Streaming data via custom ingest process\n● Kafka backed by RESTful service\n● Batch data via Informatica\n● Spark SQL through ODBC\n● Schema-less event payload\n● Date-tiered compaction\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-14-638.jpg?cb=1443270670\" title=\"Attempt[0] tl;dr&#10;● C* everywhere&#10;● Streaming data via custo...\" target=\"_blank\">\n        14.\n      </a>\n    Attempt[0] tl;dr\n● C* everywhere\n● Streaming data via custom ingest process\n● Kafka backed by RESTful service\n● Batch data via Informatica\n● Spark SQL through ODBC\n● Schema-less event payload\n● Date-tiered compaction\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-15-638.jpg?cb=1443270670\" title=\"Attempt[0] Lessons&#10;● Batch loading large data sets into C* ...\" target=\"_blank\">\n        15.\n      </a>\n    Attempt[0] Lessons\n● Batch loading large data sets into C* is silly\n● … and expensive\n● … and using Informatica to do it is SLOW\n● Kafka + REST services == unnecessary\n● No viable open source C* Hive driver\n● DTCS is broken (see CASSANDRA-9666)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-16-638.jpg?cb=1443270670\" title=\"Attempt[0] Lessons&#10;● Schema-less == bad:&#10;○ Must parse JSON ...\" target=\"_blank\">\n        16.\n      </a>\n    Attempt[0] Lessons\n● Schema-less == bad:\n○ Must parse JSON to extract key data\n○ Expensive to analyze by event type\n○ Cannot tune by event type\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-17-638.jpg?cb=1443270670\" title=\"Attempt[1] Architecture&#10;Data Lake&#10;Operational&#10;Analytics&#10;Bus...\" target=\"_blank\">\n        17.\n      </a>\n    Attempt[1] Architecture\nData Lake\nOperational\nAnalytics\nBusiness\nAnalytics\nExecutive\nDashboards\nData\nDiscovery\nData\nScience\n3rd Party\nSystem\nIntegration\nStream\nProcessing\nLong Term Raw Storage\nShort Term Storage and\nBig Data Processing\nConsumers\nAmazon SQS\nStreaming\nCustom\nIngestion\nPipeline\nEvents\n3rd Party\nOther DBs\nS3\nBatch\nSources\nStreaming\nSources\nETL\nData Access\nSQL\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-18-638.jpg?cb=1443270670\" title=\"Attempt[1] Data Model&#10;● Each event type gets its own table&#10;...\" target=\"_blank\">\n        18.\n      </a>\n    Attempt[1] Data Model\n● Each event type gets its own table\n● Tables individually tuned based on workload\n● Schema applied at ingestion:\n○ We’re reading everything anyway\n○ Makes subsequent analysis much easier\n○ Allows us to filter junk early\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-19-638.jpg?cb=1443270670\" title=\"Attempt[1] tl;dr&#10;● Use C* for streaming data&#10;○ Rolling time...\" target=\"_blank\">\n        19.\n      </a>\n    Attempt[1] tl;dr\n● Use C* for streaming data\n○ Rolling time window (TTL depends on type)\n○ Real-time access to events\n○ Data locality makes Spark jobs faster\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-20-638.jpg?cb=1443270670\" title=\"Attempt[1] tl;dr&#10;● Everything else in S3&#10;○ Batch data loads...\" target=\"_blank\">\n        20.\n      </a>\n    Attempt[1] tl;dr\n● Everything else in S3\n○ Batch data loads (mostly logs)\n○ Daily C* backups\n○ Stored as Parquet\n○ Cheap, scalable long-term storage\n○ Easy access from Spark\n○ Easy to share internally &amp; externally\n○ Open source Hive support\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-21-638.jpg?cb=1443270670\" title=\"Attempt[1] tl;dr&#10;● Kafka replaced by SQS:&#10;○ Scalable &amp; reli...\" target=\"_blank\">\n        21.\n      </a>\n    Attempt[1] tl;dr\n● Kafka replaced by SQS:\n○ Scalable &amp; reliable\n○ Already fronted by a RESTful interface\n○ Nearly free to operate (nothing to manage)\n○ Robust security model\n○ One queue per event type/platform\n○ Built-in monitoring\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-22-638.jpg?cb=1443270670\" title=\"Attempt[1] tl;dr&#10;● STCS in lieu of DTCS (and LCS)&#10;○ Because...\" target=\"_blank\">\n        22.\n      </a>\n    Attempt[1] tl;dr\n● STCS in lieu of DTCS (and LCS)\n○ Because it’s bulletproof\n○ Partitions spanning sstables is acceptable\n○ Testing Time-Window compaction (thanks Jeff\nJirsa)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-23-638.jpg?cb=1443270670\" title=\"Attempt[1] tl;dr&#10;● STCS in lieu of DTCS (and LCS)&#10;○ Because...\" target=\"_blank\">\n        23.\n      </a>\n    Attempt[1] tl;dr\n● STCS in lieu of DTCS (and LCS)\n○ Because it’s bulletproof\n○ Partitions spanning sstables is acceptable\n○ Testing Time-Window compaction (thanks Jeff\nJirsa)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-24-638.jpg?cb=1443270670\" title=\"Fine Print&#10;● Use C* &gt;= 2.1.8&#10;○ CASSANDRA-9637 - fixes Spark...\" target=\"_blank\">\n        24.\n      </a>\n    Fine Print\n● Use C* &gt;= 2.1.8\n○ CASSANDRA-9637 - fixes Spark input split\ncomputation\n○ CASSANDRA-9549 - fixes memory leak\n○ CASSANDRA-9436 - exposes rpc/broadcast\naddresses for Spark/cloud environments\n● Version incompatibilities abound (check sbt\nfile for Spark-Cassandra connector)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-25-638.jpg?cb=1443270670\" title=\"Fine Print&#10;● Two main Spark clusters:&#10;○ Co-located with C* ...\" target=\"_blank\">\n        25.\n      </a>\n    Fine Print\n● Two main Spark clusters:\n○ Co-located with C* for heavy analysis\n■ Predictable load\n■ Efficient C* access\n○ Self-serve in same DC but not co-located\n■ Unpredictable load\n■ Favors mining S3 data\n■ Isolated from production jobs\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-26-638.jpg?cb=1443270670\" title=\"Data Modeling&#10;\" target=\"_blank\">\n        26.\n      </a>\n    Data Modeling\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-27-638.jpg?cb=1443270670\" title=\"Partitioning&#10;● Opposite strategy from “normal” C* modeling&#10;...\" target=\"_blank\">\n        27.\n      </a>\n    Partitioning\n● Opposite strategy from “normal” C* modeling\n○ Model for good parallelism\n○ … not for single-partition queries\n● Avoid shuffling for most cases\n○ Shuffles occur when NOT grouping by partition key\n○ Partition for your most common grouping\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-28-638.jpg?cb=1443270670\" title=\"Secondary Indexes&#10;● Useful for C*-level filtering&#10;● Reduces...\" target=\"_blank\">\n        28.\n      </a>\n    Secondary Indexes\n● Useful for C*-level filtering\n● Reduces Spark workload and RAM footprint\n● Low cardinality is still the rule\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-29-638.jpg?cb=1443270670\" title=\"Secondary Indexes (Client Access)&#10;\" target=\"_blank\">\n        29.\n      </a>\n    Secondary Indexes (Client Access)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-30-638.jpg?cb=1443270670\" title=\"Secondary Indexes (with Spark)&#10;\" target=\"_blank\">\n        30.\n      </a>\n    Secondary Indexes (with Spark)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-31-638.jpg?cb=1443270670\" title=\"Full-text Indexes&#10;● Enabled via Stratio-Lucene custom index...\" target=\"_blank\">\n        31.\n      </a>\n    Full-text Indexes\n● Enabled via Stratio-Lucene custom index\n(https://github.com/Stratio/cassandra-lucene-index)\n● Great for C*-side filters\n● Same access pattern as secondary indexes\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-32-638.jpg?cb=1443270670\" title=\"Full-text Indexes&#10;CREATE CUSTOM INDEX email_index on emails...\" target=\"_blank\">\n        32.\n      </a>\n    Full-text Indexes\nCREATE CUSTOM INDEX email_index on emails(lucene)\nUSING 'com.stratio.cassandra.lucene.Index'\nWITH OPTIONS = {\n'refresh_seconds':'1',\n'schema': '{\nfields: {\nid : {type : \"integer\"},\nuser : {type : \"string\"},\nsubject : {type : \"text\", analyzer : \"english\"},\nbody : {type : \"text\", analyzer : \"english\"},\ntime : {type : \"date\", pattern : \"yyyy-MM-dd hh:mm:ss\"}\n}\n}'\n};\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-33-638.jpg?cb=1443270670\" title=\"Full-text Indexes&#10;SELECT * FROM emails WHERE lucene='{&#10;filt...\" target=\"_blank\">\n        33.\n      </a>\n    Full-text Indexes\nSELECT * FROM emails WHERE lucene='{\nfilter : {type:\"range\", field:\"time\", lower:\"2015-05-26 20:29:59\"},\nquery : {type:\"phrase\", field:\"subject\", values:[\"test\"]}\n}';\nSELECT * FROM emails WHERE lucene='{\nfilter : {type:\"range\", field:\"time\", lower:\"2015-05-26 18:29:59\"},\nquery : {type:\"fuzzy\", field:\"subject\", value:\"thingy\", max_edits:1}\n}';\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-34-638.jpg?cb=1443270670\" title=\"WIDE ROWS&#10;Caution:&#10;\" target=\"_blank\">\n        34.\n      </a>\n    WIDE ROWS\nCaution:\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-35-638.jpg?cb=1443270670\" title=\"Wide Rows&#10;● It only takes one to ruin your day&#10;● Monitor cf...\" target=\"_blank\">\n        35.\n      </a>\n    Wide Rows\n● It only takes one to ruin your day\n● Monitor cfstats for max partition bytes\n● Use toppartitions to find hot keys\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-36-638.jpg?cb=1443270670\" title=\"Avoid Nulls&#10;● Nulls are deletes&#10;● Deletes create tombstones...\" target=\"_blank\">\n        36.\n      </a>\n    Avoid Nulls\n● Nulls are deletes\n● Deletes create tombstones\n● Don’t write nulls!\n● Beware of nulls in prepared statements\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-37-638.jpg?cb=1443270670\" title=\"Data Exploration&#10;\" target=\"_blank\">\n        37.\n      </a>\n    Data Exploration\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-38-638.jpg?cb=1443270670\" title=\"Data Warehouse Paradigm - Old&#10;Ingest Model Transform Design...\" target=\"_blank\">\n        38.\n      </a>\n    Data Warehouse Paradigm - Old\nIngest Model Transform Design\nVisualize\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-39-638.jpg?cb=1443270670\" title=\"Data Warehouse Paradigm - New&#10;Ingest Explore Analyze Deploy...\" target=\"_blank\">\n        39.\n      </a>\n    Data Warehouse Paradigm - New\nIngest Explore Analyze Deploy\nVisualize\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-40-638.jpg?cb=1443270670\" title=\"Visualization&#10;● Critical to understanding your data&#10;● Reduc...\" target=\"_blank\">\n        40.\n      </a>\n    Visualization\n● Critical to understanding your data\n● Reduced time to visualization\n● … from &gt;1 month to minutes (!!)\n● Waterfall to agile\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-41-638.jpg?cb=1443270670\" title=\"Zeppelin&#10;● Open source Spark notebook&#10;● Interpreters for Sc...\" target=\"_blank\">\n        41.\n      </a>\n    Zeppelin\n● Open source Spark notebook\n● Interpreters for Scala, Python, Spark SQL,\nCQL, Hive, Shell, &amp; more\n● Data visualizations\n● Scheduled jobs\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-42-638.jpg?cb=1443270670\" title=\"Zeppelin&#10;\" target=\"_blank\">\n        42.\n      </a>\n    Zeppelin\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-43-638.jpg?cb=1443270670\" title=\"Zeppelin&#10;\" target=\"_blank\">\n        43.\n      </a>\n    Zeppelin\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-44-638.jpg?cb=1443270670\" title=\"Zeppelin&#10;\" target=\"_blank\">\n        44.\n      </a>\n    Zeppelin\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-45-638.jpg?cb=1443270670\" title=\"Final Thoughts&#10;\" target=\"_blank\">\n        45.\n      </a>\n    Final Thoughts\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-46-638.jpg?cb=1443270670\" title=\"Should I use DSE?&#10;● Open source culture?&#10;● On-staff C* expe...\" target=\"_blank\">\n        46.\n      </a>\n    Should I use DSE?\n● Open source culture?\n● On-staff C* expert(s)?\n● Willingness to contribute/fix stuff?\n● Moderate degree of risk is acceptable?\n● Need/desire for latest features?\n● Need/desire to control tool versions?\n● Don’t have the budget for licensing?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/lambdaweatherscalemin-150926122856-lva1-app6892/95/lambda-at-weather-scale-cassandra-summit-2015-47-638.jpg?cb=1443270670\" title=\"We’re Hiring!&#10;Robbie Strickland&#10;rstrickland@weather.com&#10;\" target=\"_blank\">\n        47.\n      </a>\n    We’re Hiring!\nRobbie Strickland\nrstrickland@weather.com\n \n  </li>\n              </ol></div></div></div><aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\">\n<dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recomendadas\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\"><ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"53222636\" title=\"Presentaciones orales online efectivas\" href=\"https://es.linkedin.com/learning/presentaciones-orales-online-efectivas?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Presentaciones orales online efectivas\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Presentaciones orales online efectivas\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=DwmEzrA0i7j97jkoSbD3GUg7C0s%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-kXiyv89KfZXLgfM7YZLSiol8XfiwBlgE6eu-hQznpF469LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Presentaciones orales online efectivas\n          Espanhol</p><p>Curso on-line - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"53222636\" title=\"Estrategias de comunicación para startups y pymes\" href=\"https://es.linkedin.com/learning/estrategias-de-comunicacion-para-startups-y-pymes?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Estrategias de comunicación para startups y pymes\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Estrategias de comunicación para startups y pymes\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=FsBHpddnOM6MTSXEC%2BdLLKMogOo%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-kXiao-N2fZXLqe8XXZLSiol4RcCgHlAU7e-yoQzPkGo69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Estrategias de comunicación para startups y pymes\n          Espanhol</p><p>Curso on-line - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"53222636\" title=\"Cómo comunicar de forma efectiva en la empresa\" href=\"https://es.linkedin.com/learning/como-comunicar-de-forma-efectiva-en-la-empresa?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Cómo comunicar de forma efectiva en la empresa\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Cómo comunicar de forma efectiva en la empresa\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=zRGvLjrR8qBx5WOd%2BJ6I4B%2FbLL4%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lWSGi_9CfZHXtccLaZLSiol8QcCQFkwA1fOevQjTlEI69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Cómo comunicar de forma efectiva en la empresa\n          Espanhol</p><p>Curso on-line - LinkedIn Learning</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"65872786\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Always On: Building Highly Available Applications on Cassandra\" href=\"https://pt.slideshare.net/rastrick/always-on-building-highly-available-applications-on-cassandra\">\n    \n    <div class=\"related-content\"><p>Always On: Building Highly Available Applications on Cassandra</p><p>Robbie Strickland</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"31631025\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Big Data Grows Up - A (re)introduction to Cassandra\" href=\"https://pt.slideshare.net/rastrick/big-data-growsupdevnexus\">\n    \n    <div class=\"related-content\"><p>Big Data Grows Up - A (re)introduction to Cassandra</p><p>Robbie Strickland</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"66743192\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"A Journey to Modern Apps with Containers, Microservices and Big Data\" href=\"https://pt.slideshare.net/EdwardHsu7/a-journey-to-modern-apps-with-containers-microservices-and-big-data\">\n    \n    <div class=\"related-content\"><p>A Journey to Modern Apps with Containers, Microservices and Big Data</p><p>Edward Hsu</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"55627969\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Cassandra for impatients\" href=\"https://pt.slideshare.net/CarlosAlonsoPrez/cassandra-for-impatients\">\n    \n    <div class=\"related-content\"><p>Cassandra for impatients</p><p>Carlos Alonso Pérez</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"68500942\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Scalable data modelling by example - Cassandra Summit '16\" href=\"https://pt.slideshare.net/CarlosAlonsoPrez/scalable-data-modelling-by-example-cassandra-summit-16\">\n    \n    <div class=\"related-content\"><p>Scalable data modelling by example - Cassandra Summit '16</p><p>Carlos Alonso Pérez</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"59422971\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Cassandra Scalable data modelling by example\" href=\"https://pt.slideshare.net/CarlosAlonsoPrez/cassandra-scalable-data-modelling-by-example\">\n    \n    <div class=\"related-content\"><p>Cassandra Scalable data modelling by example</p><p>Carlos Alonso Pérez</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"61594002\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Enterprise Analytics at Scale Using Graph Database\" href=\"https://pt.slideshare.net/camsemantics/enterprise-analytics-at-scale-using-graph-database\">\n    \n    <div class=\"related-content\"><p>Enterprise Analytics at Scale Using Graph Database</p><p>Cambridge Semantics</p></div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div></div><footer><div class=\"row\"><div class=\"columns\"><ul class=\"j-languages-selector language-links text-center\"><li class=\"smt-item j-www\">\n                    <a class=\"smt-link\" href=\"https://www.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" title=\"Lambda at Weather Scale - Cassandra Summit 2015 - English\" lang=\"en\" hreflang=\"en\">English\n                    </a>\n                  </li>\n                  <li class=\"smt-item j-es\">\n                    <a class=\"smt-link\" href=\"https://es.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" title=\"Lambda at Weather Scale - Cassandra Summit 2015 - Espanol\" lang=\"es\" hreflang=\"es\">Español\n                    </a>\n                  </li>\n                  <li class=\"smt-item j-pt\">\n                    <a class=\"smt-link\" href=\"https://pt.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" title=\"Lambda at Weather Scale - Cassandra Summit 2015 - Portugues\" lang=\"pt\" hreflang=\"pt\">Português\n                    </a>\n                  </li>\n                  <li class=\"smt-item j-fr\">\n                    <a class=\"smt-link\" href=\"https://fr.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" title=\"Lambda at Weather Scale - Cassandra Summit 2015 - Français\" lang=\"fr\" hreflang=\"fr\">Français\n                    </a>\n                  </li>\n                  <li class=\"smt-item j-de\">\n                    <a class=\"smt-link\" href=\"https://de.slideshare.net/rastrick/lambda-at-weather-scale-cassandra-summit-2015\" title=\"Lambda at Weather Scale - Cassandra Summit 2015 - Deutsche\" lang=\"de\" hreflang=\"de\">Deutsch\n                    </a>\n                  </li>\n                </ul></div></div>\n          <div class=\"row\"><div class=\"columns\"><ul class=\"main-links text-center\"><li><a href=\"https://pt.slideshare.net/about\">Sobre nós</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://pt.slideshare.net/terms\">Termos</a></li>\n                <li><a href=\"https://pt.slideshare.net/privacy\">Privacidade</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Direitos Autorais</a></li>\n                \n              </ul></div></div>\n          \n          <div class=\"row\"><div class=\"columns\"><p class=\"copyright text-center\">LinkedIn Corporation © 2018</p></div></div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Painéis de recortes públicos que contêm este slide</h4><hr /><p>Nenhum painel de recortes público que contém este slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h4 class=\"modal-title\">Selecionar outro painel de recortes</h4>\n    <hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Parece que você já adicionou este slide ao painel <strong class=\"default-clipboard-title\"></strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Criar painel de recortes</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h3>Você recortou seu primeiro slide!</h3>\n      \n        Recortar slides é uma maneira fácil de colecionar slides importantes para acessar mais tarde. Agora, personalize o nome do seu painel de recortes.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Descrição\n          \n        </label></h4></div>\n    <div class=\"row\"><label>Visibilidade\n        <small id=\"privacy-switch-description\">Outras pessoas podem visualizar meu painel de recortes</small>\n          </label><label for=\"privacy-switch\">\n      </label></div>\n        \n    </div>\n    \n    \n      <noscript>\n    </noscript>",
        "created_at": "2018-02-20T20:55:28+0000",
        "updated_at": "2018-03-10T20:01:21+0000",
        "published_at": null,
        "published_by": [
          "Robbie Strickland"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "pt",
        "reading_time": 7,
        "domain_name": "pt.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/lambdaweatherscalemin-150926122856-lva1-app6892-thumbnail-4.jpg?cb=1443270670",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9336"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 58,
            "label": "angular",
            "slug": "angular"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9330,
        "uid": null,
        "title": "doanduyhai/killrchat",
        "url": "https://github.com/doanduyhai/killrchat",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>A hand's on exercise for Cassandra 2.1.</p>\n<p>This hands-on will make you, step by step with unit tests, create a working chat application using</p>\n<ul><li><strong><a href=\"http://planetcassandra.org/cassandra\" rel=\"nofollow\">Apache Cassandra™</a></strong></li>\n<li><strong><a href=\"http://www.achilles.io\" rel=\"nofollow\">Achilles</a></strong></li>\n<li><strong><a href=\"http://projects.spring.io/spring-boot\" rel=\"nofollow\">Spring Boot</a></strong></li>\n<li><strong><a href=\"https://angularjs.org\" rel=\"nofollow\">AngularJS</a></strong> + <strong><a href=\"http://angular-ui.github.io/bootstrap\" rel=\"nofollow\">UI Bootstrap</a></strong></li>\n</ul><p>The hands-on will focus on the data modelling part, you need to:</p>\n<ol><li>understand the data model (tables)</li>\n<li>implement the services to make the tests pass using <strong><a href=\"http://www.achilles.io\" rel=\"nofollow\">Achilles</a></strong></li>\n</ol><p>All the front-end, as well as the REST resource and all Spring configuration config and other glue code is provided as a\nconvenience so that participants can focus solely on the data modelling and service layer.</p>\n<p>For object mapping, we use <strong><a href=\"http://www.achilles.io\" rel=\"nofollow\">Achilles</a></strong> which provides many tools to make development more effective and easier. We'll\nuse the <strong><a href=\"https://github.com/doanduyhai/Achilles/wiki/Unit-testing#usage\">JUnit rule support</a></strong> from <strong><a href=\"http://www.achilles.io\" rel=\"nofollow\">Achilles</a></strong> to start an embedded Cassandra in memory for unit testing.</p>\n<p>Once all the exercises are done, we can have some fun using the real chat!</p>\n<p>If you're not familiar with <strong>Cassandra</strong>, please take a look at the <a href=\"https://raw.github.com/doanduyhai/killrchat/master/KillrChat%20Hands%20On%20-%20Cassandra%20Intro.pdf\">introduction slides</a></p>\n<p>For a presentation of <strong>KillrChat</strong>, look at the slides <a href=\"https://raw.github.com/doanduyhai/killrchat/master/KillrChat%20Hands%20On%20-%20Exercises%20Handbook.pdf\">here</a></p>\n<blockquote>\n<p>Warning! You'll need a recent and decent browser (no IE8) to make the chat front-end work:\nIE10, Chrome, FireFox ...</p>\n</blockquote>\n<blockquote>\n<p>Warning! You should have Maven and Java (1.7+) installed and functionnal, other component will be installed automatically</p>\n</blockquote>\n<p>First clone the repository with <code>git clone https://github.com/doanduyhai/killrchat.git</code>\nThen enter the folder <code>cd killrchat</code></p>\n<h2><a href=\"#development-mode\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-development-mode\"></a>Development mode</h2>\n<p>To run the application in the development mode:</p>\n<pre>killrchat&gt; mvn clean test\nkillrchat&gt; mvn spring-boot:run -Pdev\n</pre>\n<p>When running the application in dev mode, <strong>Achilles</strong> will start an embedded Cassandra server and create\nthe following data folders:</p>\n<ol><li><code>/tmp/killrchat_cassandra/data</code></li>\n<li><code>/tmp/killrchat_cassandra/commitlog</code></li>\n<li><code>/tmp/killrchat_cassandra/saved_caches</code></li>\n</ol><p>You can change those default values in the <code>src/main/resources/config/application.properties</code> file.</p>\n<p>Then connect to the chat by opening your browser at\n<a href=\"http://localhost:8080/killrchat/index.html\" rel=\"nofollow\">http://localhost:8080/killrchat/index.html</a>.</p>\n<h2><a href=\"#production-mode\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-production-mode\"></a>Production mode</h2>\n<p>To run the application in the production mode:</p>\n<pre>killrchat&gt; mvn clean test\nkillrchat&gt; mvn spring-boot:run -Pprod\n</pre>\n<p>When running the application in prod mode, <strong>Achilles</strong> will connect to an existing Cassandra server. You can\nconfigure the server host and port in the the <code>src/main/resources/config/application.properties</code> file.\nBy default <strong>Achilles</strong> will execute the <code>src/main/resources/cassandra/schema_creation.cql</code> script to create the\n<code>killrchat</code> keyspace and appropriate tables.</p>\n<p>Then connect to the chat by opening your browser at\n<a href=\"http://localhost:8080/killrchat/index.html\" rel=\"nofollow\">http://localhost:8080/killrchat/index.html</a>.</p>\n<p>To deploy the application in multiple back-end servers, you will need to reconfigure the messaging system in the\n<strong><code>ChatRoomResource</code></strong> and <strong><code>MessageResource</code></strong>. For the hand's on, we use an in-memory messaging system but for\nproduction you'd probably want to plugin a distributed messaging broker like RabbitMQ.</p>\n<h2><a href=\"#packaging-the-application\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-packaging-the-application\"></a>Packaging the application</h2>\n<p>To package <strong>KillrChat</strong> and build a stand-alone Java jar archive, type <code>mvn package</code>. It will generate a\n<strong>killrchat-1.0.war</strong> file in the <code>target</code> folder</p>\n<p>To run the application in development mode:</p>\n<pre>&gt; java -jar killrchat-1.0.war --spring.profiles.active=dev -Dlogback.configurationFile=logback_dev.xml\n</pre>\n<p>To run the application in production mode:</p>\n<pre>&gt; java -jar killrchat-1.0.war --spring.profiles.active=prod -Dlogback.configurationFile=logback_prod.xml\n</pre>\n<p>The data model for chat room message is still not perfect because it is a wide row. Typically the partition will grow\nover time and performance will suffer.</p>\n<p>The solution is to use <strong><a href=\"http://www.datastax.com/dev/blog/advanced-time-series-with-cassandra\" rel=\"nofollow\">bucketing</a></strong> techniques but it is an advanced data modelling topic, far beyond the goal of\nthis hands-on.</p>\n<p>Alternatively, we can use the <strong><a href=\"http://www.datastax.com/dev/blog/datetieredcompactionstrategy\" rel=\"nofollow\">DateTieredCompactionStrategy</a></strong> to make reading recent messages faster.</p>\n</article>",
        "created_at": "2018-02-18T21:45:28+0000",
        "updated_at": "2018-03-10T22:03:47+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/1532977?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9330"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 12,
            "label": "video",
            "slug": "video"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9325,
        "uid": null,
        "title": "How to Fail with Apache Cassandra",
        "url": "http://www.databasetube.com/nosql/how-to-fail-with-apache-cassandra/",
        "content": "<p>Apache Cassandra is one of the most renowned NoSQL databases. Although it’s often associated with great scalability, improper usage might result in shooting yourself in the foot. This talk presents a set of ideas and guidelines – both for developers and administrators – which will help you to make your Apache Cassandra project an epic failure.</p><iframe width=\"960\" height=\"540\" src=\"https://www.youtube.com/embed/E4DL99YsoOo?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"> </iframe><p>Video producer: <a href=\"http://geecon.org/\">http://geecon.org/</a></p><h3 class=\"related_post_title\">Related Content:</h3><p>Tags:<a href=\"http://www.databasetube.com/tag/cassandra/\" rel=\"tag\">cassandra</a></p>",
        "created_at": "2018-02-17T22:43:28+0000",
        "updated_at": "2018-03-10T22:05:39+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 0,
        "domain_name": "www.databasetube.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9325"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9304,
        "uid": null,
        "title": "More Efficient Repairs in 2.1",
        "url": "https://www.datastax.com/dev/blog/more-efficient-repairs",
        "content": "<p dir=\"ltr\" style=\"text-align: justify;\">Repairs are important for every Cassandra cluster, especially when frequently deleting data. Running the <code>nodetool repair</code> command initiates the repair process on a specific node which in turn computes a Merkle tree for each range of data on that node. The <a title=\"Merkle Tree\" href=\"http://en.wikipedia.org/wiki/Merkle_tree\" target=\"_blank\">merkle tree</a> is a binary tree of hashes used by Cassandra for calculating the differences in datasets between nodes in a cluster. Every time a repair is carried out, the tree has to be calculated, each node that is involved in the repair has to construct its merkle tree from all the sstables it stores making the calculation very expensive. This allows for repairs to be network efficient as only targeted rows identified by the merkle tree as inconsistencies are sent across the network.</p><p dir=\"ltr\" style=\"text-align: justify;\">Scanning every sstable to allow for the creation of merkle trees is an expensive operation. To avoid the need for constant tree construction incremental repairs are being introduced in Cassandra 2.1. The idea is to persist already repaired data, and only calculate merkle trees for sstables that haven’t previously undergone repairs allowing the repair process to stay performant and lightweight even as datasets grow so long as repairs are run frequently.</p><p><img class=\"aligncenter\" src=\"https://www.datastax.com/wp-content/uploads/2014/02/inc-vs-regular-repair-trees1.png\" alt=\"Tree construction comparison between regular and incremental repairs\" /></p><p dir=\"ltr\" style=\"text-align: justify;\">Incremental repairs begin with the repair leader sending out a prepare message to its peers. Each node builds a merkle tree from the unrepaired sstables, which it can distinguish by the new <code>repairedAt</code> field in each sstable’s metadata. Once the leader receives a merkle tree from each node, it compares the trees and issues streaming requests, just as in the classic repair case. Finally, the leader issues an anticompaction command. Anticompaction is the process of segregating repaired and unrepaired ranges into separate sstables; repaired sstables are written with a new <code>repairedAt</code> field denoting the time of repair. Since sstable are not locked against compaction during the repair, they might get removed via compaction before the process completes. This costs us some efficiency, since they will be repaired again later, but does not harm correctness.</p><h2>Compaction with incremental repairs</h2><p dir=\"ltr\" style=\"text-align: justify;\">Maintaining separate pools of repaired and unrepaired sstables causes some extra complexity for compaction to deal with. For example, in the diagram below we repair a range covering half of the initial sstable. After repair, anticompaction splits it into a set of repaired and unrepaired sstables, at which point leveled and size-tiered compaction strategies handle segregation of the data differently.</p><p>Size-Tiered compaction takes a simple approach of splitting repaired and unrepaired sstables into separate pools, each of which is compacted independently. Leveled compaction simply performs size-tiered compaction on unrepaired data, moving into the proper levels after repair. This cuts down on write amplification compared to maintaining two leveling pools.</p><p><img class=\"aligncenter\" src=\"https://www.datastax.com/wp-content/uploads/2014/02/STCS-vs-Leveled-in-5351-repairs.png\" alt=\"SizeTiered Compaction vs Leveled compaction in incremental repairs\" /></p><h2>Migrating to incremental repairs</h2><p dir=\"ltr\" style=\"text-align: justify;\">Full repairs remain the default, largely so Cassandra doesn't have to guess the repaired state of existing sstables. Guessing that everything is fully repaired is obviously problematic; guessing that nothing is repaired is less obviously so: LCS would start size-tiering everything, since that is what it does now with unrepaired data! To avoid this, compaction remains unchanged until incremental repair is first performed and compaction detects sstables with the <code>repairedAt</code> flag.</p><p>Incremental repairs can be opted into via the <code>-inc</code> option to <code>nodetool repair</code>. This is compatible with both sequential and parallel (<code>-par</code>) repair, e.g., <code>bin/nodetool -par -inc &lt;ks&gt; &lt;cf&gt;</code>. When an sstable is fully covered by a repaired range, no anticompaction will occur, it will just rewrite the repairedAt field in sstable metadata. Recovering from missing data or corrupted sstables will require a non-incremental full repair.  (For more on anticompaction, see <a href=\"https://www.datastax.com/dev/blog/anticompaction-in-cassandra-2-1\">Marcus's post here</a>.)</p><h2>Effect of tools / commands on repair status</h2><p dir=\"ltr\" style=\"text-align: justify;\">Since the sstable’s repair status is now tracked via it’s metadata, understanding how the set of tools provided with open-source Cassandra can impact this repair status becomes important.</p><ul><li>Bulk Loading - even if repaired in a different cluster, loaded tables will be unrepaired.</li>\n<li>Scrubbing - if scrubbing results in dropping rows, new sstables will be become unrepaired, however if no bad rows are detected, the sstable will keep its original repairedAt field.</li>\n<li>Major compaction - STCS will combine each of its pools into a single sstable, one repaired and one not. Major compaction continues to have no effect under LCS.</li>\n<li>Setting Repaired Status - a new tool added in 2.1 beta 2 can be found in <code>tools/bin/sstablerepairedset</code> that allows users to mark an sstable as repaired manually allowing for an easy migration to using incremental repairs by using the <code>sstablerepairedset --is-repaired &lt;sstable&gt;</code> command. It's important to only use this tool on repaired sstables, the status of an sstable can be checked via the <code>/tools/bin/sstablemetadata</code> tool by looking at the repairedAt field.</li>\n</ul><hr /><p><a href=\"https://www.datastax.com/\">DataStax</a> has many ways for you to advance in your career and knowledge. \n</p><p>You can take <a href=\"https://academy.datastax.com/user/register?destination=home&amp;utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_register\" target=\"_self\" title=\"academy.datastax.com\">free classes</a>, <a href=\"https://academy.datastax.com/certifications?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_certifications\" target=\"_self\" title=\"academy.datastax.com/certifications\">get certified</a>, or read <a href=\"https://www.datastax.com/dbas-guide-to-nosql\" target=\"_self\" title=\"dbas-guide-to-nosql\">one of our many white papers</a>.\n</p><p><a href=\"https://academy.datastax.com/user/register?destination=home&amp;utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_register\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"academy.datastax.com\">register for classes</a>\n</p><p><a href=\"https://academy.datastax.com/certifications?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_certifications\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"academy.datastax.com/certifications\">get certified</a>\n</p><p><a href=\"http://www.datastax.com/dbas-guide-to-nosql?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_dbasguidetonosql\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"dbas-guide-to-nosql\">DBA's Guide to NoSQL</a>\n</p><br class=\"clear\" /><div id=\"mto_newsletter_121316_Css\"><p>Subscribe for newsletter:</p><br /></div>",
        "created_at": "2018-02-13T21:57:00+0000",
        "updated_at": "2018-02-15T16:32:21+0000",
        "published_at": "2014-02-26T01:40:39+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 4,
        "domain_name": "www.datastax.com",
        "preview_picture": "https://www.datastax.com/wp-content/uploads/2014/02/inc-vs-regular-repair-trees1.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9304"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9303,
        "uid": null,
        "title": "Repair in Cassandra",
        "url": "https://www.datastax.com/dev/blog/repair-in-cassandra",
        "content": "<div class=\"DXDvBlgCtaAD_wrp1  DXDvBlgCtaAD_img2\"><p>Learn more about Apache Cassandra</p></div><p>We have talked about <a href=\"https://www.datastax.com/dev/blog/more-efficient-repairs\">new</a> and <a href=\"https://www.datastax.com/dev/blog/advanced-repair-techniques\">advanced</a> features available in repair a few times, but in this post I am going to cover an overview of repair, and how to think about what happens when using it. </p><p><a href=\"https://www.datastax.com/wp-content/uploads/2014/07/10nodering.png\"><img src=\"https://www.datastax.com/wp-content/uploads/2014/07/10nodering-250x259.png\" alt=\"10 node ring\" class=\"right size-medium wp-image-24705\" srcset=\"https://www.datastax.com/wp-content/uploads/2014/07/10nodering-250x259.png 250w, https://www.datastax.com/wp-content/uploads/2014/07/10nodering-120x124.png 120w, https://www.datastax.com/wp-content/uploads/2014/07/10nodering-32x32.png 32w, https://www.datastax.com/wp-content/uploads/2014/07/10nodering.png 605w\" /></a> When thinking about repair in Cassandra you need to think in terms of token ranges not tokens. Lets start with the simple case of having one data center with 10 nodes and 100 tokens. Where node N0 is assigned token 0, N1 is assigned token 10, and so on.</p><p>With a replication factor of 3, N3 will own data for tokens 1–30. And if we look at where that data will be replicated, you get range 1–10 shared by N1, N2, N3; 11–20 shared by N2, N3, N4; 21–30 shared by N3, N4, N5.</p><p>When nodetool repair is run against a node it initiates a repair for some range of tokens. The range being repaired depends on what options are specified. The default options, just calling “nodetool repair”, initiate a repair of every token range owned by the node. The node you issued the call to becomes the coordinator for the repair operation, and it coordinates repairing those token ranges between all of the nodes that own them. </p><p>So for our example cluster, if you call “nodetool repair” on N3 the first thing that will hapen is to split the 1–30 range into the subranges based on groups of nodes which share that data. For each of those sub ranges, the 3 nodes will all compare the data in those ranges, and fix up any differences between themselves. So for sub range 1–10, N1 and N2 compare, N1 and N3 compare, N2 and N3 compare. After the repair finishes, those 3 nodes will be completely in sync for that token range. And a similar process happens for ranges 11–20 (N2, N3, N4) and 21–30 (N3, N4, N5). So just running “nodetool repair” on N3 there will be 5 nodes repairing data with each other, and at the end tokens 1–30 will be in sync across their respective owners. Now if we next run a “nodetool repair” on N4, we will repair the range 11–40 in a similar fashion. </p><p>But wait, we just repaired range 11–30, if we repair that again we will be wasting resources. This is where the “-pr” option comes in to help. When you use “nodetool repair -pr” each node picks a subset of its token range to schedule for repair, such that if “-pr” is run on EVERY node in the cluster, every token range will only be repaired once. What that means is, when ever you use -pr, you need to be repairing the entire ring (every node in every data center). If you use “-pr” on just one node, or just the nodes in one data center, you will only repair a subset of the data on those nodes. This is very important, so I’m going to say it again, if you are using “nodetool repair -pr” you must run it on <strong>EVERY</strong> node in <strong>EVERY</strong> data center, no skipping allowed. </p><p>Back to the example ring. Since we don’t want to repair a given piece of data multiple times, we are going to use the “-pr” option. When you issue “nodetool repair -pr” against node N3 only the range 21–30 will be repair. Similarly “nodetool repair -pr” on N4 will repair 31–40. As you can see we no longer have any overlapping data being repaired, but this also means, if you only ran “nodetool repair -pr” on N3, tokens 1–20 would not have been repaired. This is very important to know if you are running repair to fix a problem, and not as part of general maintenance. When running repair to fix a problem, like a node being down for longer than the hint windows, you need to repair the entire token range of that node. So you can’t just run “nodetool repair -pr” on it. You need to initiate a full “nodetool repair” on it, or do a full cluster repair with “nodetool repair -pr”. </p><p>If you have multiple data centers, by default when running repair all nodes in all data centers will sync with each other on the range being repaired. So for an RF of {DC1:3, DC2:3} for a given token range there will be 6 nodes all comparing data with each other and streaming any differences back and forth. If you have 4 data centers {DC1:3, DC2:3, DC3:3, DC4:3} you will have 12 nodes all comparing with each other and streaming data to each other at the same time for each token range. This makes using “-pr” even more important, as if you don’t use it you repair a given token range 3+3+3+3+=12 times for the 4 DC case if you ran without using “-pr” on every node in the cluster.</p><p>A new feature available in Cassandra 2.1 is <a href=\"https://www.datastax.com/dev/blog/more-efficient-repairs\">incremental repair</a>. You can see the linked blog post for an in depth description of it, but in brief, incremental repair only performs the synchronization steps described above on data that has not been repaired previously. This helps to greatly reduce the amount of time it takes to repair new data.</p><hr /><p><a href=\"https://www.datastax.com/\">DataStax</a> has many ways for you to advance in your career and knowledge. \n</p><p>You can take <a href=\"https://academy.datastax.com/user/register?destination=home&amp;utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_register\" target=\"_self\" title=\"academy.datastax.com\">free classes</a>, <a href=\"https://academy.datastax.com/certifications?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_certifications\" target=\"_self\" title=\"academy.datastax.com/certifications\">get certified</a>, or read <a href=\"https://www.datastax.com/dbas-guide-to-nosql\" target=\"_self\" title=\"dbas-guide-to-nosql\">one of our many white papers</a>.\n</p><p><a href=\"https://academy.datastax.com/user/register?destination=home&amp;utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_register\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"academy.datastax.com\">register for classes</a>\n</p><p><a href=\"https://academy.datastax.com/certifications?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_certifications\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"academy.datastax.com/certifications\">get certified</a>\n</p><p><a href=\"http://www.datastax.com/dbas-guide-to-nosql?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_dbasguidetonosql\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"dbas-guide-to-nosql\">DBA's Guide to NoSQL</a>\n</p><br class=\"clear\" /><div id=\"mto_newsletter_121316_Css\"><p>Subscribe for newsletter:</p><br /></div>",
        "created_at": "2018-02-13T21:56:55+0000",
        "updated_at": "2018-02-15T16:32:29+0000",
        "published_at": "2014-07-21T11:26:43+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 4,
        "domain_name": "www.datastax.com",
        "preview_picture": "https://www.datastax.com/wp-content/uploads/2014/07/10nodering.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9303"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 883,
            "label": "java",
            "slug": "java"
          },
          {
            "id": 996,
            "label": "monitoring",
            "slug": "monitoring"
          }
        ],
        "is_public": false,
        "id": 9298,
        "uid": null,
        "title": "Monitoring Cassandra with Prometheus",
        "url": "https://www.robustperception.io/monitoring-cassandra-with-prometheus/",
        "content": "<div class=\"post-body entry-content\"><p>Cassandra is one of many Java-based systems that offers metrics via JMX. The <a href=\"https://github.com/prometheus/jmx_exporter\">JMX Exporter</a> offers way to use these with <a href=\"https://prometheus.io\">Prometheus</a>. By following these steps you can be up and running in under a minute!</p><p><a href=\"https://www.robustperception.io/wp-content/uploads/2015/10/Screen-Shot-2017-09-06-at-16.25.53.png\"><img class=\"aligncenter size-rect-image wp-image-3233\" src=\"https://www.robustperception.io/wp-content/uploads/2015/10/Screen-Shot-2017-09-06-at-16.25.53-600x450.png\" alt=\"\" width=\"600\" height=\"450\" srcset=\"https://www.robustperception.io/wp-content/uploads/2015/10/Screen-Shot-2017-09-06-at-16.25.53-600x450.png 600w, https://www.robustperception.io/wp-content/uploads/2015/10/Screen-Shot-2017-09-06-at-16.25.53-300x225.png 300w, https://www.robustperception.io/wp-content/uploads/2015/10/Screen-Shot-2017-09-06-at-16.25.53-800x600.png 800w\" /></a></p><p>We’ll start from scratch, first we download and extract the latest Cassandra tarball:</p><pre>wget http://archive.apache.org/dist/cassandra/2.2.4/apache-cassandra-2.2.4-bin.tar.gz&#13;\ntar -xzf apache-cassandra-*-bin.tar.gz&#13;\ncd apache-cassandra-*</pre><p>We’ll also need the JMX exporter java agent, configuration, and to tell Cassandra to use it:</p><pre>wget https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.5/jmx_prometheus_javaagent-0.1.0.jar&#13;\nwget https://raw.githubusercontent.com/prometheus/jmx_exporter/master/example_configs/cassandra.yml&#13;\necho 'JVM_OPTS=\"$JVM_OPTS -javaagent:'$PWD/jmx_prometheus_javaagent-0.1.0.jar=7070:$PWD/cassandra.yml'\"' &gt;&gt; conf/cassandra-env.sh</pre><p>Now we can run Cassandra:</p><pre>./bin/cassandra &amp;</pre><p>If you visit <a href=\"http://localhost:7070/metrics\">http://localhost:7070/metrics</a> you’ll see the metrics.</p><p>Metrics alone aren’t very useful, let’s setup a quick Prometheus server:</p><pre>wget https://github.com/prometheus/prometheus/releases/download/v2.0.0/prometheus-2.0.0.linux-amd64.tar.gz&#13;\ntar -xzf prometheus-2.0.0.linux-amd64.tar.gz&#13;\ncd prometheus-*&#13;\ncat &lt;&lt;'EOF' &gt; prometheus.yml&#13;\nglobal:&#13;\n scrape_interval: 10s&#13;\n evaluation_interval: 10s&#13;\nscrape_configs:&#13;\n - job_name: 'cassandra'&#13;\n   static_configs:&#13;\n    - targets:&#13;\n      - localhost:7070&#13;\nEOF&#13;\n./prometheus&#13;\n&#13;\n</pre><p>Wait half a minute to let Prometheus gather data and then you can access the data via the <a href=\"http://localhost:9090/\">expression browser</a>!</p></div><div class=\"post-body entry-content\"><p>Have you ever wondered what percentage of time a given service or application spends up or down?</p><a href=\"https://www.robustperception.io/what-percentage-of-time-is-my-service-down-for/\" class=\"more-link\">read more</a></div><div class=\"post-body entry-content\"><p>Prometheus <a href=\"https://github.com/prometheus/prometheus/releases/tag/v2.0.0\">2.1.0</a> is now out, following on from <a href=\"https://www.robustperception.io/new-features-in-prometheus-2-0-0/\">2.0.0</a> last month with several fixes and improvements.</p><a href=\"https://www.robustperception.io/new-features-in-prometheus-2-1-0/\" class=\"more-link\">read more</a></div>",
        "created_at": "2018-02-11T16:10:18+0000",
        "updated_at": "2018-04-26T17:01:46+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 1,
        "domain_name": "www.robustperception.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9298"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 568,
            "label": "azure",
            "slug": "azure"
          },
          {
            "id": 1017,
            "label": "cosmos",
            "slug": "cosmos"
          }
        ],
        "is_public": false,
        "id": 9292,
        "uid": null,
        "title": "New Cassandra API for Azure Cosmos DB",
        "url": "http://www.youtube.com/oembed?url=https://www.youtube.com/watch?v=1Sf4McGN1AQ&format=xml",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/1Sf4McGN1AQ?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-02-11T02:15:52+0000",
        "updated_at": "2018-03-11T15:57:20+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/1Sf4McGN1AQ/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9292"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 568,
            "label": "azure",
            "slug": "azure"
          },
          {
            "id": 1017,
            "label": "cosmos",
            "slug": "cosmos"
          }
        ],
        "is_public": false,
        "id": 9291,
        "uid": null,
        "title": "Dear Cassandra Developers, welcome to Azure #CosmosDB!",
        "url": "https://azure.microsoft.com/en-us/blog/dear-cassandra-developers-welcome-to-azure-cosmosdb/",
        "content": "<div class=\"blog-author\"><div class=\"image\"><img src=\"https://gravatar.com/avatar/c9eeeb5a06664a4e4e19e63d272ab774?s=62\" alt=\"\" role=\"presentation\" /></div><p>GPM, Azure Cosmos DB + Open Source Software Analytics</p></div><div class=\"blog-postContent\" lang=\"en\"><p>Today we're excited to launch native support for <strong>Apache Cassandra API in Azure Cosmos DB</strong> – offering you Cassandra as-a-service powered by Azure Cosmos DB. You can now experience the power of <a href=\"https://azure.microsoft.com/en-us/blog/a-technical-overview-of-azure-cosmos-db/\">Azure Cosmos DB</a> platform as a managed service with the familiarity of your favorite Cassandra SDKs and tools—without any app code changes.</p><p><a href=\"https://azure.microsoft.com/en-us/blog/azure-cosmos-db-microsofts-globally-distributed-multi-model-database-service/\">Azure Cosmos DB</a> is the industry’s first fully managed globally distributed, massively scalable, and multi-model database service. It is designed to allow developers to elastically scale throughput and storage across any number of geographical regions worldwide—backed by industry-leading comprehensive <a href=\"https://azure.microsoft.com/en-us/support/legal/sla/cosmos-db/v1_1/\">SLAs</a> including throughput, availability, consistency, and &lt;10ms latency guarantees.</p><p><a href=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/21dfc07f-ebce-45d2-b956-4d4f7761d6fc.png\"><img alt=\"image\" border=\"0\" height=\"226\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/b5cce20b-327a-4d89-a1aa-c4822bf28376.png\" title=\"image\" width=\"624\" /></a></p><h2 style=\"text-align: left;\">Bring your Cassandra apps to Azure Cosmos DB in 3 simple steps:</h2><ol><li>Create a new Azure Cosmos DB account in the Azure Portal and choose the new Cassandra API while creating an Azure Cosmos DB account.</li> <li>Connect your Cassandra application to Azure Cosmos DB copying a simple connection code snippet provided to you upon creation of your new account.</li> <li>Use your favorite Cassandra tools and drivers to manage and query your Cassandra data in Azure Cosmos DB</li> </ol><p style=\"text-align: left;\">This <a href=\"https://www.youtube.com/watch?v=1Sf4McGN1AQ&amp;feature=youtu.be\">short video</a> shows how easy and quickly it is to get started with Azure Cosmos DB’s native support for Apache Cassandra API.<a href=\"https://www.youtube.com/watch?v=1Sf4McGN1AQ&amp;feature=youtu.be\"><img alt=\"image\" border=\"0\" height=\"351\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/acf435ae-0aeb-48af-b8e1-e17c22d21fbd.png\" title=\"image\" width=\"498\" /></a></p><h2>Enterprise-grade, battle-tested platform for your Cassandra apps</h2><blockquote><div>&#13;\n<p>“We are using the Cassandra API on Azure Cosmos DB for several mission-critical use cases. In particular, the geo-redundancy and dynamic scale of the solution are key advantages and we look forward to reaping more benefits in the future.”</p>&#13;\n&#13;\n<p style=\"text-align: right;\">- Christoph Leinemann, Senior Director, Data Engineering, <a href=\"https://jet.com/\">Jet.com</a></p>&#13;\n</div></blockquote><p>With Azure Cosmos DB’s native support for Cassandra APIs you will get the following benefits:</p><ul><li><strong>Fully managed, serverless Cassandra as-a-service</strong>. As a <a href=\"https://azure.microsoft.com/en-us/blog/azure-cosmos-db-microsofts-globally-distributed-multi-model-database-service/\">true PaaS service</a>, Azure Cosmos DB ensures that you do not have to worry about managing and monitoring myriad of settings across OS, JVM and YAML files and deal with complex interdependencies. Azure Cosmos DB provides first-class monitoring of throughput, latency, consistency, storage and availability and configurable alerts to take action on changes across them.</li> <li><strong>Turnkey global distribution</strong>. Azure Cosmos DB was designed as a <a href=\"https://azure.microsoft.com/en-us/blog/a-technical-overview-of-azure-cosmos-db/\">globally distributed service from the ground up</a> to ensure that your data is made available wherever your users are. The service transparently and automatically replicates the data across any number of Azure regions associated with your Cassandra tables. You can add or remove regions for your Cassandra tables with a few clicks in Azure portal or programmatically, at any time. </li> <li><strong>Elastic and transparent scaling of storage</strong>. Azure Cosmos DB provides <a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/partition-data\">automatic storage management</a> without the need for any manual intervention and grows the capacity as your application storage needs increase. You don’t need to worry about the complexities of capacity planning or having to deal with adding cluster nodes and tuning configs anymore.</li> <li><strong>Elastic scaling of throughput all around the world</strong>. With Azure Cosmos DB you don’t need to worry about tuning config settings for CPU, memory, disk IO, and compaction. Azure Cosmos DB allows you to <a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/request-units\">scale throughput</a> for your Cassandra tables all around the world and guarantees the configured throughput regardless of the volume of data being stored.</li> <li><strong>Guaranteed low latency reads and writes</strong>. As the first and only schema-agnostic database, Azure Cosmos DB automatically indexes all your data so you can perform blazing fast queries. The service offers guaranteed &lt;10 ms latencies at the 99th percentile for near real-time query results.</li> <li><strong>Multiple well-defined consistency models with clear tradeoffs</strong>. Writing correct distributed application logic against an eventually consistent database is often difficult. Azure Cosmos DB helps you by providing five well-defined, intuitive and practical consistency levels, each with a clear trade-off between desired consistency and performance, guaranteed correctness and backed by SLAs. You can now choose from <a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels\">strong, bounded staleness, session, consistent prefix and eventual consistency models</a> and can configure them any time and change on a per request basis.</li> <li><strong>Secure and compliant and enterprise-ready, by default</strong>. Azure Cosmos DB is secure, compliant and enterprise-ready service for mission-critical apps. Azure Cosmos DB has met the stringent compliance standards including <a href=\"https://www.microsoft.com/en-us/trustcenter/compliance/iso-iec-27001\">ISO 27001</a>, <a href=\"https://www.microsoft.com/en-us/trustcenter/compliance/iso-iec-27018\">ISO 27018</a>, <a href=\"https://www.microsoft.com/en-us/trustcenter/compliance/eu-model-clauses\">EUMC</a>, <a href=\"https://www.microsoft.com/en-us/trustcenter/compliance/pci\">PCI DSS</a>, <a href=\"https://www.microsoft.com/en-us/trustcenter/compliance/soc\">SOC 1,2,3</a>, <a href=\"https://www.microsoft.com/en-us/trustcenter/compliance/hipaa\">HIPAA/HITECH</a> and <a href=\"https://www.microsoft.com/en-us/trustcenter/compliance/complianceofferings\">other compliance certifications</a>. Azure Cosmos DB also provides <a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/database-encryption-at-rest\">encryption at rest</a> and in motion, <a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/firewall-support\">IP firewall</a> and <a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/logging\">audit log for your database activities</a> to ensure demanding security standards of enterprises.</li> <li><strong>Backed by industry leading, comprehensive SLAs</strong>. Azure Cosmos DB provides industry-leading, <a href=\"https://azure.microsoft.com/en-us/support/legal/sla/cosmos-db/v1_1/\">comprehensive SLAs</a> for 99.99% high availability for single region and 99,999% read availability at global scale, consistency, throughput and low latency reads and writes at the 99th percentile. Users do not need to worry about operational overheads and tuning many dozens of configuration options to get good performance. Azure Cosmos DB <a href=\"https://azure.microsoft.com/en-us/blog/azure-documentdb-service-level-agreements/\">takes away the worry of managing all these issues</a> and lets you focus on your application logic instead.</li> </ul><p>Azure Cosmos DB provides <a href=\"https://aka.ms/cassandraintro\">wire protocol level compatibility with the Cassandra API</a>. This ensures you can continue using your existing application and OSS tools with no code changes and gives you the flexibility to run your Cassandra apps fully managed with no vendor lock-in. While Azure Cosmos DB exposes APIs for the popular open source databases, it does not rely on the implementations of those  databases for realizing the semantics of the corresponding APIs. Our unique approach of providing wire-compatible APIs for the popular open sourcedatabases ensures that you can continue to use Azure Cosmos DB in a cloud-agnostic manner while still leveraging a robust database platform natively designed for the cloud.</p><p>Finally, Azure Cosmos DB has been <a href=\"https://azure.microsoft.com/en-us/blog/how-azure-documentdb-planet-scale-nosql-helps-run-microsoft-s-own-businesses/\">used extensively within Microsoft</a> over the years and by some of the largest enterprise customers with their mission critical workloads at an unprecedented global scale. You will now enjoy the same battle-tested, fully managed, globally distributed database service that provides the <a href=\"https://azure.microsoft.com/en-us/updates/documentdb-total-cost-of-non-ownership-paper-available/\">lowest TCO</a> while still using your familiar Cassandra API.</p><h2>Get started today!</h2><p>With Azure Cosmos DB, our mission is to enable the world’s developers to build amazingly powerful, cosmos-scale apps, more easily and today we are excited to welcome the Cassandra developer community!</p><p>Please <a href=\"https://aka.ms/cosmosdb-createcassandra\">sign up</a> to try the <a href=\"https://aka.ms/cassandraintro\">Apache Cassandra API</a> and the new capabilities Azure Cosmos DB can bring to your Cassandra application. After you sign up for a Cassandra API account, get started with our <a href=\"https://aka.ms/cassapidotnetqs\">Quick Start for Cassandra API using .NET</a>, <a href=\"https://aka.ms/cassapijavaqs\">Java</a>, <a href=\"https://aka.ms/cassapinodeqs\">Node.js</a> and <a href=\"https://aka.ms/cassapipython\">Python</a>.</p><p>If you need any help or have questions or feedback, please reach out to us on the <a href=\"https://stackoverflow.com/questions/tagged/azure-cosmosdb\">developer forums on Stack Overflow</a>, and follow us on Twitter <a href=\"https://twitter.com/AzureCosmosDB\">@AzureCosmosDB</a>, <a href=\"https://twitter.com/search?q=%23cosmosDB&amp;src=typd\">#CosmosDB</a>, for the latest news and announcements.</p><p>- Your friends at Azure Cosmos DB</p></div>",
        "created_at": "2018-02-11T02:15:12+0000",
        "updated_at": "2018-03-11T15:57:08+0000",
        "published_at": null,
        "published_by": [
          "Rimma Nehme"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 5,
        "domain_name": "azure.microsoft.com",
        "preview_picture": "https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/b5cce20b-327a-4d89-a1aa-c4822bf28376.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9291"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 902,
            "label": "kubernetes",
            "slug": "kubernetes"
          }
        ],
        "is_public": false,
        "id": 9284,
        "uid": null,
        "title": "vgkowski/cassandra-operator",
        "url": "https://github.com/vgkowski/cassandra-operator",
        "content": "<h3>\n      \n      Readme.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>Kubernetes version: 1.9</p>\n<p>This operator use the kubernetes code-generator for</p>\n<ul><li>clientset: used to manipulate objects defined in the CRD (cassandraCluster)</li>\n<li>informers: cache for registering to events on objects defined in the CRD</li>\n<li>listers</li>\n<li>deep copy</li>\n</ul><ol><li>Initialize the dependancies with <code>dep init</code> in the project root directory</li>\n<li>Clone the <a href=\"https://github.com/kubernetes/code-generator\">code-generator</a> repo in the <code>vendor/k8s.io</code> directory.\nBe careful of cloning the branch matching the Kubernetes version\n<code>git clone -b &lt;branch&gt; https://github.com/kubernetes/code-generator</code></li>\n<li>Run the script <code>vendor/k8s.io/code-generator/generate-groups.sh all github.com/vgkowski/cassandra-operator/pkg/client github.com/vgkowski/cassandra-operator/pkg/apis cassandra:v1</code></li>\n</ol>\n<ul><li>Currently the relationship between native Kubernetes objects and CassandraClusters is done with the name which is equal.\nFor multiple resources of the same type (services for instance) a postfix is added to the name (\"-internode\" for internode service and \"-access\" for access service)\nA better option would be to use the <code>metadata.ownerReference</code> but it requires to search and get objects by this reference instead of by name.\nFor indexed objects like Pods or PVCs the search and get is done through the label <code>CassandraCluster</code> which contains the name of the cluster</li>\n<li>Implement proper Cassandra admin logic (current code isn't working)</li>\n</ul></article>",
        "created_at": "2018-02-09T21:14:08+0000",
        "updated_at": "2018-03-11T17:23:37+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/8300357?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9284"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          }
        ],
        "is_public": false,
        "id": 9261,
        "uid": null,
        "title": "Fourth Contact with a Monolith - Instaclustr",
        "url": "https://www.instaclustr.com/fourth-contact-monolith/",
        "content": "<h6><i>“The thing’s hollow — it goes on forever — and — oh my God! — it’s full of stars!”</i></h6><h2>It’s full of Spreadsheets! (DataFrames)</h2><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/10/Fourth-Contact-with-a-monolith-Paul-Brebner-Instaclustr.gif\"><img class=\"alignnone size-full wp-image-7409\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/10/Fourth-Contact-with-a-monolith-Paul-Brebner-Instaclustr.gif\" alt=\"Fourth Contact with a monolith Paul Brebner Instaclustr\" width=\"373\" height=\"220\" /></a></p><p>Given that a dog, Laika, was the 1st astronaut to orbit the earth, it’s appropriate for a dog to travel through the wormhole.</p><p>After travelling through the wormhole, the 2001 story (book version, as is the “it’s full of stars” quote which is not in the movie) concludes.</p><p>Dave leaves the pod and explores the hotel room. He finds a telephone and telephone book, but the phone doesn’t work and the telephone book is blank. He explores more and finds a refrigerator, where there is a variety of packaged food, but it all contains the same blue substance (even, to his disappointment, in the beer cans). He eats the blue food, and drinks the tap water – which tasted terrible – because, being distilled water, it had no taste at all. He turns on a television but all the programs were two years old. Dave lies down on the bed, turns off the light and “So, for the last time, David Bowman slept.”</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/10/Fourth-Contact-with-a-monolith-Space-Odyssey.png\"><img class=\"aligncenter wp-image-7412 size-full\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/10/Fourth-Contact-with-a-monolith-Space-Odyssey.png\" alt=\"Fourth Contact with a monolith space odyssey\" width=\"599\" height=\"269\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Fourth-Contact-with-a-monolith-Space-Odyssey.png 599w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Fourth-Contact-with-a-monolith-Space-Odyssey-300x135.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Fourth-Contact-with-a-monolith-Space-Odyssey-107x48.png 107w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Fourth-Contact-with-a-monolith-Space-Odyssey-240x108.png 240w\" /></a></p><p>What’s new in this blog? DataFrames (giant Spreadsheets), ML Pipelines and Scala!</p><p>As I noted <a href=\"https://www.instaclustr.com/third-contact-monolith-part-c-pod/\">last blog, </a>the trip took a while and the TV shows are old. The ML API for Spark is now based on the DataFrame API:</p><p><a href=\"https://spark.apache.org/docs/latest/ml-guide.html\">https://spark.apache.org/docs/latest/ml-guide.html</a></p><p><b>The MLlib RDD-based API is now in maintenance mode.</b></p><p>As of Spark 2.0, the <a href=\"https://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds\">RDD</a>-based APIs in the spark.mllib package has entered maintenance mode. The primary Machine Learning API for Spark is now the <a href=\"https://spark.apache.org/docs/latest/sql-programming-guide.html\">DataFrame</a>-based API in the spark.ml package.</p><p>This architecture diagram shows the current Spark architecture. The components of particular interest are Cassandra (the data source), MLlib, DataFrames, ML Pipelines and Scala:</p><p><img class=\"alignnone\" src=\"https://dhenschen.files.wordpress.com/2015/06/spark-2015-vision.jpg\" alt=\"Apache Spark Architecture Diagram Instaclustr\" width=\"793\" height=\"559\" /></p><p><a href=\"https://www.instaclustr.com/third-contact-monolith-part-c-pod/\">Last blog</a> we explored Decision Tree Machine Learning based on RDDs, using a sample of the Instametrics monitoring data, to try and predict long JVM Garbage Collections. Now we’ll update the code to DataFrames and use all the available real monitoring data (a snapshot from the Instaclustr pre-production clusters).</p><p>What are <a href=\"https://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes\">DataFrames</a>? Big, distributed, scalable spreadsheets! They are immutable and can be transformed with a DSL, <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\">pre-defined functions, and user-defined functions.</a>  <a href=\"https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-sql-DataFrame.html\">This is a good introduction</a>. </p><h2>Scala code</h2><p>Why did I ditch Java (at least temporarily)? Have I become “transcendent” after watching 2001 too many times?</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/10/Scala-Code-Fourth-contact-with-a-monolith.png\"><img class=\"aligncenter wp-image-7414 size-full\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/10/Scala-Code-Fourth-contact-with-a-monolith.png\" alt=\"Scala Code Fourth Contact with a monolith instaclustr\" width=\"606\" height=\"351\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Scala-Code-Fourth-contact-with-a-monolith.png 606w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Scala-Code-Fourth-contact-with-a-monolith-300x174.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Scala-Code-Fourth-contact-with-a-monolith-83x48.png 83w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Scala-Code-Fourth-contact-with-a-monolith-186x108.png 186w\" /></a></p><p>Not exactly. Because Instaclustr provides a <a href=\"https://support.instaclustr.com/hc/en-us/articles/213097877-Getting-Started-with-Instaclustr-Spark-Cassandra\">fully managed cluster option which includes Cassandra, Spark and Zeppelin</a> (a Spark Scala web-based notebook) it was easier to quickly deploy, run and debug Scala code than Java. You just type Scala code into Zeppelin in a browser, run it and look at the output (and repeat). There’s no need to set up a separate client instance in AWS. Trying to learn both Scala and Spark at the same time was “fun”. Scala is syntactically similar to Java (just throw away the semi-colons as Scala is line-oriented, and you don’t need variable type declarations as types are inferred, but statically typed), so the example should still make sense even without any Scala experience (like me).</p><p>For example:</p><p>var anyGreeting = “Hello world!” // universal greeting<br />val queenGreeting = “How do you do?” // How to greet the Queen (of England)</p><p>// equivalent to<br />var anyGreeting : String = “Hello world!”<br />val queenGreeting : String = “How do you do?”</p><p>vars are mutable and can be changed, vals are immutable and can’t be changed, so:</p><p>anyGreeting = “Kia ora” // hello in Maori<br />queenGreeting = “Ow ya goin mate” // hello in Australian, won’t work as immutable<br />anyGreeting = 3.14159 // won’t work, as anyGreeting is a String</p><p>Let’s revisit the Decision Tree ML code in Scala + DataFrames.</p><p><a href=\"https://spark.apache.org/docs/latest/ml-classification-regression.html#decision-tree-classifier\">Here’s the latest documentation for the Decision Tree Classifier for DataFrames. </a></p><p>As the Instametrics monitoring data we have is in Cassandra we need to read the data into Spark from Cassandra first. To do this we use the <a href=\"https://github.com/datastax/spark-cassandra-connector/blob/master/doc/14_data_frames.md\">spark cassandra connector.</a></p><p>Instaclustr provides a <a href=\"https://support.instaclustr.com/hc/en-us/articles/213097877-Getting-Started-with-Instaclustr-Spark-Cassandra\">Spark Cassandra assembly Jar</a>.</p><p>In Zeppelin (more on Zeppelin in a future blog) this is loaded as follows:</p><p>Then lots of imports (in Zeppelin you have to run each line separately):</p><p>In Zeppelin, the spark context will already be defined for you, and there are <a href=\"https://github.com/datastax/spark-cassandra-connector/blob/master/doc/14_data_frames.md#example-using-format-helper-functions\">helper functions </a>to simplify reading in a Cassandra table. For some reason, the 1st argument to cassandraFormat is the <i>table</i> name, and the 2nd is the <i>keyspace</i>.  This returns (lazily) a DataFrame object.</p><p>val data = spark<br />.read<br />.cassandraFormat(“mllib_wide”, “instametrics”)<br />.load()</p><p>Loading lots of data from Cassandra works well, it’s fast, and DataFrames infers the schema correctly.  The format of the data DataFrame is lots of examples (Rows) with columns as follows (wide table format has each data variable/feature in a separate column):</p><p>&lt;host, bucket_time, label, metric1, metric2, metric3, …&gt;</p><p>Where did this data come from? From some complex pre-processing (next blog). The label column was also previously computed and saved (the label is the class to be learned, in this case either 1.0 for positive examples, or 0.0 for negative examples).</p><p>The next trick is to replace null values with something else (e.g. 0), as we run into problems with nulls later on (even though in theory the MLLib algorithms cope with sparse vectors).</p><p>val data2 = data.na.fill(0)</p><p>Here are the relevant documents:</p><p><a href=\"https://spark.apache.org/docs/1.3.1/api/scala/index.html#org.apache.spark.sql.DataFrame\">https://spark.apache.org/docs/1.3.1/api/scala/index.html#org.apache.spark.sql.DataFrame</a></p><p><a href=\"https://spark.apache.org/docs/1.4.0/api/java/org/apache/spark/sql/DataFrameNaFunctions.html\">https://spark.apache.org/docs/1.4.0/api/java/org/apache/spark/sql/DataFrameNaFunctions.html</a></p><p>As in the previous RDD example we need two subsets of the data, for training and testing:</p><p>val Array(trainingData, testData) = data2.randomSplit(Array(0.7, 0.3))</p><p>Next we need to produce an Array(String) of column names to be used as features. In theory all of the columns can be used as features, except the label column, but in practice some columns are not the correct type for the classifier and must be filtered out (e.g. for this example we can only have Doubles):</p><p>val featureCols = data2.columns.filter(!_.equals(“host”)).filter(!_.equals(“bucket_time”)).filter(!_.equals(“label”))</p><p>Next we need to create a single column of features. VectorAssembler is a transformer that combines a list of columns into a single vector column, which is what we want.  However, it turns out to have problems with null values (which is why we replaced them above).  </p><p>val features = new VectorAssembler()<br />.setInputCols(featureCols)<br />.setOutputCol(“features”)</p><p>Here’s the VectorAssembler documentation:</p><p><a href=\"https://spark.apache.org/docs/2.1.0/ml-features.html#vectorassembler\">https://spark.apache.org/docs/2.1.0/ml-features.html#vectorassembler</a></p><p><a href=\"https://spark.apache.org/docs/2.1.0/api/scala/index.html#org.apache.spark.ml.feature.VectorAssembler\">https://spark.apache.org/docs/2.1.0/api/scala/index.html#org.apache.spark.ml.feature.VectorAssembler</a></p><p>Next create a DecisionTreeClassifier, with the column to predict as “label”, and the features to use called “features” (from the VectorAssembler above):</p><p>val dt = new DecisionTreeClassifier()<br />.setLabelCol(“label”)<br />.setFeaturesCol(“features”)</p><p>More documentation:</p><p><a href=\"https://spark.apache.org/docs/1.5.2/ml-decision-tree.html\">https://spark.apache.org/docs/1.5.2/ml-decision-tree.html</a></p><p><a href=\"https://spark.apache.org/docs/2.0.0/api/java/org/apache/spark/ml/classification/DecisionTreeClassifier.html\">https://spark.apache.org/docs/2.0.0/api/java/org/apache/spark/ml/classification/DecisionTreeClassifier.html</a></p><h2>Pipelines, Tunnels, Wormholes</h2><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/10/Pipelines-tunnels-Wormholes-Instaclustr.png\"><img class=\"aligncenter wp-image-7422 size-full\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/10/Pipelines-tunnels-Wormholes-Instaclustr.png\" alt=\"Pipelines, tunnels, Wormholes Instaclustr\" width=\"1364\" height=\"964\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Pipelines-tunnels-Wormholes-Instaclustr.png 1364w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Pipelines-tunnels-Wormholes-Instaclustr-300x212.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Pipelines-tunnels-Wormholes-Instaclustr-768x543.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Pipelines-tunnels-Wormholes-Instaclustr-1024x724.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Pipelines-tunnels-Wormholes-Instaclustr-872x616.png 872w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Pipelines-tunnels-Wormholes-Instaclustr-640x452.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Pipelines-tunnels-Wormholes-Instaclustr-68x48.png 68w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Pipelines-tunnels-Wormholes-Instaclustr-153x108.png 153w\" /></a></p><p>Another new Spark feature is <a href=\"https://spark.apache.org/docs/2.1.1/ml-pipeline.html\">Pipelines</a> (tunnels, wormholes, etc).  A Pipeline chains multiple Transformers and Estimators together for a ML workflow. A Transformer is an algorithm (feature transformers or learned models) that can transform one DataFrame into another DataFrame (e.g. a ML model transforms a DataFrame with labels and features into a DataFrame with predictions). An Estimator is an algorithm which can be fit on a DataFrame to produce a Model (which is a Transformer). This Pipeline consists of only two stages, features (feature transformer) and dt (estimator) (More on <a href=\"https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-mllib/spark-mllib-models.html#PipelineModel\">stages</a>):</p><p>val pipeline = new Pipeline()<br />.setStages(Array(features, dt))</p><p>Now we actually train the model by calling the pipeline fit method on the trainingData:</p><p>val model = pipeline.fit(trainingData)</p><p>And then test it by applying the model to the testData. This produces a DataFrame with a “predictions” column (by default), a “label” column (actual class), and the “features” column:</p><p>val predictions = model.transform(testData)<br />predictions.select(“prediction”, “label”, “features”)</p><p>  The <a href=\"https://spark.apache.org/docs/2.0.1/api/java/org/apache/spark/ml/evaluation/MulticlassClassificationEvaluator.html\">MulticlassClassificationEvaluator</a> can compute a limited number of evaluation metrics, here’s an example for “accuracy”:</p><p>val evaluator = new MulticlassClassificationEvaluator()<br />        .setLabelCol(“label”)<br />.setPredictionCol(“prediction”)<br />.setMetricName(“accuracy”)</p><p>val accuracy = evaluator.evaluate(predictions)<br />println(“Test Error = ” + (1.0 – accuracy))</p><p>However,  as pointed out the previous blog the model accuracy isn’t a particularly useful metric depending on the actual ratio of positive/negative examples.  We can use the <i>MulticlassMetrics</i> to compute the confusion matrix, precision, and recall. And then also compute the actual rate of negative examples for comparison with the model accuracy metric. <a href=\"https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/evaluation/MulticlassMetrics.scala\"><i>MulticlassMetrics</i></a> has an auxiliary constructor for DataFrames but I still had to convert to an RDD before use. See <a href=\"https://books.google.com.au/books?id=NJwnDwAAQBAJ&amp;pg=PA78&amp;lpg=PA78&amp;dq=MulticlassMetrics+Dataframe&amp;source=bl&amp;ots=fcuHIgop3m&amp;sig=1-VuKMSYhBGyCGFxhvb2rnGeuA0&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjJgvnWlefWAhXLwLwKHWNVD6EQ6AEISzAG#v=onepage&amp;q=MulticlassMetrics%20Dataframe&amp;f=false\">this book f</a>or more information and ways of creating a confusion matrix from a DataFrame directly.</p><p>And now print the model out.</p><p>val treeModel = model.stages(1).asInstanceOf[DecisionTreeClassificationModel]<br />println(“Learned classification tree model:\\n” + treeModel.toDebugString)</p><p>The decision tree actually doesn’t make much “sense” as the named features have been replaced by feature numbers (e.g. If (feature 23 &lt;= 8720.749999999996) Predict 0.0 Else etc). Ideally there should be some way of automatically converting them back to the original names.</p><h2>Results</h2><p>Did it work? I’m sorry, I can’t tell you. In theory after going through the wormhole, information flow to the outside world is limited (the telephone didn’t work).  Ok, we’re not really in some virtual reality hotel/zoo at the other end of the universe (probably), and Dave did travel back to this universe, so I can tell you…</p><p>For this example we used real data, but obtained from our pre-production clusters. Rather than pre-process all the raw data I took a shortcut and use the rolled up data which had min, avg and max values computed for every metric over 5 minute periods. However, I forgot to check the bucket_time values, and later discovered that the bucket_time was actually hourly, so I ended up using hours as the default time period for learning. I also noticed that the JVM GC durations had not been rolled up for this snapshot of data, so I used these read and write SLA metrics to compute the label:</p><p>/cassandra/sla/latency/read_avg(avg)<br />/cassandra/sla/latency/read_max(max)<br />/cassandra/sla/latency/write_avg(avg)<br />/cassandra/sla/latency/write_max(max)</p><p>These metrics were combined with thresholds to ensure that about 5% of the examples were positive (i.e. had “long” read/write times). And these metrics were removed from the data before use to prevent cheating by the machine learning algorithm.</p><p>The wide data table read from Cassandra had 1518 examples (rows), which was split into training and test data. The training data set had 1067 examples, but only 60 positive examples (not a huge number for a Big Data problem, and in practice the number of examples needs to be &gt;&gt; the number of features, at least 2x). There were 2839 features (columns).</p><p>Learning only took a few minutes. Model accuracy was 0.96, however, as the negative example rate was 0.95 this isn’t much better than guessing. Precision was 0.60 and Recall was 0.66.</p><p>Is this result any good? A precision of 0.6 means that given a <i>prediction</i> of a SLA violation by the model, it is likely to be correct 60% of the time (but gets it wrong 40% of the time). A recall of 0.66 means that given an <i>actual</i> SLA violation, the model will correctly predict it 66% of the time (i.e. but misses it 34% of the time).  By comparison, using a sample of the data in the previous blog the recall was only 45%, so 66% is an improvement! Assuming that the cost of checking a SLA violation warning (from the model prediction) is minimal, then it’s more important to correctly predict as many actual SLA violations (assuming we can do something to prevent/mitigate them in advance, and that SLA violations are expensive if they occur), and increase the recall accuracy.   How could this be done? More and better quality data (e.g. from production clusters, as pre-production clusters and metrics are somewhat atypical, for example, many nodes may be spun up and down for short time periods, and they don’t have typical user workloads running on them), reducing the number of features (e.g. <a href=\"https://spark.apache.org/docs/2.1.0/ml-features.html#feature-selectors\">feature selection</a>, by removing redundant or highly correlated features), and different ML algorithms… </p><p>What features did the model pick to use to predict the long SLAs times from the 2839 features available? Only these 11 (which appear to be plausible):</p><ol><li>/cassandra/jvm/memory/heapMemoryUsage/max</li>\n<li>/cassandra/jvm/memory/heapMemoryUsage/used_avg</li>\n<li>/cassandra/metrics/type=ClientRequest/scope=Read/name=Latency/95thPercentile_avg</li>\n<li>/cassandra/metrics/type=ClientRequest/scope=Read/name=Latency/count_max</li>\n<li>/cassandra/metrics/type=ClientRequest/scope=Read/name=Latency/latency_per_operation_avg</li>\n<li>/cassandra/metrics/type=ClientRequest/scope=Write/name=Latency/95thPercentile_avg</li>\n<li>/cassandra/metrics/type=ClientRequest/scope=Write/name=Latency/latency_per_operation_avg</li>\n<li>/cassandra/metrics/type=ColumnFamily/keyspace=test/scope=testuncompressed/name=TotalDiskSpaceUsed_max</li>\n<li>/proc/diskstats/xvda1/sectorsWritten_max</li>\n<li>/proc/diskstats/xvdx/sectorsRead_avg</li>\n<li>/proc/stat/cpu-agg/user_max</li>\n</ol><p>Thanks to Instaclustr people for help (with Instametrics data, support for Cassandra, Spark and Zeppelin, and machine learning discussions) including Christophe, Jordan, Alwyn, Alex_I, Juan, Joe &amp; Jen.</p><h2>Next Blog</h2><p>How did we get here? Next blog we’ll look behind the scenes at the code that was used to preprocess the raw metrics from Cassandra, clean the data, convert it into the wide table format, and correctly label the examples. And, how to run Spark Scala code in Zeppelin!</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/10/How-to-run-Spark-Scala-Code-in-Zeppelin-Instaclustr.png\"><img class=\"size-full wp-image-7425 aligncenter\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/10/How-to-run-Spark-Scala-Code-in-Zeppelin-Instaclustr.png\" alt=\"How to run Spark Scala Code in Zeppelin Instaclustr\" width=\"1248\" height=\"778\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/How-to-run-Spark-Scala-Code-in-Zeppelin-Instaclustr.png 1248w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/How-to-run-Spark-Scala-Code-in-Zeppelin-Instaclustr-300x187.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/How-to-run-Spark-Scala-Code-in-Zeppelin-Instaclustr-768x479.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/How-to-run-Spark-Scala-Code-in-Zeppelin-Instaclustr-1024x638.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/How-to-run-Spark-Scala-Code-in-Zeppelin-Instaclustr-966x602.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/How-to-run-Spark-Scala-Code-in-Zeppelin-Instaclustr-640x399.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/How-to-run-Spark-Scala-Code-in-Zeppelin-Instaclustr-77x48.png 77w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/How-to-run-Spark-Scala-Code-in-Zeppelin-Instaclustr-173x108.png 173w\" /></a></p><h2>Instaclustr SPARK trial offer</h2><h5>You can try out a special offer of an Instaclustr trial cluster provisioned with <b>Cassandra, Spark and Zepplin </b>using the coupon code ST2M14:</h5><p><a href=\"https://console.instaclustr.com/user/signup?coupon-code=ST2M14\">https://console.instaclustr.com/user/signup?coupon-code=ST2M14</a></p><h2>CODE</h2>",
        "created_at": "2018-02-03T17:19:16+0000",
        "updated_at": "2018-03-11T17:42:41+0000",
        "published_at": "2017-10-20T11:45:45+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 11,
        "domain_name": "www.instaclustr.com",
        "preview_picture": "https://www.instaclustr.com/wp-content/uploads/2017/10/Fourth-Contact-with-a-monolith-Paul-Brebner-Instaclustr.gif",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9261"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 12,
            "label": "video",
            "slug": "video"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 996,
            "label": "monitoring",
            "slug": "monitoring"
          }
        ],
        "is_public": false,
        "id": 9259,
        "uid": null,
        "title": "Monitoring Cassandra: Don't Miss a Thing (Alain Rodriguez, The Last Pickle) | C* Summit 2016",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=Q9AAR4UQzMk",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/Q9AAR4UQzMk?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-02-03T17:12:35+0000",
        "updated_at": "2018-03-12T15:37:21+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/Q9AAR4UQzMk/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9259"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9258,
        "uid": null,
        "title": "Monitoring Cassandra garbage collector",
        "url": "https://medium.com/@mlowicki/monitoring-cassandra-garbage-collector-83c8a515e403",
        "content": "<section class=\"section section--body section--first\"><div class=\"section-divider\"><hr class=\"section-divider\" /></div><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><figure id=\"c4a2\" class=\"graf graf--figure graf--leading\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*8uxzqct_Xvj5hzFqG3N0Rw.png\" data-width=\"716\" data-height=\"292\" data-action=\"zoom\" data-action-value=\"1*8uxzqct_Xvj5hzFqG3N0Rw.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*8uxzqct_Xvj5hzFqG3N0Rw.png\" alt=\"image\" /></div></div></figure><h1 id=\"ea01\" class=\"graf graf--h3 graf-after--figure graf--title\">Monitoring Cassandra garbage collector</h1><p id=\"2ff6\" class=\"graf graf--p graf-after--h3 graf--trailing\">Having database hiccups is the last thing you want on production. Very common cause of misbehaving nodes are long GC pauses — while running f.ex. full GC node doesn’t handle requests and if cycle is long enough then requests will timeout. I’ll describe how we monitor garbage collector in Cassandra clusters used by <a href=\"http://www.opera.com/computer/features/sync\" data-href=\"http://www.opera.com/computer/features/sync\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Opera sync</a> so you can easily see affected nodes, spot patterns which are great help while debugging issues or tuning configuration.</p></div></div></section><section class=\"section section--body\"><div class=\"section-divider\"><hr class=\"section-divider\" /></div><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"55c7\" class=\"graf graf--p graf--leading\">You can configure C* to output GC logs to dedicated file through <em class=\"markup--em markup--p-em\">cassandra-env.sh</em>. Edit this file and uncomment:</p><pre id=\"2093\" class=\"graf graf--pre graf-after--p\">JVM_OPTS=\"$JVM_OPTS -XX:+PrintGCDateStamps\"<br />JVM_OPTS=\"$JVM_OPTS -Xloggc:/var/log/cassandra/gc.log\"</pre><p id=\"6915\" class=\"graf graf--p graf-after--pre\">When done C* needs to be restarted. Soon after restart you should see in <em class=\"markup--em markup--p-em\">gc.log</em> entries like:</p><pre id=\"72c1\" class=\"graf graf--pre graf-after--p\">2015–12–07T09:52:01.159+0000: 247122.436: [GC 6496818K-&gt;4840023K(8178944K), 0.0380030 secs]</pre><p id=\"3fca\" class=\"graf graf--p graf-after--pre\">How to parse and visualise these logs?</p><p id=\"4a03\" class=\"graf graf--p graf-after--p\">We’re using <a href=\"https://www.elastic.co/products/logstash\" data-href=\"https://www.elastic.co/products/logstash\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Logstash</a> to parse logs and <a href=\"https://www.elastic.co/products/kibana\" data-href=\"https://www.elastic.co/products/kibana\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Kibana</a> to display diagrams. GC-specific patterns are defined as follows:</p><pre id=\"d15f\" class=\"graf graf--pre graf-after--p\">FLOAT %{INT}\\.[0–9]+<br />GCTYPE (GC)|(Full GC)<br />GCREASON [^)]+<br />JVMGCLOG (%{TIMESTAMP_ISO8601:timestamp}: )?%{FLOAT}: (#%{INT}: )?\\[%{GCTYPE:gc_type} (\\(%{GCREASON:gc_reason}\\) )?%{INT:gc_memory_before:int}K-&gt;%{INT:gc_memory_after:int}K\\(%{INT}K\\), %{FLOAT:gc_duration:float} secs\\]</pre><p id=\"c53f\" class=\"graf graf--p graf-after--pre\">and configuration to parse desired entries:</p><pre id=\"666b\" class=\"graf graf--pre graf-after--p\">input {<br />  file {<br />    type =&gt; \"cassandra-gc\"<br />    path =&gt; \"/var/log/cassandra/gc.log\"<br />  }<br />}</pre><pre id=\"a576\" class=\"graf graf--pre graf-after--pre\">filter {<br />  if [type] == \"cassandra-gc\" {<br />    grok {<br />      patterns_dir =&gt; [ \"/etc/logstash/conf.d/patterns\" ]<br />      match =&gt; [ \"message\", \"%{JVMGCLOG}\" ]<br />      remove_field =&gt; [ \"message\" ]<br />    }</pre><pre id=\"9010\" class=\"graf graf--pre graf-after--pre\">    mutate {<br />      add_field =&gt; [ \"program\", \"cassandra-gc\" ]<br />      add_field =&gt; [ \"fqdn\", \"db8.sync.ams.osa\" ]<br />    }</pre><pre id=\"38df\" class=\"graf graf--pre graf-after--pre\">    if [gc_type] == \"Full GC\" {<br />      mutate {<br />        replace =&gt; [ \"gc_type\", \"Full\" ]<br />      }<br />    }<br />  }<br />}</pre><pre id=\"ad81\" class=\"graf graf--pre graf-after--pre\">output {<br />  if [type] in [ \"cassandra-gc\" ] {<br />    redis {<br />      host =&gt; \"logs.sync.ams.osa\"<br />      data_type =&gt; \"list\"<br />      key =&gt; \"logstash\"<br />    }<br />  }<br />}</pre><p id=\"89d1\" class=\"graf graf--p graf-after--pre\">To each log we’re adding “fqdn” field with hostname set to it. The value of this field is set by <em class=\"markup--em markup--p-em\">Puppet</em> in our case. Having “fqdn” will allow us to filter based on specific host or datacenter (f.ex “ams.osa” to get only boxes in Amsterdam).</p><p id=\"57f6\" class=\"graf graf--p graf-after--p\">Having everything in Elasticsearch (passed through Redis) we can easily add dashboards in Kibana:</p><figure id=\"0735\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*FLSdmh1q0B8iq20CZAvvvA.png\" data-width=\"1024\" data-height=\"310\" data-action=\"zoom\" data-action-value=\"1*FLSdmh1q0B8iq20CZAvvvA.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*FLSdmh1q0B8iq20CZAvvvA.png\" alt=\"image\" /></div></div></figure><figure id=\"cc8e\" class=\"graf graf--figure graf-after--figure\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*iXa4XXu51ANzlKfIC9TlXg.png\" data-width=\"884\" data-height=\"319\" data-action=\"zoom\" data-action-value=\"1*iXa4XXu51ANzlKfIC9TlXg.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*iXa4XXu51ANzlKfIC9TlXg.png\" alt=\"image\" /></div></div></figure><figure id=\"2752\" class=\"graf graf--figure graf-after--figure\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*eOiCcGBeVkWrMUA0exEjHA.png\" data-width=\"880\" data-height=\"320\" data-action=\"zoom\" data-action-value=\"1*eOiCcGBeVkWrMUA0exEjHA.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*eOiCcGBeVkWrMUA0exEjHA.png\" alt=\"image\" /></div></div></figure><figure id=\"9848\" class=\"graf graf--figure graf-after--figure\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*oP34AjE2o_539IESIDC4jA.png\" data-width=\"878\" data-height=\"325\" data-action=\"zoom\" data-action-value=\"1*oP34AjE2o_539IESIDC4jA.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*oP34AjE2o_539IESIDC4jA.png\" alt=\"image\" /></div></div></figure><p id=\"45fe\" class=\"graf graf--p graf-after--figure\">Setup we’ve can be applied to any tool on top of JVM where you want to monitor garbage collector. Logstash can be also helpful if you want to extract other information from Cassandra’s system.log like information about compacting large partitions:</p><pre id=\"f651\" class=\"graf graf--pre graf-after--p\">WARN [CompactionExecutor:77446] 2015–12–07 08:02:40,920 SSTableWriter.java:241 — Compacting large partition sync/entity2:85373422:32904 (105951664 bytes)</pre><p id=\"b1c2\" class=\"graf graf--p graf-after--pre\">Such information is easily parseable and is a great help while deciding if changes in database schema are required:</p><figure id=\"9559\" class=\"graf graf--figure graf-after--p graf--trailing\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*1fkSEFVgIiAiTOWxTLhbCg.png\" data-width=\"752\" data-height=\"354\" data-action=\"zoom\" data-action-value=\"1*1fkSEFVgIiAiTOWxTLhbCg.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*1fkSEFVgIiAiTOWxTLhbCg.png\" alt=\"image\" /></div></div></figure></div></div></section><section class=\"section section--body section--last\"><div class=\"section-divider\"><hr class=\"section-divider\" /></div><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"aa5d\" class=\"graf graf--p graf--leading\">Using tool like <em class=\"markup--em markup--p-em\">jconsole</em>,<em class=\"markup--em markup--p-em\"> jstat </em>or<em class=\"markup--em markup--p-em\"> jvisualvm</em> is extremely helpful while incidents but to have a bigger picture you need to have a history to detect patterns or make sure everything was fine last night or during last weekend. This is why we’re friends now with Logstash and Kibana. Right away you can say if current incident is caused by GC or you should start looking for problems in other places.</p><p id=\"438c\" class=\"graf graf--p graf-after--p graf--trailing\">Having proper monitoring of garbage collector accompanied with tons of other <a href=\"http://medium.com/@mlowicki/monitoring-cassandra-a7fde5b2de9c\" data-href=\"http://medium.com/@mlowicki/monitoring-cassandra-a7fde5b2de9c\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">metrics</a> save us lots of time needed to diagnose issues or prove that latest changes in configuration actually make things better.</p></div></div></section>",
        "created_at": "2018-02-03T17:11:14+0000",
        "updated_at": "2018-02-15T16:32:55+0000",
        "published_at": "2015-12-07T10:58:44+0000",
        "published_by": [
          "Michał Łowicki"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 2,
        "domain_name": "medium.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*8uxzqct_Xvj5hzFqG3N0Rw.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9258"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 996,
            "label": "monitoring",
            "slug": "monitoring"
          }
        ],
        "is_public": false,
        "id": 9257,
        "uid": null,
        "title": "Cassandra metrics and their use in Grafana",
        "url": "https://medium.com/@mlowicki/cassandra-metrics-and-their-use-in-grafana-1f0dc33f9cca",
        "content": "<section class=\"section section--body section--first\"><div class=\"section-divider\"><hr class=\"section-divider\" /></div><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><h1 id=\"63c7\" class=\"graf graf--h3 graf--leading graf--title\">Cassandra metrics and their use in Grafana</h1><p id=\"7a4d\" class=\"graf graf--p graf-after--h3\">During our adventure of adopting Cassandra for <a href=\"https://www.opera.com/computer/features/sync\" data-href=\"https://www.opera.com/computer/features/sync\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">sync feature</a> we’ve been hit by many things. Most of the time it was because we weren’t proficient with C* yet and equipped with detailed knowledge and experience in many areas related to it. Sometimes it also happened because of some nasty bugs either in our infrastructure or inside database itself.</p><p id=\"b9dd\" class=\"graf graf--p graf-after--p\">We would safe lots of energy and frustrating days at work having solid monitoring from the very beginning, allowing us to detect anomalies very early. Far too often we’re discovering metrics skyrocketing by running <a href=\"https://wiki.apache.org/cassandra/NodeTool\" data-href=\"https://wiki.apache.org/cassandra/NodeTool\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">nodetool</a> command during the work on usual tasks and not having proper charts with historic data. We’ve learned over time what to monitor and visualise so hopefully solution presented below will save lots of coffee and stress to anyone who’s seriously starting his journey with Cassandra. Many of changes we made to our monitoring infrastructure were dictated by incidents we had so this text is also a collection of our war stories. You can read about our infrastructure in previous posts:</p><ul class=\"postList\"><li id=\"9e0e\" class=\"graf graf--li graf-after--p\"><a href=\"http://medium.com/@mlowicki/statsd-centered-monitoring-setup-14d6eb391fcd\" data-href=\"http://medium.com/@mlowicki/statsd-centered-monitoring-setup-14d6eb391fcd\" class=\"markup--anchor markup--li-anchor\" target=\"_blank\">StatsD-centered monitoring setup</a></li><li id=\"38f7\" class=\"graf graf--li graf-after--li\"><a href=\"http://medium.com/@mlowicki/monitoring-cassandra-a7fde5b2de9c\" data-href=\"http://medium.com/@mlowicki/monitoring-cassandra-a7fde5b2de9c\" class=\"markup--anchor markup--li-anchor\" target=\"_blank\">Monitoring Cassandra</a></li><li id=\"ed23\" class=\"graf graf--li graf-after--li\"><a href=\"https://medium.com/@mlowicki/monitoring-cassandra-garbage-collector-83c8a515e403\" data-href=\"https://medium.com/@mlowicki/monitoring-cassandra-garbage-collector-83c8a515e403\" class=\"markup--anchor markup--li-anchor\" target=\"_blank\">Monitoring Cassandra garbage collector</a></li></ul><p id=\"5f9e\" class=\"graf graf--p graf-after--li\">Here I’ll focus on final output — charts in <a href=\"http://grafana.org\" data-href=\"http://grafana.org\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Grafana</a> which is used extensively by us and is fed by couple of collecting tools of our choice. Till the end of this post I’ll omit prefix “sync.$environment.$datacenter.$host.” of each metric for readability.</p><p id=\"e5cb\" class=\"graf graf--p graf-after--p\">At the moment we’ve two major dashboards. First one called <a href=\"https://gist.github.com/mlowicki/304ecff7c35a99c46ef48e58bd271b57\" data-href=\"https://gist.github.com/mlowicki/304ecff7c35a99c46ef48e58bd271b57\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">“Sync database -general”</a> contains generic things not strictly related to Cassandra itself but required to spot anomalies related to slow disk I/O, network issues or CPU.</p><p id=\"d5fe\" class=\"graf graf--p graf-after--p\">Data are gathered mainly by <a href=\"http://github.com/python-diamond/Diamond\" data-href=\"http://github.com/python-diamond/Diamond\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Diamond</a>.</p><h4 id=\"a5d1\" class=\"graf graf--h4 graf-after--p\">CPU</h4><figure id=\"91b6\" class=\"graf graf--figure graf-after--h4\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*7dCDUqQgWvT1vdYRb1KsoQ.png\" data-width=\"2500\" data-height=\"980\" data-action=\"zoom\" data-action-value=\"1*7dCDUqQgWvT1vdYRb1KsoQ.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*7dCDUqQgWvT1vdYRb1KsoQ.png\" alt=\"image\" /></div></div></figure><figure id=\"6d4e\" class=\"graf graf--figure graf--iframe graf-after--figure\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><div class=\"iframeContainer\"><iframe width=\"700\" height=\"250\" src=\"https://medium.com/media/b39583aad2051289dbf2c64cbb40aa91?postId=1f0dc33f9cca\" data-media-id=\"b39583aad2051289dbf2c64cbb40aa91\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\">[embedded content]</iframe></div></div></div></figure><p id=\"f70c\" class=\"graf graf--p graf-after--figure\">Metrics are provided by <a href=\"http://diamond.readthedocs.org/en/latest/collectors/LoadAverageCollector/\" data-href=\"http://diamond.readthedocs.org/en/latest/collectors/LoadAverageCollector/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">LoadAverageCollector</a>. On several occasions we get some old hanging processes so number of total and running processes could be useful.</p><h4 id=\"4c53\" class=\"graf graf--h4 graf-after--p\">Memory</h4><p id=\"d660\" class=\"graf graf--p graf-after--h4\">There were many memory leaks in 2.1.x branch (f.ex. <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-9549\" data-href=\"https://issues.apache.org/jira/browse/CASSANDRA-9549\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">CASSANDRA-9549</a> or <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-9681\" data-href=\"https://issues.apache.org/jira/browse/CASSANDRA-9681\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">CASSANDRA-9681</a>) so this one helps out to detect them:</p><pre id=\"0f56\" class=\"graf graf--pre graf-after--p\">gauges.memory.MemAvailable</pre><p id=\"ab5a\" class=\"graf graf--p graf-after--pre\">It’s important to not use <em class=\"markup--em markup--p-em\">MemFree</em> here which is something <a href=\"https://github.com/python-diamond/Diamond/issues/108\" data-href=\"https://github.com/python-diamond/Diamond/issues/108\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">very different</a> .</p><figure id=\"d8f8\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*gijpkEzT-7X0hpbl75M_HA.png\" data-width=\"2500\" data-height=\"968\" data-action=\"zoom\" data-action-value=\"1*gijpkEzT-7X0hpbl75M_HA.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*gijpkEzT-7X0hpbl75M_HA.png\" alt=\"image\" /></div></div></figure><h4 id=\"28a0\" class=\"graf graf--h4 graf-after--figure\">Network</h4><figure id=\"8e2f\" class=\"graf graf--figure graf-after--h4\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*oaT71TFhahjRMlsrdCNkiw.png\" data-width=\"2048\" data-height=\"898\" data-action=\"zoom\" data-action-value=\"1*oaT71TFhahjRMlsrdCNkiw.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*oaT71TFhahjRMlsrdCNkiw.png\" alt=\"image\" /></div></div></figure><p id=\"7e7d\" class=\"graf graf--p graf-after--figure\">We’ve there f.ex:</p><figure id=\"f958\" class=\"graf graf--figure graf--iframe graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><div class=\"iframeContainer\"><iframe width=\"700\" height=\"250\" src=\"https://medium.com/media/50e084c1d1fa38589524e84720129dea?postId=1f0dc33f9cca\" data-media-id=\"50e084c1d1fa38589524e84720129dea\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\">[embedded content]</iframe></div></div></div></figure><p id=\"b9f6\" class=\"graf graf--p graf-after--figure\">and similar for transferred data (tx_byte, tx_errors, …) handy to debug things together with NetOps team.</p><h4 id=\"80de\" class=\"graf graf--h4 graf-after--p\">Disks</h4><figure id=\"a401\" class=\"graf graf--figure graf-after--h4\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*lo2Zo6mWtHkXeup_eOYTag.png\" data-width=\"2048\" data-height=\"922\" data-action=\"zoom\" data-action-value=\"1*lo2Zo6mWtHkXeup_eOYTag.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*lo2Zo6mWtHkXeup_eOYTag.png\" alt=\"image\" /></div></div></figure><p id=\"5042\" class=\"graf graf--p graf-after--figure\">This is quite large group (row) but really helpful in making sure our SSD disks are behaving well and aren’t saturated. It contains metrics like:</p><figure id=\"400f\" class=\"graf graf--figure graf--iframe graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><div class=\"iframeContainer\"><iframe width=\"700\" height=\"250\" src=\"https://medium.com/media/b87f5d8694c7ac9c894d570eeff195b6?postId=1f0dc33f9cca\" data-media-id=\"b87f5d8694c7ac9c894d570eeff195b6\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\">[embedded content]</iframe></div></div></div></figure><p id=\"871d\" class=\"graf graf--p graf-after--figure\">provided by Diamond’s <a href=\"http://diamond.readthedocs.org/en/latest/collectors/DiskUsageCollector/\" data-href=\"http://diamond.readthedocs.org/en/latest/collectors/DiskUsageCollector/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">DiskUsageCollector</a>.</p><h4 id=\"b846\" class=\"graf graf--h4 graf-after--p\">StatsD</h4><figure id=\"5c47\" class=\"graf graf--figure graf-after--h4\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*PuTkol6xQmSUEONzGjbGiA.png\" data-width=\"2512\" data-height=\"456\" data-action=\"zoom\" data-action-value=\"1*PuTkol6xQmSUEONzGjbGiA.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*PuTkol6xQmSUEONzGjbGiA.png\" alt=\"image\" /></div></div></figure><p id=\"5816\" class=\"graf graf--p graf-after--figure\">To immediately see if everything is fine with StatsD daemon, number of metrics seems reasonable and that process is handling them in rational time we’re using two simple metrics sent automatically:</p><pre id=\"51b5\" class=\"graf graf--pre graf-after--p\">statsd.numStats<br />statsd.processing_time</pre><p id=\"6053\" class=\"graf graf--p graf-after--pre graf--trailing\">Useful to verify that everything works fine after upgrade.</p></div></div></section><section class=\"section section--body\"><div class=\"section-divider\"><hr class=\"section-divider\" /></div><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"d5c3\" class=\"graf graf--p graf--leading\">The second dashboard <a href=\"https://gist.github.com/mlowicki/e6aa3804ac2098082c692288f134554e\" data-href=\"https://gist.github.com/mlowicki/e6aa3804ac2098082c692288f134554e\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">“Sync — Cassandra”</a> is purely about Cassandra and it was built over time while learning about new places worth to monitor or when we had suspicion that particular component isn’t working as it should.</p><h4 id=\"c2a7\" class=\"graf graf--h4 graf-after--p\">Compaction</h4><p id=\"5f69\" class=\"graf graf--p graf-after--h4\">This one is about SSTables and compaction process. It’s important to know early if compaction stuck on certain node(s), number of pending tasks is growing or SSTables count exploded. We had such incidents in the past and detecting it early or even discovering it at all saved us headaches and allowed to react quickly.</p><figure id=\"99fc\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*2Z_aVqjIZ6W6eEhg3ImTbw.png\" data-width=\"2048\" data-height=\"1054\" data-action=\"zoom\" data-action-value=\"1*2Z_aVqjIZ6W6eEhg3ImTbw.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*2Z_aVqjIZ6W6eEhg3ImTbw.png\" alt=\"image\" /></div></div></figure><figure id=\"8e63\" class=\"graf graf--figure graf--iframe graf-after--figure\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><div class=\"iframeContainer\"><iframe width=\"700\" height=\"250\" src=\"https://medium.com/media/65826272b139587848c6d332a0a1246d?postId=1f0dc33f9cca\" data-media-id=\"65826272b139587848c6d332a0a1246d\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\">[embedded content]</iframe></div></div></div></figure><h4 id=\"1198\" class=\"graf graf--h4 graf-after--figure\">Compacting large partitions</h4><figure id=\"779e\" class=\"graf graf--figure graf-after--h4\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*GJxyB8GyUzkFWipKgm2ERQ.png\" data-width=\"1041\" data-height=\"475\" data-action=\"zoom\" data-action-value=\"1*GJxyB8GyUzkFWipKgm2ERQ.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*GJxyB8GyUzkFWipKgm2ERQ.png\" alt=\"image\" /></div></div></figure><p id=\"fa9f\" class=\"graf graf--p graf-after--figure\">C* logs warnings while compacting partition longer than <em class=\"markup--em markup--p-em\">compaction_large_partition_warning_threshold_mb</em> (<a href=\"https://docs.datastax.com/en/cassandra/3.x/cassandra/configuration/configCassandra_yaml.html\" data-href=\"https://docs.datastax.com/en/cassandra/3.x/cassandra/configuration/configCassandra_yaml.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">cassandra.yaml</a>). We’re using <a href=\"https://medium.com/@mlowicki/monitoring-cassandra-garbage-collector-83c8a515e403#.4s2h29v6u\" data-href=\"https://medium.com/@mlowicki/monitoring-cassandra-garbage-collector-83c8a515e403#.4s2h29v6u\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">Logstash</a> to parse these warnings and put into StatsD. Lots of such events with high numbers may indicate problems with schema.</p><p id=\"5a73\" class=\"graf graf--p graf-after--p\">Two charts at the bottom are specific to our project where we’ve a concept of data type used a part of <a href=\"http://docs.datastax.com/en/cql/3.1/cql/cql_reference/refCompositePk.html\" data-href=\"http://docs.datastax.com/en/cql/3.1/cql/cql_reference/refCompositePk.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">composite partition key</a>.</p><h4 id=\"2aa9\" class=\"graf graf--h4 graf-after--p\">Disk</h4><figure id=\"f630\" class=\"graf graf--figure graf-after--h4\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*mm9zSbdaunNEp1JRZFcUBQ.png\" data-width=\"2504\" data-height=\"1252\" data-action=\"zoom\" data-action-value=\"1*mm9zSbdaunNEp1JRZFcUBQ.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*mm9zSbdaunNEp1JRZFcUBQ.png\" alt=\"image\" /></div></div></figure><p id=\"7328\" class=\"graf graf--p graf-after--figure\">Cassandra provides also data about read / write latency:</p><figure id=\"2831\" class=\"graf graf--figure graf--iframe graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><div class=\"iframeContainer\"><iframe width=\"700\" height=\"250\" src=\"https://medium.com/media/35adf3886c488d21b0cdbf3c38ad3b14?postId=1f0dc33f9cca\" data-media-id=\"35adf3886c488d21b0cdbf3c38ad3b14\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\">[embedded content]</iframe></div></div></div></figure><p id=\"17ad\" class=\"graf graf--p graf--startsWithDoubleQuote graf-after--figure\">“Scanned tombstones” is fed by parsing C* logs and getting warnings about scanning large number of tombstones.</p><h4 id=\"9684\" class=\"graf graf--h4 graf-after--p\">Requests</h4><figure id=\"78d4\" class=\"graf graf--figure graf-after--h4\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*pHKYWPTXUHB1_LNW_uCy2Q.png\" data-width=\"2048\" data-height=\"1002\" data-action=\"zoom\" data-action-value=\"1*pHKYWPTXUHB1_LNW_uCy2Q.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*pHKYWPTXUHB1_LNW_uCy2Q.png\" alt=\"image\" /></div></div></figure><figure id=\"caba\" class=\"graf graf--figure graf--iframe graf-after--figure\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><div class=\"iframeContainer\"><iframe width=\"700\" height=\"250\" src=\"https://medium.com/media/4e2c1b3adedece7dec69e705b9107dc7?postId=1f0dc33f9cca\" data-media-id=\"4e2c1b3adedece7dec69e705b9107dc7\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\">[embedded content]</iframe></div></div></div></figure><h4 id=\"ecfa\" class=\"graf graf--h4 graf-after--figure\">Errors</h4><figure id=\"6fe7\" class=\"graf graf--figure graf-after--h4\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*uGxgHatUTa628LXqPm992Q.png\" data-width=\"2048\" data-height=\"373\" data-action=\"zoom\" data-action-value=\"1*uGxgHatUTa628LXqPm992Q.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*uGxgHatUTa628LXqPm992Q.png\" alt=\"image\" /></div></div></figure><figure id=\"e6dd\" class=\"graf graf--figure graf--iframe graf-after--figure\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><div class=\"iframeContainer\"><iframe width=\"700\" height=\"250\" src=\"https://medium.com/media/db73c0f8ae52c67f9d5eee521f7c8224?postId=1f0dc33f9cca\" data-media-id=\"db73c0f8ae52c67f9d5eee521f7c8224\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\">[embedded content]</iframe></div></div></div></figure><p id=\"2765\" class=\"graf graf--p graf-after--figure\">These two accumulative metrics (use <em class=\"markup--em markup--p-em\">perSecond</em> to get number of occurrences per second) show if either C* throws many exceptions and would be good to grep through its system.log or it can’t connect to other nodes which might be a sign of overloaded node(s) or network issues.</p><h4 id=\"efe3\" class=\"graf graf--h4 graf-after--p\">Thread pools</h4><figure id=\"52a4\" class=\"graf graf--figure graf-after--h4\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*NtwnlNlTD1e8Rebd1SvcxA.png\" data-width=\"2520\" data-height=\"1208\" data-action=\"zoom\" data-action-value=\"1*NtwnlNlTD1e8Rebd1SvcxA.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*NtwnlNlTD1e8Rebd1SvcxA.png\" alt=\"image\" /></div></div></figure><p id=\"7b15\" class=\"graf graf--p graf-after--figure\">This might seem overwhelming but shows what threads are doing like creating Merkle trees (<em class=\"markup--em markup--p-em\">ValidationExecutor</em>), compacting (<em class=\"markup--em markup--p-em\">CompactionExecutor</em>) or just handling read requests.</p><figure id=\"e1b1\" class=\"graf graf--figure graf--iframe graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><div class=\"iframeContainer\"><iframe width=\"700\" height=\"250\" src=\"https://medium.com/media/fedc0ef06e91c7ef94cfd56d3eff375e?postId=1f0dc33f9cca\" data-media-id=\"fedc0ef06e91c7ef94cfd56d3eff375e\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\">[embedded content]</iframe></div></div></div></figure><h4 id=\"ffde\" class=\"graf graf--h4 graf-after--figure\">Key cache</h4><figure id=\"9285\" class=\"graf graf--figure graf-after--h4\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*-laQNkzg6iDLLUCR-P048w.png\" data-width=\"2530\" data-height=\"976\" data-action=\"zoom\" data-action-value=\"1*-laQNkzg6iDLLUCR-P048w.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*-laQNkzg6iDLLUCR-P048w.png\" alt=\"image\" /></div></div></figure><p id=\"6acf\" class=\"graf graf--p graf-after--figure\">We don’t have hot data in Sync so row cache is disabled (<em class=\"markup--em markup--p-em\">row_cache_size_in_mb</em> set to 0 in cassandra.yaml<em class=\"markup--em markup--p-em\">)</em> but having charts for key cache to monitor its performance is precaution to save our time in the future.</p><figure id=\"f176\" class=\"graf graf--figure graf--iframe graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><div class=\"iframeContainer\"><iframe width=\"700\" height=\"250\" src=\"https://medium.com/media/8c738888a683bcd2456f6c1d06a8c102?postId=1f0dc33f9cca\" data-media-id=\"8c738888a683bcd2456f6c1d06a8c102\" data-thumbnail=\"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars2.githubusercontent.com%2Fu%2F97633%3Fv%3D3%26s%3D400&amp;key=4fce0568f2ce49e8b54624ef71a8a5bd\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\">[embedded content]</iframe></div></div></div></figure><p id=\"c01b\" class=\"graf graf--p graf-after--figure\">Be careful with metrics ending with “.Value”. They hold accumulative values since node’s start so use <em class=\"markup--em markup--p-em\">perSecond</em> function to get rate per second.</p><p id=\"eaf4\" class=\"graf graf--p graf-after--p\"><em class=\"markup--em markup--p-em\">HitRate</em> might seem redundant if Graphite has <a href=\"http://graphite.readthedocs.org/en/1.0/functions.html#graphite.render.functions.asPercent\" data-href=\"http://graphite.readthedocs.org/en/1.0/functions.html#graphite.render.functions.asPercent\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">asPercent</a> but we can’t use it now because of <a href=\"https://github.com/graphite-project/graphite-web/issues/207\" data-href=\"https://github.com/graphite-project/graphite-web/issues/207\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">#207</a> which seems abandoned…</p><h4 id=\"fd37\" class=\"graf graf--h4 graf-after--p\">Memtables</h4><p id=\"ee46\" class=\"graf graf--p graf-after--h4\">There are couple of options in cassandra.yaml to tune M<a href=\"https://wiki.apache.org/cassandra/MemtableSSTable\" data-href=\"https://wiki.apache.org/cassandra/MemtableSSTable\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">emtables</a> (like <em class=\"markup--em markup--p-em\">memtable_cleanup_threshold</em> or <em class=\"markup--em markup--p-em\">memtable_flush_writers</em>) and this row will help you to discover eventual issues. We didn’t have any incidents with this part but always better to have such charts ready. Memtables are created per-ColumnFamily but for now we’re using aggregated metrics:</p><figure id=\"2a22\" class=\"graf graf--figure graf--iframe graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><div class=\"iframeContainer\"><iframe width=\"700\" height=\"250\" src=\"https://medium.com/media/76721f1696ab644cce734cbff016f07c?postId=1f0dc33f9cca\" data-media-id=\"76721f1696ab644cce734cbff016f07c\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\">[embedded content]</iframe></div></div></div></figure><h4 id=\"3ae5\" class=\"graf graf--h4 graf-after--figure\">Bloom filter</h4><figure id=\"b986\" class=\"graf graf--figure graf-after--h4\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*Kj-OQ8KH2q3giRNyOWKQyA.png\" data-width=\"2048\" data-height=\"715\" data-action=\"zoom\" data-action-value=\"1*Kj-OQ8KH2q3giRNyOWKQyA.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*Kj-OQ8KH2q3giRNyOWKQyA.png\" alt=\"image\" /></div></div></figure><p id=\"0237\" class=\"graf graf--p graf-after--figure\">Added for the future (didn’t have any problems with this component so far) but we’re planning to tune <em class=\"markup--em markup--p-em\">bloom_filter_fp_chance</em> soon so will be used to get feedback after scheduled changes.</p><figure id=\"800c\" class=\"graf graf--figure graf--iframe graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><div class=\"iframeContainer\"><iframe width=\"700\" height=\"250\" src=\"https://medium.com/media/d1c6f1806e9cdd50c62e70264e53212a?postId=1f0dc33f9cca\" data-media-id=\"d1c6f1806e9cdd50c62e70264e53212a\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\">[embedded content]</iframe></div></div></div></figure><h4 id=\"7245\" class=\"graf graf--h4 graf-after--figure\">Snapshosts</h4><pre id=\"e290\" class=\"graf graf--pre graf-after--h4\">gauges.cassandra.jmx.org.apache.cassandra.metrics.ColumnFamily.SnapshotsSize.Value</pre><p id=\"166c\" class=\"graf graf--p graf-after--pre\">This single metrics is used to immediately see if <em class=\"markup--em markup--p-em\">`nodetool clearsnapshot`</em> can be fired to save significant disk space.</p><h4 id=\"e0da\" class=\"graf graf--h4 graf-after--p\">Hints</h4><figure id=\"0ad5\" class=\"graf graf--figure graf--iframe graf-after--h4\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><div class=\"iframeContainer\"><iframe width=\"700\" height=\"250\" src=\"https://medium.com/media/c09b8fd20aef20d76ae3b6eeac0c6def?postId=1f0dc33f9cca\" data-media-id=\"c09b8fd20aef20d76ae3b6eeac0c6def\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\">[embedded content]</iframe></div></div></div></figure><p id=\"e28d\" class=\"graf graf--p graf-after--figure\">Accumulative metrics to show hints across cluster.</p><h4 id=\"cb03\" class=\"graf graf--h4 graf-after--p\">Streams</h4><figure id=\"88d4\" class=\"graf graf--figure graf-after--h4\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*nIX2FE8jIgy4teHVoVixhg.png\" data-width=\"2048\" data-height=\"371\" data-action=\"zoom\" data-action-value=\"1*nIX2FE8jIgy4teHVoVixhg.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*nIX2FE8jIgy4teHVoVixhg.png\" alt=\"image\" /></div></div></figure><figure id=\"0df9\" class=\"graf graf--figure graf--iframe graf-after--figure\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><div class=\"iframeContainer\"><iframe width=\"700\" height=\"250\" src=\"https://medium.com/media/0a91777e498552cf4377ea4150c5581b?postId=1f0dc33f9cca\" data-media-id=\"0a91777e498552cf4377ea4150c5581b\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\">[embedded content]</iframe></div></div></div></figure><p id=\"d73d\" class=\"graf graf--p graf-after--figure\">This part is used to see how nodes are busy with streaming operations.</p><h4 id=\"3223\" class=\"graf graf--h4 graf-after--p\">Connected clients</h4><figure id=\"c66a\" class=\"graf graf--figure graf--iframe graf-after--h4\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><div class=\"iframeContainer\"><iframe width=\"700\" height=\"250\" src=\"https://medium.com/media/60ca814628287dda3704cba61b5ffb1a?postId=1f0dc33f9cca\" data-media-id=\"60ca814628287dda3704cba61b5ffb1a\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\">[embedded content]</iframe></div></div></div></figure><p id=\"f566\" class=\"graf graf--p graf-after--figure graf--trailing\">It happens (usually while restarts) that binary or thrift protocol suddenly stops to work (or isn’t started) or number of connected clients isn’t balanced. This group will tell us if something like that takes place.</p></div></div></section><section class=\"section section--body section--last\"><div class=\"section-divider\"><hr class=\"section-divider\" /></div><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"3228\" class=\"graf graf--p graf--leading\">Most metrics we use are calculated per C* node basis. We don’t host multiple keyspaces per cluster and having charts per table wasn’t useful yet. Fortunately it’s a matter of modifying configuration file for <a href=\"https://jolokia.org\" data-href=\"https://jolokia.org\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Jolokia</a> (through <a href=\"https://github.com/python-diamond/Diamond/blob/master/src/collectors/jolokia/cassandra_jolokia.py\" data-href=\"https://github.com/python-diamond/Diamond/blob/master/src/collectors/jolokia/cassandra_jolokia.py\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">CassandraJolokia</a> collector) and run Puppet which eventually restarts Diamond’s collectors.</p><p id=\"2bcf\" class=\"graf graf--p graf-after--p graf--trailing\">Our <a href=\"https://gist.github.com/mlowicki/95bd388ccd717fe305a1fcb693fa8fc3\" data-href=\"https://gist.github.com/mlowicki/95bd388ccd717fe305a1fcb693fa8fc3\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Jolokia configuration file</a> became very long at some point so using regexps alleviate this problem a bit.</p></div></div></section>",
        "created_at": "2018-02-03T17:11:00+0000",
        "updated_at": "2018-03-11T17:42:35+0000",
        "published_at": "2016-07-10T20:30:40+0000",
        "published_by": [
          "Michał Łowicki"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 5,
        "domain_name": "medium.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*7dCDUqQgWvT1vdYRb1KsoQ.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9257"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 603,
            "label": "book",
            "slug": "book"
          }
        ],
        "is_public": false,
        "id": 9201,
        "uid": null,
        "title": "Learn Cassandra",
        "url": "https://www.gitbook.com/book/teddyma/learncassandra/details",
        "content": null,
        "created_at": "2018-01-30T20:22:15+0000",
        "updated_at": "2018-03-14T17:28:07+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": null,
        "language": null,
        "reading_time": 0,
        "domain_name": "www.gitbook.com",
        "preview_picture": null,
        "http_status": null,
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9201"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 207,
            "label": "system",
            "slug": "system"
          },
          {
            "id": 230,
            "label": "operations",
            "slug": "operations"
          },
          {
            "id": 1151,
            "label": "resources",
            "slug": "resources"
          },
          {
            "id": 1173,
            "label": "troubleshooting and tuning",
            "slug": "troubleshooting-and-tuning"
          }
        ],
        "is_public": true,
        "id": 9193,
        "uid": "5b578403cde9f1.04951013",
        "title": "Jon Haddad: Cassandra Summit Recap - Diagnosing Problems in Production",
        "url": "http://rustyrazorblade.com/2014/09/cassandra-summit-recap-diagnosing-problems-in-production/",
        "content": "<p>Last week at the Cassandra Summit I gave a talk with <a href=\"https://twitter.com/rustyrazorblade/status/511932312515526656\">Blake Eggleston</a> on diagnosing performance problems in production.  We spoke to about 300 people for about 25 minutes followed by a healthy Q&amp;A session.  I've expanded on our presentation to include a few extra tools, screenshots, and more clarity on our talking points.</p><p>There's finally a lot of material available for someone looking to get started with Cassandra.  There's several introductory videos on YouTube by both <a href=\"https://www.youtube.com/watch?v=W45Ysb9b6oE\">me</a> and <a href=\"https://www.youtube.com/watch?v=B-bTPSwhsDY\">Patrick McFadin</a> as well as videos on <a href=\"https://www.youtube.com/watch?v=Vv3QJxAdjic\">time series data modeling</a>.  I've posted videos for my own project, cqlengine, (<a href=\"https://www.youtube.com/watch?v=zrbQcPNMbB0\">intro</a> &amp; <a href=\"https://www.youtube.com/watch?v=clXN9pnakvI\">advanced</a>), and plenty more on the <a href=\"https://www.youtube.com/channel/UCvP-AXuCr-naAeEccCfKwUA\">PlanetCassandra channel</a>.  There's also a boatload of <a href=\"http://planetcassandra.org/client-drivers-tools/\">getting started</a> material on PlanetCassandra written by <a href=\"https://twitter.com/rebccamills\">Rebecca Mills</a>.</p>\n<p>This is the guide for what to do once you've built your application and you're ready to put Cassandra in production.  Whether you've been in operations for years or you are first getting started, this post should give you a good sense of what you need in order to address any issues you encounter.</p>\n<p>The original slides are available via <a href=\"http://www.slideshare.net/JonHaddad/diagnosing-problems-in-production-cassandra-summit-2014\">Slideshare</a>.</p>\n<p>Update: the presentation is now <a href=\"https://www.youtube.com/watch?v=QOwVDcLZd0A\">available on YouTube</a>!</p>\n<iframe width=\"560\" height=\"315\" src=\"http://www.youtube.com/embed/QOwVDcLZd0A\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>\n<p>Before you even put your cluster under load, there's a few things you can set up that will help you diagnose problems if they pop up.</p>\n<ol><li>\n<p>Ops center</p>\n<p>This is the standard management tool for Cassandra clusters.  This is recommended for every cluster.  While not open source, the community version is free.  It gives you a high level overview of your cluster and provides historical metrics for the most important information.  It comes with a variety of graphs that handle about 90% of what you need on a day to day basis.</p>\n<p><img src=\"http://www.datastax.com/wp-content/themes/datastax-2013/images/opscenter/opsc4-ring-view-c-hadoop-solr.jpg\" alt=\"image\" /></p>\n</li>\n<li>\n<p>Metrics plugins</p>\n<p>Cassandra has since version 1.1 included the <a href=\"https://dropwizard.github.io/metrics/3.1.0/\">metrics library</a>.  In every release it tracks more metrics using it.  <strong>Why is this awesome?</strong>  In previous persons of Cassandra, the standard way to access what was going on in the internals was over JMX, a very Java centric communications protocol.  That meant writing a Java Agent, setting up mx4j, or Jolokia, then digging through JMX, which can be a little hairy.  Not everyone wants to do this much work.</p>\n<p>The metrics library allows you to tell Cassandra to report its internal, table level metrics out to a whole slew of different places.  Out to CSV, Ganglia, Graphite, and STDOUT, and it's pluggable to push metrics to anywhere you want.</p>\n<p><img src=\"http://www.datastax.com/wp-content/uploads/2013/11/client-vs-cf.png\" alt=\"image\" /></p>\n<p><a href=\"http://www.datastax.com/dev/blog/pluggable-metrics-reporting-in-cassandra-2-0-2\">Read more about the metrics library integration.</a></p>\n</li>\n<li>\n<p>Munin, Nagios, Icinga (or other system metrics monitoring)</p>\n<p>I've found these tools to be incredibly useful at graphing system metrics as well as custom application metrics.  There are many options.  If you're already familiar with one tool, you can probably keep using it.  There are hosted solutions as well (server density, data dog, etc)</p>\n<p><img alt=\"munin\" src=\"http://rustyrazorblade.com/images/cassandra_writes.png\" /></p>\n</li>\n<li>\n<p>Statsd, Graphite, Grafana</p>\n<p>Your application should be tracking internal metrics.  Timing queries, frequently called functions, etc.  These tools let you get a profile of what's going on with your code in production.  Statsd collects raw stats and aggregates them together, then kicks them to graphite.  Grafana is an optional (better) front end to Graphite.</p>\n<p>There was a great post by etsy, <a href=\"http://codeascraft.com/2011/02/15/measure-anything-measure-everything/\">Measure Anything, Measure Everything</a>, that introduced statsd and outlined its usage with Graphite.  </p>\n<p><img alt=\"graphite\" src=\"http://rustyrazorblade.com/images/cassandra-graphite2.png\" /></p>\n</li>\n<li>\n<p>Logstash</p>\n<p>We didn't mention <a href=\"http://logstash.net/\">Logstash</a> in our presentation, but we've found it to be incredibly useful in correlating application issues with other failures.  This is useful for application logging aggregation.  If you don't want to host your own log analysis tool, there are hosted services for this as well.</p>\n<p><img alt=\"logstash\" src=\"http://rustyrazorblade.com/images/logstash_blog-1024x514.png\" /></p>\n</li>\n</ol><p>There's a bunch of system tools that are useful if you're logged onto a machine and want to see real time information.  </p>\n<ol><li>\n<p>iostat</p>\n<p>iostat is useful for seeing what's happening with each disk on your machine.  If you're hitting I/O issues, you'll see it here.  Specifically, you're looking for high read &amp; write rates and a big avgqu-sz (disk queue), or a high svctm (service time) there's a good chance you're bottlenecked on your disk.  You either want to use more disks or faster disks.  Cassandra loves SSDs.</p>\n<p><img alt=\"iostat\" src=\"http://rustyrazorblade.com/images/iostat.png\" /></p>\n</li>\n<li>\n<p>htop</p>\n<p>Htop is a better version of top, which is useful for getting a quick glance at your system.  It shows load, running processes, memory usage, and a bunch of other information at a quick glance.</p>\n<p><img alt=\"htop\" src=\"http://rustyrazorblade.com/images/htop.png\" /></p>\n</li>\n<li>\n<p>iftop &amp; netstat</p>\n<p>iftop is like top, but shows you active connections and the transfer rates between your server and whoever is at the other end.  </p>\n<p><img alt=\"iftop\" src=\"http://rustyrazorblade.com/images/iftop.png\" /></p>\n<p>Netstat is more of a networking swiss army knife.  You can see network connections, routing tables, interface statistics, and a variety of other network information.</p>\n<p><img alt=\"netstat\" src=\"http://rustyrazorblade.com/images/netstat.png\" /></p>\n</li>\n<li>\n<p>dstat</p>\n<p>I prefer to use dstat over iostat now since it includes all of its functionality and much of the functionality of other tools as well.</p>\n<p><img alt=\"dstat\" src=\"http://rustyrazorblade.com/images/dstat.png\" /></p>\n</li>\n<li>\n<p>strace</p>\n<p>strace is useful when you want to know what system calls are happening for a given process.</p>\n<p><img alt=\"strace\" src=\"http://rustyrazorblade.com/images/strace.png\" /></p>\n</li>\n<li>\n<p>pcstat</p>\n<p>This tool, written by <a href=\"https://twitter.com/AlTobey\">Al Tobey</a>, allows you to examine a bunch of files and quickly determine how much of each file is in the buffer cache.  If you're trying to figure out why table access is slow, this tool can tell you if your data is in cache already or if you have to go out to disk.  <a href=\"http://www.linuxatemyram.com/\">Here's a good read</a> to get familiar with buffer cache.  <a href=\"https://github.com/tobert/pcstat\">Check out the repo</a>.</p>\n<p><img alt=\"pcstat\" src=\"http://rustyrazorblade.com/images/pcstat.png\" /></p>\n</li>\n</ol><p>There's a few issues that are easy to run into that I'd consider \"gotchas\", things that come up often enough that they're worth mentioning.</p>\n<h2>Clock skew</h2>\n<p>A important design decision in Cassandra is that it uses last write wins when there are two inserts, updates, or deletes to a cell.  To determine the last update, Cassandra uses the system clock (or the client can specify the time explicitly).  If server times are different, the last write may not actually win, it'll be the one that's the most skewed into the future.  </p>\n<p>To address this issue, always make sure your clocks are synced.  Ntpd will constantly correct for drift.  ntpdate will perform a hard adjustment to your system clock.  Ntpdate needs to be used if you clock is significantly off, and ntpd will keep it at the correct time.</p>\n<p><img alt=\"ntpdate\" src=\"http://rustyrazorblade.com/images/ntpdate.png\" /></p>\n<h2>Disk space not reclaimed</h2>\n<p>if you add new nodes to a cluster, each replica is responsible for less data.  it's streamed to the new nodes.  however, it is not removed from the old nodes.  If you're adding new nodes because you're running low on disk space, this is extremely important.  You are required to run <code>nodetool cleanup</code> in order to reclaim that disk space.  This is a good idea any time you change your database topology.</p>\n<h2>Issues adding nodes, or running repairs</h2>\n<p>There are two common problems that come up with repair.  The first is that repairs take forever in 2.0.  <a href=\"http://www.datastax.com/dev/blog/more-efficient-repairs\">This is solved in 2.1</a> which uses an incremental repair, and does not repair data which has already been repaired.  The second issue relates to trying to repair (or add nodes) to a cluster when the versions do not match.  It is, in general, not a good idea (yet) to stream data between servers which are of different versions.  It will appear to have started, but will just hang around doing nothing.</p>\n<p>Cassandra comes with several tools to help diagnose performance problems.  They are available via <code>nodetool</code>, Cassandra's multipurpose administration tool.</p>\n<h2>Compaction</h2>\n<p>Compaction is the process of merging SSTables together.  It reduces the number of seeks required to return a result.  It's a necessary part of Cassandra.  If not configured correctly, it can be problematic.  You can limit the I/O used by compaction by using <code>nodetool setcompactionthroughput</code>.</p>\n<p>There's 2 types of compaction available out of the box.  Size Tiered is the default and great for write heavy workloads.  Leveled compaction is good for read &amp; update heavy workloads, but since it uses much higher I/O it's recommended you use this only if you're on SSD.  I recommend reading through the <a href=\"http://datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_configure_compaction_t.html\">documentation</a> to understand more about which is right for your workload.</p>\n<p><img src=\"http://www.datastax.com/documentation/cassandra/2.0/cassandra/images/dml_compaction.png\" alt=\"image\" /></p>\n<h2>Cfstats and Histograms</h2>\n<p>Histograms let you quickly understand at both a high level and table level what your performance looks like on a single node in your cluster.  The first histogram, <code>proxyhistograms</code>, give you a quick top level view of all your tables on a node.  This includes network latency.  Histogram output has changed between versions to be more user friendly.  The screenshot below is from Cassandra 2.1.</p>\n<p><img alt=\"proxyhistograms\" src=\"http://rustyrazorblade.com/images/proxyhistograms.png\" /></p>\n<p>If you'd like to find out if you've got a performance problem isolated to a particular table, I suggest first running <code>nodetool cfstats</code> on a keyspace.  You'll be able to scan the list of tables and see if there's any abnormalities.  You'll be able to quickly tell which tables are queried the most (both reads and writes).</p>\n<p><img alt=\"cfstats\" src=\"http://rustyrazorblade.com/images/cfstats.png\" /></p>\n<p><code>nodetool cfhistograms</code> lets you identify performance problems with a single table on a single node.  The statistics are more easily read in Cassandra 2.1.</p>\n<p><img alt=\"cfhistograms\" src=\"http://rustyrazorblade.com/images/cfhistograms.png\" /></p>\n<h2>Query Tracing</h2>\n<p>If you've narrowed down your problem to a particular table, you can start to trace the queries that you execute.  If you're coming from a something like MySQL, you're used to the command <code>explain</code>, which tells in in advance what the query plan is for a given query.  Tracing takes a different approach.  Instead of showing a query plan, query tracing keeps track of the events in the system whewn it actually executes.  Here's an example where we've created a whole bunch of tombstones on a partition.  Even on a SSD you still want to avoid a lot of tombstones - it's disk, CPU, and memory intensive.</p>\n<p><img alt=\"tracing\" src=\"http://rustyrazorblade.com/images/tracing.png\" /></p>\n<p>The JVM gets a reputation for being a bit of a beast.  It's a really impressive feat of engineering, but it shouldn't be regarded as black magic.  I strongly recommend reading through <a href=\"http://blakeeggleston.com/cassandra-tuning-the-jvm-for-read-heavy-workloads.html\">Blake Eggleston's post on the JVM</a>, it's well written and does a great job of explaining things. (Much better than I would here).  </p>\n<p>OK - we've got all these tools under our belt.  Now we can start to narrow down the problem.</p>\n<ul><li>\n<p>Are you seeing weird consistency issues, even on consistency level ALL?<br />It's possible you're dealing with a clock sync issue.  If you're sending queries really close to one another, they might also be getting the same millisecond level timestamp due to an async race condition in your code.  If you're sending lots of writes at the same time to the same row, you may have a problem in your application.  Try to rethink your data model to avoid this.</p>\n</li>\n<li>\n<p>Has query performance dropped?\n    Are you bottlenecked on disk, network, CPU, memory?  Use the tools above to figure out your bottleneck.  Did the number of queries to your cluster increase?  Are you seeing longer than normal garbage collection times?  Ops center has historical graphs that are useful here.  Is there a single table affected, or every table?  Use histograms and cfstats to dig into it.  </p>\n</li>\n<li>\n<p>Are nodes going up and down?\n    Use a combination of ops center and your system metrics to figure out which node it is.  If it's the same node, start investigating why.  Is there a hot partition?  Is it doing a lot of garbage collection?  Is your application opening more connections than before?  You should have system metrics that show these trends over time.  Maybe you just have additional load on the system - it may be necessary to add new nodes.  Don't forget to run cleanup.</p>\n</li>\n</ul><p>This started out as a small recap but has evolved into much more than that.  The tools above have helped me a wide variety of problems, not just Cassandra ones.  If you follow the above recommendations you should be in a great spot to diagnose most problems that come your way.  </p>\n<p>You can find me on <a href=\"https://twitter.com/rustyrazorblade\">Twitter</a> for any comments or suggestions.</p>",
        "created_at": "2018-01-30T14:08:07+0000",
        "updated_at": "2018-07-24T19:54:43+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 9,
        "domain_name": "rustyrazorblade.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9193"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 217,
            "label": "tool",
            "slug": "tool"
          }
        ],
        "is_public": false,
        "id": 9172,
        "uid": null,
        "title": "brianmhess/cassandra-loader",
        "url": "https://github.com/brianmhess/cassandra-loader",
        "content": "<h2>Introduction</h2><p>cassandra-loader is a general-purpose, delimited-file, bulk loader for\nCassandra. It supports a number of configuration options to enable bulk\nloading of various types of delimited files, including</p><ul><li>comma-separated values</li>\n<li>tab-separated values</li>\n<li>customer delimiter-separated values</li>\n<li>header row</li>\n<li>comma as decimal separator</li>\n<li>...</li>\n</ul><h2>Getting it</h2><h3>Downloading</h3><p>This utility has already been built, and is available at\n<a href=\"https://github.com/brianmhess/cassandra-loader/releases/download/v0.0.27/cassandra-loader\">https://github.com/brianmhess/cassandra-loader/releases/download/v0.0.27/cassandra-loader</a></p><p>Get it with wget:</p><pre>wget https://github.com/brianmhess/cassandra-loader/releases/download/v0.0.27/cassandra-loader\n</pre><h3>Building</h3><p>To build this repository, simply clone this repo and run:</p><pre>gradle loader\n</pre><p>All of the dependencies are included (namely, the Java driver - currently\nversion 3.0.0).  The output will be the cassandra-loader executable\nin the build directory.  There will also be an jar with all of the\ndependencies included in the build/libs/cassandra-loader-uber-.jar</p><h2>Documentation</h2><p>To extract this README document, simply run (on the cassandra-loader\nexecutable - (e.g., on build/cassandra-loader):</p><pre>jar xf cassandra-loader README.md\n</pre><h2>Run</h2><p>To run cassandra-loader, simply run the cassandra-loader executable\n(e.g., located at build/cassandra-loader):</p><pre>cassandra-loader\n</pre><p>If you built this with gradle, you can also run:</p><pre>gradle run\n</pre><p>This will print the usage statement.</p><p>The following will load the <code>myFileToLoad.csv</code> file into the Cassandra\ncluster at IP address 1.2.3.4 into the <code>test.ltest</code> column family where\nthe myFileToLoad file has the format of 4 columns - and it gets the\ndata type information from the database - and using the default options:</p><pre>cassandra-loader -f myFileToLoad.csv -host 1.2.3.4 -schema \"test.ltest(a, b, c, d)\"\n</pre><h2>Options:</h2><table><thead><tr><th align=\"right\">Switch</th>\n<th align=\"right\">Option</th>\n<th align=\"right\">Default</th>\n<th align=\"left\">Description</th>\n</tr></thead><tbody><tr><td align=\"right\"><code>-configFile</code></td>\n<td align=\"right\">Filename</td>\n<td align=\"right\">none</td>\n<td align=\"left\">Filename of configuration options</td>\n</tr><tr><td align=\"right\"><code>-f</code></td>\n<td align=\"right\">Filename</td>\n<td align=\"right\">&lt;REQUIRED&gt;</td>\n<td align=\"left\">Filename to load - required.</td>\n</tr><tr><td align=\"right\"><code>-host</code></td>\n<td align=\"right\">IP Address</td>\n<td align=\"right\">&lt;REQUIRED&gt;</td>\n<td align=\"left\">Cassandra connection point - required.</td>\n</tr><tr><td align=\"right\"><code>-format</code></td>\n<td align=\"right\">Input format</td>\n<td align=\"right\">delim</td>\n<td align=\"left\">Format of the data.  Options are \"delim\" or \"json\".</td>\n</tr><tr><td align=\"right\"><code>-schema</code></td>\n<td align=\"right\">CQL schema</td>\n<td align=\"right\">\n</td><td align=\"left\">Schema of input data - required for delim In the format \"keySpace.table(col1,col2,...)\" and in the order that the data will be in the file.</td>\n</tr><tr><td align=\"right\"><code>-keyspace</code></td>\n<td align=\"right\">Keyspace name</td>\n<td align=\"right\">\n</td><td align=\"left\">Name of keyspace (case sensitive) to load in to - required for json</td>\n</tr><tr><td align=\"right\"><code>-table</code></td>\n<td align=\"right\">Table name</td>\n<td align=\"right\">\n</td><td align=\"left\">Name of table (case sensitive) to load in to - required for json</td>\n</tr><tr><td align=\"right\"><code>-port</code></td>\n<td align=\"right\">Port Number</td>\n<td align=\"right\">9042</td>\n<td align=\"left\">Cassandra native protocol port number</td>\n</tr><tr><td align=\"right\"><code>-user</code></td>\n<td align=\"right\">Username</td>\n<td align=\"right\">none</td>\n<td align=\"left\">Cassandra username</td>\n</tr><tr><td align=\"right\"><code>-pw</code></td>\n<td align=\"right\">Password</td>\n<td align=\"right\">none</td>\n<td align=\"left\">Cassandra password</td>\n</tr><tr><td align=\"right\"><code>-ssl-truststore-path</code></td>\n<td align=\"right\">Truststore Path</td>\n<td align=\"right\">none</td>\n<td align=\"left\">Path to SSL truststore</td>\n</tr><tr><td align=\"right\"><code>-ssl-truststore-pw</code></td>\n<td align=\"right\">Truststore Password</td>\n<td align=\"right\">none</td>\n<td align=\"left\">Password to SSL truststore</td>\n</tr><tr><td align=\"right\"><code>-ssl-keystore-path</code></td>\n<td align=\"right\">Keystore Path</td>\n<td align=\"right\">none</td>\n<td align=\"left\">Path to SSL keystore</td>\n</tr><tr><td align=\"right\"><code>-ssl-keystore-path</code></td>\n<td align=\"right\">Keystore Password</td>\n<td align=\"right\">none</td>\n<td align=\"left\">Password to SSL keystore</td>\n</tr><tr><td align=\"right\"><code>-consistencyLevel</code></td>\n<td align=\"right\">Consistency Level</td>\n<td align=\"right\">ONE</td>\n<td align=\"left\">CQL Consistency Level</td>\n</tr><tr><td align=\"right\"><code>-numThreads</code></td>\n<td align=\"right\">Number of threads</td>\n<td align=\"right\">Number of CPUs</td>\n<td align=\"left\">Number of threads to use (one per file)</td>\n</tr><tr><td align=\"right\"><code>-numFutures</code></td>\n<td align=\"right\">Number of Futures</td>\n<td align=\"right\">1000</td>\n<td align=\"left\">Number of Java driver futures in flight.</td>\n</tr><tr><td align=\"right\"><code>-numRetries</code></td>\n<td align=\"right\">Number of retries</td>\n<td align=\"right\">1</td>\n<td align=\"left\">Number of times to retry the INSERT before declaring defeat.</td>\n</tr><tr><td align=\"right\"><code>-queryTimeout</code></td>\n<td align=\"right\">Timeout in seconds</td>\n<td align=\"right\">2</td>\n<td align=\"left\">Amount of time to wait for a query to finish before timing out.</td>\n</tr><tr><td align=\"right\"><code>-ttl</code></td>\n<td align=\"right\">Time To Live</td>\n<td align=\"right\">none</td>\n<td align=\"left\">TTL to use when inserting these rows</td>\n</tr><tr><td align=\"right\"><code>-delim</code></td>\n<td align=\"right\">Delimiter</td>\n<td align=\"right\">,</td>\n<td align=\"left\">Delimiter to use</td>\n</tr><tr><td align=\"right\"><code>-charsPerColumn</code></td>\n<td align=\"right\">Characters per column</td>\n<td align=\"right\">4096</td>\n<td align=\"left\">Maximum characters per column</td>\n</tr><tr><td align=\"right\"><code>-nullString</code></td>\n<td align=\"right\">Null String</td>\n<td align=\"right\">&lt;empty string&gt;</td>\n<td align=\"left\">String to represent NULL data</td>\n</tr><tr><td align=\"right\"><code>-boolStyle</code></td>\n<td align=\"right\">Boolean Style</td>\n<td align=\"right\">TRUE_FALSE</td>\n<td align=\"left\">String for boolean values.  Options are \"1_0\", \"Y_N\", \"T_F\", \"YES_NO\", \"TRUE_FALSE\".</td>\n</tr><tr><td align=\"right\"><code>-decimalDelim</code></td>\n<td align=\"right\">Decimal delimiter</td>\n<td align=\"right\">.</td>\n<td align=\"left\">Delimiter for decimal values.  Options are \".\" or \",\"</td>\n</tr><tr><td align=\"right\"><code>-dateFormat</code></td>\n<td align=\"right\">Date Format String</td>\n<td align=\"right\">default for Locale.ENGLISH</td>\n<td align=\"left\">Date format string as specified in the SimpleDateFormat Java class: <a href=\"http://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html\" rel=\"nofollow\">http://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html</a></td>\n</tr><tr><td align=\"right\"><code>-skipRows</code></td>\n<td align=\"right\">Rows to skip</td>\n<td align=\"right\">0</td>\n<td align=\"left\">Number of rows to skip at the beginning of the file</td>\n</tr><tr><td align=\"right\"><code>-skipCols</code></td>\n<td align=\"right\">Columns to skip</td>\n<td align=\"right\">&lt;not set&gt;</td>\n<td align=\"left\">Comma-separated list of columns to skip loading (0-counted)</td>\n</tr><tr><td align=\"right\"><code>-maxRows</code></td>\n<td align=\"right\">Max rows to read</td>\n<td align=\"right\">-1</td>\n<td align=\"left\">Maximum rows to read (after optional skipping of rows).  -1 signifies all rows.</td>\n</tr><tr><td align=\"right\"><code>-maxErrors</code></td>\n<td align=\"right\">Max parse errors</td>\n<td align=\"right\">10</td>\n<td align=\"left\">Maximum number of rows that do not parse to allow before exiting.</td>\n</tr><tr><td align=\"right\"><code>-maxInsertErrors</code></td>\n<td align=\"right\">Max insert errors</td>\n<td align=\"right\">10</td>\n<td align=\"left\">Maximum number of rows that do not insert to allow before exiting.</td>\n</tr><tr><td align=\"right\"><code>-badDir</code></td>\n<td align=\"right\">Bad directory</td>\n<td align=\"right\">current directory</td>\n<td align=\"left\">Directory to write badly parsed and badly inserted rows - as well as the log file.</td>\n</tr><tr><td align=\"right\"><code>-rate</code></td>\n<td align=\"right\">Ingest rate</td>\n<td align=\"right\">unlimited</td>\n<td align=\"left\">Maximum rate to insert data - in rows/sec.</td>\n</tr><tr><td align=\"right\"><code>-progressRate</code></td>\n<td align=\"right\">Progress rate</td>\n<td align=\"right\">100000</td>\n<td align=\"left\">How often to report the ingest rate (number of rows)</td>\n</tr><tr><td align=\"right\"><code>-rateFile</code></td>\n<td align=\"right\">Rate Stats File</td>\n<td align=\"right\">&lt;not set&gt;</td>\n<td align=\"left\">File to contain CSV rate statistics</td>\n</tr><tr><td align=\"right\"><code>-successDir</code></td>\n<td align=\"right\">Success directory</td>\n<td align=\"right\">&lt;not set&gt;</td>\n<td align=\"left\">Location to move successfully loaded files</td>\n</tr><tr><td align=\"right\"><code>-failureDir</code></td>\n<td align=\"right\">Failure directory</td>\n<td align=\"right\">&lt;not set&gt;</td>\n<td align=\"left\">Location to move files that failed to load</td>\n</tr><tr><td align=\"right\"><code>-batchSize</code></td>\n<td align=\"right\">Batch size</td>\n<td align=\"right\">1</td>\n<td align=\"left\">Size of unlogged batches. If set to 1 then no batching.</td>\n</tr><tr><td align=\"right\"><code>-comment</code></td>\n<td align=\"right\">Comment character</td>\n<td align=\"right\">$lt;not set&gt;</td>\n<td align=\"left\">Comment character.</td>\n</tr></tbody></table><h2>Comments</h2><h3>Using stdin</h3><p>You can send data in on stdin by specifying the filename (via the -f switch) as \"stdin\" (case insensitive).\nThat way, you could pipe data in from other commands:</p><pre>grep IMPORTANT data.csv | cassandra-loader -f stdin -h 1.2.3.4 -schema \"test.itest(a, b)\"\n</pre><h3>Support for collections</h3><p>Collections are supported.  Their format is the CQL native one.\nSets are started with '{' and ended with '}' and enclose a comma-separated list\n{1,2,3} or {\"a\",\"b\",\"c\"}\nLists are started with '[' and ended with ']' and enclose a comma-separated list\n[1,2,3] or [\"a\",\"b\",\"c\"]\nMaps are started with '{' and ended with '}' and enclose a comma-separated list\nof pairs that are separated by ':'\n{1:1,2:2,3:3} or {\"a\":1, \"b\":2, \"c\":3}\nAll collections must be enclosed in double-quotes.</p><h3>Username/Password</h3><p>If you specify either the username or the password, then you must specify both.</p><h3>Boolean Style</h3><p>boolStyle is a case-insensitive test of the True and False strings.  For the\ndifferent styles, the True and False strings are as follows:</p><pre>    Style   | True | False\n------------|------|-------\n     0_1    |    1 |     0 \n     Y_N    |    Y |     N \n     T_F    |    T |     F \n   YES_NO   |  YES |    NO \n TRUE_FALSE | TRUE | FALSE \n</pre><h3>Configuration file</h3><p>configFile is a file with configuration options that are formatted just like on\nthe command line.  This allows you to not specify arguments on the command line.\nFor example, you can specify passwords in the configFile and avoid having them on\nthe command line.  The format is one switch and option per line:</p><pre>-pwd mypassword\n-host 1.2.3.4\n</pre><h3>Miscelaneous</h3><p>numFutures is a way to control the level of parallelism, but at some point\ntoo many will actually slow down the load.  The default of 500 is a decent\nplace to start.</p><p>If you use quoted strings, you need to use double-quotes.  To escape a double-quote inside a quoted string, use the backslash to escape it (\"\"\").  To create a backslash inside a quoted string, use two backslashes in a row (\"\\\").  If you quote your string, it will not be trimmed, but if you do not quote your string it will be trimmed.</p><p>Loading into counter columns is not supported.</p><p>The default nullString is the empty string.  If you want empty strings to be saved as empty strings, set the nullString to something else.</p><p>If you do not set the successDir then files that successfully loaded will remain in their input directory.  The same is true for failed files if you do not set the failureDir.  You cannot set either if the input file is \"stdin\".</p><p>When using <code>jsonline</code>, all JSON field names are case-sensitive.  When using <code>jsonline</code> or <code>jsonarray</code>, the <code>-keyspace</code> and <code>-table</code> arguments are case-sensitive.</p><h2>Usage Statement:</h2><pre>version: 0.0.27\nUsage: -f &lt;filename&gt; -host &lt;ipaddress&gt; [OPTIONS]\nOPTIONS:\n  -schema &lt;schema&gt;                   Table schema (when using delim)\n  -table &lt;tableName&gt;                 Table name (when using json)\n  -keyspace &lt;keyspaceName&gt;           Keyspace name (when using json)\n  -configFile &lt;filename&gt;             File with configuration options\n  -delim &lt;delimiter&gt;                 Delimiter to use [,]\n  -charsPerColumn &lt;chars&gt;            Max number of chars per column [4096]\n  -dateFormat &lt;dateFormatString&gt;     Date format for TIMESTAMP [default for Locale.ENGLISH]\n  -localDateFormat &lt;formatString&gt;    Date format for DATE [yyyy-MM-dd]\n  -nullString &lt;nullString&gt;           String that signifies NULL [none]\n  -comment &lt;commentString&gt;           Comment symbol to use [none]\n  -skipRows &lt;skipRows&gt;               Number of rows to skip [0]\n  -skipCols &lt;columnsToSkip&gt;          Comma-separated list of columsn to skip in the input file\n  -maxRows &lt;maxRows&gt;                 Maximum number of rows to read (-1 means all) [-1]\n  -maxErrors &lt;maxErrors&gt;             Maximum parse errors to endure [10]\n  -badDir &lt;badDirectory&gt;             Directory for where to place badly parsed rows. [none]\n  -port &lt;portNumber&gt;                 CQL Port Number [9042]\n  -user &lt;username&gt;                   Cassandra username [none]\n  -pw &lt;password&gt;                     Password for user [none]\n  -ssl-truststore-path &lt;path&gt;        Path to SSL truststore [none]\n  -ssl-truststore-pw &lt;pwd&gt;           Password for SSL truststore [none]\n  -ssl-keystore-path &lt;path&gt;          Path to SSL keystore [none]\n  -ssl-keystore-pw &lt;pwd&gt;             Password for SSL keystore [none]\n  -consistencyLevel &lt;CL&gt;             Consistency level [LOCAL_ONE]\n  -numFutures &lt;numFutures&gt;           Number of CQL futures to keep in flight [1000]\n  -batchSize &lt;batchSize&gt;             Number of INSERTs to batch together [1]\n  -decimalDelim &lt;decimalDelim&gt;       Decimal delimiter [.] Other option is ','\n  -boolStyle &lt;boolStyleString&gt;       Style for booleans [TRUE_FALSE]\n  -numThreads &lt;numThreads&gt;           Number of concurrent threads (files) to load [num cores]\n  -queryTimeout &lt;# seconds&gt;          Query timeout (in seconds) [2]\n  -numRetries &lt;numRetries&gt;           Number of times to retry the INSERT [1]\n  -maxInsertErrors &lt;# errors&gt;        Maximum INSERT errors to endure [10]\n  -rate &lt;rows-per-second&gt;            Maximum insert rate [50000]\n  -progressRate &lt;num txns&gt;           How often to report the insert rate [100000]\n  -rateFile &lt;filename&gt;               Where to print the rate statistics\n  -successDir &lt;dir&gt;                  Directory where to move successfully loaded files\n  -failureDir &lt;dir&gt;                  Directory where to move files that did not successfully load\n  -nullsUnset [false|true]           Treat nulls as unset [faslse]\n  -format [delim|jsonline|jsonarray] Format of data: delimited or JSON [delim]\n  -table &lt;tableName&gt;                 Table name (when using JSON)\n  -keyspace &lt;keyspaceName&gt;           Keyspace name (when using JSON)\n  -ttl &lt;TTL&gt;                         TTL for all rows in this invocation [unset]\nExamples:\ncassandra-loader -f /path/to/file.csv -host localhost -schema \"test.test3(a, b, c)\"\ncassandra-loader -f /path/to/directory -host 1.2.3.4 -schema \"test.test3(a, b, c)\" -delim \"\\t\" -numThreads 10\ncassandra-loader -f stdin -host localhost -schema \"test.test3(a, b, c)\" -user myuser -pw mypassword\n</pre><p>##Examples:</p><p>Load file /path/to/file.csv into the test3 table in the test keyspace using\nthe cluster at localhost.  Use the default options:</p><pre>cassandra-loader -f /path/to/file.csv -host localhost -schema \"test.test3(a, b, c)\"\n</pre><p>Load all the files from /path/to/directory into the test3 table in the test\nkeyspace using the cluster at 1.2.3.4.  Use 10 threads and use tab as the\ndelimiter:</p><pre>cassandra-loader -f /path/to/directory -host 1.2.3.4 -schema \"test.test3(a, b, c)\" -delim \"\\t\" -numThreads 10\n</pre><p>Load the data from stdin into the test3 table in the test keyspace using the\ncluster at localhost.  Use \"myuser\" as the username and \"mypassword\" as the\npassword:</p><pre>cassandra-loader -f stdin -host localhost -schema \"test.test3(a, b, c)\" -user myuser -pw mypassword\n</pre><p>##Sample</p><p>Included here is a set of sample data.  It is in the sample/ directory.\nYou can set up the table and keyspace by running:</p><pre>cqlsh -f sample/cassandra-schema.cql\n</pre><p>To load the data, run:</p><pre>cd sample\n./load.sh\n</pre><p>To check that things have succeeded, you can run:</p><pre>wc -l titanic.csv\n</pre><p>And:</p><pre>cqlsh -e \"SELECT COUNT(*) FROM titanic.surviors\"\n</pre><p>Both should return 891.</p><h2>cassandra-unloader</h2><p>cassandra-unloader is a utility to dump the contents\nof a Cassandra table to delimited file format.  It uses\nthe same sorts of options as cassandra-loader so that the\noutput of cassandra-unloader could be piped into\ncassandra-loader:</p><pre>cassandra-unloader -f stdout -host host1 -schema \"ks.table(a,b,c)\" | cassandra-loader -f stdin -host host2 -schema \"ks2.table2(x,y,z)\"\n</pre><p>Get it with wget:</p><pre>wget https://github.com/brianmhess/cassandra-loader/releases/download/v0.0.27/cassandra-unloader\n</pre><p>To build, run:</p><pre>gradle unloader\n</pre><p>To run cassandra-unloader, simply run the cassandra-unloader executable\n(e.g., located at build/cassandra-unloader):</p><pre>cassandra-unloader\n</pre><p>###Usage statement:</p><pre>version: 0.0.27\nUsage: -f &lt;outputStem&gt; -host &lt;ipaddress&gt; -schema &lt;schema&gt; [OPTIONS]\nOPTIONS:\n  -configFile &lt;filename&gt;             File with configuration options\n  -format [delim|jsonline|jsonarray] Format of data: delimited or JSON [delim]\n  -delim &lt;delimiter&gt;                 Delimiter to use [,]\n  -dateFormat &lt;dateFormatString&gt;     Date format for TIMESTAMP [default for Locale.ENGLISH]\n  -localDateFormat &lt;FormatString&gt;    Date format for DATE [yyyy-MM-dd]\n  -nullString &lt;nullString&gt;           String that signifies NULL [none]\n  -port &lt;portNumber&gt;                 CQL Port Number [9042]\n  -user &lt;username&gt;                   Cassandra username [none]\n  -pw &lt;password&gt;                     Password for user [none]\n  -ssl-truststore-path &lt;path&gt;        Path to SSL truststore [none]\n  -ssl-truststore-pw &lt;pwd&gt;           Password for SSL truststore [none]\n  -ssl-keystore-path &lt;path&gt;          Path to SSL keystore [none]\n  -ssl-keystore-pw &lt;pwd&gt;             Password for SSL keystore [none]\n  -consistencyLevel &lt;CL&gt;             Consistency level [LOCAL_ONE]\n  -decimalDelim &lt;decimalDelim&gt;       Decimal delimiter [.] Other option is ','\n  -boolStyle &lt;boolStyleString&gt;       Style for booleans [TRUE_FALSE]\n  -numThreads &lt;numThreads&gt;           Number of concurrent threads to unload [5]\n  -beginToken &lt;tokenString&gt;          Begin token [none]\n  -endToken &lt;tokenString&gt;            End token [none]\n  -where &lt;predicate&gt;                 WHERE clause [none]\n  -fetchSize &lt;fetchSize&gt;             Fetch size to use [0]\n</pre><p>A few simple examples using the <code>-where</code> are as follows:</p><pre>cassandra-unloader -host localhost -f stdout -schema \"testks.testtable(pkey,ccol,x,y)\" -where \"pkey=5\"\ncassandra-unloader -host localhost -f stdout -schema \"testks.testtable(pkey,ccol,x,y)\" -where \"x = 100 ALLOW FILTERING\"\n</pre>",
        "created_at": "2018-01-25T21:04:46+0000",
        "updated_at": "2019-01-09T00:01:53+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 11,
        "domain_name": "github.com",
        "preview_picture": "https://avatars3.githubusercontent.com/u/7047840?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9172"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 217,
            "label": "tool",
            "slug": "tool"
          }
        ],
        "is_public": false,
        "id": 9167,
        "uid": null,
        "title": "JGiard/pycfstats",
        "url": "https://github.com/JGiard/pycfstats",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>pŷthon3 tool to convert 'nodetool cfstats' output to csv format</p>\n<p>usage : python3 pycfstats.py -f cfstats.log -o cfstats.csv</p>\n</article>",
        "created_at": "2018-01-25T12:50:01+0000",
        "updated_at": "2018-01-25T20:10:42+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/1167353?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9167"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 217,
            "label": "tool",
            "slug": "tool"
          }
        ],
        "is_public": false,
        "id": 9166,
        "uid": null,
        "title": "thejaspm/cfstatsParseVisualize",
        "url": "https://github.com/thejaspm/cfstatsParseVisualize",
        "content": "<p>cassndra's nodtool utility command \"cfstats\"( - Print statistics on column families) output is not user friendly. Involves efforts to understand the output and make decisions based on the output. Here we aim at parsing the output and generate visualizations to aid and expedite the analysis.</p>",
        "created_at": "2018-01-25T12:49:52+0000",
        "updated_at": "2018-03-07T12:35:32+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "github.com",
        "preview_picture": "https://avatars3.githubusercontent.com/u/3212343?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9166"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 217,
            "label": "tool",
            "slug": "tool"
          }
        ],
        "is_public": false,
        "id": 9111,
        "uid": null,
        "title": "Copying Data Between Cassandra Tables",
        "url": "http://www.wildengineer.com/code-experiments/2016/6/12/copying-cassandra-data",
        "content": "<p>Over the last few months I've been working a great deal with cassandra. Recently I needed to copy data from one cluster to another for testing purposes. The primary problem I faced was that user defined types (UDTs) weren't supported by some of the popular migration tools out there, so I created my own tool. The tool is implemented in java 8 using spring-boot as the foundational framework and the cassandra java driver for data operations. To run it you'll need to download from github, build the project with maven, and invoke the jar with java like so:</p>",
        "created_at": "2018-01-23T03:51:34+0000",
        "updated_at": "2018-01-25T21:22:01+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 0,
        "domain_name": "www.wildengineer.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9111"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 217,
            "label": "tool",
            "slug": "tool"
          }
        ],
        "is_public": false,
        "id": 9110,
        "uid": null,
        "title": "wildengineer/cassandra-data-copy-tool",
        "url": "https://github.com/wildengineer/cassandra-data-copy-tool",
        "content": "<p>This simple java based tool copies data from a live cassandra table to another. The source and destination tables do not need to be on the same cluster or keyspace. All you need to ensure is that the destination table is compatible with the source table.</p><h3>Build</h3><p>The project requires:</p><ul><li>Java 8</li>\n<li>Maven 2 or later versions to run.</li>\n</ul><p>Once you've got your environment setup run from the project directory.</p><blockquote>\n<p>mvn package</p>\n</blockquote><p>This builds a jar in ./target/cassandra-data-copy-tool--SNAPSHOT.jar\"</p><h3>Configuration</h3><p>To configure the tool you'll need to create a properties file. Here's an example:</p><pre>copy.tables=table1,table2,table3=&gt;other_table4,...,tableN\ncopy.ignoreColumns=tab1e1.columnX,table2.columnY\ncopy.batchSize=20000\ncopy.queryPageSize=1000\ncopy.batchesPerSecond=1\nsource.cassandra.contactPoints=127.0.0.1\nsource.cassandra.port=9142\nsource.cassandra.keyspace=test_keyspace\nsource.cassandra.username=cassandra\nsource.cassandra.password=cassandra\ndestination.cassandra.contactPoints=127.0.0.1\ndestination.cassandra.port=9142\ndestination.cassandra.keyspace=test_keyspace\ndestination.cassandra.username=cassandra\ndestination.cassandra.password=cassandra\n</pre><h4>Properties</h4><p>There are three configuration groups:</p><ul><li>Copy</li>\n<li>Source</li>\n<li>Destination</li>\n</ul><h5>Copy</h5><table><thead><tr><th>Property Name</th>\n<th>Description</th>\n<th>Default Value</th>\n</tr></thead><tbody><tr><td>copy.tables</td>\n<td>A column delimited list of table names to copy. If the table names don't match between source and destination use source_table=&gt;dest_table</td>\n<td>\"\"</td>\n</tr><tr><td>copy.ignoreColumns</td>\n<td>A comma delimited list of columns from the source to ignore. Format is TABLE_NAME.COLUMN_NAME</td>\n<td>\"\"</td>\n</tr><tr><td>copy.batchSize</td>\n<td>Size of batches to insert into destination database.</td>\n<td>20000</td>\n</tr><tr><td>copy.queryPageSize</td>\n<td>Size of pages as read from source</td>\n<td>1000</td>\n</tr><tr><td>copy.batchesPerSecond</td>\n<td>Maximum rate of batches copied per second</td>\n<td>1</td>\n</tr></tbody></table><h5>Source</h5><table><thead><tr><th>Property Name</th>\n<th>Description</th>\n<th>Default Value</th>\n</tr></thead><tbody><tr><td>source.cassandra.contactPoints</td>\n<td>Comma delimited list of source cluster's contact points</td>\n<td>127.0.0.1</td>\n</tr><tr><td>source.cassandra.port</td>\n<td>Source cluster's port</td>\n<td>9142</td>\n</tr><tr><td>source.cassandra.keyspace</td>\n<td>Source keyspace name</td>\n<td>\"\"</td>\n</tr><tr><td>source.cassandra.username</td>\n<td>Source username</td>\n<td>cassandra</td>\n</tr><tr><td>source.cassandra.password</td>\n<td>Source plaintext password</td>\n<td>cassandra</td>\n</tr></tbody></table><h5>Destination</h5><table><thead><tr><th>Property Name</th>\n<th>Description</th>\n<th>Default Value</th>\n</tr></thead><tbody><tr><td>destination.cassandra.contactPoints</td>\n<td>Comma delimited list of destination cluster's contact points</td>\n<td>127.0.0.1</td>\n</tr><tr><td>destination.cassandra.port</td>\n<td>Destination cluster's port</td>\n<td>9142</td>\n</tr><tr><td>destination.cassandra.keyspace</td>\n<td>Destination keyspace name</td>\n<td>\"\"</td>\n</tr><tr><td>destination.cassandra.username</td>\n<td>Destination username</td>\n<td>cassandra</td>\n</tr><tr><td>destination.cassandra.password</td>\n<td>Destination plaintext password</td>\n<td>cassandra</td>\n</tr></tbody></table><h3>Run</h3><p>Once you have your property file ready, simply run:</p><blockquote>\n<p>java -jar cassandra-data-copy-tool--SNAPSHOT.jar --spring.config.location=/path/to/config.properties</p>\n</blockquote>",
        "created_at": "2018-01-23T03:45:49+0000",
        "updated_at": "2018-01-25T21:21:41+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://avatars3.githubusercontent.com/u/522791?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9110"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 873,
            "label": "mysql",
            "slug": "mysql"
          }
        ],
        "is_public": false,
        "id": 9104,
        "uid": null,
        "title": "Real-Time Replication from MySQL to Cassandra",
        "url": "https://mcbguru.blog/2014/02/27/real-time-replication-from-mysql-to-cassandra/",
        "content": "<p>Earlier this month I blogged about our new Hadoop applier, I published the docs for that this week (<a href=\"http://docs.continuent.com/tungsten-replicator-3.0/deployment-hadoop.html\">http://docs.continuent.com/tungsten-replicator-3.0/deployment-hadoop.html</a>) as part of the Tungsten Replicator 3.0 documentation (<a href=\"http://docs.continuent.com/tungsten-replicator-3.0/index.html\">http://docs.continuent.com/tungsten-replicator-3.0/index.html</a>). It contains some additional interesting nuggets that will appear in future blog posts.</p>\n<p>The main part of that functionality that performs the actual applier for Hadoop is based around a JavaScript applier engine – there will eventually be docs for that as part of the Batch Applier content (<a href=\"http://docs.continuent.com/tungsten-replicator-3.0/deployment-batchloading.html\">http://docs.continuent.com/tungsten-replicator-3.0/deployment-batchloading.html</a>). The core of this system is that it    takes the information from the data stream of the THL and the CSV file that was written by the batch applier system, and runs the commands necessary to load it into Hadoop and perform any necessary merges.</p>\n<p>I wanted to see how easy it would be to use the same system to use that same flexible system and bend it to another database system, in my case, I chose Cassandra.</p>\n<p>For the record, it took me a couple of hours to have this working, and I’m guessing another hour will file down some of the rough edges.</p>\n<p>Cassandra is interesting as a database because it mixes a big distributed key/value store with a close-enough to SQL like interface in the form of CQL. And that means we can make use of the CQL to help us perform the merging into the final tables in a manner not dissimilar to the method we use for loading into Vertica.</p>\n<p>Back to the Javascript batch loader, the applier provides five different implementable functions (all are technically optional) that you can use at different stages of the applier process. These are:</p>\n<ul><li>prepare() – called once when the applier goes online and can be used to create temporary directories or spaces</li>\n<li>begin() – called at the start of each transaction</li>\n<li>apply() – called at the end of the transaction once the data file has been written, but before the commit</li>\n<li>commit() – called after each transaction commit has taken place; this where we can consolidate info.</li>\n<li>release() – called when the applier goes offline</li>\n</ul><p>We can actually align these functions with a typical transaction – prepare() happens before the statements even start, begin() is the same as BEGIN, apply() happens immediately before COMMIT and commit() happens just after. release() can be used to do any clean up afterwards.</p>\n<p>So let’s put this into practice and use it for Cassandra.</p>\n<p>The basic process for loading is as follows:</p>\n<ol><li>Write a CSV file to load into Cassandra</li>\n<li>Load the CSV file into a staging table within Cassandra; this is easy through CQL using the ‘COPY tablename FROM filename’ CQL statement.</li>\n<li>Merge the staging table data with a live table to create a carbon copy of our MySQL table content.</li>\n</ol><p>For the loading portion, what we’ll do is load the CSV into a staging table, and then we’ll merge the staging table and live table data together during the commit stage of our batch applier. We’ll return to this in more detail.</p>\n<p>For the merging, we’ll take the information from the staging table, which includes the sequence number and operation type, and then write the ‘latest’ version of that row and put it into the live table. That gives us a structure like this:</p>\n<p><a href=\"https://mcslp.files.wordpress.com/2014/02/cassandra-loader.png\"><img data-attachment-id=\"10003\" data-permalink=\"https://mcbguru.blog/2014/02/27/real-time-replication-from-mysql-to-cassandra/cassandra-loader/\" data-orig-file=\"https://mcslp.files.wordpress.com/2014/02/cassandra-loader.png\" data-orig-size=\"2441,1812\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}\" data-image-title=\"Cassandra Loader\" data-image-description=\"\" data-medium-file=\"https://mcslp.files.wordpress.com/2014/02/cassandra-loader.png?w=300&amp;h=222\" data-large-file=\"https://mcslp.files.wordpress.com/2014/02/cassandra-loader.png?w=810\" class=\"alignnone size-medium wp-image-10003\" alt=\"Cassandra Loader\" src=\"https://mcslp.files.wordpress.com/2014/02/cassandra-loader.png?w=300&amp;h=222\" width=\"300\" height=\"222\" srcset=\"https://mcslp.files.wordpress.com/2014/02/cassandra-loader.png?w=300&amp;h=222 300w, https://mcslp.files.wordpress.com/2014/02/cassandra-loader.png?w=598&amp;h=444 598w, https://mcslp.files.wordpress.com/2014/02/cassandra-loader.png?w=150&amp;h=111 150w\" /></a></p>\n<p>Tungsten Replicator is going to manage this entire process for us – all we need to do ins install the replicators, plug in these custom bits, and let it run.</p>\n<p>As with the Hadoop applier, what we’re going to do is use the batch applier to generate only insert and delete rows; UPDATE statements will be converted into a delete of the original version and insert of the new version. So:</p>\n<pre><code>INSERT INTO sample VALUES (1,’Message’)</code></pre>\n<p>Is an insert…</p>\n<pre><code>DELETE sample WHERE id  = 1</code></pre>\n<p>Is a delete, and:</p>\n<pre><code>UPDATE sample SET message = ’Now you see me’ WHERE id = 1</code></pre>\n<p>is actually:</p>\n<pre><code>DELETE sample WHERE id  = 1&#13;\n INSERT INTO sample VALUES (1,’Now you see me’)</code></pre>\n<p>This gets round the problem of doing updates (which in big data stores are expensive, particularly Hadoop which doesn’t support updating existing data), into a more efficient delete and insert.</p>\n<p>In the CSV data itself, this is represented by prefix every row with three fields:</p>\n<pre><code>optype, sequence number, unique id</code></pre>\n<p>Optype is ‘D’ for a delete and ‘I’ for an insert and is used to identify what needs to be done. The sequence number is the unique transaction ID from the replicator THL. This number increases by one for every transaction, and this means we can always identify the ‘latest’ version of a row, which is important to us when processing the transaction into Cassandra. the unique ID is the primary key (or compound key) from the source data. We need this to ensure we update the right row. To replicate data in this way, we must have a primary key on the data. If you don’t have primary keys, you are probably in a world of hurt anyway, so it shouldn’t be a stretch.</p>\n<p>One difficulty here is that we need to cope with an idiosyncracy of Cassandra, which is that by default, Cassandra orders fields in the ‘tables’ (really collections of key/values) so that integers and numbers appear first in the table, and text appears last. This is an optimisation that Cassandra makes that complicates things for us, but only in a very small way. For the moment, we’ll handle it by assuming that we are loading only one table with a known format into Cassandra. We could handle multiple tables by using a simple IF statement in the JS and using different formats for that, or we could actually extract the info from the incoming data; I’m going to skip that because it keeps us away from the cool element of actually getting the data in.</p>\n<p>Within Cassandra then we have two tables, the table we are loading data into, and the staging table that we load the CSV data into. For our sample, the live schema is ‘sample’, the live table is ‘sample’ and the staging table is ‘staging_sample’.</p>\n<p>The definitions for these in Cassandra are for the sample live table:</p>\n<pre><code> CREATE TABLE sample (&#13;\n id int,&#13;\n message text,&#13;\n PRIMARY KEY (id)&#13;\n ) WITH&#13;\n bloom_filter_fp_chance=0.010000 AND&#13;\n caching='KEYS_ONLY' AND&#13;\n comment='' AND&#13;\n dclocal_read_repair_chance=0.000000 AND&#13;\n gc_grace_seconds=864000 AND&#13;\n index_interval=128 AND&#13;\n read_repair_chance=0.100000 AND&#13;\n replicate_on_write='true' AND&#13;\n populate_io_cache_on_flush='false' AND&#13;\n default_time_to_live=0 AND&#13;\n speculative_retry='99.0PERCENTILE' AND&#13;\n memtable_flush_period_in_ms=0 AND&#13;\n compaction={'class': 'SizeTieredCompactionStrategy'} AND&#13;\n compression={'sstable_compression': 'LZ4Compressor'};</code></pre>\n<p>And for the staging_sample table:</p>\n<pre><code>CREATE TABLE staging_sample (&#13;\n optype text,&#13;\n seqno int,&#13;\n fragno int,&#13;\n id int,&#13;\n message text,&#13;\n PRIMARY KEY (optype, seqno, fragno, id)&#13;\n ) WITH&#13;\n bloom_filter_fp_chance=0.010000 AND&#13;\n caching='KEYS_ONLY' AND&#13;\n comment='' AND&#13;\n dclocal_read_repair_chance=0.000000 AND&#13;\n gc_grace_seconds=864000 AND&#13;\n index_interval=128 AND&#13;\n read_repair_chance=0.100000 AND&#13;\n replicate_on_write='true' AND&#13;\n populate_io_cache_on_flush='false' AND&#13;\n default_time_to_live=0 AND&#13;\n speculative_retry='99.0PERCENTILE' AND&#13;\n memtable_flush_period_in_ms=0 AND&#13;\n compaction={'class': 'SizeTieredCompactionStrategy'} AND&#13;\n compression={'sstable_compression': 'LZ4Compressor'};</code></pre>\n<p>I’ve put both tables into a ‘sample’ collection.</p>\n<p>Remember that that idiosyncrasy I mentioned? Here it is, a bare table loading from CSV will actually order the data as:</p>\n<pre><code>seqno,uniqno,id,optype,message</code></pre>\n<p>This is Cassandra’s way of optimising integers over text to speed up lookups, but for us is a minor niggle. Right now, I’m going to handle it by assuming we are replicating only one schema/table and we we not what the structure of that looks like. Longer term, I want to pull it out of the metadata, but that’s a refinement.</p>\n<p>So let’s start by having a look at the basic JS loader script, it’s really the component that is going to handle the core element of the work, managing the CSV files that come in from the batch engine and applying them into Cassandra. Remember, there are five functions that we can define, but for the purposes of this demonstration we’re going to use only two of them, apply(), which will load the CSV file into Cassandra, and the commit() function, which will perform the steps to merge the stage data.</p>\n<p>The apply() function does two things, it identifies the table and schema, and then runs the command to load this data into Cassandra through the cqlsh command-line tool. We actually can’t run CQL directly from this command line, but I wrote a quick shell script that pipes CQL from the command-line into a running cqlsh.</p>\n<p>The commit() function on the other hand is simpler, although it does a much more complicated job using another external script, this time written in Ruby.</p>\n<p>So this gives us a cassandra.js script for the batch applier that looks like this:</p>\n<pre>function apply(csvinfo)&#13;\n{&#13;\n   sqlParams = csvinfo.getSqlParameters();&#13;\n   csv_file = sqlParams.get(\"%%CSV_FILE%%\");&#13;\n   schema = csvinfo.schema;&#13;\n   table = csvinfo.table;&#13;\n  runtime.exec(\"/opt/continuent/share/applycqlsh.sh \" + schema + ' \"copy staging_' + table + \" (optype,seqno,uniqno,id,message) from '\" + csv_file + \"';\\\"\");&#13;\n}&#13;\n&#13;\nfunction commit()&#13;\n{&#13;\n  runtime.exec(\"/opt/continuent/share/merge.rb \" + schema);&#13;\n}</pre>\n<p>So, the apply() function is called for each event as written into the THL from the MySQL binary log, and the content of the CSV file generated at that point contains the contents of the THL event; if it’s one row, it’s a one-row CSV file; if it’s a statement or transaction that created 2000 rows, it’s a 2000 row CSV file.</p>\n<p>The csvinfo object that is provided contains information about the batch file that is written, including, as you can see here, the schema and table names, and the sequence number. Note that we could, at this point, pull out table info, but we’re going to concentrate on pulling a single table here just for demo purposes.</p>\n<p>The CQL for loading the CSV data is:</p>\n<pre><code>COPY staging_tablename (optype,seqno,uniqno,id,message) from ‘FILENAME’;</code></pre>\n<p>This says, copy the the specific columns in this order from the file into the specified table.  As I mentioned, currently this is hard coded into the applier JS, but would be easy to handle for more complex schemas and structures.</p>\n<p>The commit() function is even simpler, because it just calls a script that will do the merging for us – we’ll get to that in a minute.</p>\n<p>So here’s the script that applies an arbitrary CQL statement into Cassandra:</p>\n<pre> #!/bin/bash&#13;\nSCHEMA=$1;shift&#13;\necho \"$*\" |cqlsh -k $SCHEMA tr-cassandra2</pre>\n<p>Really simple, but gets round a simple issue.</p>\n<p>The script that does the merge work is more complex; in other environments we might be able to do this all within SQL, but CQL is fairly limited with no sub-queries. So we do it long-hand using Ruby. The basic sequence is quite simple, and is in two phases:</p>\n<ol><li>Delete every row mentioned in the staging table with an optype of D with a matching unique key</li>\n<li>Insert the *last* version of an insert for each unique ID – the last version will be the latest one in the output. We can pick this out by just iterating over every insert and picking the one with the highest Sequence number as generated by the THL transaction ID.</li>\n<li>Delete the content from the staging table because we’ve finished with it. That empties the staging table ready for the next set of transactions.</li>\n</ol><p>That file looks like this:</p>\n<pre>#!/usr/bin/ruby&#13;\n&#13;\nrequire 'cql'&#13;\n&#13;\nclient = Cql::Client.connect(hosts: ['192.168.1.51'])&#13;\nclient.use('sample')&#13;\n&#13;\nrows = client.execute(\"SELECT id FROM staging_sample where optype = 'D'\")&#13;\n&#13;\ndeleteids = Array.new()&#13;\n&#13;\nrows.each do |row|&#13;\nputs \"Found ID #{row['id']} has to be deleted\"&#13;\ndeleteids.push(row['id'])&#13;\nend&#13;\n&#13;\ndeleteidlist = deleteids.join(\",\")&#13;\n&#13;\nclient.execute(\"delete from sample where id in (#{deleteidlist})\");&#13;\nputs(\"delete from sample where id in (#{deleteidlist})\");&#13;\nrows = client.execute(\"SELECT * FROM staging_sample where optype = 'I'\");&#13;\n&#13;\nupdateids = Hash.new()&#13;\nupdatedata = Hash.new()&#13;\n&#13;\nrows.each do |row|&#13;\nid = row['id']&#13;\nputs \"Found ID #{id} seq #{row['seqno']} has to be inserted\"&#13;\nif updateids[id]&#13;\nif updateids[id] &lt; row['seqno']&#13;\nupdateids[id] = row['seqno']&#13;\nrow.delete('seqno')&#13;\nrow.delete('fragno')&#13;\nrow.delete('optype')&#13;\nupdatedata[id] = row&#13;\nend&#13;\nelse&#13;\nupdateids[id] = row['seqno']&#13;\nrow.delete('seqno')&#13;\nrow.delete('fragno')&#13;\nrow.delete('optype')&#13;\nupdatedata[id] = row&#13;\nend&#13;\nend&#13;\n&#13;\nupdatedata.each do |rowid,rowdata|&#13;\nputs \"Should update #{rowdata['id']} with #{rowdata['message']}\"&#13;\ncollist = rowdata.keys.join(',')&#13;\ncolcount = rowdata.keys.length&#13;\nsubstbase = Array.new()&#13;\n#  (1..colcount).each {substbase.push('?')}&#13;\nrowdata.values.each do |value|&#13;\nif value.is_a?(String)&#13;\nsubstbase.push(\"'\" + value.to_s + \"'\")&#13;\nelse&#13;\nsubstbase.push(value)&#13;\nend&#13;\nend&#13;\n&#13;\nsubstlist = substbase.join(',')&#13;\n&#13;\nputs('Column list: ',collist)&#13;\nputs('Subst list: ',substlist)&#13;\ncqlinsert = \"insert into sample (\"+collist+\") values (\"+substlist+\")\"&#13;\nputs(\"Statement: \" + cqlinsert)&#13;\nclient.execute(cqlinsert)&#13;\nend&#13;\n&#13;\nclient.execute(\"delete from staging_sample where optype in ('D','I')\")</pre>\n<p>Again, currently, this is hard coded, but I could easily of got the schema/table name from the JS batch applier – the actual code is table agnostic and will work with any table.</p>\n<p>So, I’ve setup two replicators – one uses the cassandra.js rather than hadoop.js but works the same way, and copied the applycqlsh.sh and merge.rb into /opt/continuent/share.</p>\n<p>And we’re ready to run. Let’s try it:</p>\n<pre>mysql&gt; insert into sample values (0,'First Message’);&#13;\nQuery OK, 1 row affected (0.01 sec)</pre>\n<p>We’ve inserted one row. Let’s go check Cassandra:</p>\n<pre>cqlsh:sample&gt; select * from sample;&#13;\n&#13;\nid  | message&#13;\n-----+---------------&#13;\n489 | First Message</pre>\n<p>Woohoo – data from MySQL straight into Cassandra.</p>\n<p>Now let’s try updating it:</p>\n<pre>mysql&gt; update sample set message = 'Updated Message' where id = 489;&#13;\nQuery OK, 1 row affected (0.01 sec)&#13;\nRows matched: 1  Changed: 1  Warnings: 0</pre>\n<p>And in Cassandra:</p>\n<pre>cqlsh:sample&gt; select * from sample;&#13;\n&#13;\nid  | message&#13;\n-----+-----------------&#13;\n489 | Updated Message</pre>\n<p>Bigger woohoo. Not only am I loading data directly into Cassandra, but I can update it as well. Now I can have a stream of update and information within MySQL replicated over to Cassandra for whatever analysis or information that I need without any issues.</p>\n<p>Cool huh? I certainly think so (OK, but I’m biased).</p>\n<p>Now I haven’t tested it, but this should just as easily work from Oracle; I’ll be testing that and let you know.</p>\n<p>Any other database destinations people would like to see for replicating into? If so, let me know and I’ll see what I can do.</p>",
        "created_at": "2018-01-23T00:05:27+0000",
        "updated_at": "2018-03-14T22:43:33+0000",
        "published_at": "2014-02-27T19:09:34+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 13,
        "domain_name": "mcbguru.blog",
        "preview_picture": "https://mcslp.files.wordpress.com/2014/02/cassandra-loader.png?w=300",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9104"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 253,
            "label": "analytics",
            "slug": "analytics"
          },
          {
            "id": 391,
            "label": "big.data",
            "slug": "big-data"
          }
        ],
        "is_public": false,
        "id": 9081,
        "uid": null,
        "title": "Cassandra as an event sourced journal for big data analytics Cassandr…",
        "url": "https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015",
        "content": "Cassandra as an event sourced journal for big data analytics Cassandr…\n      \n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            </a><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div>\n    <div class=\"wrapper\"><p>Successfully reported this slideshow.</p><div id=\"slideview-container\" class=\"\"><div class=\"row\"><div id=\"main-panel\" class=\"small-12 large-8 columns\"><div class=\"sectionElements\"><div class=\"playerWrapper\"><div><div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\">Cassandra as an event sourced journal for big data analytics Cassandra Summit 2015<div class=\"stage valign-first-slide\"><div class=\"slide_container\"><section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-1-638.jpg?cb=1443131432\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-1-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-1-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-1-1024.jpg?cb=1443131432\" alt=\"Martin Zapletal @zapletal_martin&#10;Cake Solutions @cakesolutions&#10;#CassandraSummit&#10;Presented by Anirvan Chakraborty @anirvan_c&#10;\" /></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-2-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-2-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-2-1024.jpg?cb=1443131432\" alt=\"● Introduction&#10;● Event sourcing and CQRS&#10;● An emerging technology stack to handle data&#10;● A reference application and it’s ...\" /></i></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-3-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-3-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-3-1024.jpg?cb=1443131432\" alt=\"● Increasing importance of data analytics&#10;● Current state&#10;○ Destructive updates&#10;○ Analytics tools with poor scalability an...\" /></i></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-4-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-4-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-4-1024.jpg?cb=1443131432\" alt=\"● Whole lifecycle of data&#10;● Data processing&#10;● Data stores&#10;● Integration and messaging&#10;● Distributed computing primitives&#10;●...\" /></i></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-5-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-5-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-5-1024.jpg?cb=1443131432\" alt=\"ACID Mutable State&#10;\" /></i></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-6-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-6-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-6-1024.jpg?cb=1443131432\" alt=\"● Create, Read, Update, Delete&#10;● Exposes mutable internal state&#10;● Many read methods on repositories&#10;● Mapping of data mode...\" /></i></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-7-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-7-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-7-1024.jpg?cb=1443131432\" alt=\"[1]&#10;CQRS&#10;Client&#10;QueryCommand&#10;DBDB&#10;Denormalise&#10;/Precompute&#10;Kappa architecture&#10;Batch-Pipeline&#10;Kafka&#10;Allyourdata&#10;NoSQL&#10;SQL&#10;Sp...\" /></i></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-8-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-8-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-8-1024.jpg?cb=1443131432\" alt=\"[2, 3]&#10;\" /></i></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-9-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-9-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-9-1024.jpg?cb=1443131432\" alt=\"● Append only data store&#10;● No updates or deletes (rewriting history)&#10;● Immutable data model&#10;● Decouples data model of the ...\" /></i></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-10-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-10-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-10-1024.jpg?cb=1443131432\" alt=\"userId date change&#10;1&#10;1&#10;1&#10;10/10/2015&#10;11/10/2015&#10;23/10/2015&#10;+300&#10;-100&#10;-200&#10;1 24/10/2015 +100&#10;balanceChanged&#10;event&#10;balanceCha...\" /></i></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-11-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-11-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-11-1024.jpg?cb=1443131432\" alt=\"● Command Query Responsibility Segregation&#10;● Read and write logically and physically separated&#10;● Reasoning about the appli...\" /></i></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-12-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-12-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-12-1024.jpg?cb=1443131432\" alt=\"Command&#10;● Write side&#10;● Messages, requests to mutate state&#10;● Behaviour, serialized method call essentially&#10;● Don’t expose s...\" /></i></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-13-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-13-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-13-1024.jpg?cb=1443131432\" alt=\"userId = 1&#10;updateBalance&#10;(+100)&#10;Write&#10;Command&#10;Event&#10;userId date change&#10;1&#10;1&#10;1&#10;10/10/2015&#10;11/10/2015&#10;23/10/2015&#10;+300&#10;-100&#10;-2...\" /></i></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-14-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-14-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-14-1024.jpg?cb=1443131432\" alt=\"● Partial order of events for each entity&#10;● Operation semantics, CRDTs&#10;UserNameUpdated(B)&#10;UserNameUpdated(B)&#10;UserNameUpdat...\" /></i></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-15-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-15-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-15-1024.jpg?cb=1443131432\" alt=\"● Localization&#10;● Conflicting concurrent histories&#10;○ Resubmission&#10;○ Deduplication&#10;○ Replication&#10;● Identifier&#10;● Version&#10;● Ti...\" /></i></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-16-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-16-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-16-1024.jpg?cb=1443131432\" alt=\"● Actor framework for truly concurrent and distributed systems&#10;● Thread safe mutable state - consistency boundary&#10;● Domain...\" /></i></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-17-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-17-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-17-1024.jpg?cb=1443131432\" alt=\"?&#10;?&#10;? + 1&#10;? + 1&#10;? + 2&#10;UserId = 1&#10;Name = Bob&#10;BankAccountId = 1&#10;Balance = 1000&#10;UserId = 1&#10;Name = Alice&#10;\" /></i></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-18-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-18-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-18-1024.jpg?cb=1443131432\" alt=\"● Distributed domain modelling&#10;● In memory&#10;● Ordering, consistency&#10;id = 1&#10;\" /></i></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-19-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-19-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-19-1024.jpg?cb=1443131432\" alt=\"● Actor backed by data store&#10;● Immutable event sourced journal&#10;● Supports CQRS (write and read side)&#10;● Persistence, replay...\" /></i></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-20-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-20-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-20-1024.jpg?cb=1443131432\" alt=\"user1, event 2&#10;user1, event 3&#10;user1, event 4&#10;user1, event 1&#10;\" /></i></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-21-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-21-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-21-1024.jpg?cb=1443131432\" alt=\"class UserActor extends PersistentActor {&#10;override def persistenceId: String = UserPersistenceId(self.path.name).persisten...\" /></i></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-22-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-22-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-22-1024.jpg?cb=1443131432\" alt=\"● Akka Persistence Cassandra journal&#10;○ Globally distributed journal&#10;○ Scalable, resilient, highly available&#10;○ Performant, ...\" /></i></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-23-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-23-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-23-1024.jpg?cb=1443131432\" alt=\"● Partition-size&#10;● Events in each cluster partition ordered (persistenceId - partition pair)&#10;CREATE TABLE IF NOT EXISTS ${...\" /></i></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-24-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-24-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-24-1024.jpg?cb=1443131432\" alt=\"● Internal state, moment in time&#10;● Read optimization&#10;CREATE TABLE IF NOT EXISTS ${tableName} (&#10;processor_id text,&#10;sequence...\" /></i></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-25-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-25-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-25-1024.jpg?cb=1443131432\" alt=\"● Uses Akka serialization&#10;0x0a6643b334 …&#10;PersistentRepr&#10;Akka.Serialization&#10;Payload: T&#10;Protobuff&#10;actor {&#10;serialization-bind...\" /></i></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-26-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-26-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-26-1024.jpg?cb=1443131432\" alt=\"class UserActorView(userId: String) extends PersistentView {&#10;override def persistenceId: String = UserPersistenceId(userId...\" /></i></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-27-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-27-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-27-1024.jpg?cb=1443131432\" alt=\"● Akka 2.4&#10;● Potentially infinite stream of data&#10;● Ordered, replayable, resumable&#10;● Aggregation, transformation, moving da...\" /></i></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-28-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-28-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-28-1024.jpg?cb=1443131432\" alt=\"val readJournal =&#10;PersistenceQuery(system).readJournalFor(CassandraJournal.Identifier)&#10;val source = readJournal.query(&#10;Eve...\" /></i></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-29-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-29-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-29-1024.jpg?cb=1443131432\" alt=\"● Potentially infinite stream of events&#10;Source[Any].map(process).filter(condition)&#10;Publisher Subscriber&#10;process&#10;condition&#10;...\" /></i></section><section data-index=\"30\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-30-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-30-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-30-1024.jpg?cb=1443131432\" alt=\"● In Akka we have the read and write sides separated,&#10;in Cassandra we don’t&#10;● Different data model&#10;● Avoid using operation...\" /></i></section><section data-index=\"31\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-31-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-31-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-31-1024.jpg?cb=1443131432\" alt=\"● Computations and analytics queries on the data&#10;● Often iterative, complex, expensive computations&#10;● Prepared and interac...\" /></i></section><section data-index=\"32\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-32-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-32-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-32-1024.jpg?cb=1443131432\" alt=\"● Cassandra 3.0 - user defined functions, functional indexes, aggregation&#10;functions, materialized views&#10;● Server side deno...\" /></i></section><section data-index=\"33\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-33-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-33-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-33-1024.jpg?cb=1443131432\" alt=\"● In memory dataflow distributed data processing framework, streaming&#10;and batch&#10;● Distributes computation using a higher l...\" /></i></section><section data-index=\"34\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-34-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-34-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-34-1024.jpg?cb=1443131432\" alt=\"● Resilient Distributed Datasets&#10;● Fault tolerance&#10;● Caching&#10;● Serialization&#10;● Transformations&#10;○ Lazy, form the DAG&#10;○ map,...\" /></i></section><section data-index=\"35\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-35-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-35-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-35-1024.jpg?cb=1443131432\" alt=\"textFile mapmap&#10;reduceByKey&#10;collect&#10;sc.textFile(&quot;counts&quot;)&#10;.map(line =&gt; line.split(&quot;t&quot;))&#10;.map(word =&gt; (word(0), word(1).toI...\" /></i></section><section data-index=\"36\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-36-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-36-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-36-1024.jpg?cb=1443131432\" alt=\"Spark master&#10;Spark worker&#10;Cassandra&#10;\" /></i></section><section data-index=\"37\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-37-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-37-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-37-1024.jpg?cb=1443131432\" alt=\"● Cassandra can store&#10;● Spark can process&#10;● Gathering large amounts of heterogeneous data&#10;● Queries&#10;● Transformations&#10;● Co...\" /></i></section><section data-index=\"38\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-38-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-38-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-38-1024.jpg?cb=1443131432\" alt=\"lazy val sparkConf: SparkConf =&#10;new SparkConf()&#10;.setAppName(...).setMaster(...).set(&quot;spark.cassandra.connection.host&quot;, &quot;12...\" /></i></section><section data-index=\"39\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-39-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-39-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-39-1024.jpg?cb=1443131432\" alt=\"● Akka Analytics project&#10;● Handles custom Akka serialization&#10;case class JournalKey(persistenceId: String, partition: Long,...\" /></i></section><section data-index=\"40\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-40-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-40-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-40-1024.jpg?cb=1443131432\" alt=\"● Spark streaming&#10;● Precomputing using spark or replication often aiming for different data&#10;model&#10;Operational cluster Anal...\" /></i></section><section data-index=\"41\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-41-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-41-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-41-1024.jpg?cb=1443131432\" alt=\"val events: RDD[(JournalKey, Any)] = sc.eventTable().cache().filterClass[EntireResistanceExerciseSession].flatMap(_.deviat...\" /></i></section><section data-index=\"42\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-42-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-42-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-42-1024.jpg?cb=1443131432\" alt=\"def toVector(user: User): mllib.linalg.Vector =&#10;Vectors.dense(&#10;user.frequency, user.performanceIndex, user.improvementInde...\" /></i></section><section data-index=\"43\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-43-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-43-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-43-1024.jpg?cb=1443131432\" alt=\"val weight: RDD[(JournalKey, Any)] = sc.eventTable().cache()&#10;val exerciseDeviations = events&#10;.filterClass[EntireResistance...\" /></i></section><section data-index=\"44\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-44-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-44-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-44-1024.jpg?cb=1443131432\" alt=\"val events = sc.eventTable().cache().toDF()&#10;val lr = new LinearRegression()&#10;val pipeline = new Pipeline().setStages(Array(...\" /></i></section><section data-index=\"45\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-45-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-45-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-45-1024.jpg?cb=1443131432\" alt=\"val events: RDD[(JournalKey, Any)] = sc.eventTable().cache()&#10;val connections = events.filterClass[Connections]&#10;val vertice...\" /></i></section><section data-index=\"46\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-46-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-46-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-46-1024.jpg?cb=1443131432\" alt=\"7 * Dumbbell&#10;Alternating Curl&#10;\" /></i></section><section data-index=\"47\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-47-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-47-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-47-1024.jpg?cb=1443131432\" alt=\"Data&#10;Data&#10;Preprocessing&#10;Preprocessing&#10;Features&#10;Features&#10;Training&#10;Testing&#10;Error %&#10;\" /></i></section><section data-index=\"48\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-48-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-48-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-48-1024.jpg?cb=1443131432\" alt=\"● Exercise domain as an example&#10;● Analytics of both batch (offline) and streaming (online) data&#10;● Analytics important in o...\" /></i></section><section data-index=\"49\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-49-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-49-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-49-1024.jpg?cb=1443131432\" alt=\"● Event sourcing&#10;● CQRS&#10;● Technologies to handle the data&#10;○ Spark&#10;○ Mesos&#10;○ Akka&#10;○ Cassandra&#10;○ Kafka&#10;● Handling data&#10;● Ins...\" /></i></section><section data-index=\"50\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-50-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-50-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-50-1024.jpg?cb=1443131432\" alt=\"● Jobs at www.cakesolutions.net/careers&#10;● Code at https://github.com/muvr&#10;● Martin Zapletal @zapletal_martin&#10;● Anirvan Cha...\" /></i></section><section data-index=\"51\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-51-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-51-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-51-1024.jpg?cb=1443131432\" alt=\"[1] http://www.benstopford.com/2015/04/28/elements-of-scale-composing-and-scaling-data-platforms/&#10;[2] http://malteschwarzk...\" /></i></section><section data-index=\"52\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-52-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-52-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-52-1024.jpg?cb=1443131432\" alt=\"Cassandra as an event sourced journal for big data analytics Cassandra Summit 2015\" /></i></section><section data-index=\"53\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-53-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-53-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-53-1024.jpg?cb=1443131432\" alt=\"Cassandra as an event sourced journal for big data analytics Cassandra Summit 2015\" /></i></section><section data-index=\"54\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-54-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-54-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-54-1024.jpg?cb=1443131432\" alt=\"Cassandra as an event sourced journal for big data analytics Cassandra Summit 2015\" /></i></section><section data-index=\"55\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-55-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-55-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-55-1024.jpg?cb=1443131432\" alt=\"Cassandra as an event sourced journal for big data analytics Cassandra Summit 2015\" /></i></section><section data-index=\"56\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-56-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-56-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-56-1024.jpg?cb=1443131432\" alt=\"Cassandra as an event sourced journal for big data analytics Cassandra Summit 2015\" /></i></section><section data-index=\"57\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/MartinZapletal/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015\" data-small=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/85/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-57-320.jpg?cb=1443131432\" data-normal=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-57-638.jpg?cb=1443131432\" data-full=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-57-1024.jpg?cb=1443131432\" alt=\"Cassandra as an event sourced journal for big data analytics Cassandra Summit 2015\" /></i></section><div class=\"j-next-container next-container\"><div class=\"content-container\"><div class=\"next-slideshow-wrapper\"><div class=\"j-next-slideshow next-slideshow\"><p>Upcoming SlideShare</p></div><p>Loading in …5</p><p>×</p></div></div></div></div></div></div></div></div></div><div class=\"slideshow-info-container\" itemscope=\"itemscope\" itemtype=\"https://schema.org/MediaObject\"><div class=\"slideshow-tabs-container show-for-medium-up\"><ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    0 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\">\n                \n                  10 Likes\n                \n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\">\n                Statistics\n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\">\n                Notes\n              </i></a>\n            </li>\n          </ul><div class=\"tabs-content\"><div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\"><ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"itemscope\"><li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"GregYeo\" rel=\"nofollow\" href=\"https://www.slideshare.net/GregYeo?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Greg Yeo\n                            \n                              \n                                \n                                \n                              \n                              \n                                \n                                \n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"RobODoherty\" rel=\"nofollow\" href=\"https://www.slideshare.net/RobODoherty?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Rob O'Doherty\n                            \n                              \n                                \n                                \n                              \n                              \n                                \n                                \n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"ChristianHellstr\" rel=\"nofollow\" href=\"https://www.slideshare.net/ChristianHellstr?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Christian Hellström\n                            \n                              \n                                , \n                                Data Engineer at Spotify\n                              \n                              \n                                 at \n                                Spotify\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"bpmtri\" rel=\"nofollow\" href=\"https://www.slideshare.net/bpmtri?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Tri Bui Pham Minh\n                            \n                              \n                                , \n                                Solutions Architect, Hands-on Developer\n                              \n                              \n                                 at \n                                PYCOGROUP\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"pavelkudinov\" rel=\"nofollow\" href=\"https://www.slideshare.net/pavelkudinov?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Pavel Kudinov\n                            \n                              \n                                , \n                                Software Engineering &amp; Data Science\n                              \n                              \n                                 at \n                                RedMart\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n              </ul><div class=\"more-container text-center\"><a href=\"#\" class=\"j-more-favs\">\n                    Show More\n                    \n                  </a></div></div><div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No Downloads</p></div><div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No notes for slide</p></div></div></div><div class=\"notranslate transcript add-padding-right j-transcript\"><ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    Martin Zapletal @zapletal_martin\nCake Solutions @cakesolutions\n#CassandraSummit\nPresented by Anirvan Chakraborty @anirvan_c\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-2-638.jpg?cb=1443131432\" title=\"● Introduction&#10;● Event sourcing and CQRS&#10;● An emerging tech...\" target=\"_blank\">\n        2.\n      </a>\n    ● Introduction\n● Event sourcing and CQRS\n● An emerging technology stack to handle data\n● A reference application and it’s architecture\n● A few use cases of the reference application\n● Conclusion\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-3-638.jpg?cb=1443131432\" title=\"● Increasing importance of data analytics&#10;● Current state&#10;○...\" target=\"_blank\">\n        3.\n      </a>\n    ● Increasing importance of data analytics\n● Current state\n○ Destructive updates\n○ Analytics tools with poor scalability and integration\n○ Manual processes\n○ Slow iterations\n○ Not suitable for large amounts of data\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-4-638.jpg?cb=1443131432\" title=\"● Whole lifecycle of data&#10;● Data processing&#10;● Data stores&#10;●...\" target=\"_blank\">\n        4.\n      </a>\n    ● Whole lifecycle of data\n● Data processing\n● Data stores\n● Integration and messaging\n● Distributed computing primitives\n● Cluster managers and task schedulers\n● Deployment, configuration management and DevOps\n● Data analytics and machine learning\n● Spark, Mesos, Akka, Cassandra, Kafka (SMACK, Infinity)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-5-638.jpg?cb=1443131432\" title=\"ACID Mutable State&#10;\" target=\"_blank\">\n        5.\n      </a>\n    ACID Mutable State\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-6-638.jpg?cb=1443131432\" title=\"● Create, Read, Update, Delete&#10;● Exposes mutable internal s...\" target=\"_blank\">\n        6.\n      </a>\n    ● Create, Read, Update, Delete\n● Exposes mutable internal state\n● Many read methods on repositories\n● Mapping of data model and objects (impedance mismatch)\n● No auditing\n● No separation of concerns (read / write, command / event)\n● Strongly consistent\n● Difficult optimizations of reads / writes\n● Difficult to scale\n● Intent, behaviour, history, is lost\nBalance = 5\nBalance = 10\nUpdate\nAccount\nBalance = 10\nAccount\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-7-638.jpg?cb=1443131432\" title=\"[1]&#10;CQRS&#10;Client&#10;QueryCommand&#10;DBDB&#10;Denormalise&#10;/Precompute&#10;K...\" target=\"_blank\">\n        7.\n      </a>\n    [1]\nCQRS\nClient\nQueryCommand\nDBDB\nDenormalise\n/Precompute\nKappa architecture\nBatch-Pipeline\nKafka\nAllyourdata\nNoSQL\nSQL\nSpark\nClient\nClient\nClient Views\nStream\nprocessor\nFlume\nScoop\nHive\nImpala\nOozie\nHDFS\nLambda Architecture\nBatch Layer Serving\nLayer\nStream layer (fast)\nQuery\nQuery\nAllyourdata Serving DB\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-8-638.jpg?cb=1443131432\" title=\"[2, 3]&#10;\" target=\"_blank\">\n        8.\n      </a>\n    [2, 3]\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-9-638.jpg?cb=1443131432\" title=\"● Append only data store&#10;● No updates or deletes (rewriting...\" target=\"_blank\">\n        9.\n      </a>\n    ● Append only data store\n● No updates or deletes (rewriting history)\n● Immutable data model\n● Decouples data model of the application and storage\n● Current state not persisted, but derived. A sequence of updates that led to it.\n● History, state known at any point in time\n● Replayable\n● Source of truth\n● Optimisations possible\n● Works well in distributed environment - easy partitioning, conflicts\n● Helps avoiding transactions\n● Works well with DDD\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-10-638.jpg?cb=1443131432\" title=\"userId date change&#10;1&#10;1&#10;1&#10;10/10/2015&#10;11/10/2015&#10;23/10/2015&#10;+...\" target=\"_blank\">\n        10.\n      </a>\n    userId date change\n1\n1\n1\n10/10/2015\n11/10/2015\n23/10/2015\n+300\n-100\n-200\n1 24/10/2015 +100\nbalanceChanged\nevent\nbalanceChanged\nbalanceChanged\nbalanceChanged\nEvent journal\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-11-638.jpg?cb=1443131432\" title=\"● Command Query Responsibility Segregation&#10;● Read and write...\" target=\"_blank\">\n        11.\n      </a>\n    ● Command Query Responsibility Segregation\n● Read and write logically and physically separated\n● Reasoning about the application\n● Clear separation of concerns (business logic)\n● Often different technology, scalability\n● Often lower consistency - eventual, causal\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-12-638.jpg?cb=1443131432\" title=\"Command&#10;● Write side&#10;● Messages, requests to mutate state&#10;●...\" target=\"_blank\">\n        12.\n      </a>\n    Command\n● Write side\n● Messages, requests to mutate state\n● Behaviour, serialized method call essentially\n● Don’t expose state\n● Validated and may be rejected or emit one or more events (e.g. submitting a form)\nEvent\n● Write side\n● Immutable\n● Indicating something that has happened\n● Atomic record of state change\n● Audit log\nQuery\n● Read side\n● Precomputed\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-13-638.jpg?cb=1443131432\" title=\"userId = 1&#10;updateBalance&#10;(+100)&#10;Write&#10;Command&#10;Event&#10;userId ...\" target=\"_blank\">\n        13.\n      </a>\n    userId = 1\nupdateBalance\n(+100)\nWrite\nCommand\nEvent\nuserId date change\n1\n1\n1\n10/10/2015\n11/10/2015\n23/10/2015\n+300\n-100\n-200\n1 24/10/2015 +100\nbalance\nChanged\nevent\nbalance\nChanged\nbalance\nChanged\nbalance\nChanged\nEvent journal\nCommand\nhandler\nRead\nbalance\n1 100\nuserId = 1\nbalance = 100\nQuery\nuserId\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-14-638.jpg?cb=1443131432\" title=\"● Partial order of events for each entity&#10;● Operation seman...\" target=\"_blank\">\n        14.\n      </a>\n    ● Partial order of events for each entity\n● Operation semantics, CRDTs\nUserNameUpdated(B)\nUserNameUpdated(B)\nUserNameUpdated(A)\nUserNameUpdated(A)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-15-638.jpg?cb=1443131432\" title=\"● Localization&#10;● Conflicting concurrent histories&#10;○ Resubmi...\" target=\"_blank\">\n        15.\n      </a>\n    ● Localization\n● Conflicting concurrent histories\n○ Resubmission\n○ Deduplication\n○ Replication\n● Identifier\n● Version\n● Timestamp\n● Vector clock\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-16-638.jpg?cb=1443131432\" title=\"● Actor framework for truly concurrent and distributed syst...\" target=\"_blank\">\n        16.\n      </a>\n    ● Actor framework for truly concurrent and distributed systems\n● Thread safe mutable state - consistency boundary\n● Domain modelling, distributed state\n● Simple programming model - asynchronously send messages, create\nnew actors, change behaviour\n● Supports CQRS/ES\n● Fully distributed - asynchronous, delivery guarantees, failures, time\nand order, consistency, availability, communication patterns, data\nlocality, persistence, durability, concurrent updates, conflicts,\ndivergence, invariants, ...\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-17-638.jpg?cb=1443131432\" title=\"?&#10;?&#10;? + 1&#10;? + 1&#10;? + 2&#10;UserId = 1&#10;Name = Bob&#10;BankAccountId =...\" target=\"_blank\">\n        17.\n      </a>\n    ?\n?\n? + 1\n? + 1\n? + 2\nUserId = 1\nName = Bob\nBankAccountId = 1\nBalance = 1000\nUserId = 1\nName = Alice\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-18-638.jpg?cb=1443131432\" title=\"● Distributed domain modelling&#10;● In memory&#10;● Ordering, cons...\" target=\"_blank\">\n        18.\n      </a>\n    ● Distributed domain modelling\n● In memory\n● Ordering, consistency\nid = 1\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-19-638.jpg?cb=1443131432\" title=\"● Actor backed by data store&#10;● Immutable event sourced jour...\" target=\"_blank\">\n        19.\n      </a>\n    ● Actor backed by data store\n● Immutable event sourced journal\n● Supports CQRS (write and read side)\n● Persistence, replay on failure, rebalance, at least once delivery\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-20-638.jpg?cb=1443131432\" title=\"user1, event 2&#10;user1, event 3&#10;user1, event 4&#10;user1, event 1&#10;\" target=\"_blank\">\n        20.\n      </a>\n    user1, event 2\nuser1, event 3\nuser1, event 4\nuser1, event 1\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-21-638.jpg?cb=1443131432\" title=\"class UserActor extends PersistentActor {&#10;override def pers...\" target=\"_blank\">\n        21.\n      </a>\n    class UserActor extends PersistentActor {\noverride def persistenceId: String = UserPersistenceId(self.path.name).persistenceId\noverride def receiveCommand: Receive = notRegistered(DistributedData(context.system).replicator)\ndef notRegistered(distributedData: ActorRef): Receive = {\ncase cmd: AccountCommand =&gt;\npersist(AccountEvent(cmd.account)){ acc =&gt;\ncontext.become(registered(acc))\nsender() ! /-()\n}\n}\ndef registered(account: Account): Receive = {\ncase eres @ EntireResistanceExerciseSession(id, session, sets, examples, deviations) =&gt;\npersist(eres)(data =&gt; sender() ! /-(id))\n}\noverride def receiveRecover: Receive = {\n...\n}\n}\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-22-638.jpg?cb=1443131432\" title=\"● Akka Persistence Cassandra journal&#10;○ Globally distributed...\" target=\"_blank\">\n        22.\n      </a>\n    ● Akka Persistence Cassandra journal\n○ Globally distributed journal\n○ Scalable, resilient, highly available\n○ Performant, operational database\n● Community plugins\nakka {\npersistence {\njournal.plugin = \"cassandra-journal\"\nsnapshot-store.plugin = \"cassandra-snapshot-store\"\n}\n}\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-23-638.jpg?cb=1443131432\" title=\"● Partition-size&#10;● Events in each cluster partition ordered...\" target=\"_blank\">\n        23.\n      </a>\n    ● Partition-size\n● Events in each cluster partition ordered (persistenceId - partition pair)\nCREATE TABLE IF NOT EXISTS ${tableName} (\nprocessor_id text,\npartition_nr bigint,\nsequence_nr bigint,\nmarker text,\nmessage blob,\nPRIMARY KEY ((processor_id, partition_nr),\nsequence_nr, marker))\nWITH COMPACT STORAGE\nAND gc_grace_seconds = ${config.\ngc_grace_seconds}\nprocessor_id partition_nr sequence_nr marker message\nuser-1 0 0 H 0x0a6643b334...\nuser-1 0 1 A 0x0ab2020801...\nuser-1 0 2 A 0x0a98020801...\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-24-638.jpg?cb=1443131432\" title=\"● Internal state, moment in time&#10;● Read optimization&#10;CREATE...\" target=\"_blank\">\n        24.\n      </a>\n    ● Internal state, moment in time\n● Read optimization\nCREATE TABLE IF NOT EXISTS ${tableName} (\nprocessor_id text,\nsequence_nr bigint,\ntimestamp bigint,\nsnapshot blob,\nPRIMARY KEY (processor_id, sequence_nr))\nWITH CLUSTERING ORDER BY (sequence_nr DESC)\nprocessor_id sequence_nr snapshot timestamp\nuser-1 16 0x0400000001... 1441696908210\nuser-1 20 0x0400000001... 1441697587765\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-25-638.jpg?cb=1443131432\" title=\"● Uses Akka serialization&#10;0x0a6643b334 …&#10;PersistentRepr&#10;Akk...\" target=\"_blank\">\n        25.\n      </a>\n    ● Uses Akka serialization\n0x0a6643b334 …\nPersistentRepr\nAkka.Serialization\nPayload: T\nProtobuff\nactor {\nserialization-bindings {\n\"io.muvr.exercise.ExercisePlanDeviation\" = kryo,\n\"io.muvr.exercise.ResistanceExercise\" = kryo,\n}\nserializers {\njava = \"akka.serialization.JavaSerializer\"\nkryo = \"com.twitter.chill.akka.AkkaSerializer\"\n}\n}\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-26-638.jpg?cb=1443131432\" title=\"class UserActorView(userId: String) extends PersistentView ...\" target=\"_blank\">\n        26.\n      </a>\n    class UserActorView(userId: String) extends PersistentView {\noverride def persistenceId: String = UserPersistenceId(userId).persistenceId\noverride def viewId: String = UserPersistenceId(userId).persistentViewId\noverride def autoUpdateInterval: FiniteDuration = FiniteDuration(100, TimeUnit.MILLISECONDS)\ndef receive: Receive = viewState(List.empty)\ndef viewState(processedDeviations: List[ExercisePlanProcessedDeviation]): Receive = {\ncase EntireResistanceExerciseSession(_, _, _, _, deviations) if isPersistent =&gt;\ncontext.become(viewState(deviations.filter(condition).map(process) ::: processedDeviations))\ncase GetProcessedDeviations =&gt; sender() ! processedDeviations\n}\n}\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-27-638.jpg?cb=1443131432\" title=\"● Akka 2.4&#10;● Potentially infinite stream of data&#10;● Ordered,...\" target=\"_blank\">\n        27.\n      </a>\n    ● Akka 2.4\n● Potentially infinite stream of data\n● Ordered, replayable, resumable\n● Aggregation, transformation, moving data\n● EventsByPersistenceId\n● AllPersistenceids\n● EventsByTag\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-28-638.jpg?cb=1443131432\" title=\"val readJournal =&#10;PersistenceQuery(system).readJournalFor(C...\" target=\"_blank\">\n        28.\n      </a>\n    val readJournal =\nPersistenceQuery(system).readJournalFor(CassandraJournal.Identifier)\nval source = readJournal.query(\nEventsByPersistenceId(UserPersistenceId(name).persistenceId, 0, Long.MaxValue), NoRefresh)\n.map(_.event)\n.collect{ case s: EntireResistanceExerciseSession =&gt; s }\n.mapConcat(_.deviations)\n.filter(condition)\n.map(process)\nimplicit val mat = ActorMaterializer()\nval result = source.runFold(List.empty[ExercisePlanDeviation])((x, y) =&gt; y :: x)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-29-638.jpg?cb=1443131432\" title=\"● Potentially infinite stream of events&#10;Source[Any].map(pro...\" target=\"_blank\">\n        29.\n      </a>\n    ● Potentially infinite stream of events\nSource[Any].map(process).filter(condition)\nPublisher Subscriber\nprocess\ncondition\nbackpressure\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-30-638.jpg?cb=1443131432\" title=\"● In Akka we have the read and write sides separated,&#10;in Ca...\" target=\"_blank\">\n        30.\n      </a>\n    ● In Akka we have the read and write sides separated,\nin Cassandra we don’t\n● Different data model\n● Avoid using operational datastore\n● Eventual consistency\n● Streaming transformations to different format\n● Unify journalled and other data\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-31-638.jpg?cb=1443131432\" title=\"● Computations and analytics queries on the data&#10;● Often it...\" target=\"_blank\">\n        31.\n      </a>\n    ● Computations and analytics queries on the data\n● Often iterative, complex, expensive computations\n● Prepared and interactive queries\n● Data from multiple sources, joins and transformations\n● Often directly on a stream of data\n● Whole history of events\n● Historical behaviour\n● Works retrospectively, can answer questions in the future that we don’t\nknow exist yet\n● Various data types from various sources\n● Large amounts of fast data\n● Automated analytics\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-32-638.jpg?cb=1443131432\" title=\"● Cassandra 3.0 - user defined functions, functional indexe...\" target=\"_blank\">\n        32.\n      </a>\n    ● Cassandra 3.0 - user defined functions, functional indexes, aggregation\nfunctions, materialized views\n● Server side denormalization\n● Eventual consistency\n● Copy of data with different partitioning\nuserId\nperformance\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-33-638.jpg?cb=1443131432\" title=\"● In memory dataflow distributed data processing framework,...\" target=\"_blank\">\n        33.\n      </a>\n    ● In memory dataflow distributed data processing framework, streaming\nand batch\n● Distributes computation using a higher level API\n● Load balancing\n● Moves computation to data\n● Fault tolerant\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-34-638.jpg?cb=1443131432\" title=\"● Resilient Distributed Datasets&#10;● Fault tolerance&#10;● Cachin...\" target=\"_blank\">\n        34.\n      </a>\n    ● Resilient Distributed Datasets\n● Fault tolerance\n● Caching\n● Serialization\n● Transformations\n○ Lazy, form the DAG\n○ map, filter, flatMap, union, group, reduce, sort, join, repartition, cartesian, glom, ...\n● Actions\n○ Execute DAG, retrieve result\n○ reduce, collect, count, first, take, foreach, saveAs…, min, max, ...\n● Accumulators\n● Broadcast Variables\n● Integration\n● Streaming\n● Machine Learning\n● Graph Processing\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-35-638.jpg?cb=1443131432\" title=\"textFile mapmap&#10;reduceByKey&#10;collect&#10;sc.textFile(&quot;counts&quot;)&#10;....\" target=\"_blank\">\n        35.\n      </a>\n    textFile mapmap\nreduceByKey\ncollect\nsc.textFile(\"counts\")\n.map(line =&gt; line.split(\"t\"))\n.map(word =&gt; (word(0), word(1).toInt))\n.reduceByKey(_ + _)\n.collect()\n[4]\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-36-638.jpg?cb=1443131432\" title=\"Spark master&#10;Spark worker&#10;Cassandra&#10;\" target=\"_blank\">\n        36.\n      </a>\n    Spark master\nSpark worker\nCassandra\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-37-638.jpg?cb=1443131432\" title=\"● Cassandra can store&#10;● Spark can process&#10;● Gathering large...\" target=\"_blank\">\n        37.\n      </a>\n    ● Cassandra can store\n● Spark can process\n● Gathering large amounts of heterogeneous data\n● Queries\n● Transformations\n● Complex computations\n● Machine learning, data mining, analytics\n● Now possible\n● Prepared and interactive queries\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-38-638.jpg?cb=1443131432\" title=\"lazy val sparkConf: SparkConf =&#10;new SparkConf()&#10;.setAppName...\" target=\"_blank\">\n        38.\n      </a>\n    lazy val sparkConf: SparkConf =\nnew SparkConf()\n.setAppName(...).setMaster(...).set(\"spark.cassandra.connection.host\", \"127.0.0.1\")\nval sc = new SparkContext(sparkConf)\nval data = sc.cassandraTable[T](\"keyspace\", \"table\").select(\"columns\")\nval processedData = data.flatMap(...)...\nprocessedData.saveToCassandra(\"keyspace\", \"table\")\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-39-638.jpg?cb=1443131432\" title=\"● Akka Analytics project&#10;● Handles custom Akka serializatio...\" target=\"_blank\">\n        39.\n      </a>\n    ● Akka Analytics project\n● Handles custom Akka serialization\ncase class JournalKey(persistenceId: String, partition: Long, sequenceNr: Long)\nlazy val sparkConf: SparkConf =\nnew SparkConf()\n.setAppName(...).setMaster(...).set(\"spark.cassandra.connection.host\", \"127.0.0.1\")\nval sc = new SparkContext(sparkConf)\nval events: RDD[(JournalKey, Any)] = sc.eventTable()\nevents.sortByKey().map(...).filter(...).collect().foreach(println)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-40-638.jpg?cb=1443131432\" title=\"● Spark streaming&#10;● Precomputing using spark or replication...\" target=\"_blank\">\n        40.\n      </a>\n    ● Spark streaming\n● Precomputing using spark or replication often aiming for different data\nmodel\nOperational cluster Analytics cluster\nPrecomputation /\nreplication\nIntegration with\nother data sources\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-41-638.jpg?cb=1443131432\" title=\"val events: RDD[(JournalKey, Any)] = sc.eventTable().cache(...\" target=\"_blank\">\n        41.\n      </a>\n    val events: RDD[(JournalKey, Any)] = sc.eventTable().cache().filterClass[EntireResistanceExerciseSession].flatMap(_.deviations)\nval deviationsFrequency = sqlContext.sql(\n\"\"\"SELECT planned.exercise, hour(time), COUNT(1)\nFROM exerciseDeviations\nWHERE planned.exercise = 'bench press'\nGROUP BY planned.exercise, hour(time)\"\"\")\nval deviationsFrequency2 = exerciseDeviationsDF\n.where(exerciseDeviationsDF(\"planned.exercise\") === \"bench press\")\n.groupBy(\nexerciseDeviationsDF(\"planned.exercise\"),\nexerciseDeviationsDF(\"time”))\n.count()\nval deviationsFrequency3 = exerciseDeviations\n.filter(_.planned.exercise == \"bench press\")\n.groupBy(d =&gt; (d.planned.exercise, d.time.getHours))\n.map(d =&gt; (d._1, d._2.size))\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-42-638.jpg?cb=1443131432\" title=\"def toVector(user: User): mllib.linalg.Vector =&#10;Vectors.den...\" target=\"_blank\">\n        42.\n      </a>\n    def toVector(user: User): mllib.linalg.Vector =\nVectors.dense(\nuser.frequency, user.performanceIndex, user.improvementIndex)\nval events: RDD[(JournalKey, Any)] = sc.eventTable().cache()\nval users: RDD[User] = events.filterClass[User]\nval kmeans = new KMeans()\n.setK(5)\n.set...\nval clusters = kmeans.run(users.map(_.toVector))\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-43-638.jpg?cb=1443131432\" title=\"val weight: RDD[(JournalKey, Any)] = sc.eventTable().cache(...\" target=\"_blank\">\n        43.\n      </a>\n    val weight: RDD[(JournalKey, Any)] = sc.eventTable().cache()\nval exerciseDeviations = events\n.filterClass[EntireResistanceExerciseSession]\n.flatMap(session =&gt;\nsession.sets.flatMap(set =&gt;\nset.sets.map(exercise =&gt; (session.id.id, exercise.exercise))))\n.groupBy(e =&gt; e)\n.map(g =&gt;\nRating(normalize(g._1._1), normalize(g._1._2),\nnormalize(g._2.size)))\nval model = new ALS().run(ratings)\nval predictions = model.predict(recommend)\nbench\npress\nbicep\ncurl\ndead\nlift\nuser 1 5 2\nuser 2 4 3\nuser 3 5 2\nuser 4 3 1\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-44-638.jpg?cb=1443131432\" title=\"val events = sc.eventTable().cache().toDF()&#10;val lr = new Li...\" target=\"_blank\">\n        44.\n      </a>\n    val events = sc.eventTable().cache().toDF()\nval lr = new LinearRegression()\nval pipeline = new Pipeline().setStages(Array(new UserFilter(), new ZScoreNormalizer(),\nnew IntensityFeatureExtractor(), lr))\nval paramGrid = new ParamGridBuilder()\n.addGrid(lr.regParam, Array(0.1, 0.01))\n.addGrid(lr.fitIntercept, Array(true, false))\ngetEligibleUsers(events, sessionEndedBefore)\n.map { user =&gt;\nval trainValidationSplit = new TrainValidationSplit()\n.setEstimator(pipeline)\n.setEvaluator(new RegressionEvaluator)\n.setEstimatorParamMaps(paramGrid)\nval model = trainValidationSplit.fit(\nevents,\nParamMap(ParamPair(userIdParam, user)))\nval testData = // Prepare test data.\nval predictions = model.transform(testData)\nsubmitResult(userId, predictions, config)\n}\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-45-638.jpg?cb=1443131432\" title=\"val events: RDD[(JournalKey, Any)] = sc.eventTable().cache(...\" target=\"_blank\">\n        45.\n      </a>\n    val events: RDD[(JournalKey, Any)] = sc.eventTable().cache()\nval connections = events.filterClass[Connections]\nval vertices: RDD[(VertexId, Long)] =\nconnections.map(c =&gt; (c.id, 1l))\nval edges: RDD[Edge[Long]] = connections\n.flatMap(c =&gt; c.connections\n.map(Edge(c.id, _, 1l)))\nval graph = Graph(vertices, edges)\nval ranks = graph.pageRank(0.0001).vertices\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-46-638.jpg?cb=1443131432\" title=\"7 * Dumbbell&#10;Alternating Curl&#10;\" target=\"_blank\">\n        46.\n      </a>\n    7 * Dumbbell\nAlternating Curl\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-47-638.jpg?cb=1443131432\" title=\"Data&#10;Data&#10;Preprocessing&#10;Preprocessing&#10;Features&#10;Features&#10;Tra...\" target=\"_blank\">\n        47.\n      </a>\n    Data\nData\nPreprocessing\nPreprocessing\nFeatures\nFeatures\nTraining\nTesting\nError %\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-48-638.jpg?cb=1443131432\" title=\"● Exercise domain as an example&#10;● Analytics of both batch (...\" target=\"_blank\">\n        48.\n      </a>\n    ● Exercise domain as an example\n● Analytics of both batch (offline) and streaming (online) data\n● Analytics important in other areas (banking, stock market, network,\ncluster monitoring, business intelligence, commerce, internet of things, ...)\n● Enabling value of data\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-49-638.jpg?cb=1443131432\" title=\"● Event sourcing&#10;● CQRS&#10;● Technologies to handle the data&#10;○...\" target=\"_blank\">\n        49.\n      </a>\n    ● Event sourcing\n● CQRS\n● Technologies to handle the data\n○ Spark\n○ Mesos\n○ Akka\n○ Cassandra\n○ Kafka\n● Handling data\n● Insights and analytics enable value in data\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-50-638.jpg?cb=1443131432\" title=\"● Jobs at www.cakesolutions.net/careers&#10;● Code at https://g...\" target=\"_blank\">\n        50.\n      </a>\n    ● Jobs at www.cakesolutions.net/careers\n● Code at https://github.com/muvr\n● Martin Zapletal @zapletal_martin\n● Anirvan Chakraborty @anirvan_c\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891/95/cassandra-as-an-event-sourced-journal-for-big-data-analytics-cassandra-summit-2015-51-638.jpg?cb=1443131432\" title=\"[1] http://www.benstopford.com/2015/04/28/elements-of-scale...\" target=\"_blank\">\n        51.\n      </a>\n    [1] http://www.benstopford.com/2015/04/28/elements-of-scale-composing-and-scaling-data-platforms/\n[2] http://malteschwarzkopf.de/research/assets/google-stack.pdf\n[3] http://malteschwarzkopf.de/research/assets/facebook-stack.pdf\n[4] http://www.slideshare.net/LisaHua/spark-overview-37479609\n \n  </li>\n              </ol></div></div></div><aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\"><dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\"><ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"53172174\" title=\"Teaching Techniques: Creating Multimedia Learning\" href=\"https://www.linkedin.com/learning/teaching-techniques-creating-multimedia-learning?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Teaching Techniques: Creating Multimedia Learning\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Teaching Techniques: Creating Multimedia Learning\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=5yNrggG44Flut51QOcRX28HD9g0%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-iXSWq-dafY3HpecTcZLSioVsTfy8EkQ07eO6vRzjoG469LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Teaching Techniques: Creating Multimedia Learning</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"53172174\" title=\"Test Prep: PSAT\" href=\"https://www.linkedin.com/learning/test-prep-psat?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Test Prep: PSAT\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Test Prep: PSAT\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=Q%2BFidF7LbN0xBChPyihpXXlxDzw%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-iUyav_defY3_qfMDdZLSiol4TfSkFlgY3d-2hSTfoG469LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Test Prep: PSAT</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"53172174\" title=\"Training Tips Weekly\" href=\"https://www.linkedin.com/learning/training-tips-weekly?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Training Tips Weekly\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Training Tips Weekly\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=UoZplPy9FATFjkL1LbbbsAnvaBo%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-kUyKs_NGfZX_uf8HbZLSiol4Sey8Gkw02fO6gSTjmFI69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Training Tips Weekly</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"53479444\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"codecentric AG: CQRS and Event Sourcing Applications with Cassandra\" href=\"https://www.slideshare.net/planetcassandra/codecentric-ag-cqrs-and-event-sourcing-applications-with-cassandra\">\n    \n    <div class=\"related-content\"><p>codecentric AG: CQRS and Event Sourcing Applications with Cassandra</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"45107681\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Deep dive into event store using Apache Cassandra\" href=\"https://www.slideshare.net/AhmedabadJavaMeetup/deep-dive-into-event-store-using-apache-cassandra\">\n    \n    <div class=\"related-content\"><p>Deep dive into event store using Apache Cassandra</p><p>AhmedabadJavaMeetup</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64489074\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"CQRS and Event Sourcing with Akka, Cassandra and RabbitMQ\" href=\"https://www.slideshare.net/mieldonkers/cqrs-and-event-sourcing-with-akka-cassandra-and-rabbitmq\">\n    \n    <div class=\"related-content\"><p>CQRS and Event Sourcing with Akka, Cassandra and RabbitMQ</p><p>Miel Donkers</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"33145568\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Akka persistence == event sourcing in 30 minutes\" href=\"https://www.slideshare.net/ktoso/akka-persistence-event-sourcing-in-30-minutes\">\n    \n    <div class=\"related-content\"><p>Akka persistence == event sourcing in 30 minutes</p><p>Konrad Malawski</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"52194412\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Developing functional domain models with event sourcing (sbtb, sbtb2015)\" href=\"https://www.slideshare.net/chris.e.richardson/developing-functional-domain-models-with-event-sourcing-sbtb-sbtb2015\">\n    \n    <div class=\"related-content\"><p>Developing functional domain models with event sourcing (sbtb, sbtb2015)</p><p>Chris Richardson</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"52661852\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Data processing platforms architectures with Spark, Mesos, Akka, Cassandra and Kafka\" href=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\">\n    \n    <div class=\"related-content\"><p>Data processing platforms architectures with Spark, Mesos, Akka, Cassandra an...</p><p>Anton Kirillov</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"53517118\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Developing event-driven microservices with event sourcing and CQRS  (svcc, svcc2015)\" href=\"https://www.slideshare.net/chris.e.richardson/developing-eventdriven-microservices-with-event-sourcing-and-cqrs-svcc-svcc2015\">\n    \n    <div class=\"related-content\"><p>Developing event-driven microservices with event sourcing and CQRS  (svcc, sv...</p><p>Chris Richardson</p></div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div></div><footer>\n          <div class=\"row\"><div class=\"columns\"><ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div></div>\n          \n          <div class=\"row\"><div class=\"columns\"><p class=\"copyright text-center\">LinkedIn Corporation © 2018</p></div></div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div>\n    <div class=\"row\"><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></div>\n        \n    </div>\n    \n    \n      <noscript>\n    </noscript>",
        "created_at": "2018-01-16T16:48:09+0000",
        "updated_at": "2018-03-14T22:51:07+0000",
        "published_at": null,
        "published_by": [
          "Martin Zapletal"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 10,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/cassandraasaneventsourcedjournalforbigdataanalyticscassandrasummit2015-150924214814-lva1-app6891-thumbnail-4.jpg?cb=1443131432",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9081"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1039,
            "label": "redis",
            "slug": "redis"
          }
        ],
        "is_public": false,
        "id": 9075,
        "uid": null,
        "title": "How we moved our Food Feed from REDIS to Cassandra",
        "url": "https://www.zomato.com/blog/how-we-moved-our-food-feed-from-redis-to-cassandra",
        "content": null,
        "created_at": "2018-01-15T20:16:05+0000",
        "updated_at": "2018-03-14T22:54:03+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": null,
        "language": null,
        "reading_time": 0,
        "domain_name": "www.zomato.com",
        "preview_picture": null,
        "http_status": null,
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9075"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 182,
            "label": "mongo",
            "slug": "mongo"
          },
          {
            "id": 1039,
            "label": "redis",
            "slug": "redis"
          }
        ],
        "is_public": false,
        "id": 9072,
        "uid": null,
        "title": "citrusbyte/redis-comparison",
        "url": "https://github.com/citrusbyte/redis-comparison",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>I've been tasked with doing a presentation on Redis, and I wanted to include some benchmarks against other NoSQL databases such as MongoDB, Cassandra, CouchDB and Riak.</p>\n<p>The goal of this benchmarks was to make it as simple as possible in all aspects, with the following rationale:</p>\n<ul><li>download database source;</li>\n<li>compile;</li>\n<li>run with defaults.</li>\n</ul><p>I tried to install everything with the minimum dependencies and simplicity, without spending too much time trying to fix any possible issue I had. The idea is to having a server running in the least amount of time possible.</p>\n<p><strong>NOTE</strong>: this is a far from perfect benchmark, I know.</p>\n<h2><a href=\"#db-server\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-db-server\"></a>DB server</h2>\n<ul><li>EC2 Medium\n<ul><li>3.75 GiB memory</li>\n<li>2 EC2 Compute Unit (1 virtual core with 2 EC2 Compute Unit)</li>\n<li>410 GB instance storage</li>\n<li>32-bit or 64-bit platform</li>\n<li>I/O Performance: Moderate</li>\n<li>EBS-Optimized Available: No</li>\n<li>API name: m1.medium</li>\n</ul></li>\n<li>Ubuntu Server 12.04.1 LTS</li>\n</ul><h3><a href=\"#setup\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-setup\"></a>Setup</h3>\n<pre>sudo apt-get update\nsudo apt-get -y install build-essential tmux\nmkdir src\n</pre>\n<h4><a href=\"#redis\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-redis\"></a>Redis</h4>\n<pre>cd ~/src\nwget http://redis.googlecode.com/files/redis-2.6.10.tar.gz\ntar zxf redis-2.6.10.tar.gz\ncd redis-2.6.10\nmake\n</pre>\n<p>Note: less than a minute. No dependencies.</p>\n<h4><a href=\"#mongodb\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-mongodb\"></a>MongoDB</h4>\n<pre>cd ~/src\nwget http://downloads.mongodb.org/src/mongodb-src-r2.2.3.tar.gz\ntar zxf mongodb-src-r2.2.3.tar.gz\ncd mongodb-src-r2.2.3\nsudo apt-get -y install scons\n</pre>\n<p>Note: there's no indication in the README that I needed to install scons in order to compile MongoDB. More than 5 minutes compiling.</p>\n<h4><a href=\"#cassandra\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-cassandra\"></a>Cassandra</h4>\n<pre>cd ~/src\nwget http://apache.dattatec.com/cassandra/1.2.2/apache-cassandra-1.2.2-src.tar.gz\ntar zxf apache-cassandra-1.2.2-src.tar.gz\nsudo apt-get -Y install openjdk-6-jre ant\nant\n</pre>\n<p>Note: build failed. Binary core dumped.</p>\n<h4><a href=\"#couchdb\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-couchdb\"></a>CouchDB</h4>\n<pre>cd ~/src\nwget http://mirrors.dcarsat.com.ar/apache/couchdb/1.2.1/apache-couchdb-1.2.1.tar.gz\ntar zxf apache-couchdb-1.2.1.tar.gz\ncd  apache-couchdb-1.2.1\nsudo apt-get -y install erlang libicu-dev libmozjs-dev libcurl4-openssl-dev\n./configure\nmake\n</pre>\n<p>Note: binary core dumped.</p>\n<h2><a href=\"#client\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-client\"></a>Client</h2>\n<ul><li>EC2 Micro\n<ul><li>613 MiB memory</li>\n<li>Up to 2 EC2 Compute Units (for short periodic bursts)</li>\n<li>EBS storage only</li>\n<li>32-bit or 64-bit platform</li>\n<li>I/O Performance: Low</li>\n<li>EBS-Optimized Available: No</li>\n<li>API name: t1.micro</li>\n</ul></li>\n<li>Ubuntu Server 12.04.1 LTS</li>\n</ul><h3><a href=\"#setup-1\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-setup-1\"></a>Setup</h3>\n<pre>sudo apt-get -y install ruby-1.9.3\nsudo gem install redis cassandra couchdb mongodb riak\n</pre>\n<h3><a href=\"#script\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-script\"></a>Script</h3>\n<p>The benchmark ran it was very easy: iterate <code>1</code>, <code>10</code>, <code>100</code>, <code>1,000</code>, <code>10,000</code>, <code>100,000</code> and <code>1,000,000</code> over a simple model that has the following attributes:</p>\n<ul><li>id: variable in the format <code>player:$i</code>;</li>\n<li>name: variable in the format <code>Player $i</code>;</li>\n<li>type: constant string <code>Developer</code>;</li>\n<li>age: constant number <code>34</code>;</li>\n<li>level: initially a constant <code>0</code>.</li>\n</ul><p>The benchmarks performs three operations in each instance of the model:</p>\n<ul><li>storing;</li>\n<li>reading;</li>\n<li>updating <code>level</code> to the current iteration.</li>\n</ul><p>This script accepts the following two flags:</p>\n<ul><li><code>-h</code> indicate host to connect (default <code>127.0.0.1</code>)</li>\n<li><code>-n</code> indicate number of iterations (default <code>1,000</code>)</li>\n</ul><p>You can find the latest version of the script in the following repository: <a href=\"https://github.com/citrusbyte/redis-benchmarks\">https://github.com/citrusbyte/redis-benchmarks</a></p>\n<h3><a href=\"#benchmark\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-benchmark\"></a>Benchmark</h3>\n<p>I've ran the following command in my command line for benchmarking these database servers:</p>\n<pre>for i in 1 10 1000 10000 100000 1000000\ndo\n  ./benchs.rb -h 10.0.0.1 -n $i | tee $i.log\ndone\n</pre>\n<p>You can find the results logs in the following repository: <a href=\"https://github.com/citrusbyte/redis-benchmarks\">https://github.com/citrusbyte/redis-benchmarks</a></p>\n<p>Remember that, as I couldn't get Cassandra nor CouchDB working easily, those weren't benchmarked.</p>\n<h4><a href=\"#10-iterations\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-10-iterations\"></a><code>10</code> iterations</h4>\n<pre>Redis\n--------------------------------------------------------\n                 user     system      total        real\nwrite        0.000000   0.000000   0.000000 (  0.005587)\nread         0.000000   0.000000   0.000000 (  0.005741)\nupdate       0.010000   0.000000   0.010000 (  0.005117)\nTotal benchmark time for Redis: 0.01978361s\nMongoDB\n--------------------------------------------------------\n                 user     system      total        real\nwrite        0.000000   0.000000   0.000000 (  0.007547)\nread         0.000000   0.000000   0.000000 (  0.007479)\nupdate       0.000000   0.000000   0.000000 (  0.014906)\nTotal benchmark time for MongoDB: 0.034245423s\nRiak\n--------------------------------------------------------\n                 user     system      total        real\nwrite        0.010000   0.000000   0.010000 (  0.058750)\nread         0.000000   0.000000   0.000000 (  0.033434)\nupdate       0.020000   0.000000   0.020000 (  0.087879)\nTotal benchmark time for Riak: 0.190552601s\n</pre>\n<h4><a href=\"#100-iterations\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-100-iterations\"></a><code>100</code> iterations</h4>\n<pre>Redis\n--------------------------------------------------------\n                 user     system      total        real\nwrite        0.010000   0.000000   0.010000 (  0.055583)\nread         0.020000   0.010000   0.030000 (  0.057843)\nupdate       0.010000   0.000000   0.010000 (  0.049560)\nTotal benchmark time for Redis: 0.166689463s\nMongoDB\n--------------------------------------------------------\n                 user     system      total        real\nwrite        0.030000   0.010000   0.040000 (  0.087663)\nread         0.020000   0.000000   0.020000 (  0.073081)\nupdate       0.040000   0.000000   0.040000 (  0.164124)\nTotal benchmark time for MongoDB: 0.329832415s\nRiak\n--------------------------------------------------------\n                 user     system      total        real\nwrite        0.140000   0.010000   0.150000 (  0.597286)\nread         0.120000   0.010000   0.130000 (  0.382307)\nupdate       0.290000   0.020000   0.310000 (  1.024487)\nTotal benchmark time for Riak: 2.019069396s\n</pre>\n<h4><a href=\"#1000-iterations\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-1000-iterations\"></a><code>1000</code> iterations</h4>\n<pre>Redis\n--------------------------------------------------------\n                 user     system      total        real\nwrite        0.230000   0.010000   0.240000 (  0.822115)\nread         0.210000   0.030000   0.240000 (  0.674979)\nupdate       0.140000   0.010000   0.150000 (  0.498526)\nTotal benchmark time for Redis: 2.000050905s\nMongoDB\n--------------------------------------------------------\n                 user     system      total        real\nwrite        0.240000   0.020000   0.260000 (  0.741164)\nread         0.210000   0.020000   0.230000 (  1.053450)\nupdate       0.430000   0.050000   0.480000 (  2.194942)\nTotal benchmark time for MongoDB: 3.994837796s\nRiak\n--------------------------------------------------------\n                 user     system      total        real\nwrite        1.390000   0.170000   1.560000 (  6.635914)\nread         1.350000   0.160000   1.510000 (  4.061969)\nupdate       3.030000   0.280000   3.310000 ( 11.235330)\nTotal benchmark time for Riak: 21.959080865s\n</pre>\n<h4><a href=\"#other-iterations\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-other-iterations\"></a>Other iterations</h4>\n<p>You can really appreciate that Redis performs faster, so there's no need to paste the other iterations numbers.</p>\n<p>All I can say is that when I ran 1,000,000 iterations, MongoDB filled the hard disk.</p>\n</article>",
        "created_at": "2018-01-15T19:51:17+0000",
        "updated_at": "2018-03-14T22:55:39+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 5,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/6864?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9072"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1039,
            "label": "redis",
            "slug": "redis"
          }
        ],
        "is_public": false,
        "id": 9071,
        "uid": null,
        "title": "Cassandra vs. Redis",
        "url": "https://www.slideshare.net/tim.lossen.de/cassandra-vs-redis",
        "content": "<div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div><div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><h4 class=\"modal-title\">Select another clipboard</h4><hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><h3>You just clipped your first slide!</h3>Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div><p><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></p></div>",
        "created_at": "2018-01-15T19:51:14+0000",
        "updated_at": "2018-03-14T22:55:58+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/cassandravsredis-100928060358-phpapp02-thumbnail-4.jpg?cb=1285721044",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9071"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 1040,
            "label": "csv",
            "slug": "csv"
          }
        ],
        "is_public": false,
        "id": 9070,
        "uid": null,
        "title": "Read CSV File in Spark and Write it to Cassandra",
        "url": "https://stackoverflow.com/questions/29966951/read-csv-file-in-spark-and-write-it-to-cassandra",
        "content": "<div class=\"vote\"><h3>Vote count:&#13;\n        &#13;\n        1&#13;\n        &#13;\n&#13;\n        &#13;\n        &#13;\n&#13;\n&#13;\n</h3></div><div class=\"post-text\" itemprop=\"text\">&#13;\n&#13;\n<p>I am trying to read a CVS File with Spark and then save it to Cassandra. Saving to Cassandra is working, when I'm using trivial values.</p>\n<p>I have a file with the following values:</p>\n<p><code>id,name,tag1|tag2|tag3</code></p>\n<p>I want to store it in a cassandra table: </p>\n<p><code>id bigint, name varchar, tags set</code></p>\n<p>I defined a case class for this:</p>\n<p><code>case class Item(id:Integer,name:String,tag:Set[String])</code></p>\n<p>Then I use this expression for getting the RDD out of the CVS file</p>\n<p><code>val items = sc.textFile(\"items.csv\").map(l =&gt; l.split(\",\") match {case Array (a,b,c) =&gt; Item(Integer.parseInt(a),b,c.split(\"\\\\|\").toSet)})</code></p>\n<p>When I now call <code>collect</code> or <code>saveToCassandra</code> on items (which starts the processing) I get the following error:</p>\n<p><code>org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 29.0 failed 1 times, most recent failure: Lost task 1.0 in stage 29.0 (TID 38, localhost): scala.MatchError: [Ljava.lang.String;@6030bbe6 (of class [Ljava.lang.String;)\n    at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$2.apply(&lt;console&gt;:33)\n    at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$2.apply(&lt;console&gt;:33)\n    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n    at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:249)\n    at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:172)\n    at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:79)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n    at org.apache.spark.scheduler.Task.run(Task.scala:64)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n</code></p></div><div class=\"user-action-time\">&#13;\n        <a href=\"https://stackoverflow.com/posts/29966951/revisions\" title=\"show all edits to this post\">edited Apr 30 '15 at 12:15</a>&#13;</div><div class=\"user-details\">&#13;\n        &#13;\n        &#13;</div><div class=\"user-action-time\">&#13;\n        asked Apr 30 '15 at 11:48&#13;</div><div class=\"user-details\">&#13;\n        <a href=\"https://stackoverflow.com/users/4203061/mniehoff\">mniehoff</a>&#13;\n        <div class=\"-flair\">&#13;\n            2191213&#13;\n        </div>&#13;</div><h2 data-answercount=\"2\">&#13;\n                                2 Answers&#13;\n                                2&#13;</h2><div class=\"vote\"><h3>Vote count:&#13;\n        &#13;\n        2&#13;\n        &#13;\n&#13;\n&#13;\n&#13;\n        accepted&#13;\n&#13;\n</h3></div><div class=\"post-text\" itemprop=\"text\">&#13;\n<p>As mentioned, the issue is that splitting on some inputs is generating an array that has less or more than the 3 elements used in the match.  </p>\n<p>But the <code>partialFuntion</code> used to do the match can be used to filter on the elements that <em>do</em> fit the match criteria. <code>rdd.collect{partialFunction}</code> is exactly meant for that:</p>\n<pre>val data = sc.textFile(\"items.csv\")\nval arrayData = data.map(l =&gt; l.split(\",\"))\nval items = arrayData.collect{case Array (a,b,c) =&gt; Item(Integer.parseInt(a),b,c.split(\"\\\\|\").toSet)})\n items.saveToCassandra(...)\n</pre>\n<ul><li>Note1: you should also protect against dirty values. e.g. parseInt on a value that's not an int number,...) </li>\n<li>Note2: <code>rdd.collect{partialFunc}</code> (filters/map data using a partial function) should not be confused with <code>rdd.collect</code> (get back data to the driver))</li>\n</ul></div><div class=\"user-action-time\">&#13;\n        answered Apr 30 '15 at 14:57&#13;</div><div class=\"user-details\">&#13;\n        <a href=\"https://stackoverflow.com/users/764040/maasg\">maasg</a>&#13;\n        <div class=\"-flair\">&#13;\n            27.4k76093&#13;\n        </div>&#13;</div><div class=\"vote\"><h3>Vote count:&#13;\n        &#13;\n        1&#13;\n        &#13;\n&#13;\n&#13;\n&#13;\n&#13;\n</h3></div><div class=\"post-text\" itemprop=\"text\">&#13;\n<p>You'll get that match error if your input <em>isn't</em> an array of 3 entries e.g.</p>\n<pre>String(\"a,b\").split(\",\") match {\n   case Array(a,b,c) =&gt; ....\n}\n</pre>\n<p>so I suspect this is some input data issue, and you need to cater for it in your <code>match</code>.</p></div><div class=\"user-action-time\">&#13;\n        answered Apr 30 '15 at 11:55&#13;</div><div class=\"user-details\">&#13;\n        <a href=\"https://stackoverflow.com/users/12960/brian-agnew\">Brian Agnew</a>&#13;\n        <div class=\"-flair\">&#13;\n            211k25262371&#13;\n        </div>&#13;</div>",
        "created_at": "2018-01-15T19:27:42+0000",
        "updated_at": "2018-03-15T14:18:52+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 3,
        "domain_name": "stackoverflow.com",
        "preview_picture": "https://cdn.sstatic.net/Sites/stackoverflow/img/apple-touch-icon@2.png?v=73d79a89bded",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9070"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 1040,
            "label": "csv",
            "slug": "csv"
          }
        ],
        "is_public": false,
        "id": 9069,
        "uid": null,
        "title": "markthebault/importCSVSparkCassandra",
        "url": "https://github.com/markthebault/importCSVSparkCassandra",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>This short example will show you how easily is to import CSV files from your AWS S3 buckets\nusing spark into cassandra.</p>\n<h2><a href=\"#setting-up-your-applicaiton\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-setting-up-your-applicaiton\"></a>Setting up your applicaiton</h2>\n<p>Clone this repository <code>git clone http://gitlab.ippon.fr/mthebault/simplecsvexportspark.git</code>\nOpen the file 'src/main/ressources/project.conf' and change your settings.</p>\n<p><strong>You need to change the following values:</strong></p>\n<ul><li>Cassandra\n<ul><li>host</li>\n<li>port</li>\n<li>keyspace</li>\n<li>table</li>\n</ul></li>\n<li>AWS\n<ul><li>accessKey</li>\n<li>secretKey</li>\n<li>bucket</li>\n<li>fileName</li>\n</ul></li>\n</ul><h2><a href=\"#build-a-jar\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-build-a-jar\"></a>Build a jar</h2>\n<p>To build the Jar of your application you just need to run the command <code>sbt clean assembly</code></p>\n<h2><a href=\"#deploy-the-jar-on-a-spark-cluster\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-deploy-the-jar-on-a-spark-cluster\"></a>Deploy the Jar on a spark cluster</h2>\n<p>To deploy a jar on a spark cluster you have to make sure you have the port 7077 accessible from the outside.\nYou have to push this Jar to a S3 public bucket <code>aws s3 cp ./target/scala-2.10/ImportCSV.jar s3://YOUR_BUCKET/ImportCVS.jar</code></p>\n<p><em><strong>Once you have done that, you just need to run the spark-submit command as following:</strong></em></p>\n<pre>$SPARK_HOME/bin/spark-submit \\\n\t--verbose \\\n\t--master spark://IP_SPARK_MASTER:PORT \\\n\t--deploy-mode cluster \\\n\t--driver-class-path /spark/spark-1.6.1-bin-hadoop2.6/lib/spark-assembly-1.6.1-hadoop2.6.0.jar \\\n\t--class Application \\\n\thttps://s3-eu-west-1.amazonaws.com/YOUR_BUCKET/ImportCSV.jar\n</pre>\n<p><em><strong>Note:</strong></em>\nHere I am using a public s3 bucket for the jars. If you want to use your private buckets you can use the following link:\n<code>http://AWS_S3_ACCESS_KEY:AWS_S3_SECRET_KEY@YOUR_BUCKET/ImportCSV.jar</code>\nplease consider of the http link have to be encoded <a href=\"http://www.freeformatter.com/url-encoder.html\" rel=\"nofollow\">you can use this website to encode the link</a></p>\n<h2><a href=\"#next\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-next\"></a>Next</h2>\n<p>If you want to contribute to this project feel free to do it, if you see some mistake please leave me an issue.</p>\n</article>",
        "created_at": "2018-01-15T19:26:02+0000",
        "updated_at": "2018-03-15T14:19:04+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/3846664?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9069"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          },
          {
            "id": 1040,
            "label": "csv",
            "slug": "csv"
          }
        ],
        "is_public": false,
        "id": 9068,
        "uid": null,
        "title": "From Cassandra to S3, with Spark",
        "url": "https://objectpartners.com/2016/11/30/from-cassandra-to-s3-with-spark/",
        "content": "<p>Apache Cassandra, a scalable and high-availability platform, is a good choice for high volume event management applications, such as large deployments of sensors. Applications include telematics data for large fleets, smart meter telemetry in electric, gas or water utility systems, and wide area weather station reporting. By analyzing this raw event data, system level intelligence can be extracted to discover trends and clustering along dimensions such as space, time and environmental parameters. Apache Spark enables this analysis, connecting directly to Cassandra and performing fault-tolerant processing with an architecture that scales out with Cassandra clustering.</p><p>This focus of this blog is showing how to connect Spark to Cassandra, analyze event data from Cassandra, and store the results of the analysis into S3, making it available for reporting or further analysis. The example uses 911 call event data collected over a number of weeks. For an example of Spark processing this same data from a Kafka based event stream, see <a href=\"https://objectpartners.com/2016/10/13/analyzing-kafka-data-streams-with-spark/\">this earlier blog</a>, based on the same 911 call event data set.</p><p><strong>Demonstration components</strong><br />To provide a demonstration that can be run locally, this blog describes a runnable Spark application <a href=\"https://github.com/waldmark/spark-cassandra-batch-s3-examples\">available from Github</a>,  along with Docker images that provide a stand alone (single node) Cassandra cluster and a local S3 object store. The Docker images are described at the end of this blog.</p><p><strong>Initializing Cassandra with demo data</strong><br />To provide a repeatable demo with minimal pre-conditions, the Cassandra schema is created and populated with demo data on application startup.</p><div><pre>    private void createSchema() {&#13;\n        try {&#13;\n            session.execute(dropKeyspaceCommand);&#13;\n        } catch (Exception e) {&#13;\n            System.out.println(e.getMessage());&#13;\n            System.exit(-1);&#13;\n        }&#13;\n&#13;\n        session.execute(createKeyspaceCommand);&#13;\n        session.execute(createRT911TableCommand);&#13;\n    }&#13;\n</pre></div><p> <br />The createSchema will drop the demo Cassandra keyspace if it exists, and then creates the keyspace and a table in the keyspace. These commands are CQL statements (Cassandra’s form of SQL) loaded into the application from an application.yml file:</p><div><pre>---&#13;\n# cassandra&#13;\ncassandra:&#13;\n  defaultQueryConsistency: ONE&#13;\n  defaultUpdateConsistency: ONE&#13;\n  updateTimeoutMillis: 5000&#13;\n  compression: LZ4&#13;\n  nodeAddress: localhost&#13;\n  host: 127.0.0.1&#13;\n&#13;\n  keyspaces:&#13;\n    - name: rt911&#13;\n      createCommand: \"CREATE KEYSPACE testkeyspace WITH replication = {'class':'SimpleStrategy', 'replication_factor':1};\"&#13;\n      dropCommand: \"DROP KEYSPACE IF EXISTS testkeyspace;\"&#13;\n      truncateCommand: \"TRUNCATE testkeyspace.rt911\"&#13;\n      tables:&#13;\n        - name: calls&#13;\n          createCommand:&#13;\n              CREATE TABLE testkeyspace.rt911 (&#13;\n                address varchar,&#13;\n                calltype varchar,&#13;\n                calltime varchar,&#13;\n                latitude varchar,&#13;\n                longitude varchar,&#13;\n                location varchar,&#13;\n                id varchar PRIMARY KEY);&#13;\n          insertPreparedStatementCommand:&#13;\n              INSERT INTO testkeyspace.rt911 (address, calltype, calltime, latitude, longitude, location, id)&#13;\n                VALUES ( ?, ?, ?, ?, ?, ?, ? );&#13;\n</pre></div><p>Once the schema and table are created, the application loads the 911 call data from a gzip csv file, populating Cassandra with the 911 call data. The application uses the Cassandra session to execute this CQL prepared statement:</p><div><pre>            insertPreparedStatementCommand:&#13;\n              INSERT INTO testkeyspace.rt911 (address, calltype, calltime, latitude, longitude, location, id)&#13;\n                VALUES ( ?, ?, ?, ?, ?, ?, ? );&#13;\n</pre></div><p> <br />When finished, the table ‘rt911’ in keyspace ‘testkeyspace’ contains call event data.</p><p><strong>Connecting Spark with Cassandra</strong><br />DataStax provides a ready to use <a href=\"https://github.com/datastax/spark-cassandra-connector\">Spark to Cassandra connector</a>. This connector exposes Cassandra data in terms of Spark structures, such as RDDs, supports writing Spark data to Cassandra, and allows CQL queries to be made from a Spark application. A quick start guide for the connector is also <a href=\"https://github.com/datastax/spark-cassandra-connector/blob/master/doc/0_quick_start.md\">available</a>. The Spark Cassandra Connector’s Java API utility class static method javaFunctions is used to read data from Cassandra. The data is read and transformed into Java objects that represent 911 calls, all in a single statemement:</p><div><pre>        JavaRDD callData = javaFunctions(sc)&#13;\n                .cassandraTable(\"testkeyspace\", \"rt911\")&#13;\n                .map(new Map911Call());&#13;\n</pre></div><p><strong>Analyzing the event data with Spark</strong><br />The 911 call data is filtered by event type based on simple classification using a contains match on the event type (in this case, event types that contain ‘Fire’).</p><div><pre>callData = callData.filter( c -&gt; (c.getCallType().matches(\"(?i:.*\\\\bFire\\\\b.*)\")));&#13;\n</pre></div><p> <br />The data is then mapped to key/value pairs, keyed by week of year:<br /></p><div><pre>        MapByCallDate mapByCallDate = new MapByCallDate();&#13;\n        return callData.mapToPair(mapByCallDate);&#13;\n</pre></div><p> <br />which uses this class to do the mapping:<br /></p><div><pre>public class MapByCallDate implements PairFunction {&#13;\n&#13;\n    @Override&#13;\n    public Tuple2 call(RealTime911 realTime911) throws Exception {&#13;\n        // create time bucket to group by dates (no time) - use MM/dd/yyyy&#13;\n        String timeBucket = realTime911.getDateTime().substring(0,10);&#13;\n        return new Tuple2(timeBucket, realTime911);&#13;\n    }&#13;\n}&#13;\n</pre></div><p> <br />The event object data is then grouped by date (MM/dd/yyyy –  e.g. 08/03/2015):<br /></p><div><pre>        JavaPairRDD&lt;String, Iterable&gt; groupedCalls = callsByCallDate.groupByKey();&#13;\n</pre></div><p> <br />The pair data is then transformed from Java objects into JSON documents, one for each date group, and written to a Map with the date as key and the JSON as the value:<br /></p><div><pre>        Map&lt;String, Iterable&gt; groupedCallMap = groupedCalls.collectAsMap();&#13;\n        Set keys = groupedCallMap.keySet();&#13;\n&#13;\n        ObjectMapper mapper = new ObjectMapper();&#13;\n&#13;\n        Map s3BucketData = new HashMap();&#13;\n        for(String key: keys) {&#13;\n            List jsonArrayElements = new ArrayList();&#13;\n            Iterable iterable = groupedCallMap.get(key);&#13;\n            Iterator iterator = iterable.iterator();&#13;\n            while(iterator.hasNext()) {&#13;\n                RealTime911 rt911 = iterator.next();&#13;\n                LOG.debug(rt911.getDateTime() + \" \" + rt911.getCallType());&#13;\n                try {&#13;\n                    String jsonRT911 = mapper.writeValueAsString(rt911);&#13;\n                    jsonArrayElements.add(jsonRT911);&#13;\n                } catch (JsonProcessingException e) {&#13;\n                    LOG.error(e.getMessage());&#13;\n                }&#13;\n            }&#13;\n&#13;\n            StringJoiner joiner = new StringJoiner(\",\");&#13;\n            jsonArrayElements.forEach(joiner::add);&#13;\n            s3BucketData.put(key, \"[\" + joiner.toString() + \"]\");&#13;\n        }&#13;\n</pre></div><p> <br />To store the data into S3, the AWS Java SDK is used to create an S3 client to the Scality S3 server.</p><div><pre>@Component&#13;\npublic class S3Client {&#13;\n&#13;\n    private AmazonS3Client s3 = null;&#13;\n&#13;\n    public S3Client() {&#13;\n        s3 = getClient();&#13;\n    }&#13;\n&#13;\n    private AmazonS3Client getClient() {&#13;\n        if(null == s3) {&#13;\n            System.setProperty(SDKGlobalConfiguration.DISABLE_CERT_CHECKING_SYSTEM_PROPERTY, \"true\");&#13;\n            BasicAWSCredentials credentials = new BasicAWSCredentials(\"accessKey1\", \"verySecretKey1\");&#13;\n            s3 = new AmazonS3Client(credentials);&#13;\n            S3ClientOptions options = S3ClientOptions.builder().setPathStyleAccess(true).build();&#13;\n            s3.setS3ClientOptions(options);&#13;\n            s3.setEndpoint(\"http://127.0.0.1:8000/\");&#13;\n        }&#13;\n        return s3;&#13;\n    }&#13;\n&#13;\n... the complete class can viewed in the GitHub project.&#13;\n }&#13;\n</pre></div><p> <br />Note that this client is written specifically for the Scality S3 server, which uses “accessKey1” as the access key and “verySecretKey1” as the secret key. These are the default AWSCredentials for a Scality S3 server.</p><p>This code saves the JSON documents to S3. First it removes the bucket if it already exists, then it creates the S3 bucket and writes key/value pairs to the bucket, storing a JSON document per date for which Cassandra has data. The bucket removal is to provide a repeatable demo, one that does not depend on, or conflict with previous runs.</p><div><pre>         try {&#13;\n            // remove the S3 bucket, this removes all objects in the bucket first&#13;\n            s3Client.removeBucket(bucketName);&#13;\n            LOG.info(\"S3 bucket \" + bucketName + \" deleted\");&#13;\n        } catch (Exception e) {&#13;\n            // bucket not deleted, may not have been there&#13;\n        }&#13;\n&#13;\n        try {&#13;\n            // create the bucket to start fresh&#13;\n            s3Client.createBucket(bucketName);&#13;\n            LOG.info(\"S3 bucket \" + bucketName + \" created\");&#13;\n&#13;\n            Set bucketKeys = s3BucketData.keySet();&#13;\n            // save to S3&#13;\n            for(String key: bucketKeys) {&#13;\n                s3Client.storeString(bucketName, key, s3BucketData.get(key));&#13;\n            }&#13;\n            LOG.info(\"finished saving JSON to S3 completed\");&#13;\n&#13;\n            LOG.info(\"displaying all JSON objects and their keys saved to \" + bucketName + \"\\n\");&#13;\n            for(String key: bucketKeys) {&#13;\n                String storedObject = s3Client.readS3Object(bucketName, key);&#13;\n                LOG.info(\"key: \" + key + \" value: \" + storedObject);&#13;\n            }&#13;\n        } catch (Exception e) {&#13;\n            LOG.error(e.getMessage());&#13;\n        } finally {&#13;\n            // clean up&#13;\n            s3Client.removeBucket(bucketName);&#13;\n        }&#13;\n</pre></div><p> <br />The end to end process of extraction, computing new information from this data, and storage into S3 for reporting, business intelligence analysis and other repurposing is a pattern applicable to a wide variety of needs.</p><p><strong>Installing Scality S3 server Docker image</strong><br />The S3 object store is based on the Scality S3 Docker image, which provides an S3 instance without requiring Amazon Web Service (AWS) credentials, and is a convenient way to develop against S3 without using AWS credits.</p><p>Installing Scality’s S3 server as a Docker image and running are described at <a href=\"https://hub.docker.com/r/scality/s3server\">https://hub.docker.com/r/scality/s3server</a></p><p><strong>Installing Cassandra Docker image</strong><br />The instructions on installing a docker image of Cassandra are described at <a href=\"https://hub.docker.com/_/cassandra/\">https://hub.docker.com/_/cassandra/</a></p><p>The instructions include installing the server image, as well as instructions for starting a cluster locally, and running Cassandra’s Query Language Shell (e.g. cqlsh) as a client to the Cassandra server.</p>",
        "created_at": "2018-01-15T19:25:22+0000",
        "updated_at": "2018-12-18T21:19:42+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 6,
        "domain_name": "objectpartners.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9068"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 907,
            "label": "mesos",
            "slug": "mesos"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 963,
            "label": "akka",
            "slug": "akka"
          },
          {
            "id": 1041,
            "label": "smack",
            "slug": "smack"
          }
        ],
        "is_public": false,
        "id": 9067,
        "uid": null,
        "title": "Alluxio Mesos Meetup - SMACK to SMAACK",
        "url": "https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack",
        "content": "Alluxio Mesos Meetup - SMACK to SMAACK\n      \n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            </a><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div>\n    <div class=\"wrapper\"><p>Successfully reported this slideshow.</p><div id=\"slideview-container\" class=\"\"><div class=\"row\"><div id=\"main-panel\" class=\"small-12 large-8 columns\"><div class=\"sectionElements\"><div class=\"playerWrapper\"><div><div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\">Alluxio Mesos Meetup - SMACK to SMAACK<div class=\"stage valign-first-slide\"><div class=\"slide_container\"><section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-1-638.jpg?cb=1503617560\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-1-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-1-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-1-1024.jpg?cb=1503617560\" alt=\"© 2016 Mesosphere, Inc. All Rights Reserved.&#10;From SMACK to&#10;SMAACK&#10;Alluxio meets DC/OS&#10;Jörg Schad, Mesosphere&#10;Adit Madan, A...\" /></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-2-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-2-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-2-1024.jpg?cb=1503617560\" alt=\"© 2017 Mesosphere, Inc. All Rights Reserved.&#10;20% OFF&#10;MCDCOS20&#10;September 13th - 15th&#10;● Dedicated Tracks&#10;● MesosCon Universi...\" /></i></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-3-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-3-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-3-1024.jpg?cb=1503617560\" alt=\"© 2017 Mesosphere, Inc. All Rights Reserved. 3&#10;Fast Data&#10;Batch Event ProcessingMicro-Batch&#10;Days Hours Minutes Seconds Micr...\" /></i></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-4-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-4-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-4-1024.jpg?cb=1503617560\" alt=\"© 2017 Mesosphere, Inc. All Rights Reserved. 4&#10;The SMACK Stack&#10;EVENTS&#10;Ubiquitous data streams&#10;from connected devices&#10;INGES...\" /></i></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-5-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-5-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-5-1024.jpg?cb=1503617560\" alt=\"© 2017 Mesosphere, Inc. All Rights Reserved. 5&#10;Datacenter&#10;\" /></i></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-6-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-6-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-6-1024.jpg?cb=1503617560\" alt=\"© 2017 Mesosphere, Inc. All Rights Reserved. 6&#10;NAIVE APPROACH&#10;Typical Datacenter&#10;siloed, over-provisioned servers,&#10;low uti...\" /></i></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-7-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-7-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-7-1024.jpg?cb=1503617560\" alt=\"© 2017 Mesosphere, Inc. All Rights Reserved. 7&#10;\" /></i></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-8-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-8-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-8-1024.jpg?cb=1503617560\" alt=\"© 2017 Mesosphere, Inc. All Rights Reserved. 8&#10;MULTIPLEXING OF DATA, SERVICES, USERS, ENVIRONMENTS&#10;Typical Datacenter&#10;silo...\" /></i></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-9-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-9-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-9-1024.jpg?cb=1503617560\" alt=\"Datacenter Operating System (DC/OS)&#10;Distributed Systems Kernel (Mesos)&#10;DC/OS ENABLES MODERN DISTRIBUTED APPS&#10;Big Data + An...\" /></i></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-10-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-10-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-10-1024.jpg?cb=1503617560\" alt=\"© 2017 Mesosphere, Inc. All Rights Reserved. 10&#10;The SMACK Stack&#10;EVENTS&#10;Ubiquitous data streams&#10;from connected devices&#10;INGE...\" /></i></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-11-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-11-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-11-1024.jpg?cb=1503617560\" alt=\"© 2017 Mesosphere, Inc. All Rights Reserved. 11&#10;The SMACK Stack&#10;EVENTS&#10;Ubiquitous data streams&#10;from connected devices&#10;INGE...\" /></i></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-12-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-12-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-12-1024.jpg?cb=1503617560\" alt=\"© 2016 Mesosphere, Inc. All Rights Reserved.&#10;BIG DATA ECOSYSTEM YESTERDAY&#10;© 2017 Alluxio 12&#10;\" /></i></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-13-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-13-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-13-1024.jpg?cb=1503617560\" alt=\"© 2016 Mesosphere, Inc. All Rights Reserved.&#10;BIG DATA ECOSYSTEM TODAY&#10;© 2017 Alluxio&#10;…&#10;…&#10;13&#10;\" /></i></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-14-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-14-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-14-1024.jpg?cb=1503617560\" alt=\"© 2016 Mesosphere, Inc. All Rights Reserved.&#10;BIG DATA ECOSYSTEM ISSUES&#10;© 2017 Alluxio&#10;…&#10;…&#10;14&#10;\" /></i></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-15-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-15-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-15-1024.jpg?cb=1503617560\" alt=\"© 2017 Mesosphere, Inc. All Rights Reserved. 15&#10;The SMAACK Stack&#10;EVENTS&#10;Ubiquitous data streams&#10;from connected devices&#10;ING...\" /></i></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-16-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-16-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-16-1024.jpg?cb=1503617560\" alt=\"© 2017 Mesosphere, Inc. All Rights Reserved. 16© 2017 Alluxio&#10;\" /></i></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-17-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-17-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-17-1024.jpg?cb=1503617560\" alt=\"© 2016 Mesosphere, Inc. All Rights Reserved.&#10;BIG DATA ECOSYSTEM WITH ALLUXIO&#10;…&#10;…&#10;FUSE Compatible File&#10;System Interface&#10;Had...\" /></i></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-18-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-18-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-18-1024.jpg?cb=1503617560\" alt=\"© 2016 Mesosphere, Inc. All Rights Reserved.&#10;BIG DATA ECOSYSTEM WITH ALLUXIO&#10;…&#10;…&#10;FUSE Compatible File&#10;System Interface&#10;Had...\" /></i></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-19-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-19-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-19-1024.jpg?cb=1503617560\" alt=\"© 2016 Mesosphere, Inc. All Rights Reserved.&#10;WHY ALLUXIO&#10;© 2017 Alluxio&#10;Co-located compute and data with memory-speed acce...\" /></i></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-20-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-20-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-20-1024.jpg?cb=1503617560\" alt=\"© 2016 Mesosphere, Inc. All Rights Reserved.&#10;ALLUXIO BENEFITS&#10;© 2017 Alluxio&#10;Unification&#10;New workflows across&#10;any data in ...\" /></i></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-21-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-21-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-21-1024.jpg?cb=1503617560\" alt=\"© 2017 Mesosphere, Inc. All Rights Reserved. 21© 2017 Alluxio&#10;\" /></i></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-22-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-22-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-22-1024.jpg?cb=1503617560\" alt=\"© 2016 Mesosphere, Inc. All Rights Reserved. 22&#10;WHY DATA SERVICES ON DC/OS?&#10;On-demand provisioning1&#10;2&#10;3&#10;Simplified operati...\" /></i></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-23-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-23-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-23-1024.jpg?cb=1503617560\" alt=\"© 2016 Mesosphere, Inc. All Rights Reserved. 23&#10;ALLUXIO ON MESOSPHERE DC/OS&#10;Fast, On-demand Unified Data at Memory Speed f...\" /></i></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-24-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-24-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-24-1024.jpg?cb=1503617560\" alt=\"© 2016 Mesosphere, Inc. All Rights Reserved. 24&#10;ALLUXIO ON MESOSPHERE DC/OS&#10;Fast, On-demand Unified Data at Memory Speed f...\" /></i></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-25-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-25-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-25-1024.jpg?cb=1503617560\" alt=\"© 2016 Mesosphere, Inc. All Rights Reserved.&#10;WHY ALLUXIO ON MESOSPHERE DC/OS?&#10;● Without Mesosphere DC/OS, provisioning of ...\" /></i></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-26-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-26-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-26-1024.jpg?cb=1503617560\" alt=\"© 2016 Mesosphere, Inc. All Rights Reserved. 26&#10;BIG DATA STACK WITH ALLUXIO ON MESOSPHERE DC/OS&#10;Fast, On-demand Unified Da...\" /></i></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-27-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-27-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-27-1024.jpg?cb=1503617560\" alt=\"© 2017 Mesosphere, Inc. All Rights Reserved. 27© 2017 Alluxio&#10;DEMO&#10;\" /></i></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-28-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-28-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-28-1024.jpg?cb=1503617560\" alt=\"© 2016 Mesosphere, Inc. All Rights Reserved.&#10;WHAT HAPPENED?&#10;● Alluxio scheduler (developed using the DC/OS SDK) launched a...\" /></i></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Alluxio/alluxio-mesos-meetup-smack-to-smaack\" data-small=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/85/alluxio-mesos-meetup-smack-to-smaack-29-320.jpg?cb=1503617560\" data-normal=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-29-638.jpg?cb=1503617560\" data-full=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-29-1024.jpg?cb=1503617560\" alt=\"© 2016 Mesosphere, Inc. All Rights Reserved.&#10;GET STARTED TODAY&#10;Read:&#10;● Mesosphere Blog: http://ow.ly/ou0530ax9aM&#10;● Alluxio...\" /></i></section><div class=\"j-next-container next-container\"><div class=\"content-container\"><div class=\"next-slideshow-wrapper\"><div class=\"j-next-slideshow next-slideshow\"><p>Upcoming SlideShare</p></div><p>Loading in …5</p><p>×</p></div></div></div></div></div></div></div></div></div><div class=\"slideshow-info-container\" itemscope=\"itemscope\" itemtype=\"https://schema.org/MediaObject\"><div class=\"slideshow-tabs-container show-for-medium-up\"><ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    0 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\">\n                \n                  1 Like\n                \n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\">\n                Statistics\n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\">\n                Notes\n              </i></a>\n            </li>\n          </ul><div class=\"tabs-content\"><div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\"><ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"itemscope\"><li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"StreamingAnalytics\" rel=\"nofollow\" href=\"https://www.slideshare.net/StreamingAnalytics?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Streaming Analytics\n                            \n                              \n                                , \n                                Technology Manager at Trivadis\n                              \n                              \n                                 at \n                                Trivadis\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n              </ul></div><div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No Downloads</p></div><div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No notes for slide</p></div></div></div><div class=\"notranslate transcript add-padding-right j-transcript\"><ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    © 2016 Mesosphere, Inc. All Rights Reserved.\nFrom SMACK to\nSMAACK\nAlluxio meets DC/OS\nJörg Schad, Mesosphere\nAdit Madan, Alluxio\n#smack @Alluxio @dcos @joerg_schad @madanadit\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-2-638.jpg?cb=1503617560\" title=\"© 2017 Mesosphere, Inc. All Rights Reserved.&#10;20% OFF&#10;MCDCOS...\" target=\"_blank\">\n        2.\n      </a>\n    © 2017 Mesosphere, Inc. All Rights Reserved.\n20% OFF\nMCDCOS20\nSeptember 13th - 15th\n● Dedicated Tracks\n● MesosCon University\n● Town Halls\n● Hackathon\nAccelerating Spark workloads in a Mesos\nenvironment with Alluxio, 09/15, 11AM\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-3-638.jpg?cb=1503617560\" title=\"© 2017 Mesosphere, Inc. All Rights Reserved. 3&#10;Fast Data&#10;Ba...\" target=\"_blank\">\n        3.\n      </a>\n    © 2017 Mesosphere, Inc. All Rights Reserved. 3\nFast Data\nBatch Event ProcessingMicro-Batch\nDays Hours Minutes Seconds Microseconds\nSolves problems using predictive and prescriptive analyticsReports what has happened using descriptive analytics\nPredictive User InterfaceReal-time Pricing and Routing Real-time AdvertisingBilling, Chargeback Product recommendations\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-4-638.jpg?cb=1503617560\" title=\"© 2017 Mesosphere, Inc. All Rights Reserved. 4&#10;The SMACK St...\" target=\"_blank\">\n        4.\n      </a>\n    © 2017 Mesosphere, Inc. All Rights Reserved. 4\nThe SMACK Stack\nEVENTS\nUbiquitous data streams\nfrom connected devices\nINGEST\nApache Kafka\nSTORE\nApache Spark\nANALYZE\nApache Cassandra\nACT\nAkka\nIngest millions of events\nper second\nDistributed &amp; highly\nscalable database\nReal-time and batch\nprocess data\nVisualize data and build\ndata driven applications\nMesos/ DC/OS\nSensors\nDevices\nClients\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-5-638.jpg?cb=1503617560\" title=\"© 2017 Mesosphere, Inc. All Rights Reserved. 5&#10;Datacenter&#10;\" target=\"_blank\">\n        5.\n      </a>\n    © 2017 Mesosphere, Inc. All Rights Reserved. 5\nDatacenter\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-6-638.jpg?cb=1503617560\" title=\"© 2017 Mesosphere, Inc. All Rights Reserved. 6&#10;NAIVE APPROA...\" target=\"_blank\">\n        6.\n      </a>\n    © 2017 Mesosphere, Inc. All Rights Reserved. 6\nNAIVE APPROACH\nTypical Datacenter\nsiloed, over-provisioned servers,\nlow utilization\nIndustry Average\n12-15% utilization\nmySQL\nmicroservice\nCassandra\nSpark/Hadoop\nKafka\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-7-638.jpg?cb=1503617560\" title=\"© 2017 Mesosphere, Inc. All Rights Reserved. 7&#10;\" target=\"_blank\">\n        7.\n      </a>\n    © 2017 Mesosphere, Inc. All Rights Reserved. 7\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-8-638.jpg?cb=1503617560\" title=\"© 2017 Mesosphere, Inc. All Rights Reserved. 8&#10;MULTIPLEXING...\" target=\"_blank\">\n        8.\n      </a>\n    © 2017 Mesosphere, Inc. All Rights Reserved. 8\nMULTIPLEXING OF DATA, SERVICES, USERS, ENVIRONMENTS\nTypical Datacenter\nsiloed, over-provisioned servers,\nlow utilization\nMesos/ DC/OS\nautomated schedulers, workload multiplexing onto the\nsame machines\nmySQL\nmicroservice\nCassandra\nSpark/Hadoop\nKafka\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-9-638.jpg?cb=1503617560\" title=\"Datacenter Operating System (DC/OS)&#10;Distributed Systems Ker...\" target=\"_blank\">\n        9.\n      </a>\n    Datacenter Operating System (DC/OS)\nDistributed Systems Kernel (Mesos)\nDC/OS ENABLES MODERN DISTRIBUTED APPS\nBig Data + Analytics EnginesMicroservices (in containers)\nStreaming\nBatch\nMachine Learning\nAnalytics\nFunctions &amp;\nLogic\nSearch\nTime Series\nSQL / NoSQL\nDatabases\nModern App Components\nAny Infrastructure (Physical, Virtual, Cloud)\n9\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-10-638.jpg?cb=1503617560\" title=\"© 2017 Mesosphere, Inc. All Rights Reserved. 10&#10;The SMACK S...\" target=\"_blank\">\n        10.\n      </a>\n    © 2017 Mesosphere, Inc. All Rights Reserved. 10\nThe SMACK Stack\nEVENTS\nUbiquitous data streams\nfrom connected devices\nINGEST\nApache Kafka\nSTORE\nApache Spark\nANALYZE\nApache Cassandra\nACT\nAkka\nIngest millions of events\nper second\nDistributed &amp; highly\nscalable database\nReal-time and batch\nprocess data\nVisualize data and build\ndata driven applications\nMesos/ DC/OS\nSensors\nDevices\nClients\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-11-638.jpg?cb=1503617560\" title=\"© 2017 Mesosphere, Inc. All Rights Reserved. 11&#10;The SMACK S...\" target=\"_blank\">\n        11.\n      </a>\n    © 2017 Mesosphere, Inc. All Rights Reserved. 11\nThe SMACK Stack\nEVENTS\nUbiquitous data streams\nfrom connected devices\nINGEST\nApache Kafka\nSTORE\nApache Spark\nANALYZE\nApache Cassandra\nACT\nAkka\nIngest millions of events\nper second\nDistributed &amp; highly\nscalable database\nReal-time and batch\nprocess data\nVisualize data and build\ndata driven applications\nMesos/ DC/OS\nSensors\nDevices\nClients\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-12-638.jpg?cb=1503617560\" title=\"© 2016 Mesosphere, Inc. All Rights Reserved.&#10;BIG DATA ECOSY...\" target=\"_blank\">\n        12.\n      </a>\n    © 2016 Mesosphere, Inc. All Rights Reserved.\nBIG DATA ECOSYSTEM YESTERDAY\n© 2017 Alluxio 12\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-13-638.jpg?cb=1503617560\" title=\"© 2016 Mesosphere, Inc. All Rights Reserved.&#10;BIG DATA ECOSY...\" target=\"_blank\">\n        13.\n      </a>\n    © 2016 Mesosphere, Inc. All Rights Reserved.\nBIG DATA ECOSYSTEM TODAY\n© 2017 Alluxio\n…\n…\n13\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-14-638.jpg?cb=1503617560\" title=\"© 2016 Mesosphere, Inc. All Rights Reserved.&#10;BIG DATA ECOSY...\" target=\"_blank\">\n        14.\n      </a>\n    © 2016 Mesosphere, Inc. All Rights Reserved.\nBIG DATA ECOSYSTEM ISSUES\n© 2017 Alluxio\n…\n…\n14\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-15-638.jpg?cb=1503617560\" title=\"© 2017 Mesosphere, Inc. All Rights Reserved. 15&#10;The SMAACK ...\" target=\"_blank\">\n        15.\n      </a>\n    © 2017 Mesosphere, Inc. All Rights Reserved. 15\nThe SMAACK Stack\nEVENTS\nUbiquitous data streams\nfrom connected devices\nINGEST\nApache Kafka\nSTORE\nApache Spark\nANALYZE\nApache Cassandra\nACT\nAkka\nIngest millions of events\nper second\nDistributed &amp; highly\nscalable database\nReal-time and batch\nprocess data\nVisualize data and build\ndata driven applications\nMesos/ DC/OS\nSensors\nDevices\nClients\nAlluxio\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-16-638.jpg?cb=1503617560\" title=\"© 2017 Mesosphere, Inc. All Rights Reserved. 16© 2017 Allux...\" target=\"_blank\">\n        16.\n      </a>\n    © 2017 Mesosphere, Inc. All Rights Reserved. 16© 2017 Alluxio\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-17-638.jpg?cb=1503617560\" title=\"© 2016 Mesosphere, Inc. All Rights Reserved.&#10;BIG DATA ECOSY...\" target=\"_blank\">\n        17.\n      </a>\n    © 2016 Mesosphere, Inc. All Rights Reserved.\nBIG DATA ECOSYSTEM WITH ALLUXIO\n…\n…\nFUSE Compatible File\nSystem Interface\nHadoop Compatible File\nSystem Interface\nNative Key-Value\nInterface\nNative File System\nInterface\nHDFS Interface Amazon S3 Interface Swift Interface GlusterFS Interface\n© 2017 Alluxio 17\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-18-638.jpg?cb=1503617560\" title=\"© 2016 Mesosphere, Inc. All Rights Reserved.&#10;BIG DATA ECOSY...\" target=\"_blank\">\n        18.\n      </a>\n    © 2016 Mesosphere, Inc. All Rights Reserved.\nBIG DATA ECOSYSTEM WITH ALLUXIO\n…\n…\nFUSE Compatible File\nSystem Interface\nHadoop Compatible File\nSystem Interface\nNative Key-Value\nInterface\nNative File System\nInterface\nHDFS Interface Amazon S3 Interface Swift Interface GlusterFS Interface\nEnabling Application to Access Data from any\nStorage System at Memory-speed\n© 2017 Alluxio 18\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-19-638.jpg?cb=1503617560\" title=\"© 2016 Mesosphere, Inc. All Rights Reserved.&#10;WHY ALLUXIO&#10;© ...\" target=\"_blank\">\n        19.\n      </a>\n    © 2016 Mesosphere, Inc. All Rights Reserved.\nWHY ALLUXIO\n© 2017 Alluxio\nCo-located compute and data with memory-speed access to data\nVirtualized across different storage systems under a unified namespace\nScale-out architecture\nFile system API, software only\n19\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-20-638.jpg?cb=1503617560\" title=\"© 2016 Mesosphere, Inc. All Rights Reserved.&#10;ALLUXIO BENEFI...\" target=\"_blank\">\n        20.\n      </a>\n    © 2016 Mesosphere, Inc. All Rights Reserved.\nALLUXIO BENEFITS\n© 2017 Alluxio\nUnification\nNew workflows across\nany data in any storage\nsystem\nOrders of magnitude\nimprovement in run\ntime\nChoice in compute and\nstorage – grow each\nindependently, buy\nonly what is needed\nPerformance Flexibility\n20\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-21-638.jpg?cb=1503617560\" title=\"© 2017 Mesosphere, Inc. All Rights Reserved. 21© 2017 Allux...\" target=\"_blank\">\n        21.\n      </a>\n    © 2017 Mesosphere, Inc. All Rights Reserved. 21© 2017 Alluxio\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-22-638.jpg?cb=1503617560\" title=\"© 2016 Mesosphere, Inc. All Rights Reserved. 22&#10;WHY DATA SE...\" target=\"_blank\">\n        22.\n      </a>\n    © 2016 Mesosphere, Inc. All Rights Reserved. 22\nWHY DATA SERVICES ON DC/OS?\nOn-demand provisioning1\n2\n3\nSimplified operations\nElastic data infrastructure\n● Single command install of services\n● Runtime software upgrade\n● Runtime application settings update\n● Monitoring &amp; metrics\n● Managed persistent storage volumes\n● Data services and containerized apps share resources\n● Deploy instances with different versions on the same\ninfrastructure\n● Resize instances\n● Add more instances\n© 2017 Alluxio\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-23-638.jpg?cb=1503617560\" title=\"© 2016 Mesosphere, Inc. All Rights Reserved. 23&#10;ALLUXIO ON ...\" target=\"_blank\">\n        23.\n      </a>\n    © 2016 Mesosphere, Inc. All Rights Reserved. 23\nALLUXIO ON MESOSPHERE DC/OS\nFast, On-demand Unified Data at Memory Speed for Analytics\nAlluxio\nMesosphere DC/OS\nAny Infrastructure\nBuild apps once in DC/OS, and\nrun anywhere\nRuns distributed apps anywhere as\nsimply as running apps on your laptop\nUnify Data at Memory Speed Unify Data at Memory Speed\n© 2017 Alluxio\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-24-638.jpg?cb=1503617560\" title=\"© 2016 Mesosphere, Inc. All Rights Reserved. 24&#10;ALLUXIO ON ...\" target=\"_blank\">\n        24.\n      </a>\n    © 2016 Mesosphere, Inc. All Rights Reserved. 24\nALLUXIO ON MESOSPHERE DC/OS\nFast, On-demand Unified Data at Memory Speed for Analytics\n© 2017 Alluxio\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-25-638.jpg?cb=1503617560\" title=\"© 2016 Mesosphere, Inc. All Rights Reserved.&#10;WHY ALLUXIO ON...\" target=\"_blank\">\n        25.\n      </a>\n    © 2016 Mesosphere, Inc. All Rights Reserved.\nWHY ALLUXIO ON MESOSPHERE DC/OS?\n● Without Mesosphere DC/OS, provisioning of infrastructure is tedious\n○ Mesosphere DC/OS automates app &amp; cluster provisioning, management &amp; elastic scaling\n● Alluxio brings\n○ A unified view of data across disparate storage systems\n○ High performance &amp; predictable SLA for analytics workloads\n● Benefits include:\n○ Process data in your existing cluster faster with Spark and other analytics frameworks\n○ Process data from hybrid cloud storage systems (HDFS, S3, On-prem Object Stores etc)\n© 2017 Alluxio 25\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-26-638.jpg?cb=1503617560\" title=\"© 2016 Mesosphere, Inc. All Rights Reserved. 26&#10;BIG DATA ST...\" target=\"_blank\">\n        26.\n      </a>\n    © 2016 Mesosphere, Inc. All Rights Reserved. 26\nBIG DATA STACK WITH ALLUXIO ON MESOSPHERE DC/OS\nFast, On-demand Unified Data at Memory Speed for Analytics\nMesos\nContainer Orchestration Management &amp; Monitoring Tools Apps Universe\nSecurity Advanced Operations Multitenancy Adv. Network &amp; Storage\nUnifying Data at Memory Speed\n© 2017 Alluxio\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-27-638.jpg?cb=1503617560\" title=\"© 2017 Mesosphere, Inc. All Rights Reserved. 27© 2017 Allux...\" target=\"_blank\">\n        27.\n      </a>\n    © 2017 Mesosphere, Inc. All Rights Reserved. 27© 2017 Alluxio\nDEMO\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-28-638.jpg?cb=1503617560\" title=\"© 2016 Mesosphere, Inc. All Rights Reserved.&#10;WHAT HAPPENED?...\" target=\"_blank\">\n        28.\n      </a>\n    © 2016 Mesosphere, Inc. All Rights Reserved.\nWHAT HAPPENED?\n● Alluxio scheduler (developed using the DC/OS SDK) launched as a Marathon application\n○ Marathon manages and restarts the scheduler in case of failures\n○ Scheduler consists of YAML + scripting\n● Alluxio scheduler launched master and worker processes\n○ Scheduler manages the configured number of instances even w/ failures\n● Configuration changes take effect on the fly\n○ Scaled up the worker instances\n© 2017 Alluxio 28\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/alluxiomesosmeetup-smacktosmaack-170824232904/95/alluxio-mesos-meetup-smack-to-smaack-29-638.jpg?cb=1503617560\" title=\"© 2016 Mesosphere, Inc. All Rights Reserved.&#10;GET STARTED TO...\" target=\"_blank\">\n        29.\n      </a>\n    © 2016 Mesosphere, Inc. All Rights Reserved.\nGET STARTED TODAY\nRead:\n● Mesosphere Blog: http://ow.ly/ou0530ax9aM\n● Alluxio Blog: http://ow.ly/ILOZ30ax8YE\nTry it out:\n● Install Alluxio from DC/OS Universe\nQuestions?\n© 2017 Alluxio 29\n \n  </li>\n              </ol></div></div></div><aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\">\n<dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\"><ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"79137765\" title=\"Teacher Tech Tips Weekly\" href=\"https://www.linkedin.com/learning/teacher-tech-tips-weekly?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Teacher Tech Tips Weekly\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Teacher Tech Tips Weekly\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=lgfgIgjjJAKVM5DVfST6aI0mqOo%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-iXCat8tafY3Dqfs_cZLSiol8fey4Hlg07feerRDXiEI69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Teacher Tech Tips Weekly</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"79137765\" title=\"Insights from a Content Marketer\" href=\"https://www.linkedin.com/learning/insights-from-a-content-marketer?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Insights from a Content Marketer\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Insights from a Content Marketer\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=SmaDgeUm4oxJMJqZcKGjYT2sC7Q%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-gWy2r-dOfYXfheMTZZLSiol4fcCoJmA06euuqQzbjEY69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Insights from a Content Marketer</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"79137765\" title=\"Educational Technology for Student Success\" href=\"https://www.linkedin.com/learning/educational-technology-for-student-success?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Educational Technology for Student Success\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Educational Technology for Student Success\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=oL4sQVpiStV6448%2Bo7RH5VraJnw%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lWiCp_9KfZHbsesLYZLSioVUXeSwIkwY6fe-rQTngG469LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Educational Technology for Student Success</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"84193453\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"The Architecture of Decoupling Compute and Storage with Alluxio\" href=\"https://www.slideshare.net/Alluxio/the-architecture-of-decoupling-compute-and-storage-with-alluxio\">\n    \n    <div class=\"related-content\"><p>The Architecture of Decoupling Compute and Storage with Alluxio</p><p>Alluxio, Inc.</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"84120822\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Best Practices for Using Alluxio with Spark\" href=\"https://www.slideshare.net/Alluxio/best-practices-for-using-alluxio-with-spark-84120822\">\n    \n    <div class=\"related-content\"><p>Best Practices for Using Alluxio with Spark</p><p>Alluxio, Inc.</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"84120762\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Spark Pipelines in the Cloud with Alluxio\" href=\"https://www.slideshare.net/Alluxio/spark-pipelines-in-the-cloud-with-alluxio\">\n    \n    <div class=\"related-content\"><p>Spark Pipelines in the Cloud with Alluxio</p><p>Alluxio, Inc.</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"84120674\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Accelerating Spark Workloads in a Mesos Environment with Alluxio\" href=\"https://www.slideshare.net/Alluxio/accelerating-spark-workloads-in-a-mesos-environment-with-alluxio\">\n    \n    <div class=\"related-content\"><p>Accelerating Spark Workloads in a Mesos Environment with Alluxio</p><p>Alluxio, Inc.</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"84120522\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Accelerating Spark Workloads in an Apache Mesos Environment with Alluxio\" href=\"https://www.slideshare.net/Alluxio/accelerating-spark-workloads-in-an-apache-mesos-environment-with-alluxio\">\n    \n    <div class=\"related-content\"><p>Accelerating Spark Workloads in an Apache Mesos Environment with Alluxio</p><p>Alluxio, Inc.</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"80472156\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Best Practices for Using Alluxio with Spark\" href=\"https://www.slideshare.net/Alluxio/strata-new-york\">\n    \n    <div class=\"related-content\"><p>Best Practices for Using Alluxio with Spark</p><p>Alluxio, Inc.</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"76876344\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Best Practices for Using Alluxio with Spark\" href=\"https://www.slideshare.net/Alluxio/best-practices-for-using-alluxio-with-spark\">\n    \n    <div class=\"related-content\"><p>Best Practices for Using Alluxio with Spark</p><p>Alluxio, Inc.</p></div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div></div><footer>\n          <div class=\"row\"><div class=\"columns\"><ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div></div>\n          \n          <div class=\"row\"><div class=\"columns\"><p class=\"copyright text-center\">LinkedIn Corporation © 2018</p></div></div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div>\n    <div class=\"row\"><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></div>\n        \n    </div>\n    \n    \n      <noscript>\n    </noscript>",
        "created_at": "2018-01-15T17:14:03+0000",
        "updated_at": "2018-03-15T14:20:59+0000",
        "published_at": null,
        "published_by": [
          "Alluxio, Inc."
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 6,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/alluxiomesosmeetup-smacktosmaack-170824232904-thumbnail-4.jpg?cb=1503617560",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9067"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 907,
            "label": "mesos",
            "slug": "mesos"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 963,
            "label": "akka",
            "slug": "akka"
          },
          {
            "id": 1041,
            "label": "smack",
            "slug": "smack"
          }
        ],
        "is_public": false,
        "id": 9066,
        "uid": null,
        "title": "Kick-Start with SMACK Stack",
        "url": "https://www.slideshare.net/knoldus/kickstart-with-smack-stack",
        "content": "Kick-Start with SMACK Stack\n      \n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            </a><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div>\n    <div class=\"wrapper\"><p>Successfully reported this slideshow.</p><div id=\"slideview-container\" class=\"\"><div class=\"row\"><div id=\"main-panel\" class=\"small-12 large-8 columns\"><div class=\"sectionElements\"><div class=\"playerWrapper\"><div><div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\">Kick-Start with SMACK Stack<div class=\"stage valign-first-slide\"><div class=\"slide_container\"><section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-1-638.jpg?cb=1487577725\" data-small=\"https://image.slidesharecdn.com/smackmeetup-170220075445/85/kickstart-with-smack-stack-1-320.jpg?cb=1487577725\" data-normal=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-1-638.jpg?cb=1487577725\" data-full=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-1-1024.jpg?cb=1487577725\" alt=\"Kick-Start with SMACK Stack&#10;Sandeep Purohit&#10;Software Consultant&#10;Knoldus Software LLP&#10;\" /></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/knoldus/kickstart-with-smack-stack\" data-small=\"https://image.slidesharecdn.com/smackmeetup-170220075445/85/kickstart-with-smack-stack-2-320.jpg?cb=1487577725\" data-normal=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-2-638.jpg?cb=1487577725\" data-full=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-2-1024.jpg?cb=1487577725\" alt=\"Agenda:&#10;● What is SMACK?&#10;● Why SMACK?&#10;● Brief introduction of technologies&#10;● How to Integrate all the technologies to crea...\" /></i></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/knoldus/kickstart-with-smack-stack\" data-small=\"https://image.slidesharecdn.com/smackmeetup-170220075445/85/kickstart-with-smack-stack-3-320.jpg?cb=1487577725\" data-normal=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-3-638.jpg?cb=1487577725\" data-full=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-3-1024.jpg?cb=1487577725\" alt=\"What is SMACK?&#10;● Spark :Apache Spark is a fast and general-purpose cluster&#10;computing system.&#10;● Mesos :Cluster resource man...\" /></i></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/knoldus/kickstart-with-smack-stack\" data-small=\"https://image.slidesharecdn.com/smackmeetup-170220075445/85/kickstart-with-smack-stack-4-320.jpg?cb=1487577725\" data-normal=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-4-638.jpg?cb=1487577725\" data-full=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-4-1024.jpg?cb=1487577725\" alt=\"Why SMACK?&#10;● Smack is used for pipelined data architecture which is&#10;required for the real time data analysis.&#10;● Smack is u...\" /></i></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/knoldus/kickstart-with-smack-stack\" data-small=\"https://image.slidesharecdn.com/smackmeetup-170220075445/85/kickstart-with-smack-stack-5-320.jpg?cb=1487577725\" data-normal=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-5-638.jpg?cb=1487577725\" data-full=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-5-1024.jpg?cb=1487577725\" alt=\"SMACK Pipeline Architecture&#10;\" /></i></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/knoldus/kickstart-with-smack-stack\" data-small=\"https://image.slidesharecdn.com/smackmeetup-170220075445/85/kickstart-with-smack-stack-6-320.jpg?cb=1487577725\" data-normal=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-6-638.jpg?cb=1487577725\" data-full=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-6-1024.jpg?cb=1487577725\" alt=\"Why Spark?&#10;● Its general purpose big data processing engine which have&#10;4 main components spark core, spark streaming, spar...\" /></i></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/knoldus/kickstart-with-smack-stack\" data-small=\"https://image.slidesharecdn.com/smackmeetup-170220075445/85/kickstart-with-smack-stack-7-320.jpg?cb=1487577725\" data-normal=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-7-638.jpg?cb=1487577725\" data-full=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-7-1024.jpg?cb=1487577725\" alt=\"Why Cassandra?&#10;● Cassandra implements “no single points of failure&#10;● Cassandra Write-path is so fast so it can handle real...\" /></i></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/knoldus/kickstart-with-smack-stack\" data-small=\"https://image.slidesharecdn.com/smackmeetup-170220075445/85/kickstart-with-smack-stack-8-320.jpg?cb=1487577725\" data-normal=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-8-638.jpg?cb=1487577725\" data-full=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-8-1024.jpg?cb=1487577725\" alt=\"Why Mesos?&#10;Mesos Master&#10;Mesos Master&#10;Standby&#10;Mesos Master&#10;Standby&#10;Zookepeer&#10;Mesos Slave&#10;Mesos Slave&#10;Mesos Slave&#10;\" /></i></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/knoldus/kickstart-with-smack-stack\" data-small=\"https://image.slidesharecdn.com/smackmeetup-170220075445/85/kickstart-with-smack-stack-9-320.jpg?cb=1487577725\" data-normal=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-9-638.jpg?cb=1487577725\" data-full=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-9-1024.jpg?cb=1487577725\" alt=\"Models in SMACK&#10;● In SMACK models are Scala and AKKA.&#10;● We can use models to write highly concurrent and parallel&#10;applicat...\" /></i></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/knoldus/kickstart-with-smack-stack\" data-small=\"https://image.slidesharecdn.com/smackmeetup-170220075445/85/kickstart-with-smack-stack-10-320.jpg?cb=1487577725\" data-normal=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-10-638.jpg?cb=1487577725\" data-full=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-10-1024.jpg?cb=1487577725\" alt=\"Models use in SMACK&#10;Akka-Http&#10;Akka-Scheduler&#10;\" /></i></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/knoldus/kickstart-with-smack-stack\" data-small=\"https://image.slidesharecdn.com/smackmeetup-170220075445/85/kickstart-with-smack-stack-11-320.jpg?cb=1487577725\" data-normal=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-11-638.jpg?cb=1487577725\" data-full=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-11-1024.jpg?cb=1487577725\" alt=\"Why Kafka&#10;● streams of data efficiently and in real time&#10;● Use Kafka for fault tolerance.&#10;● To create bridge between two a...\" /></i></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/knoldus/kickstart-with-smack-stack\" data-small=\"https://image.slidesharecdn.com/smackmeetup-170220075445/85/kickstart-with-smack-stack-12-320.jpg?cb=1487577725\" data-normal=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-12-638.jpg?cb=1487577725\" data-full=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-12-1024.jpg?cb=1487577725\" alt=\"Architecture of Spark and cassandra&#10;Cassandra Cluster&#10;Spark Worker&#10;Spark Worker&#10;Spark Worker&#10;Spark Worker&#10;Spark worker nod...\" /></i></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/knoldus/kickstart-with-smack-stack\" data-small=\"https://image.slidesharecdn.com/smackmeetup-170220075445/85/kickstart-with-smack-stack-13-320.jpg?cb=1487577725\" data-normal=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-13-638.jpg?cb=1487577725\" data-full=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-13-1024.jpg?cb=1487577725\" alt=\"Spark, Mesos, Cassandra&#10;Mesos Slaves and cassandra nodes are collocated to enforce the better data&#10;locality for spark.&#10;Dri...\" /></i></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/knoldus/kickstart-with-smack-stack\" data-small=\"https://image.slidesharecdn.com/smackmeetup-170220075445/85/kickstart-with-smack-stack-14-320.jpg?cb=1487577725\" data-normal=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-14-638.jpg?cb=1487577725\" data-full=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-14-1024.jpg?cb=1487577725\" alt=\"Demo Application Architecture&#10;Tweets&#10;Store tweets in&#10;kafka topic&#10;Retrieve&#10;hashtags&#10;Evaluate Top&#10;hashtag in&#10;every 10&#10;second...\" /></i></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/knoldus/kickstart-with-smack-stack\" data-small=\"https://image.slidesharecdn.com/smackmeetup-170220075445/85/kickstart-with-smack-stack-15-320.jpg?cb=1487577725\" data-normal=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-15-638.jpg?cb=1487577725\" data-full=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-15-1024.jpg?cb=1487577725\" alt=\"Demo&#10;SMACK_Tweets&#10;\" /></i></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/knoldus/kickstart-with-smack-stack\" data-small=\"https://image.slidesharecdn.com/smackmeetup-170220075445/85/kickstart-with-smack-stack-16-320.jpg?cb=1487577725\" data-normal=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-16-638.jpg?cb=1487577725\" data-full=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-16-1024.jpg?cb=1487577725\" alt=\"Thank You!!&#10;\" /></i></section><div class=\"j-next-container next-container\"><div class=\"content-container\"><div class=\"next-slideshow-wrapper\"><div class=\"j-next-slideshow next-slideshow\"><p>Upcoming SlideShare</p></div><p>Loading in …5</p><p>×</p></div></div></div></div></div></div></div></div></div><div class=\"slideshow-info-container\" itemscope=\"itemscope\" itemtype=\"https://schema.org/MediaObject\"><div class=\"slideshow-tabs-container show-for-medium-up\"><ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    0 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\">\n                \n                  2 Likes\n                \n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\">\n                Statistics\n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\">\n                Notes\n              </i></a>\n            </li>\n          </ul><div class=\"tabs-content\"><div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\"><ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"itemscope\"><li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"paulslattery1\" rel=\"nofollow\" href=\"https://www.slideshare.net/paulslattery1?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Paul Slattery\n                            \n                              \n                                , \n                                Technical Director\n                              \n                              \n                                 at \n                                AKQA\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"StreamingAnalytics\" rel=\"nofollow\" href=\"https://www.slideshare.net/StreamingAnalytics?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Streaming Analytics\n                            \n                              \n                                , \n                                Technology Manager at Trivadis\n                              \n                              \n                                 at \n                                Trivadis\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n              </ul></div><div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No Downloads</p></div><div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No notes for slide</p></div></div></div><div class=\"notranslate transcript add-padding-right j-transcript\"><ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    Kick-Start with SMACK Stack\nSandeep Purohit\nSoftware Consultant\nKnoldus Software LLP\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-2-638.jpg?cb=1487577725\" title=\"Agenda:&#10;● What is SMACK?&#10;● Why SMACK?&#10;● Brief introduction ...\" target=\"_blank\">\n        2.\n      </a>\n    Agenda:\n● What is SMACK?\n● Why SMACK?\n● Brief introduction of technologies\n● How to Integrate all the technologies to create the data pipeline\n● Demo\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-3-638.jpg?cb=1487577725\" title=\"What is SMACK?&#10;● Spark :Apache Spark is a fast and general-...\" target=\"_blank\">\n        3.\n      </a>\n    What is SMACK?\n● Spark :Apache Spark is a fast and general-purpose cluster\ncomputing system.\n● Mesos :Cluster resource management system that provide\nefficient resource allocation.\n● Akka :Akka is a toolkit and runtime for building highly\nconcurrent, distributed, and resilient message-driven\napplications on the JVM.\n● Cassandra :The Apache Cassandra database is the right\nchoice when you need scalability and high availability.\n● Kafka :distributed messaging system for handling real\ntime data.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-4-638.jpg?cb=1487577725\" title=\"Why SMACK?&#10;● Smack is used for pipelined data architecture ...\" target=\"_blank\">\n        4.\n      </a>\n    Why SMACK?\n● Smack is used for pipelined data architecture which is\nrequired for the real time data analysis.\n● Smack is use to integrate all the technology at the right\nplace to efficient data pipeline.\n● Smack is use to linearly scale your whole cluster without\nany hassle\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-5-638.jpg?cb=1487577725\" title=\"SMACK Pipeline Architecture&#10;\" target=\"_blank\">\n        5.\n      </a>\n    SMACK Pipeline Architecture\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-6-638.jpg?cb=1487577725\" title=\"Why Spark?&#10;● Its general purpose big data processing engine...\" target=\"_blank\">\n        6.\n      </a>\n    Why Spark?\n● Its general purpose big data processing engine which have\n4 main components spark core, spark streaming, spark\nml, spark graphx\n● So we can process our data which any of the component\nat real time.\n● Its provide fault tolerant for real time application.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-7-638.jpg?cb=1487577725\" title=\"Why Cassandra?&#10;● Cassandra implements “no single points of ...\" target=\"_blank\">\n        7.\n      </a>\n    Why Cassandra?\n● Cassandra implements “no single points of failure\n● Cassandra Write-path is so fast so it can handle real-time data easily\n● It will support Datacenter architecture so we can easily use different\nDC for different things.\nIngestion DC Analysis DC\nCassandra Cluster\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-8-638.jpg?cb=1487577725\" title=\"Why Mesos?&#10;Mesos Master&#10;Mesos Master&#10;Standby&#10;Mesos Master&#10;S...\" target=\"_blank\">\n        8.\n      </a>\n    Why Mesos?\nMesos Master\nMesos Master\nStandby\nMesos Master\nStandby\nZookepeer\nMesos Slave\nMesos Slave\nMesos Slave\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-9-638.jpg?cb=1487577725\" title=\"Models in SMACK&#10;● In SMACK models are Scala and AKKA.&#10;● We ...\" target=\"_blank\">\n        9.\n      </a>\n    Models in SMACK\n● In SMACK models are Scala and AKKA.\n● We can use models to write highly concurrent and parallel\napplications.\n● Example: We can use akka modules according to our use\ncase like akka-http, akka-scheduler, akka priority\nmailboxes etc.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-10-638.jpg?cb=1487577725\" title=\"Models use in SMACK&#10;Akka-Http&#10;Akka-Scheduler&#10;\" target=\"_blank\">\n        10.\n      </a>\n    Models use in SMACK\nAkka-Http\nAkka-Scheduler\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-11-638.jpg?cb=1487577725\" title=\"Why Kafka&#10;● streams of data efficiently and in real time&#10;● ...\" target=\"_blank\">\n        11.\n      </a>\n    Why Kafka\n● streams of data efficiently and in real time\n● Use Kafka for fault tolerance.\n● To create bridge between two applications.\nStreaming\nSource\nKafka\nBroker\nSpark Receiver\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-12-638.jpg?cb=1487577725\" title=\"Architecture of Spark and cassandra&#10;Cassandra Cluster&#10;Spark...\" target=\"_blank\">\n        12.\n      </a>\n    Architecture of Spark and cassandra\nCassandra Cluster\nSpark Worker\nSpark Worker\nSpark Worker\nSpark Worker\nSpark worker nodes\nwill get the data on\nlocal node so it will\navoid latency\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-13-638.jpg?cb=1487577725\" title=\"Spark, Mesos, Cassandra&#10;Mesos Slaves and cassandra nodes ar...\" target=\"_blank\">\n        13.\n      </a>\n    Spark, Mesos, Cassandra\nMesos Slaves and cassandra nodes are collocated to enforce the better data\nlocality for spark.\nDriver\nProgram\nMesos\nMaster\nMesos slave\nCassandra node\nMesos slave\nCassandra node\nMesos slave\nCassandra node\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-14-638.jpg?cb=1487577725\" title=\"Demo Application Architecture&#10;Tweets&#10;Store tweets in&#10;kafka ...\" target=\"_blank\">\n        14.\n      </a>\n    Demo Application Architecture\nTweets\nStore tweets in\nkafka topic\nRetrieve\nhashtags\nEvaluate Top\nhashtag in\nevery 10\nseconds\nStore tweets in\ncassandra table\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-15-638.jpg?cb=1487577725\" title=\"Demo&#10;SMACK_Tweets&#10;\" target=\"_blank\">\n        15.\n      </a>\n    Demo\nSMACK_Tweets\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackmeetup-170220075445/95/kickstart-with-smack-stack-16-638.jpg?cb=1487577725\" title=\"Thank You!!&#10;\" target=\"_blank\">\n        16.\n      </a>\n    Thank You!!\n \n  </li>\n              </ol></div></div></div><aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\"><dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\"><ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"72350606\" title=\"PowerPoint 2016: Tips and Tricks\" href=\"https://www.linkedin.com/learning/powerpoint-2016-tips-and-tricks?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_PowerPoint 2016: Tips and Tricks\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"PowerPoint 2016: Tips and Tricks\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=nIcokYvYRZ365i4sqNHaREMZ6bI%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lUyKj_tWfZH_ucMPfZLSiol8eeywDlAE0e-moQTPjFI69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>PowerPoint 2016: Tips and Tricks</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"72350606\" title=\"Elearning Techniques: Visual Design\" href=\"https://www.linkedin.com/learning/elearning-techniques-visual-design?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Elearning Techniques: Visual Design\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Elearning Techniques: Visual Design\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=6p1m1b6Lqo18UsM42Md19Wgc7Eg%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-kXyWp_tGfZXPpesPbZLSiol8VeiQCmAwzd-mrSDfmFY69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Elearning Techniques: Visual Design</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"72350606\" title=\"Teaching Techniques: Creating Multimedia Learning\" href=\"https://www.linkedin.com/learning/teaching-techniques-creating-multimedia-learning?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Teaching Techniques: Creating Multimedia Learning\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Teaching Techniques: Creating Multimedia Learning\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=5yNrggG44Flut51QOcRX28HD9g0%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-iXSWq-dafY3HpecTcZLSioVsTfy8EkQ07eO6vRzjoG469LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Teaching Techniques: Creating Multimedia Learning</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"72977315\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Akka Finite State Machine\" href=\"https://www.slideshare.net/knoldus/akka-finite-state-machine\">\n    \n    <div class=\"related-content\"><p>Akka Finite State Machine</p><p>Knoldus Software LLP.</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"73126074\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Introduction to AWS IAM\" href=\"https://www.slideshare.net/knoldus/introduction-to-aws-iam\">\n    \n    <div class=\"related-content\"><p>Introduction to AWS IAM</p><p>Knoldus Software LLP.</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"72543895\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Reactive Fast Data &amp; the Data Lake with Akka, Kafka, Spark\" href=\"https://www.slideshare.net/ToddFritz/reactive-fast-data-the-data-lake-with-akka-kafka-spark\">\n    \n    <div class=\"related-content\"><p>Reactive Fast Data &amp; the Data Lake with Akka, Kafka, Spark</p><p>Todd Fritz</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"52661852\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Data processing platforms architectures with Spark, Mesos, Akka, Cassandra and Kafka\" href=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\">\n    \n    <div class=\"related-content\"><p>Data processing platforms architectures with Spark, Mesos, Akka, Cassandra an...</p><p>Anton Kirillov</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"54491621\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Real-Time Anomaly Detection  with Spark MLlib, Akka and  Cassandra\" href=\"https://www.slideshare.net/natalinobusa/realtime-anomaly-detection-with-spark-mllib-akka-and-cassandra\">\n    \n    <div class=\"related-content\"><p>Real-Time Anomaly Detection  with Spark MLlib, Akka and  Cassandra</p><p>Natalino Busa</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"51816698\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Reactive dashboard’s using apache spark\" href=\"https://www.slideshare.net/RahulKumar405/reactive-dashboards-using-apache-spark\">\n    \n    <div class=\"related-content\"><p>Reactive dashboard’s using apache spark</p><p>Rahul Kumar</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"54703139\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Data Science lifecycle with Apache Zeppelin and Spark by Moonsoo Lee\" href=\"https://www.slideshare.net/SparkSummit/data-science-lifecycle-with-apache-zeppelin-and-spark-by-moonsoo-lee\">\n    \n    <div class=\"related-content\"><p>Data Science lifecycle with Apache Zeppelin and Spark by Moonsoo Lee</p><p>Spark Summit</p></div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div></div><footer>\n          <div class=\"row\"><div class=\"columns\"><ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div></div>\n          \n          <div class=\"row\"><div class=\"columns\"><p class=\"copyright text-center\">LinkedIn Corporation © 2018</p></div></div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div>\n    <div class=\"row\"><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></div>\n        \n    </div>\n    \n    \n      <noscript>\n    </noscript>",
        "created_at": "2018-01-15T16:27:18+0000",
        "updated_at": "2018-03-15T14:21:19+0000",
        "published_at": null,
        "published_by": [
          "Knoldus Software LLP."
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/smackmeetup-170220075445-thumbnail-4.jpg?cb=1487577725",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9066"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 907,
            "label": "mesos",
            "slug": "mesos"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 963,
            "label": "akka",
            "slug": "akka"
          },
          {
            "id": 1041,
            "label": "smack",
            "slug": "smack"
          }
        ],
        "is_public": false,
        "id": 9065,
        "uid": null,
        "title": "SMACK Stack 1.1",
        "url": "https://www.slideshare.net/charmalloc/smack-stack-11",
        "content": "SMACK Stack 1.1\n      \n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            </a><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div>\n    <div class=\"wrapper\"><p>Successfully reported this slideshow.</p><div id=\"slideview-container\" class=\"\"><div class=\"row\"><div id=\"main-panel\" class=\"small-12 large-8 columns\"><div class=\"sectionElements\"><div class=\"playerWrapper\"><div><div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\">SMACK Stack 1.1<div class=\"stage valign-first-slide\"><div class=\"slide_container\"><section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-1-638.jpg?cb=1459351563\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-1-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-1-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-1-1024.jpg?cb=1459351563\" alt=\"SMACK Stack 1.1&#10;\" /></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-2-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-2-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-2-1024.jpg?cb=1459351563\" alt=\"Elodina is a big data as a service platform built on top&#10;of open source software.&#10;The Elodina platform solves today’s data...\" /></i></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-3-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-3-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-3-1024.jpg?cb=1459351563\" alt=\"Whats SMACK Stack?&#10;SMACK stack 1.0 has been traditionally Spark, Mesos, Akka, Cassandra and&#10;Kafka lots https://dzone.com/a...\" /></i></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-4-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-4-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-4-1024.jpg?cb=1459351563\" alt=\"The free lunch is over!&#10;http://www.gotw.ca/publications/concurrency-ddj.htm&#10;\" /></i></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-5-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-5-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-5-1024.jpg?cb=1459351563\" alt=\"Many industries still don’t get it&#10;XML is everywhere but we have alternatives!&#10;We can support XML interface but don’t have...\" /></i></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-6-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-6-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-6-1024.jpg?cb=1459351563\" alt=\"You need to be running Mesos. Lots of options here!&#10;What is most important is that you abstract your “Provider” from your ...\" /></i></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-7-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-7-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-7-1024.jpg?cb=1459351563\" alt=\"“Provider” of compute resources&#10;\" /></i></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-8-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-8-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-8-1024.jpg?cb=1459351563\" alt=\"The Grid … 2.0 ...&#10;https://github.com/elodina/sawfly/blob/master/cloud-deploy-grid.md&#10;Program against your datacenter like...\" /></i></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-9-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-9-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-9-1024.jpg?cb=1459351563\" alt=\"Data Center Optimization!&#10;\" /></i></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-10-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-10-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-10-1024.jpg?cb=1459351563\" alt=\"But there is more!&#10;● Provisioning&#10;● Micro Segmentation&#10;● Orchestration&#10;● Configuration Management&#10;● Service Discovery&#10;● De...\" /></i></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-11-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-11-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-11-1024.jpg?cb=1459351563\" alt=\"Stack Deploy to the rescue!&#10;\" /></i></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-12-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-12-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-12-1024.jpg?cb=1459351563\" alt=\"In the Grid you need Schedulers!&#10;● Kafka – Producer/Consumer-based message queue management&#10;● Exhibitor – Supervisor for d...\" /></i></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-13-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-13-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-13-1024.jpg?cb=1459351563\" alt=\"Virtual Telemetry “Data Center” In the Grid&#10;ZipkinQATeamBuild92&#10;● 1x Exhibitor-Mesos&#10;● 1x Exhibitor&#10;● 1x DSE-Mesos&#10;● 1x Ca...\" /></i></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-14-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-14-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-14-1024.jpg?cb=1459351563\" alt=\"Stack Deploy In Action&#10;./stack-deploy addlayer --file stacks/cassandra_dc.stack --level datacenter&#10;./stack-deploy addlayer...\" /></i></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-15-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-15-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-15-1024.jpg?cb=1459351563\" alt=\"Full Stack Deployments&#10;\" /></i></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-16-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-16-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-16-1024.jpg?cb=1459351563\" alt=\"Cassandra&#10;\" /></i></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-17-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-17-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-17-1024.jpg?cb=1459351563\" alt=\"Cassandra Multi DC&#10;\" /></i></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-18-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-18-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-18-1024.jpg?cb=1459351563\" alt=\"Casandra https://github.com/elodina/datastax-enterprise-mesos&#10;\" /></i></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-19-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-19-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-19-1024.jpg?cb=1459351563\" alt=\"Start your nodes!&#10;\" /></i></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-20-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-20-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-20-1024.jpg?cb=1459351563\" alt=\"Apache Kafka&#10;• Apache Kafka&#10;o http://kafka.apache.org&#10;• Apache Kafka Source Code&#10;o https://github.com/apache/kafka&#10;• Docum...\" /></i></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-21-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-21-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-21-1024.jpg?cb=1459351563\" alt=\"It often starts with just one data pipeline&#10;\" /></i></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-22-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-22-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-22-1024.jpg?cb=1459351563\" alt=\"Reuse of data pipelines for new producers&#10;\" /></i></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-23-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-23-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-23-1024.jpg?cb=1459351563\" alt=\"Reuse of existing providers for new consumers&#10;\" /></i></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-24-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-24-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-24-1024.jpg?cb=1459351563\" alt=\"Eventually the solution becomes the problem&#10;\" /></i></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-25-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-25-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-25-1024.jpg?cb=1459351563\" alt=\"Kafka decouples data-pipelines&#10;\" /></i></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-26-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-26-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-26-1024.jpg?cb=1459351563\" alt=\"Topics &amp; Partitions&#10;\" /></i></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-27-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-27-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-27-1024.jpg?cb=1459351563\" alt=\"A high-throughput distributed messaging system&#10;rethought as a distributed commit log.&#10;\" /></i></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-28-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-28-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-28-1024.jpg?cb=1459351563\" alt=\"Intra Cluster Replication&#10;\" /></i></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-29-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-29-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-29-1024.jpg?cb=1459351563\" alt=\"Mesos Kafka http://github.com/mesos/kafka&#10;\" /></i></section><section data-index=\"30\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-30-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-30-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-30-1024.jpg?cb=1459351563\" alt=\"Streaming &amp; Analytics&#10;● The landscape of streaming is about to get more fragmented and harder to&#10;navigate. This is not all...\" /></i></section><section data-index=\"31\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-31-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-31-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-31-1024.jpg?cb=1459351563\" alt=\"GearPump&#10;\" /></i></section><section data-index=\"32\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-32-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-32-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-32-1024.jpg?cb=1459351563\" alt=\"Airflow&#10;\" /></i></section><section data-index=\"33\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-33-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-33-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-33-1024.jpg?cb=1459351563\" alt=\"Spring Cloud Data Flow&#10;\" /></i></section><section data-index=\"34\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-34-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-34-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-34-1024.jpg?cb=1459351563\" alt=\"Storm (and Storm Topology based systems)&#10;\" /></i></section><section data-index=\"35\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-35-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-35-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-35-1024.jpg?cb=1459351563\" alt=\"Storm Nimbus&#10;{&#10;&quot;id&quot;: &quot;storm-nimbus&quot;,&#10;&quot;cmd&quot;: &quot;cp storm.yaml storm-mesos-0.9.6/conf &amp;&amp; cd storm-mesos-0.9.6 &amp;&amp; ./bin/storm-m...\" /></i></section><section data-index=\"36\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-36-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-36-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-36-1024.jpg?cb=1459351563\" alt=\"Storm UI&#10;{&#10;&quot;id&quot;: &quot;storm-ui&quot;,&#10;&quot;cmd&quot;: &quot;cp storm.yaml storm-mesos-0.9.6/conf &amp;&amp; cd storm-mesos-0.9.6 &amp;&amp; ./bin/storm ui -c ui....\" /></i></section><section data-index=\"37\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-37-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-37-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-37-1024.jpg?cb=1459351563\" alt=\"Storm Kafka - new spouts &amp; bolts for Kafka 8, 9, ...&#10;\" /></i></section><section data-index=\"38\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-38-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-38-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-38-1024.jpg?cb=1459351563\" alt=\"Apache Kafka Streams&#10;\" /></i></section><section data-index=\"39\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-39-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-39-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-39-1024.jpg?cb=1459351563\" alt=\"Go Kafka Client - Fan Out Processing&#10;https://github.com/elodina/go-kafka-client-mesos&#10;● Dynamic Kafka Log workers&#10;● Blue/G...\" /></i></section><section data-index=\"40\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-40-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-40-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-40-1024.jpg?cb=1459351563\" alt=\"Questions?&#10;http://www.elodina.net&#10;\" /></i></section><section data-index=\"41\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-41-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-41-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-41-1024.jpg?cb=1459351563\" alt=\"SMACK Stack 1.1\" /></i></section><section data-index=\"42\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-42-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-42-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-42-1024.jpg?cb=1459351563\" alt=\"SMACK Stack 1.1\" /></i></section><section data-index=\"43\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-43-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-43-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-43-1024.jpg?cb=1459351563\" alt=\"SMACK Stack 1.1\" /></i></section><section data-index=\"44\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-44-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-44-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-44-1024.jpg?cb=1459351563\" alt=\"SMACK Stack 1.1\" /></i></section><section data-index=\"45\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-45-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-45-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-45-1024.jpg?cb=1459351563\" alt=\"SMACK Stack 1.1\" /></i></section><section data-index=\"46\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-46-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-46-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-46-1024.jpg?cb=1459351563\" alt=\"SMACK Stack 1.1\" /></i></section><section data-index=\"47\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-47-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-47-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-47-1024.jpg?cb=1459351563\" alt=\"SMACK Stack 1.1\" /></i></section><section data-index=\"48\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-48-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-48-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-48-1024.jpg?cb=1459351563\" alt=\"SMACK Stack 1.1\" /></i></section><section data-index=\"49\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-49-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-49-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-49-1024.jpg?cb=1459351563\" alt=\"SMACK Stack 1.1\" /></i></section><section data-index=\"50\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-50-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-50-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-50-1024.jpg?cb=1459351563\" alt=\"SMACK Stack 1.1\" /></i></section><section data-index=\"51\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-51-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-51-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-51-1024.jpg?cb=1459351563\" alt=\"SMACK Stack 1.1\" /></i></section><section data-index=\"52\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-52-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-52-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-52-1024.jpg?cb=1459351563\" alt=\"SMACK Stack 1.1\" /></i></section><section data-index=\"53\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-53-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-53-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-53-1024.jpg?cb=1459351563\" alt=\"SMACK Stack 1.1\" /></i></section><section data-index=\"54\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-54-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-54-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-54-1024.jpg?cb=1459351563\" alt=\"SMACK Stack 1.1\" /></i></section><section data-index=\"55\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-55-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-55-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-55-1024.jpg?cb=1459351563\" alt=\"SMACK Stack 1.1\" /></i></section><section data-index=\"56\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-56-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-56-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-56-1024.jpg?cb=1459351563\" alt=\"SMACK Stack 1.1\" /></i></section><section data-index=\"57\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-57-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-57-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-57-1024.jpg?cb=1459351563\" alt=\"SMACK Stack 1.1\" /></i></section><section data-index=\"58\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-58-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-58-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-58-1024.jpg?cb=1459351563\" alt=\"SMACK Stack 1.1\" /></i></section><section data-index=\"59\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-59-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-59-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-59-1024.jpg?cb=1459351563\" alt=\"SMACK Stack 1.1\" /></i></section><section data-index=\"60\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-60-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-60-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-60-1024.jpg?cb=1459351563\" alt=\"SMACK Stack 1.1\" /></i></section><section data-index=\"61\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/charmalloc/smack-stack-11\" data-small=\"https://image.slidesharecdn.com/smackstack1-160330152334/85/smack-stack-11-61-320.jpg?cb=1459351563\" data-normal=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-61-638.jpg?cb=1459351563\" data-full=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-61-1024.jpg?cb=1459351563\" alt=\"SMACK Stack 1.1\" /></i></section><div class=\"j-next-container next-container\"><div class=\"content-container\"><div class=\"next-slideshow-wrapper\"><div class=\"j-next-slideshow next-slideshow\"><p>Upcoming SlideShare</p></div><p>Loading in …5</p><p>×</p></div></div></div></div></div></div></div></div></div><div class=\"slideshow-info-container\" itemscope=\"itemscope\" itemtype=\"https://schema.org/MediaObject\"><div class=\"slideshow-tabs-container show-for-medium-up\"><ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    0 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\">\n                \n                  3 Likes\n                \n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\">\n                Statistics\n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\">\n                Notes\n              </i></a>\n            </li>\n          </ul><div class=\"tabs-content\"><div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\"><ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"itemscope\"><li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"HenrikSkriver\" rel=\"nofollow\" href=\"https://www.slideshare.net/HenrikSkriver?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Henrik Skriver Rasmussen\n                            \n                              \n                                , \n                                Software Consultant\n                              \n                              \n                                 at \n                                Teletronics\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"DhyaneshwaranMuralidharan\" rel=\"nofollow\" href=\"https://www.slideshare.net/DhyaneshwaranMuralidharan?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Dhyaneshwaran Muralidharan\n                            \n                              \n                                , \n                                Senior Software Engineer at Yottaa\n                              \n                              \n                                 at \n                                Yottaa\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"seohoseok14\" rel=\"nofollow\" href=\"https://www.slideshare.net/seohoseok14?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Seo Hoseok\n                            \n                              \n                                \n                                \n                              \n                              \n                                \n                                \n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n              </ul></div><div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No Downloads</p></div><div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No notes for slide</p></div></div></div><div class=\"notranslate transcript add-padding-right j-transcript\"><ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    SMACK Stack 1.1\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-2-638.jpg?cb=1459351563\" title=\"Elodina is a big data as a service platform built on top&#10;of...\" target=\"_blank\">\n        2.\n      </a>\n    Elodina is a big data as a service platform built on top\nof open source software.\nThe Elodina platform solves today’s data\nanalytics needs by providing the tools and\nsupport necessary to utilize open source\ntechnologies.\nhttp://www.elodina.net/\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-3-638.jpg?cb=1459351563\" title=\"Whats SMACK Stack?&#10;SMACK stack 1.0 has been traditionally S...\" target=\"_blank\">\n        3.\n      </a>\n    Whats SMACK Stack?\nSMACK stack 1.0 has been traditionally Spark, Mesos, Akka, Cassandra and\nKafka lots https://dzone.com/articles/smack-stack-guide and lots lots more https:\n//www.google.com/webhp?q=smack%20stack\nNow we are going to introduce SMACK Stack 1.1 and talk more about dynamic\ncompute, micro services, orchestration, micro segmentation all part of what you\ncan do now with Streaming, Mesos, Analytics, Cassandra and Kafka\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-4-638.jpg?cb=1459351563\" title=\"The free lunch is over!&#10;http://www.gotw.ca/publications/con...\" target=\"_blank\">\n        4.\n      </a>\n    The free lunch is over!\nhttp://www.gotw.ca/publications/concurrency-ddj.htm\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-5-638.jpg?cb=1459351563\" title=\"Many industries still don’t get it&#10;XML is everywhere but we...\" target=\"_blank\">\n        5.\n      </a>\n    Many industries still don’t get it\nXML is everywhere but we have alternatives!\nWe can support XML interface but don’t have to take on the burden of the extra\ndata. You can save A LOT of overheard just by having a pre-processing step\ntaking the XML, turning it into Avro and processing and storing that.\nIt works https://github.com/elodina/xml-avro\nYou can even process the response in Avro but return the result in XML, more on\nthat later though!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-6-638.jpg?cb=1459351563\" title=\"You need to be running Mesos. Lots of options here!&#10;What is...\" target=\"_blank\">\n        6.\n      </a>\n    You need to be running Mesos. Lots of options here!\nWhat is most important is that you abstract your “Provider” from your “Grid”.\nWhat is “The Grid”?\nIt is your PaaS layer you deploy too that runs your software. (aka your new\nawesome super computer)\nThe grid is your mesos cluster. You are likely going to have more than one so plan\naccordingly. Think of it as immutable infrastructure, the computer does.\nStep 1\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-7-638.jpg?cb=1459351563\" title=\"“Provider” of compute resources&#10;\" target=\"_blank\">\n        7.\n      </a>\n    “Provider” of compute resources\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-8-638.jpg?cb=1459351563\" title=\"The Grid … 2.0 ...&#10;https://github.com/elodina/sawfly/blob/m...\" target=\"_blank\">\n        8.\n      </a>\n    The Grid … 2.0 ...\nhttps://github.com/elodina/sawfly/blob/master/cloud-deploy-grid.md\nProgram against your datacenter like it’s a single pool of resources Apache Mesos abstracts CPU,\nmemory, storage, and other compute resources away from machines (physical or virtual), enabling\nfault-tolerant and elastic distributed systems to easily be built and run effectively. Mesosphere’s Data\nCenter Operating System (DCOS) is an operating system that spans all of the machines in a datacenter\nor cloud and treats them as a single computer, providing a highly elastic and highly scalable way of\ndeploying applications, services, and big data infrastructure on shared resources. DCOS is based on\nApache Mesos and includes a distributed systems kernel with enterprise-grade security.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-9-638.jpg?cb=1459351563\" title=\"Data Center Optimization!&#10;\" target=\"_blank\">\n        9.\n      </a>\n    Data Center Optimization!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-10-638.jpg?cb=1459351563\" title=\"But there is more!&#10;● Provisioning&#10;● Micro Segmentation&#10;● Or...\" target=\"_blank\">\n        10.\n      </a>\n    But there is more!\n● Provisioning\n● Micro Segmentation\n● Orchestration\n● Configuration Management\n● Service Discovery\n● Deployment Isolation and Identification\n● Telemetry, Tracing, Ops Stuff, Etc\n● Oh My!\nIt boils back down into stacks! https://github.com/elodina/stack-deploy and how\nyou are working with your schedulers in your cluster ultimatlly.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-11-638.jpg?cb=1459351563\" title=\"Stack Deploy to the rescue!&#10;\" target=\"_blank\">\n        11.\n      </a>\n    Stack Deploy to the rescue!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-12-638.jpg?cb=1459351563\" title=\"In the Grid you need Schedulers!&#10;● Kafka – Producer/Consume...\" target=\"_blank\">\n        12.\n      </a>\n    In the Grid you need Schedulers!\n● Kafka – Producer/Consumer-based message queue management\n● Exhibitor – Supervisor for distributed persistence (like ZooKeeper)\n● Cassandra/DSE – HA, scalable, distributed NoSQL data storage\n● Storm – Topology-based Real-time distributed data streaming\n● Monarch – Distributed Remote Procedure Calls, Kafka REST interface and schema repository\n● Zipkin – Configure, launch and manage Zipkin distributed trace on Mesos\n● HDFS – Configure, launch and manage HDFS on Mesos (coming soon)\n● Stockpile – Consumer to “stock pile” data into persistent storage (mesos scheduler only for c* now)\n● MirrorMaker – Consumer to make a mirror copy of data to destination\n● StatsD – Producer to pump StatsD on Mesos into Kafka for consumption, preserves layers\n● SysLog – Producer to pump Syslog on Mesos into Kafka for consumption, preserves layers\nhttps://github.com/elodina/\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-13-638.jpg?cb=1459351563\" title=\"Virtual Telemetry “Data Center” In the Grid&#10;ZipkinQATeamBui...\" target=\"_blank\">\n        13.\n      </a>\n    Virtual Telemetry “Data Center” In the Grid\nZipkinQATeamBuild92\n● 1x Exhibitor-Mesos\n● 1x Exhibitor\n● 1x DSE-Mesos\n● 1x Cassandra node\n● 1x Kafka-Mesos\n● 1x Kafka 0.8 broker\n● 1x Zipkin-Mesos\n● 1x Zipkin Collector\n● 1x Zipkin Query\n● 1x Zipkin Web\n“cluster”\n“zone”\n“Stack” - defaultSimpleZipkinFull\n“data center”\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-14-638.jpg?cb=1459351563\" title=\"Stack Deploy In Action&#10;./stack-deploy addlayer --file stack...\" target=\"_blank\">\n        14.\n      </a>\n    Stack Deploy In Action\n./stack-deploy addlayer --file stacks/cassandra_dc.stack --level datacenter\n./stack-deploy addlayer --file stacks/cassandra_cluster.stack --level cluster --parent cassandra_dc\n./stack-deploy addlayer --file stacks/cassandra_zone1.stack --level zone --parent cassandra_cluster\n./stack-deploy addlayer --file stacks/cassandra_zone2.stack --level zone --parent cassandra_cluster\n./stack-deploy add --file stacks/cassandra.stack\n./stack-deploy run cassandra --zone cassandra_zone1\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-15-638.jpg?cb=1459351563\" title=\"Full Stack Deployments&#10;\" target=\"_blank\">\n        15.\n      </a>\n    Full Stack Deployments\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-16-638.jpg?cb=1459351563\" title=\"Cassandra&#10;\" target=\"_blank\">\n        16.\n      </a>\n    Cassandra\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-17-638.jpg?cb=1459351563\" title=\"Cassandra Multi DC&#10;\" target=\"_blank\">\n        17.\n      </a>\n    Cassandra Multi DC\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-18-638.jpg?cb=1459351563\" title=\"Casandra https://github.com/elodina/datastax-enterprise-mes...\" target=\"_blank\">\n        18.\n      </a>\n    Casandra https://github.com/elodina/datastax-enterprise-mesos\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-19-638.jpg?cb=1459351563\" title=\"Start your nodes!&#10;\" target=\"_blank\">\n        19.\n      </a>\n    Start your nodes!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-20-638.jpg?cb=1459351563\" title=\"Apache Kafka&#10;• Apache Kafka&#10;o http://kafka.apache.org&#10;• Apa...\" target=\"_blank\">\n        20.\n      </a>\n    Apache Kafka\n• Apache Kafka\no http://kafka.apache.org\n• Apache Kafka Source Code\no https://github.com/apache/kafka\n• Documentation\no http://kafka.apache.org/documentation.html\n• Wiki\no https://cwiki.apache.org/confluence/display/KAFKA/Index\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-21-638.jpg?cb=1459351563\" title=\"It often starts with just one data pipeline&#10;\" target=\"_blank\">\n        21.\n      </a>\n    It often starts with just one data pipeline\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-22-638.jpg?cb=1459351563\" title=\"Reuse of data pipelines for new producers&#10;\" target=\"_blank\">\n        22.\n      </a>\n    Reuse of data pipelines for new producers\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-23-638.jpg?cb=1459351563\" title=\"Reuse of existing providers for new consumers&#10;\" target=\"_blank\">\n        23.\n      </a>\n    Reuse of existing providers for new consumers\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-24-638.jpg?cb=1459351563\" title=\"Eventually the solution becomes the problem&#10;\" target=\"_blank\">\n        24.\n      </a>\n    Eventually the solution becomes the problem\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-25-638.jpg?cb=1459351563\" title=\"Kafka decouples data-pipelines&#10;\" target=\"_blank\">\n        25.\n      </a>\n    Kafka decouples data-pipelines\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-26-638.jpg?cb=1459351563\" title=\"Topics &amp; Partitions&#10;\" target=\"_blank\">\n        26.\n      </a>\n    Topics &amp; Partitions\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-27-638.jpg?cb=1459351563\" title=\"A high-throughput distributed messaging system&#10;rethought as...\" target=\"_blank\">\n        27.\n      </a>\n    A high-throughput distributed messaging system\nrethought as a distributed commit log.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-28-638.jpg?cb=1459351563\" title=\"Intra Cluster Replication&#10;\" target=\"_blank\">\n        28.\n      </a>\n    Intra Cluster Replication\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-29-638.jpg?cb=1459351563\" title=\"Mesos Kafka http://github.com/mesos/kafka&#10;\" target=\"_blank\">\n        29.\n      </a>\n    Mesos Kafka http://github.com/mesos/kafka\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-30-638.jpg?cb=1459351563\" title=\"Streaming &amp; Analytics&#10;● The landscape of streaming is about...\" target=\"_blank\">\n        30.\n      </a>\n    Streaming &amp; Analytics\n● The landscape of streaming is about to get more fragmented and harder to\nnavigate. This is not all bad news and it is not much different than where we\nwere with NoSQL 6 years ago or so.\n● Different systems are getting really (really (really)) good at different things.\n○ Dag based systems\n○ Event based systems\n○ Query &amp; Execution Engines\n○ Streaming Engines\n○ Etc!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-31-638.jpg?cb=1459351563\" title=\"GearPump&#10;\" target=\"_blank\">\n        31.\n      </a>\n    GearPump\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-32-638.jpg?cb=1459351563\" title=\"Airflow&#10;\" target=\"_blank\">\n        32.\n      </a>\n    Airflow\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-33-638.jpg?cb=1459351563\" title=\"Spring Cloud Data Flow&#10;\" target=\"_blank\">\n        33.\n      </a>\n    Spring Cloud Data Flow\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-34-638.jpg?cb=1459351563\" title=\"Storm (and Storm Topology based systems)&#10;\" target=\"_blank\">\n        34.\n      </a>\n    Storm (and Storm Topology based systems)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-35-638.jpg?cb=1459351563\" title=\"Storm Nimbus&#10;{&#10;&quot;id&quot;: &quot;storm-nimbus&quot;,&#10;&quot;cmd&quot;: &quot;cp storm.yaml ...\" target=\"_blank\">\n        35.\n      </a>\n    Storm Nimbus\n{\n\"id\": \"storm-nimbus\",\n\"cmd\": \"cp storm.yaml storm-mesos-0.9.6/conf &amp;&amp; cd storm-mesos-0.9.6 &amp;&amp; ./bin/storm-mesos nimbus -c mesos.master.url=zk:\n//zookeeper.service:2181/mesos -c storm.zookeeper.servers=\"[\"zookeeper.service\"]\" -c nimbus.thrift.port=$PORT0 -c topology.\nmesos.worker.cpu=0.5 -c topology.mesos.worker.mem.mb=615 -c worker.childopts=-Xmx512m -c topology.mesos.executor.cpu=0.1 -c\ntopology.mesos.executor.mem.mb=160 -c supervisor.childopts=-Xmx128m -c mesos.executor.uri=http://repo.elodina.s3.amazonaws.\ncom/storm-mesos-0.9.6.tgz -c storm.log.dir=$(pwd)/logs\",\n\"cpus\": 1.0,\n\"mem\": 1024,\n\"ports\": [31056],\n\"requirePorts\": true,\n\"instances\": 1,\n\"uris\": [\n\"http://repo.elodina.s3.amazonaws.com/storm-mesos-0.9.6.tgz\",\n\"http://repo.elodina.s3.amazonaws.com/storm.yaml\"\n]\n}\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-36-638.jpg?cb=1459351563\" title=\"Storm UI&#10;{&#10;&quot;id&quot;: &quot;storm-ui&quot;,&#10;&quot;cmd&quot;: &quot;cp storm.yaml storm-me...\" target=\"_blank\">\n        36.\n      </a>\n    Storm UI\n{\n\"id\": \"storm-ui\",\n\"cmd\": \"cp storm.yaml storm-mesos-0.9.6/conf &amp;&amp; cd storm-mesos-0.9.6 &amp;&amp; ./bin/storm ui -c ui.port=$PORT0 -c nimbus.thrift.port=31056 -c nimbus.\nhost=storm-nimbus.service -c storm.log.dir=$(pwd)/logs\",\n\"cpus\": 0.2,\n\"mem\": 512,\n\"ports\": [31067],\n\"requirePorts\": true,\n\"instances\": 1,\n\"uris\": [\n\"http://repo.elodina.s3.amazonaws.com/storm-mesos-0.9.6.tgz\",\n\"http://repo.elodina.s3.amazonaws.com/storm.yaml\"\n],\n\"healthChecks\": [\n{\n\"protocol\": \"HTTP\",\n\"portIndex\": 0,\n\"path\": \"/\",\n\"gracePeriodSeconds\": 120,\n\"intervalSeconds\": 20,\n\"maxConsecutiveFailures\": 3\n}\n]\n}\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-37-638.jpg?cb=1459351563\" title=\"Storm Kafka - new spouts &amp; bolts for Kafka 8, 9, ...&#10;\" target=\"_blank\">\n        37.\n      </a>\n    Storm Kafka - new spouts &amp; bolts for Kafka 8, 9, ...\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-38-638.jpg?cb=1459351563\" title=\"Apache Kafka Streams&#10;\" target=\"_blank\">\n        38.\n      </a>\n    Apache Kafka Streams\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-39-638.jpg?cb=1459351563\" title=\"Go Kafka Client - Fan Out Processing&#10;https://github.com/elo...\" target=\"_blank\">\n        39.\n      </a>\n    Go Kafka Client - Fan Out Processing\nhttps://github.com/elodina/go-kafka-client-mesos\n● Dynamic Kafka Log workers\n● Blue/Green Deploy Support\n● Fan Out Processing\n● Auditable\n● Batches\n● Scalable/Auto-Scalable\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstack1-160330152334/95/smack-stack-11-40-638.jpg?cb=1459351563\" title=\"Questions?&#10;http://www.elodina.net&#10;\" target=\"_blank\">\n        40.\n      </a>\n    Questions?\nhttp://www.elodina.net\n \n  </li>\n              </ol></div></div></div><aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\"><dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\"><ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"60229924\" title=\"Teaching Techniques: Creating Effective Learning Assessments\" href=\"https://www.linkedin.com/learning/teaching-techniques-creating-effective-learning-assessments?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Teaching Techniques: Creating Effective Learning Assessments\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Teaching Techniques: Creating Effective Learning Assessments\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=Q4uk6hRyMd1fgp%2BiwEZcGbrIUeQ%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-kWCWs8tKfZXTpf8_YZLSiolwTey0DlAcxfOerRTLnFY69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Teaching Techniques: Creating Effective Learning Assessments</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"60229924\" title=\"Teaching Techniques: Creating Multimedia Learning\" href=\"https://www.linkedin.com/learning/teaching-techniques-creating-multimedia-learning?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Teaching Techniques: Creating Multimedia Learning\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Teaching Techniques: Creating Multimedia Learning\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=5yNrggG44Flut51QOcRX28HD9g0%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-iXSWq-dafY3HpecTcZLSioVsTfy8EkQ07eO6vRzjoG469LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Teaching Techniques: Creating Multimedia Learning</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"60229924\" title=\"Creative Inspirations: Duarte Design, Presentation Design Studio\" href=\"https://www.linkedin.com/learning/creative-inspirations-duarte-design-presentation-design-studio?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Creative Inspirations: Duarte Design, Presentation Design Studio\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Creative Inspirations: Duarte Design, Presentation Design Studio\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=gN0XHJkdhuJ3Fk75H6mdK0VF9ag%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-kWiGr-MqFYXPoe9ref_qougdWLw\" /></div>\n    <div class=\"lynda-content\"><p>Creative Inspirations: Duarte Design, Presentation Design Studio</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"76349002\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Data Analytics Using Container Persistence Through SMACK - Manny Rodriguez-Perez and Kendrick Coleman - Dell EMC World 2017\" href=\"https://www.slideshare.net/codedellemc/data-analytics-using-container-persistence-through-smack-manny-rodriguezperez-and-kendrick-coleman-dell-emc-world-2017\">\n    \n    <div class=\"related-content\"><p>Data Analytics Using Container Persistence Through SMACK - Manny Rodriguez-Pe...</p><p>{code} by Dell EMC</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"81441315\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Smack Stack and Beyond—Building Fast Data Pipelines with Jorg Schad\" href=\"https://www.slideshare.net/SparkSummit/smack-stack-and-beyondbuilding-fast-data-pipelines-with-jorg-schad\">\n    \n    <div class=\"related-content\"><p>Smack Stack and Beyond—Building Fast Data Pipelines with Jorg Schad</p><p>Spark Summit</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"60627776\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Data processing platforms with SMACK:  Spark and Mesos internals\" href=\"https://www.slideshare.net/akirillov/decomposing-smack-stack-spark-and-mesos-internals\">\n    \n    <div class=\"related-content\"><p>Data processing platforms with SMACK:  Spark and Mesos internals</p><p>Anton Kirillov</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"57135923\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Laying down the smack on your data pipelines\" href=\"https://www.slideshare.net/patrickmcfadin/laying-down-the-smack-on-your-data-pipelines\">\n    \n    <div class=\"related-content\"><p>Laying down the smack on your data pipelines</p><p>Patrick McFadin</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"52661852\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Data processing platforms architectures with Spark, Mesos, Akka, Cassandra and Kafka\" href=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\">\n    \n    <div class=\"related-content\"><p>Data processing platforms architectures with Spark, Mesos, Akka, Cassandra an...</p><p>Anton Kirillov</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"81679511\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"FPGA-Based Acceleration Architecture for Spark SQL Qi Xie and Quanfu Wang\" href=\"https://www.slideshare.net/SparkSummit/fpgabased-acceleration-architecture-for-spark-sql-qi-xie-and-quanfu-wang-81679511\">\n    \n    <div class=\"related-content\"><p>FPGA-Based Acceleration Architecture for Spark SQL Qi Xie and Quanfu Wang</p><p>Spark Summit</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"43475359\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Streaming Big Data with Spark, Kafka, Cassandra, Akka &amp; Scala (from webinar)\" href=\"https://www.slideshare.net/helenaedelson/streaming-bigdata-helenawebinarv3\">\n    \n    <div class=\"related-content\"><p>Streaming Big Data with Spark, Kafka, Cassandra, Akka &amp; Scala (from webinar)</p><p>Helena Edelson</p></div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div></div><footer>\n          <div class=\"row\"><div class=\"columns\"><ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div></div>\n          \n          <div class=\"row\"><div class=\"columns\"><p class=\"copyright text-center\">LinkedIn Corporation © 2018</p></div></div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div>\n    <div class=\"row\"><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></div>\n        \n    </div>\n    \n    \n      <noscript>\n    </noscript>",
        "created_at": "2018-01-15T16:27:10+0000",
        "updated_at": "2018-03-15T14:21:35+0000",
        "published_at": null,
        "published_by": [
          "Joe Stein"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 7,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/smackstack1-160330152334-thumbnail-4.jpg?cb=1459351563",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9065"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 907,
            "label": "mesos",
            "slug": "mesos"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 963,
            "label": "akka",
            "slug": "akka"
          },
          {
            "id": 1041,
            "label": "smack",
            "slug": "smack"
          }
        ],
        "is_public": false,
        "id": 9064,
        "uid": null,
        "title": "SMACK (Spark, Mesos, Akka, Cassandra and Kafka) & Fast Data – Cuelogic Blog",
        "url": "http://www.cuelogic.com/blog/smack-spark-mesos-akka-cassandra-and-kafka-fast-data/",
        "content": "<p>&#13;\n\tby <a href=\"http://www.cuelogic.com/blog/author/rajdeep-limbucuelogic-com/\">Rajdeep Limbu</a>,\t<a href=\"http://www.cuelogic.com/blog/smack-spark-mesos-akka-cassandra-and-kafka-fast-data/\">June 13, 2017</a>  &#13;</p><p>&#13;\n\tCategory: <a href=\"http://www.cuelogic.com/blog/category/analytics-2/\" rel=\"category tag\">Analytics</a>, <a href=\"http://www.cuelogic.com/blog/category/big-data-2/\" rel=\"category tag\">Big Data</a>, <a href=\"http://www.cuelogic.com/blog/category/big-data-analytics/\" rel=\"category tag\">Big Data Analytics</a>, <a href=\"http://www.cuelogic.com/blog/category/fast-data/\" rel=\"category tag\">Fast Data</a>\t\t\t| Tag: <a href=\"http://www.cuelogic.com/blog/tag/akka/\" rel=\"tag\">Akka</a>, <a href=\"http://www.cuelogic.com/blog/tag/big-data/\" rel=\"tag\">big data</a>, <a href=\"http://www.cuelogic.com/blog/tag/cassandra/\" rel=\"tag\">Cassandra</a>, <a href=\"http://www.cuelogic.com/blog/tag/fast-data/\" rel=\"tag\">fast data</a>, <a href=\"http://www.cuelogic.com/blog/tag/hadoop/\" rel=\"tag\">hadoop</a>, <a href=\"http://www.cuelogic.com/blog/tag/kafka/\" rel=\"tag\">Kafka</a>, <a href=\"http://www.cuelogic.com/blog/tag/lambda-architecture/\" rel=\"tag\">Lambda Architecture</a>, <a href=\"http://www.cuelogic.com/blog/tag/mesos/\" rel=\"tag\">Mesos</a></p><p>As enterprises strive to realize and identify the value in Big Data, many now seek more agile and capable analytic systems. Some of the key factors supporting this include improving customer retention, influence product development and quality and increasing operational efficiencies among others. Organizations are looking to enhance their analytics workloads to drive several real-time cases such as recommendation engines, fraud detection and advertising analytics. In this whitepaper, I will try to draw your attention toward the applications of SMACK and how it has emerged as a crucial open source technology to handle high volume and velocity of “Big Data”.</p><p><strong><em> </em></strong></p><p><strong><em>Big Data is not only Big it’s Fast also</em></strong></p><p>The way that big data gets big is through a continuous and constant steam of incoming data. In high-volume environments, that information/data arrives at incredible rates, yet still needs to be stored and analyzed.</p><p>Less than a dozen years ago, it was nearly impossible to analyze and process those large chunks of data using commodity hardware. Today, Hadoop clusters built from thousands of nodes are almost everywhere to analyze petabytes of historical data. Apache Hadoop is a software framework that is being adopted by many enterprises as a cost-effective analytics platform for Big Data analytics. Hadoop is an ecosystem of several services rather than a single product, and is designed for storing and processing petabytes of data in a linear scale-out model. Each service in the Hadoop ecosystem may use different technologies in the processing pipeline to ingest, store, process and visualize data.</p><p><strong><em> </em></strong></p><p><strong><em>What’s next After Big Data?</em></strong></p><p><strong><em>What Hadoop Can’t Do</em></strong></p><p>The big data movement was pretty much driven by the demand for scale in volume, variety and velocity of data. This is often supported by data that is generated at high speeds, such as financial ticker data, click-stream data, sensor data or log aggregation. Certainly, Apache Hadoop is one of the best technologies to analyze all those large amount of data. However, the technology has a limitation as it cannot be used when the data is moving or in motion. Some of the areas where the use of Hadoop is not recommended are transactional data. Transactional data, by its very nature, is highly complex, as a transaction on an ecommerce site can generate many steps that all have to be implemented quickly. That scenario is not at all ideal for Hadoop.</p><p>Nor would it be optimal for structured data sets that require very minimal latency, like when a Web site is served up by a MySQL database in a typical LAMP stack. That’s a speed requirement that Hadoop would poorly serve.</p><p><strong><em>Fast Data</em></strong><strong> – Analyze data when they are in motion or in real-time</strong></p><p>SMACK stands for Spark, Mesos, Akka, Cassandra and Kafka – a combination that’s being adopted for ‘fast data’.</p><p>The SMACK stack (Spark, Mesos, Akka, Cassandra and Kafka) is known to be as the ideal platform for constructing “fast data” applications. The term fast data underlines how big data applications and architectures are evolving to be stream oriented, so that the data is extracted as quickly as possible from incoming data, while still supporting traditional data scenarios such as batch processing, data warehousing, and interactive exploration.</p><p><strong> </strong></p><p><strong>The SMACK Stack</strong></p><p>The SMACK Stack architecture pattern consists of following technologies:</p><ul class=\"list-icon\"><li><strong>Apache Spark (Processing Engine)</strong> – Fast and general engine for distributed, large-scale data processing. Often, used to implement ETL, queries, aggregations and applications of machine learning etc. Spark is well-suited to machine learning algorithms, and exposes its API in Scala, Python, R and Java. The approach of Spark is to provide a unified interface that can be used to mix SQL queries, machine learning, graph analysis, and streaming (micro-batched) processing.</li>\n<li><strong>Apache Mesos (The Container) – </strong>A flexible cluster resource management system that provides efficient resource isolation and sharing across distributed applications. Mesos uses a two-level scheduling mechanism where resource offers are made to frameworks (applications that run on top of Mesos). The Mesos master node decides how many resources to offer each framework, while each framework determines the resources it accepts and what application to execute on those resources. This method of resource allocation allows near-optimal data locality when sharing a cluster of nodes amongst diverse frameworks.</li>\n<li><strong>Akka (The Model) – </strong>Microservice development with high scalability, durability, and low-latency processing. A toolkit and runtime for building highly concurrent, distributed, and resilient message-driven applications on the JVM (Java Virtual Machine). The actor model in computer science is a mathematical model of concurrent computation that treats “actors” as the universal primitives of concurrent computation: in response to a message that it receives, an actor can make local decisions, create more actors, send more messages, and determine how to respond to the next message received.</li>\n<li><strong>Apache Cassandra (The Storage)</strong> – Scalable, resilient, distributed database designed for persistent, durable storage across multiple data centers. Most environments will also use a distributed file system like HDFS or S3.</li>\n<li><strong>Apache Kafka (The Broker) – </strong>A high-throughput, low-latency distributed messaging system designed for handling real-time data feeds. In Kafka, data streams are partitioned and spread over a cluster of machines to allow data streams larger than the capability of any single machine and to allow clusters of coordinated consumers.</li>\n</ul><p>The SMACK stack helps in simplifying the processing of both real-time and batch workloads as single data stream in real or near real time.</p><p><a href=\"http://www.cuelogic.com/blog/wp-content/uploads/2017/06/SMACK-Architecture.jpg\"><img class=\"alignnone size-full wp-image-4393\" src=\"http://www.cuelogic.com/blog/wp-content/uploads/2017/06/SMACK-Architecture.jpg\" alt=\"SMACK-Architecture\" width=\"856\" height=\"460\" srcset=\"http://www.cuelogic.com/blog/wp-content/uploads/2017/06/SMACK-Architecture.jpg 856w, http://www.cuelogic.com/blog/wp-content/uploads/2017/06/SMACK-Architecture-300x161.jpg 300w, http://www.cuelogic.com/blog/wp-content/uploads/2017/06/SMACK-Architecture-768x413.jpg 768w, http://www.cuelogic.com/blog/wp-content/uploads/2017/06/SMACK-Architecture-240x129.jpg 240w\" /></a></p><p><strong>SMACK stack: How to put the pieces together</strong></p><p><strong>Capturing value in fast data</strong></p><p>The best way to capture the value of incoming data is to react to it the instant it arrives. Processing of incoming data in batches always takes lot of time which ultimately takes away the value of that data.</p><p>Basically, to handle or process data which is arriving at thousands to millions of events per second, two technologies are required i.e. a streaming system capable of delivering events as fast as they come in and a data store capable of processing each item as fast as it arrives.</p><p><strong> </strong></p><p><strong>Delivering the fast data</strong><strong> </strong></p><p>In order to deliver fast data, two popular streaming systems i.e. Apache Storm and Apache Kafka have evolved in the market over the past few years. Apache Storm (developed by Twitter) reliably processes unbounded streams of data at rates of millions of messages per second. On the other hand, Kafka (developed by LinkedIn), is a high-throughput distributed message queue system. Both streaming systems address the need of processing fast data. Kafka, however, stands apart.</p><p><strong>Processing the fast data</strong><strong> </strong></p><p>It is noteworthy that Hadoop MapReduce has very well solved the batch processing needs of customers across the industries. However, the introduction of more flexible developed big data tools for fast data processing paved the demand of big data darling Apache Spark. Owing to its fast data processing power, the technology is setting fire across the entire big data world. The technology has actually taken over Hadoop in terms of fast data processing of interactive data mining algorithms and iterative machine learning algorithms.</p><p><strong>Lambda Architecture </strong>is data processing architecture and has both stream and batch processing capabilities. However, most of the lambdas solutions cannot serve the two needs at the same time:</p><p>Handles a massive data stream in real time.</p><p>Handles different and multiple data models from multiple data source.</p><p>To easily meet both of these needs, we have Apache Spark which is mainly responsible for real-time analysis of both recent and historical information. The analysis results are often persisted in Apache Cassandra which helps the user to extract the real-time information anytime in case of failure. Spark is increasingly adopted as an alternate processing framework to MapReduce, due to its ability to speed up batch, interactive and streaming analytics. Spark enables new analytics use cases like machine learning and graph analysis with its rich and easy to use programming libraries.</p>",
        "created_at": "2018-01-15T16:27:01+0000",
        "updated_at": "2018-05-13T16:05:11+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 6,
        "domain_name": "www.cuelogic.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9064"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 907,
            "label": "mesos",
            "slug": "mesos"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 931,
            "label": "data.engineering",
            "slug": "data-engineering"
          },
          {
            "id": 963,
            "label": "akka",
            "slug": "akka"
          },
          {
            "id": 1041,
            "label": "smack",
            "slug": "smack"
          }
        ],
        "is_public": false,
        "id": 9063,
        "uid": null,
        "title": "Stratio/sparta",
        "url": "https://github.com/Stratio/sparta",
        "content": "<p>After around two years of development, we have decided to discontinue this project due to a major refactor in its structure and in a near future we will launch Sparta 2.0.</p><p>We would like to thank all the open source community for their contribution.\nNeedless to say that you can continue using this repository as a basis for your developments as it contains the latest stable version as of today and minor issues will be attended.</p><p>If you are interested in the new Sparta 2.0 with pipelines and workflows, please contact with us in the email <a href=\"mailto:sparta@stratio.com\">sparta@stratio.com</a></p><p>At Stratio, we have implemented several real-time analytics projects based on Apache Spark, Kafka, Flume, Cassandra, ElasticSearch or MongoDB.\nThese technologies were always a perfect fit, but soon we found ourselves writing the same pieces of integration code over and over again.\nStratio Sparta is the easiest way to make use of the Apache Spark Streaming technology and all its ecosystem.\nChoose your input, operations and outputs, and start extracting insights out of your data in real-time.</p><p><a href=\"https://github.com/Stratio/sparta/blob/master/images/StrataKibana.jpg\" target=\"_blank\"><img src=\"https://github.com/Stratio/sparta/raw/master/images/StrataKibana.jpg\" width=\"600\" height=\"300\" alt=\"Strata Twitter Analytics with Kibana\" /></a></p><ul><li>Pure Spark</li>\n<li>No need of coding, only declarative analytical workflows</li>\n<li>Data continuously streamed in &amp; processed in near real-time</li>\n<li>Ready to use out-of-the-box</li>\n<li>Plug &amp; play: flexible workflows (inputs, outputs, transformations, etc…)</li>\n<li>High performance and Fault Tolerance</li>\n<li>Scalable and High Availability</li>\n<li>Big Data OLAP on real-time to small data</li>\n<li>ETLs</li>\n<li>Triggers over streaming data</li>\n<li>Spark SQL language with streaming and batch data</li>\n<li>Kerberos and CAS compatible</li>\n</ul><p><a href=\"https://github.com/Stratio/sparta/blob/master/images/mainFeatures.jpg\" target=\"_blank\"><img src=\"https://github.com/Stratio/sparta/raw/master/images/mainFeatures.jpg\" alt=\"Main Features\" /></a></p><p>Send one workflow as a JSON to Sparta API and execute in one Spark Cluster your own real-time plugins\n<a href=\"https://github.com/Stratio/sparta/blob/master/images/architecture.jpg\" target=\"_blank\"><img src=\"https://github.com/Stratio/sparta/raw/master/images/architecture.jpg\" alt=\"Architecture\" /></a></p><h2>Sparta as a Job Manager</h2><p>Send more than one Streaming Job in the Spark Cluster and manage them with a simple UI</p><p><a href=\"https://github.com/Stratio/sparta/blob/master/images/jobManager.jpg\" target=\"_blank\"><img src=\"https://github.com/Stratio/sparta/raw/master/images/jobManager.jpg\" alt=\"Job Manager\" /></a></p><p>Run workflows over Mesos, Yarn or SparkStandAlone</p><p><a href=\"https://github.com/Stratio/sparta/blob/master/images/architectureJobs.jpg\" target=\"_blank\"><img src=\"https://github.com/Stratio/sparta/raw/master/images/architectureJobs.jpg\" alt=\"Job Manager Architecture\" /></a></p><h2>Sparta as a SDK</h2><p>Modular components extensible with simple SDK</p><ul><li>You can extend several points of the platform to fulfill your needs, such as adding new inputs, outputs, operators, transformations.</li>\n<li>Add new functions to Kite SDK in order to extend the data cleaning, enrichment and normalization capabilities.\n<a href=\"https://github.com/Stratio/sparta/blob/master/images/architectureDetail.jpg\" target=\"_blank\"><img src=\"https://github.com/Stratio/sparta/raw/master/images/architectureDetail.jpg\" alt=\"Architecture Detail\" /></a></li>\n</ul><p>On each workflow multiple components can be defined, but now all have the following architecture\n<a href=\"https://github.com/Stratio/sparta/blob/master/images/workflow.jpg\" target=\"_blank\"><img src=\"https://github.com/Stratio/sparta/raw/master/images/workflow.jpg\" alt=\"workflow\" /></a>\n<a href=\"https://github.com/Stratio/sparta/blob/master/images/components.jpg\" target=\"_blank\"><img src=\"https://github.com/Stratio/sparta/raw/master/images/components.jpg\" alt=\"Components\" /></a></p><h2>Core components</h2><p>Several plugins are been implemented by Stratio Sparta team\n<a href=\"https://github.com/Stratio/sparta/blob/master/images/plugins.jpg\" target=\"_blank\"><img src=\"https://github.com/Stratio/sparta/raw/master/images/plugins.jpg\" alt=\"Main plugins\" /></a></p><h2>Trigger component</h2><p>With Sparta is possible to execute queries over the streaming data, execute ETL, aggregations and Simple Event\nProcessing mixing streaming data with batch data on the trigger process.\n<a href=\"https://github.com/Stratio/sparta/blob/master/images/triggers.jpg\" target=\"_blank\"><img src=\"https://github.com/Stratio/sparta/raw/master/images/triggers.jpg\" alt=\"triggers\" /></a></p><h2>Aggregation component</h2><p>The aggregation process in Sparta is very powerful because is possible to generate efficient OLAP processes with\nstreaming data\n<a href=\"https://github.com/Stratio/sparta/blob/master/images/OLAPintegration.jpg\" target=\"_blank\"><img src=\"https://github.com/Stratio/sparta/raw/master/images/OLAPintegration.jpg\" alt=\"OLAP\" /></a></p><p>Advanced feature are been implemented in order to optimize the stateful operations over Spark Streaming\n<a href=\"https://github.com/Stratio/sparta/blob/master/images/aggregation.jpg\" target=\"_blank\"><img src=\"https://github.com/Stratio/sparta/raw/master/images/aggregation.jpg\" alt=\"Aggregations\" /></a></p><h2>Inputs</h2><ul><li>Twitter</li>\n<li>Kafka</li>\n<li>Flume</li>\n<li>RabbitMQ</li>\n<li>Socket</li>\n<li>WebSocket</li>\n<li>HDFS/S3</li>\n</ul><h2>Outputs</h2><ul><li>MongoDB</li>\n<li>Cassandra</li>\n<li>ElasticSearch</li>\n<li>Redis</li>\n<li>JDBC</li>\n<li>CSV</li>\n<li>Parquet</li>\n<li>Http</li>\n<li>Kafka</li>\n<li>HDFS/S3</li>\n<li>Http Rest</li>\n<li>Avro</li>\n<li>Logger</li>\n</ul><p><a href=\"https://github.com/Stratio/sparta/blob/master/images/outputs.png\" target=\"_blank\"><img src=\"https://github.com/Stratio/sparta/raw/master/images/outputs.png\" alt=\"Outputs\" /></a></p><ul><li>[Spark Streaming &amp; Spark]  (<a href=\"http://spark.apache.org\">http://spark.apache.org</a>)</li>\n<li>[SparkSQL] (<a href=\"https://spark.apache.org/sql\">https://spark.apache.org/sql</a>)</li>\n<li>[Akka] (<a href=\"http://akka.io\">http://akka.io</a>)</li>\n<li>[MongoDB] (<a href=\"http://www.mongodb.org/\">http://www.mongodb.org/</a>)</li>\n<li>[Apache Cassandra] (<a href=\"http://cassandra.apache.org\">http://cassandra.apache.org</a>)</li>\n<li>[ElasticSearch] (<a href=\"https://www.elastic.co\">https://www.elastic.co</a>)</li>\n<li>[Redis] (<a href=\"http://redis.io\">http://redis.io</a>)</li>\n<li>[Apache Parquet] (<a href=\"http://parquet.apache.org/\">http://parquet.apache.org/</a>)</li>\n<li>[HDFS] (<a href=\"http://hadoop.apache.org/docs/r1.2.1/hdfs_design.html\">http://hadoop.apache.org/docs/r1.2.1/hdfs_design.html</a>)</li>\n<li>[Apache Kafka] (<a href=\"http://kafka.apache.org\">http://kafka.apache.org</a>)</li>\n<li>[Apache Flume] (<a href=\"https://flume.apache.org/\">https://flume.apache.org/</a>)</li>\n<li>[RabbitMQ] (<a href=\"https://www.rabbitmq.com/\">https://www.rabbitmq.com/</a>)</li>\n<li>[Spray] (<a href=\"http://spray.io/\">http://spray.io/</a>)</li>\n<li>[KiteSDK (morphlines)] (<a href=\"http://kitesdk.org/docs/current\">http://kitesdk.org/docs/current</a>)</li>\n<li>[Apache Avro] (<a href=\"https://avro.apache.org/\">https://avro.apache.org/</a>)</li>\n</ul><p>Sparta provide several advantages to final Users\n<a href=\"https://github.com/Stratio/sparta/blob/master/images/features.jpg\" target=\"_blank\"><img src=\"https://github.com/Stratio/sparta/raw/master/images/features.jpg\" alt=\"Advantages\" /></a></p><p>You can generate rpm and deb packages by running:</p><p><code>mvn clean package -Ppackage</code></p><p><strong>Note:</strong> you need to have installed the following programs in order to build these packages:</p><p>In a debian distribution:</p><ul><li>fakeroot</li>\n<li>dpkg-dev</li>\n<li>rpm</li>\n<li>jq</li>\n</ul><p>In a centOS distribution:</p><ul><li>fakeroot</li>\n<li>dpkg-dev</li>\n<li>rpmdevtools</li>\n<li>jq</li>\n</ul><p>In all distributions:</p><ul><li>Java 8</li>\n<li>Maven 3</li>\n</ul><p>Licensed to STRATIO (C) under one or more contributor license agreements.\nSee the NOTICE file distributed with this work for additional information\nregarding copyright ownership.  The STRATIO (C) licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at</p><p><a href=\"http://www.apache.org/licenses/LICENSE-2.0\">http://www.apache.org/licenses/LICENSE-2.0</a></p><p>Unless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.</p>",
        "created_at": "2018-01-15T16:26:37+0000",
        "updated_at": "2018-05-13T16:06:55+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/5228027?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9063"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 963,
            "label": "akka",
            "slug": "akka"
          }
        ],
        "is_public": false,
        "id": 9062,
        "uid": null,
        "title": "Streaming Recommender with Spark",
        "url": "https://www.madewithtea.com/streaming-recommender-with-spark.html",
        "content": "<div id=\"article-header\"><div><div><p><time class=\"article-date\" datetime=\"2015-11-16T00:00:00+01:00\" pubdate=\"\">Mon 16 November 2015</time> - 6 min read</p></div></div><p>In this article, I explain how I implemented a distributed streaming article recommender with Apache Spark, also using Akka, RabbitMQ and Cassandra.</p><p>Category: <a href=\"https://www.madewithtea.com/category/spark.html\">Spark</a></p></div><p>In this article, I explain how I implemented a distributed streaming article recommender in <strong>Scala</strong> with <strong>Akka</strong>, <strong>RabbitMQ</strong>, <strong>Cassandra</strong> and <strong>Spark</strong>. If you dont know the technologies it is difficult to understand the following explainations. Therefore I <strong>recommend</strong> to read the introductions on the linked sites first.</p><p><a href=\"http://akka.io\">Akka</a> is a framework for highly concurrent, distributed, and resilient message-driven applications. <a href=\"https://www.rabbitmq.com/\">RabbitMQ</a> is a message broker. <a href=\"https://cassandra.apache.org/\">Cassandra</a> is a distributed NoSQL database providing high availability. Last but not least, <a href=\"https://spark.apache.org/\">Spark</a> is a framework for distributed computation which can run on scalable and high availability cluster managers like <a href=\"https://mesos.apache.org/\">Mesos</a>.  </p><p><strong>I try to keep the article as short but informative as possible</strong>. The article is quite long, <strong>so let me start with an outline</strong>: First, I will explain the setting and give some numbers. Second, I will explain why the initial monolithic design failed. Afterwards, I will describe the <strong>current service-oriented design</strong>, including the functionality of all involved components. Furthermore, I will go into the details of the <strong>model storage</strong> and <strong>fault tolerancy</strong> in the system in general. In the end, I will conclude and present possible future efforts for further improving the system.  </p><h3>Production System Facts</h3><p>The recommender will <strong>recommend articles based on their similarity score of features extracted from the full text</strong>, for instance <a href=\"https://code.google.com/p/word2vec/\">word2vec</a> word vectors. The current architecture is running successfully in a production environment with thousands of items distributed among isolated sets. Also, about 6000 of similarities are calcuated each second. The recommender calcuates the similarities of all possible article pairs. While this approach runs well in production, the design has to continously improve since it has to cope with <strong>increasing throughput of item create and delete events</strong>. Which also implies a changing model size.  </p><h3>Prototype</h3><p>In the company where I work, we are doing the transition to Spark-based recommenders, but <strong>when we started we had no experience so far with Spark</strong> nor <strong>experts of the field</strong> in the team. Even though, there is a great documentation of the project, still, <strong>best practices for certain use cases have not yet been formed</strong>. So I began with an <strong>explorative prototype</strong>, to understand how I could use Spark to scale the computation process of the similarity calculation. I started with a <strong>monolithic prototype</strong>, a standalone Spark streaming driver application.</p><h3>A Better Approach</h3><p>The main and innovative idea of Spark is to do distributed computation of a processing flow graph. Without going too much into the details, the driver coordinates the processing tasks which are parallized and balanced among different executors. Building a model and maintaining it (for instance doing I/O with a database) would be possible in the Spark driver, but it is a <strong>very bad idea to do so</strong> for the following reasons: </p><p>Blocking due to e.g. I/O in the driver, <strong>would block the whole streaming process for an undetermined time</strong>. Furthermore, one has to tune the memory limit of the driver <strong>not only based on the computation tasks</strong> but also <strong>according to the I/O usage</strong>. Because of these reasons I decided to use Spark Streaming just for its intended use, <strong>streaming computation</strong> and implement another service for <strong>model maintainance</strong>. </p><h3>Components</h3><p>The main components of the architecture are (1) the model service, which is a Scala service which is implemented using Akka and (2) the Spark driver application. The first component is the <strong>entry point of incoming item create and delete events</strong>. It also does preprocessing like <strong>stop word filtering, language detection</strong> and so on. Its <strong>core functionality is the management</strong> of the model stored in the Cassandra cluster. The Spark driver application receives processing tasks from the model service, does the distributed calculation in the cluster, and sends the results back to the model service.</p><p>The components communicate over the message broker <strong>RabbitMQ</strong>. I chose it, because it was part of the existing infrastructure. Besides, it has a nifty UI and also <strong>has support for message acknowledgement</strong>, which is useful for fault tolerancy in the model service. For the ingestion of item events into the system the log of <a href=\"https://kafka.apache.org/\">Kafka</a> would <strong>be very beneficial to replay item events to retrain the recommender with past item events</strong>. </p><h3>Functionality of the Model Service</h3><p>In the situation of incoming item create events, it will (1) group new item create events of a domain within a certain time window, (2) fetch feature data of all articles of this domain from Cassandra and (3) send the processing task with all required data to the Spark driver application. Keep in mind that the article recommendations are calculated in <strong>isolated domains</strong>, so <strong>we can parallelize step one, two and three on a domain level</strong>. </p><p>At the point of sending the task to the Spark driver, the model services sets this domain to a waiting state. <strong>It has to wait until the model is updated</strong> with new information, before it can send another task request. Keep in mind, that <strong>only this domain is blocked for the time</strong> between publishing and finishing updating the model.</p><h3>Functionality of the Spark Driver</h3><p>The Spark driver runs on a cluster and receives the computation tasks via RabbitMQ. In the concrete implementation it does <strong>feature extraction</strong> as well as the <strong>computation of similarities</strong> based on the features of new articles and features of existing articles in the model (which were fetched from Cassandra in the model service). The result is send back to the model service via RabbitMQ. </p><h3>Model Storage</h3><p>In this paragraph, I want to get into the details of the model storage. Since the storage is only use to store data, a simple key-value store like Redis would be sufficient. But it was clear, that <strong>this solution does not scale</strong> in the future or further work would be necessary. </p><p>So, at this point, I decided to use the <a href=\"http://www.planetcassandra.org/apache-cassandra-use-cases/\">widely-used</a> Cassandra database for a couple of reasons: (1) A Cassandra cluster scales with new nodes and data is distributed equally, when the definition of the data model is right. (2) Cassandra has been designed to allow fast writes, and reads with the help of built-in caching. (3) It supports synchronous and asynchronous (not waiting for Cassandra to accept) queries. Important to mention: Scalability comes with the downside of <a href=\"https://en.wikipedia.org/wiki/Eventual_consistency\">eventual consistency</a> and last but not least: (4) High availability of the Cassandra cluster nodes.  </p><h3>Fault Tolerancy</h3><p>The model service is built upon the Akka framework which is a <strong>huge help</strong> when developing fault-tolerant applications. Akka is a framework for distributed and fault-tolerant Actor systems. If you are not familiar with Akka or actor-based systems, I like to refer to the official <a href=\"http://doc.akka.io/docs/akka/2.4.0/intro/what-is-akka.html\">introduction</a>. </p><p>Akka handles unexpected errors in the following way: If an exception is thrown in an actor, the actor is simply restarted, which means it will loose its state. If this actor is stateless, the message will just be discarded and it will continue working. If there was a state, you should handle this unexpected behaviour with a custom failure protocol or an escalation to a severe exception that will halt the whole application; Of course, the latter should be avoided.</p><h3>Restart, Replication and Timeouts</h3><p><strong>As a last resort</strong>, the cluster manager, e.g. Mesos, will restart the model service, or the Spark driver. Furthermore, a <strong>replication of the model service and the Spark driver is possible to handle hardware outtages</strong>. In this case, multiple instances balance the load (sharded dependend on the domain).  </p><p>In the concrete implementation, I also added a duration supervisor which acts a timeout observer of computation tasks sent to the Spark driver. Consequently, an <strong>outtage of the Spark driver, will not result into a blocked state because the model service waits for the results</strong>.   </p><h3>Conclusion and Future Efforts</h3><p>As I stated in the beginning, I wanted to keep the article as short but informative as possible. <strong>I hope that this article gave you some insights of how a streaming recommender with Spark can be deployed</strong>. Needless to say, there is much room for improvement, for instance: </p><p>Since the model service can easily keep up with the throughput of item events and model data for now, there is no need for scaling this part of the architecture yet. Nevertheless, two options are possible: (1) Sharding model services dependend on the domain groups, or (2) scaling the instances of remote Akka actors across the cluster. Both options can be seen as possible future efforts. If you have questions regarding the implementation <strong>feel free to write me an email</strong>.  </p>",
        "created_at": "2018-01-15T16:25:54+0000",
        "updated_at": "2018-05-13T16:07:21+0000",
        "published_at": "2015-11-16T00:00:00+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 7,
        "domain_name": "www.madewithtea.com",
        "preview_picture": "https://www.madewithtea.com/theme/img/lotus.svg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9062"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 907,
            "label": "mesos",
            "slug": "mesos"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 963,
            "label": "akka",
            "slug": "akka"
          },
          {
            "id": 1041,
            "label": "smack",
            "slug": "smack"
          }
        ],
        "is_public": false,
        "id": 9061,
        "uid": null,
        "title": "What is the SMACK Stack!?",
        "url": "https://medium.com/pintail-labs/what-is-the-smack-stack-31bc85131a9a",
        "content": "<div class=\"section-inner sectionLayout--insetColumn\"><h1 id=\"092b\" class=\"graf graf--h3 graf--leading graf--title\">What is the SMACK Stack!?</h1><figure id=\"4918\" class=\"graf graf--figure graf-after--h3\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*yQZt17wm6s3OGeFB8iyU7g.png\" data-width=\"800\" data-height=\"388\" data-is-featured=\"true\" data-action=\"zoom\" data-action-value=\"1*yQZt17wm6s3OGeFB8iyU7g.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*yQZt17wm6s3OGeFB8iyU7g.png\" alt=\"image\" /></div></div></figure><p id=\"51d9\" class=\"graf graf--p graf-after--figure\">The SMACK stack is a group of tools that I have seen popping up more and more recently. I am very excited about this technology stack because it brings together a few of my favorite tools. The SMACK stack makes it easy to build powerful and scalable applications. Over the next few weeks I’m going to be writing various articles discussing each part of the SMACK stack.</p><p id=\"b575\" class=\"graf graf--p graf-after--p\">To kick off this new series, I want to get started by talking about the purpose of the SMACK stack. Then I will do a skin deep introduction to the five tools that make up the stack.</p><p id=\"4c2e\" class=\"graf graf--p graf-after--p\">Let’s say you wanted to build a new dating app. Something identical to other apps on the market. You need to be able to see who is in your area, swipe right and left, and chat. Your decide to take advantage of some tech within your comfort zone.</p><figure id=\"4482\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*v4cnSn8wR6r7oYA1.\" data-width=\"892\" data-height=\"355\" data-action=\"zoom\" data-action-value=\"0*v4cnSn8wR6r7oYA1.\" src=\"https://cdn-images-1.medium.com/max/1600/0*v4cnSn8wR6r7oYA1.\" alt=\"image\" /></div></div></figure><p id=\"fd2a\" class=\"graf graf--p graf-after--figure\">Apache Tomcat as the app server, a java application to write your code, and postgres as your database. All this running on top of a linux server.</p><p id=\"5cc2\" class=\"graf graf--p graf-after--p\">Your first priority when building this new application is getting it work. You need to store the profile information for all your customers and as well as what people swipe. You will also need to be able to quickly deliver new recommendations to people using the app. You work for a few weeks to build out all this logic in Java and store the data in postgres. For months everything is running great but then it happens. You go viral!</p><p id=\"2fcd\" class=\"graf graf--p graf-after--p\">Everyone loves you app and traffic has explodes! Your java application is bursting at the seams. Postgres can’t handle the transactions comes screeching to a halt. You’re left with two options. You can migrate everything to massive cloud machine which will cost cost a pretty penny. But even that it’s a temporary solution. Your left having to redesign your application from the ground up.</p><p id=\"dba1\" class=\"graf graf--p graf-after--p\">I bet you wish you had thought about scalability day one. Todays technology consumers have high expectations for your the functionality and performance of the tools they use.</p><p id=\"46ec\" class=\"graf graf--p graf-after--p\">So what is a systems architect to do?</p><figure id=\"10f9\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*jvt55H6gIH9YqPTJ.\" data-width=\"892\" data-height=\"439\" data-action=\"zoom\" data-action-value=\"0*jvt55H6gIH9YqPTJ.\" src=\"https://cdn-images-1.medium.com/max/1600/0*jvt55H6gIH9YqPTJ.\" alt=\"image\" /></div></div></figure><p id=\"75e9\" class=\"graf graf--p graf-after--figure\">In walks the SMACK stack! The SMACK stack is made up of 5 different tools covering different areas. Spark, Mesos, Akka, Cassandra, and Kafka. The stack as a whole handles big data batch processing, stream processing, distributed queuing, serverless development, and distributed storage all running on top of a scalable infrastructure. The cherry on top, all these tools are all Open Source, have amazing documentation, and great support communities!</p><p id=\"0308\" class=\"graf graf--p graf-after--p\">For the rest of this article I will touch on each tool in the SMACK stack. By the end I hope you will be as excited about these tools as I am.</p><figure id=\"034b\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*seVfV_Mx7-g_m3MF.\" data-width=\"461\" data-height=\"240\" src=\"https://cdn-images-1.medium.com/max/1600/0*seVfV_Mx7-g_m3MF.\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"http://spark.apache.org/\" data-href=\"http://spark.apache.org/\" class=\"markup--anchor markup--figure-anchor\" rel=\"noopener nofollow\" target=\"_blank\">http://spark.apache.org/</a></figcaption></div></figure><p id=\"64f7\" class=\"graf graf--p graf-after--figure\">The first tool in the SMACK stack is Apache Spark. I will be completely honest and say I am biased when it comes to this tool. I love it!! Apache Spark is a distributed batch and stream processing tool built as a successor to Hadoop.</p><p id=\"910e\" class=\"graf graf--p graf-after--p\">The Berkeley AmpLab developed Apache Spark as a next generation alternative to Hadoop. Apache Spark takes advantage of modern hardware improvements to deliver lightning speeds. Apache Spark is also easy to use with APIs written in Java, Scala, R, and Python.</p><figure id=\"6faf\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*KtyiZPqrXCFl7VI9.\" data-width=\"1024\" data-height=\"879\" data-action=\"zoom\" data-action-value=\"0*KtyiZPqrXCFl7VI9.\" src=\"https://cdn-images-1.medium.com/max/1600/0*KtyiZPqrXCFl7VI9.\" alt=\"image\" /></div></div></figure><p id=\"95d9\" class=\"graf graf--p graf-after--figure graf--trailing\">Spark relies on one simple concept, divide and conquer. When a task seems overwhelming, one approach is to split that large task into the smallest possible pieces. Apache Spark leverages a cluster of machines to execute a massive number of simple operations on a data set. These simple operations add up to sophisticated jobs on huge amounts of data.</p></div><div class=\"section-inner sectionLayout--insetColumn\"><figure id=\"d75a\" class=\"graf graf--figure graf--leading\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*SQAY6DL2QkkMShao.\" data-width=\"460\" data-height=\"240\" src=\"https://cdn-images-1.medium.com/max/1600/0*SQAY6DL2QkkMShao.\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"http://mesos.apache.org/\" data-href=\"http://mesos.apache.org/\" class=\"markup--anchor markup--figure-anchor\" rel=\"noopener nofollow\" target=\"_blank\">http://mesos.apache.org</a></figcaption></div></figure><p id=\"955b\" class=\"graf graf--p graf-after--figure\">Apache Mesos is the platform on which everything runs. Think of Apache Mesos as an operating system for distributed systems. Mesos runs on a cluster of machines and delegates work where resources are free.</p><figure id=\"d45f\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*wntCpqf6uvdsujtC.\" data-width=\"836\" data-height=\"563\" data-action=\"zoom\" data-action-value=\"0*wntCpqf6uvdsujtC.\" src=\"https://cdn-images-1.medium.com/max/1600/0*wntCpqf6uvdsujtC.\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"http://mesos.apache.org/documentation/latest/architecture/\" data-href=\"http://mesos.apache.org/documentation/latest/architecture/\" class=\"markup--anchor markup--figure-anchor\" rel=\"noopener nofollow\" target=\"_blank\">http://mesos.apache.org/documentation/latest/architecture/</a></figcaption></div></figure><blockquote id=\"f76a\" class=\"graf graf--blockquote graf--startsWithDoubleQuote graf-after--figure\"><div>“Apache Mesos abstracts CPU, memory, storage, and other compute resources away from machines (physical or virtual), enabling fault-tolerant and elastic distributed systems to easily be built and run effectively.” — Apache Mesos homepage</div></blockquote><p id=\"6506\" class=\"graf graf--p graf-after--blockquote\">Many people compare Apache Mesos to Docker Swarm or Kubernetes. This is a fair comparison because Mesos is able to run and orchestrate containers. Apache Mesos has a core difference from other container managements systems. Most container management systems focus on starting up long running containers. Apache Mesos shines by creating a short lived container to execute one specific unit of work. This allows Mesos to operate more fine grained and maximize the usage of all the resources in your cluster.</p><p id=\"8252\" class=\"graf graf--p graf-after--p graf--trailing\">When you run all pieces of the SMACK stack on top of Apache Mesos you can add more resources at any time. Add another machine to your Mesos cluster and Mesos will take care of the rest.</p></div><div class=\"section-inner sectionLayout--insetColumn\"><figure id=\"4ecb\" class=\"graf graf--figure graf--leading\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*dhOHvKgPXUXxbzc6.\" data-width=\"585\" data-height=\"240\" src=\"https://cdn-images-1.medium.com/max/1600/0*dhOHvKgPXUXxbzc6.\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"https://akka.io/\" data-href=\"https://akka.io/\" class=\"markup--anchor markup--figure-anchor\" rel=\"noopener nofollow\" target=\"_blank\">https://akka.io/</a></figcaption></div></figure><p id=\"6451\" class=\"graf graf--p graf-after--figure\">Akka is a set of Scala libraries used for writing serverless applications. Akka allows you to write concurrent applications without worrying about low level concurrency constructs like atomics and locks. Akka also makes it easy to spin up an elastic and highly available cluster of workers to run you Akka code.</p><figure id=\"c46f\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*Fb_M0kMXhEPwKZfd.\" data-width=\"599\" data-height=\"190\" src=\"https://cdn-images-1.medium.com/max/1600/0*Fb_M0kMXhEPwKZfd.\" alt=\"image\" /></div></div></figure><p id=\"1b35\" class=\"graf graf--p graf-after--figure\">Think of Akka as the cooler younger nephew of messaging busses triggering COBAL jobs. Is funny how time seems to repeat itself. The actor pattern used by Akka has been around for a long time in mainframes. With Akka this old pattern has been given a makeover with new distributed computing tools making it easy to build our your own serverless application platform.</p><p id=\"fb8c\" class=\"graf graf--p graf-after--p\">In Akka all jobs are places on queues. When an actor(worker) becomes available it pulls a message off the queue and completes the job. These jobs can be data processing, responding to a HTTP calls, or pulling data from a database. In each case rather that having a thread wait for another thread to finish just move on to the next job.</p><p id=\"538e\" class=\"graf graf--p graf-after--p graf--trailing\">This pattern makes Akka the right development framework to code your new scalable application along side the rest of the SMACK stack.</p></div><div class=\"section-inner sectionLayout--insetColumn\"><figure id=\"b3b8\" class=\"graf graf--figure graf--leading\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*fDSuBLF3D6Y4rxeD.\" data-width=\"358\" data-height=\"240\" src=\"https://cdn-images-1.medium.com/max/1600/0*fDSuBLF3D6Y4rxeD.\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"http://cassandra.apache.org/\" data-href=\"http://cassandra.apache.org/\" class=\"markup--anchor markup--figure-anchor\" rel=\"noopener nofollow\" target=\"_blank\">http://cassandra.apache.org/</a></figcaption></div></figure><blockquote id=\"8380\" class=\"graf graf--blockquote graf--startsWithDoubleQuote graf-after--figure\"><div>“The Apache Cassandra database is the right choice when you need scalability and high availability without compromising performance. ” — Cassandra Website</div></blockquote><p id=\"3d40\" class=\"graf graf--p graf-after--blockquote\">If you are looking for a fault tolerant, scalable, decentralized, distributed database, with battle tested experience to store your important data, look no further. Apache Cassandra is the the NoSQL database you have been looking for.</p><p id=\"378d\" class=\"graf graf--p graf-after--p\">Wow that is a mouthful!</p><p id=\"9d78\" class=\"graf graf--p graf-after--p\">Apache Cassandra is a row oriented NoSql database built for the Big Data era. Facebook built Apache Cassandra using tech from Google Big Table and Amazon Dynamo.</p><p id=\"34f3\" class=\"graf graf--p graf-after--p\">What does this all mean?</p><figure id=\"e4b4\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*GiONM_y7mq4OWeIK.\" data-width=\"403\" data-height=\"353\" src=\"https://cdn-images-1.medium.com/max/1600/0*GiONM_y7mq4OWeIK.\" alt=\"image\" /></div></div></figure><p id=\"9144\" class=\"graf graf--p graf-after--figure\">Apache Cassandra is a big database that can run across of a lot of servers without slowing down. More servers means more disk, more memory, and more cpu. But more servers also means more coordination and potential problems. Apache Cassandra takes care of all that coordination, including data replication and node orchestration. That way you can focus on inserting and retrieving lots of data.</p><p id=\"ff45\" class=\"graf graf--p graf-after--p graf--trailing\">Apache Cassandra allows you to add more nodes when you need it as well as remove them when you don’t. This makes Cassandra a great addition along with the other members of the stack. Each of these tools allow you to start small and scale up or down depending on your needs.</p></div><div class=\"section-inner sectionLayout--insetColumn\"><figure id=\"a49c\" class=\"graf graf--figure graf--leading\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*huQJ98ACvPWDe_7Z.\" data-width=\"804\" data-height=\"240\" data-action=\"zoom\" data-action-value=\"0*huQJ98ACvPWDe_7Z.\" src=\"https://cdn-images-1.medium.com/max/1600/0*huQJ98ACvPWDe_7Z.\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"http://kafka.apache.org/\" data-href=\"http://kafka.apache.org/\" class=\"markup--anchor markup--figure-anchor\" rel=\"noopener nofollow\" target=\"_blank\">http://kafka.apache.org/</a></figcaption></div></figure><p id=\"1e73\" class=\"graf graf--p graf-after--figure\">Last but not least, Apache Kafka. Apache Kafka is a distributed Pub/Sub messaging system. This means it is basically a supercharged messaging queue. Apache Kafka is able to manage a queue running across many servers with many publishers and subscribers.</p><p id=\"b2fe\" class=\"graf graf--p graf-after--p\">To make this possible Kafka splits a specific topic into many partitions. Then it distributes these partitions across different brokers running on different physical machines. This allows Kafka to scale without loosing data.</p><figure id=\"5f0a\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*06w4JmB5UioTOLpy.\" data-width=\"632\" data-height=\"420\" src=\"https://cdn-images-1.medium.com/max/1600/0*06w4JmB5UioTOLpy.\" alt=\"image\" /></div></div></figure><figure id=\"9911\" class=\"graf graf--figure graf-after--figure\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*VqwHRU3Qpl3WBX3L.\" data-width=\"710\" data-height=\"278\" data-action=\"zoom\" data-action-value=\"0*VqwHRU3Qpl3WBX3L.\" src=\"https://cdn-images-1.medium.com/max/1600/0*VqwHRU3Qpl3WBX3L.\" alt=\"image\" /></div></div></figure><blockquote id=\"b453\" class=\"graf graf--blockquote graf--startsWithDoubleQuote graf-after--figure\"><div>“Kafka® is used for building real-time data pipelines and streaming apps. It is horizontally scalable, fault-tolerant, wicked fast, and runs in production in thousands of companies.” — Apache Kafka homepage</div></blockquote><p id=\"7d51\" class=\"graf graf--p graf-after--blockquote\">LinkedIn developed Apache Kakfa to handle huge numbers of producers and consumers reading and writing simultaneously. This ability allows Kafka to act as a massive distributed buffer within your SMACK stack. Kafka will keep track of any information as it comes into your application until there are available machine resources. Both Apache Spark streaming and Akka play well with queues specifically Kafka.</p><p id=\"d43d\" class=\"graf graf--p graf-after--p graf--trailing\">All this makes Kafka another welcome member of the SMACK stack.</p></div><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"701d\" class=\"graf graf--p graf--leading\">So now I will apologies to those of you who have been completely overwhelmed. I covered a lot of information very quickly but i have barely scratched the tip of the iceberg.</p><p id=\"ee09\" class=\"graf graf--p graf-after--p graf--trailing\">Over the next few weeks I will write more thorough articles for each piece of the SMACK stack. I will also building out some demos so we can better see where the SMACK stack thrives and where it falls over.</p></div>",
        "created_at": "2018-01-15T16:18:30+0000",
        "updated_at": "2018-06-27T04:14:24+0000",
        "published_at": "2018-01-10T12:11:59+0000",
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 7,
        "domain_name": "medium.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*yQZt17wm6s3OGeFB8iyU7g.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9061"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 907,
            "label": "mesos",
            "slug": "mesos"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 963,
            "label": "akka",
            "slug": "akka"
          },
          {
            "id": 1041,
            "label": "smack",
            "slug": "smack"
          }
        ],
        "is_public": false,
        "id": 9060,
        "uid": null,
        "title": "Open Source Partners: Focus on fast big data",
        "url": "https://blogs.technet.microsoft.com/msuspartner/2016/11/15/open-source-fast-big-data/",
        "content": "<p><img width=\"700\" height=\"144\" class=\"alignnone wp-image-18555\" alt=\"Tim Walton - Technology Solutions Professional Open Source\" src=\"https://msdnshared.blob.core.windows.net/media/2016/09/Tim-Walton-author-block1.jpg\" /></p>\n<p>In the October community call, we discussed the opportunity for partners to produce domain specific solutions using Azure Container Service. The November focus for the Open Source Solutions Partner Community is OSS and big data. In this post, I'll provide an in-depth look at a trending solution called the SMACK stack - a valuable solution for every partner's digital transformation toolkit.</p>\n<p><a target=\"_blank\" href=\"https://msuspartner.eventbuilder.com/event?eventid=m7n5i2\">Sign up for the November 30 community call </a></p>\n<p><a target=\"_blank\" href=\"https://youtu.be/Ac7AxbYVeJ0?list=PLz7jPMmpNrjkkS_cbNdXnSz6t16I9X-WE\">Watch the October community call on demand </a></p>\n<p>SMACK is a technology solution stack that comprises Spark, Mesos, Akka, Cassandra, and Kafka. It is a data-processing architecture designed to handle massive quantities of data that can take advantage of both batch and stream processing methods. It becomes incredibly important when trying to solve problems such as ingesting and querying data produced from the Internet of Things and today's big data producers.</p>\n<p><img width=\"600\" height=\"339\" class=\"wp-image-20145 size-full alignright\" alt=\"Extract, Transform, and Load (ETL) systems\" src=\"https://msdnshared.blob.core.windows.net/media/2016/11/ETL-systems-OSS-blog-image-Nov-2016.png\" />Data ingestion from various systems was typically achieved through Extract, Transform, and Load (ETL) systems. However, ETL has some inherent problems:</p>\n<ul><li>Loss of data</li>\n<li>Duplicate data after failover</li>\n<li>Decreases throughput</li>\n<li>Expensive to scale</li>\n<li>Increases the complexity of the pipeline</li>\n</ul><p>The SMACK stack is an attempt to rationalize various data processing scenarios. It is made up of highly scalable, reactive frameworks to deliver a fast, Highly-Available Redundantly-Distributed (HARD) system.</p>\n<p>The diagram below shows how the SMACK stack relates to the first-party services in Microsoft Azure. Partners are increasingly plugging Microsoft first-party services into the SMACK stack where it makes sense. For example, <a target=\"_blank\" href=\"https://azure.microsoft.com/en-us/resources/videos/announcing-apache-spark-on-azure-hdinsight/\">Apache Spark on Azure HDInsight</a>.</p>\n<p><img width=\"500\" height=\"600\" class=\"wp-image-20155 aligncenter\" alt=\"smack-stack-image-oss-nov-2016-blog\" src=\"https://msdnshared.blob.core.windows.net/media/2016/11/SMACK-Stack-image-OSS-Nov-2016-blog.png\" /></p>\n<h2>Spark</h2>\n<p>Apache Spark is the processing layer, an open source cluster computing framework that addresses the disk-based limitations in traditional map reduce solutions. Specifically, Spark focuses on providing distributed shared memory primitives that drastically improve performance and interactivity of the data. Spark provides a unified interface allowing SQL queries, machine learning, graph analysis, and streaming (micro-batched) processing.</p>\n<p>Advantages:</p>\n<ul><li>Distributed analytics platform</li>\n<li>Simple abstraction of datasets</li>\n<li>Multiple language support</li>\n<li>Streaming support</li>\n<li>Machine learning</li>\n<li>Integrated SQL queries</li>\n</ul><p><a target=\"_blank\" href=\"http://spark.apache.org/\">Learn more about Apache Spark </a></p>\n<p><a target=\"_blank\" href=\"https://myignite.microsoft.com/videos/49989\">R &amp; Spark as Yin and Yang of Scalable Machine Learning in Azure HDInsight</a></p>\n<p><a target=\"_blank\" href=\"https://myignite.microsoft.com/videos/3095\">Leverage R and Spark in Azure HDInsight for scalable machine learning</a></p>\n<p><a target=\"_blank\" href=\"https://myignite.microsoft.com/videos/50006\">Big, Fast, and Data-Furious...with Spark</a></p>\n<p><a target=\"_blank\" href=\"https://myignite.microsoft.com/videos/4097\">Build interactive data analysis environments using Apache Spark</a></p>\n<h2>Mesos</h2>\n<p>Mesos can be thought as the resource manager or service fabric for the other frameworks. Apache Mesos is an open-source cluster, providing efficient resource isolation and sharing across distributed applications, or frameworks. The software enables resource sharing in a fine-grained manner, improving cluster utilization.</p>\n<p><a target=\"_blank\" href=\"http://mesos.apache.org/\">Learn more about Apache Mesos </a></p>\n<p><a target=\"_blank\" href=\"https://channel9.msdn.com/Search?term=mesos%20#lang-en=en&amp;ch9Search\">Mesos videos on Channel 9 </a></p>\n<p><a target=\"_blank\" href=\"https://info.microsoft.com/US-MSFT-WBNR-FY16-05May-03-MesosphereandAzureContainerService-Registration.html\">On-demand webcast: Mesosphere and Azure Container Service </a></p>\n<h2>Akka<strong> </strong></h2>\n<p>Akka ingests the data and is an open-source toolkit and runtime simplifying the construction of concurrent and distributed applications on the JVM. Akka is message focused and emphasizes actor-based concurrency and is similar to <a target=\"_blank\" href=\"https://azure.microsoft.com/en-us/services/service-fabric/\">Azure ServiceFabric</a>.</p>\n<ul><li>Fault tolerant</li>\n<li>Hierarchical supervision</li>\n<li>Customizable failure strategies and detection</li>\n<li>Asynchronous data passing</li>\n<li>Parallelization</li>\n<li>Adaptive/predictive</li>\n<li>Load-balanced across cluster nodes</li>\n</ul><p><a target=\"_blank\" href=\"http://akka.io/\">Learn more about Akka </a></p>\n<p><a target=\"_blank\" href=\"https://channel9.msdn.com/Search?term=Akka#ch9Search&amp;lang-en=en\">Akka videos on Channel 9</a></p>\n<h2>Cassandra<strong> </strong></h2>\n<p>Apache Cassandra is the storage layer of the stack, an open source distributed database management system designed to handle large amounts of data across many commodity servers. Cassandra is used to persist distributed events, providing high availability with no single point of failure.</p>\n<ul><li>Massively scalable</li>\n<li>High performance</li>\n<li>Always on</li>\n<li>Masterless</li>\n<li>Multiple datacenter cluster support</li>\n</ul><p><a target=\"_blank\" href=\"http://cassandra.apache.org/\">Learn more about Apache Cassandra </a></p>\n<p><a target=\"_blank\" href=\"https://channel9.msdn.com/Search?term=Cassandra#ch9Search&amp;lang-en=en\">Cassandra videos on Channel 9 </a></p>\n<p><a href=\"https://info.microsoft.com/en-us-wbnr-BuildingGeoDistributedPublicCloudAppsOnCassandra-register.html\">December 16: Building geo-distributed public cloud apps on Cassandra</a></p>\n<h2>Kafka</h2>\n<p>Apache Kafka is the transportation layer and buffer for dealing with event streams. It provides:</p>\n<ul><li>High throughput distributed messaging</li>\n<li>Decouples data pipelines</li>\n<li>Handles massive data load</li>\n<li>Support massive number of consumers</li>\n<li>Distribution and partitioning across cluster nodes</li>\n<li>Automatic recovery from broker failures</li>\n</ul><p><a target=\"_blank\" href=\"http://kafka.apache.org/\">Learn more about Apache Kafka</a></p>\n<p>The SMACK stack simplifies streaming analytics, but there is a need for partners with a full stack knowledge and domain expertise. For example, <a target=\"_blank\" href=\"http://www.esri.com/\">ESRI,</a> a Microsoft Gold Application Development Partner, recently demonstrated a fantastic partner-created solution with its forthcoming <a target=\"_blank\" href=\"https://youtu.be/BPsLGibQ_dQ?list=PLGeM09tlguZQVL7ZsfNMffX9h1rGNVqnC\">ArcGIS service</a>, which utilizes DC/OS by way of <a target=\"_blank\" href=\"https://blogs.technet.microsoft.com/msuspartner/2016/10/17/oss-partners-containers-focus/\">Azure Container Service</a>. ArcGIS takes advantage of data systems such as Spark Streaming, Kafka, and Elasticsearch, as well as Azure IoT Hub, in order to analyze and visualize geospatial data in real time. This is a packaged offering that ESRI can provide to their customers as a managed service.</p>\n<p>Whether your partner business focuses on data platform, advanced analytics, IoT, or application development, understanding SMACK is critical for your architects. The attributes of each of the frameworks that make up the SMACK stack act as a patterns for reactive, Highly-Available Redundantly-Distributed systems.</p>\n<p>The demand for SMACK expertise is growing rapidly, it provides deep business value, and is a perfect fit for hyperscale properties of Microsoft Azure.</p>\n<h2>Microsoft Ignite sessions on demand</h2>\n<p><a href=\"https://myignite.microsoft.com/videos/49993\">Streaming in the Cloud: We've Got It All Covered</a></p>\n<p><a target=\"_blank\" href=\"https://myignite.microsoft.com/videos?q=Azure%20container%20Services\">Azure Container Service sessions</a></p>\n<h2>Training recommendations</h2>\n<p><a target=\"_blank\" href=\"https://aka.ms/odxnrp\">Webcast series about open source on Microsoft Azure </a></p>\n<p><a target=\"_blank\" href=\"https://azure.microsoft.com/en-us/community/training/\">Training and certification for Azure </a></p>\n<ul><li><a target=\"_blank\" href=\"https://aka.ms/usosscall\">Community call schedule </a></li>\n<li><a target=\"_blank\" href=\"https://aka.ms/nebn5q\">Blog series </a></li>\n<li><a target=\"_blank\" href=\"https://aka.ms/usossyammer\">Yammer group</a></li>\n<li><a target=\"_blank\" href=\"https://aka.ms/hlexn9\">Training and enablement</a></li>\n</ul><p><a href=\"https://msuspartner.eventbuilder.com/event?eventid=m7n5i2&amp;regNow=1\"><img width=\"240\" height=\"120\" class=\"alignnone size-full wp-image-20165\" alt=\"oss-community-call-nov-30\" src=\"https://msdnshared.blob.core.windows.net/media/2016/11/OSS-community-call-Nov-30.png\" /></a>     <a href=\"https://aka.ms/usossyammer\"><img width=\"240\" height=\"120\" class=\"alignnone wp-image-18585 size-full\" alt=\"CTA - OSS Partners Yammer group\" src=\"https://msdnshared.blob.core.windows.net/media/2016/09/CTA-OSS-Partners-Yammer-group.jpg\" /></a>     <a href=\"http://open.microsoft.com/\"><img width=\"240\" height=\"120\" class=\"alignnone size-full wp-image-20175\" alt=\"microsoft-open-source-website\" src=\"https://msdnshared.blob.core.windows.net/media/2016/11/Microsoft-Open-Source-website.png\" /></a></p>",
        "created_at": "2018-01-15T16:18:12+0000",
        "updated_at": "2018-05-13T16:07:51+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 4,
        "domain_name": "blogs.technet.microsoft.com",
        "preview_picture": "https://msdnshared.blob.core.windows.net/media/2016/09/Tim-Walton-author-block1.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9060"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 35,
            "label": "docker",
            "slug": "docker"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 207,
            "label": "system",
            "slug": "system"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          }
        ],
        "is_public": false,
        "id": 9037,
        "uid": null,
        "title": "cloudurable/cassandra-image",
        "url": "https://github.com/cloudurable/cassandra-image",
        "content": "<p>Goal of this project is to create AMI, Vagrant Box and Docker base images that one could use to deploy Cassandra.</p><p>The docker image is hosted on docker hub (see <a href=\"https://hub.docker.com/r/cloudurable/cassandra/\" rel=\"nofollow\">DockerHub</a>).\nThe vagrant box is hosted on Atlas see (<a href=\"https://atlas.hashicorp.com/cloudurable/boxes/cassandra\" rel=\"nofollow\">Atlas</a>).\nThe source code is hosted on GitHub (see <a href=\"https://github.com/cloudurable/cassandra-image\">GitHub</a>).</p><p>Key features of image, AMI, Vagrant box</p><ul><li>Uses JBOD instead of RAID (not done)</li>\n<li>Uses ergonomics to configure Cassandra based on deployment environment (preliminary work done)</li>\n<li>Sets up security if requested (not done)</li>\n<li>Sets up TLS/SSL if requested (not done)</li>\n<li>Sets up HD encryption if requested (not done, or use encrypted EBS instances)</li>\n<li>Sets up users (not done)</li>\n<li>Installs Cassandra as a systemd service (not done)</li>\n<li>Allows cloud deploy (not done)</li>\n<li>Monitoring (CloudWatch, InfluxDB) (not done)</li>\n<li>Log aggregation (CloudWatch, ELK) (not done)</li>\n<li>Installs JEMalloc and configures Cassandra to use off heap no JVM (done)</li>\n<li>Install JNA (done)</li>\n<li>Configures OS (Linux) to be performant (done)</li>\n</ul><h2>Create a vagrant box</h2><pre>git clone https://github.com/cloudurable/cassandra-image.git\ncd cassandra-image \nvagrant up \n# Connect to vagrant box\ncqlsh localhost 19042\n</pre><h2>Connect to vagrant image</h2><pre>vagrant ssh\n</pre><h2>Create a docker image</h2><pre>git clone https://github.com/cloudurable/cassandra-image.git\ncd cassandra-image \nbin/start-image.sh\n# Connect to docker image\ncqlsh localhost 29042\n</pre><h2>Provisioning</h2><p>We use packer and vagrant to create images.</p><h4>Running setup scripts</h4><pre>## cd ~; mkdir github; cd github; git clone https://github.com/cloudurable/cassandra-image\n$ cd ~/github/cassandra-image\n$ pwd\n~/github/cassandra-image\n## Setup keys\n$ bin/setupkeys-cassandra-security.sh\n## Download binaries\n$ bin/prepare_binaries.sh\n## Bring Vagrant cluster up\n$ vagrant up\n</pre><h4>Working with ansible from bastion</h4><div class=\"highlight highlight-source-shell\"><pre>$ vagrant ssh bastion</pre></div><p>First setup ssh-agent and add keys to it.</p><h4>Start ssh-agent and add keys</h4><pre>$ ssh-agent bash\n$ ssh-add ~/.ssh/test_rsa\n</pre><h4>Ansible Ping server</h4><div class=\"highlight highlight-source-shell\"><pre>$ ansible node0 -m ping</pre></div><p>Output</p><pre>node0 | SUCCESS =&gt; {\n    \"changed\": false, \n    \"ping\": \"pong\"\n}\n</pre><h4>Ansible Ping servers</h4><div class=\"highlight highlight-source-shell\"><pre>$ ansible nodes  -m ping</pre></div><p>Output</p><pre>node0 | SUCCESS =&gt; {\n    \"changed\": false, \n    \"ping\": \"pong\"\n}\nnode2 | SUCCESS =&gt; {\n    \"changed\": false, \n    \"ping\": \"pong\"\n}\nnode1 | SUCCESS =&gt; {\n    \"changed\": false, \n    \"ping\": \"pong\"\n}\n</pre><h2>Setting up my MacOSX to run Ansible against instances</h2><p>Move to the where you checked out the <a href=\"https://github.com/cloudurable/cassandra-image\">project</a>.</p><div class=\"highlight highlight-source-shell\"><pre>cd ~/github/cassandra-image</pre></div><h4>Add bastion, node0, etc. to /etc/hosts</h4><div class=\"highlight highlight-source-shell\"><pre>$ cat /etc/hosts\n### Used for ansible/ vagrant\n192.168.50.20  bastion\n192.168.50.4  node0\n192.168.50.5  node1\n192.168.50.6  node2\n192.168.50.7  node3\n192.168.50.8  node4\n192.168.50.9  node5\n</pre></div><h4>Add keys to known_hosts to avoid prompts</h4><div class=\"highlight highlight-source-shell\"><pre>$ ssh-keyscan node0 node1 node2  &gt;&gt; ~/.ssh/known_hosts\n</pre></div><h4>Start ssh-agent and add keys</h4><pre>$ ssh-agent bash\n$ ssh-add ~/.ssh/test_rsa\n</pre><h4>Notice the ansible.cfg file and inventory.ini file in the project dir</h4><pre>$ cd ~/github/cassandra-image\n$ cat ansible.cfg \n[defaults]\nhostfile = inventory.ini\ncat inventory.ini \n[nodes]\nnode0 ansible_user=vagrant\nnode1 ansible_user=vagrant\nnode2 ansible_user=vagrant\n</pre><p>Ansible will use these.</p><h4>Ansible Ping server</h4><div class=\"highlight highlight-source-shell\"><pre>$ ansible node0 -m ping</pre></div><p>Output</p><pre>node0 | SUCCESS =&gt; {\n    \"changed\": false, \n    \"ping\": \"pong\"\n}\n</pre><h4>Ansible Ping servers</h4><div class=\"highlight highlight-source-shell\"><pre>$ ansible nodes  -m ping</pre></div><p>Output</p><pre>node0 | SUCCESS =&gt; {\n    \"changed\": false, \n    \"ping\": \"pong\"\n}\nnode2 | SUCCESS =&gt; {\n    \"changed\": false, \n    \"ping\": \"pong\"\n}\nnode1 | SUCCESS =&gt; {\n    \"changed\": false, \n    \"ping\": \"pong\"\n}\n</pre><h2>More details to follow</h2><h2>About us</h2><p><a href=\"http://cloudurable.com/\" rel=\"nofollow\">Cloudurable</a> provides AMIs, cloudformation templates and monitoring tools\nto support <a href=\"http://cloudurable.com/services/index.html\" rel=\"nofollow\">Cassandra in production running in EC2</a>.\nWe also teach advanced <a href=\"http://cloudurable.com/services/index.html\" rel=\"nofollow\">Cassandra courses which teaches how one could develop, support and deploy Cassandra to production in AWS EC2</a>.</p>",
        "created_at": "2018-01-12T20:02:32+0000",
        "updated_at": "2019-02-05T15:02:17+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/24500753?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9037"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 15,
            "label": "tutorial",
            "slug": "tutorial"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          }
        ],
        "is_public": false,
        "id": 9036,
        "uid": null,
        "title": "GumGum: Multi-Region Cassandra in AWS",
        "url": "https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws",
        "content": "GumGum: Multi-Region Cassandra in AWS\n      \n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            </a><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div>\n    <div class=\"wrapper\"><p>Successfully reported this slideshow.</p><div id=\"slideview-container\" class=\"\"><div class=\"row\"><div id=\"main-panel\" class=\"small-12 large-8 columns\"><div class=\"sectionElements\"><div class=\"playerWrapper\"><div><div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\">GumGum: Multi-Region Cassandra in AWS<div class=\"stage valign-first-slide\"><div class=\"slide_container\"><section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-1-638.jpg?cb=1443906825\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-1-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-1-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-1-1024.jpg?cb=1443906825\" alt=\"CASSANDRA SUMMIT 2015CASSANDRA SUMMIT 2015&#10;Mario Lazaro&#10;September 24th 2015&#10;#CassandraSummit2015&#10;MULTI-REGION CASSANDRA IN...\" /></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-2-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-2-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-2-1024.jpg?cb=1443906825\" alt=\"WHOAMIWHOAMI&#10;Mario Cerdan Lazaro&#10;Big Data Engineer&#10;Born and raised in Spain&#10;Joined GumGum 18 months ago&#10;About a year and a...\" /></i></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-3-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-3-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-3-1024.jpg?cb=1443906825\" alt=\"#5 Ad Platform in the U.S&#10;10B impressions / month&#10;2,000 brand-safe premium&#10;publisher partners&#10;1B+ global unique visitors&#10;D...\" /></i></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-4-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-4-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-4-1024.jpg?cb=1443906825\" alt=\"AGENDAAGENDA&#10;Old cluster&#10;International Expansion&#10;Challenges&#10;Testing&#10;Modus Operandi&#10;Tips&#10;Questions &amp; Answers&#10;\" /></i></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-5-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-5-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-5-1024.jpg?cb=1443906825\" alt=\"25 Classic nodes cluster&#10;1 Region / 1 Rack hosted in&#10;AWS EC2 US East&#10;Version 2.0.8&#10;Datastax CQL driver&#10;GumGum's metadata i...\" /></i></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-6-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-6-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-6-1024.jpg?cb=1443906825\" alt=\"OLD C* CLUSTER - REALTIME USE CASEOLD C* CLUSTER - REALTIME USE CASE&#10;Billions of rows&#10;Heavy read workload&#10;60/40&#10;TTLs every...\" /></i></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-7-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-7-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-7-1024.jpg?cb=1443906825\" alt=\"OLD C* CLUSTER - ANALYTICS USE CASEOLD C* CLUSTER - ANALYTICS USE CASE&#10;Daily ETL jobs to extract / join data&#10;from C*&#10;​Hado...\" /></i></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-8-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-8-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-8-1024.jpg?cb=1443906825\" alt=\"INTERNATIONAL EXPANSIONINTERNATIONAL EXPANSION&#10;\" /></i></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-9-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-9-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-9-1024.jpg?cb=1443906825\" alt=\"FIRST STEPSFIRST STEPS&#10;Start C* test datacenters in US&#10;East &amp; EU West and test how C*&#10;multi region works in AWS&#10;Run capaci...\" /></i></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-10-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-10-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-10-1024.jpg?cb=1443906825\" alt=\"TOO GOOD TO BE TRUE ...TOO GOOD TO BE TRUE ...&#10;\" /></i></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-11-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-11-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-11-1024.jpg?cb=1443906825\" alt=\"CHALLENGESCHALLENGES&#10;Problems between Cassandra in EC2&#10;Classic / VPC and Datastax Java&#10;driver&#10;EC2MultiRegionSnitch uses pu...\" /></i></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-12-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-12-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-12-1024.jpg?cb=1443906825\" alt=\"Datastax Java Driver Load Balancing&#10;Multiple choices&#10;DCAware + TokenAware&#10;Datastax Java Driver Load Balancing&#10;Multiple cho...\" /></i></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-13-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-13-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-13-1024.jpg?cb=1443906825\" alt=\"Zone Aware Connection:&#10;Webapps in 3 different AZs: 1A, 1B, and 1C&#10;C* datacenter spanning 3 AZs with 3 replicas&#10;CHALLENGESC...\" /></i></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-14-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-14-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-14-1024.jpg?cb=1443906825\" alt=\"We added it! - Rack/AZ awareness to&#10;TokenAware Policy&#10;CHALLENGESCHALLENGES&#10;\" /></i></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-15-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-15-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-15-1024.jpg?cb=1443906825\" alt=\"CHALLENGESCHALLENGES&#10;Third Datacenter: Analytics&#10;Do not impact realtime data access&#10;Spark on top of Cassandra&#10;Spark-Cassan...\" /></i></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-16-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-16-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-16-1024.jpg?cb=1443906825\" alt=\"CHALLENGESCHALLENGES&#10;Third Datacenter: Analytics&#10;Cassandra Only DC&#10;Realtime&#10;Cassandra + Spark DC&#10;Analytics&#10;\" /></i></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-17-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-17-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-17-1024.jpg?cb=1443906825\" alt=\"CHALLENGESCHALLENGES&#10;Upgrade from 2.0.8 to 2.1.5&#10;Counters implementation is buggy in&#10;pre-2.1 versions&#10;“ My code never has ...\" /></i></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-18-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-18-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-18-1024.jpg?cb=1443906825\" alt=\"CHALLENGESCHALLENGES&#10;“ To choose, or not to choose VNodes. That is the question.&#10;(M. Lazaro, 1990 - 2500)&#10;Previous DC usin...\" /></i></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-19-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-19-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-19-1024.jpg?cb=1443906825\" alt=\"TESTINGTESTING&#10;\" /></i></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-20-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-20-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-20-1024.jpg?cb=1443906825\" alt=\"TESTINGTESTING&#10;Testing requires creating and modifying&#10;many C* nodes&#10;Create and conﬁguring a C* cluster&#10;is time-consuming ...\" /></i></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-21-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-21-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-21-1024.jpg?cb=1443906825\" alt=\"TESTING - PERFORMANCETESTING - PERFORMANCE&#10;Performance tests using new&#10;Cassandra 2.1 Stress Tool:&#10;Recreate GumGum metadata...\" /></i></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-22-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-22-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-22-1024.jpg?cb=1443906825\" alt=\"TESTING - PERFORMANCETESTING - PERFORMANCE&#10;Main worry:&#10;Latency and replication overseas&#10;Use LOCAL_X consistency levels in ...\" /></i></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-23-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-23-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-23-1024.jpg?cb=1443906825\" alt=\"TESTING - PERFORMANCETESTING - PERFORMANCE&#10;Main worries:&#10;Latency&#10;\" /></i></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-24-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-24-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-24-1024.jpg?cb=1443906825\" alt=\"TESTING - INSTANCE TYPETESTING - INSTANCE TYPE&#10;Test all kind of instance types. We decided to&#10;go with r3.2xlarge machines ...\" /></i></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-25-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-25-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-25-1024.jpg?cb=1443906825\" alt=\"TESTING - UPGRADETESTING - UPGRADE&#10;Upgrade C* Datacenter from 2.0.8 to 2.1.5&#10;Both versions can cohabit in the same DC&#10;New ...\" /></i></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-26-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-26-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-26-1024.jpg?cb=1443906825\" alt=\"MODUS OPERANDIMODUS OPERANDI&#10;\" /></i></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-27-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-27-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-27-1024.jpg?cb=1443906825\" alt=\"MODUS OPERANDIMODUS OPERANDI&#10;Sum up&#10;From: One cluster / One DC in US East&#10;To: One cluster / Two DCs in US East and one DC&#10;...\" /></i></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-28-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-28-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-28-1024.jpg?cb=1443906825\" alt=\"MODUS OPERANDIMODUS OPERANDI&#10;First step:&#10;Upgrade old cluster snitch from EC2Snitch to&#10;EC2MultiRegionSnitch&#10;Upgrade clients...\" /></i></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-29-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-29-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-29-1024.jpg?cb=1443906825\" alt=\"MODUS OPERANDIMODUS OPERANDI&#10;Second step:&#10;Upgrade old datacenter from 2.0.8 to 2.1.5&#10;nodetool upgradesstables (multiple no...\" /></i></section><section data-index=\"30\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-30-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-30-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-30-1024.jpg?cb=1443906825\" alt=\"MODUS OPERANDIMODUS OPERANDI&#10;Third step:&#10;Start EU West and new US East DCs within the same&#10;cluster&#10;Replication factor in n...\" /></i></section><section data-index=\"31\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-31-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-31-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-31-1024.jpg?cb=1443906825\" alt=\"MODUS OPERANDIMODUS OPERANDI&#10;RF 3&#10;Clients&#10;RF 3:0:0:0RF 3:3:3:1&#10;US East Realtime&#10;EU West Realtime&#10;US East Analytics&#10;Rebuild...\" /></i></section><section data-index=\"32\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-32-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-32-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-32-1024.jpg?cb=1443906825\" alt=\"From 39d8f76d9cae11b4db405f5a002e2a4f6f764b1d Mon Sep 17 00:00:00 2001&#10;From: mario &lt;mario@gumgum.com&gt;&#10;Date: Wed, 17 Jun 20...\" /></i></section><section data-index=\"33\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-33-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-33-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-33-1024.jpg?cb=1443906825\" alt=\"RF 3:3:3:1&#10;MODUS OPERANDIMODUS OPERANDI&#10;Clients&#10;US East Realtime&#10;EU West Realtime&#10;US East Analytics&#10;RF 0:3:3:1&#10;\" /></i></section><section data-index=\"34\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-34-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-34-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-34-1024.jpg?cb=1443906825\" alt=\"MODUS OPERANDIMODUS OPERANDI&#10;Clients&#10;US East Realtime&#10;EU West Realtime&#10;US East Analytics&#10;RF 0:3:3:1&#10;RF 3:3:1Decomission&#10;\" /></i></section><section data-index=\"35\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-35-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-35-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-35-1024.jpg?cb=1443906825\" alt=\"TIPSTIPS&#10;\" /></i></section><section data-index=\"36\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-36-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-36-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-36-1024.jpg?cb=1443906825\" alt=\"TIPS - AUTOMATED MAINTENANCETIPS - AUTOMATED MAINTENANCE&#10;Maintenance in a multi-region C* cluster:&#10;Ansible + Cassandra mai...\" /></i></section><section data-index=\"37\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-37-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-37-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-37-1024.jpg?cb=1443906825\" alt=\"TIPS - SPARKTIPS - SPARK&#10;Number of workers above number of total C*&#10;nodes in analytics&#10;Each worker uses:&#10;1/4 number of cor...\" /></i></section><section data-index=\"38\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-38-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-38-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-38-1024.jpg?cb=1443906825\" alt=\"val conf = new SparkConf()&#10;.set(&quot;spark.cassandra.connection.host&quot;, cassandraNodes)&#10;.set(&quot;spark.cassandra.connection.local_...\" /></i></section><section data-index=\"39\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-39-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-39-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-39-1024.jpg?cb=1443906825\" alt=\"SINCE C* IN EU WEST ...SINCE C* IN EU WEST ...&#10;US West Datacenter!&#10;EU West DC US East DC Analytics DC US West DC&#10;\" /></i></section><section data-index=\"40\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/gumgum-multiregion-cassandra-in-aws\" data-small=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/85/gumgum-multiregion-cassandra-in-aws-40-320.jpg?cb=1443906825\" data-normal=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-40-638.jpg?cb=1443906825\" data-full=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-40-1024.jpg?cb=1443906825\" alt=\"Q&amp;AQ&amp;A&#10;GumGum is hiring!&#10;http://gumgum.com/careers&#10;\" /></i></section><div class=\"j-next-container next-container\"><div class=\"content-container\"><div class=\"next-slideshow-wrapper\"><div class=\"j-next-slideshow next-slideshow\"><p>Upcoming SlideShare</p></div><p>Loading in …5</p><p>×</p></div></div></div></div></div></div></div></div></div><div class=\"slideshow-info-container\" itemscope=\"itemscope\" itemtype=\"https://schema.org/MediaObject\"><div class=\"slideshow-tabs-container show-for-medium-up\"><ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    0 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\">\n                \n                  2 Likes\n                \n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\">\n                Statistics\n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\">\n                Notes\n              </i></a>\n            </li>\n          </ul><div class=\"tabs-content\"><div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\"><ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"itemscope\"><li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"jinwookBaek\" rel=\"nofollow\" href=\"https://www.slideshare.net/jinwookBaek?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            jinwook Baek\n                            \n                              \n                                , \n                                CTO\n                              \n                              \n                                 at \n                                JJS Media\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"balajibal\" rel=\"nofollow\" href=\"https://www.slideshare.net/balajibal?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Balaji Bal\n                            \n                              \n                                , \n                                Founder\n                              \n                              \n                                 at \n                                TeslaB\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n              </ul></div><div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No Downloads</p></div><div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No notes for slide</p></div></div></div><div class=\"notranslate transcript add-padding-right j-transcript\"><ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    CASSANDRA SUMMIT 2015CASSANDRA SUMMIT 2015\nMario Lazaro\nSeptember 24th 2015\n#CassandraSummit2015\nMULTI-REGION CASSANDRA IN AWSMULTI-REGION CASSANDRA IN AWS\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-2-638.jpg?cb=1443906825\" title=\"WHOAMIWHOAMI&#10;Mario Cerdan Lazaro&#10;Big Data Engineer&#10;Born and...\" target=\"_blank\">\n        2.\n      </a>\n    WHOAMIWHOAMI\nMario Cerdan Lazaro\nBig Data Engineer\nBorn and raised in Spain\nJoined GumGum 18 months ago\nAbout a year and a half experience with\nCassandra\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-3-638.jpg?cb=1443906825\" title=\"#5 Ad Platform in the U.S&#10;10B impressions / month&#10;2,000 bra...\" target=\"_blank\">\n        3.\n      </a>\n    #5 Ad Platform in the U.S\n10B impressions / month\n2,000 brand-safe premium\npublisher partners\n1B+ global unique visitors\nDaily inventory Impressions\nprocessed - 213M\nMonthly Image Impressions\nprocessed - 2.6B\n123 employees in seven ofﬁces\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-4-638.jpg?cb=1443906825\" title=\"AGENDAAGENDA&#10;Old cluster&#10;International Expansion&#10;Challenges...\" target=\"_blank\">\n        4.\n      </a>\n    AGENDAAGENDA\nOld cluster\nInternational Expansion\nChallenges\nTesting\nModus Operandi\nTips\nQuestions &amp; Answers\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-5-638.jpg?cb=1443906825\" title=\"25 Classic nodes cluster&#10;1 Region / 1 Rack hosted in&#10;AWS EC...\" target=\"_blank\">\n        5.\n      </a>\n    25 Classic nodes cluster\n1 Region / 1 Rack hosted in\nAWS EC2 US East\nVersion 2.0.8\nDatastax CQL driver\nGumGum's metadata including\nvisitors, images, pages, and ad\nperformance\nUsage: realtime data access\nand analytics (MR jobs)\nOLD C* CLUSTER - MARCH 2015OLD C* CLUSTER - MARCH 2015\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-6-638.jpg?cb=1443906825\" title=\"OLD C* CLUSTER - REALTIME USE CASEOLD C* CLUSTER - REALTIME...\" target=\"_blank\">\n        6.\n      </a>\n    OLD C* CLUSTER - REALTIME USE CASEOLD C* CLUSTER - REALTIME USE CASE\nBillions of rows\nHeavy read workload\n60/40\nTTLs everywhere - Tombstones\nHeavy and critical use of counters\nRTB - Read Latency constraints\n(total execution time ~50 ms)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-7-638.jpg?cb=1443906825\" title=\"OLD C* CLUSTER - ANALYTICS USE CASEOLD C* CLUSTER - ANALYTI...\" target=\"_blank\">\n        7.\n      </a>\n    OLD C* CLUSTER - ANALYTICS USE CASEOLD C* CLUSTER - ANALYTICS USE CASE\nDaily ETL jobs to extract / join data\nfrom C*\n​Hadoop MR jobs\nAdHoc queries with Presto\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-8-638.jpg?cb=1443906825\" title=\"INTERNATIONAL EXPANSIONINTERNATIONAL EXPANSION&#10;\" target=\"_blank\">\n        8.\n      </a>\n    INTERNATIONAL EXPANSIONINTERNATIONAL EXPANSION\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-9-638.jpg?cb=1443906825\" title=\"FIRST STEPSFIRST STEPS&#10;Start C* test datacenters in US&#10;East...\" target=\"_blank\">\n        9.\n      </a>\n    FIRST STEPSFIRST STEPS\nStart C* test datacenters in US\nEast &amp; EU West and test how C*\nmulti region works in AWS\nRun capacity/performance tests.\nWe expect 3x times more trafﬁc\nin 2015 Q4\nFIRST THOUGHTSFIRST THOUGHTS\nUse AWS Virtual Private\nCloud (VPC)\nCassandra &amp; VPC present\nsome connectivity\nissues challenges\nReplicate entire data\nwith same number of replicas\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-10-638.jpg?cb=1443906825\" title=\"TOO GOOD TO BE TRUE ...TOO GOOD TO BE TRUE ...&#10;\" target=\"_blank\">\n        10.\n      </a>\n    TOO GOOD TO BE TRUE ...TOO GOOD TO BE TRUE ...\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-11-638.jpg?cb=1443906825\" title=\"CHALLENGESCHALLENGES&#10;Problems between Cassandra in EC2&#10;Clas...\" target=\"_blank\">\n        11.\n      </a>\n    CHALLENGESCHALLENGES\nProblems between Cassandra in EC2\nClassic / VPC and Datastax Java\ndriver\nEC2MultiRegionSnitch uses public\nIPs. EC2 instances do not have an\ninterface with public IP address -\nCannot connect between instances in\nthe same region using Public IPs\n/**\n* Implementation of {@link AddressTranslater} used by the driver that\n* translate external IPs to internal IPs.\n* @author Mario &lt;mario@gumgum.com&gt;\n*/\npublic class Ec2ClassicTranslater implements AddressTranslater {\nprivate static final Logger LOGGER = LoggerFactory.getLogger(Ec2Cla\nprivate ClusterService clusterService;\nprivate Cluster cluster;\nprivate List&lt;Instance&gt; publicDnss;\n@PostConstruct\npublic void build() {\npublicDnss = clusterService.getInstances(cluster);\n}\n/**\n* Translates a Cassandra {@code rpc_address} to another address if\n* &lt;p&gt;\n*\n* @param address the address of a node as returned by Cassandra.\n* @return {@code address} translated IP address of the source.\n*/\npublic InetSocketAddress translate(InetSocketAddress address) {\nfor (final Instance server : publicDnss) {\nif (server.getPublicIpAddress().equals(address.getHostStrin\nLOGGER.info(\"IP address: {} translated to {}\", address.\nreturn new InetSocketAddress(server.getPrivateIpAddress\n}\n}\nreturn null;\n}\npublic void setClusterService(ClusterService clusterService) {\nthis.clusterService = clusterService;\n}\npublic void setCluster(Cluster cluster) {\nthis.cluster = cluster;\n}\n}\nProblems between Cassandra in EC2\nClassic / VPC and Datastax Java\ndriver\nEC2MultiRegionSnitch uses public\nIPs. EC2 instances do not have an\ninterface with public IP address -\nCannot connect between instances in\nthe same region using Public IPs\nRegion to Region connectivity will use\npublic IPs - Trust those IPs or use\nsoftware/hardware VPN\nProblems between Cassandra in EC2\nClassic / VPC and Datastax Java\ndriver\nEC2MultiRegionSnitch uses public\nIPs. EC2 instances do not have an\ninterface with public IP address -\nCannot connect between instances in\nthe same region using Public IPs\nRegion to Region connectivity will use\npublic IPs - Trust those IPs or use\nsoftware/hardware VPN\nYour application needs to connect to\nC* using private IPs - Custom EC2\ntranslator\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-12-638.jpg?cb=1443906825\" title=\"Datastax Java Driver Load Balancing&#10;Multiple choices&#10;DCAwar...\" target=\"_blank\">\n        12.\n      </a>\n    Datastax Java Driver Load Balancing\nMultiple choices\nDCAware + TokenAware\nDatastax Java Driver Load Balancing\nMultiple choices\nDCAware + TokenAware + ?\nDatastax Java Driver Load Balancing\nMultiple choices\nCHALLENGES\n“ Clients in one AZ attempt to always communicate with C*\nnodes in the same AZ. We call this zone-aware connections. This\nfeature is built into , Netﬂix’s C* Java client library.Astyanax\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-13-638.jpg?cb=1443906825\" title=\"Zone Aware Connection:&#10;Webapps in 3 different AZs: 1A, 1B, ...\" target=\"_blank\">\n        13.\n      </a>\n    Zone Aware Connection:\nWebapps in 3 different AZs: 1A, 1B, and 1C\nC* datacenter spanning 3 AZs with 3 replicas\nCHALLENGESCHALLENGES\n1A\n1B\n1C\n1B1B\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-14-638.jpg?cb=1443906825\" title=\"We added it! - Rack/AZ awareness to&#10;TokenAware Policy&#10;CHALL...\" target=\"_blank\">\n        14.\n      </a>\n    We added it! - Rack/AZ awareness to\nTokenAware Policy\nCHALLENGESCHALLENGES\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-15-638.jpg?cb=1443906825\" title=\"CHALLENGESCHALLENGES&#10;Third Datacenter: Analytics&#10;Do not imp...\" target=\"_blank\">\n        15.\n      </a>\n    CHALLENGESCHALLENGES\nThird Datacenter: Analytics\nDo not impact realtime data access\nSpark on top of Cassandra\nSpark-Cassandra Datastax connector\nReplicate speciﬁc keyspaces\nLess nodes with larger disk space\nSettings are different\nEx: Bloom ﬁlter chance\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-16-638.jpg?cb=1443906825\" title=\"CHALLENGESCHALLENGES&#10;Third Datacenter: Analytics&#10;Cassandra ...\" target=\"_blank\">\n        16.\n      </a>\n    CHALLENGESCHALLENGES\nThird Datacenter: Analytics\nCassandra Only DC\nRealtime\nCassandra + Spark DC\nAnalytics\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-17-638.jpg?cb=1443906825\" title=\"CHALLENGESCHALLENGES&#10;Upgrade from 2.0.8 to 2.1.5&#10;Counters i...\" target=\"_blank\">\n        17.\n      </a>\n    CHALLENGESCHALLENGES\nUpgrade from 2.0.8 to 2.1.5\nCounters implementation is buggy in\npre-2.1 versions\n“ My code never has bugs. It just develops random\nunexpected features\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-18-638.jpg?cb=1443906825\" title=\"CHALLENGESCHALLENGES&#10;“ To choose, or not to choose VNodes. ...\" target=\"_blank\">\n        18.\n      </a>\n    CHALLENGESCHALLENGES\n“ To choose, or not to choose VNodes. That is the question.\n(M. Lazaro, 1990 - 2500)\nPrevious DC using Classic Nodes\nWorks with MR jobs\nComplexity for adding/removing nodes\nManual manage token ranges\nNew DCs will use VNodes\nApache Spark + Spark Cassandra Datastax\nconnector\nEasy to add/remove new nodes as trafﬁc\nincreases\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-19-638.jpg?cb=1443906825\" title=\"TESTINGTESTING&#10;\" target=\"_blank\">\n        19.\n      </a>\n    TESTINGTESTING\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-20-638.jpg?cb=1443906825\" title=\"TESTINGTESTING&#10;Testing requires creating and modifying&#10;many...\" target=\"_blank\">\n        20.\n      </a>\n    TESTINGTESTING\nTesting requires creating and modifying\nmany C* nodes\nCreate and conﬁguring a C* cluster\nis time-consuming / repetitive task\nCreate fully automated process for\ncreating/modifying/destroying\nCassandra clusters with Ansible\n# Ansible settings for provisioning the EC2 instance\n---\nec2_instance_type: r3.2xlarge\nec2_count:\n- 0 # How many in us-east-1a ?\n- 7 # How many in us-east-1b ?\nec2_vpc_subnet:\n- undefined\n- subnet-c51241b2\n- undefined\n- subnet-80f085d9\n- subnet-f9138cd2\nec2_sg:\n- va-ops\n- va-cassandra-realtime-private\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-21-638.jpg?cb=1443906825\" title=\"TESTING - PERFORMANCETESTING - PERFORMANCE&#10;Performance test...\" target=\"_blank\">\n        21.\n      </a>\n    TESTING - PERFORMANCETESTING - PERFORMANCE\nPerformance tests using new\nCassandra 2.1 Stress Tool:\nRecreate GumGum metadata /\nschemas\nRecreate workload and make\nit 3 times bigger\nTry to ﬁnd limits / Saturate\nclients\n# Keyspace Name\nkeyspace: stresscql\n#keyspace_definition: |\n# CREATE KEYSPACE stresscql WITH replication = {'class': #'\n### Column Distribution Specifications ###\ncolumnspec:\n- name: visitor_id\nsize: gaussian(32..32) #domain names are relatively\npopulation: uniform(1..999M) #10M possible domains to pic\n- name: bidder_code\ncluster: fixed(5)\n- name: bluekai_category_id\n- name: bidder_custom\nsize: fixed(32)\n- name: bidder_id\nsize: fixed(32)\n- name: bluekai_id\nsize: fixed(32)\n- name: dt_pd\n- name: rt_exp_dt\n- name: rt_opt_out\n### Batch Ratio Distribution Specifications ###\ninsert:\npartitions: fixed(1) # Our partition key is the v\nselect: fixed(1)/5 # We have 5 bidder_code per dom\nbatchtype: UNLOGGED # Unlogged batches\n#\n# A list of queries you wish to run against the schema\n#\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-22-638.jpg?cb=1443906825\" title=\"TESTING - PERFORMANCETESTING - PERFORMANCE&#10;Main worry:&#10;Late...\" target=\"_blank\">\n        22.\n      </a>\n    TESTING - PERFORMANCETESTING - PERFORMANCE\nMain worry:\nLatency and replication overseas\nUse LOCAL_X consistency levels in your client\nOnly one C* node will contact only one C* node in a\ndifferent DC for sending replicas/mutations\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-23-638.jpg?cb=1443906825\" title=\"TESTING - PERFORMANCETESTING - PERFORMANCE&#10;Main worries:&#10;La...\" target=\"_blank\">\n        23.\n      </a>\n    TESTING - PERFORMANCETESTING - PERFORMANCE\nMain worries:\nLatency\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-24-638.jpg?cb=1443906825\" title=\"TESTING - INSTANCE TYPETESTING - INSTANCE TYPE&#10;Test all kin...\" target=\"_blank\">\n        24.\n      </a>\n    TESTING - INSTANCE TYPETESTING - INSTANCE TYPE\nTest all kind of instance types. We decided to\ngo with r3.2xlarge machines for our cluster:\n60 GB RAM\n8 Cores\n160GB Ephemeral SSD Storage for commit logs and\nsaved caches\nRAID 0 over 4 SSD EBS Volumes for data\nPerformance / Cost and GumGum use\ncase makes r3.2xlarge the best option\nDisclosure: I2 instance family is the best if\nyou can afford it\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-25-638.jpg?cb=1443906825\" title=\"TESTING - UPGRADETESTING - UPGRADE&#10;Upgrade C* Datacenter fr...\" target=\"_blank\">\n        25.\n      </a>\n    TESTING - UPGRADETESTING - UPGRADE\nUpgrade C* Datacenter from 2.0.8 to 2.1.5\nBoth versions can cohabit in the same DC\nNew settings and features tried\n​DateTieredCompactionStrategy:\nCompaction for Time Series Data\nIncremental repairs\nCounters new architecture\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-26-638.jpg?cb=1443906825\" title=\"MODUS OPERANDIMODUS OPERANDI&#10;\" target=\"_blank\">\n        26.\n      </a>\n    MODUS OPERANDIMODUS OPERANDI\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-27-638.jpg?cb=1443906825\" title=\"MODUS OPERANDIMODUS OPERANDI&#10;Sum up&#10;From: One cluster / One...\" target=\"_blank\">\n        27.\n      </a>\n    MODUS OPERANDIMODUS OPERANDI\nSum up\nFrom: One cluster / One DC in US East\nTo: One cluster / Two DCs in US East and one DC\nin EU West\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-28-638.jpg?cb=1443906825\" title=\"MODUS OPERANDIMODUS OPERANDI&#10;First step:&#10;Upgrade old cluste...\" target=\"_blank\">\n        28.\n      </a>\n    MODUS OPERANDIMODUS OPERANDI\nFirst step:\nUpgrade old cluster snitch from EC2Snitch to\nEC2MultiRegionSnitch\nUpgrade clients to handle it (aka translators)\nMake sure your clients do not lose connection to upgraded\nC* nodes (JIRA DataStax - )JAVA-809\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-29-638.jpg?cb=1443906825\" title=\"MODUS OPERANDIMODUS OPERANDI&#10;Second step:&#10;Upgrade old datac...\" target=\"_blank\">\n        29.\n      </a>\n    MODUS OPERANDIMODUS OPERANDI\nSecond step:\nUpgrade old datacenter from 2.0.8 to 2.1.5\nnodetool upgradesstables (multiple nodes at a time)\nNot possible to rebuild a 2.1.X C* node from a 2.0.X C*\ndatacenter.\nrebuild\nWARN [Thread-12683] 2015-06-17 10:17:22,845 IncomingTcpConnection.java:91 -\nUnknownColumnFamilyException reading from socket;\nclosing org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=XXX\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-30-638.jpg?cb=1443906825\" title=\"MODUS OPERANDIMODUS OPERANDI&#10;Third step:&#10;Start EU West and ...\" target=\"_blank\">\n        30.\n      </a>\n    MODUS OPERANDIMODUS OPERANDI\nThird step:\nStart EU West and new US East DCs within the same\ncluster\nReplication factor in new DCs: 0\nUse dc_sufﬁx to differentiate new Virginia DC from old one\nClients do not talk to new DCs. Only C* knows they exist\nReplication factor to 3 on all except analytics 1\n​​Start receiving new data\nNodetool rebuild &lt;old-datacenter&gt;\n​Old data\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-31-638.jpg?cb=1443906825\" title=\"MODUS OPERANDIMODUS OPERANDI&#10;RF 3&#10;Clients&#10;RF 3:0:0:0RF 3:3:...\" target=\"_blank\">\n        31.\n      </a>\n    MODUS OPERANDIMODUS OPERANDI\nRF 3\nClients\nRF 3:0:0:0RF 3:3:3:1\nUS East Realtime\nEU West Realtime\nUS East Analytics\nRebuild\nRebuild\nRebuild\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-32-638.jpg?cb=1443906825\" title=\"From 39d8f76d9cae11b4db405f5a002e2a4f6f764b1d Mon Sep 17 00...\" target=\"_blank\">\n        32.\n      </a>\n    From 39d8f76d9cae11b4db405f5a002e2a4f6f764b1d Mon Sep 17 00:00:00 2001\nFrom: mario &lt;mario@gumgum.com&gt;\nDate: Wed, 17 Jun 2015 14:21:32 -0700\nSubject: [PATCH] AT-3576 Start using new Cassandra realtime cluster\n---\nsrc/main/java/com/gumgum/cassandra/Client.java | 30 ++++------------------\n.../com/gumgum/cassandra/Ec2ClassicTranslater.java | 30 ++++++++++++++--------\nsrc/main/java/com/gumgum/cluster/Cluster.java | 3 ++-\n.../resources/applicationContext-cassandra.xml | 13 ++++------\nsrc/main/resources/dev.properties | 2 +-\nsrc/main/resources/eu-west-1.prod.properties | 3 +++\nsrc/main/resources/prod.properties | 3 +--\nsrc/main/resources/us-east-1.prod.properties | 3 +++\n.../CassandraAdPerformanceDaoImplTest.java | 2 --\n.../asset/cassandra/CassandraImageDaoImplTest.java | 2 --\n.../CassandraExactDuplicatesDaoTest.java | 2 --\n.../com/gumgum/page/CassandraPageDoaImplTest.java | 2 --\n.../cassandra/CassandraVisitorDaoImplTest.java | 2 --\n13 files changed, 39 insertions(+), 58 deletions(-)\nMODUS OPERANDIMODUS OPERANDI\nStart using new Cassandra DCs\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-33-638.jpg?cb=1443906825\" title=\"RF 3:3:3:1&#10;MODUS OPERANDIMODUS OPERANDI&#10;Clients&#10;US East Rea...\" target=\"_blank\">\n        33.\n      </a>\n    RF 3:3:3:1\nMODUS OPERANDIMODUS OPERANDI\nClients\nUS East Realtime\nEU West Realtime\nUS East Analytics\nRF 0:3:3:1\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-34-638.jpg?cb=1443906825\" title=\"MODUS OPERANDIMODUS OPERANDI&#10;Clients&#10;US East Realtime&#10;EU We...\" target=\"_blank\">\n        34.\n      </a>\n    MODUS OPERANDIMODUS OPERANDI\nClients\nUS East Realtime\nEU West Realtime\nUS East Analytics\nRF 0:3:3:1\nRF 3:3:1Decomission\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-35-638.jpg?cb=1443906825\" title=\"TIPSTIPS&#10;\" target=\"_blank\">\n        35.\n      </a>\n    TIPSTIPS\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-36-638.jpg?cb=1443906825\" title=\"TIPS - AUTOMATED MAINTENANCETIPS - AUTOMATED MAINTENANCE&#10;Ma...\" target=\"_blank\">\n        36.\n      </a>\n    TIPS - AUTOMATED MAINTENANCETIPS - AUTOMATED MAINTENANCE\nMaintenance in a multi-region C* cluster:\nAnsible + Cassandra maintenance keyspace +\nemail report = zero human intervention!\nCREATE TABLE maintenance.history (\ndc text,\nop text,\nts timestamp,\nip text,\nPRIMARY KEY ((dc, op), ts)\n) WITH CLUSTERING ORDER BY (ts ASC) AND\nbloom_filter_fp_chance=0.010000 AND\ncaching='{\"keys\":\"ALL\", \"rows_per_partition\":\"NONE\"}' AND\ncomment='' AND\ndclocal_read_repair_chance=0.100000 AND\ngc_grace_seconds=864000 AND\nread_repair_chance=0.000000 AND\ncompaction={'class': 'SizeTieredCompactionStrategy'} AND\ncompression={'sstable_compression': 'LZ4Compressor'};\nCREATE INDEX history_kscf_idx ON maintenance.history (kscf);\n3-133-65:/opt/scripts/production/groovy$ groovy CassandraMaintenanceCheck.groovy -dc us-east-va-realtime -op compaction -e mari\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-37-638.jpg?cb=1443906825\" title=\"TIPS - SPARKTIPS - SPARK&#10;Number of workers above number of ...\" target=\"_blank\">\n        37.\n      </a>\n    TIPS - SPARKTIPS - SPARK\nNumber of workers above number of total C*\nnodes in analytics\nEach worker uses:\n1/4 number of cores of each instance\n1/3 total available RAM of each instance\nCassandra-Spark connector\n​SpanBy\n.joinWithCassandraTable(:x, :y)\nSpark.cassandra.output.batch.size.bytes\nSpark.cassandra.output.concurrent.writes\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-38-638.jpg?cb=1443906825\" title=\"val conf = new SparkConf()&#10;.set(&quot;spark.cassandra.connection...\" target=\"_blank\">\n        38.\n      </a>\n    val conf = new SparkConf()\n.set(\"spark.cassandra.connection.host\", cassandraNodes)\n.set(\"spark.cassandra.connection.local_dc\", \"us-east-va-analytics\")\n.set(\"spark.cassandra.connection.factory\", \"com.gumgum.spark.bluekai.DirectLinkConnectionFactory\")\n.set(\"spark.driver.memory\",\"4g\")\n.setAppName(\"Cassandra presidential candidates app\")\nTIPS - SPARKTIPS - SPARK\nCreate \"translator\" if using EC2MultiRegionSnitch\nSpark.cassandra.connection.factory\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-39-638.jpg?cb=1443906825\" title=\"SINCE C* IN EU WEST ...SINCE C* IN EU WEST ...&#10;US West Data...\" target=\"_blank\">\n        39.\n      </a>\n    SINCE C* IN EU WEST ...SINCE C* IN EU WEST ...\nUS West Datacenter!\nEU West DC US East DC Analytics DC US West DC\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891/95/gumgum-multiregion-cassandra-in-aws-40-638.jpg?cb=1443906825\" title=\"Q&amp;AQ&amp;A&#10;GumGum is hiring!&#10;http://gumgum.com/careers&#10;\" target=\"_blank\">\n        40.\n      </a>\n    Q&amp;AQ&amp;A\nGumGum is hiring!\nhttp://gumgum.com/careers\n \n  </li>\n              </ol></div></div></div><aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\"><dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\"><ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"53500779\" title=\"Test Prep: GRE\" href=\"https://www.linkedin.com/learning/test-prep-gre?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Test Prep: GRE\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Test Prep: GRE\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=zRN9cYKLcI5pMFN7G1fZgp5AHQw%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lXSSj-9OfZHHocMbZZLSiol8QcS4DkQA2e-ahSTLnEI69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Test Prep: GRE</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"53500779\" title=\"PowerPoint for Teachers: Creating Interactive Lessons\" href=\"https://www.linkedin.com/learning/powerpoint-for-teachers-creating-interactive-lessons?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_PowerPoint for Teachers: Creating Interactive Lessons\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"PowerPoint for Teachers: Creating Interactive Lessons\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=NrZ6QSg%2BBWntE4IYVUxrWwdlQFs%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lXCeu-NKfZHDrfcXYZLSiol4QfyoDmQEyfuerQzPmFY69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>PowerPoint for Teachers: Creating Interactive Lessons</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"53500779\" title=\"Communication in the 21st Century Classroom\" href=\"https://www.linkedin.com/learning/communication-in-the-21st-century-classroom?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Communication in the 21st Century Classroom\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Communication in the 21st Century Classroom\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=jpGZZw5ymAXBiG%2B8nbN88R%2BVVZo%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lWCWq_defZHTpecDdZLSiol4QfyoDlg00fuutSDPoEo69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Communication in the 21st Century Classroom</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"53858704\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Multi-Region Cassandra Clusters\" href=\"https://www.slideshare.net/Instaclustr/cassandra-multi-region-clusters-cassandra-summit-2014\">\n    \n    <div class=\"related-content\"><p>Multi-Region Cassandra Clusters</p><p>Instaclustr</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"77306108\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Forrester CXNYC 2017 - Delivering great real-time cx is a true craft\" href=\"https://www.slideshare.net/planetcassandra/forrester-cxnyc-2017-delivering-great-realtime-cx-is-a-true-craft\">\n    \n    <div class=\"related-content\"><p>Forrester CXNYC 2017 - Delivering great real-time cx is a true craft</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64947458\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Introduction to DataStax Enterprise Graph Database\" href=\"https://www.slideshare.net/planetcassandra/introduction-to-datastax-enterprise-graph-database\">\n    \n    <div class=\"related-content\"><p>Introduction to DataStax Enterprise Graph Database</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64947204\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Introduction to DataStax Enterprise Advanced Replication with Apache Cassandra\" href=\"https://www.slideshare.net/planetcassandra/introduction-to-datastax-enterprise-advanced-replication-with-apache-cassandra\">\n    \n    <div class=\"related-content\"><p>Introduction to DataStax Enterprise Advanced Replication with Apache Cassandra</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64223739\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Cassandra on Docker @ Walmart Labs\" href=\"https://www.slideshare.net/planetcassandra/cassandra-on-docker-walmart-labs\">\n    \n    <div class=\"related-content\"><p>Cassandra on Docker @ Walmart Labs</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64222983\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Cassandra 3.0 Data Modeling\" href=\"https://www.slideshare.net/planetcassandra/cassandra-30-data-modeling\">\n    \n    <div class=\"related-content\"><p>Cassandra 3.0 Data Modeling</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64222866\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Cassandra Adoption on Cisco UCS &amp; Open stack\" href=\"https://www.slideshare.net/planetcassandra/cassandra-adoption-on-cisco-ucs-open-stack\">\n    \n    <div class=\"related-content\"><p>Cassandra Adoption on Cisco UCS &amp; Open stack</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div></div><footer>\n          <div class=\"row\"><div class=\"columns\"><ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div></div>\n          \n          <div class=\"row\"><div class=\"columns\"><p class=\"copyright text-center\">LinkedIn Corporation © 2018</p></div></div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div>\n    <div class=\"row\"><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></div>\n        \n    </div>\n    \n    \n      <noscript>\n    </noscript>",
        "created_at": "2018-01-12T19:57:23+0000",
        "updated_at": "2018-05-13T16:16:34+0000",
        "published_at": null,
        "published_by": [
          "DataStax Academy"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 10,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/cassandra-summit-2015-gumgum6-151003201902-lva1-app6891-thumbnail-4.jpg?cb=1443906825",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9036"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 15,
            "label": "tutorial",
            "slug": "tutorial"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          }
        ],
        "is_public": false,
        "id": 9035,
        "uid": null,
        "title": "Multi-Region Cassandra in AWS | DataStax Academy: Free Cassandra Tutorials and Training",
        "url": "https://academy.datastax.com/resources/Multi-Region-Cassandra-in-AWS",
        "content": "<p><a href=\"https://academy.datastax.com/\">Home</a> » <a href=\"https://academy.datastax.com/presentations\">Presentations</a> » Multi-Region Cassandra in AWS</p><div class=\"container\"><div class=\"row\"><div class=\"col-md-12\"><h3>About this Presentation</h3><p>GumGum relies heavily on Cassandra for storing different kinds of metadata. Currently GumGum reaches 1 billion unique visitors per month using 3 Cassandra datacenters in Amazon Web Services (AWS) spread across the globe. This presentation details how we scaled out from one local Cassandra datacenter to a multi-datacenter Cassandra cluster and all the problems we encountered and choices we made while implementing it.&#13;\n &#13;\nHow did we architect multi-region Cassandra in AWS? What were our experiences in implementing multi-datacenter Cassandra? How did we achieve low latency with multi-region Cassandra and the Datastax Driver? What are the different Cassandra use cases at GumGum? How did we integrate our Cassandra with Spark?\"</p></div></div></div>",
        "created_at": "2018-01-12T19:56:55+0000",
        "updated_at": "2018-05-13T16:16:46+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "academy.datastax.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9035"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 51,
            "label": "blog",
            "slug": "blog"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 883,
            "label": "java",
            "slug": "java"
          },
          {
            "id": 963,
            "label": "akka",
            "slug": "akka"
          },
          {
            "id": 964,
            "label": "scala",
            "slug": "scala"
          }
        ],
        "is_public": false,
        "id": 8990,
        "uid": null,
        "title": "Stuff & Information",
        "url": "http://batey.info/",
        "content": "<h3> I'm interested in and some times blog about: </h3><ul><li><a href=\"http://batey.info/functional-scala\">An introduction to functional Scala</a></li>\n        <li><a href=\"http://batey.info/jvm\">Java and the JVM</a></li>\n        <li><a href=\"http://batey.info/cassandra\">Cassandra</a></li>\n        <li><a href=\"http://batey.info/performance\">Performance and Monitoring</a></li>\n        <li><a href=\"http://batey.info/nerd\">General nerdery</a></li>\n        \n    </ul><h3>Slides for the talks I've given recently</h3><ul><li><a href=\"http://batey.info/talks/akka-flowcontrol\">Building responsive, back pressured\n          services with Akka</a></li>\n    </ul><p>Or you can view all posts <a href=\"http://batey.info/all\">here</a>\n    or subscribe <a href=\"http://batey.info/feed.xml\">via RSS</a></p><br />",
        "created_at": "2018-01-05T22:13:48+0000",
        "updated_at": "2018-01-05T22:14:01+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 0,
        "domain_name": "batey.info",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/8990"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 217,
            "label": "tool",
            "slug": "tool"
          },
          {
            "id": 235,
            "label": "rest",
            "slug": "rest"
          },
          {
            "id": 883,
            "label": "java",
            "slug": "java"
          }
        ],
        "is_public": false,
        "id": 8989,
        "uid": null,
        "title": "cyngn/ChronoServer",
        "url": "https://github.com/cyngn/ChronoServer",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\"><p><a href=\"https://travis-ci.org/cyngn/ChronoServer\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/b7af716b6bb1fb518d07b96d337300413cd06a3d/68747470733a2f2f7472617669732d63692e6f72672f63796e676e2f4368726f6e6f5365727665722e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/cyngn/ChronoServer.svg?branch=master\" /></a></p>\n<p>A test server for sampling how long it takes mobile &amp; web clients to make various types of requests to a server doing common request patterns. The idea is you can set this server up in various locations (think DCs) around the globe or country and test what the latency and experience for users will be as you vary the size and style of requests. You can also use the server to see how different CDNs might perform when sitting in front of your server and caching content.</p>\n<p>The basic flow is as follows:</p>\n<ul><li>Client gets a collection of URLs to call from server, a mix of GET &amp; POST calls.</li>\n<li>Client executes the URLs timing each one.</li>\n<li>Client reports back to server all the timing data for each call and metadata about the client's connection.</li>\n</ul><h2><a href=\"#getting-started\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-getting-started\"></a>Getting Started</h2>\n<p>####Clone the repo &amp; build</p>\n<div class=\"highlight highlight-source-shell\"><pre>./gradlew clean shadowJar</pre></div>\n<p>####Setup the Cassandra DB</p>\n<p>Options:</p>\n<ul><li>run standalone</li>\n<li>run ccm</li>\n<li>point at your internal cluster</li>\n</ul><p>Execute the scheme file in <strong>db/scheme.cql</strong> on your setup.</p>\n<p>####Configuration</p>\n<p>The server takes the following config</p>\n<div class=\"highlight highlight-source-json\"><pre>{\n    \"port\" : &lt;port&gt;,\n    \"default_test_base_url\" : \"&lt;default http://localhost&gt;\",\n    \"data_retention_seconds\" : &lt;default 86400&gt;,\n    \"cassandra\": {\n        \"seeds\": [\"localhost\"],\n        \"reconnect\": {\n            \"name\": \"exponential\",\n            \"base_delay\": 1000,\n            \"max_delay\": 10000\n        }\n    }\n}</pre></div>\n<p>For example:</p>\n<div class=\"highlight highlight-source-json\"><pre>{\n    \"port\" : 7345,\n    \"default_test_base_url\" : \"http://localhost\",\n    \"data_retention_seconds\" : 86400,\n    \"cassandra\": {\n        \"seeds\": [\"localhost\"],\n        \"reconnect\": {\n            \"name\": \"exponential\",\n            \"base_delay\": 1000,\n            \"max_delay\": 10000\n        }\n    }\n}</pre></div>\n<p>Field breakdown:</p>\n<ul><li><code>port</code> the port the server listens on</li>\n<li><code>default_test_base_url</code> the base url used in creating the bootstrap sample data</li>\n<li><code>data_retention_seconds</code> the seconds to allow the upload test data from users to live in the DB, this defaults to a day.</li>\n<li><code>cassandra</code> The config to initialize the Cassandra driver with, see <a href=\"https://github.com/englishtown/vertx-cassandra\">vertx-cassandra</a> for more details.</li>\n</ul><p>####Running Server</p>\n<div class=\"highlight highlight-source-shell\"><pre>#start the server\njava -jar build/libs/ChronoServer-0.5.0-fat.jar -conf conf.json -instances 2</pre></div>\n<h2><a href=\"#rest-apis\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-rest-apis\"></a>REST APIs</h2>\n<h4><a href=\"#apiv1test_endpoints---get\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-apiv1test_endpoints---get\"></a>/api/v1/test_endpoints - GET</h4>\n<p>Gets a set of URLs for the client to execute.</p>\n<p>Parameters:</p>\n<ul><li><code>test_batch</code> - a named collection of URLs you want to pass back to a client to run. Defaults to the bootstrap collection if not specified.</li>\n</ul><p>Response:</p>\n<p>See <a href=\"https://github.com/cyngn/ChronoServer/blob/master/src/main/java/com/cyngn/chrono/http/ConfigResponse.java\">ConfigResponse.java</a></p>\n<p>####/api/v1/report - POST</p>\n<p>Allows a user to upload detailed information on the timings for all the calls it made.</p>\n<p>Parameters:</p>\n<p>See <a href=\"https://github.com/cyngn/ChronoServer/blob/master/src/main/java/com/cyngn/chrono/storage/entity/MetricReport.java\">MetricReport.java</a></p>\n<p>####/api/v1/timing - GET\nSub APIs</p>\n<ul><li>/static - grabs <code>N</code> data from memory</li>\n<li>/static_cached - grabs <code>N</code> data from memory and puts the appropriate cache headers on</li>\n<li>/dynamic - grabs <code>N</code> data from the DB</li>\n<li>/dynamic_cached -  grabs <code>N</code> data from the DB and puts the appropriate cache headers on</li>\n</ul><p>Required Common Parameters:</p>\n<ul><li><code>unit</code> - the unit of the data to get, either <code>kb</code> or <code>mb</code></li>\n<li><code>size</code> - the size of data to get back</li>\n</ul><p>Response:</p>\n<div class=\"highlight highlight-source-json\"><pre>{\n  \"data\" : \"&lt;a bunch of random data matching your parameters&gt;\"\n}</pre></div>\n<p>####/api/v1/timing - POST\nSub APIs</p>\n<ul><li>/store - upload <code>N</code> amount data to the DB, server returns after it is successfully written to Cassandra</li>\n<li>/store_mem - upload <code>N</code> amount data to server memory, server returns after it gets the full body of the post</li>\n</ul><p>Parameters:</p>\n<p>See <a href=\"https://github.com/cyngn/ChronoServer/blob/master/src/main/java/com/cyngn/chrono/http/StorageRequest.java\">StorageRequest.java</a></p>\n<p>####/healthcheck - GET</p>\n<p>Standard health check for things like a LB to hit.</p>\n<p>####/api/v1/timing_streaming - SockJS interface</p>\n<p>See: <a href=\"https://github.com/cyngn/ChronoServer/blob/master/src/main/java/com/cyngn/chrono/http/streaming\">streaming namespace</a> and sample client below.</p>\n<h2><a href=\"#examples\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-examples\"></a>Examples</h2>\n<p>Piping curl output to something like <a href=\"https://github.com/jmhodges/jsonpp\">jsonpp</a> makes it much more readable.</p>\n<p>###curl requests</p>\n<div class=\"highlight highlight-source-shell\"><pre>#start the server\njava -jar build/libs/ChronoServer-0.5.0-fat.jar -conf conf.json -instances 2\n#grab a sample config package, take note of the api_key\ncurl -v \"http://localhost:7345/api/v1/test_endpoints\" | jsonpp\n#grab static data with no caching\ncurl -v --header \"X-API-Key: [api_key here]\" \"http://localhost:7345/api/v1/timing/static?unit=kb&amp;size=1\" | jsonpp\n#grab static data with caching\ncurl -v --header \"X-API-Key: [api_key here]\" \"http://localhost:7345/api/v1/timing/static_cached?unit=kb&amp;size=1\" | jsonpp\n#grab data from the DB with no caching\ncurl -v --header \"X-API-Key: [api_key here]\" \"http://localhost:7345/api/v1/timing/dynamic?unit=kb&amp;size=1\" | jsonpp\n#grab data from the DB with caching\ncurl -v --header \"X-API-Key: [api_key here]\" \"http://localhost:7345/api/v1/timing/dynamic_cached?unit=kb&amp;size=1\" | jsonpp</pre></div>\n<p>For testing POST calls via the store api <a href=\"https://www.getpostman.com/\" rel=\"nofollow\">Postman</a> is great for this.</p>\n<p>###sample SockJS client</p>\n<div class=\"highlight highlight-text-html-basic\"><pre>&lt;html&gt;\n    &lt;script src=\"http:////cdn.jsdelivr.net/sockjs/0.3.4/sockjs.min.js\"&gt;&lt;/script&gt;\n    &lt;script src=\"vertxbus-3.0.0.js\"&gt;&lt;/script&gt;\n    &lt;!-- You're going to want to turn off security in your browser when running this probably --&gt;\n    &lt;script&gt;\n        // this is acquired from the server\n        var apiKey = \"\"\n        var sock = new SockJS('http://localhost:7345/api/v1/timing_streaming');\n        // download via socket\n        var getData = function(address, unit, size) {\n            sock.send(JSON.stringify({address: address, \"api_key\": apiKey,\n                        body : {unit : unit, size: size}}));\n        };\n        // helper func for generating fake data\n        var getKb = function(number) {\n            var data = ['a', 'b', 'c', 'd', 'e', '1', '2', '3', '4', '5'];\n            var str = \"\";\n            var totalKb = 1024 * number;\n            for(var i = 0; i &lt; totalKb; i++) {\n                str += data[Math.floor((Math.random() * 10))];\n            }\n            return str;\n        };\n        // upload via socket\n        var putData = function(size) {\n            sock.send(JSON.stringify({address: \"streaming.store\", \"api_key\": apiKey,\n                        body : {test_batch: \"some_batch\", unit : \"kb\", size: size, data: getKb(size)}}));\n        };\n        // get the config, ie api key\n        var getApiKey = function() {\n            sock.send(JSON.stringify({address: \"streaming.config\", \"api_key\": \"\", body : {}}));\n        }\n        sock.onopen = function() {\n            console.log('connected to server');\n            getApiKey();\n        };\n        sock.onmessage = function(e) {\n            console.log('message', e.data);\n            var data = JSON.parse(e.data);\n            // once we have the api key make some test calls\n            if(data.type == \"streaming.config\") {\n                apiKey = data.api_key;\n                getData('streaming.static', \"kb\", 1);\n                getData('streaming.dynamic', \"kb\", 5);\n                putData(5);\n            }\n        };\n        sock.onclose = function() {\n            console.log('lost connection to server');\n        };\n    &lt;/script&gt;\n    &lt;body&gt;\n    &lt;/body&gt;\n&lt;/html&gt;</pre></div>\n</article>",
        "created_at": "2018-01-05T22:02:43+0000",
        "updated_at": "2018-01-05T22:05:13+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 4,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/4184248?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/8989"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          },
          {
            "id": 869,
            "label": "devops",
            "slug": "devops"
          }
        ],
        "is_public": false,
        "id": 8911,
        "uid": null,
        "title": "LoyaltyOne/cassandra-aws",
        "url": "https://github.com/LoyaltyOne/cassandra-aws",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>This CloudFormation template deploys 3 node Cassandra cluster in a single AWS region\nacross multiple Availability Zones.</p>\n<p>Cassandra storage is backed by AWS Elastic File System. Each cassandra node maintains its own data folder on a single File System.\nThis file system spans across multiple availability zones for redundancy.</p>\n<h3><a href=\"#why-efs\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-why-efs\"></a>Why EFS?</h3>\n<ul><li>No need to resize volumes when data increases (automatically handled)</li>\n<li>Backups of volumes are less crucial since EFS is replicated across Availability Zones</li>\n<li>Performance is good enough for non-prod environments</li>\n</ul><p><strong>Note</strong> Using EFS to back a cassandra cluster is only suitable for development purposes.\nConsider using EBS volumes or instance storage for production grade setups.</p>\n<p>TODO</p>\n<p>To generate a keystore and truststore for use by cassandra for encryption use\nthe <code>truststore-setup</code> script.</p>\n<p><code>truststore-setup</code> does the following:</p>\n<ul><li>Generates a keystore with a user supplied password which contains a private key used by Cassandra to establish secure\ncommunication amongst nodes in the cluster</li>\n<li>Generates a truststore which contains the public cert (corresponding to the private key in the keystore) for the cluster</li>\n<li>Generates a client PEM file containing the public cert for the cluster intended to be used with <strong>cqlsh</strong> for secure client-to-node communication</li>\n<li>Updates cassandra.yaml with the user supplied password for the keystore and truststore</li>\n</ul><p>Run the truststore-setup script and specify the password and cluster name.</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ ./truststore-setup &lt;password&gt; &lt;clustername&gt;</pre></div>\n<p>This will generate the keystore and the truststore in <code>cassandra-config</code>:</p>\n<pre>cassandra-config/\n├── conf\n│   └── certs\n│       ├── cassandra.keystore\n│       └── cassandra.truststore\n...\n</pre>\n<p>For clients connecting to the cluster, the cassandra.truststore can be used to establish secure communication. This is a JKS based truststore.</p>\n<p>It will also generate the client pem in the root folder. This file can be used with <strong>cqlsh</strong></p>\n<pre>cluster-ca-certificate.pem\n</pre>\n<p>Sync the s3 bucket holding the Cassandra configuration and certs.</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ aws s3 sync cassandra-config s3://cassandra-configuration/&lt;clustername&gt;</pre></div>\n<p><strong>Note:</strong> You must name the CloudFormation stack exactly the same as <code>&lt;clustername&gt;</code> defined above in order for\nencryption to work correctly.</p>\n<p>For example, we have named our bucket <code>sandbox-cassandra-configuration</code> and the name of the cluster is <code>sandbox-cassandra</code>:\n<a href=\"https://user-images.githubusercontent.com/14280155/33042220-03f05ef0-ce0f-11e7-8729-e4a9798079f2.png\" target=\"_blank\"><img src=\"https://user-images.githubusercontent.com/14280155/33042220-03f05ef0-ce0f-11e7-8729-e4a9798079f2.png\" alt=\"image\" /></a></p>\n</article>",
        "created_at": "2017-12-25T20:30:34+0000",
        "updated_at": "2017-12-25T20:30:46+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/428106?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/8911"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 11,
            "label": "database",
            "slug": "database"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 8863,
        "uid": null,
        "title": "Upgrading to DataStax Enterprise 5.0 | DataStax Upgrade guide",
        "url": "http://docs.datastax.com/en/upgrade/doc/upgrade/datastax_enterprise/upgdDSE50.html",
        "content": "<p class=\"shortdesc\">Instructions to upgrade to DataStax Enterprise 5.0 from 4.7 or 4.8.</p><p class=\"p\">Follow these instructions to upgrade from DataStax Enterprise 4.7 or 4.8 to 5.0. If you have\n      an earlier version, upgrade to 4.8 before continuing.</p>\n    <div class=\"note attention\" readability=\"12\">Attention: You have probably seen the recommendation to\n      read all the instructions. This is a time when careful review of the upgrade instructions will\n      make a difference. By understanding what to do beforehand, you can ensure a smooth upgrade and\n      avoid pitfalls and frustrations.<p class=\"p\">Read and understand these instructions before\n        upgrading.</p>\n</div>\n    <section class=\"section\">On this page:<ul class=\"ul\" id=\"upgdDSE50__ul_ds2_4zr_g5\"><li class=\"li\"><a class=\"xref\" href=\"http://docs.datastax.com/en/upgrade/doc/upgrade/datastax_enterprise/upgdDSE50.html#upgdDSE50__cstar-version-change\">Apache Cassandra™ version change</a></li>\n      <li class=\"li\"><a class=\"xref\" href=\"http://docs.datastax.com/en/upgrade/doc/upgrade/datastax_enterprise/upgdDSE50.html#upgdDSE50__recommendations\">General recommendations</a></li>\n      <li class=\"li\"><a class=\"xref\" href=\"http://docs.datastax.com/en/upgrade/doc/upgrade/datastax_enterprise/upgdDSE50.html#upgdDSE50__restrictions\">General restrictions and limitations during the upgrade process</a></li>\n      <li class=\"li\"><a class=\"xref\" href=\"http://docs.datastax.com/en/upgrade/doc/upgrade/datastax_enterprise/upgdDSE50.html#upgdDSE50__prepUpg50\">Preparing to upgrade</a></li>\n      <li class=\"li\"><a class=\"xref\" href=\"http://docs.datastax.com/en/upgrade/doc/upgrade/datastax_enterprise/upgdDSE50.html#upgdDSE50__upg50\">Upgrade steps</a></li>\n      <li class=\"li\"><a class=\"xref\" href=\"http://docs.datastax.com/en/upgrade/doc/upgrade/datastax_enterprise/upgdDSE50.html#upgdDSE50__msgsIgnore\">Warning messages during and after upgrade</a></li>\n      </ul></section><section class=\"section\" id=\"upgdDSE50__cstar-version-change\" readability=\"1\"><h2 class=\"title sectiontitle\">Apache  Cassandra™ version change</h2>\n      \n      <div class=\"p\" readability=\"7\">Upgrading from DataStax Enterprise 4.7 or 4.8 to 5.0 includes a major Cassandra version\n        change. <ul class=\"ul\"><li class=\"li\">DataStax Enterprise 5.1 uses Cassandra 3.11.</li>\n          <li class=\"li\">DataStax Enterprise 5.0 uses Cassandra 3.0.</li>\n          <li class=\"li\">DataStax Enterprise 4.7 to 4.8 uses Cassandra 2.1.</li>\n          <li class=\"li\">DataStax Enterprise 4.0 to 4.6 uses Cassandra 2.0.</li>\n        </ul>\nBe sure to follow the recommendations for upgrading the SSTables.</div>\n    </section><section class=\"section\" id=\"upgdDSE50__recommendations\" readability=\"-20\"><h2 class=\"title sectiontitle\">General recommendations</h2>\n      \n      <p class=\"p\">DataStax recommends backing up your data prior to any version upgrade. A\n            backup provides the ability to revert and restore all the data used in the previous\n            version if necessary.</p>\n      <p class=\"p opsc-icon\">OpsCenter provides a\n             that manages enterprise-wide backup and\n            restore operations for DataStax Enterprise clusters.</p>\n    </section><section class=\"section\" id=\"upgdDSE50__restrictions\" readability=\"5\"><h2 class=\"title sectiontitle\">General restrictions and limitations during the upgrade process</h2>\n      \n      <p class=\"p\">Restrictions and limitations apply while a cluster is in a\n          <strong class=\"ph b\">partially upgraded</strong> state. </p>\n      <p class=\"p\">With these exceptions, the cluster continues to work as though it\n        were on the earlier version of DataStax Enterprise until all of the nodes in the cluster are\n        upgraded.</p>\n      <dl class=\"dl\"><dt class=\"dt dlterm\">General upgrade restrictions</dt>\n          <dd class=\"dd\">\n            <ul class=\"ul\" id=\"upgdDSE50__d11e840\"><li class=\"li\" id=\"upgdDSE50__d11e842\"><strong class=\"ph b\">Do not</strong> enable new features.</li>\n              <li class=\"li\" id=\"upgdDSE50__d11e847\">Do not run nodetool repair. If you have the OpsCenter\n                Repair Service configured, turn off the Repair\n                  Service. </li>\n              <li class=\"li\" id=\"upgdDSE50__d11e853\">During the upgrade, do not bootstrap or decommission nodes. </li>\n              <li class=\"li\" id=\"upgdDSE50__d11e856\">Do not issue these types of CQL queries during a rolling restart:\n                  <code data-swiftype-name=\"codeph\" data-swiftype-type=\"text\" class=\"ph codeph\">DDL</code> and <code data-swiftype-name=\"codeph\" data-swiftype-type=\"text\" class=\"ph codeph\">TRUNCATE</code>.</li>\n              <li class=\"li\" id=\"upgdDSE50__d11e865\">During the upgrade, the nodes on different versions might show a schema\n                disagreement.</li>\n              <li class=\"li\" id=\"upgdDSE50__d11e869\">Failure to upgrade SSTables when required results in a significant performance\n                impact and increased disk usage. Upgrading is not complete until the SSTables are\n                upgraded.</li>\n            </ul></dd>\n        \n        \n          <dt class=\"dt dlterm\">Restrictions for DSE Analytic (Hadoop and Spark) nodes</dt>\n          <dd class=\"dd\">\n            <ul class=\"ul\" id=\"upgdDSE50__d11e882\"><li class=\"li\">Do not run analytics jobs until all nodes are upgraded.</li>\n              <li class=\"li\">Kill all Spark worker processes before you stop the node and install the new\n                version.</li>\n            </ul></dd>\n        \n        \n          <dt class=\"dt dlterm\" id=\"upgdDSE50__upgrdSrchLim\">DSE Search (Solr) upgrade restrictions and limitations</dt>\n          <dd class=\"dd\">\n            <ul class=\"ul\" id=\"upgdDSE50__ul_w1g_vgf_cs\"><li class=\"li\">Do not update schemas.</li>\n              <li class=\"li\">Do not reindex DSE Search nodes during upgrade.</li>\n              <li class=\"li\">Do not issue these types of queries during a rolling restart: <code data-swiftype-name=\"codeph\" data-swiftype-type=\"text\" class=\"ph codeph\">DDL</code>\n                or <code data-swiftype-name=\"codeph\" data-swiftype-type=\"text\" class=\"ph codeph\">TRUNCATE</code>.</li>\n              <li class=\"li\">While mixed versions of nodes exist during an upgrade, DataStax Enterprise runs\n                two different servers for backward compatibility. One based on\n                shard_transport_options, the other based on internode_messaging_options. (These\n                options are located in dse.yaml.) After all nodes are upgraded\n                to 5.0, internode_messaging_options are used. The <a class=\"xref\" href=\"http://docs.datastax.com/en/datastax_enterprise/5.0/datastax_enterprise/config/configDseYaml.html#configDseYaml__interNode\" target=\"_blank\"> internode_messaging_options</a> are used by\n                several components of DataStax Enterprise. For 5.0 and later, all internode\n                messaging requests use this service.</li>\n            </ul></dd>\n        \n        \n          <dt class=\"dt dlterm\">Restrictions for nodes using any kind of security</dt>\n          <dd class=\"dd\">\n            <ul class=\"ul\" id=\"upgdDSE50__d11e955\"><li class=\"li\">Do not change security credentials or permissions until after the upgrade is\n                complete.</li>\n              <li class=\"li\">If you are not already using Kerberos, do not set up Kerberos authentication\n                before upgrading. First upgrade the cluster, and then set up Kerberos.</li>\n            </ul></dd>\n        \n        \n          <dt class=\"dt dlterm\">Upgrading drivers and possible impact when driver versions are incompatible</dt>\n          <dd class=\"dd\">Be sure to check <a class=\"xref\" href=\"http://docs.datastax.com/en/developer/driver-matrix/doc/common/driverMatrix.html\" target=\"_blank\">driver compatibility</a>. Depending on the driver version, you\n              might need to recompile your client application code. See <a class=\"xref\" href=\"http://docs.datastax.com/en/upgrade/doc/upgrade/upgrdDrivers.html\" title=\"Upgrading the DataStax C/C++, C#, Java, Node.js, PHP, Python, and Ruby drivers.\">Upgrading DataStax drivers</a>.</dd>\n          <dd class=\"dd ddexpand\">During upgrades, you might experience driver-specific impact when clusters have mixed\n            versions of drivers. If your cluster has mixed versions, the protocol version is\n            negotiated with the first host that the driver connects to. To avoid driver version\n            incompatibility during upgrades, use one of these workarounds:<ul class=\"ul\" id=\"upgdDSE50__d11e999\"><li class=\"li\">Force a protocol version at startup. For example, keep the Java driver at v2 while\n                the upgrade is happening. Switch to the Java driver v3 only after the entire cluster\n                is upgraded.</li>\n              <li class=\"li\">Ensure that the list of initial contact points contains only hosts with the oldest\n                driver version. For example, the initial contact points contain only Java driver\n                v2.</li>\n            </ul>\nFor details on protocol version negotiation, see protocol versions with mixed\n            clusters in the Java driver version you're using, for example, <a class=\"xref\" href=\"http://docs.datastax.com/en/developer/java-driver-dse/latest\" target=\"_blank\">Java\n              driver</a>. </dd>\n        \n      </dl></section><section class=\"section\" id=\"upgdDSE50__prepUpg50\" readability=\"1\"><h2 class=\"title sectiontitle\">Preparing to upgrade</h2>\n      \n      <div class=\"p\" readability=\"6\">Follow these steps to prepare each node for upgrading from DataStax Enterprise 4.7 or 4.8\n        to DataStax Enterprise 5.0:<ol class=\"ol\" readability=\"15\"><li class=\"li\" readability=\"2\">Before upgrading, be sure that each node has ample free disk space.\n            <p class=\"p\">The required space depends on the compaction strategy. See <a class=\"xref\" href=\"http://docs.datastax.com/en/dse/5.1/dse-planning/planning/planningHardware.html#planningHardware__disk\" target=\"_blank\">Disk space</a> in <em class=\"ph i\">Planning and testing DataStax\n              Enterprise deployments</em>.</p>\n</li>\n          <li class=\"li\" id=\"upgdDSE50__familarize\">Familiarize yourself with the changes and features in this\n          release: <ul class=\"ul\"><li class=\"li\" id=\"upgdDSE50__d11e240\">Be sure your <a class=\"xref\" href=\"http://docs.datastax.com/en/landing_page/doc/landing_page/supportedPlatforms.html\" target=\"_blank\">platform is supported</a>.</li>\n            <li class=\"li\" id=\"upgdDSE50__d11e246\"><a class=\"xref\" href=\"http://docs.datastax.com/en/cassandra/3.0/cassandra/install/installJDKabout.html\" target=\"_blank\">Oracle Java SE Runtime Environment 8 (JDK)</a>\n                (1.8.0_40 minimum) or <a class=\"xref\" href=\"http://openjdk.java.net/\" target=\"_blank\">OpenJDK 8</a>.\n              Earlier or later versions are not supported.</li>\n            <li class=\"li\">DataStax Enterprise <a class=\"xref\" href=\"http://docs.datastax.com/en/datastax_enterprise/5.0/datastax_enterprise/RNdse.html\" target=\"_blank\">5.0 release notes</a>.</li>\n            <li class=\"li\"><em class=\"ph i\">General upgrading advice for any version</em> and <em class=\"ph i\">New features</em> for Apache\n              Cassandra™ 3.0 in <a class=\"xref\" href=\"http://github.com/apache/cassandra/blob/cassandra-3.0/NEWS.txt\" target=\"_blank\">NEWS.txt</a>. Be sure to read the NEWS.txt\n              all the way back to your current version.</li>\n            <li class=\"li\">Apache Cassandra™ changes in <a class=\"xref\" href=\"http://github.com/apache/cassandra/blob/cassandra-3.0/CHANGES.txt\" target=\"_blank\">CHANGES.txt</a>. </li>\n            <li class=\"li\">DataStax Enterprise 5.0 <a class=\"xref\" href=\"http://docs.datastax.com/en/datastax_enterprise/5.0/datastax_enterprise/RNcassChanges.html\" target=\"_blank\">production-certified changes</a> to Apache\n              Cassandra.</li>\n            <li class=\"li\"><a class=\"xref\" href=\"http://docs.datastax.com/en/upgrade/doc/upgrade/upgrdDrivers.html\" title=\"Upgrading the DataStax C/C++, C#, Java, Node.js, PHP, Python, and Ruby drivers.\">DataStax driver changes</a>.</li>\n          </ul></li>\n          <li class=\"li\" readability=\"16\">Verify your current product version. If necessary, upgrade to an interim version:\n              \n<table cellpadding=\"4\" cellspacing=\"0\" summary=\"\" id=\"upgdDSE50__versionTable\" class=\"table\" frame=\"border\" border=\"1\" rules=\"all\"><colgroup><col/><col/></colgroup><tbody class=\"tbody\" readability=\"35\"><tr readability=\"2\"><td class=\"entry cellrowborder\" headers=\"d7765e387 \">DataStax Enterprise 4.7 or 4.8</td>\n                    <td class=\"entry cellrowborder\" headers=\"d7765e390 \">DataStax Enterprise 5.0</td>\n                  </tr><tr readability=\"4\"><td class=\"entry cellrowborder\" headers=\"d7765e387 \">DataStax Enterprise 4.0, 4.5, or 4.6</td>\n                    <td class=\"entry cellrowborder\" headers=\"d7765e390 \">DataStax Enterprise 4.8</td>\n                  </tr><tr readability=\"2\"><td class=\"entry cellrowborder\" headers=\"d7765e387 \">DataStax Community or open source Apache Cassandra™ 2.0.x</td>\n                    <td class=\"entry cellrowborder\" headers=\"d7765e390 \">DataStax Enterprise 4.8</td>\n                  </tr><tr readability=\"2\"><td class=\"entry cellrowborder\" headers=\"d7765e387 \">DataStax Community 3.0.x</td>\n                    <td class=\"entry cellrowborder\" headers=\"d7765e390 \">No interim version required.</td>\n                  </tr><tr readability=\"2\"><td class=\"entry cellrowborder\" headers=\"d7765e387 \">DataStax Distribution of Apache Cassandra™ 3.x</td>\n                    <td class=\"entry cellrowborder\" headers=\"d7765e390 \">Upgrade not available.</td>\n                  </tr></tbody></table></li>\n          <li class=\"li\" readability=\"11\">Upgrade the SSTables on each node to ensure that all SSTables\n          are on the current version. <div class=\"p\" readability=\"10\">This is required for DataStax Enterprise upgrades that\n            include  changes.<p>Warning: Failure to upgrade SSTables when required results in a significant\n              performance impact and increased disk usage.\n              </p>\n</div>\n<pre class=\"pre screen\">$ nodetool upgradesstables</pre>\n<p class=\"p\">If the SSTables are already on the current version, the\n              command returns immediately and no action is taken. See <a class=\"xref\" href=\"http://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/tools/toolsSStables/ToolsSSTableupgrade.html\" target=\"_blank\">SSTable compatibility and upgrade\n              version</a>.</p>\n<p class=\"p\">Use the <code data-swiftype-name=\"codeph\" data-swiftype-type=\"text\" class=\"ph codeph\">--jobs</code> option to\n              set the number of SStables that upgrade simultaneously. The default setting is 2,\n              which minimizes impact on the cluster. Set to 0 to use all available compaction\n              threads.</p>\n</li>\n          <li class=\"li\" readability=\"8\">Verify the Java runtime version and upgrade to the recommended\n            version.<pre class=\"pre screen\">$ java -version</pre>\n<p class=\"p\" id=\"upgdDSE50__d11e62\">The latest version of <a class=\"xref\" href=\"http://docs.datastax.com/en/cassandra/3.0/cassandra/install/installJDKabout.html\" target=\"_blank\">Oracle Java SE Runtime Environment 8 (JDK)</a> (1.8.0_40 minimum)\n            or <a class=\"xref\" href=\"http://openjdk.java.net/\" target=\"_blank\">OpenJDK 8</a>\n            is recommended. The JDK is recommended for development and production systems. The JDK\n            provides useful troubleshooting tools that are not in the JRE, such as jstack, jmap,\n            jps, and jstat.</p>\n</li>\n          <li class=\"li\">Run <a class=\"xref\" href=\"http://docs.datastax.com/en/cassandra/3.0/cassandra/tools/toolsRepair.html\" target=\"_blank\">nodetool repair</a> to ensure that data on each replica is consistent with data on\n          other nodes. </li>\n          \n          <li class=\"li\" id=\"upgdDSE50__keyName\" readability=\"5\"><strong class=\"ph b\">DSE Search partition key names</strong><div class=\"p\" readability=\"17\">The partition key names of COMPACT\n              STORAGE tables backed by DSE Search indexes match the uniqueKey\n              in schema.xml. For example, consider the following table is\n              created with compact\n              storage:<pre>CREATE TABLE keyspace_name.table_name (key text PRIMARY KEY, foo text, solr_query text) \nWITH COMPACT STORAGE</pre>and\n              the Solr schema.xml\n              is:<pre>...\n&lt;uniqueKey&gt;id&lt;/uniqueKey&gt;\n...</pre>then rename the key\n              in the table to match the\n              schema:<pre class=\"pre screen\">ALTER TABLE ks.table RENAME key TO id;</pre>\n</div>\n</li>\n          <li class=\"li\" readability=\"1\">Back up the  files you use to a folder\n            that is not in the directory where you normally run commands. <p class=\"p\">The configuration files are overwritten with default values during installation of the\n            new version. </p>\n</li>\n        </ol></div>\n    </section><section class=\"section\" id=\"upgdDSE50__upg50\" readability=\"5\"><h2 class=\"title sectiontitle\">Upgrade steps</h2>\n      \n      <p>Tip: The <a class=\"xref\" href=\"http://docs.datastax.com/en/upgrade/doc/upgrade/datastax_enterprise/upgdDSEinstaller.html\" title=\"DataStax Enterprise upgrade instructions for the DataStax Installer.\">DataStax installer</a>\n        upgrades DataStax Enterprise and automatically performs many upgrade tasks. </p>\n      <div class=\"p\" readability=\"7\">Follow these steps on each node to upgrade from DataStax Enterprise 4.7 or 4.8 to DataStax\n        Enterprise 5.0. Some <a class=\"xref\" href=\"http://docs.datastax.com/en/upgrade/doc/upgrade/datastax_enterprise/upgdDSE50.html#upgdDSE50__msgsIgnore\">warning messages are displayed during and\n          after upgrade</a>.<ol class=\"ol\" id=\"upgdDSE50__stepUpg\" readability=\"18\"><li class=\"li\">Upgrade order matters. Upgrade nodes in this order:<ul class=\"ul\" readability=\"2\"><li class=\"li\">In multiple datacenter clusters, upgrade every node in one datacenter before\n              upgrading another datacenter.</li>\n            <li class=\"li\" readability=\"5\">Upgrade the seed nodes within a datacenter first.<p class=\"p\">For DSE Analytics nodes using\n                DSE Hadoop, upgrade the Job Tracker node first. Then upgrade Hadoop nodes, followed\n                by Spark nodes.</p>\n</li>\n            <li class=\"li\">Upgrade types in this order:<ol class=\"ol\" type=\"a\"><li class=\"li\">DSE Analytics nodes or datacenters</li>\n                <li class=\"li\">Transactional/DSE Graph nodes or datacenters</li>\n                <li class=\"li\">DSE Search nodes or datacenters</li>\n              </ol></li>\n          </ul>\nWith a few exceptions, the cluster continues to work as\n            though it were on the earlier version of DataStax Enterprise until all of the nodes in\n            the cluster are upgraded. Upgrade and restart the nodes one at a time. Other nodes in\n            the cluster continue to operate at the earlier version until all nodes are\n            upgraded.</li>\n          <li class=\"li\"><strong class=\"ph b\">DSE Analytics nodes:</strong> Kill all Spark worker processes. </li>\n          <li class=\"li\"><strong class=\"ph b\">DSE Search nodes:</strong> Review these considerations and take appropriate\n            actions:</li>\n          <li class=\"li\" readability=\"6\">Run <a class=\"xref\" href=\"http://docs.datastax.com/en/cassandra/3.0/cassandra/tools/toolsDrain.html\" target=\"_blank\">nodetool drain</a> to flush the commit log of the old\n            installation:<pre class=\"pre screen\">$ nodetool drain -h <var class=\"keyword varname\">hostname</var></pre>\n<div class=\"p\" readability=\"16\">This\n            step saves time when nodes start up after the upgrade and prevents DSE Search nodes from\n            having to reindex data. <p>Important:  This step is mandatory when upgrading\n              between major Cassandra versions that change SSTable formats, rendering commit logs\n              from the previous version incompatible with the new version.</p>\n</div>\n</li>\n          <li class=\"li\"><a class=\"xref\" href=\"http://docs.datastax.com/en/datastax_enterprise/5.0/datastax_enterprise/admin/startStopDseToc.html\" target=\"_blank\">Stop the node</a>:</li>\n          <li class=\"li\" readability=\"5\">Use the appropriate method to install the new product version on a\n            <a class=\"xref\" href=\"http://docs.datastax.com/en/landing_page/doc/landing_page/supportedPlatforms.html\" target=\"_blank\">supported platform</a>:<ul class=\"ul\" id=\"upgdDSE50__d11e559\"><li class=\"li\">RHEL: <a class=\"xref\" href=\"http://docs.datastax.com/en/archived/datastax_enterprise/4.6/datastax_enterprise/install/installRHELdse.html\" target=\"_blank\">4.6</a>, <a class=\"xref\" href=\"http://docs.datastax.com/en/datastax_enterprise/4.7/datastax_enterprise/install/installRHELdse.html\" target=\"_blank\">4.7</a>, <a class=\"xref\" href=\"http://docs.datastax.com/en/datastax_enterprise/4.8/datastax_enterprise/install/installRHELdse.html\" target=\"_blank\">4.8</a>, <a class=\"xref\" href=\"http://docs.datastax.com/en/datastax_enterprise/5.0/datastax_enterprise/install/installRHELdse.html\" target=\"_blank\">5.0</a></li>\n            <li class=\"li\">Debian: <a class=\"xref\" href=\"http://docs.datastax.com/en/archived/datastax_enterprise/4.6/datastax_enterprise/install/installDEBdse.html\" target=\"_blank\">4.6</a>, <a class=\"xref\" href=\"http://docs.datastax.com/en/datastax_enterprise/4.7/datastax_enterprise/install/installDEBdse.html\" target=\"_blank\">4.7</a>, <a class=\"xref\" href=\"http://docs.datastax.com/en/datastax_enterprise/4.8/datastax_enterprise/install/installDEBdse.html\" target=\"_blank\">4.8</a>, <a class=\"xref\" href=\"http://docs.datastax.com/en/datastax_enterprise/5.0/datastax_enterprise/install/installDEBdse.html\" target=\"_blank\">5.0</a>\n            </li>\n            <li class=\"li\">Tarball: <a class=\"xref\" href=\"http://docs.datastax.com/en/archived/datastax_enterprise/4.6/datastax_enterprise/install/installTARdse.html\" target=\"_blank\">4.6</a>, <a class=\"xref\" href=\"http://docs.datastax.com/en/datastax_enterprise/4.7/datastax_enterprise/install/installTARdse.html\" target=\"_blank\">4.7</a>, <a class=\"xref\" href=\"http://docs.datastax.com/en/datastax_enterprise/4.8/datastax_enterprise/install/installTARdse.html\" target=\"_blank\">4.8</a>, <a class=\"xref\" href=\"http://docs.datastax.com/en/datastax_enterprise/5.0/datastax_enterprise/install/installTARdse.html\" target=\"_blank\">5.0</a>\n            </li>\n          </ul><p>Note: Install the new product version using the same installation type that is on the\n            system. The upgrade proceeds with installation regardless of the installation type and\n            might result in issues. </p>\n</li>\n          <li class=\"li\" readability=\"4\">If the cluster will run Hadoop in a Kerberos secure environment, change\n            the task-controller file ownership to root and access permissions\n            to 4750. For\n              example:<pre class=\"pre screen\">$ sudo chown root /usr/share/dse/resources/hadoop/native/Linux-amd64-64/bin/task-controller\n$ sudo chmod 4750 /usr/share/dse/resources/hadoop/native/Linux-amd64-64/bin/task-controller</pre>\n<p class=\"p\">Package\n              installations only: The default location of the <code data-swiftype-name=\"codeph\" data-swiftype-type=\"text\" class=\"ph codeph\">task-controller</code> file\n              should be\n                /usr/share/dse/resources/hadoop/native/Linux-amd64-64/bin/task-controller.</p>\n</li>\n          <li class=\"li\" id=\"upgdDSE50__stepConfNew\">To configure the new product version:<ol class=\"ol\" type=\"a\"><li class=\"li\">Compare your backup  files to the\n                new configuration files: <ul class=\"ul\"><li class=\"li\">Look for any deprecated, removed, or changed settings.</li>\n                  <li class=\"li\">Be sure you are <a class=\"xref\" href=\"http://docs.datastax.com/en/upgrade/doc/upgrade/datastax_enterprise/upgdDSE50.html#upgdDSE50__familarize\">familiar</a> with the\n                    Apache Cassandra and DataStax Enterprise changes and features in the new\n                    release. </li>\n                </ul></li>\n              <li class=\"li\">Merge the applicable modifications into the new version.</li>\n            </ol></li>\n          <li class=\"li\">Start the node.<ul class=\"ul\"><li class=\"li\">Installer-Services and Package installations: See <a class=\"xref\" href=\"http://docs.datastax.com/en/datastax_enterprise/5.0/datastax_enterprise/admin/startDseService.html\" target=\"_blank\">Starting DataStax Enterprise as a\n              service</a>.</li>\n            <li class=\"li\">Installer-No Services and Tarball installations: See <a class=\"xref\" href=\"http://docs.datastax.com/en/datastax_enterprise/5.0/datastax_enterprise/admin/startDseStandalone.html\" target=\"_blank\">Starting DataStax Enterprise as a stand-alone\n                process</a>.</li>\n          </ul></li>\n          <li class=\"li\">Verify that the upgraded datacenter names match the datacenter\n          names in the keyspace schema definition:<pre class=\"pre screen\">$ nodetool status</pre>\n</li>\n          <li class=\"li\" readability=\"13\">Review the logs for warnings, errors, and exceptions. Because DataStax Enterprise 5.0 uses\n            Cassandra 3.0, the output.log might include warnings about the following:<ul class=\"ul\"><li class=\"li\">sstable_compression</li>\n              <li class=\"li\">chunk_length_kb </li>\n              <li class=\"li\">memory_allocator</li>\n              <li class=\"li\">memtable_allocation_type</li>\n              <li class=\"li\">offheap_objects</li>\n              <li class=\"li\">netty_server_port - used only during the upgrade to 5.0. After all nodes are\n                running 5.0, requests that are coordinated by this node no longer contact other\n                nodes on this port. Instead requests use inter-node messaging options. The <a class=\"xref\" href=\"http://docs.datastax.com/en/datastax_enterprise/5.0/datastax_enterprise/config/configDseYaml.html#configDseYaml__interNode\" target=\"_blank\"> internode_messaging_options</a> are used by\n                several components of DataStax Enterprise. For 5.0 and later, all internode\n                messaging requests use this service.</li>\n            </ul><p class=\"p\">Warnings, errors, and exceptions are frequently found in the\n            logs when starting up an upgraded node. Some of these log entries are informational to\n            help you execute specific upgrade-related steps. If you find unexpected warnings,\n            errors, or exceptions, contact <a class=\"xref\" href=\"https://support.datastax.com/hc/en-us\" target=\"_blank\">DataStax Support</a>.</p>\n<p class=\"p\">During upgrade of DSE Analytics\n              nodes, exceptions about the Task Tracker are logged in the nodes that are not yet\n              upgraded to 5.0. The jobs succeed after the entire cluster is upgraded. </p>\n</li>\n          <li class=\"li\" id=\"upgdDSE50__stepRepeat\">Repeat the upgrade on each node in the cluster following the\n            recommended .</li>\n          <li class=\"li\" readability=\"6\">After all nodes are upgraded, drop the following legacy tables: system_auth.users,\n            system_auth.credentials and system_auth.permissions.<p class=\"p\">As described in Cassandra <a class=\"xref\" href=\"http://github.com/apache/cassandra/blob/cassandra-3.0/NEWS.txt#L310-L331\" target=\"_blank\">NEWS.txt</a>, the authentication and authorization\n              subsystems have been redesigned to support role-based access control (RBAC), which\n              results in a change to the schema of the system_auth keyspace. </p>\n</li>\n          <li class=\"li\" id=\"upgdDSE50__stepSSTablesAfterDSE50\" readability=\"11\">After the new version is install on all nodes, upgrade the\n            SSTables: <pre class=\"pre screen\">$ nodetool upgradesstables</pre>\n<p>Warning: Failure to\n              upgrade SSTables when required results in a significant performance impact and\n              increased disk usage. Upgrading is not complete until the SSTables are\n              upgraded.</p>\n<p class=\"p\">Use the <code data-swiftype-name=\"codeph\" data-swiftype-type=\"text\" class=\"ph codeph\">--jobs</code> option to\n              set the number of SStables that upgrade simultaneously. The default setting is 2,\n              which minimizes impact on the cluster. Set to 0 to use all available compaction\n              threads.</p>\n</li>\n          <li class=\"li\">For multiple datacenter deployments, change the replication factor of the\n            system_distributed keyspace to NetworkTopologyStrategy.  </li>\n          <li class=\"li\">If you use the OpsCenter Repair Service, turn on the Repair Service. </li>\n        </ol></div>\n      \n    </section><section class=\"section\" readability=\"0\"/>",
        "created_at": "2017-12-21T22:52:17+0000",
        "updated_at": "2017-12-22T02:10:10+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 10,
        "domain_name": "docs.datastax.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/8863"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 11,
            "label": "database",
            "slug": "database"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 207,
            "label": "system",
            "slug": "system"
          }
        ],
        "is_public": false,
        "id": 8862,
        "uid": null,
        "title": "How to Setup a Highly Available Multi-AZ Cassandra Cluster on AWS EC2 - High Scalability -",
        "url": "http://highscalability.com/blog/2016/8/1/how-to-setup-a-highly-available-multi-az-cassandra-cluster-o.html",
        "content": "<div><img src=\"https://c1.staticflickr.com/9/8606/28622928606_10df5eb9e8_o.jpg\" alt=\"\" /></div><p dir=\"ltr\"><em>This is a guest post by Alessandro Pieri, Software Architect at <a href=\"https://getstream.io/?ref=hs\">Stream</a>. Try out this <a href=\"https://getstream.io/get_started/?ref=hs\">5 minute interactive tutorial</a> to learn more about Stream’s API.</em></p><p dir=\"ltr\">Originally built by Facebook in 2009, <a href=\"http://cassandra.apache.org/\">Apache Cassandra</a> is a free and open-source distributed database designed to handle large amounts of data across a large number of servers. At <a href=\"https://getstream.io/?ref=hs\">Stream</a>, we use Cassandra as the primary data store for our feeds. Cassandra stands out because it’s able to:</p><ul><li dir=\"ltr\">\n<p dir=\"ltr\">Shard data automatically</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\">Handle partial outages without data loss or downtime</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\">Scales close to linearly</p>\n</li>\n</ul><p dir=\"ltr\">If you’re already using Cassandra, your cluster is likely configured to handle the loss of 1 or 2 nodes. However, <strong>what happens when a full availability zone goes down</strong>?</p><p dir=\"ltr\">In this article <strong>you will</strong> <strong>learn how to setup Cassandra to survive a full availability zone outage</strong>. Afterwards, we will analyze how moving from a single to a multi availability zone cluster impacts availability, cost, and performance.</p><h2 dir=\"ltr\">Recap 1: What Are Availability Zones?</h2><p dir=\"ltr\">AWS operates off of geographically isolated locations called regions. Each region is composed of a small amount (usually 3 or 4) physically independent availability zones. Availability zones are connected with a low latency network, while regions are completely independent of each other, as shown in the diagram below:</p><p dir=\"ltr\"><img src=\"https://lh5.googleusercontent.com/taKgfxUy-J49NVs4lVTAQKc3o8DNj17r3ZJMniJh2dAD593cjXMiCH32LZbHtW5o6Vh0fmfD58cHxe-4B8kBbeNc0mJOwooUd2CF1Enem4KMl_sRZ2-1Xp0y411s8aeElLeRKvY\" alt=\"\" width=\"504\" height=\"267\" /></p><p dir=\"ltr\">In order to achieve high availability, AWS resources should be hosted in multiple availability zones. Hosting in multiple availability zones allows you to ensure that if one goes down, your app will stay up and running.</p><h2 dir=\"ltr\">Recap 2: Cassandra and High Availability</h2><p dir=\"ltr\">One of the primary benefits of Cassandra is that it automatically shards your data across multiple nodes. It even manages to scale almost linearly, so doubling the number of nodes give you nearly double the capacity.</p><p dir=\"ltr\"><img src=\"https://lh3.googleusercontent.com/vaSRqgcXK82r2gcFlbHhlWYXghP0iSyEdWWaS106h55FxUhRBdalycwbxsIZ_D5r79fr4MtyGcx987Mfuk-eRzRr00yQM6lPiFaglR9VhlBeCO7FcTl0DCt7yPSBwgZMZbQ1OBI\" alt=\"\" width=\"624\" height=\"205\" /></p><p dir=\"ltr\">Cassandra has a setting called “<a href=\"https://docs.datastax.com/en/cassandra/3.x/cassandra/architecture/archDataDistributeReplication.html\">replication factor</a>” that defines how many copies of your data should exist. If your replication factor is set to 1 and a node goes down, you will lose your data because it was only stored in 1 place. A replication factor of 3 will insure that your data is always stored on 3 different nodes, ensuring that your data is safe when a single node breaks down.</p><h2 dir=\"ltr\">Configuring Cassandra for multi AZ availability</h2><p dir=\"ltr\">Now that we’ve covered the basics, let’s explain how to setup Cassandra for multi-AZ availability.</p><p dir=\"ltr\">If you’re new to Cassandra and want to learn how to setup your own cluster, <a href=\"https://www.digitalocean.com/community/tutorials/how-to-run-a-multi-node-cluster-database-with-cassandra-on-ubuntu-14-04\">this article</a> is a good starting point. </p><h2>Part 1 - The Snitch</h2><p dir=\"ltr\">As a first step we have to make sure Cassandra knows which region and availability zone it’s in. This is handled by the “snitch”, which keeps track of the information related to the network topology. Cassandra provides several built-in snitches. The Ec2Snitch and Ec2MultiRegionSnitch work well for AWS. The Ec2Snitch is meant for a single region deployment, and the Ec2MultiRegionSnitch is meant for clusters that span multiple regions.</p><p dir=\"ltr\">Cassandra understands the concept of a data center and a rack. The EC2 snitches treat each EC2 region as a data center and the availability zone as the rack.</p><p dir=\"ltr\">You can change the Snitch setting in cassandra.yaml. Beware that changing the Snitch setting is a potentially destructive operations and should be planned with care. Read the <a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/operations/opsSwitchSnitch.html\">Cassandra documentation about changing the Snitch setting</a>.</p><table><colgroup><col width=\"*\" /></colgroup><tbody><tr><td><p dir=\"ltr\"># IF YOU CHANGE THE SNITCH AFTER DATA IS INSERTED INTO THE CLUSTER,</p>\n<p dir=\"ltr\"># YOU MUST RUN A FULL REPAIR, SINCE THE SNITCH AFFECTS WHERE REPLICAS</p>\n<p dir=\"ltr\"># ARE PLACED.</p>\n<p dir=\"ltr\">#</p>\n<p dir=\"ltr\"># Out of the box, Cassandra provides</p>\n<p dir=\"ltr\">...</p>\n<p dir=\"ltr\">#  - Ec2Snitch:</p>\n<p dir=\"ltr\">#    Appropriate for EC2 deployments in a single Region.  Loads Region</p>\n<p dir=\"ltr\">#    and Availability Zone information from the EC2 API. The Region is</p>\n<p dir=\"ltr\">#    treated as the Datacenter, and the Availability Zone as the rack.</p>\n<p dir=\"ltr\">#    Only private IPs are used, so this will not work across multiple</p>\n<p dir=\"ltr\">#    Regions.</p>\n<p dir=\"ltr\">#  - Ec2MultiRegionSnitch:</p>\n<p dir=\"ltr\">#    Uses public IPs as broadcast_address to allow cross-region</p>\n<p dir=\"ltr\">#    connectivity.  (Thus, you should set seed addresses to the public</p>\n<p dir=\"ltr\">#    IP as well.) You will need to open the storage_port or</p>\n<p dir=\"ltr\">#    ssl_storage_port on the public IP firewall.  (For intra-Region</p>\n<p dir=\"ltr\">#    traffic, Cassandra will switch to the private IP after</p>\n<p dir=\"ltr\">#    establishing a connection.)</p>\n<p dir=\"ltr\">#</p>\n<p dir=\"ltr\"># You can use a custom Snitch by setting this to the full class name</p>\n<p dir=\"ltr\"># of the snitch, which will be assumed to be on your classpath.</p>\n<p dir=\"ltr\">endpoint_snitch: Ec2Snitch </p>\n</td>\n</tr></tbody></table><p dir=\"ltr\"><em>The above is a snippet from cassandra.yaml</em></p><h2>Part 2 - The Replication Factor</h2><p dir=\"ltr\">The replication factor determines the number of replicas that should exist in the cluster. Replication strategy, also known as replica placement strategy, determines how replicas are distributed across the cluster. Both settings are keyspace properties.</p><p dir=\"ltr\">By default Cassandra uses the “SimpleStrategy” replication strategy. This strategy places replicas in the cluster ignoring which region or availability zone it’s in. The <a href=\"https://docs.datastax.com/en/cassandra/2.0/cassandra/architecture/architectureDataDistributeReplication_c.html\">NetworkTopologyStrategy</a> is rack aware and is designed to support multi-datacenter deployments.</p><table><colgroup><col width=\"*\" /></colgroup><tbody><tr><td><p dir=\"ltr\">CREATE KEYSPACE mykeyspace WITH replication = {</p>\n<p dir=\"ltr\">  'class': 'NetworkTopologyStrategy',</p>\n<p dir=\"ltr\">  'us-east': '3'</p>\n<p dir=\"ltr\">};</p>\n</td>\n</tr></tbody></table><p dir=\"ltr\">In the above code snippet we’ve declared a keyspace called “mykeyspace” with a NetworkReplicationStrategy which will place the replicas in the “us-east” datacenter only, with a replication factor of 3.</p><p dir=\"ltr\">To change an existing keyspace you can use the example below. Beware that changing the replication strategy of a running Cassandra’s cluster is a sensitive operation. <a href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_using/useUpdateKeyspaceRF.html\">Read the full documentation</a>.</p><table><colgroup><col width=\"*\" /></colgroup><tbody><tr><td><p dir=\"ltr\">ALTER KEYSPACE mykeyspace WITH REPLICATION = { </p>\n<p dir=\"ltr\">   'class' : 'NetworkTopologyStrategy', </p>\n<p dir=\"ltr\">   'us-east' : '3' </p>\n<p dir=\"ltr\">};</p>\n<br /></td>\n</tr></tbody></table><h2>Part 3 - Consistency levels</h2><p dir=\"ltr\">When you read or write from Cassandra, you have the ability to specify the “<a href=\"https://docs.datastax.com/en/cassandra/2.0/cassandra/dml/dml_config_consistency_c.html\">consistency level</a>” on the client-side. In other words, you can specify how many nodes in the Cassandra cluster are required to agree before a read or write request is valid.</p><p dir=\"ltr\">If you ask for a higher consistency level than Cassandra is able to answer with nodes in the local availability zone, it will query the other zones. To stay up during an availability zone outage, you need to use a consistency level that the remaining nodes are able to satisfy. The next section will discuss failure scenarios and consistency levels in more detail.</p><h2>Handling AZ(s) Outages</h2><p dir=\"ltr\">How a Cassandra cluster behaves when an availability zone goes down depends on several factors:</p><ul><li dir=\"ltr\">\n<p dir=\"ltr\">Scale of the failure (how many AZs are down)</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\">Number of AZs used by the cluster</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\">Replication factor</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\">Consistency level</p>\n</li>\n</ul><p dir=\"ltr\">Let’s have a look at the diagram below, which shows a couple of scenarios:</p><p dir=\"ltr\"><img src=\"https://lh5.googleusercontent.com/64YpnsN_mfrVsRDrL9ke12vVvX7RxdjMATjYnhuxy_6U7hYLyyN-v3X-alyo6NOCuIUiG8xg6Ewf0FLYJrTupY9X9KXLH2aWGwA_Kukaa6ffgxudVJhjv2RgUu680me92mZAYh8\" alt=\"\" width=\"624\" height=\"355\" /></p><p dir=\"ltr\"><em>Figure 2. How consistency level affects availability</em></p><p dir=\"ltr\">In the first scenario shown on the left we show a <strong>cluster running on 2 AZs with 6 nodes (3 per AZ) and a RF=2</strong>. When 1 AZ goes down, half of our cluster will be offline. With 2 AZs and a RF=2, we will have the guarantee that our entire dataset is still present on at least 1 node. As you can see in the table next to the cluster diagram, the outcome of a query depends on the requested consistency level. For example, a query with CL=ONE will succeed because we still have at least 1 node available. On the other hand, queries with higher CL requirements such as. QUORUM and ALL will always fail because they both require responses from 2 nodes.</p><p dir=\"ltr\">In the second scenario, <strong>we run Cassandra with 9 nodes on 3 different AZs and a replica factor of 3</strong>. With this deployment, our cluster is clearly more resilient in the event of 1 AZ failure. Cassandra will still be able to satisfy queries with CL=QUORUM.</p><p dir=\"ltr\">It is worth noting that in the event of an availability zone outage, the capacity left in service for the 2 clusters is different. With the first cluster setup you lose 50% of the capacity, while <strong>the second setup only affects 33% of the capacity</strong>.</p><h2>How Much Latency Is Introduced by a Multi-AZs Setup?</h2><p dir=\"ltr\">Estimating query latency introduced by the multi-AZ setup is not easy due to the nature of Cassandra and the number of factors that fluctuate in a cloud environment (e.g. network latency, disk I/O, host utilization, etc.).</p><p dir=\"ltr\">For our tests we used the <a href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsCStress_t.html\">cassandra-stress</a> tool to generate read and write load on clusters running on single and multiple AZs. In order to keep the variance as low as possible, and to lower the deviation on disk I/O, we used instances with ephemeral storage instead of network attached storage (EBS).</p><p dir=\"ltr\">We then came up with <strong>two test scenarios</strong>:</p><p dir=\"ltr\">The first used a cluster of 6 i2.xlarge instances (AWS network performance = “moderate”) and was running without enhanced networking:</p><table><colgroup><col width=\"324\" /><col width=\"151\" /><col width=\"149\" /></colgroup><tbody><tr><td><br /></td>\n<td>\n<p dir=\"ltr\">Median</p>\n</td>\n<td>\n<p dir=\"ltr\">95th percentile</p>\n</td>\n</tr><tr><td colspan=\"3\">\n<p dir=\"ltr\">WRITE</p>\n</td>\n</tr><tr><td>\n<p dir=\"ltr\">Single AZ</p>\n</td>\n<td>\n<p dir=\"ltr\">1.0</p>\n</td>\n<td>\n<p dir=\"ltr\">2.5</p>\n</td>\n</tr><tr><td>\n<p dir=\"ltr\">Multi AZ (3 AZs)</p>\n</td>\n<td>\n<p dir=\"ltr\">1.5</p>\n</td>\n<td>\n<p dir=\"ltr\">2.8</p>\n</td>\n</tr><tr><td colspan=\"3\">\n<p dir=\"ltr\">READ</p>\n</td>\n</tr><tr><td>\n<p dir=\"ltr\">Single AZ</p>\n</td>\n<td>\n<p dir=\"ltr\">1.0</p>\n</td>\n<td>\n<p dir=\"ltr\">2.6</p>\n</td>\n</tr><tr><td>\n<p dir=\"ltr\">Multi AZ (3 AZs)</p>\n</td>\n<td>\n<p dir=\"ltr\">1.5</p>\n</td>\n<td>\n<p dir=\"ltr\">23.5</p>\n</td>\n</tr></tbody></table><p dir=\"ltr\"><em>Table 1. Scenario 1: performance test Single AZ vs Multi AZ (time in milliseconds). Setup: Cassandra 2.0.15; RF=3; CL=1</em></p><p dir=\"ltr\"> </p><p dir=\"ltr\">The second scenario used a cluster of 6 i2.2xlarge (AWS network performance = “high”), with enhanced networking turned on:</p><table><colgroup><col width=\"321\" /><col width=\"153\" /><col width=\"150\" /></colgroup><tbody><tr><td><br /></td>\n<td>\n<p dir=\"ltr\">Median</p>\n</td>\n<td>\n<p dir=\"ltr\">95th percentile</p>\n</td>\n</tr><tr><td colspan=\"3\">\n<p dir=\"ltr\">WRITE</p>\n</td>\n</tr><tr><td>\n<p dir=\"ltr\">Single AZ</p>\n</td>\n<td>\n<p dir=\"ltr\">0.9</p>\n</td>\n<td>\n<p dir=\"ltr\">2.4</p>\n</td>\n</tr><tr><td>\n<p dir=\"ltr\">Multi AZ (3 AZs)</p>\n</td>\n<td>\n<p dir=\"ltr\">1.1</p>\n</td>\n<td>\n<p dir=\"ltr\">2.3</p>\n</td>\n</tr><tr><td colspan=\"3\">\n<p dir=\"ltr\">READ</p>\n</td>\n</tr><tr><td>\n<p dir=\"ltr\">Single AZ</p>\n</td>\n<td>\n<p dir=\"ltr\">0.7</p>\n</td>\n<td>\n<p dir=\"ltr\">1.5</p>\n</td>\n</tr><tr><td>\n<p dir=\"ltr\">Multi AZ (3 AZs)</p>\n</td>\n<td>\n<p dir=\"ltr\">1.0</p>\n</td>\n<td>\n<p dir=\"ltr\">1.9</p>\n</td>\n</tr></tbody></table><p dir=\"ltr\"><em>Table 2. Scenario 2: performance test Single AZ vs Multi AZ (time in milliseconds). Setup: Cassandra 2.0.15; RF=3; CL=1</em></p><p dir=\"ltr\"> </p><p dir=\"ltr\">Interesting enough, networking performance varies between the two instance types. <strong>When using i2.2xlarge with enhanced networking enabled, we saw very little difference between single AZ and multi AZ deployments</strong>. Therefor we recommend enabling enhanced networking and selecting an instance types with “high” network performance.</p><p dir=\"ltr\">Another interesting fact is that Cassandra reads are – to a certain extent – rack-aware. When coordinating a query, Cassandra nodes will route the request to the peer with lowest latency. This feature is called “<a href=\"http://www.datastax.com/dev/blog/dynamic-snitching-in-cassandra-past-present-and-future\">dynamic snitching</a>” and has been part of Cassandra since version 0.6.5.</p><p dir=\"ltr\">Thanks to <strong>dynamic snitching, most of the read queries to Cassandra do not “hit” nodes on different availability zones</strong> and give Cassandra some sort of rack-awareness. We could reproduce this behavior on our read tests as shown in the following chart:</p><p dir=\"ltr\"> <img src=\"https://lh6.googleusercontent.com/nfXiH0qO3JJNb6MjfQlqnvudZqStqgACx76nkLaELKpdZILASz02YEKlXB9uTzh3WjKYEVHUdv832Oz8B24qM0TFD6Yfvm3K2P95qjHmWwsclyzWJngxOBkn06mjm6A0VOgQKVQ\" alt=\"\" width=\"538\" height=\"409\" /></p><p dir=\"ltr\"><em>Figure 1. Number of local read-requests per node on multi-AZ setup. Replicas in the same AZ are preferred. Set up: 6 nodes cluster spanning across 3 Azs. Read are performed with Consistency Level=ONE</em></p><p dir=\"ltr\"> </p><p dir=\"ltr\">Figure 1 shows how 10M read requests are distributed across the cluster. As you can see, <strong>most requests are handled within the local availability zone</strong>.</p><p dir=\"ltr\">About enhanced networking: AWS offers enhanced networking on their most recent instance families. <strong>Using enhanced networking results in consistently lower inter-instance latency</strong>. For more information about this topic, please follow <a href=\"http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html\">this link</a>.</p><h2>Guidelines for Deciding the Number of Availability Zones to Use</h2><p dir=\"ltr\">Cassandra can be configured in such a way that every availability zone has at least 1 entire copy of the dataset. Cassandra refers to this scenario as making an AZ self-contained. To achieve this, you need to place your nodes across a number of AZs that is less or equal your replication factor. It is also recommended to have the same number of nodes running on every AZ.</p><p dir=\"ltr\">In general it is beneficial to have:</p><p dir=\"ltr\">Availability Zones &lt;= Replication Factor</p><p dir=\"ltr\">At <a href=\"https://getstream.io/\">Stream</a> <strong>we’ve chosen to use a replication factor of 3 with 3 different availability zones</strong>. This ensures that every availability zone has a copy of the data, and that we have enough capacity left to handle read and write requests in the unlikely event of an AZ outage.</p><h2 dir=\"ltr\">Conclusion</h2><p dir=\"ltr\">Cassandra is an amazing database. At Stream we rely heavily on it to keep the feeds running for tens of millions of end users. In short, we do so because Cassandra has the ability to:</p><ul><li dir=\"ltr\">\n<p dir=\"ltr\">Shard data automatically</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\">Handle instance failures without data loss or downtime</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\">Scale (almost) linearly</p>\n</li>\n</ul><p dir=\"ltr\">In this post we’ve explained how to configure Cassandra in a highly available Multi-AZ setup on AWS EC2. The costs and performance are almost identical to a single availability zone deployment. A few key takeaways:</p><ul><li dir=\"ltr\">\n<p dir=\"ltr\"><strong>Placing nodes across multiple availability zones makes your Cassandra cluster more available and resilient to availability zone outages</strong>.</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><strong>No additional storage is needed to run on multiple AZs and the cost increase is minimal</strong>. Traffic between AZs isn’t free but for most use cases this isn’t a major concern.</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><strong>A replication factor of 3 combined with using 3 availability zones is a good starting point for most use cases</strong>. This enables your Cassandra cluster to be self-contained.</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><strong>AWS has done a great job at keeping the latency between availability zones low</strong>. Especially if you use an instance with network performance set to “high” and have enhanced networking enabled. </p>\n</li>\n</ul>",
        "created_at": "2017-12-21T22:52:10+0000",
        "updated_at": "2018-10-10T17:29:47+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 10,
        "domain_name": "highscalability.com",
        "preview_picture": "https://lh6.googleusercontent.com/nfXiH0qO3JJNb6MjfQlqnvudZqStqgACx76nkLaELKpdZILASz02YEKlXB9uTzh3WjKYEVHUdv832Oz8B24qM0TFD6Yfvm3K2P95qjHmWwsclyzWJngxOBkn06mjm6A0VOgQKVQ",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/8862"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 182,
            "label": "mongo",
            "slug": "mongo"
          },
          {
            "id": 253,
            "label": "analytics",
            "slug": "analytics"
          },
          {
            "id": 529,
            "label": "ravendb",
            "slug": "ravendb"
          }
        ],
        "is_public": false,
        "id": 7118,
        "uid": null,
        "title": "Cassandra, Hive, and Hadoop: How We Picked Our Analytics Stack",
        "url": "http://blog.markedup.com/2013/02/cassandra-hive-and-hadoop-how-we-picked-our-analytics-stack/",
        "content": "wallabag can't retrieve contents for this article. Please <a href=\"http://doc.wallabag.org/en/master/user/errors_during_fetching.html#how-can-i-help-to-fix-that\">troubleshoot this issue</a>.\n",
        "created_at": "2015-03-18T12:35:32+0000",
        "updated_at": "2018-07-19T22:42:20+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "",
        "reading_time": 0,
        "domain_name": "blog.markedup.com",
        "preview_picture": "http://markedupblog.blob.core.windows.net/wordpress/2013/02/traditionalreadwritecharactertistics.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/7118"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 26,
            "label": "research.and.development",
            "slug": "research-and-development"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 182,
            "label": "mongo",
            "slug": "mongo"
          },
          {
            "id": 254,
            "label": "nosql",
            "slug": "nosql"
          }
        ],
        "is_public": false,
        "id": 7095,
        "uid": null,
        "title": "Cassandra vs MongoDB vs CouchDB vs Redis vs Riak vs HBase vs Couchbase vs Hypertable vs ElasticSearch vs Accumulo vs VoltDB vs Scalaris comparison",
        "url": "https://kkovacs.eu/cassandra-vs-mongodb-vs-couchdb-vs-redis",
        "content": "wallabag can't retrieve contents for this article. Please <a href=\"http://doc.wallabag.org/en/master/user/errors_during_fetching.html#how-can-i-help-to-fix-that\">troubleshoot this issue</a>.\n",
        "created_at": "2015-03-26T03:07:27+0000",
        "updated_at": "2018-07-19T22:43:02+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "",
        "reading_time": 0,
        "domain_name": "kkovacs.eu",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/7095"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 207,
            "label": "system",
            "slug": "system"
          },
          {
            "id": 1126,
            "label": "openshift",
            "slug": "openshift"
          }
        ],
        "is_public": false,
        "id": 5342,
        "uid": null,
        "title": "Adding Cassandra Node in OpenShift to Existing Cluster",
        "url": "https://serverfault.com/questions/619052/adding-cassandra-node-in-openshift-to-existing-cluster",
        "content": "<p>I have 3 cassandra nodes(S1,S2,S3) which are installed on CentOS 6. Two of them (S1,S2) are in datacenter A and one of them (S3) is in openshift. I am trying to make benchmark test with multiple node clusters in multiple data centers.</p>\n<p>Cassandra installations and configurations on nodes S1 and S2 are based on <a href=\"http://www.datastax.com/documentation/cassandra/2.0/cassandra/install/installRHEL_t.html\" rel=\"nofollow noreferrer\">Installing DataStax Community on RHEL-based systems</a> on the other hand cassandra installation on node S3 (openshift) is based on <a href=\"https://www.openshift.com/blogs/cassandra-on-openshift\" rel=\"nofollow noreferrer\">How To Configure and Run Cassandra on OpenShift</a>.</p>\n<p>All servers running fine but i could not add node S3 to cluster. All servers has exactly same configuration files (cassandra.yaml and cassandra-topology.properties)</p>\n<p>Nodes S1 and S2 have 1.2.18 version of cassandra.</p>\n<pre>[root@node1 ~]# cassandra -v\n1.2.18\n</pre>\n<p>Node S3 has 1.1.6 version of cassandra.</p>\n<pre>[cassandra-xxxx.rhcloud.com xxxxxxxx]\\&gt; app-root/data/cassandra/bin/cassandra -v\nxss =  -ea -javaagent:app-root/data/cassandra/bin/../lib/jamm-0.2.5.jar -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -Xms1834M -Xmx1834M -Xmn256M -XX:+HeapDumpOnOutOfMemoryError -Xss180k\n1.1.6 \n</pre>\n<p>All nodes have same cassandra.yaml and cassandra-topology.properties configurations as shown below(except listening ip address).</p>\n<p><strong>cassandra.yaml</strong></p>\n<pre>cluster_name: 'MyDemoCluster'\nnum_tokens: 256\nseed_provider:\n  - class_name: org.apache.cassandra.locator.SimpleSeedProvider\n    parameters:\n         - seeds:  \"x.x.x.236\"\nlisten_address: x.x.x.x\nendpoint_snitch: PropertyFileSnitch\n</pre>\n<p><strong>cassandra-topology.properties</strong></p>\n<pre># Cassandra Node IP=Data Center:Rack\nx.x.x.236=XXX:RAC1\nx.x.x.237=XXX:RAC2\ny.y.y.yyy=OPS:RAC1\n</pre>\n<p>First of all , i started seed node while other nodes stopping. After a successfully start action i started other nodes(without any error messages in log files). </p>\n<p>Then when i run nodetool on seed node , i only see nodes S1 and S2 but not node S3.</p>\n<pre>[root@node1]# nodetool status\nDatacenter: XXX\n===============\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address          Load       Tokens  Owns (effective)  Host ID                               Rack\nUN  x.x.x.236  79.25 KB   256     100.0%            xxxxxxxx-xxxx-404a-aa3f-fbddfb5da124  RAC2\nUN  x.x.x.237  79.56 KB   256     100.0%            xxxxxxxx-xxxx-4b27-970b-76944c382c9f  RAC1\n</pre>\n<p><strong>cassandra log of Node S2</strong></p>\n<pre> INFO [HANDSHAKE-/x.x.x.236] 2014-08-08 11:25:26,120 OutboundTcpConnection.java (line 418) Handshaking version with /x.x.x.\n</pre>\n<p><strong>Running cassandra on node S3</strong></p>\n<pre>    [cassandra-biberltd.rhcloud.com data]\\&gt; cassandra/bin/cassandra -f\nxss =  -ea -javaagent:cassandra/bin/../lib/jamm-0.2.5.jar -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -Xms1834M -Xmx1834M -Xmn256M -XX:+HeapDumpOnOutOfMemoryError -Xss180k\n INFO 11:36:31,296 Logging initialized\n INFO 11:36:31,305 JVM vendor/version: OpenJDK Server VM/1.7.0_65\n INFO 11:36:31,305 Heap size: 1896284160/1896284160\n INFO 11:36:31,305 Classpath: cassandra/bin/../conf:cassandra/bin/../build/classes/main:cassandra/bin/../build/classes/thrift:cassandra/bin/../lib/antlr-3.2.jar:cassandra/bin/../lib/apache-cassandra-1.1.6.jar:cassandra/bin/../lib/apache-cassandra-clientutil-1.1.6.jar:cassandra/bin/../lib/apache-cassandra-thrift-1.1.6.jar:cassandra/bin/../lib/avro-1.4.0-fixes.jar:cassandra/bin/../lib/avro-1.4.0-sources-fixes.jar:cassandra/bin/../lib/commons-cli-1.1.jar:cassandra/bin/../lib/commons-codec-1.2.jar:cassandra/bin/../lib/commons-lang-2.4.jar:cassandra/bin/../lib/compress-lzf-0.8.4.jar:cassandra/bin/../lib/concurrentlinkedhashmap-lru-1.3.jar:cassandra/bin/../lib/guava-r08.jar:cassandra/bin/../lib/high-scale-lib-1.1.2.jar:cassandra/bin/../lib/jackson-core-asl-1.9.2.jar:cassandra/bin/../lib/jackson-mapper-asl-1.9.2.jar:cassandra/bin/../lib/jamm-0.2.5.jar:cassandra/bin/../lib/jline-0.9.94.jar:cassandra/bin/../lib/json-simple-1.1.jar:cassandra/bin/../lib/libthrift-0.7.0.jar:cassandra/bin/../lib/log4j-1.2.16.jar:cassandra/bin/../lib/metrics-core-2.0.3.jar:cassandra/bin/../lib/servlet-api-2.5-20081211.jar:cassandra/bin/../lib/slf4j-api-1.6.1.jar:cassandra/bin/../lib/slf4j-log4j12-1.6.1.jar:cassandra/bin/../lib/snakeyaml-1.6.jar:cassandra/bin/../lib/snappy-java-1.0.4.1.jar:cassandra/bin/../lib/snaptree-0.1.jar:cassandra/bin/../lib/jamm-0.2.5.jar\n INFO 11:36:31,308 JNA not found. Native methods will be disabled.\n INFO 11:36:31,329 Loading settings from file:/var/lib/openshift/53e4cf805004462c540000a9/app-root/data/cassandra/conf/cassandra.yaml\n INFO 11:36:31,649 32bit JVM detected.  It is recommended to run Cassandra on a 64bit JVM for better performance.\n INFO 11:36:31,650 DiskAccessMode 'auto' determined to be standard, indexAccessMode is standard\n INFO 11:36:32,322 Global memtable threshold is enabled at 602MB\n INFO 11:36:33,196 Initializing key cache with capacity of 90 MBs.\n INFO 11:36:33,218 Scheduling key cache save to each 14400 seconds (going to save all keys).\n INFO 11:36:33,220 Initializing row cache with capacity of 0 MBs and provider org.apache.cassandra.cache.SerializingCacheProvider\n INFO 11:36:33,228 Scheduling row cache save to each 0 seconds (going to save all keys).\n INFO 11:36:33,595 Couldn't detect any schema definitions in local storage.\n INFO 11:36:33,596 Found table data in data directories. Consider using the CLI to define your schema.\n INFO 11:36:33,664 No commitlog files found; skipping replay\n INFO 11:36:33,730 Cassandra version: 1.1.6\n INFO 11:36:33,730 Thrift API version: 19.32.0\n INFO 11:36:33,735 CQL supported versions: 2.0.0,3.0.0-beta1 (default: 2.0.0)\n INFO 11:36:33,835 Loading persisted ring state\n INFO 11:36:33,839 Starting up server gossip\n INFO 11:36:33,862 Enqueuing flush of Memtable-LocationInfo@8039520(135/168 serialized/live bytes, 3 ops)\n INFO 11:36:33,863 Writing Memtable-LocationInfo@8039520(135/168 serialized/live bytes, 3 ops)\n INFO 11:36:33,945 Completed flushing $HOME/app-root/data/cassandra-data/data/system/LocationInfo/system-LocationInfo-hf-1-Data.db (243 bytes) for commitlog position ReplayPosition(segmentId=1407512193559, position=595)\n INFO 11:36:33,982 Starting Messaging Service on port 17000\n INFO 11:36:33,995 JOINING: waiting for ring information\n INFO 11:37:04,035 JOINING: schema complete, ready to bootstrap\n INFO 11:37:04,036 JOINING: getting bootstrap token\n INFO 11:37:04,045 Enqueuing flush of Memtable-LocationInfo@21839213(53/66 serialized/live bytes, 2 ops)\n INFO 11:37:04,046 Writing Memtable-LocationInfo@21839213(53/66 serialized/live bytes, 2 ops)\n INFO 11:37:04,079 Completed flushing $HOME/app-root/data/cassandra-data/data/system/LocationInfo/system-LocationInfo-hf-2-Data.db (163 bytes) for commitlog position ReplayPosition(segmentId=1407512193559, position=776)\n INFO 11:37:04,082 JOINING: sleeping 30000 ms for pending range setup\n INFO 11:37:34,083 JOINING: Starting to bootstrap...\n INFO 11:37:34,103 Enqueuing flush of Memtable-LocationInfo@21895329(53/66 serialized/live bytes, 2 ops)\n INFO 11:37:34,103 Writing Memtable-LocationInfo@21895329(53/66 serialized/live bytes, 2 ops)\n INFO 11:37:34,140 Completed flushing $HOME/app-root/data/cassandra-data/data/system/LocationInfo/system-LocationInfo-hf-3-Data.db (163 bytes) for commitlog position ReplayPosition(segmentId=1407512193559, position=957)\n INFO 11:37:34,154 Node /y.y.y.yyy state jump to normal\n INFO 11:37:34,155 Bootstrap/Replace/Move completed! Now serving reads.\n INFO 11:37:34,221 Binding thrift service to /y.y.y.yyy:19160\n INFO 11:37:34,226 Using TFastFramedTransport with a max frame size of 15728640 bytes.\n INFO 11:37:34,232 Using synchronous/threadpool thrift server on /y.y.y.yyy : 19160\n INFO 11:37:34,233 Listening for thrift clients...\n</pre>\n<p>I only found </p>\n<pre> INFO 11:36:31,308 JNA not found. Native methods will be disabled.\n</pre>\n<p>My questions are </p>\n<ol><li>How can i add openshift node to cluster?</li>\n<li>Does openshift support nodes which is a member of multiple data centers cluster?</li>\n<li>Must all nodes have same version of cassandra?</li>\n</ol>",
        "created_at": "2017-01-06T12:53:19+0000",
        "updated_at": "2018-07-20T11:40:09+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "",
        "reading_time": 6,
        "domain_name": "serverfault.com",
        "preview_picture": "https://cdn.sstatic.net/Sites/serverfault/img/apple-touch-icon@2.png?v=9b1f48ae296b",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/5342"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 5276,
        "uid": null,
        "title": "This Team Used Apache Cassandra… You Won’t Believe What Happened Next",
        "url": "https://blog.parse.ly/post/1928/cass/",
        "content": "<p>You know that old saying, “If it seems too good to be true, it probably is?” We technologists should probably apply that saying to database vendor claims pretty regularly.</p>\n<p>In the summer of 2014, the Parse.ly team finally kicked the tires on Apache Cassandra. I say “finally”, because after years of using other distributed database technologies on our real-time analytics problems, I was tired of hearing from technologists — even ones on my own team — saying that our problem domain was “so obviously a fit for Cassandra”, that I figured we should at least entertain the notion.</p>\n<p>After all, Cassandra is a highly-available, linearly-scalable data store. It is supposedly battle-tested at Facebook, Apple, and Netflix scale. It is supposedly the key to Netflix’s horizontal and elastic scalability in the AWS cloud. It is supposedly built to let your ops staff sleep easily at night, handling relentless write volumes of hundreds of thousands per second with ease and grace. It supposedly has an unassailable reliability model that lets you pull the plug on nodes without intervention, thanks to cluster auto-healing. And here’s the kicker — it’s supposedly optimized for Parse.ly’s data set: analytics data, and especially for time series data.</p>\n<div style=\"margin: auto;\"><a href=\"https://i0.wp.com/blog.parse.ly/wp-content/uploads/2015/04/cassandra-e1430921268579.png\"><img data-attachment-id=\"1890\" data-permalink=\"https://blog.parse.ly/post/1822/event-loops/cassandra/\" data-orig-file=\"https://i0.wp.com/blog.parse.ly/wp-content/uploads/2015/04/cassandra-e1430921268579.png?fit=500%2C335&amp;ssl=1\" data-orig-size=\"500,335\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"cassandra\" data-image-description=\"\" data-medium-file=\"https://i0.wp.com/blog.parse.ly/wp-content/uploads/2015/04/cassandra-e1430921268579.png?fit=300%2C201&amp;ssl=1\" data-large-file=\"https://i0.wp.com/blog.parse.ly/wp-content/uploads/2015/04/cassandra-e1430921268579.png?fit=1024%2C687&amp;ssl=1\" src=\"https://i0.wp.com/blog.parse.ly/wp-content/uploads/2015/04/cassandra-e1430921268579.png?resize=500%2C335\" alt=\"cassandra\" width=\"500\" height=\"335\" class=\"alignnone size-full wp-image-1890\" data-recalc-dims=\"1\" /></a></div>\n<p>That’s a lot of “supposedly’s!”</p>\n<p>What we learned over the course of the next 6 months really, really surprised us. I think it’ll surprise you, too. We learned that Cassandra, though a very cool technology, is not a panacea for real-time analytics or time series problems. We also learned that the technology is loaded with traps that require deep knowledge of Cassandra internals to work around.</p>\n<p>We also find the technology to very much violate the “principle of least surprise” — indeed, almost every Cassandra feature has some surprising behavior.</p>\n<p>The rest of this article explores these surprises through the use of fun “Internet news style headlines” followed by a short explanation of the problem. We hope it helps other teams use Cassandra with more confidence and fewer battle scars.</p>\n<p>Here’s a preview of the headlines:</p>\n<h2>INDUSTRY SHOCKER: CQL is not SQL</h2>\n<p>Our team had studied Cassandra before the project had introduced CQL, the “Cassandra Query Language”, and the 2.x line of Cassandra releases. You would be wise to study Cassandra’s history before diving into CQL, because CQL has nothing to do with SQL, and any relationship will only lead you to surprises.</p>\n<p>You see, in Cassandra 1.x, the data model is centered around what Cassandra calls “column families”. A column family contains rows, which are identified by a row key. The row key is what you need to fetch the data from the row. The row can then have one or more columns, each of which has a name, value, and timestamp. (A value is also called a “cell”). Cassandra’s data model flexibility comes from the following facts:</p>\n<ul><li>column names are defined per-row</li>\n<li>rows can be “wide” — that is, have hundreds, thousands, or even millions of columns</li>\n<li>columns can be sorted, and ranges of ordered columns can be selected efficiently using “slices”</li>\n</ul><div style=\"margin: auto;\"><img data-attachment-id=\"1945\" data-permalink=\"https://blog.parse.ly/post/1928/cass/cass_data_model/\" data-orig-file=\"https://i2.wp.com/blog.parse.ly/wp-content/uploads/2015/05/cass_data_model.png?fit=595%2C416&amp;ssl=1\" data-orig-size=\"595,416\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"cass_data_model\" data-image-description=\"\" data-medium-file=\"https://i2.wp.com/blog.parse.ly/wp-content/uploads/2015/05/cass_data_model.png?fit=300%2C210&amp;ssl=1\" data-large-file=\"https://i2.wp.com/blog.parse.ly/wp-content/uploads/2015/05/cass_data_model.png?fit=595%2C416&amp;ssl=1\" src=\"https://i2.wp.com/blog.parse.ly/wp-content/uploads/2015/05/cass_data_model.png?resize=595%2C416\" alt=\"cass_data_model\" width=\"595\" height=\"416\" class=\"alignnone size-full wp-image-1945\" srcset=\"https://i2.wp.com/blog.parse.ly/wp-content/uploads/2015/05/cass_data_model.png?w=595&amp;ssl=1 595w, https://i2.wp.com/blog.parse.ly/wp-content/uploads/2015/05/cass_data_model.png?resize=300%2C210&amp;ssl=1 300w\" data-recalc-dims=\"1\" /></div>\n<p>Cassandra’s data partitioning scheme comes from ensuring sharding and replication occurs on a row key basis. That is, ranges of row keys will establish cluster “shards”, and rows will be automatically replicated among cluster nodes.</p>\n<p>Notice that I haven’t mentioned CQL yet. That’s because, CQL tries to hide every detail I described above from you, even though this knowledge is <em>critical</em> to running a Cassandra cluster. In CQL and Cassandra 2.x, column families are renamed to “tables”. Row keys become “primary keys” on those tables. A syntax that looks like a restricted subset of SQL (SELECT, INSERT, CREATE TABLE) offers a facade on these Cassandra facilities, but with none of the underlying SQL machinations.</p>\n<p>The fact that rows can be wide or narrow falls out of how you design your CQL schema and how you use the resultant “tables” (that is, column families). We will discuss wide vs skinny rows later, so I’ll get there.</p>\n<p>Here’s the core warning: CQL is SQL in the same way JavaScript is Java. (That is, it’s <strong>not</strong>.)</p>\n<p>JavaScript was designed to look somewhat “Java-like”, because Java was trendy and well-understood by technologists when JavaScript was designed. But JavaScript is not Java. In fact, JavaScript, in all respects other than a superficial set of syntax similarities, couldn’t be more different than Java if it tried.</p>\n<p>CQL was designed to look somewhat “SQL-like”, because SQL is trendy and well-understood by technologists today. But the relationship with SQL is the same: in all respects other than the superficial, it is a completely different beast.</p>\n<p>I hope this warning helps. It will definitely help you interpret the following statement that is in Apache Cassandra’s Github README: “The Cassandra Query Language (CQL) is a close relative of SQL.” This is a false statement. You should ignore it. It’s more like a distant cousin — who later finds out she was actually adopted by her parents.</p>\n<p>The way I read this line in the README is that the developers are excited, because CQL does actually make Cassandra easier to use. It does something important for the community: it unifies the client libraries around a single way of interacting with the database, like SQL did for many SQL databases. This is A Good Thing, but it doesn’t mean that CQL is anything like SQL.</p>\n<p>There’s more to learn, so let’s keep going.</p>\n<h2>We Didn’t Use COMPACT STORAGE… You Won’t Believe What Happened Next</h2>\n<p>OK, so I cheated and re-used the headline format of this blog post itself, but I couldn’t resist. It just fit so well.</p>\n<p>What’s COMPACT STORAGE, you ask?</p>\n<p>Well, along with Cassandra 2.0 and CQL, the Cassandra project introduced some more capabilities that are built directly into CQL’s schema modeling capabilities. It tried to codify patterns from the community, specifically, ways of storing maps, lists, and sets of data inside a Cassandra row.</p>\n<p>A Cassandra row is already sort of like an ordered map, where each column is a key in the map; so, storing maps/lists/sets in a Cassandra row is like storing maps/lists/sets inside an ordered map. This is an admirable goal, since it does provide some data modeling flexibility. Cassandra calls these CQL Collections, and, like sirens singing beautiful hymns off a sun-kissed shoreline, they seem very attractive.</p>\n<p>CQL Collections required some enforced structure onto the rows that are stored in column families. This structure allows a mixture of map fields, list fields, and set fields to exist within a single logical table (remember: table = column family). Under the hood, Cassandra only knows about keys, column names, and values. So, this structure is built up by smartly inserting “marker columns” and embedding concatenated key names into column names appropriately. Imagine if in Python, rather than storing a dict in a dict, you needed to store the sub-dict keys inside the parent dict’s keys.</p>\n<pre># \"embedding\" of unstructured data\npoints = {\n    \"point1\": {\"x\": 12, \"y\": 10},\n    \"point2\": {\"x\": 8: \"y\": 3},\n    # ... and so on ...\n}\n# vs \"encoding\" of unstructured data\npoints = {\n    \"point1::\": \"dict\"\n    \"point1::x\": 12,\n    \"point1::y\": 10,\n    \"point2::\": \"dict\"\n    \"point2::x\": 8,\n    \"point2::y\": 3,\n    # ... and so on ...\n}\n</pre>\n<p>CQL Collections don’t really “embed” structure into cells, so much as they “encode” structure into columns. The cost of all of this structure: data storage on disk.</p>\n<p>To be fair, Cassandra operates with the assumption that “disk is cheap”, but in a simple example we had at Parse.ly where we attempted to use a CQL Map to store analytics data, we saw 30X data size overhead vs using a simpler storage format and Cassandra’s old storage format, now called COMPACT STORAGE. Ah, that’s where the name comes from: COMPACT, as in small, lightweight. Put another way, Cassandra and CQL’s new default storage format is NOT COMPACT, that is, large and heavyweight.</p>\n<div style=\"margin: auto;\"><a href=\"https://i0.wp.com/blog.parse.ly/wp-content/uploads/2015/05/say_big_data.jpg\"><img data-attachment-id=\"1948\" data-permalink=\"https://blog.parse.ly/post/1928/cass/say_big_data/\" data-orig-file=\"https://i0.wp.com/blog.parse.ly/wp-content/uploads/2015/05/say_big_data.jpg?fit=500%2C351&amp;ssl=1\" data-orig-size=\"500,351\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"say_big_data\" data-image-description=\"\" data-medium-file=\"https://i0.wp.com/blog.parse.ly/wp-content/uploads/2015/05/say_big_data.jpg?fit=300%2C211&amp;ssl=1\" data-large-file=\"https://i0.wp.com/blog.parse.ly/wp-content/uploads/2015/05/say_big_data.jpg?fit=500%2C351&amp;ssl=1\" src=\"https://i0.wp.com/blog.parse.ly/wp-content/uploads/2015/05/say_big_data.jpg?resize=500%2C351\" alt=\"say_big_data\" width=\"500\" height=\"351\" class=\"alignnone size-full wp-image-1948\" srcset=\"https://i0.wp.com/blog.parse.ly/wp-content/uploads/2015/05/say_big_data.jpg?w=500&amp;ssl=1 500w, https://i0.wp.com/blog.parse.ly/wp-content/uploads/2015/05/say_big_data.jpg?resize=300%2C211&amp;ssl=1 300w\" data-recalc-dims=\"1\" /></a></div>\n<p>Now, the whole reason we were adopting Cassandra in the first place is because we have huge datasets. Terabytes upon terabytes of raw analytics and event data. Suffering a 30X data overhead for storage (and that’s even after compression) is not a trivial matter. We were actually hoping that Cassandra could store things <strong>more compactly</strong> than our raw data, but this wasn’t happening, due to the overhead of CQL Collections and the CQL 3.0 row format.</p>\n<p>So, we did what any team that enjoys practicality over purity would do. We scrapped our hopes of using the “new shiny” and stuck with the appropriately-named COMPACT STORAGE. Though the Cassandra documentation is full of warnings about how COMPACT STORAGE is only for backwards-compatibility with CQL 2.0 and that this restricts your use of the more advanced features, what we found is that the cost of those features simply did not make sense at large data scale.</p>\n<p>This team didn’t use COMPACT STORAGE, and the result was they got storage that wasn’t compact!</p>\n<h2>REVEALED: \\x01 Separators Are Not The Ugly Hack They Appear To Be</h2>\n<p>Speaking of compact vs non-compact storage types, if we aren’t able to use Cassandra CQL 3.0 and CQL Maps to store unstructured data in Cassandra, how do we pull it off?</p>\n<p>We decided to pick an extremely simple separated-value storage format for every analytics event, each of which goes into a column that is named with that analytics event identifier and timestamp. We lovingly labeled this scheme “xsv”, because the separator we chose to use is the Unicode escape character, \\x01.</p>\n<p>So, for example, in storing a visit to a specific URL from Twitter, we might store values like this:</p>\n<pre>name=d, ts=2015-02-01T08:00:01.123Z::\nhttp://site.com/url1x01http://t.co/1234\nname=d, ts=2015-02-01T08:00:05.832Z::\nhttp://site.com/url2x01http://t.co/1235\n</pre>\n<p>The way to read the xsv values is to imagine them being unpacked by Python that looks like this:</p>\n<pre class=\"python\">&gt;&gt;&gt; url, urlref = xsv_parse(line)\n&gt;&gt;&gt; url\n\"http://site.com/url1\"\n&gt;&gt;&gt; urlref\n\"http://t.co/1234\"\n</pre>\n<p>In other words, the keys for these separated values are not stored in Cassandra itself; instead, they are stored in our serialization/deserialization code.</p>\n<p>This seems like a terribly ugly hack, but it’s actually really not.</p>\n<p>The \\x01 value shows up in a different color in Cassandra’s CQL shell, cqlsh. It is easy to split and serialize. It doesn’t require quoting of values, like traditional CSV would (because, every byte counts!). Of course, you need to worry about \\x01’s showing up in your raw data, but you can escape these, and they are quite rare.</p>\n<div style=\"margin: auto;\"><a href=\"https://i1.wp.com/blog.parse.ly/wp-content/uploads/2015/05/xsv_data.png\"><img data-attachment-id=\"1950\" data-permalink=\"https://blog.parse.ly/post/1928/cass/xsv_data/\" data-orig-file=\"https://i1.wp.com/blog.parse.ly/wp-content/uploads/2015/05/xsv_data.png?fit=777%2C434&amp;ssl=1\" data-orig-size=\"777,434\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"xsv_data\" data-image-description=\"\" data-medium-file=\"https://i1.wp.com/blog.parse.ly/wp-content/uploads/2015/05/xsv_data.png?fit=300%2C168&amp;ssl=1\" data-large-file=\"https://i1.wp.com/blog.parse.ly/wp-content/uploads/2015/05/xsv_data.png?fit=777%2C434&amp;ssl=1\" src=\"https://i1.wp.com/blog.parse.ly/wp-content/uploads/2015/05/xsv_data.png?resize=777%2C434\" alt=\"xsv_data\" width=\"777\" height=\"434\" class=\"alignnone size-full wp-image-1950\" srcset=\"https://i1.wp.com/blog.parse.ly/wp-content/uploads/2015/05/xsv_data.png?w=777&amp;ssl=1 777w, https://i1.wp.com/blog.parse.ly/wp-content/uploads/2015/05/xsv_data.png?resize=300%2C168&amp;ssl=1 300w\" data-recalc-dims=\"1\" /></a></div>\n<p>The downside is that the schema stored in each row is not “self-describing”, as it might be with CQL Maps. But, the storage savings are so tremendous that we can live with that. And, since we use a consistent order, we can “grow” our schema over time by simply appending new values on the right side of the serialization form.</p>\n<p>Here’s the other amazing thing. The client performance of these xsv values is much, much better than the CQL Collections equivalent, and even better than a schema we tried where we actually modeled every field directly in the CQL schema. Why? Because the client would spend most of its time decoding the “structure” of the row, rather than decoding the actual values, in both of those cases.</p>\n<p>I know what you’re thinking — why not use something like Avro or Protocol Buffers? We considered this, but felt that this would make Cassandra’s data storage a bit too inscrutable. You wouldn’t be able to meaningfully use the CQL shell — thus eliminating one of the main benefits of CQL. We were willing to throw out CQL 3.0 and CQL Collections, but not the entire idea of having a usable CQL shell!</p>\n<p>We also did some calculations and found that xsv performed similarly for our data set, anyway — since most of our values are strings, the binary serialization approach doesn’t actually help much.</p>\n<p>So, to store large gobs of  unstructured data in Cassandra, we recommend COMPACT STORAGE with serialized values in an “xsv” format. They are more storage-efficient and more CPU- and I/O-efficient. Go figure! It smokes all the “official” ways of modeling unstructured data in Cassandra.</p>\n<p>That means our team’s advice is to ignore all the advice against using COMPACT STORAGE in the docs, and ignore all the fancy new CQL 3 stuff. Pretty weird — but, unlike the marketed features, this scheme actually works!</p>\n<h2>Evil Cassandra Counters: They Still Hate Us! (Even in 2.1.x)</h2>\n<p>Oh, my. I was dreading this section. I got burned badly by Cassandra Counters.</p>\n<p>I shudder even now to think about the six-or-so weeks of development time I lost trying to make Counters work for our use case.</p>\n<p>If you watch a lot of Cassandra presentations online, you’ll hear different things about Counters from different people. Cassandra old-timers will say things like, “Counters are evil”, “Counters are a hack”, “Counters barely work”, “Counters are dangerous, “Never, ever use Counters…” — you get the idea.</p>\n<p>When I’m in a good mood, I sometimes ask questions about Counters in the Cassandra IRC channel, and if I’m lucky, long-time developers don’t laugh me out of the room. Sometimes, they just call me a “brave soul”.</p>\n<p>In Cassandra 2.x, things are more hopeful. Counters received several sprints of development and the feature was starting to be better understood. First, a bunch of fixes in 2.0.x that made Counters more reliable. Then, a whole new Counters implementation in 2.1.x that improved their performance, especially under contention.</p>\n<div style=\"margin: auto;\"><img data-attachment-id=\"1952\" data-permalink=\"https://blog.parse.ly/post/1928/cass/counter_incs/\" data-orig-file=\"https://i0.wp.com/blog.parse.ly/wp-content/uploads/2015/05/counter_incs.png?fit=426%2C256&amp;ssl=1\" data-orig-size=\"426,256\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"counter_incs\" data-image-description=\"\" data-medium-file=\"https://i0.wp.com/blog.parse.ly/wp-content/uploads/2015/05/counter_incs.png?fit=300%2C180&amp;ssl=1\" data-large-file=\"https://i0.wp.com/blog.parse.ly/wp-content/uploads/2015/05/counter_incs.png?fit=426%2C256&amp;ssl=1\" src=\"https://i0.wp.com/blog.parse.ly/wp-content/uploads/2015/05/counter_incs.png?resize=426%2C256\" alt=\"counter_incs\" width=\"426\" height=\"256\" class=\"alignnone size-full wp-image-1952\" srcset=\"https://i0.wp.com/blog.parse.ly/wp-content/uploads/2015/05/counter_incs.png?w=426&amp;ssl=1 426w, https://i0.wp.com/blog.parse.ly/wp-content/uploads/2015/05/counter_incs.png?resize=300%2C180&amp;ssl=1 300w\" data-recalc-dims=\"1\" /></div>\n<p>The reason Counters are so controversial and weird in the Cassandra community is because they seem to be at odds with Cassandra’s philosophy. Cassandra is meant to be a data store built on principles of immutability and idempotence. Incrementing a counter is a non-idempotent operation that seems to require an inherently mutable data store (like Redis or MongoDB).</p>\n<p>Cassandra works around this by implementing Counter increments as “read-then-write” operation, similar to another Cassandra trap feature, Light-Weight Transactions (LWT). The implementation is clever and it works, but it does not have the same performance as plain Cassandra row/column INSERT’s, for which it is optimized.</p>\n<p>Now, even if you can live with Cassandra’s different performance characteristics for Counters, you’ll discover other oddities. For example, column families that use counters have to live on their own, because the underlying storage is different. Counters can’t really be deleted — unless the delete is “definitive”, and even, then, there seem to be weird cluster bugs that surface with counter deletes. Related to counters not being deletable, you also learn that Cassandra’s time-to-live (TTL) features for data expiration simply do not work on Counters.</p>\n<p>Finally, you’ll find that by opting into Counters, you threw out the baby with the bathwater — yes, you got a convenient durable data structure for storing counts, but you lost all the benefits of having an idempotent data store. This matters in analytics use cases, although we didn’t realize quite how much it mattered until we were well into it.</p>\n<p>All of this is to say: Cassandra Counters — it’s a trap! Run!</p>\n<p>Even in 2.1.x, when many of Cassandra Counters problems were fixed, we still find it’s a trap. Since we didn’t need Counters, we also decided to roll back to the “more stable” Cassandra line, 2.0.x. It still supports CQL, but has been in production longer, and is maintained as a parallel stable branch to 2.1.x.</p>\n<h2>Check Your Row Size: Too Wide, Too Narrow, or Just Right?</h2>\n<p>Remember when I mentioned how in Cassandra’s underlying data model, the row key determines how the system distributes your data?</p>\n<p>It turns out, the most important data model challenge for Cassandra users is controlling your row size. In the same way a fashion magazine ebbs and flows on what the right waist size is for men and women, Cassandra ebbs and flows on the question of what the right row size is.</p>\n<p>The short answer, that will be frustrating, is that you want your rows to be neither too wide, nor too narrow — but, just right. This is frustrating because you have no rules of thumb to go by. Cassandra theoretically supports rows with 2 billion columns, and is quite proud of this fact. But, you ask anyone in the community, and they will tell you — storing 2 billion columns in one row is a very, very bad idea. So, how many is right? 10,000? 100,000? A million? A hundred million?</p>\n<p>We demanded answers, but we couldn’t get consistent ones. For our own data set, we settled on roughly 100,000 — and we also suggested that our average row should have many fewer columns than that — typically a few thousand. For our data set, this seems to strike the right balance between read performance and compaction/memory pressure on the cluster.</p>\n<p>But, determining the right row size for your data will be an iterative process, and will require testing. So, think hard about it during schema design and during your integration tests. In our case, we decided to partition data by event type and on every tracked URL. We further partitioned the data by hour, so that the data for a single URL on a single day was stored in 24 row keys. This seemed to work well for 99.99% of our rows, until something awful happened.</p>\n<h2>“The Dress” Breaks the Internet — and, Our Database</h2>\n<p>The silly Internet meme about “The Dress” went around many of our customer sites, who are large news and information publishers. The result was that many customer URLs were receiving upwards of 40 million unique visitors <strong>per hour</strong> to a single URL. This meant that our partitioning scheme for Cassandra would get a “very wide row” — nowhere near 2 billion columns, to be sure, but definitely in the tens and hundreds of millions.</p>\n<p>This put compaction pressure on our cluster and also led to some write contention.</p>\n<p>Unfortunately, we couldn’t come up with a more natural partitioning scheme that would handle this rare case, so, instead, we had to introduce a “synthetic partition key”. That is, we have another system that tracks very hot URLs in a lightweight way, and when they reach a certain threshold, we systematically “partition the partitions” using a split factor.</p>\n<p>So, 40 million events might now be spread among 400 row keys, ensuring that each only holds 100,000 columns of data, keeping our row size “just right”.</p>\n<div style=\"margin: auto;\"><a href=\"https://i2.wp.com/blog.parse.ly/wp-content/uploads/2015/05/partitioning.png\"><img data-attachment-id=\"1946\" data-permalink=\"https://blog.parse.ly/post/1928/cass/partitioning/\" data-orig-file=\"https://i2.wp.com/blog.parse.ly/wp-content/uploads/2015/05/partitioning.png?fit=699%2C386&amp;ssl=1\" data-orig-size=\"699,386\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"partitioning\" data-image-description=\"\" data-medium-file=\"https://i2.wp.com/blog.parse.ly/wp-content/uploads/2015/05/partitioning.png?fit=300%2C166&amp;ssl=1\" data-large-file=\"https://i2.wp.com/blog.parse.ly/wp-content/uploads/2015/05/partitioning.png?fit=699%2C386&amp;ssl=1\" src=\"https://i1.wp.com/blog.parse.ly/wp-content/uploads/2015/05/partitioning-300x166.png?resize=300%2C166\" alt=\"partitioning\" width=\"300\" height=\"166\" class=\"alignnone size-medium wp-image-1946\" srcset=\"https://i2.wp.com/blog.parse.ly/wp-content/uploads/2015/05/partitioning.png?resize=300%2C166&amp;ssl=1 300w, https://i2.wp.com/blog.parse.ly/wp-content/uploads/2015/05/partitioning.png?w=699&amp;ssl=1 699w\" data-recalc-dims=\"1\" /></a></div>\n<p>This introduced operational complexity, but was necessary to make Cassandra work for our use case.</p>\n<h2>For Future Travelers: Here Be Dragons!</h2>\n<p>A well-seasoned technologist friend of mine was not at all surprised when I walked him through some of these issues we had with Cassandra. He said, “You honestly expected that adopting a data store at your scale would not require you to learn all of its internals?” He has a point. After all, we didn’t adopt Elasticsearch until <a href=\"http://blog.parse.ly/post/1691/lucene/\">we really grokked Lucene</a>.</p>\n<p>But what about time series and analytics data? The funny thing about Parse.ly’s use of Cassandra is that our original adoption of it was driven by the desire to utilize its “time series analytics” capabilities. But it turns out, without any capability to group, filter, or aggregate (the core functions of any <a href=\"http://en.wikipedia.org/wiki/OLAP_cube#Operations\">OLAP system</a>), Cassandra simply could not play that role. We had hoped that Counters would give us “basic aggregation” (by holding cumulative sums), but, no dice!</p>\n<p>Instead, we ended up using it as a data staging area, where data sits before we index in our Lucene-based time series system. We discussed this a little bit over at the Elastic blog, in the article, <a href=\"https://www.elastic.co/blog/pythonic-analytics-with-elasticsearch\">“Pythonic Analytics with Elasticsearch”</a>.</p>\n<p>Now that we’ve come to understand its strengths and limitations, it works well in that role — because providing a time-ordered, durable, idempotent, distributed data store <strong>is</strong> something that Cassandra can handle.</p>\n<p>If you adopt Cassandra for a large data use case, I recommend you heed the above advice:</p>\n<ul><li>learn what CQL actually is;</li>\n<li>avoid CQL Collections;</li>\n<li>use COMPACT STORAGE;</li>\n<li>adopt custom serialization formats;</li>\n<li>don’t use counters;</li>\n<li>stay on 2.0.x “most-stable”;</li>\n<li>manage row size carefully;</li>\n<li>and, watch out for partitioning hotspots</li>\n</ul><p>With these guidelines in mind, you will likely end up with a better experience.</p>\n<p>“If it’s too good to be true, it probably is.” Indeed.</p>\n<p>But, overall, Cassandra is a powerful tool. It’s one of the few <a href=\"https://aphyr.com/posts/294-call-me-maybe-cassandra/\">truly “AP” (Highly Available and Partition-Tolerant) data stores</a> with a very powerful data distribution and cluster scale-out model. Its main fault is over-marketing its bugs as features, and trying too hard to make its quirky features appealing to the mass market, by dumbing them down.</p>\n<p>For experienced distributed systems practitioners, adopt Cassandra with the comfort of knowing the scale of its existing deployments, but with the caution that comes from knowing that in large-scale data management, there is no silver bullet.</p>\n<h2>Postscript: Living with Cassandra</h2>\n<p><em>I asked Didier Deshommes, a long-time Parse.ly backend engineer, if he had any tips for newcomers. His tips are included here as the postscript to this article.</em></p>\n<p>Though this article discussed many of the traps we hit with Cassandra, there are many resources online for how to model data “the Cassandra way”. These can serve as some positive instruction.</p>\n<p>I find that modeling data in Cassandra involves essentially one trick with several variations (wide rows).</p>\n<p>Although there are several Cassandra how-to and data modeling tutorials, I usually keep going back to only a handful of links when I want to refresh my memory on them. The funny thing about these links is that many of them come from around the time CQL was introduced, or even before.</p>\n<h3>The WHERE Clause</h3>\n<p>Writing data is so easy in Cassandra that you often forget how to read it efficiently. The best guide I know for knowing what you can get away with is <a href=\"http://mechanics.flite.com/blog/2013/11/05/breaking-down-the-cql-where-clause/\">Breaking Down the CQL Where Clause</a>.</p>\n<p>As a nice side effect, this will also inform how to structure your writes so that you can take advantage of some of these rules.</p>\n<h3>Time Series Models</h3>\n<p>Cassandra is a one-trick pony, so sometimes you need a little creativity for fitting it to your problem. When I’m feeling stuck and worried I might run out of wide row tricks, I go back to <a href=\"http://www.datastax.com/dev/blog/advanced-time-series-with-cassandra\">Advanced Time Series with Cassandra</a> to give me ideas.</p>\n<p>This builds on Cassandra modeling techniques developed in 2011, in <a href=\"http://rubyscale.com/blog/2011/03/06/basic-time-series-with-cassandra/\">Basic Time Series with Cassandra</a>. I don’t go back to this older article as often, but it does introduce the well-worn idea of time-based rollups.</p>\n<h3>Tyler Hobbes</h3>\n<p>Tyler Hobbes, the author of the advanced time series post, also recently put out <a href=\"http://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\">Basic Rules of Cassandra Data Modeling</a>, which is a great place to get started. It’s the article I point newer Cassandra users to right away.</p>\n<p>Tyler is the primary author of Cassandra’s <a href=\"https://github.com/datastax/python-driver\">CQL Python driver</a>, so there is also much to learn from him via his public slides, such as:</p>\n<ul><li><a href=\"http://www.slideshare.net/tylerhobbs/intro-to-cassandra\">Intro to Cassandra</a> — focuses on the “what makes Cassandra special” parts without marketing hype</li>\n<li><a href=\"http://www.slideshare.net/tylerhobbs/cassandra-for-python-developers\">Cassandra for Python developers</a> — these slides are interesting because they predate CQL</li>\n<li><a href=\"https://speakerdeck.com/tylerhobbs/principles-of-cassandra-data-modeling\">Principles of Cassandra data modeling</a> — visual aid for the above “rules of data modeling” blog post</li>\n</ul><p>Avoid the traps, embrace the tricks. Good luck!</p>",
        "created_at": "2017-04-11T17:46:45+0000",
        "updated_at": "2018-10-05T11:09:33+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 19,
        "domain_name": "blog.parse.ly",
        "preview_picture": "https://i2.wp.com/blog.parsely.com/wp-content/uploads/2015/04/cassandra-e1430921268579.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/5276"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1233,
            "label": "data.modeling",
            "slug": "data-modeling"
          }
        ],
        "is_public": false,
        "id": 5272,
        "uid": null,
        "title": "Basic Rules of Cassandra Data Modeling",
        "url": "https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling",
        "content": "<p>Picking the right <a href=\"http://planetcassandra.org/getting-started-with-time-series-data-modeling/\">data model</a> is the hardest part of using Cassandra.  If you have a relational background, CQL will look familiar, but the way you use it can be very different.  The goal of this post is to explain the basic rules you should keep in mind when designing your schema for Cassandra.  If you follow these rules, you'll get pretty good performance out of the box.  Better yet, your performance should scale linearly as you add nodes to the cluster.</p>\n<h2>Non-Goals</h2>\n<p>Developers coming from a relational background usually carry over rules about relational modeling and try to apply them to Cassandra.  To avoid wasting time on rules that don't really matter with Cassandra, I want to point out some <em>non</em>-goals:</p>\n<h3>Minimize the Number of Writes</h3>\n<p>Writes in Cassandra aren't free, but they're awfully cheap.  Cassandra is optimized for high write throughput, and almost all writes are equally efficient <a href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling#footnote\"><sup>[1]</sup></a>.  If you can perform extra writes to improve the efficiency of your read queries, it's almost always a good tradeoff.  Reads tend to be more expensive and are much more difficult to tune.</p>\n<h3>Minimize Data Duplication</h3>\n<p>Denormalization and duplication of data is a fact of life with Cassandra.  Don't be afraid of it.  Disk space is generally the cheapest resource (compared to CPU, memory, disk IOPs, or network), and Cassandra is architected around that fact.  In order to get the most efficient reads, you often need to duplicate data.</p>\n<p>Besides, Cassandra doesn't have <code>JOIN</code>s, and you don't really want to use those in a distributed fashion.</p>\n<h2>Basic Goals</h2>\n<p>These are the two high-level goals for your data model:</p>\n<ol><li>Spread data evenly around the cluster</li>\n<li>Minimize the number of partitions read</li>\n</ol><p>There are other, lesser goals to keep in mind, but these are the most important. For the most part, I will focus on the basics of achieving these two goals.  There are other fancy tricks you can use, but you should know how to evaluate them, first.</p>\n<h3>Rule 1: Spread Data Evenly Around the Cluster</h3>\n<p>You want every node in the cluster to have roughly the same amount of data. Cassandra makes this easy, but it's not a given.  Rows are spread around the cluster based on a hash of the <a href=\"https://www.datastax.com/documentation/cassandra/2.1/cassandra/architecture/architectureDataDistributeHashing_c.html\"><em>partition key</em></a>, which is the first element of the <a href=\"https://www.datastax.com/documentation/cql/3.0/cql/ddl/ddl_compound_keys_c.html\"><code>PRIMARY KEY</code></a>.  So, the key to spreading data evenly is this: <b>pick a good primary key</b>.  I'll explain how to do this in a bit.</p>\n<h3>Rule 2: Minimize the Number of Partitions Read</h3>\n<p>Partitions are groups of rows that share the same partition key.  When you issue a read query, you want to read rows from as few partitions as possible.</p>\n<p>Why is this important?  Each partition may reside on a different node. The coordinator will generally need to issue separate commands to separate nodes for each partition you request.  This adds a lot of overhead and increases the variation in latency.  Furthermore, even on a single node, it's more expensive to read from multiple partitions than from a single one due to the way rows are stored.</p>\n<h3>Conflicting Rules?</h3>\n<p>If it's good to minimize the number of partitions that you read from, why not put everything in a single big partition?  You would end up violating Rule #1, which is to spread data evenly around the cluster.</p>\n<p>The point is, these two goals often conflict, so you'll need to try to balance them.</p>\n<h2>Model Around Your Queries</h2>\n<p>The way to minimize partition reads is to model your data to fit your queries. Don't model around relations.  Don't model around objects.  Model around your queries.  Here's how you do that:</p>\n<h3>Step 1: Determine What Queries to Support</h3>\n<p>Try to determine <em>exactly</em> what queries you need to support.  This can include a lot of considerations that you may not think of at first.  For example, you may need to think about:</p>\n<ul><li>Grouping by an attribute</li>\n<li>Ordering by an attribute</li>\n<li>Filtering based on some set of conditions</li>\n<li>Enforcing uniqueness in the result set</li>\n<li>etc ...</li>\n</ul><p>Changes to just one of these query requirements will frequently warrant a data model change for maximum efficiency.</p>\n<h3>Step 2: Try to create a table where you can satisfy your query by reading (roughly) one partition</h3>\n<p>In practice, this generally means you will use roughly one table per query pattern. If you need to support multiple query patterns, you usually need more than one table.</p>\n<p>To put this another way, each table should pre-build the \"answer\" to a high-level query that you need to support.  If you need different types of answers, you usually need different tables.  This is how you optimize for reads.</p>\n<p>Remember, data duplication is okay.  Many of your tables may repeat the same data.</p>\n<h2>Applying the Rules: Examples</h2>\n<p>To show some examples of a good throught process, I will walk you through the design of a data model for some simple problems.</p>\n<h3>Example 1: User Lookup</h3>\n<p>The high-level requirement is \"we have users and want to look them up\".  Let's go through the steps:</p>\n<p><b>Step 1</b>: <em>Determine what specific queries to support</em><br />Let's say we want to either be able to look up a user by their username or their email.  With either lookup method, we should get the full set of user details.</p>\n<p><b>Step 2</b>: <em>Try to create a table where you can satisfy your query by reading (roughly) one partition</em><br />Since we want to get the full details for the user with either lookup method, it's best to use two tables:</p>\n<pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nCREATE TABLE users_by_username (&#13;\n    username text PRIMARY KEY,&#13;\n    email text,&#13;\n    age int&#13;\n)&#13;\n&#13;\nCREATE TABLE users_by_email (&#13;\n    email text PRIMARY KEY,&#13;\n    username text,&#13;\n    age int&#13;\n)&#13;\n</pre>\n<p>Now, let's check the two rules for this model:</p>\n<p><b>Spreads data evenly?</b> Each user gets their own partition, so yes.<br /><b>Minimal partitions read?</b> We only have to read one partition, so yes.</p>\n<p>Now, let's suppose we tried to optimize for the <em>non</em>-goals, and came up with this data model instead:</p>\n<pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nCREATE TABLE users (&#13;\n    id uuid PRIMARY KEY,&#13;\n    username text,&#13;\n    email text,&#13;\n    age int&#13;\n)&#13;\n&#13;\nCREATE TABLE users_by_username (&#13;\n    username text PRIMARY KEY,&#13;\n    id uuid&#13;\n)&#13;\n&#13;\nCREATE TABLE users_by_email (&#13;\n    email text PRIMARY KEY,&#13;\n    id uuid&#13;\n)&#13;\n</pre>\n<p>This data model also spreads data evenly, but there's a downside: we now have to read two partitions, one from <code>users_by_username</code> (or <code>users_by_email</code>) and then one from <code>users</code>.  So reads are roughly twice as expensive.</p>\n<h3>Example 2: User Groups</h3>\n<p>Now the high-level requirement has changed.  Users are in groups, and we want to get all users in a group.</p>\n<p><b>Step 1</b>: <em>Determine what specific queries to support</em><br />We want to get the full user info for every user in a particular group.  Order of users does not matter.</p>\n<p><b>Step 2</b>: <em>Try to create a table where you can satisfy your query by reading (roughly) one partition</em><br />How do we fit a group into a partition?  We can use a compound <code>PRIMARY KEY</code> for this:</p>\n<pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nCREATE TABLE groups (&#13;\n    groupname text,&#13;\n    username text,&#13;\n    email text,&#13;\n    age int,&#13;\n    PRIMARY KEY (groupname, username)&#13;\n)&#13;\n</pre>\n<p>Note that the <code>PRIMARY KEY</code> has two components: <code>groupname</code>, which is the partitioning key, and <code>username</code>, which is called the clustering key.  This will give us one partition per <code>groupname</code>.  Within a particular partition (group), rows will be ordered by <code>username</code>.  Fetching a group is as simple as doing the following:</p>\n<pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nSELECT * FROM groups WHERE groupname = ?&#13;\n</pre>\n<p>This satisfies the goal of minimizing the number of partitions that are read, because we only need to read one partition.  However, it doesn't do so well with the first goal of evenly spreading data around the cluster.  If we have thousands or millions of small groups with hundreds of users each, we'll get a pretty even spread.  But if there's one group with millions of users in it, the entire burden will be shouldered by one node (or one set of replicas).</p>\n<p>If we want to spread the load more evenly, there are a few strategies we can use. The basic technique is to add another column to the PRIMARY KEY to form a compound partition key.  Here's one example:</p>\n<pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nCREATE TABLE groups (&#13;\n    groupname text,&#13;\n    username text,&#13;\n    email text,&#13;\n    age int,&#13;\n    hash_prefix int,&#13;\n    PRIMARY KEY ((groupname, hash_prefix), username)&#13;\n)&#13;\n</pre>\n<p>The new column, <code>hash_prefix</code>, holds a prefix of a hash of the username.  For example, it could be the first byte of the hash modulo four. Together with <code>groupname</code>, these two columns form the compound partition key.  Instead of a group residing on one partition, it's now spread across four partitions.  Our data is more evenly spread out, but we now have to read four times as many partitions.  This is an example of the two goals conflicting.  You need to find a good balance for your particular use case.  If you do a lot of reads and groups don't get too large, maybe changing the modulo value from four to two would be a good choice.  On the other hand, if you do very few reads, but any given group can grow very large, changing from four to ten would be a better choice.</p>\n<p>There are other ways to split up a partition, which I will cover in the next example.</p>\n<p>Before we move on, let me point out something else about this data model: we're duplicating user info potentially many times, once for each group. You might be tempted to try a data model like this to reduce duplication:</p>\n<pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nCREATE TABLE users (&#13;\n    id uuid PRIMARY KEY,&#13;\n    username text,&#13;\n    email text,&#13;\n    age int&#13;\n)&#13;\n&#13;\nCREATE TABLE groups (&#13;\n    groupname text,&#13;\n    user_id uuid,&#13;\n    PRIMARY KEY (groupname, user_id)&#13;\n)&#13;\n</pre>\n<p>Obviously, this minimizes duplication.  But how many partitions do we need to read? If a group has 1000 users, we need to read 1001 partitions.  This is probably 100x more expensive to read than our first data model.  If reads need to be efficient at all, this isn't a good model.  On the other hand, if reads are extremely infrequent, but updates to user info (say, the username) are extremely common, this data model might actually make sense.  Make sure to take your read/update ratio into account when designing your schema.</p>\n<h3>Example 3: User Groups by Join Date</h3>\n<p>Suppose we continue with the previous example of groups, but need to add support for getting the X newest users in a group.</p>\n<p>We can use a similar table to the last one:</p>\n<pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nCREATE TABLE group_join_dates (&#13;\n    groupname text,&#13;\n    joined timeuuid,&#13;\n    username text,&#13;\n    email text,&#13;\n    age int,&#13;\n    PRIMARY KEY (groupname, joined)&#13;\n)&#13;\n</pre>\n<p>Here we're using a <code>timeuuid</code> (which is like a timestamp, but avoids collisions) as the clustering column.  Within a group (partition), rows will be ordered by the time the user joined the group.  This allows us to get the newest users in a group like so:</p>\n<pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nSELECT * FROM group_join_dates&#13;\n    WHERE groupname = ?&#13;\n    ORDER BY joined DESC&#13;\n    LIMIT ?&#13;\n</pre>\n<p>This is reasonably efficient, as we're reading a slice of rows from a single partition.  However, instead of always using <code>ORDER BY joined DESC</code>, which makes the query less efficient, we can simply reverse the clustering order:</p>\n<pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nCREATE TABLE group_join_dates (&#13;\n    groupname text,&#13;\n    joined timeuuid,&#13;\n    username text,&#13;\n    email text,&#13;\n    age int,&#13;\n    PRIMARY KEY (groupname, joined)&#13;\n) WITH CLUSTERING ORDER BY (joined DESC)&#13;\n</pre>\n<p>Now we can use the slightly more efficient query:</p>\n<pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nSELECT * FROM group_join_dates&#13;\n    WHERE groupname = ?&#13;\n    LIMIT ?&#13;\n</pre>\n<p>As with the previous example, we could have problems with data being spread evenly around the cluster if any groups get too large.  In that example, we split partitions somewhat randomly, but in this case, we can utilize our knowledge about the query patterns to split partitions a different way: by a time range.</p>\n<p>For example, we might split partitions by date:</p>\n<pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nCREATE TABLE group_join_dates (&#13;\n    groupname text,&#13;\n    joined timeuuid,&#13;\n    join_date text,&#13;\n    username text,&#13;\n    email text,&#13;\n    age int,&#13;\n    PRIMARY KEY ((groupname, join_date), joined)&#13;\n) WITH CLUSTERING ORDER BY (joined DESC)&#13;\n</pre>\n<p>We're using a compound partition key again, but this time we're using the join date.  Each day, a new partition will start.  When querying the X newest users, we will first query today's partition, then yesterday's, and so on, until we have X users.  We may have to read multiple partitions before the limit is met.</p>\n<p>To minimize the number of partitions you need to query, try to select a time range for splitting partitions that will typically let you query only one or two partitions.  For example, if we usually need the ten newest users, and groups usually acquire three users per day, we should split by four-day ranges instead of a single day <a href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling#footnote\"><sup>[2]</sup></a>.</p>\n<h2>Summary</h2>\n<p>The basic rules of data modeling covered here apply to all (currently) existing versions of Cassandra, and are very likely to apply to all future versions.  Other lesser data modeling problems, such as <a href=\"https://medium.com/@foundev/domain-modeling-around-deletes-1cc9b6da0d24\">dealing with tombstones</a>, may also need to be considered, but these problems are more likely to change (or be mitigated) by future versions of Cassandra.</p>\n<p>Besides the basic strategies covered here, some of Cassandra's fancier features, like <a href=\"https://www.datastax.com/dev/blog/cql3_collections\">collections</a>, <a href=\"https://www.datastax.com/dev/blog/cql-in-2-1\">user-defined types</a>, and <a href=\"https://www.datastax.com/documentation/cql/3.1/cql/cql_reference/refStaticCol.html\">static columns</a>, can also be used to reduce the number of partitions that you need to read to satisfy a query.  Don't forget to consider these options when designing your schema.</p>\n<p>Hopefully I've given you some useful fundamental tools for evaluating different schema designs.  If you want to go further, I suggest taking <a href=\"https://academy.datastax.com/courses/ds220-data-modeling?dxt=blogposting\">Datastax's free, self-paced online data modeling course (DS220)</a>.  Good luck!</p>\n<p>[1]: Notable exceptions: <a href=\"https://www.datastax.com/dev/blog/whats-new-in-cassandra-2-1-a-better-implementation-of-counters\">counters</a>, <a href=\"https://www.datastax.com/dev/blog/lightweight-transactions-in-cassandra-2-0\">lightweight transactions</a>, and <a href=\"http://cassandra.apache.org/doc/cql3/CQL.html#collections\">inserting into the middle of a list collection</a>.</p>\n<p>[2]: I suggest using a timestamp truncated by some number of seconds.  For example, to handle four-day ranges, you might use something like this:</p>\n<pre class=\"brush: python; gutter: true; title: ; notranslate\" title=\"\">&#13;\nnow = time()&#13;\nfour_days = 4 * 24 * 60 * 60&#13;\nshard_id = now - (now % four_days)&#13;\n</pre>\n\n<hr /><p><a href=\"https://www.datastax.com/\">DataStax</a> has many ways for you to advance in your career and knowledge. \n</p><p>You can take <a href=\"https://academy.datastax.com/user/register?destination=home&amp;utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_register\" target=\"_self\" title=\"academy.datastax.com\">free classes</a>, <a href=\"https://academy.datastax.com/certifications?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_certifications\" target=\"_self\" title=\"academy.datastax.com/certifications\">get certified</a>, or read <a href=\"https://www.datastax.com/dbas-guide-to-nosql\" target=\"_self\" title=\"dbas-guide-to-nosql\">one of our many white papers</a>.\n</p><p><a href=\"https://academy.datastax.com/user/register?destination=home&amp;utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_register\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"academy.datastax.com\">register for classes</a>\n</p><p><a href=\"https://academy.datastax.com/certifications?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_certifications\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"academy.datastax.com/certifications\">get certified</a>\n</p><p><a href=\"http://www.datastax.com/dbas-guide-to-nosql?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_dbasguidetonosql\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"dbas-guide-to-nosql\">DBA's Guide to NoSQL</a>\n</p>\t\t\t<br class=\"clear\" /><div id=\"mto_newsletter_121316_Css\"><p>Subscribe for newsletter:</p><br /></div>",
        "created_at": "2017-04-21T17:24:37+0000",
        "updated_at": "2018-10-05T11:00:07+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 12,
        "domain_name": "www.datastax.com",
        "preview_picture": "https://www.datastax.com/wp-content/themes/datastax-2014-08/images/common/DataStax_Web_Social_DefaultGenericV2_1024x351_wide.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/5272"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 15,
            "label": "tutorial",
            "slug": "tutorial"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 254,
            "label": "nosql",
            "slug": "nosql"
          }
        ],
        "is_public": false,
        "id": 5174,
        "uid": null,
        "title": "Migration Best Practices: From RDBMS to Cassandra without a Hitch",
        "url": "https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch",
        "content": "<head prefix=\"og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# 2490221586: http://ogp.me/ns/fb/2490221586#\"><meta charset=\"utf-8\"/><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, maximum-scale=1, user-scalable=no\"/><meta name=\"include_mode\" content=\"async\"/><!-- SL:start:notranslate --><title>Migration Best Practices: From RDBMS to Cassandra without a Hitch</title><meta name=\"description\" content=\"Presenter: Duy Hai Doan, Technical Advocate at Datastax Libon is a messaging service designed to improve mobile communications through free calls, chat and a v…\"/><!-- SL:end:notranslate --><meta name=\"robots\" content=\"index\"/><meta id=\"globalTrackingUrl\" content=\"https://www.linkedin.com/li/track\"/><!-- SL:start:notranslate --><meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"/><meta http-equiv=\"x-dns-prefetch-control\" content=\"on\"/><meta name=\"thumbnail\" content=\"https://cdn.slidesharecdn.com/ss_thumbnails/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01-thumbnail.jpg?cb=1422447584\"/><!-- SL:end:notranslate --><meta content=\"{&quot;lynda.domain&quot;:&quot;lynda&quot;,&quot;lynda.premium_video&quot;:&quot;PREMIUM VIDEO&quot;,&quot;lynda.tld&quot;:&quot;.com&quot;,&quot;right_rail.recommended&quot;:&quot;Recommended&quot;,&quot;views.other&quot;:&quot;views&quot;,&quot;share&quot;:&quot;Share&quot;,&quot;select&quot;:&quot;Select&quot;,&quot;selected&quot;:&quot;Selected&quot;,&quot;clipping.select_clipboard_modal.select_another_clipboard&quot;:&quot;Select another clipboard&quot;,&quot;clipping.select_clipboard_modal.select_new_clipboard&quot;:&quot;Select a new clipboard&quot;,&quot;clipping.toast.change_clipboard&quot;:&quot;Change clipboard&quot;,&quot;clipping.toast.share_clip&quot;:&quot;Share clip&quot;,&quot;li_connect.slideshare_added&quot;:&quot;Your SlideShare was successfully added to your LinkedIn profile.&quot;,&quot;li_connect.login.no_match&quot;:&quot;Login does not match, please try again.&quot;,&quot;ajax_signup.login.download&quot;:&quot;Login to SlideShare to download\\u2026&quot;,&quot;ajax_signup.login.addcontact&quot;:&quot;Login to SlideShare to follow\\u2026&quot;,&quot;ajax_signup.login.favorite&quot;:&quot;Login to SlideShare to like\\u2026&quot;,&quot;ajax_signup.login.comments&quot;:&quot;Login to SlideShare to post a comment\\u2026&quot;,&quot;ajax_signup.login.AddToCommunity&quot;:&quot;Login to SlideShare to add this document to a group/event\\u2026&quot;,&quot;ajax_signup.login.follow&quot;:&quot;Login to SlideShare to follow this user\\u2026&quot;,&quot;ajax_signup.login.business&quot;:&quot;Login to SlideShare to continue\\u2026&quot;,&quot;ajax_signup.login.upload&quot;:&quot;Login to SlideShare to start uploading\\u2026&quot;,&quot;ajax_signup.login.contest&quot;:&quot;Login to SlideShare to vote\\u2026&quot;,&quot;ajax_signup.login.rsvp&quot;:&quot;Login to SlideShare to join this meeting\\u2026&quot;,&quot;ajax_signup.login.user&quot;:&quot;Login to SlideShare to register the username\\u2026&quot;,&quot;ajax_signup.login.zipcast&quot;:&quot;Login to SlideShare to schedule a meeting\\u2026&quot;,&quot;ajax_signup.login.create&quot;:&quot;Login to SlideShare to start creating\\u2026&quot;,&quot;ajax_signup.login.clip&quot;:&quot;Login to SlideShare. Don\\u2019t lose your clips!&quot;,&quot;ajax_signup.signup.download&quot;:&quot;Signup for SlideShare to download\\u2026&quot;,&quot;ajax_signup.signup.addcontact&quot;:&quot;Signup for SlideShare to follow\\u2026&quot;,&quot;ajax_signup.signup.favorite&quot;:&quot;Signup for SlideShare to like\\u2026&quot;,&quot;ajax_signup.signup.comments&quot;:&quot;Signup for SlideShare to post a comment\\u2026&quot;,&quot;ajax_signup.signup.AddToCommunity&quot;:&quot;Signup for SlideShare to add this document to a group/event\\u2026&quot;,&quot;ajax_signup.signup.follow&quot;:&quot;Signup for SlideShare to follow this user\\u2026&quot;,&quot;ajax_signup.signup.business&quot;:&quot;Signup for SlideShare to continue\\u2026&quot;,&quot;ajax_signup.signup.upload&quot;:&quot;Signup for SlideShare to start uploading\\u2026&quot;,&quot;ajax_signup.signup.contest&quot;:&quot;Signup for SlideShare to vote\\u2026&quot;,&quot;ajax_signup.signup.rsvp&quot;:&quot;Signup for SlideShare to join this meeting\\u2026&quot;,&quot;ajax_signup.signup.user&quot;:&quot;Signup for SlideShare to register the username\\u2026&quot;,&quot;ajax_signup.signup.zipcast&quot;:&quot;Signup for SlideShare to schedule a meeting\\u2026&quot;,&quot;ajax_signup.signup.create&quot;:&quot;Signup for SlideShare to start creating\\u2026&quot;,&quot;ajax_signup.signup.clip&quot;:&quot;Signup for SlideShare. Don\\u2019t lose your clips!&quot;,&quot;ajax_signup.connect&quot;:&quot;connect&quot;,&quot;ajax_signup.signup_for_ss&quot;:&quot;Signup for SlideShare&quot;,&quot;ajax_signup.login_to_ss&quot;:&quot;Login to SlideShare&quot;,&quot;clipping.filmstrip.message&quot;:&quot;Top clipped slide&quot;,&quot;slideview.comments_loggedin.delete_comment.title&quot;:&quot;Are you sure you want to delete this comment?&quot;,&quot;slideview.comments_loggedin.spam_comment.title&quot;:&quot;Are you sure you want to mark this comment as spam?&quot;,&quot;slideview.comments_loggedin.block_user.title&quot;:&quot;Are you sure you want to block this user?&quot;,&quot;slideview.comments_loggedin.block_user.success&quot;:&quot;The user is blocked!&quot;,&quot;slideview.comments_loggedin.block_user.failure&quot;:&quot;Oops! Something went wrong. Please try again after sometime.&quot;,&quot;slideview.comments_loggedin.organization&quot;:&quot;at {:organization}&quot;,&quot;slideview.comments_loggedin.timestamp&quot;:&quot;less than a second ago&quot;,&quot;slideview.comments_loggedin.commenting.failure&quot;:&quot;Hey something went wrong :-(\\nCould you try again?&quot;,&quot;slideshow.foundation_slideview.likes_count&quot;:&quot;{:count,number} {:count,choice,singular#Like|plural#Likes}&quot;,&quot;action-bar.like.why_not_share&quot;:&quot;Like this? Why not share?&quot;,&quot;slideshow.foundation_slideview.like&quot;:&quot;Like&quot;,&quot;action-bar.unlike.text&quot;:&quot;Unlike&quot;}\" name=\"ss-i18n-translations\"/><meta content=\"{&quot;ssplayer.event_manager.playershare.share_slideshare&quot;:&quot;Share SlideShare&quot;,&quot;profile.edit_marketing_settings.saved&quot;:&quot;Saved&quot;,&quot;slideshow.foundation_slideview.loading&quot;:&quot;Loading\\u2026&quot;,&quot;clip.slideview.share_message&quot;:&quot;Share Slide {0,number} from {1,text}&quot;,&quot;clip.slideview.success_toast.message&quot;:&quot;Slide {0,number} clipped to: \\u003Cbr /\\u003E {1,anchor,text#{2,text}}&quot;,&quot;clip.clips.error_clipping&quot;:&quot;Error while clipping.&quot;,&quot;clip.clips.error_fetching_clipboards&quot;:&quot;Error fetching clipboards.&quot;,&quot;clip.clips.error_creating_clipboard&quot;:&quot;Error creating a clipboard.&quot;,&quot;clip.clips.error_adding_to_clipboard&quot;:&quot;Error adding to clipboard.&quot;}\" name=\"ss-i18n-translations\"/><!--[if IE 9]><link href=\"https://public.slidesharecdn.com/ss_foundation/stylesheets/ie9_nav_bar_fix.css?8fb8af5274\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\" /><![endif]--><meta content=\"index\" name=\"robots\"/><meta content=\"https://public.slidesharecdn.com/images/artdeco/icons.svg?43e81fd2ef\" name=\"ss-svg-icons\"/><meta name=\"apple-itunes-app\" content=\"app-id=917418728, affiliate-data=ct=smart_banner&amp;pt=10746, app-argument=slideshare-app://ss/44005594\"/><!-- fb open graph meta tags --><!-- SL:start:notranslate --><!-- SL:end:notranslate --><!-- SL:start:notranslate --><meta name=\"twitter:card\" value=\"player\"/><meta name=\"twitter:site\" value=\"@slideshare\"/><meta name=\"twitter:player:width\" value=\"342\"/><meta name=\"twitter:player:height\" value=\"291\"/><meta name=\"twitter:app:name:googleplay\" content=\"SlideShare Android\"/><meta name=\"twitter:app:id:googleplay\" content=\"net.slideshare.mobile\"/><meta name=\"twitter:app:url:googleplay\" content=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\"/><meta name=\"twitter:app:name:iphone\" content=\"SlideShare iOS\"/><meta name=\"twitter:app:id:iphone\" content=\"917418728\"/><meta name=\"twitter:app:url:iphone\" content=\"slideshare-app://ss/44005594\"/><meta name=\"twitter:app:name:ipad\" content=\"SlideShare iOS\"/><meta name=\"twitter:app:id:ipad\" content=\"917418728\"/><meta name=\"twitter:app:url:ipad\" content=\"slideshare-app://ss/44005594\"/><meta property=\"al:android:url\" content=\"slideshare-app://ss/44005594\"/><meta property=\"al:android:app_name\" content=\"SlideShare Android\"/><meta property=\"al:android:package\" content=\"net.slideshare.mobile\"/><meta property=\"al:ios:url\" content=\"slideshare-app://ss/44005594\"/><meta property=\"al:ios:app_store_id\" content=\"917418728\"/><meta property=\"al:ios:app_name\" content=\"SlideShare iOS\"/><!-- SL:end:notranslate --></head><body id=\"pagekey-slideshare_desktop_slideview_loggedout\" class=\"readabilityBody\" readability=\"51\">\n      <!-- TOS update banner -->\n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\" readability=\"0\">\n        <p>\n          <a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            <label id=\"home\">SlideShare</label>\n          </a>\n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\"/>\n            <label id=\"explore\">Explore</label>\n          </a>\n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\"/>\n              <label id=\"you\">You</label>\n            </a>\n        </p>\n        \n      </div>\n    <div class=\"wrapper\">\n        \n        \n      \n      \n<div id=\"slideview-container\" class=\"\">\n  \n  <div class=\"row\">\n    <div id=\"main-panel\" class=\"small-12 large-8 columns\">\n      <div class=\"sectionElements\">\n          \n        <div class=\"playerWrapper\">\n            <div readability=\"5\">\n              <!-- For slideview page , combined js for player is now combined with slideview javascripts for logged out users-->\n<div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\" readability=\"6\">\n  \n    Migration Best Practices: From RDBMS to Cassandra without a Hitch\n  \n  <div class=\"stage valign-first-slide\">\n    \n    \n    <div class=\"slide_container\">\n            <section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-1-638.jpg?cb=1422447584\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-1-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-1-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-1-1024.jpg?cb=1422447584\" alt=\"Migration Best Practices: From RDBMS to Cassandra without a Hitch&#10;#Cassandra @doanduyhai&#10; \"/></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-2-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-2-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-2-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Who am I ?&#10;2&#10;DuyHai Doan&#10;Achilles&#10;Cassandra Technical Advocate @ Datastax&#10;Former Java Developer @ L...\"/></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-3-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-3-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-3-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Agenda&#10;•  Libon context&#10;•  Migration strategy&#10;•  Business code migration&#10;•  Data Modeling&#10;•  Take A...\"/></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-4-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-4-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-4-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Libon Context&#10; \"/></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-5-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-5-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-5-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;What is Libon ?&#10;•  Messaging app&#10;•  VOIP (out)&#10;•  Custom voicemail &amp; greetings&#10;•  SMS/chat/ﬁle tran...\"/></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-6-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-6-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-6-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Contact Matching&#10;6&#10;Libon User&#10; \"/></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-7-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-7-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-7-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Contact Matching&#10;7&#10;Libon User Friend&#10; \"/></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-8-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-8-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-8-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Contact Matching&#10;8&#10;Libon User Friend&#10;Contact matching&#10; \"/></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-9-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-9-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-9-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Contact Matching&#10;9&#10;Libon User Friend&#10;Accept link&#10; \"/></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-10-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-10-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-10-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Project Context&#10;•  Application grew over the years&#10;10&#10; \"/></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-11-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-11-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-11-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Project Context&#10;•  Application grew over the years&#10;•  Already using Cassandra to handle events&#10;•  m...\"/></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-12-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-12-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-12-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Project Context&#10;•  About contacts …&#10;12&#10; \"/></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-13-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-13-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-13-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Project Context&#10;•  About contacts …&#10;•  stored as relational model in RDBMS (Oracle)&#10;13&#10; \"/></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-14-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-14-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-14-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Project Context&#10;•  About contacts …&#10;•  stored as relational model in RDBMS (Oracle)&#10;•  1 user ≈ 300...\"/></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-15-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-15-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-15-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Project Context&#10;•  About contacts …&#10;•  stored as relational model in RDBMS (Oracle)&#10;•  1 user ≈ 300...\"/></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-16-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-16-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-16-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Project Context&#10;•  About contacts …&#10;•  stored as relational model in RDBMS (Oracle)&#10;•  1 user ≈ 300...\"/></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-17-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-17-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-17-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai17&#10; \"/></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-18-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-18-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-18-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Fixing the problem&#10;•  Tune the RDBMS&#10;18&#10; \"/></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-19-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-19-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-19-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Fixing the problem&#10;•  Tune the RDBMS&#10;•  indices&#10;19&#10; \"/></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-20-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-20-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-20-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Fixing the problem&#10;•  Tune the RDBMS&#10;•  indices&#10;•  partitioning&#10;20&#10; \"/></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-21-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-21-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-21-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Fixing the problem&#10;•  Tune the RDBMS&#10;•  indices&#10;•  partitioning&#10;•  less joins, simpliﬁed relational...\"/></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-22-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-22-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-22-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Fixing the problem&#10;•  Tune the RDBMS&#10;•  indices&#10;•  partitioning&#10;•  less joins, simpliﬁed relational...\"/></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-23-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-23-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-23-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Fixing the problem&#10;•  Tune the RDBMS&#10;•  indices&#10;•  partitioning&#10;•  less joins, simpliﬁed relational...\"/></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-24-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-24-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-24-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Fixing the problem&#10;•  Tune the RDBMS&#10;•  indices&#10;•  partitioning&#10;•  less joins, simpliﬁed relational...\"/></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-25-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-25-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-25-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Back-end application&#10;RDBMS Cassandra&#10;25&#10; \"/></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-26-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-26-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-26-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Back-end application&#10;RDBMS Cassandra&#10;26&#10;We need to&#10;choose&#10; \"/></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-27-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-27-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-27-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Next Challenges&#10;•  High Availability (DB failure, site failure …)&#10;27&#10; \"/></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-28-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-28-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-28-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Next Challenges&#10;•  High Availability (DB failure, site failure …)&#10;•  Predictable performance at sca...\"/></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-29-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-29-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-29-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Next Challenges&#10;•  High Availability (DB failure, site failure …)&#10;•  Predictable performance at sca...\"/></section><section data-index=\"30\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-30-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-30-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-30-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Next Challenges&#10;•  High Availability (DB failure, site failure …)&#10;•  Predictable performance at sca...\"/></section><section data-index=\"31\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-31-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-31-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-31-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Data Migration Strategy&#10; \"/></section><section data-index=\"32\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-32-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-32-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-32-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Objectives&#10;•  No downtime&#10;32&#10; \"/></section><section data-index=\"33\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-33-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-33-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-33-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Objectives&#10;•  No downtime&#10;•  No concurrency corner-cases &#10;33&#10; \"/></section><section data-index=\"34\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-34-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-34-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-34-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Objectives&#10;•  No downtime&#10;•  No concurrency corner-cases &#10;•  Safe rollback possible&#10;34&#10; \"/></section><section data-index=\"35\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-35-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-35-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-35-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Objectives&#10;•  No downtime&#10;•  No concurrency corner-cases &#10;•  Safe rollback possible&#10;•  Replay-abili...\"/></section><section data-index=\"36\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-36-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-36-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-36-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Strategy&#10;•  4 phases&#10;36&#10; \"/></section><section data-index=\"37\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-37-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-37-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-37-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Strategy&#10;•  4 phases&#10;•  Write contacts to both data stores&#10;37&#10; \"/></section><section data-index=\"38\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-38-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-38-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-38-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Strategy&#10;•  4 phases&#10;•  Write contacts to both data stores&#10;•  Old contacts migration&#10;38&#10; \"/></section><section data-index=\"39\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-39-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-39-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-39-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Strategy&#10;•  4 phases&#10;•  Write contacts to both data stores&#10;•  Old contacts migration&#10;•  Switch to C...\"/></section><section data-index=\"40\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-40-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-40-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-40-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Strategy&#10;•  4 phases&#10;•  Write contacts to both data stores&#10;•  Old contacts migration&#10;•  Switch to C...\"/></section><section data-index=\"41\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-41-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-41-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-41-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Migration Phase 1&#10;41&#10;Back end server&#10;·&#10;·&#10;·&#10;SQLSQLSQL&#10;C*&#10;C*&#10;C*&#10;C*&#10;C*&#10;Write&#10;contactUUID&#10;contactId … c...\"/></section><section data-index=\"42\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-42-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-42-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-42-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Migration Phase 1&#10;42&#10;Back end server&#10;·&#10;·&#10;·&#10;SQLSQLSQL&#10;C*&#10;C*&#10;C*&#10;C*&#10;C*&#10;Read&#10; \"/></section><section data-index=\"43\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-43-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-43-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-43-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Migration Phase 2&#10;•  On live production, migrate old contacts&#10;43&#10;SQLSQLSQL&#10;C*&#10;C*&#10;C*&#10;C*&#10;C*&#10;For each ...\"/></section><section data-index=\"44\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-44-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-44-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-44-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Migration Phase 2&#10;•  On live production, migrate old contacts&#10;44&#10;SQLSQLSQL&#10;C*&#10;C*&#10;C*&#10;C*&#10;C*&#10;For each ...\"/></section><section data-index=\"45\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-45-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-45-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-45-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Migration Phase 2&#10;45&#10;USING TIMESTAMP now() - 1 week ????&#10; \"/></section><section data-index=\"46\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-46-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-46-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-46-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Migration Phase 2&#10;•  During data migration …&#10;46&#10; \"/></section><section data-index=\"47\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-47-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-47-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-47-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Migration Phase 2&#10;•  During data migration …&#10;•  … concurrent writes from the migration batch …&#10;47&#10; \"/></section><section data-index=\"48\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-48-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-48-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-48-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Migration Phase 2&#10;•  During data migration …&#10;•  … concurrent writes from the migration batch …&#10;•  …...\"/></section><section data-index=\"49\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-49-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-49-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-49-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Migration Phase 2&#10;49&#10;contact_uuid&#10;name (now -1 week)&#10; …&#10; name (now)&#10; …&#10;Johny …&#10; Johnny …&#10;Insert fro...\"/></section><section data-index=\"50\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-50-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-50-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-50-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Migration Phase 2&#10;50&#10;contact_uuid&#10;name (now -1 week)&#10; …&#10; name (now)&#10; …&#10;Johny …&#10; Johnny …&#10;Future rea...\"/></section><section data-index=\"51\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-51-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-51-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-51-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Last Write Win in action&#10;51&#10;Case 1 Case 2&#10;Batchpast(Johny)&#10; t1&#10;Prodnow(Johnny)&#10; t2&#10;t3&#10; Read(Johnny)...\"/></section><section data-index=\"52\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-52-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-52-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-52-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Migration Phase 2&#10;52&#10;&quot;Write to the Past…&#10;to save the Future&quot;&#10;Libon – 2014/10/08&#10; \"/></section><section data-index=\"53\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-53-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-53-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-53-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Migration Phase 3&#10;53&#10;Back end server&#10;·&#10;·&#10;·&#10;SQLSQLSQL&#10;C*&#10;C*&#10;C*&#10;C*&#10;C*&#10;Write&#10; \"/></section><section data-index=\"54\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-54-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-54-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-54-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Migration Phase 4&#10;54&#10;Back end server&#10;·&#10;·&#10;·&#10;SQLSQLSQL&#10;C*&#10;C*&#10;C*&#10;C*&#10;C*&#10;Write&#10;❌&#10; \"/></section><section data-index=\"55\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-55-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-55-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-55-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Business Code Refactoring&#10; \"/></section><section data-index=\"56\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-56-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-56-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-56-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Code Inventory&#10;•  Written for RDBMS&#10;56&#10; \"/></section><section data-index=\"57\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-57-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-57-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-57-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Code Inventory&#10;•  Written for RDBMS&#10;•  Lots of joins (no surprise) &#10;57&#10; \"/></section><section data-index=\"58\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-58-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-58-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-58-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Code Inventory&#10;•  Written for RDBMS&#10;•  Lots of joins (no surprise) &#10;•  Designed around transactions...\"/></section><section data-index=\"59\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-59-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-59-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-59-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Code Inventory&#10;•  Written for RDBMS&#10;•  Lots of joins (no surprise) &#10;•  Designed around transactions...\"/></section><section data-index=\"60\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-60-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-60-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-60-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Code Inventory cont.&#10;•  Entities go through Services &amp; Repositories&#10;60&#10;Repositories&#10;&#10;Services&#10;Conta...\"/></section><section data-index=\"61\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-61-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-61-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-61-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Code Inventory cont.&#10;•  Hibernate is auto-magic&#10;61&#10; \"/></section><section data-index=\"62\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-62-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-62-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-62-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Code Inventory cont.&#10;•  Hibernate is auto-magic&#10;•  lazy loading&#10;•  1st level cache&#10;•  N+1 select&#10;62...\"/></section><section data-index=\"63\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-63-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-63-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-63-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Which options ?&#10;•  Throw existing code …&#10;•  … and re-design from scratch for Cassandra&#10;&#10;63&#10; \"/></section><section data-index=\"64\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-64-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-64-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-64-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Which options ?&#10;•  Throw existing code …&#10;•  … and re-design from scratch for Cassandra&#10;&#10;64&#10;No way !&#10; \"/></section><section data-index=\"65\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-65-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-65-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-65-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Code Quality&#10;•  Existing business code has…&#10;•  … ≈ 3500 unit tests&#10;&#10;65&#10; \"/></section><section data-index=\"66\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-66-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-66-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-66-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Code Quality&#10;•  Existing business code has…&#10;•  … ≈ 3500 unit tests&#10;•  and ≈600+ integration tests&#10;66&#10; \"/></section><section data-index=\"67\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-67-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-67-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-67-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Code Quality&#10;67&#10;&quot;The code coverage&#10;is one of your most&#10;valuable technical asset&quot;&#10;Libon – since begi...\"/></section><section data-index=\"68\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-68-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-68-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-68-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Repositories&#10;Services&#10;Refactoring Strategy&#10;68&#10;ContactMatchingService&#10;ContactService&#10;ContactSync&#10;Con...\"/></section><section data-index=\"69\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-69-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-69-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-69-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Repositories&#10;Services&#10;Refactoring Strategy&#10;69&#10;ContactMatchingService&#10;ContactService&#10;ContactNoSQLEnt...\"/></section><section data-index=\"70\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-70-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-70-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-70-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Repositories&#10;Services&#10;Refactoring Strategy&#10;70&#10;ContactMatchingService&#10;ContactService&#10;ContactNoSQLEnt...\"/></section><section data-index=\"71\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-71-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-71-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-71-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Refactoring Strategy&#10;•  Use CQRS&#10;•  ContactReadRepository&#10;•  ContactWriteRepository&#10;•  ContactUpdat...\"/></section><section data-index=\"72\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-72-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-72-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-72-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Refactoring Strategy&#10;•  ContactReadRepository&#10;•  direct sequential read&#10;•  no joins&#10;•  1 read ≈ 1 S...\"/></section><section data-index=\"73\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-73-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-73-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-73-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Refactoring Strategy&#10;•  ContactWriteRepository&#10;•  write to all denormalized tables&#10;•  using CQL log...\"/></section><section data-index=\"74\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-74-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-74-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-74-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Refactoring Strategy&#10;•  ContactUpdateRepository&#10;•  read-before-write most of the time ????&#10;•  rare upd...\"/></section><section data-index=\"75\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-75-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-75-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-75-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Refactoring Strategy&#10;•  ContactDeleteRepository&#10;•  delete by partition key&#10;75&#10; \"/></section><section data-index=\"76\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-76-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-76-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-76-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Outcome&#10;•  5 months of 2 men work&#10;76&#10; \"/></section><section data-index=\"77\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-77-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-77-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-77-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Outcome&#10;•  5 months of 2 men work&#10;•  Many iterations to ﬁx bugs (thanks to IT)&#10;77&#10; \"/></section><section data-index=\"78\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-78-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-78-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-78-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Outcome&#10;•  5 months of 2 men work&#10;•  Many iterations to ﬁx bugs (thanks to IT)&#10;•  Lots of performan...\"/></section><section data-index=\"79\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-79-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-79-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-79-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Gatling Output&#10;79&#10; \"/></section><section data-index=\"80\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-80-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-80-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-80-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Outcome&#10;•  5 months of 2 men work&#10;•  Many iterations to ﬁx bugs (thanks to IT)&#10;•  Lots of performan...\"/></section><section data-index=\"81\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-81-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-81-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-81-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Outcome&#10;•  5 months of 2 men work&#10;•  Many iterations to ﬁx bugs (thanks to IT)&#10;•  Lots of performan...\"/></section><section data-index=\"82\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-82-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-82-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-82-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Data Model&#10; \"/></section><section data-index=\"83\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-83-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-83-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-83-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Denormalization, the good&#10;•  Support fast reads&#10;•  1 read ≈ 1 SELECT&#10;•  Worthy because mostly read,...\"/></section><section data-index=\"84\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-84-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-84-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-84-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Denormalization, the bad&#10;•  Updating mutable data can be nightmare&#10;•  Data model bound by existing ...\"/></section><section data-index=\"85\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-85-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-85-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-85-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Data model in detail&#10;85&#10;Contacts_by_id&#10;Contacts_by_identiﬁers&#10;Contacts_in_proﬁles&#10;Contacts_by_modiﬁ...\"/></section><section data-index=\"86\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-86-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-86-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-86-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Data model in detail&#10;86&#10;Contacts_by_id&#10;Contacts_by_identiﬁers&#10;Contacts_in_proﬁles&#10;Contacts_by_modiﬁ...\"/></section><section data-index=\"87\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-87-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-87-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-87-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Scalable design&#10;87&#10;n1&#10;n2&#10;n3&#10;n4&#10;n5&#10;n6&#10;n7&#10;n8&#10;A&#10;B&#10;C&#10;D&#10;E&#10;F&#10;G&#10;H&#10;user_id1&#10;user_id2&#10;user_id3&#10;user_id4&#10;user...\"/></section><section data-index=\"88\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-88-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-88-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-88-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Scalable design&#10;88&#10;n1&#10;n2&#10;n3&#10;n4&#10;n5&#10;n6&#10;n7&#10;n8&#10;A&#10;B&#10;C&#10;D&#10;E&#10;F&#10;G&#10;H&#10;user_id1user_id2&#10;user_id3&#10;user_id4&#10;user_...\"/></section><section data-index=\"89\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-89-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-89-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-89-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Bloom filters in action&#10;•  For some tables, partition key = (user_id, contact_id)&#10;☞ fast look-up, l...\"/></section><section data-index=\"90\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-90-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-90-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-90-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Data model in detail&#10;90&#10;Contacts_by_id&#10;Contacts_by_identiﬁers&#10;Contacts_in_proﬁles&#10;Contacts_by_modiﬁ...\"/></section><section data-index=\"91\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-91-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-91-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-91-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;A &quot;queue&quot; story&#10;•  contacts_by_modiﬁcation_date&#10;•  queue-like pattern ????&#10;91&#10; \"/></section><section data-index=\"92\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-92-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-92-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-92-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;A &quot;queue&quot; story&#10;•  contacts_by_modiﬁcation_date&#10;•  queue-like pattern ????&#10;☞ buckets to the rescue&#10;92&#10;...\"/></section><section data-index=\"93\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-93-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-93-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-93-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Data model summary&#10;•  7 tables for denormalization&#10;93&#10; \"/></section><section data-index=\"94\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-94-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-94-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-94-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Data model summary&#10;•  7 tables for denormalization&#10;•  Normalize some tables because rare access&#10;94&#10; \"/></section><section data-index=\"95\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-95-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-95-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-95-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Data model summary&#10;•  7 tables for denormalization&#10;•  Normalize some tables because rare access&#10;•  ...\"/></section><section data-index=\"96\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-96-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-96-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-96-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Notes on contact_id&#10;•  In SQL, auto-generated long using sequence&#10;•  In Cassandra, auto-generated t...\"/></section><section data-index=\"97\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-97-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-97-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-97-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Notes on contact_id&#10;•  How to store both types ?&#10;97&#10; \"/></section><section data-index=\"98\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-98-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-98-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-98-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Notes on contact_id&#10;•  How to store both types ?&#10;•  As text ? ☞ easy solution …&#10;98&#10; \"/></section><section data-index=\"99\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-99-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-99-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-99-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Notes on contact_id&#10;•  How to store both types ?&#10;•  As text ? ☞ easy solution …&#10;•  … but waste of s...\"/></section><section data-index=\"100\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-100-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-100-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-100-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Notes on contact_id&#10;•  Long ☞ 8 bytes&#10;•  Long as text(UTF-8: 1 byte) ☞ &quot;digits count&quot; bytes&#10;100&#10; \"/></section><section data-index=\"101\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-101-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-101-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-101-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Notes on contact_id&#10;•  UUID ☞ 16 bytes&#10;E81D4C70-A638-11E4-83CB-DEB70BF9330F&#10;•  32 hex chars + 4 hyp...\"/></section><section data-index=\"102\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-102-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-102-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-102-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Notes on contact_id&#10;•  20 bytes wasted per contact uuid&#10;102&#10; \"/></section><section data-index=\"103\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-103-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-103-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-103-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Notes on contact_id&#10;•  20 bytes wasted per contact uuid&#10;•  × 7 denormalizations = 140 bytes per con...\"/></section><section data-index=\"104\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-104-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-104-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-104-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Notes on contact_id&#10;•  20 bytes wasted per contact uuid&#10;•  × 7 denormalizations = 140 bytes per con...\"/></section><section data-index=\"105\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-105-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-105-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-105-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Notes on contact_id&#10;•  ☞ just save contact id as byte[ ]&#10;105&#10; \"/></section><section data-index=\"106\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-106-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-106-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-106-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Notes on contact_id&#10;•  ☞ just save contact id as byte[ ]&#10;•  Achilles @TypeTransformer for automatic...\"/></section><section data-index=\"107\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-107-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-107-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-107-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Notes on contact_id&#10;•  ☞ just save contact id as byte[ ]&#10;•  Achilles @TypeTransformer for automatic...\"/></section><section data-index=\"108\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-108-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-108-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-108-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Advanced &quot;object mapper&quot;&#10;•  Fluent API&#10;•  Tons of features&#10;•  TDD friendly&#10;108&#10; \"/></section><section data-index=\"109\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-109-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-109-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-109-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Dirty checking, what is it ?&#10;109&#10; \"/></section><section data-index=\"110\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-110-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-110-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-110-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Dirty checking, what is it ?&#10;•  1 contact ≈ 8 mutable ﬁelds&#10;110&#10; \"/></section><section data-index=\"111\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-111-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-111-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-111-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Dirty checking, what is it ?&#10;•  1 contact ≈ 8 mutable ﬁelds&#10;•  × 7 denormalizations = 5...\"/></section><section data-index=\"112\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-112-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-112-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-112-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Dirty checking, what is it ?&#10;•  1 contact ≈ 8 mutable ﬁelds&#10;•  × 7 denormalizations = 5...\"/></section><section data-index=\"113\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-113-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-113-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-113-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Are you going to manually generate 56+ prepared&#10;statements for all possible updates ?&#10;1...\"/></section><section data-index=\"114\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-114-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-114-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-114-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Are you going to manually generate 56+ prepared&#10;statements for all possible updates ?&#10;&#10;...\"/></section><section data-index=\"115\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-115-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-115-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-115-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Dirty check in action&#10;115&#10;&#10;//No read-before-write&#10;&#10;ContactEntity proxy = manager.forUpd...\"/></section><section data-index=\"116\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-116-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-116-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-116-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Achilles&#10;116&#10;Empty&#10;Entity&#10;DirtyMap&#10;Proxy Setters interception&#10;PrimaryKey&#10; \"/></section><section data-index=\"117\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-117-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-117-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-117-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Dynamic statements generation&#10;117&#10;&#10;UPDATE contacts SET firstname=?, lastname=?,address=...\"/></section><section data-index=\"118\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-118-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-118-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-118-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Insert strategy, why is it so important ?&#10;118&#10; \"/></section><section data-index=\"119\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-119-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-119-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-119-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Simple INSERT prepared statement&#10;119&#10;&#10;INSERT INTO &#10;&#10; &#10;contacts(contact_id,name,age,addr...\"/></section><section data-index=\"120\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-120-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-120-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-120-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Runtime values binding&#10;•  some columns are optional&#10;120&#10;&#10;preparedStatement.bind(49374,’...\"/></section><section data-index=\"121\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-121-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-121-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-121-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Achilles&#10;121&#10;Wait … are you saying inserting null in CQL???&#10;????&#10; \"/></section><section data-index=\"122\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-122-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-122-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-122-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Achilles&#10;122&#10;Inserting null creating tombstones&#10; \"/></section><section data-index=\"123\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-123-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-123-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-123-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Achilles&#10;123&#10;Inserting null creating tombstones&#10;× 7 denormalizations&#10; \"/></section><section data-index=\"124\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-124-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-124-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-124-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Achilles&#10;124&#10;Inserting null creating tombstones&#10;× 7 denormalizations&#10;× billions of contacts created...\"/></section><section data-index=\"125\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-125-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-125-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-125-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Simple annotation&#10;125&#10;&#10;@Entity(table = &quot;contacts_by_id »)&#10;&#10;@Strategy(insert = InsertStr...\"/></section><section data-index=\"126\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-126-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-126-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-126-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Runtime dynamic INSERT statement&#10;126&#10;&#10;INSERT INTO &#10;&#10; &#10;contacts(contact_id, name, age, a...\"/></section><section data-index=\"127\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-127-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-127-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-127-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Remember the contactId ⇄ byte[ ] conversion ?&#10;127&#10;@PartitionKey&#10;@Column(name = &quot;contact...\"/></section><section data-index=\"128\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-128-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-128-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-128-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Achilles&#10;128&#10;public interface Codec&lt;FROM, TO&gt; {&#10;&#10;Class&lt;FROM&gt; sourceType();&#10;&#10;Class&lt;TO&gt; targetType();...\"/></section><section data-index=\"129\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-129-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-129-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-129-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Dynamic logging in action&#10;129&#10;&#10;2014-12-01 14:25:20,554 Bound statement : [INSERT INTO&#10;c...\"/></section><section data-index=\"130\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-130-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-130-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-130-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Dynamic logging&#10;•  runtime activation&#10;•  no need to recompile/re-deploy&#10;•  save us hour...\"/></section><section data-index=\"131\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-131-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-131-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-131-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Take Away&#10; \"/></section><section data-index=\"132\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-132-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-132-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-132-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Conditions for success&#10;•  Data modeling is crucial&#10;132&#10; \"/></section><section data-index=\"133\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-133-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-133-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-133-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Conditions for success&#10;•  Data modeling is crucial&#10;•  Double-run strategy &amp; timestamp trick FTW&#10;133&#10; \"/></section><section data-index=\"134\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-134-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-134-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-134-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Conditions for success&#10;•  Data modeling is crucial&#10;•  Double-run strategy &amp; timestamp trick FTW&#10;•  ...\"/></section><section data-index=\"135\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-135-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-135-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-135-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Conditions for success&#10;•  Data modeling is crucial&#10;•  Double-run strategy &amp; timestamp trick FTW&#10;•  ...\"/></section><section data-index=\"136\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-136-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-136-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-136-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Conditions for success&#10;•  Data modeling is crucial&#10;•  Double-run strategy &amp; timestamp trick FTW&#10;•  ...\"/></section><section data-index=\"137\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch\" data-small=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/85/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-137-320.jpg?cb=1422447584\" data-normal=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-137-638.jpg?cb=1422447584\" data-full=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-137-1024.jpg?cb=1422447584\" alt=\"#Cassandra @doanduyhai&#10;Thank You&#10;! &quot;&quot;&#10; \"/></section><div class=\"j-next-container next-container\">\n          <div class=\"content-container\">\n            <div class=\"next-slideshow-wrapper\">\n              <div class=\"j-next-slideshow next-slideshow\">\n                <p>\n                  Upcoming SlideShare\n                </p>\n                \n              </div>\n              <p>Loading in …5</p>\n              <p>×</p>\n            </div>\n          </div>\n        </div>\n    </div>\n  </div> <!-- end stage -->\n  \n  \n  \n</div>\n            </div>\n        </div>\n        \n      </div>\n      <div class=\"slideshow-info-container\" itemscope=\"\" itemtype=\"https://schema.org/MediaObject\" readability=\"7\">\n        \n        <div class=\"slideshow-tabs-container show-for-medium-up\">\n          <ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    1 Comment\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\"/>\n                \n                  19 Likes\n                \n              </a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\"/>\n                Statistics\n              </a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\"/>\n                Notes\n              </a>\n            </li>\n          </ul><div class=\"tabs-content\">\n              \n            <div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\">\n              <ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"\"><li itemtype=\"http://schema.org/Person\" itemscope=\"\">\n                      <div class=\"row\">\n                        \n                        <div class=\"small-11 columns\">\n                          <a class=\"favoriter notranslate\" title=\"ahmadanshori3\" rel=\"nofollow\" href=\"https://www.slideshare.net/ahmadanshori3?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Anshorimuslim Syuhada\n                            \n                              \n                                , \n                                Lead Engineer at eFishery\n                              \n                              \n                                 at \n                                eFishery\n                              \n                            \n                            \n                            </a>\n                        </div>\n                      </div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"\">\n                      <div class=\"row\">\n                        \n                        <div class=\"small-11 columns\">\n                          <a class=\"favoriter notranslate\" title=\"OpieBakare\" rel=\"nofollow\" href=\"https://www.slideshare.net/OpieBakare?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Ope Bakare\n                            \n                              \n                                , \n                                Senior Director, OCIO, Infrastructure Design, Engineering and Cloud Services at CVS Health\n                              \n                              \n                                 at \n                                CVS Health\n                              \n                            \n                            \n                            </a>\n                        </div>\n                      </div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"\">\n                      <div class=\"row\">\n                        \n                        <div class=\"small-11 columns\">\n                          <a class=\"favoriter notranslate\" title=\"benineng\" rel=\"nofollow\" href=\"https://www.slideshare.net/benineng?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            benineng\n                            \n                              \n                                \n                                \n                              \n                              \n                                \n                                \n                              \n                            \n                            \n                            </a>\n                        </div>\n                      </div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"\">\n                      <div class=\"row\">\n                        \n                        <div class=\"small-11 columns\">\n                          <a class=\"favoriter notranslate\" title=\"dcaravana\" rel=\"nofollow\" href=\"https://www.slideshare.net/dcaravana?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Diego Caravana\n                            \n                              \n                                , \n                                Self employed consultant, Software Developer\n                              \n                              \n                                 at \n                                Self employed\n                              \n                            \n                            \n                            </a>\n                        </div>\n                      </div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"\">\n                      <div class=\"row\">\n                        \n                        <div class=\"small-11 columns\">\n                          <a class=\"favoriter notranslate\" title=\"LeonardoSchmittAlves\" rel=\"nofollow\" href=\"https://www.slideshare.net/LeonardoSchmittAlves?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Leonardo Schmitt Alves\n                            \n                              \n                                , \n                                Analista de Programação Pleno na Buhler\n                              \n                              \n                                 at \n                                Analista de Programação Pleno\n                              \n                            \n                            \n                            </a>\n                        </div>\n                      </div>\n                    </li>\n              </ul><div class=\"more-container text-center\">\n                  <a href=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch#\" class=\"j-more-favs\">\n                    Show More\n                    \n                  </a>\n                </div>\n            </div>\n            <div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\">\n                <p>No Downloads</p>\n            </div>\n            \n            <div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\">\n              <p>No notes for slide</p>\n            </div>\n          </div>\n        </div>\n            <div class=\"notranslate transcript add-padding-right j-transcript\">\n              \n              <ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    Migration Best Practices: From RDBMS to Cassandra without a Hitch\n#Cassandra @doanduyhai\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-2-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Who am I ?&#10;2&#10;DuyHai Doan&#10;Achilles&#10;Ca...\" target=\"_blank\">\n        2.\n      </a>\n    #Cassandra @doanduyhai\nWho am I ?\n2\nDuyHai Doan\nAchilles\nCassandra Technical Advocate @ Datastax\nFormer Java Developer @ Libon\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-3-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Agenda&#10;•  Libon context&#10;•  Migration...\" target=\"_blank\">\n        3.\n      </a>\n    #Cassandra @doanduyhai\nAgenda\n•  Libon context\n•  Migration strategy\n•  Business code migration\n•  Data Modeling\n•  Take Away\n3\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-4-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Libon Context&#10; \" target=\"_blank\">\n        4.\n      </a>\n    #Cassandra @doanduyhai\nLibon Context\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-5-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;What is Libon ?&#10;•  Messaging app&#10;•  ...\" target=\"_blank\">\n        5.\n      </a>\n    #Cassandra @doanduyhai\nWhat is Libon ?\n•  Messaging app\n•  VOIP (out)\n•  Custom voicemail &amp; greetings\n•  SMS/chat/ﬁle transfer\n•  Contacts matching\n5\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-6-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Contact Matching&#10;6&#10;Libon User&#10; \" target=\"_blank\">\n        6.\n      </a>\n    #Cassandra @doanduyhai\nContact Matching\n6\nLibon User\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-7-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Contact Matching&#10;7&#10;Libon User Friend&#10; \" target=\"_blank\">\n        7.\n      </a>\n    #Cassandra @doanduyhai\nContact Matching\n7\nLibon User Friend\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-8-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Contact Matching&#10;8&#10;Libon User Friend...\" target=\"_blank\">\n        8.\n      </a>\n    #Cassandra @doanduyhai\nContact Matching\n8\nLibon User Friend\nContact matching\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-9-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Contact Matching&#10;9&#10;Libon User Friend...\" target=\"_blank\">\n        9.\n      </a>\n    #Cassandra @doanduyhai\nContact Matching\n9\nLibon User Friend\nAccept link\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-10-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Project Context&#10;•  Application grew ...\" target=\"_blank\">\n        10.\n      </a>\n    #Cassandra @doanduyhai\nProject Context\n•  Application grew over the years\n10\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-11-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Project Context&#10;•  Application grew ...\" target=\"_blank\">\n        11.\n      </a>\n    #Cassandra @doanduyhai\nProject Context\n•  Application grew over the years\n•  Already using Cassandra to handle events\n•  messaging / ﬁle sharing / SMS / notiﬁcations\n•  Cassandra R/W latencies ≈ 0,4 ms\n•  server response time under 10 ms\n11\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-12-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Project Context&#10;•  About contacts …&#10;...\" target=\"_blank\">\n        12.\n      </a>\n    #Cassandra @doanduyhai\nProject Context\n•  About contacts …\n12\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-13-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Project Context&#10;•  About contacts …&#10;...\" target=\"_blank\">\n        13.\n      </a>\n    #Cassandra @doanduyhai\nProject Context\n•  About contacts …\n•  stored as relational model in RDBMS (Oracle)\n13\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-14-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Project Context&#10;•  About contacts …&#10;...\" target=\"_blank\">\n        14.\n      </a>\n    #Cassandra @doanduyhai\nProject Context\n•  About contacts …\n•  stored as relational model in RDBMS (Oracle)\n•  1 user ≈ 300 contacts\n14\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-15-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Project Context&#10;•  About contacts …&#10;...\" target=\"_blank\">\n        15.\n      </a>\n    #Cassandra @doanduyhai\nProject Context\n•  About contacts …\n•  stored as relational model in RDBMS (Oracle)\n•  1 user ≈ 300 contacts\n•  with millions users ☞ billions of contacts to handle\n15\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-16-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Project Context&#10;•  About contacts …&#10;...\" target=\"_blank\">\n        16.\n      </a>\n    #Cassandra @doanduyhai\nProject Context\n•  About contacts …\n•  stored as relational model in RDBMS (Oracle)\n•  1 user ≈ 300 contacts\n•  with millions users ☞ billions of contacts to handle\n•  query latency unpredictable\n16\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-17-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai17&#10; \" target=\"_blank\">\n        17.\n      </a>\n    #Cassandra @doanduyhai17\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-18-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Fixing the problem&#10;•  Tune the RDBMS...\" target=\"_blank\">\n        18.\n      </a>\n    #Cassandra @doanduyhai\nFixing the problem\n•  Tune the RDBMS\n18\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-19-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Fixing the problem&#10;•  Tune the RDBMS...\" target=\"_blank\">\n        19.\n      </a>\n    #Cassandra @doanduyhai\nFixing the problem\n•  Tune the RDBMS\n•  indices\n19\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-20-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Fixing the problem&#10;•  Tune the RDBMS...\" target=\"_blank\">\n        20.\n      </a>\n    #Cassandra @doanduyhai\nFixing the problem\n•  Tune the RDBMS\n•  indices\n•  partitioning\n20\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-21-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Fixing the problem&#10;•  Tune the RDBMS...\" target=\"_blank\">\n        21.\n      </a>\n    #Cassandra @doanduyhai\nFixing the problem\n•  Tune the RDBMS\n•  indices\n•  partitioning\n•  less joins, simpliﬁed relational model\n21\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-22-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Fixing the problem&#10;•  Tune the RDBMS...\" target=\"_blank\">\n        22.\n      </a>\n    #Cassandra @doanduyhai\nFixing the problem\n•  Tune the RDBMS\n•  indices\n•  partitioning\n•  less joins, simpliﬁed relational model\n•  hardware capacity increased\n22\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-23-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Fixing the problem&#10;•  Tune the RDBMS...\" target=\"_blank\">\n        23.\n      </a>\n    #Cassandra @doanduyhai\nFixing the problem\n•  Tune the RDBMS\n•  indices\n•  partitioning\n•  less joins, simpliﬁed relational model\n•  hardware capacity increased\n23\nThat worked\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-24-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Fixing the problem&#10;•  Tune the RDBMS...\" target=\"_blank\">\n        24.\n      </a>\n    #Cassandra @doanduyhai\nFixing the problem\n•  Tune the RDBMS\n•  indices\n•  partitioning\n•  less joins, simpliﬁed relational model\n•  hardware capacity increased\n24\nThat worked\nbut …\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-25-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Back-end application&#10;RDBMS Cassandra...\" target=\"_blank\">\n        25.\n      </a>\n    #Cassandra @doanduyhai\nBack-end application\nRDBMS Cassandra\n25\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-26-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Back-end application&#10;RDBMS Cassandra...\" target=\"_blank\">\n        26.\n      </a>\n    #Cassandra @doanduyhai\nBack-end application\nRDBMS Cassandra\n26\nWe need to\nchoose\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-27-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Next Challenges&#10;•  High Availability...\" target=\"_blank\">\n        27.\n      </a>\n    #Cassandra @doanduyhai\nNext Challenges\n•  High Availability (DB failure, site failure …)\n27\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-28-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Next Challenges&#10;•  High Availability...\" target=\"_blank\">\n        28.\n      </a>\n    #Cassandra @doanduyhai\nNext Challenges\n•  High Availability (DB failure, site failure …)\n•  Predictable performance at scale\n28\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-29-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Next Challenges&#10;•  High Availability...\" target=\"_blank\">\n        29.\n      </a>\n    #Cassandra @doanduyhai\nNext Challenges\n•  High Availability (DB failure, site failure …)\n•  Predictable performance at scale\n•  Going to multi data-centers\n29\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-30-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Next Challenges&#10;•  High Availability...\" target=\"_blank\">\n        30.\n      </a>\n    #Cassandra @doanduyhai\nNext Challenges\n•  High Availability (DB failure, site failure …)\n•  Predictable performance at scale\n•  Going to multi data-centers\n☞ Cassandra, what else ?\n30\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-31-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Data Migration Strategy&#10; \" target=\"_blank\">\n        31.\n      </a>\n    #Cassandra @doanduyhai\nData Migration Strategy\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-32-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Objectives&#10;•  No downtime&#10;32&#10; \" target=\"_blank\">\n        32.\n      </a>\n    #Cassandra @doanduyhai\nObjectives\n•  No downtime\n32\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-33-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Objectives&#10;•  No downtime&#10;•  No conc...\" target=\"_blank\">\n        33.\n      </a>\n    #Cassandra @doanduyhai\nObjectives\n•  No downtime\n•  No concurrency corner-cases \n33\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-34-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Objectives&#10;•  No downtime&#10;•  No conc...\" target=\"_blank\">\n        34.\n      </a>\n    #Cassandra @doanduyhai\nObjectives\n•  No downtime\n•  No concurrency corner-cases \n•  Safe rollback possible\n34\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-35-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Objectives&#10;•  No downtime&#10;•  No conc...\" target=\"_blank\">\n        35.\n      </a>\n    #Cassandra @doanduyhai\nObjectives\n•  No downtime\n•  No concurrency corner-cases \n•  Safe rollback possible\n•  Replay-ability &amp; resume-ability\n35\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-36-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Strategy&#10;•  4 phases&#10;36&#10; \" target=\"_blank\">\n        36.\n      </a>\n    #Cassandra @doanduyhai\nStrategy\n•  4 phases\n36\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-37-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Strategy&#10;•  4 phases&#10;•  Write contac...\" target=\"_blank\">\n        37.\n      </a>\n    #Cassandra @doanduyhai\nStrategy\n•  4 phases\n•  Write contacts to both data stores\n37\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-38-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Strategy&#10;•  4 phases&#10;•  Write contac...\" target=\"_blank\">\n        38.\n      </a>\n    #Cassandra @doanduyhai\nStrategy\n•  4 phases\n•  Write contacts to both data stores\n•  Old contacts migration\n38\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-39-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Strategy&#10;•  4 phases&#10;•  Write contac...\" target=\"_blank\">\n        39.\n      </a>\n    #Cassandra @doanduyhai\nStrategy\n•  4 phases\n•  Write contacts to both data stores\n•  Old contacts migration\n•  Switch to Cassandra (but keep RDBMS in case of…)\n39\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-40-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Strategy&#10;•  4 phases&#10;•  Write contac...\" target=\"_blank\">\n        40.\n      </a>\n    #Cassandra @doanduyhai\nStrategy\n•  4 phases\n•  Write contacts to both data stores\n•  Old contacts migration\n•  Switch to Cassandra (but keep RDBMS in case of…)\n•  Remove the RDBMS code\n40\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-41-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Migration Phase 1&#10;41&#10;Back end server...\" target=\"_blank\">\n        41.\n      </a>\n    #Cassandra @doanduyhai\nMigration Phase 1\n41\nBack end server\n·\n·\n·\nSQLSQLSQL\nC*\nC*\nC*\nC*\nC*\nWrite\ncontactUUID\ncontactId … contactUUID\n129363\n 123e4567-\ne89b-12d3…\n834849\ncontacId(long) + contactUUID\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-42-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Migration Phase 1&#10;42&#10;Back end server...\" target=\"_blank\">\n        42.\n      </a>\n    #Cassandra @doanduyhai\nMigration Phase 1\n42\nBack end server\n·\n·\n·\nSQLSQLSQL\nC*\nC*\nC*\nC*\nC*\nRead\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-43-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Migration Phase 2&#10;•  On live product...\" target=\"_blank\">\n        43.\n      </a>\n    #Cassandra @doanduyhai\nMigration Phase 2\n•  On live production, migrate old contacts\n43\nSQLSQLSQL\nC*\nC*\nC*\nC*\nC*\nFor each batch of users\nSELECT * FROM contacts\nWHERE user_id = …\nAND contact_uuid IS NULL\nOld contacts created \nbefore phase 1\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-44-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Migration Phase 2&#10;•  On live product...\" target=\"_blank\">\n        44.\n      </a>\n    #Cassandra @doanduyhai\nMigration Phase 2\n•  On live production, migrate old contacts\n44\nSQLSQLSQL\nC*\nC*\nC*\nC*\nC*\nFor each batch of users\nSELECT * FROM contacts\nWHERE user_id = …\nAND contact_uuid IS NULL\nLogged batches of \nINSERT INTO contacts(..)\nVALUES(…)\nUSING TIMESTAMP\nnow() - 1 week\nOld contacts created \nbefore phase 1\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-45-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Migration Phase 2&#10;45&#10;USING TIMESTAMP...\" target=\"_blank\">\n        45.\n      </a>\n    #Cassandra @doanduyhai\nMigration Phase 2\n45\nUSING TIMESTAMP now() - 1 week ????\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-46-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Migration Phase 2&#10;•  During data mig...\" target=\"_blank\">\n        46.\n      </a>\n    #Cassandra @doanduyhai\nMigration Phase 2\n•  During data migration …\n46\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-47-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Migration Phase 2&#10;•  During data mig...\" target=\"_blank\">\n        47.\n      </a>\n    #Cassandra @doanduyhai\nMigration Phase 2\n•  During data migration …\n•  … concurrent writes from the migration batch …\n47\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-48-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Migration Phase 2&#10;•  During data mig...\" target=\"_blank\">\n        48.\n      </a>\n    #Cassandra @doanduyhai\nMigration Phase 2\n•  During data migration …\n•  … concurrent writes from the migration batch …\n•  … and updates from production for the same contact\n48\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-49-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Migration Phase 2&#10;49&#10;contact_uuid&#10;na...\" target=\"_blank\">\n        49.\n      </a>\n    #Cassandra @doanduyhai\nMigration Phase 2\n49\ncontact_uuid\nname (now -1 week)\n …\n name (now)\n …\nJohny …\n Johnny …\nInsert from batch\n(to the past)\nUpdate from production\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-50-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Migration Phase 2&#10;50&#10;contact_uuid&#10;na...\" target=\"_blank\">\n        50.\n      </a>\n    #Cassandra @doanduyhai\nMigration Phase 2\n50\ncontact_uuid\nname (now -1 week)\n …\n name (now)\n …\nJohny …\n Johnny …\nFuture reads pick the most up-to-date value \n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-51-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Last Write Win in action&#10;51&#10;Case 1 C...\" target=\"_blank\">\n        51.\n      </a>\n    #Cassandra @doanduyhai\nLast Write Win in action\n51\nCase 1 Case 2\nBatchpast(Johny)\n t1\nProdnow(Johnny)\n t2\nt3\n Read(Johnny)\nBatchpast(Johny)\nt1\nProdnow(Johnny)\nt2\nt3\n Read(Johnny)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-52-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Migration Phase 2&#10;52&#10;&quot;Write to the P...\" target=\"_blank\">\n        52.\n      </a>\n    #Cassandra @doanduyhai\nMigration Phase 2\n52\n\"Write to the Past…\nto save the Future\"\nLibon – 2014/10/08\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-53-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Migration Phase 3&#10;53&#10;Back end server...\" target=\"_blank\">\n        53.\n      </a>\n    #Cassandra @doanduyhai\nMigration Phase 3\n53\nBack end server\n·\n·\n·\nSQLSQLSQL\nC*\nC*\nC*\nC*\nC*\nWrite\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-54-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Migration Phase 4&#10;54&#10;Back end server...\" target=\"_blank\">\n        54.\n      </a>\n    #Cassandra @doanduyhai\nMigration Phase 4\n54\nBack end server\n·\n·\n·\nSQLSQLSQL\nC*\nC*\nC*\nC*\nC*\nWrite\n❌\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-55-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Business Code Refactoring&#10; \" target=\"_blank\">\n        55.\n      </a>\n    #Cassandra @doanduyhai\nBusiness Code Refactoring\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-56-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Code Inventory&#10;•  Written for RDBMS&#10;...\" target=\"_blank\">\n        56.\n      </a>\n    #Cassandra @doanduyhai\nCode Inventory\n•  Written for RDBMS\n56\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-57-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Code Inventory&#10;•  Written for RDBMS&#10;...\" target=\"_blank\">\n        57.\n      </a>\n    #Cassandra @doanduyhai\nCode Inventory\n•  Written for RDBMS\n•  Lots of joins (no surprise) \n57\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-58-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Code Inventory&#10;•  Written for RDBMS&#10;...\" target=\"_blank\">\n        58.\n      </a>\n    #Cassandra @doanduyhai\nCode Inventory\n•  Written for RDBMS\n•  Lots of joins (no surprise) \n•  Designed around transactions\n58\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-59-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Code Inventory&#10;•  Written for RDBMS&#10;...\" target=\"_blank\">\n        59.\n      </a>\n    #Cassandra @doanduyhai\nCode Inventory\n•  Written for RDBMS\n•  Lots of joins (no surprise) \n•  Designed around transactions\n•  Spring @Transactional everywhere\n59\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-60-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Code Inventory cont.&#10;•  Entities go ...\" target=\"_blank\">\n        60.\n      </a>\n    #Cassandra @doanduyhai\nCode Inventory cont.\n•  Entities go through Services &amp; Repositories\n60\nRepositories\nServices\nContactEntity\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-61-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Code Inventory cont.&#10;•  Hibernate is...\" target=\"_blank\">\n        61.\n      </a>\n    #Cassandra @doanduyhai\nCode Inventory cont.\n•  Hibernate is auto-magic\n61\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-62-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Code Inventory cont.&#10;•  Hibernate is...\" target=\"_blank\">\n        62.\n      </a>\n    #Cassandra @doanduyhai\nCode Inventory cont.\n•  Hibernate is auto-magic\n•  lazy loading\n•  1st level cache\n•  N+1 select\n62\nRepositories\nServices\nContactEntity\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-63-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Which options ?&#10;•  Throw existing co...\" target=\"_blank\">\n        63.\n      </a>\n    #Cassandra @doanduyhai\nWhich options ?\n•  Throw existing code …\n•  … and re-design from scratch for Cassandra\n63\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-64-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Which options ?&#10;•  Throw existing co...\" target=\"_blank\">\n        64.\n      </a>\n    #Cassandra @doanduyhai\nWhich options ?\n•  Throw existing code …\n•  … and re-design from scratch for Cassandra\n64\nNo way !\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-65-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Code Quality&#10;•  Existing business co...\" target=\"_blank\">\n        65.\n      </a>\n    #Cassandra @doanduyhai\nCode Quality\n•  Existing business code has…\n•  … ≈ 3500 unit tests\n65\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-66-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Code Quality&#10;•  Existing business co...\" target=\"_blank\">\n        66.\n      </a>\n    #Cassandra @doanduyhai\nCode Quality\n•  Existing business code has…\n•  … ≈ 3500 unit tests\n•  and ≈600+ integration tests\n66\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-67-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Code Quality&#10;67&#10;&quot;The code coverage&#10;i...\" target=\"_blank\">\n        67.\n      </a>\n    #Cassandra @doanduyhai\nCode Quality\n67\n\"The code coverage\nis one of your most\nvaluable technical asset\"\nLibon – since beginning\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-68-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Repositories&#10;Services&#10;Refactoring St...\" target=\"_blank\">\n        68.\n      </a>\n    #Cassandra @doanduyhai\nRepositories\nServices\nRefactoring Strategy\n68\nContactMatchingService\nContactService\nContactSync\nContactEntity\nn\n1\nn\n n\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-69-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Repositories&#10;Services&#10;Refactoring St...\" target=\"_blank\">\n        69.\n      </a>\n    #Cassandra @doanduyhai\nRepositories\nServices\nRefactoring Strategy\n69\nContactMatchingService\nContactService\nContactNoSQLEntity\nContactSync\nContactEntity\nn\n1\nn\n n\nProxy\t\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-70-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Repositories&#10;Services&#10;Refactoring St...\" target=\"_blank\">\n        70.\n      </a>\n    #Cassandra @doanduyhai\nRepositories\nServices\nRefactoring Strategy\n70\nContactMatchingService\nContactService\nContactNoSQLEntity\nContactSync\nContactEntity\nn\n1\nn\n n\nDenorm2\n …\n DenormN\nDenorm1\nProxy\t\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-71-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Refactoring Strategy&#10;•  Use CQRS&#10;•  ...\" target=\"_blank\">\n        71.\n      </a>\n    #Cassandra @doanduyhai\nRefactoring Strategy\n•  Use CQRS\n•  ContactReadRepository\n•  ContactWriteRepository\n•  ContactUpdateRepository\n•  ContactDeleteRepository\n71\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-72-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Refactoring Strategy&#10;•  ContactReadR...\" target=\"_blank\">\n        72.\n      </a>\n    #Cassandra @doanduyhai\nRefactoring Strategy\n•  ContactReadRepository\n•  direct sequential read\n•  no joins\n•  1 read ≈ 1 SELECT\n72\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-73-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Refactoring Strategy&#10;•  ContactWrite...\" target=\"_blank\">\n        73.\n      </a>\n    #Cassandra @doanduyhai\nRefactoring Strategy\n•  ContactWriteRepository\n•  write to all denormalized tables\n•  using CQL logged batches\n•  use TTLs\n73\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-74-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Refactoring Strategy&#10;•  ContactUpdat...\" target=\"_blank\">\n        74.\n      </a>\n    #Cassandra @doanduyhai\nRefactoring Strategy\n•  ContactUpdateRepository\n•  read-before-write most of the time ????\n•  rare updates ☞ acceptable perf penalty\n74\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-75-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Refactoring Strategy&#10;•  ContactDelet...\" target=\"_blank\">\n        75.\n      </a>\n    #Cassandra @doanduyhai\nRefactoring Strategy\n•  ContactDeleteRepository\n•  delete by partition key\n75\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-76-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Outcome&#10;•  5 months of 2 men work&#10;76&#10; \" target=\"_blank\">\n        76.\n      </a>\n    #Cassandra @doanduyhai\nOutcome\n•  5 months of 2 men work\n76\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-77-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Outcome&#10;•  5 months of 2 men work&#10;• ...\" target=\"_blank\">\n        77.\n      </a>\n    #Cassandra @doanduyhai\nOutcome\n•  5 months of 2 men work\n•  Many iterations to ﬁx bugs (thanks to IT)\n77\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-78-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Outcome&#10;•  5 months of 2 men work&#10;• ...\" target=\"_blank\">\n        78.\n      </a>\n    #Cassandra @doanduyhai\nOutcome\n•  5 months of 2 men work\n•  Many iterations to ﬁx bugs (thanks to IT)\n•  Lots of performance benchmarks using Gatling\n78\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-79-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Gatling Output&#10;79&#10; \" target=\"_blank\">\n        79.\n      </a>\n    #Cassandra @doanduyhai\nGatling Output\n79\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-80-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Outcome&#10;•  5 months of 2 men work&#10;• ...\" target=\"_blank\">\n        80.\n      </a>\n    #Cassandra @doanduyhai\nOutcome\n•  5 months of 2 men work\n•  Many iterations to ﬁx bugs (thanks to IT)\n•  Lots of performance benchmarks using Gatling\n☞ data model &amp; code validation\n80\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-81-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Outcome&#10;•  5 months of 2 men work&#10;• ...\" target=\"_blank\">\n        81.\n      </a>\n    #Cassandra @doanduyhai\nOutcome\n•  5 months of 2 men work\n•  Many iterations to ﬁx bugs (thanks to IT)\n•  Lots of performance benchmarks using Gatling\n☞ data model &amp; code validation\n•  … we are almost there for production\n81\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-82-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Data Model&#10; \" target=\"_blank\">\n        82.\n      </a>\n    #Cassandra @doanduyhai\nData Model\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-83-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Denormalization, the good&#10;•  Support...\" target=\"_blank\">\n        83.\n      </a>\n    #Cassandra @doanduyhai\nDenormalization, the good\n•  Support fast reads\n•  1 read ≈ 1 SELECT\n•  Worthy because mostly read, few updates\n83\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-84-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Denormalization, the bad&#10;•  Updating...\" target=\"_blank\">\n        84.\n      </a>\n    #Cassandra @doanduyhai\nDenormalization, the bad\n•  Updating mutable data can be nightmare\n•  Data model bound by existing client-facing API\n•  Update paths very error-prone without tests\n84\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-85-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Data model in detail&#10;85&#10;Contacts_by_...\" target=\"_blank\">\n        85.\n      </a>\n    #Cassandra @doanduyhai\nData model in detail\n85\nContacts_by_id\nContacts_by_identiﬁers\nContacts_in_proﬁles\nContacts_by_modiﬁcation_date\nContacts_by_ﬁrstname_lastname\nContacts_linked_user\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-86-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Data model in detail&#10;86&#10;Contacts_by_...\" target=\"_blank\">\n        86.\n      </a>\n    #Cassandra @doanduyhai\nData model in detail\n86\nContacts_by_id\nContacts_by_identiﬁers\nContacts_in_proﬁles\nContacts_by_modiﬁcation_date\nContacts_by_ﬁrstname_lastname\nContacts_linked_user\nuser_id always component\nof partition key\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-87-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Scalable design&#10;87&#10;n1&#10;n2&#10;n3&#10;n4&#10;n5&#10;n6...\" target=\"_blank\">\n        87.\n      </a>\n    #Cassandra @doanduyhai\nScalable design\n87\nn1\nn2\nn3\nn4\nn5\nn6\nn7\nn8\nA\nB\nC\nD\nE\nF\nG\nH\nuser_id1\nuser_id2\nuser_id3\nuser_id4\nuser_id5\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-88-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Scalable design&#10;88&#10;n1&#10;n2&#10;n3&#10;n4&#10;n5&#10;n6...\" target=\"_blank\">\n        88.\n      </a>\n    #Cassandra @doanduyhai\nScalable design\n88\nn1\nn2\nn3\nn4\nn5\nn6\nn7\nn8\nA\nB\nC\nD\nE\nF\nG\nH\nuser_id1user_id2\nuser_id3\nuser_id4\nuser_id5\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-89-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Bloom filters in action&#10;•  For some ...\" target=\"_blank\">\n        89.\n      </a>\n    #Cassandra @doanduyhai\nBloom filters in action\n•  For some tables, partition key = (user_id, contact_id)\n☞ fast look-up, leverages Bloom ﬁlters\n☞ touches 1 SSTable most of the time\n89\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-90-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Data model in detail&#10;90&#10;Contacts_by_...\" target=\"_blank\">\n        90.\n      </a>\n    #Cassandra @doanduyhai\nData model in detail\n90\nContacts_by_id\nContacts_by_identiﬁers\nContacts_in_proﬁles\nContacts_by_modiﬁcation_date\nContacts_by_ﬁrstname_lastname\nContacts_linked_user\nWide partition\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-91-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;A &quot;queue&quot; story&#10;•  contacts_by_modiﬁ...\" target=\"_blank\">\n        91.\n      </a>\n    #Cassandra @doanduyhai\nA \"queue\" story\n•  contacts_by_modiﬁcation_date\n•  queue-like pattern ????\n91\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-92-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;A &quot;queue&quot; story&#10;•  contacts_by_modiﬁ...\" target=\"_blank\">\n        92.\n      </a>\n    #Cassandra @doanduyhai\nA \"queue\" story\n•  contacts_by_modiﬁcation_date\n•  queue-like pattern ????\n☞ buckets to the rescue\n92\nuser_id:2014-12\ndate35\n date12 …\n …\n date47\n… …\n …\n …\nuser_id:2014-11\ndate11\n date12 …\n …\n date34\n… …\n …\n …\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-93-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Data model summary&#10;•  7 tables for d...\" target=\"_blank\">\n        93.\n      </a>\n    #Cassandra @doanduyhai\nData model summary\n•  7 tables for denormalization\n93\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-94-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Data model summary&#10;•  7 tables for d...\" target=\"_blank\">\n        94.\n      </a>\n    #Cassandra @doanduyhai\nData model summary\n•  7 tables for denormalization\n•  Normalize some tables because rare access\n94\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-95-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Data model summary&#10;•  7 tables for d...\" target=\"_blank\">\n        95.\n      </a>\n    #Cassandra @doanduyhai\nData model summary\n•  7 tables for denormalization\n•  Normalize some tables because rare access\n•  Read-before write in most update scenarios ???? \n95\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-96-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Notes on contact_id&#10;•  In SQL, auto-...\" target=\"_blank\">\n        96.\n      </a>\n    #Cassandra @doanduyhai\nNotes on contact_id\n•  In SQL, auto-generated long using sequence\n•  In Cassandra, auto-generated timeuuid \n96\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-97-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Notes on contact_id&#10;•  How to store ...\" target=\"_blank\">\n        97.\n      </a>\n    #Cassandra @doanduyhai\nNotes on contact_id\n•  How to store both types ?\n97\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-98-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Notes on contact_id&#10;•  How to store ...\" target=\"_blank\">\n        98.\n      </a>\n    #Cassandra @doanduyhai\nNotes on contact_id\n•  How to store both types ?\n•  As text ? ☞ easy solution …\n98\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-99-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Notes on contact_id&#10;•  How to store ...\" target=\"_blank\">\n        99.\n      </a>\n    #Cassandra @doanduyhai\nNotes on contact_id\n•  How to store both types ?\n•  As text ? ☞ easy solution …\n•  … but waste of space !\n•  because encoded as UTF-8 or ASCII in Cassandra\n99\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-100-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Notes on contact_id&#10;•  Long ☞ 8 byte...\" target=\"_blank\">\n        100.\n      </a>\n    #Cassandra @doanduyhai\nNotes on contact_id\n•  Long ☞ 8 bytes\n•  Long as text(UTF-8: 1 byte) ☞ \"digits count\" bytes\n100\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-101-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Notes on contact_id&#10;•  UUID ☞ 16 byt...\" target=\"_blank\">\n        101.\n      </a>\n    #Cassandra @doanduyhai\nNotes on contact_id\n•  UUID ☞ 16 bytes\nE81D4C70-A638-11E4-83CB-DEB70BF9330F\n•  32 hex chars + 4 hyphens = 36 chars\n•  UUID as text(UTF-8: 1 byte) ☞ 36 bytes\n•  Bytes overhead = 36 – 16 = 20 bytes\n101\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-102-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Notes on contact_id&#10;•  20 bytes wast...\" target=\"_blank\">\n        102.\n      </a>\n    #Cassandra @doanduyhai\nNotes on contact_id\n•  20 bytes wasted per contact uuid\n102\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-103-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Notes on contact_id&#10;•  20 bytes wast...\" target=\"_blank\">\n        103.\n      </a>\n    #Cassandra @doanduyhai\nNotes on contact_id\n•  20 bytes wasted per contact uuid\n•  × 7 denormalizations = 140 bytes per contact uuid\n103\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-104-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Notes on contact_id&#10;•  20 bytes wast...\" target=\"_blank\">\n        104.\n      </a>\n    #Cassandra @doanduyhai\nNotes on contact_id\n•  20 bytes wasted per contact uuid\n•  × 7 denormalizations = 140 bytes per contact uuid\n•  × 109 contacts = 140 GB wasted\n104\n????\nnot even counting replication factor …\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-105-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Notes on contact_id&#10;•  ☞ just save c...\" target=\"_blank\">\n        105.\n      </a>\n    #Cassandra @doanduyhai\nNotes on contact_id\n•  ☞ just save contact id as byte[ ]\n105\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-106-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Notes on contact_id&#10;•  ☞ just save c...\" target=\"_blank\">\n        106.\n      </a>\n    #Cassandra @doanduyhai\nNotes on contact_id\n•  ☞ just save contact id as byte[ ]\n•  Achilles @TypeTransformer for automatic conversion\n(see later)\n106\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-107-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Notes on contact_id&#10;•  ☞ just save c...\" target=\"_blank\">\n        107.\n      </a>\n    #Cassandra @doanduyhai\nNotes on contact_id\n•  ☞ just save contact id as byte[ ]\n•  Achilles @TypeTransformer for automatic conversion\n(see later)\n•  Use blobAsBigInt( ) or blobAsUUID( ) to view data\n107\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-108-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Advanced &quot;object mapper&quot;...\" target=\"_blank\">\n        108.\n      </a>\n    #Cassandra @doanduyhai\nAchilles\n•  Advanced \"object mapper\"\n•  Fluent API\n•  Tons of features\n•  TDD friendly\n108\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-109-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Dirty checking, what is ...\" target=\"_blank\">\n        109.\n      </a>\n    #Cassandra @doanduyhai\nAchilles\n•  Dirty checking, what is it ?\n109\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-110-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Dirty checking, what is ...\" target=\"_blank\">\n        110.\n      </a>\n    #Cassandra @doanduyhai\nAchilles\n•  Dirty checking, what is it ?\n•  1 contact ≈ 8 mutable ﬁelds\n110\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-111-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Dirty checking, what is ...\" target=\"_blank\">\n        111.\n      </a>\n    #Cassandra @doanduyhai\nAchilles\n•  Dirty checking, what is it ?\n•  1 contact ≈ 8 mutable ﬁelds\n•  × 7 denormalizations = 56 update combinations …\n111\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-112-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Dirty checking, what is ...\" target=\"_blank\">\n        112.\n      </a>\n    #Cassandra @doanduyhai\nAchilles\n•  Dirty checking, what is it ?\n•  1 contact ≈ 8 mutable ﬁelds\n•  × 7 denormalizations = 56 update combinations …\n•  and not even counting multiple ﬁelds updates … \n112\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-113-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Are you going to manuall...\" target=\"_blank\">\n        113.\n      </a>\n    #Cassandra @doanduyhai\nAchilles\n•  Are you going to manually generate 56+ prepared\nstatements for all possible updates ?\n113\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-114-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Are you going to manuall...\" target=\"_blank\">\n        114.\n      </a>\n    #Cassandra @doanduyhai\nAchilles\n•  Are you going to manually generate 56+ prepared\nstatements for all possible updates ?\n•  Or just use dynamic plain string statements and get\nsome perf penalty ?\n114\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-115-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Dirty check in action&#10;11...\" target=\"_blank\">\n        115.\n      </a>\n    #Cassandra @doanduyhai\nAchilles\n•  Dirty check in action\n115\n//No read-before-write\nContactEntity proxy = manager.forUpdate(ContactEntity.class, contactId);\nproxy.setFirstName(…);\nproxy.setLastName(…); //type-safe updates\nproxy.setAddress(…);\nmanager.update(proxy);\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-116-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Achilles&#10;116&#10;Empty&#10;Entity&#10;DirtyMap&#10;P...\" target=\"_blank\">\n        116.\n      </a>\n    #Cassandra @doanduyhai\nAchilles\n116\nEmpty\nEntity\nDirtyMap\nProxy Setters interception\nPrimaryKey\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-117-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Dynamic statements gener...\" target=\"_blank\">\n        117.\n      </a>\n    #Cassandra @doanduyhai\nAchilles\n•  Dynamic statements generation\n117\nUPDATE contacts SET firstname=?, lastname=?,address=?\nWHERE contact_id=?\nprepared statements are cached, of course\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-118-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Insert strategy, why is ...\" target=\"_blank\">\n        118.\n      </a>\n    #Cassandra @doanduyhai\nAchilles\n•  Insert strategy, why is it so important ?\n118\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-119-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Simple INSERT prepared s...\" target=\"_blank\">\n        119.\n      </a>\n    #Cassandra @doanduyhai\nAchilles\n•  Simple INSERT prepared statement\n119\nINSERT INTO \n \ncontacts(contact_id,name,age,address,gender,avatar,…) \nVALUES(?, ?, ?, ? … ?);\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-120-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Runtime values binding&#10;•...\" target=\"_blank\">\n        120.\n      </a>\n    #Cassandra @doanduyhai\nAchilles\n•  Runtime values binding\n•  some columns are optional\n120\npreparedStatement.bind(49374,’John DOE’,33, null, null, …, null);\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-121-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Achilles&#10;121&#10;Wait … are you saying i...\" target=\"_blank\">\n        121.\n      </a>\n    #Cassandra @doanduyhai\nAchilles\n121\nWait … are you saying inserting null in CQL???\n????\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-122-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Achilles&#10;122&#10;Inserting null creating...\" target=\"_blank\">\n        122.\n      </a>\n    #Cassandra @doanduyhai\nAchilles\n122\nInserting null creating tombstones\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-123-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Achilles&#10;123&#10;Inserting null creating...\" target=\"_blank\">\n        123.\n      </a>\n    #Cassandra @doanduyhai\nAchilles\n123\nInserting null creating tombstones\n× 7 denormalizations\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-124-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Achilles&#10;124&#10;Inserting null creating...\" target=\"_blank\">\n        124.\n      </a>\n    #Cassandra @doanduyhai\nAchilles\n124\nInserting null creating tombstones\n× 7 denormalizations\n× billions of contacts created\n????\nnot even counting replication factor …\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-125-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Simple annotation&#10;125&#10;&#10;@...\" target=\"_blank\">\n        125.\n      </a>\n    #Cassandra @doanduyhai\nAchilles\n•  Simple annotation\n125\n@Entity(table = \"contacts_by_id »)\n@Strategy(insert = InsertStrategy.NOT_NULL_FIELDS)\npublic class ContactById {\n}\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-126-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Runtime dynamic INSERT s...\" target=\"_blank\">\n        126.\n      </a>\n    #Cassandra @doanduyhai\nAchilles\n•  Runtime dynamic INSERT statement\n126\nINSERT INTO \n \ncontacts(contact_id, name, age, address,) \nVALUES(:contact_id, :name, :age, :address);\nprepared statements are cached, of course\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-127-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Remember the contactId ⇄...\" target=\"_blank\">\n        127.\n      </a>\n    #Cassandra @doanduyhai\nAchilles\n•  Remember the contactId ⇄ byte[ ] conversion ?\n127\n@PartitionKey\n@Column(name = \"contact_id\")\n@TypeTransformer(valueCodecClass = ContactIdToBytes.class)\nprivate ContactId contactId;\nBYOC ☞ Bring Your Own Codec\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-128-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Achilles&#10;128&#10;public interface Codec&lt;...\" target=\"_blank\">\n        128.\n      </a>\n    #Cassandra @doanduyhai\nAchilles\n128\npublic interface Codec&lt;FROM, TO&gt; {\nClass&lt;FROM&gt; sourceType();\nClass&lt;TO&gt; targetType();\nTO encode(FROM fromJava)\nFROM decode(TO fromCassandra);\n}\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-129-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Dynamic logging in actio...\" target=\"_blank\">\n        129.\n      </a>\n    #Cassandra @doanduyhai\nAchilles\n•  Dynamic logging in action\n129\n2014-12-01 14:25:20,554 Bound statement : [INSERT INTO\ncontacts.contacts_by_modiﬁcation_date(user_id,month_bucket,modiﬁcation_date,...) VALUES\n(:user_id,:month_bucket,:modiﬁcation_date,...) USING TTL :ttl;] with CONSISTENCY LEVEL [LOCAL_QUORUM]\n2014-12-01 14:25:20,554 bound values : [222130151, 2014-12, e13d0d50-7965-11e4-af38-90b11c2549e0, ...]\n2014-12-01 14:25:20,701 Bound statement : [SELECT birthday,middlename,avatar_size,... FROM\ncontacts.contacts_by_modiﬁcation_date WHERE user_id=:user_id AND month_bucket=:month_bucket AND\n(modiﬁcation_date)&gt;=(:modiﬁcation_date) ORDER BY modiﬁcation_date ASC;] with CONSISTENCY LEVEL\n[LOCAL_QUORUM]\n2014-12-01 14:25:20,701 bound values : [222130151, 2014-10, be6bc010-6109-11e4-b385-000038377ead]\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-130-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Achilles&#10;•  Dynamic logging&#10;•  runti...\" target=\"_blank\">\n        130.\n      </a>\n    #Cassandra @doanduyhai\nAchilles\n•  Dynamic logging\n•  runtime activation\n•  no need to recompile/re-deploy\n•  save us hours of debugging\n•  TRACE log level ☞ query tracing\n130\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-131-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Take Away&#10; \" target=\"_blank\">\n        131.\n      </a>\n    #Cassandra @doanduyhai\nTake Away\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-132-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Conditions for success&#10;•  Data model...\" target=\"_blank\">\n        132.\n      </a>\n    #Cassandra @doanduyhai\nConditions for success\n•  Data modeling is crucial\n132\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-133-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Conditions for success&#10;•  Data model...\" target=\"_blank\">\n        133.\n      </a>\n    #Cassandra @doanduyhai\nConditions for success\n•  Data modeling is crucial\n•  Double-run strategy &amp; timestamp trick FTW\n133\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-134-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Conditions for success&#10;•  Data model...\" target=\"_blank\">\n        134.\n      </a>\n    #Cassandra @doanduyhai\nConditions for success\n•  Data modeling is crucial\n•  Double-run strategy &amp; timestamp trick FTW\n•  Data type conversion can be tricky\n134\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-135-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Conditions for success&#10;•  Data model...\" target=\"_blank\">\n        135.\n      </a>\n    #Cassandra @doanduyhai\nConditions for success\n•  Data modeling is crucial\n•  Double-run strategy &amp; timestamp trick FTW\n•  Data type conversion can be tricky\n•  Benchmark !\n135\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-136-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Conditions for success&#10;•  Data model...\" target=\"_blank\">\n        136.\n      </a>\n    #Cassandra @doanduyhai\nConditions for success\n•  Data modeling is crucial\n•  Double-run strategy &amp; timestamp trick FTW\n•  Data type conversion can be tricky\n•  Benchmark !\n•  Mindset shifts for the team\n136\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01/95/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch-137-638.jpg?cb=1422447584\" title=\"#Cassandra @doanduyhai&#10;Thank You&#10;! &quot;&quot;&#10; \" target=\"_blank\">\n        137.\n      </a>\n    #Cassandra @doanduyhai\nThank You\n! \"\"\n \n  </li>\n              </ol></div>\n      </div>\n    </div>\n    <aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\"><dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\">\n    <ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"44005594\" title=\"Teaching Techniques: Classroom Cloud Strategy\" href=\"https://www.linkedin.com/learning/teaching-techniques-classroom-cloud-strategy?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Teaching Techniques: Classroom Cloud Strategy\" data-ga-action=\"click\" readability=\"2\">\n    <div class=\"lynda-thumbnail\">\n      <img class=\"j-thumbnail j-lazy-thumb\" alt=\"Teaching Techniques: Classroom Cloud Strategy\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=kkGTjsU36x0qgLdal7Z7yNGgXh8%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-nWySt89efZnfofs7dZLSiol4ffSUFkg03feavRjLgG469LcLmY4Yx3A\"/>\n    </div>\n    <div class=\"lynda-content\" readability=\"39\">\n      <p>\n        Teaching Techniques: Classroom Cloud Strategy\n      </p>\n      <p>\n        Online Course - LinkedIn Learning\n      </p>\n    </div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"44005594\" title=\"Teacher Tips\" href=\"https://www.linkedin.com/learning/teacher-tips?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Teacher Tips\" data-ga-action=\"click\" readability=\"1\">\n    <div class=\"lynda-thumbnail\">\n      <img class=\"j-thumbnail j-lazy-thumb\" alt=\"Teacher Tips\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=mN807ffzPmE7p%2FNpTyJseUnrM8Q%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-gXySu_NCfYXPofcHaZLSiol8eeiUIlwE0feuvQjDoEo69LcLmY4Yx3A\"/>\n    </div>\n    <div class=\"lynda-content\" readability=\"37\">\n      <p>\n        Teacher Tips\n      </p>\n      <p>\n        Online Course - LinkedIn Learning\n      </p>\n    </div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"44005594\" title=\"Bruce Heavin The Thinkable Presentation\" href=\"https://www.linkedin.com/learning/bruce-heavin-the-thinkable-presentation?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Bruce Heavin The Thinkable Presentation\" data-ga-action=\"click\" readability=\"2\">\n    <div class=\"lynda-thumbnail\">\n      <img class=\"j-thumbnail j-lazy-thumb\" alt=\"Bruce Heavin The Thinkable Presentation\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=7Pmu5jhLuzYhd3kcdQaN2ktEQv0%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-gWiKv_9yfYXbufMLWZLOn7FQIImxW\"/>\n    </div>\n    <div class=\"lynda-content\" readability=\"39\">\n      <p>\n        Bruce Heavin The Thinkable Presentation\n      </p>\n      <p>\n        Online Course - LinkedIn Learning\n      </p>\n    </div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"32052449\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Bulk Loading Data into Cassandra\" href=\"https://www.slideshare.net/DataStax/bulk-loading-data-into-cassandra\" readability=\"-24\">\n    \n    <div class=\"related-content\" readability=\"12\">\n      <p>\n        Bulk Loading Data into Cassandra\n      </p>\n        <p>DataStax</p>\n    </div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"39677149\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Cassandra Summit 2014: Apache Cassandra Best Practices at Ebay\" href=\"https://www.slideshare.net/planetcassandra/cassandra-summit-2014-39677149\" readability=\"-24\">\n    \n    <div class=\"related-content\" readability=\"12\">\n      <p>\n        Cassandra Summit 2014: Apache Cassandra Best Practices at Ebay\n      </p>\n        <p>DataStax Academy</p>\n    </div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"26627481\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Cassandra Community Webinar | Make Life Easier - An Introduction to Cassandra Query Language\" href=\"https://www.slideshare.net/DataStax/cassandra-community-webinar-make-life-easier-an-introduction-to-cassandra-query-language\" readability=\"-24\">\n    \n    <div class=\"related-content\" readability=\"12\">\n      <p>\n        Cassandra Community Webinar | Make Life Easier - An Introduction to Cassandra...\n      </p>\n        <p>DataStax</p>\n    </div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"34217961\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Webinar: DataStax Training - Everything you need to become a Cassandra Rockstar\" href=\"https://www.slideshare.net/DataStax/webinar-datastax-training-everything-you-need-to-become-a-cassandra-rockstar\" readability=\"-24\">\n    \n    <div class=\"related-content\" readability=\"12\">\n      <p>\n        Webinar: DataStax Training - Everything you need to become a Cassandra Rockstar\n      </p>\n        <p>DataStax</p>\n    </div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"44317502\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Webinar | Target Modernizes Retail with Engaging Digital Experiences\" href=\"https://www.slideshare.net/DataStax/webinar-target-modernizes-retail-with-engaging-digital-experiences\" readability=\"-24\">\n    \n    <div class=\"related-content\" readability=\"12\">\n      <p>\n        Webinar | Target Modernizes Retail with Engaging Digital Experiences\n      </p>\n        <p>DataStax</p>\n    </div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"26178015\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"How much money do you lose every time your ecommerce site goes down?\" href=\"https://www.slideshare.net/DataStax/how-much-money-do-you-lose-every-time-your-ecommerce-site-goes-down\" readability=\"-24\">\n    \n    <div class=\"related-content\" readability=\"12\">\n      <p>\n        How much money do you lose every time your ecommerce site goes down?\n      </p>\n        <p>DataStax</p>\n    </div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"40647728\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Webinar | How Clear Capital Delivers Always-on Appraisals on 122 Million Properties with DataStax\" href=\"https://www.slideshare.net/DataStax/webinar-how-clear-capital-delivers-always-on-appraisals-on-122-million-properties-with-data-stax\" readability=\"-24\">\n    \n    <div class=\"related-content\" readability=\"12\">\n      <p>\n        Webinar | How Clear Capital Delivers Always-on Appraisals on 122 Million Prop...\n      </p>\n        <p>DataStax</p>\n    </div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div>\n</div>\n      \n      \n        <footer>\n          <div class=\"row\">\n            <div class=\"columns\">\n              <ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div>\n          </div>\n          \n          <div class=\"row\" readability=\"5\">\n            <div class=\"columns\" readability=\"11\">\n              <p class=\"copyright text-center\">LinkedIn Corporation © 2017</p>\n              \n            </div>\n          </div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\" readability=\"34\">\n          \n    <div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" readability=\"7\">\n  <h4 class=\"modal-title\">Public clipboards featuring this slide</h4>\n  <hr/>\n  \n  <p>\n    No public clipboards found for this slide\n  </p>\n  \n</div>\n    \n    <div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\" readability=\"7\">\n  <p>\n    <h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr/><a class=\"close-reveal-modal button-lrg\" href=\"https://www.slideshare.net/planetcassandra/migration-best-practices-from-rdbms-to-cassandra-without-a-hitch#\" aria-label=\"Close\">×</a>\n  </p>\n  \n  <div class=\"modal-content\" readability=\"35\">\n    <div class=\"default-clipboard-panel radius\" readability=\"6\">\n      <p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"/> already.</p>\n    </div>\n    \n    <div class=\"clipboard-list-container\">\n      <div class=\"clipboard-create-new\">\n        \n        <p>Create a clipboard</p>\n      </div>\n    </div>\n  </div>\n</div>\n    <div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\" readability=\"11\">\n    <p>\n      <h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.\n      \n    </p>\n    <h4 class=\"modal-title\" id=\"modal-title\"/>\n    <hr aria-hidden=\"true\"/><form data-abide=\"\">\n    \n    <div class=\"row\">\n      <p>\n        <label>Description\n          \n        </label>\n      </p>\n    </div>\n    <div class=\"row\" readability=\"6\">\n      <label readability=\"2\">Visibility\n        <p>\n          <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          <label for=\"privacy-switch\"/>\n        </p>\n      </label>\n    </div>\n        \n    </form>\n    </div>\n    </div>\n    \n    \n      <meta content=\"{&quot;jsplayer_toolbar_clips.clip_count_info&quot;:&quot;{:count,choice,0#Be the first to clip this slide|singular#person clipped this slide|plural#people clipped this slide}&quot;}\" name=\"ss-i18n-translations\"/><noscript>\n    </noscript>\n  </body>",
        "created_at": "2017-09-14T12:54:46+0000",
        "updated_at": "2017-12-27T14:23:02+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 14,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/fromrdbmstocassandrawithoutahitch-150128121838-conversion-gate01-thumbnail-4.jpg?cb=1422447584",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/5174"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 15,
            "label": "tutorial",
            "slug": "tutorial"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 873,
            "label": "mysql",
            "slug": "mysql"
          }
        ],
        "is_public": false,
        "id": 5173,
        "uid": null,
        "title": "5 Tactics to Migrate Between MySQL and Cassandra Without Downtime",
        "url": "https://www.hakkalabs.co/articles/migrate-mysql-cassandra-without-downtime",
        "content": "<p><strong>Making and implementing a C* Migration Plan</strong>\n<br/>Migrating to a new database is hard: really hard. It’s almost impossible to do it perfectly. I’d like to break the migration issue into two parts: 1) maintaining integrity of your data during import and migration and 2) how to operationally plan and code a migration plan to migrate from MySQL to C* downtime free (fingers crossed!).</p>\n<p><strong>Maintaining Data Integrity</strong>\n<br/>In my personal experience, the most difficult component of our migration was not in writing a set of reliable scripts to read from MySQL and insert into Cassandra, but trivial coding mistakes that caused significant data discrepancies between the MySQL and Cassandra versions of the data.</p>\n<p>Because migrating from MySQL to Cassandra will most likely require a change in your data model, the logic required to “convert” your relational MySQL data to it’s de-normalized form is the hardest part of the migration and certainly has the biggest risk.</p>\n<p>Treat your migration scripts and logic not as one-off instances, but production quality code that can be run in any order, at any time. Mistakes in migration logic that result in an inconsistent version of the migrated data in Cassandra most likely will have a much greater impact than other dataset migration related bugs.</p>\n<p><strong>Personal Cassandra Migration Horror Story:</strong>\n<br/>One of our datasets is the classification result of a particular domain from various sets of logic and sources. We use all of this data to make a final decision on what category a domain should be classified as (i.e. pornography, business, etc). When we started going thru our historical MySQL data we found data that had been inserted with incorrect values due to various application bugs over the years. With each example of this incorrect data, we would add conversion logic to “fix” the data best we could prior to syncing it from MySQL to Cassandra.</p>\n<p>In one of these instances, I accidentally wrote logic that contained an equality check to see if a value was greater-than 0 to identify data in MySQL that needed to be fixed prior to migration. Unfortunately, some of the values being checked (in this case an epoch) were originally inserted as “0”. This meant my greater-than equality check ended up skipping some data during the sync. Although we had spent weeks attempting to validate the final resulting data after the migration, we missed this bug prior to production. The end result was incorrectly classifying high profile domains and ultimately inconveniencing our customers.</p>\n<p>The actual number of rows impacted from the bug was only around 1,000 out of the 100+ million correctly migrated rows however; the impact of those 1,000 rows to our customers was very real and significant.</p>\n<p><strong>Get to know Bulk Loading</strong>\n<br/>Regardless of the migration strategy you end up choosing, in almost all cases you will have to perform an initial bulk import of your existing MySQL data into C*. While it might be tempting to simply iterate over every MySQL result and then insert that result one mutation at a time into Cassandra, a more efficient way is to use the Cassandra Bulk Loader. At a high level, the Bulk Loader requires you to create a CSV file containing all of the rows and columns that need to be loaded into Cassandra. Using the Java class SSTableSimpleUnsortedWriter, you can create an sstable from your csv file, which can then be loaded directly into Cassandra using sstableloader.</p>\n<p>For more details and code samples reference <a title=\"Bulk Loading\" href=\"http://www.datastax.com/dev/blog/bulk-loading\" target=\"_blank\">this</a> article.</p>\n<p><strong>Different Ways to Migrate without Downtime</strong></p>\n<p><strong>Sync Data Method:</strong>\n<br/>When migrating to Cassandra and choosing a new data model might significantly increase your database workload. Alternatively, you might still need a live dataset in MySQL after the initial migration for legacy scripts that have not yet been migrated to use Cassandra.</p>\n<p><strong>Syncing from MySQL to Cassandra</strong>\n<br/>In some cases it might not be practicable to add Cassandra to a legacy application. In this case it might be necessary to have an external process sync data from MySQL to Cassandra while running both new and old logic in parallel.</p>\n<p>1st Solution: Add a timestamp column to the MySQL table to be synced. With each update to MySQL also update the timestamp with the last updated time. At a scheduled interval then do a SELECT query from all MySQL shards where the last updated timestamp is greater than or equal to the time your last sync started.</p>\n<p><strong>Syncing from Cassandra back to MySQL</strong>\n<br/>Some data models will be hard to sync from Cassandra back to MySQL (for example time series data). However, rows containing more de-normalized “metadata”-like information can be synced.</p>\n<p>What won’t work: Creating a sync script that executes via cron every n minutes and attempts to do a SELECT * FROM TABLE foo from Cassandra (and then update and insert all of those records into MySQL) is a recipe for failure. Inherent to Cassandra’s design is that data is sharded across multiple nodes by a hash of it’s key. Performing a SELECT * query is a Cassandra anti-pattern and should be avoided. Iterating thru every key across all nodes and returning a single paged dataset is both inefficient and impracticable.</p>\n<p>1st Solution: Implement a queue that your application additionally writes to when it modifies a row in Cassandra. Have a script consume from this queue and de-duplicate the modified keys on a time interval and then bulk insert updates into MySQL.</p>\n<p>2nd Solution: If the data can be updated less frequently into MySQL, you could write a Hadoop Map/Reduce job that iterates over the column families that you need to sync. This solution gives a practicable and reproducible way to iterate thru all keys in a column family. Using this approach as an additional sanity option to resolve missed updates from other incremental sync options is also not a bad option.</p>\n<p>3rd Solution: Another option if you can afford a greater delay in the delta between updates from Cassandra back to MySQL is to use a tool such as sstable2json to dump a column families sstables into a JSON format, which can then be parsed and then used to update MySQL. This is a pretty heavy-handed method. Additionally, you’ll have to write logic to ensure you dump the sstables from all nodes to get the entire column family.</p>\n<p><strong>Write Twice and Forget Method:</strong>\n<br/>If you are able to modify your existing application to also interface with Cassandra, you can initially start your migration by writing database updates twice, once to MySQL and an additional time to Cassandra.\n<br/>Once you have all new updates being written to both MySQL and Cassandra, you can run a migration script that pages thru all your existing MySQL data and inserts those records into Cassandra.</p>\n<p>Initially, you might want to implement this second write to Cassandra as a completely non-blocking, write and forget, operation. If you experience initial issues during your Cassandra deployment, make sure not to impact your existing application when Cassandra is down.</p>\n<p>Once you are satisfied with the fire-and-forget writes, you can slowly modify your application logic to start performing reads from Cassandra instead of MySQL. Thanks to the duel writes, if you run into issues, simply revert back to doing reads from MySQL.</p>\n<p><strong>The “Just-Rip-Off The Band-Aid” Method:</strong>\n<br/>The final method I’ll propose is a hard cutover between MySQL and Cassandra. I’d strongly recommend against this. Getting all the data fully migrated from MySQL to Cassandra an error-prone process.</p>\n<p>Stay tuned this week for more posts from Michael Kjellman. This post is an <a title=\"Tired of MySQL?\" href=\"http://planetcassandra.org/mysql-to-cassandra-migration/\">excerpt</a> from '<em>Frustrated with MySQL? Improving the scalability, reliability and performance of your application by migrating from MySQL to Cassandra.</em>' In the meantime, check out our other <a title=\"Cassandra Week\" href=\"http://www.hakkalabs.co/cassandra\" target=\"_blank\">Cassandra Week</a> posts!</p>",
        "created_at": "2017-09-14T12:55:03+0000",
        "updated_at": "2017-12-27T14:22:19+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "",
        "reading_time": 6,
        "domain_name": "www.hakkalabs.co",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/5173"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 15,
            "label": "tutorial",
            "slug": "tutorial"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          }
        ],
        "is_public": false,
        "id": 5172,
        "uid": null,
        "title": "Introduction to Spark & Cassandra — Rustyrazorblade",
        "url": "http://rustyrazorblade.com/2015/01/introduction-to-spark-cassandra/",
        "content": "<!--- title: DESTROY YOUR NEW YEARS RESOLUTIONS WITH SPARK AND CASSANDRA ---><p>I've been messing with <a href=\"https://spark.apache.org/\">Apache Spark</a> quite a bit lately.  If you aren't familiar, Spark is a general purpose engine for large scale data processing.  Initially it comes across as simply a replacement for Hadoop, but that would be selling it short.  Big time.  In addition to bulk processing (goodbye MapReduce!), Spark includes:</p><ul><li>SQL engine</li>\n<li>Stream processing via Kafka, Flume, ZeroMQ</li>\n<li>Machine Learning</li>\n<li>Graph Processing</li>\n</ul><p>Sounds awesome, right?   That's because it is, babaganoush.  The next question is where do we store our data?  Spark works with a number of projects, but my database of choice these days is <a href=\"http://cassandra.apache.org/\">Apache Cassandra</a>.  Easy scale out and always up.  It's approximately this epic:</p>\n<p><img alt=\"cat-riding-a-fire-breathing-unicorn\" src=\"http://rustyrazorblade.com/images/cat-riding-a-fire-breathing-unicorn.jpg\"/></p>\n<p>If you haven't used <a href=\"http://cassandra.apache.org/\">Apache Cassandra</a> yet, that's ok.  I've included some links at the end of the post to help you get started.</p>\n<p>We're going to set up a really simple Spark job that does a data migration for Cassandra.  We'll be copying 1 table to another, but changing the table structure.  Currently Spark / Cassandra jobs require the open source connector from DataStax.</p>\n<p>I'm going to be using <a href=\"https://www.jetbrains.com/idea/#community_edition\">IntelliJ</a>, (community version is free) and hard coding a few things that'll make it easy to test Spark jobs from IntelliJ itself.  In a future post I'll go over how to actually deploy spark into production.</p>\n<p>For convenience / reference, I've pushed the working code as a standalone project up to <a href=\"https://github.com/rustyrazorblade/spark-data-migration\">github</a>.</p>\n<p>You'll need to have a <a href=\"http://planetcassandra.org/try-cassandra/\">Cassandra server up and running locally</a> to execute the Spark job.</p>\n<p>Let's suppose we have a system with a users table.  In that table, we store a user's name, and their favorite food.  We want to be able to create a table that maps food to users.  For performance reasons, we want this in a single table.  At a CQL prompt, create the tables and add sample data:</p>\n<div class=\"highlight\"><pre>CREATE KEYSPACE tutorial WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};\nuse tutorial;\nCREATE TABLE tutorial.user (\nname text primary key,\nfavorite_food text\n);\ncreate table tutorial.food_to_user_index ( food text, user text, primary key (food, user));\ninsert into user (name, favorite_food) values ('Jon', 'bacon');\ninsert into user (name, favorite_food) values ('Luke', 'steak');\ninsert into user (name, favorite_food) values ('Al', 'salmon');\ninsert into user (name, favorite_food) values ('Chris', 'chicken');\ninsert into user (name, favorite_food) values ('Rebecca', 'bacon');\ninsert into user (name, favorite_food) values ('Patrick', 'brains');\ninsert into user (name, favorite_food) values ('Duy Hai', 'brains');\n</pre></div>\n<p>With your sample data all set up, go ahead and create your project.  Choose Scala -&gt; SBT.</p>\n<p><img alt=\"create_project\" src=\"http://rustyrazorblade.com/images/spark_intro_create_project.png\"/></p>\n<p>Make sure you pick Scala 2.10.4 as your Scala version.</p>\n<p><img alt=\"create_project2\" src=\"http://rustyrazorblade.com/images/spark_intro_create_project2.png\"/></p>\n<p>We're going to need to edit our SBT file to include the Spark and Cassandra dependencies.  Open up <code>build.sbt</code> and add the libraryDependencies of Spark and the Spark connector.  It should look like the following:</p>\n<div class=\"highlight\"><pre>name := \"intro_to_spark\"\nversion := \"1.0\"\nscalaVersion := \"2.10.4\"\nlibraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"1.2.0\"\nlibraryDependencies += \"com.datastax.spark\" %% \"spark-cassandra-connector\" %  \"1.1.0\" withSources() withJavadoc()\n</pre></div>\n<p>Right click and create a new Scala file under <code>src/main/scala</code>, and pick Object.  I called mine DataMigration.  Go ahead and open it up for editing.  For this example it'll contain all the code we need.</p>\n<p>The first thing that we're going to do is to import the SparkContext and SparkConf.  These classes are used to represent the Spark Cluster and set up configuration, respectively.</p>\n<div class=\"highlight\"><pre>import org.apache.spark.{SparkContext,SparkConf}\n</pre></div>\n<p>To talk to Apache Cassandra, we need to import the connector.  The connector adds functionality to the spark context - specifically the ability to create RDDs (Spark's term for a distributed dataset) from Cassandra tables.</p>\n<div class=\"highlight\"><pre>import com.datastax.spark.connector._\n</pre></div>\n<p>Next we're going to create our <code>object</code>.  An object in Scala is a singleton.  It's very useful for Spark jobs.  Our object has a <code>main</code> method:</p>\n<div class=\"highlight\"><pre>object DataMigration {\n  def main(args: Array[String]): Unit = {\n</pre></div>\n<p>Alright, time for some meat.  We're going to set up the SparkConf (the configuration) now.  We want to tell it where to find Cassandra.</p>\n<div class=\"highlight\"><pre>val conf = new SparkConf(true)\n  .set(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n</pre></div>\n<p>We create our Spark Context here by providing the Spark configuration.  The first parameter, <code>local</code>, is hard coded here to make development easier.  We're going to name the job \"test\" and provide the Spark Configuration we created earlier.</p>\n<div class=\"highlight\"><pre>val sc = new SparkContext(\"local\", \"test\", conf)\n</pre></div>\n<p>We need to define a case class to represent a Cassandra row.  This comes in handy later.</p>\n<div class=\"highlight\"><pre>case class FoodToUserIndex(food: String, user: String)\n</pre></div>\n<p>Next, we get a reference to our user table (cleverly named user_table) through the context.  The <code>.cassandraTable</code> method takes a keyspace and a table name.  We get back an RDD, which we can perform operations on, like mapping, and filtering.</p>\n<div class=\"highlight\"><pre>val user_table = sc.cassandraTable(\"tutorial\", \"user\")\n</pre></div>\n<p>This line's a little tricky.  What we're doing is saying, hey, <code>user_table</code>!  Let's perform an operation on every row you've got!  The map takes a function; the function will be applied to every row in the table.  Each row in the table will be \"r\" in the below example.  For each row, we're going to create a new FoodToUserIndex case class.  The parameters of the case class are the <code>favorite_food</code> and <code>name</code> from the user row.  We're going to store this in the <code>food_index</code> immutable variable.</p>\n<div class=\"highlight\"><pre>val food_index = user_table.map(r =&gt; new FoodToUserIndex(r.getString(\"favorite_food\"), r.getString(\"name\")))\n</pre></div>\n<p>Functional programming can be a bit tricky.  Don't give up!  Break it down to small pieces, and then put it all together.</p>\n<p>Here's where the data gets written to the new table.  We can take an RDD, and save it to Cassandra with... <code>saveToCassandra(keyspace, table_name)</code>.  That's actually the end of the code.</p>\n<div class=\"highlight\"><pre>    food_index.saveToCassandra(\"tutorial\", \"food_to_user_index\")\n  }\n}\n</pre></div>\n<p>OK, time to run the Spark job.  If you didn't create your <code>tutorial</code> keyspace, tables, and sample data already, you'll need to do it now or the job will fail miserably.  To run the job right in IntelliJ, right click, and \"run\".</p>\n<p><img alt=\"intro_to_spark_run\" src=\"http://rustyrazorblade.com/images/intro_to_spark_run.png\"/></p>\n<p>Assuming the job completes successfully, you should now be able to select data out of your table in Cassandra:</p>\n<div class=\"highlight\"><pre>cqlsh:tutorial&gt; select * from food_to_user_index ;\n food    | user\n---------+---------\n  salmon |      Al\n   steak |    Luke\n chicken |   Chris\n  brains | Duy Hai\n  brains | Patrick\n   bacon |     Jon\n   bacon | Rebecca\n(7 rows)\ncqlsh:tutorial&gt; select * from food_to_user_index where food = 'bacon';\n food  | user\n-------+---------\n bacon |     Jon\n bacon | Rebecca\n(2 rows)\n</pre></div>\n<p>Guess what?  You just ran your first Spark job talking to Cassandra.  The good news is, you can do a ton more than just simple <code>map()</code> calls.  I'll be posting followups on using Spark streaming and machine learning in the near future.</p>\n<p>The code in all it's glory can be found in my <a href=\"https://github.com/rustyrazorblade/spark-data-migration/blob/master/src/main/scala/DataMigration.scala\">GitHub</a>.  Please pull and try it for yourself.  If you don't want to use my IDE of choice, the build tools on the command line are perfectly fine.  Simply do the following:</p>\n<div class=\"highlight\"><pre>sbt compile\nsbt run\n</pre></div>\n<p>Well, that about does it.  I hope you've found this useful.  If you're interested in further learning materials:</p>\n<ul><li><a href=\"https://www.youtube.com/watch?v=W45Ysb9b6oE\">Cassandra Crash Course</a></li>\n<li><a href=\"https://spark.apache.org/docs/latest/\">Apache Spark docs</a></li>\n<li><a href=\"http://planetcassandra.org/try-cassandra/\">10 minute walkthough</a></li>\n</ul>",
        "created_at": "2017-09-14T13:01:25+0000",
        "updated_at": "2017-12-27T14:22:06+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 5,
        "domain_name": "rustyrazorblade.com",
        "preview_picture": "http://rustyrazorblade.com/images/cat-riding-a-fire-breathing-unicorn.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/5172"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 5171,
        "uid": null,
        "title": "logicalexpert/cassandra-etl",
        "url": "https://github.com/logicalexpert/cassandra-etl",
        "content": "<h3>\n      <svg aria-hidden=\"true\" class=\"octicon octicon-book\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"/>\n      README.md\n    </h3>\n      <article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>Here is the good news for Cassandra and Scriptella fans. I found there are good and Open Source ETL tools written in Java, here is the list of among top listed tools (in alphabetical order) 1) <a title=\"Apache Camel\" href=\"http://camel.apache.org/index.html\" rel=\"nofollow\">Apache Camel</a> 2) <a title=\"Clover ETL\" href=\"http://sourceforge.net/projects/cloveretl/\" rel=\"nofollow\">CloverETL</a>,  3) <a title=\"Pentaho Kettle\" href=\"http://community.pentaho.com/projects/data-integration/\" rel=\"nofollow\">Pentaho Kettle</a>, and 4) <a title=\"Scriptella\" href=\"http://scriptella.javaforge.com/\" rel=\"nofollow\">Scriptella</a>. After understanding basics of each tool, I felt comfortable with Scriptella code, architecture and simplicity, It is allowing to add new Driver and easy to integrate in any existing project code base.</p>\n<p>All you need to do is, checkout code from github. and keep playing around with configuration file etl.xml. As you are used to configure scriptella for other DBs, same approach has to be followed for Cassandra also.</p>\n<p>For implementing this below libraries are used.</p>\n<ol><li>Scriptella</li>\n\t<li>Datastax cassandra driver</li>\n</ol>\nbuild.gradle takes care of all these dependencies.\n<p>You can checkout code from this URL : <a href=\"https://github.com/logicalexpert/cassandra-etl\">https://github.com/logicalexpert/cassandra-etl</a></p>\n<p>Please visit website <a title=\"Website URL\" href=\"http://www.logicalexpert.com/\" rel=\"nofollow\">LogicalExpert.com</a>for such useful tools and you can request for new solutions also at website.</p>\n<p>Leave your comment if you find any difficulty or need for improvement.</p>\n</article>",
        "created_at": "2017-09-14T13:04:23+0000",
        "updated_at": "2017-12-15T08:52:49+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "github.com",
        "preview_picture": "https://avatars3.githubusercontent.com/u/10308322?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/5171"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 5168,
        "uid": null,
        "title": "NYC* 2013 — \"Using Cassandra for DVR Scheduling at Comcast\"",
        "url": "https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast",
        "content": "<head prefix=\"og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# 2490221586: http://ogp.me/ns/fb/2490221586#\"><meta charset=\"utf-8\"/><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, maximum-scale=1, user-scalable=no\"/><meta name=\"include_mode\" content=\"async\"/><!-- SL:start:notranslate --><title>NYC* 2013 — \"Using Cassandra for DVR Scheduling at Comcast\"</title><meta name=\"description\" content=\"Comcast is developing a highly scalable cloud DVR scheduling system on top of Cassandra. The system is responsible for managing all DVR data and scheduling log…\"/><!-- SL:end:notranslate --><meta name=\"robots\" content=\"index\"/><meta id=\"globalTrackingUrl\" content=\"https://www.linkedin.com/li/track\"/><!-- SL:start:notranslate --><meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"/><meta http-equiv=\"x-dns-prefetch-control\" content=\"on\"/><meta name=\"thumbnail\" content=\"https://cdn.slidesharecdn.com/ss_thumbnails/johnmccann-130409191105-phpapp01-thumbnail.jpg?cb=1365534774\"/><!-- SL:end:notranslate --><meta content=\"{&quot;lynda.domain&quot;:&quot;lynda&quot;,&quot;lynda.premium_video&quot;:&quot;PREMIUM VIDEO&quot;,&quot;lynda.tld&quot;:&quot;.com&quot;,&quot;right_rail.recommended&quot;:&quot;Recommended&quot;,&quot;views.other&quot;:&quot;views&quot;,&quot;share&quot;:&quot;Share&quot;,&quot;select&quot;:&quot;Select&quot;,&quot;selected&quot;:&quot;Selected&quot;,&quot;clipping.select_clipboard_modal.select_another_clipboard&quot;:&quot;Select another clipboard&quot;,&quot;clipping.select_clipboard_modal.select_new_clipboard&quot;:&quot;Select a new clipboard&quot;,&quot;clipping.toast.change_clipboard&quot;:&quot;Change clipboard&quot;,&quot;clipping.toast.share_clip&quot;:&quot;Share clip&quot;,&quot;li_connect.slideshare_added&quot;:&quot;Your SlideShare was successfully added to your LinkedIn profile.&quot;,&quot;li_connect.login.no_match&quot;:&quot;Login does not match, please try again.&quot;,&quot;ajax_signup.login.download&quot;:&quot;Login to SlideShare to download\\u2026&quot;,&quot;ajax_signup.login.addcontact&quot;:&quot;Login to SlideShare to follow\\u2026&quot;,&quot;ajax_signup.login.favorite&quot;:&quot;Login to SlideShare to like\\u2026&quot;,&quot;ajax_signup.login.comments&quot;:&quot;Login to SlideShare to post a comment\\u2026&quot;,&quot;ajax_signup.login.AddToCommunity&quot;:&quot;Login to SlideShare to add this document to a group/event\\u2026&quot;,&quot;ajax_signup.login.follow&quot;:&quot;Login to SlideShare to follow this user\\u2026&quot;,&quot;ajax_signup.login.business&quot;:&quot;Login to SlideShare to continue\\u2026&quot;,&quot;ajax_signup.login.upload&quot;:&quot;Login to SlideShare to start uploading\\u2026&quot;,&quot;ajax_signup.login.contest&quot;:&quot;Login to SlideShare to vote\\u2026&quot;,&quot;ajax_signup.login.rsvp&quot;:&quot;Login to SlideShare to join this meeting\\u2026&quot;,&quot;ajax_signup.login.user&quot;:&quot;Login to SlideShare to register the username\\u2026&quot;,&quot;ajax_signup.login.zipcast&quot;:&quot;Login to SlideShare to schedule a meeting\\u2026&quot;,&quot;ajax_signup.login.create&quot;:&quot;Login to SlideShare to start creating\\u2026&quot;,&quot;ajax_signup.login.clip&quot;:&quot;Login to SlideShare. Don\\u2019t lose your clips!&quot;,&quot;ajax_signup.signup.download&quot;:&quot;Signup for SlideShare to download\\u2026&quot;,&quot;ajax_signup.signup.addcontact&quot;:&quot;Signup for SlideShare to follow\\u2026&quot;,&quot;ajax_signup.signup.favorite&quot;:&quot;Signup for SlideShare to like\\u2026&quot;,&quot;ajax_signup.signup.comments&quot;:&quot;Signup for SlideShare to post a comment\\u2026&quot;,&quot;ajax_signup.signup.AddToCommunity&quot;:&quot;Signup for SlideShare to add this document to a group/event\\u2026&quot;,&quot;ajax_signup.signup.follow&quot;:&quot;Signup for SlideShare to follow this user\\u2026&quot;,&quot;ajax_signup.signup.business&quot;:&quot;Signup for SlideShare to continue\\u2026&quot;,&quot;ajax_signup.signup.upload&quot;:&quot;Signup for SlideShare to start uploading\\u2026&quot;,&quot;ajax_signup.signup.contest&quot;:&quot;Signup for SlideShare to vote\\u2026&quot;,&quot;ajax_signup.signup.rsvp&quot;:&quot;Signup for SlideShare to join this meeting\\u2026&quot;,&quot;ajax_signup.signup.user&quot;:&quot;Signup for SlideShare to register the username\\u2026&quot;,&quot;ajax_signup.signup.zipcast&quot;:&quot;Signup for SlideShare to schedule a meeting\\u2026&quot;,&quot;ajax_signup.signup.create&quot;:&quot;Signup for SlideShare to start creating\\u2026&quot;,&quot;ajax_signup.signup.clip&quot;:&quot;Signup for SlideShare. Don\\u2019t lose your clips!&quot;,&quot;ajax_signup.connect&quot;:&quot;connect&quot;,&quot;ajax_signup.signup_for_ss&quot;:&quot;Signup for SlideShare&quot;,&quot;ajax_signup.login_to_ss&quot;:&quot;Login to SlideShare&quot;,&quot;clipping.filmstrip.message&quot;:&quot;Top clipped slide&quot;,&quot;slideview.comments_loggedin.delete_comment.title&quot;:&quot;Are you sure you want to delete this comment?&quot;,&quot;slideview.comments_loggedin.spam_comment.title&quot;:&quot;Are you sure you want to mark this comment as spam?&quot;,&quot;slideview.comments_loggedin.block_user.title&quot;:&quot;Are you sure you want to block this user?&quot;,&quot;slideview.comments_loggedin.block_user.success&quot;:&quot;The user is blocked!&quot;,&quot;slideview.comments_loggedin.block_user.failure&quot;:&quot;Oops! Something went wrong. Please try again after sometime.&quot;,&quot;slideview.comments_loggedin.organization&quot;:&quot;at {:organization}&quot;,&quot;slideview.comments_loggedin.timestamp&quot;:&quot;less than a second ago&quot;,&quot;slideview.comments_loggedin.commenting.failure&quot;:&quot;Hey something went wrong :-(\\nCould you try again?&quot;,&quot;slideshow.foundation_slideview.likes_count&quot;:&quot;{:count,number} {:count,choice,singular#Like|plural#Likes}&quot;,&quot;action-bar.like.why_not_share&quot;:&quot;Like this? Why not share?&quot;,&quot;slideshow.foundation_slideview.like&quot;:&quot;Like&quot;,&quot;action-bar.unlike.text&quot;:&quot;Unlike&quot;}\" name=\"ss-i18n-translations\"/><meta content=\"{&quot;ssplayer.event_manager.playershare.share_slideshare&quot;:&quot;Share SlideShare&quot;,&quot;profile.edit_marketing_settings.saved&quot;:&quot;Saved&quot;,&quot;slideshow.foundation_slideview.loading&quot;:&quot;Loading\\u2026&quot;,&quot;clip.slideview.share_message&quot;:&quot;Share Slide {0,number} from {1,text}&quot;,&quot;clip.slideview.success_toast.message&quot;:&quot;Slide {0,number} clipped to: \\u003Cbr /\\u003E {1,anchor,text#{2,text}}&quot;,&quot;clip.clips.error_clipping&quot;:&quot;Error while clipping.&quot;,&quot;clip.clips.error_fetching_clipboards&quot;:&quot;Error fetching clipboards.&quot;,&quot;clip.clips.error_creating_clipboard&quot;:&quot;Error creating a clipboard.&quot;,&quot;clip.clips.error_adding_to_clipboard&quot;:&quot;Error adding to clipboard.&quot;}\" name=\"ss-i18n-translations\"/><!--[if IE 9]><link href=\"https://public.slidesharecdn.com/ss_foundation/stylesheets/ie9_nav_bar_fix.css?8fb8af5274\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\" /><![endif]--><meta content=\"index\" name=\"robots\"/><meta content=\"https://public.slidesharecdn.com/images/artdeco/icons.svg?43e81fd2ef\" name=\"ss-svg-icons\"/><meta name=\"apple-itunes-app\" content=\"app-id=917418728, affiliate-data=ct=smart_banner&amp;pt=10746, app-argument=slideshare-app://ss/18505789\"/><!-- fb open graph meta tags --><!-- SL:start:notranslate --><!-- SL:end:notranslate --><!-- SL:start:notranslate --><meta name=\"twitter:card\" value=\"player\"/><meta name=\"twitter:site\" value=\"@slideshare\"/><meta name=\"twitter:player:width\" value=\"342\"/><meta name=\"twitter:player:height\" value=\"291\"/><meta name=\"twitter:app:name:googleplay\" content=\"SlideShare Android\"/><meta name=\"twitter:app:id:googleplay\" content=\"net.slideshare.mobile\"/><meta name=\"twitter:app:url:googleplay\" content=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\"/><meta name=\"twitter:app:name:iphone\" content=\"SlideShare iOS\"/><meta name=\"twitter:app:id:iphone\" content=\"917418728\"/><meta name=\"twitter:app:url:iphone\" content=\"slideshare-app://ss/18505789\"/><meta name=\"twitter:app:name:ipad\" content=\"SlideShare iOS\"/><meta name=\"twitter:app:id:ipad\" content=\"917418728\"/><meta name=\"twitter:app:url:ipad\" content=\"slideshare-app://ss/18505789\"/><meta property=\"al:android:url\" content=\"slideshare-app://ss/18505789\"/><meta property=\"al:android:app_name\" content=\"SlideShare Android\"/><meta property=\"al:android:package\" content=\"net.slideshare.mobile\"/><meta property=\"al:ios:url\" content=\"slideshare-app://ss/18505789\"/><meta property=\"al:ios:app_store_id\" content=\"917418728\"/><meta property=\"al:ios:app_name\" content=\"SlideShare iOS\"/><!-- SL:end:notranslate --></head><body id=\"pagekey-slideshare_desktop_slideview_loggedout\" class=\"readabilityBody\" readability=\"50\">\n      <!-- TOS update banner -->\n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\" readability=\"0\">\n        <p>\n          <a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            <label id=\"home\">SlideShare</label>\n          </a>\n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\"/>\n            <label id=\"explore\">Explore</label>\n          </a>\n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\"/>\n              <label id=\"you\">You</label>\n            </a>\n        </p>\n        \n      </div>\n    <div class=\"wrapper\">\n        \n        \n      \n      \n<div id=\"slideview-container\" class=\"\">\n  \n  <div class=\"row\">\n    <div id=\"main-panel\" class=\"small-12 large-8 columns\">\n      <div class=\"sectionElements\">\n          \n        <div class=\"playerWrapper\">\n            <div readability=\"5\">\n              <!-- For slideview page , combined js for player is now combined with slideview javascripts for logged out users-->\n<div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\" readability=\"6\">\n  \n    NYC* 2013 — \"Using Cassandra for DVR Scheduling at Comcast\"\n  \n  <div class=\"stage valign-first-slide\">\n    \n    \n    <div class=\"slide_container\">\n            <section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-1-638.jpg?cb=1365534774\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-1-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-1-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-1-1024.jpg?cb=1365534774\" alt=\"Using Cassandra for DVR Scheduling at ComcastVERSION   1.0DATE      3/20/213AUTHOR    John McCann | @dangermccann         ...\"/></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-2-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-2-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-2-1024.jpg?cb=1365534774\" alt=\"The X1 Platform                     XFINITY On Demand                     Choose from thousands of movies and             ...\"/></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-3-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-3-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-3-1024.jpg?cb=1365534774\" alt=\"The X1 Platform                     The ultimate scoreboard                     Track multiple games at once and check    ...\"/></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-4-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-4-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-4-1024.jpg?cb=1365534774\" alt=\"The X1 Platform• Next generation of the XFINITY TV experience• New line of set-top boxes and technologies that lets XFINIT...\"/></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-5-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-5-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-5-1024.jpg?cb=1365534774\" alt=\"What Comcast Stores in Cassandra•   Customer viewing history•   Resume points for DVR and XFINITY On Demand content•   Dev...\"/></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-6-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-6-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-6-1024.jpg?cb=1365534774\" alt=\"Why Comcast Uses Cassandra• Multi-data center (active / active)• Performance of write-heavy use cases   – DVR status updat...\"/></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-7-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-7-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-7-1024.jpg?cb=1365534774\" alt=\"Redesigned DVR             Using Cassandra for DVR Scheduling at Comcast   4/9/2013   7 \"/></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-8-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-8-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-8-1024.jpg?cb=1365534774\" alt=\"DVR System ArchitectureEast Data Center                                            West Data Center API Server        Work...\"/></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-9-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-9-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-9-1024.jpg?cb=1365534774\" alt=\"Cassandra Configuration•   4-node rings in 2 data centers•   NetworkTopologyStrategy replication strategy•   Replication f...\"/></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-10-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-10-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-10-1024.jpg?cb=1365534774\" alt=\"Comcast Message Bushttps://github.com/Comcast/cmb                                                                        C...\"/></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-11-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-11-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-11-1024.jpg?cb=1365534774\" alt=\"Use Case ExplorationScheduling a Series Recording                          Using Cassandra for DVR Scheduling at Comcast  ...\"/></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-12-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-12-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-12-1024.jpg?cb=1365534774\" alt=\"User Interaction         Set-top boxUser         UI Server                       Using Cassandra for DVR Scheduling at Com...\"/></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-13-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-13-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-13-1024.jpg?cb=1365534774\" alt=\"User Interaction         Set-top box                                                    API Server              Worker    ...\"/></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-14-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-14-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-14-1024.jpg?cb=1365534774\" alt=\"Recording Schedule Update                                            API Server              Worker                       ...\"/></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-15-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-15-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-15-1024.jpg?cb=1365534774\" alt=\"Recording Schedule Update        Set-top box                                                   API Server              Wor...\"/></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-16-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-16-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-16-1024.jpg?cb=1365534774\" alt=\"Recording Schedule Update        Set-top box                                                   API Server              Wor...\"/></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-17-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-17-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-17-1024.jpg?cb=1365534774\" alt=\"UI Update                                                 API Server              Worker                                  ...\"/></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-18-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-18-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-18-1024.jpg?cb=1365534774\" alt=\"UI Update                                                 API Server              Worker                                  ...\"/></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-19-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-19-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-19-1024.jpg?cb=1365534774\" alt=\"System Diagram       Set-top box                                                  API Server              Worker          ...\"/></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-20-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-20-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-20-1024.jpg?cb=1365534774\" alt=\"Data Model Analysis1. Recording Instructions2. Recording Schedule3. Completed Recordings                            Using ...\"/></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-21-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-21-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-21-1024.jpg?cb=1365534774\" alt=\"Recording Instructions Schema• Stores the instructions used to produce the recording schedule  in the order that the instr...\"/></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-22-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-22-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-22-1024.jpg?cb=1365534774\" alt=\"Recording Schedule Schema• Stores the data for the most recent recording schedule that  has been delivered to the Recorder...\"/></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-23-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-23-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-23-1024.jpg?cb=1365534774\" alt=\"Completed Recordings Schema• Stores the data for all of the completed recordings on the STB.Recording AccountID/DeviceID  ...\"/></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-24-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-24-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-24-1024.jpg?cb=1365534774\" alt=\"Hardware Configuration•   HP DL360 G8•   64GB RAM•   2 x 600 GB SATA Hard Drive•   RAID 1 for OS Partition•   LSI controll...\"/></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-25-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-25-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-25-1024.jpg?cb=1365534774\" alt=\"Solid State DrivesSamsung 840 512GB•   50% cost increase per node•   400% capacity increase per node•   Smaller rings, bet...\"/></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-26-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-26-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-26-1024.jpg?cb=1365534774\" alt=\"Lessons Learned• If youre using Hector, pay close attention to  CassandraHostConfigurator.maxActive.• Don’t enable the row...\"/></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast\" data-small=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/85/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-27-320.jpg?cb=1365534774\" data-normal=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-27-638.jpg?cb=1365534774\" data-full=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-27-1024.jpg?cb=1365534774\" alt=\"Q&amp;AThanks for your attention!http://xfinity.comcast.net/x1John McCann@dangermccann                                Using Ca...\"/></section><div class=\"j-next-container next-container\">\n          <div class=\"content-container\">\n            <div class=\"next-slideshow-wrapper\">\n              <div class=\"j-next-slideshow next-slideshow\">\n                <p>\n                  Upcoming SlideShare\n                </p>\n                \n              </div>\n              <p>Loading in …5</p>\n              <p>×</p>\n            </div>\n          </div>\n        </div>\n    </div>\n  </div> <!-- end stage -->\n  \n  \n  \n</div>\n            </div>\n        </div>\n        \n      </div>\n      <div class=\"slideshow-info-container\" itemscope=\"\" itemtype=\"https://schema.org/MediaObject\" readability=\"7\">\n        \n        <div class=\"slideshow-tabs-container show-for-medium-up\">\n          <ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    0 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\"/>\n                \n                  3 Likes\n                \n              </a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\"/>\n                Statistics\n              </a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\"/>\n                Notes\n              </a>\n            </li>\n          </ul><div class=\"tabs-content\">\n              \n            <div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\">\n              <ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"\"><li itemtype=\"http://schema.org/Person\" itemscope=\"\">\n                      <div class=\"row\">\n                        \n                        <div class=\"small-11 columns\">\n                          <a class=\"favoriter notranslate\" title=\"maekawa1\" rel=\"nofollow\" href=\"https://www.slideshare.net/maekawa1?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            David Maekawa\n                            \n                              \n                                , \n                                Experienced Technical Product Manager for Content and Technology Software Companies\n                              \n                              \n                                 at \n                                MobiTV\n                              \n                            \n                            \n                            </a>\n                        </div>\n                      </div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"\">\n                      <div class=\"row\">\n                        \n                        <div class=\"small-11 columns\">\n                          <a class=\"favoriter notranslate\" title=\"cripeoutloud\" rel=\"nofollow\" href=\"https://www.slideshare.net/cripeoutloud?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Joshua Fox\n                            \n                              \n                                , \n                                Director of Product Management - IP Platform Strategy &amp; Planning at AT&amp;T\n                              \n                              \n                                 at \n                                AT&amp;T\n                              \n                            \n                            \n                            </a>\n                        </div>\n                      </div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"\">\n                      <div class=\"row\">\n                        \n                        <div class=\"small-11 columns\">\n                          <a class=\"favoriter notranslate\" title=\"morganymendes\" rel=\"nofollow\" href=\"https://www.slideshare.net/morganymendes?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Morgany Mendes\n                            \n                              \n                                , \n                                Principal Architect at America Movil Brasil (NET/Claro/Embratel)\n                              \n                              \n                                 at \n                                América Móvil\n                              \n                            \n                            \n                            </a>\n                        </div>\n                      </div>\n                    </li>\n              </ul></div>\n            <div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\">\n                <p>No Downloads</p>\n            </div>\n            \n            <div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\">\n              <p>No notes for slide</p>\n                  <li class=\"slide-note\"/>\n                  <li class=\"slide-note\"/>\n                  <li class=\"slide-note\"/>\n                  <li class=\"slide-note\"/>\n                  <li class=\"slide-note\"/>\n                  <li class=\"slide-note\"/>\n                  <li class=\"slide-note\"/>\n                  <li class=\"slide-note\"/>\n                  <li class=\"slide-note\"/>\n                  <li class=\"slide-note\"/>\n                  <li class=\"slide-note\"/>\n                  <li class=\"slide-note\"/>\n                  <li class=\"slide-note\"/>\n                  <li class=\"slide-note\"/>\n                  <li class=\"slide-note\"/>\n                  <li class=\"slide-note\"/>\n                  <li class=\"slide-note\"/>\n                  <li class=\"slide-note\"/>\n                  <li class=\"slide-note\"/>\n                  <li class=\"slide-note\"/>\n                  <li class=\"slide-note\"/>\n                  <li class=\"slide-note\"/>\n                  <li class=\"slide-note\"/>\n                  <li class=\"slide-note\">Split the backplane2x4 planesInternal controller drives HDD @ Raid 1LSI controller drives SSD @ Raid 0 </li>\n                  <li class=\"slide-note\"/>\n                  <li class=\"slide-note\">Pending writes prevent ability to delete rowNormal compaction will not clean up rowMajor compaction can not clean up if there a pending writes to the row</li>\n            </div>\n          </div>\n        </div>\n            <div class=\"notranslate transcript add-padding-right j-transcript\">\n              \n              <ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    Using Cassandra for DVR Scheduling at ComcastVERSION   1.0DATE      3/20/213AUTHOR    John McCann | @dangermccann                                        Using Cassandra for DVR Scheduling at Comcast   4/9/2013   1 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-2-638.jpg?cb=1365534774\" title=\"The X1 Platform                     XFINITY On Demand      ...\" target=\"_blank\">\n        2.\n      </a>\n    The X1 Platform                     XFINITY On Demand                     Choose from thousands of movies and                     shows.    More than a back button    See the last nine shows and channels    youve watched with a single tap.                       Using Cassandra for DVR Scheduling at Comcast   4/9/2013   2 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-3-638.jpg?cb=1365534774\" title=\"The X1 Platform                     The ultimate scoreboard...\" target=\"_blank\">\n        3.\n      </a>\n    The X1 Platform                     The ultimate scoreboard                     Track multiple games at once and check                     the latest scores, standings, and schedules.     Apps right on your TV     Now your TV is for more than just     watching.                        Using Cassandra for DVR Scheduling at Comcast   4/9/2013   3 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-4-638.jpg?cb=1365534774\" title=\"The X1 Platform• Next generation of the XFINITY TV experien...\" target=\"_blank\">\n        4.\n      </a>\n    The X1 Platform• Next generation of the XFINITY TV experience• New line of set-top boxes and technologies that lets XFINITY  TV run in the cloud• Premium content viewing experience, with access to XFINITY  On Demand, DVR and Pay-Per-View video• De-coupled, service-based architecture where services are  developed and deployed independently                       Using Cassandra for DVR Scheduling at Comcast   4/9/2013   4 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-5-638.jpg?cb=1365534774\" title=\"What Comcast Stores in Cassandra•   Customer viewing histor...\" target=\"_blank\">\n        5.\n      </a>\n    What Comcast Stores in Cassandra•   Customer viewing history•   Resume points for DVR and XFINITY On Demand content•   Device and account preferences•   Pay-Per-View purchases•   DVR recordings and scheduling instructions                      Using Cassandra for DVR Scheduling at Comcast   4/9/2013   5 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-6-638.jpg?cb=1365534774\" title=\"Why Comcast Uses Cassandra• Multi-data center (active / act...\" target=\"_blank\">\n        6.\n      </a>\n    Why Comcast Uses Cassandra• Multi-data center (active / active)• Performance of write-heavy use cases   – DVR status updates   – Remembering resume points during content playback• Developers (not DBAs) model and manage the data• Open source encourages in-house expertise                          Using Cassandra for DVR Scheduling at Comcast   4/9/2013   6 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-7-638.jpg?cb=1365534774\" title=\"Redesigned DVR             Using Cassandra for DVR Scheduli...\" target=\"_blank\">\n        7.\n      </a>\n    Redesigned DVR             Using Cassandra for DVR Scheduling at Comcast   4/9/2013   7 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-8-638.jpg?cb=1365534774\" title=\"DVR System ArchitectureEast Data Center                    ...\" target=\"_blank\">\n        8.\n      </a>\n    DVR System ArchitectureEast Data Center                                            West Data Center API Server        Worker                                      API Server          Worker                              Worker Queue                                                                                            Worker Queue API Server        Worker                                      API Server          Worker API Server        Worker                                      API Server          Worker          Cassandra                                                         Cassandra                            Using Cassandra for DVR Scheduling at Comcast      4/9/2013                    8 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-9-638.jpg?cb=1365534774\" title=\"Cassandra Configuration•   4-node rings in 2 data centers• ...\" target=\"_blank\">\n        9.\n      </a>\n    Cassandra Configuration•   4-node rings in 2 data centers•   NetworkTopologyStrategy replication strategy•   Replication factor is 3 (per data center)•   LOCAL_QUORUM consistency level for most operations                      Using Cassandra for DVR Scheduling at Comcast   4/9/2013   9 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-10-638.jpg?cb=1365534774\" title=\"Comcast Message Bushttps://github.com/Comcast/cmb          ...\" target=\"_blank\">\n        10.\n      </a>\n    Comcast Message Bushttps://github.com/Comcast/cmb                                                                        CQS Endpoint•   Horizontally scalable queuing and    notification service•   Compatible with Amazon SQS and SNS                        Cassandra                Redis•   Cassandra used to persist messages•   Redis used as caching layer•   Open source (Apache license)                                                                        CNS Endpoint                                                                         CNS Worker                             Using Cassandra for DVR Scheduling at Comcast      4/9/2013       10 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-11-638.jpg?cb=1365534774\" title=\"Use Case ExplorationScheduling a Series Recording          ...\" target=\"_blank\">\n        11.\n      </a>\n    Use Case ExplorationScheduling a Series Recording                          Using Cassandra for DVR Scheduling at Comcast   4/9/2013   11 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-12-638.jpg?cb=1365534774\" title=\"User Interaction         Set-top boxUser         UI Server ...\" target=\"_blank\">\n        12.\n      </a>\n    User Interaction         Set-top boxUser         UI Server                       Using Cassandra for DVR Scheduling at Comcast   4/9/2013   12 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-13-638.jpg?cb=1365534774\" title=\"User Interaction         Set-top box                       ...\" target=\"_blank\">\n        13.\n      </a>\n    User Interaction         Set-top box                                                    API Server              Worker                                                                                       Worker QueueUser                                                    API Server              Worker         UI Server                                  API Server              Worker                                                                       Cassandra        Memcached                       Using Cassandra for DVR Scheduling at Comcast        4/9/2013                  13 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-14-638.jpg?cb=1365534774\" title=\"Recording Schedule Update                                  ...\" target=\"_blank\">\n        14.\n      </a>\n    Recording Schedule Update                                            API Server              Worker                                                                               Worker Queue                                            API Server              Worker                                            API Server              Worker                                                               Cassandra               Using Cassandra for DVR Scheduling at Comcast        4/9/2013                  14 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-15-638.jpg?cb=1365534774\" title=\"Recording Schedule Update        Set-top box               ...\" target=\"_blank\">\n        15.\n      </a>\n    Recording Schedule Update        Set-top box                                                   API Server              Worker                                                                                      Worker Queue                                                   API Server              Worker                                                   API Server              Worker                                                                      Cassandra                      Using Cassandra for DVR Scheduling at Comcast        4/9/2013                  15 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-16-638.jpg?cb=1365534774\" title=\"Recording Schedule Update        Set-top box               ...\" target=\"_blank\">\n        16.\n      </a>\n    Recording Schedule Update        Set-top box                                                   API Server              Worker                                                                                      Worker Queue                                                   API Server              Worker                                                   API Server              Worker                                                                      Cassandra                      Using Cassandra for DVR Scheduling at Comcast        4/9/2013                  16 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-17-638.jpg?cb=1365534774\" title=\"UI Update                                                 A...\" target=\"_blank\">\n        17.\n      </a>\n    UI Update                                                 API Server              Worker                                                                                    Worker Queue                                                 API Server              Worker        UI Server                                API Server              Worker                                                                    Cassandra                    Using Cassandra for DVR Scheduling at Comcast        4/9/2013                  17 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-18-638.jpg?cb=1365534774\" title=\"UI Update                                                 A...\" target=\"_blank\">\n        18.\n      </a>\n    UI Update                                                 API Server              Worker                                                                                    Worker Queue                                                 API Server              Worker        UI Server                                API Server              Worker                                                                    Cassandra       Memcached                    Using Cassandra for DVR Scheduling at Comcast        4/9/2013                  18 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-19-638.jpg?cb=1365534774\" title=\"System Diagram       Set-top box                           ...\" target=\"_blank\">\n        19.\n      </a>\n    System Diagram       Set-top box                                                  API Server              Worker                                                                                     Worker QueueUser                                                  API Server              Worker        UI Server                                 API Server              Worker                                                                     Cassandra       Memcached                     Using Cassandra for DVR Scheduling at Comcast        4/9/2013                  19 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-20-638.jpg?cb=1365534774\" title=\"Data Model Analysis1. Recording Instructions2. Recording Sc...\" target=\"_blank\">\n        20.\n      </a>\n    Data Model Analysis1. Recording Instructions2. Recording Schedule3. Completed Recordings                            Using Cassandra for DVR Scheduling at Comcast   4/9/2013   20 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-21-638.jpg?cb=1365534774\" title=\"Recording Instructions Schema• Stores the instructions used...\" target=\"_blank\">\n        21.\n      </a>\n    Recording Instructions Schema• Stores the instructions used to produce the recording schedule  in the order that the instructions were generated.ScheduleInstruction AccountID/DeviceID    TimeUUID : [instruction]             TimeUUID : [instruction]         …   Composite row key      Instructions sorted by creation time using TimeUUID as                          the column name. The column value contains the instruction data.                                 Using Cassandra for DVR Scheduling at Comcast    4/9/2013       21 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-22-638.jpg?cb=1365534774\" title=\"Recording Schedule Schema• Stores the data for the most rec...\" target=\"_blank\">\n        22.\n      </a>\n    Recording Schedule Schema• Stores the data for the most recent recording schedule that  has been delivered to the Recorder on the STB.DvrSchedule AccountID/DeviceID                      Properties of each recording stored as separate columns using                      composite column names: recording_ID/[property_name]                             Using Cassandra for DVR Scheduling at Comcast   4/9/2013   22 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-23-638.jpg?cb=1365534774\" title=\"Completed Recordings Schema• Stores the data for all of the...\" target=\"_blank\">\n        23.\n      </a>\n    Completed Recordings Schema• Stores the data for all of the completed recordings on the STB.Recording AccountID/DeviceID                      Properties of each recording stored as separate columns using                      composite column names: recording_ID/[property_name]                             Using Cassandra for DVR Scheduling at Comcast   4/9/2013   23 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-24-638.jpg?cb=1365534774\" title=\"Hardware Configuration•   HP DL360 G8•   64GB RAM•   2 x 60...\" target=\"_blank\">\n        24.\n      </a>\n    Hardware Configuration•   HP DL360 G8•   64GB RAM•   2 x 600 GB SATA Hard Drive•   RAID 1 for OS Partition•   LSI controller•   2 x Samsung 840 512GB SSD (room for up to 4 SSDs)                       Using Cassandra for DVR Scheduling at Comcast   4/9/2013   24 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-25-638.jpg?cb=1365534774\" title=\"Solid State DrivesSamsung 840 512GB•   50% cost increase pe...\" target=\"_blank\">\n        25.\n      </a>\n    Solid State DrivesSamsung 840 512GB•   50% cost increase per node•   400% capacity increase per node•   Smaller rings, better read performance•   Read performance consistency (flat 99th percentiles for reads)                         Using Cassandra for DVR Scheduling at Comcast   4/9/2013   25 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-26-638.jpg?cb=1365534774\" title=\"Lessons Learned• If youre using Hector, pay close attention...\" target=\"_blank\">\n        26.\n      </a>\n    Lessons Learned• If youre using Hector, pay close attention to  CassandraHostConfigurator.maxActive.• Don’t enable the row cache if you need to perform slice  queries.• Don’t delete a row if you plan to write columns to the same row  later (better in 1.2).• Don’t run Cassandra on shared storage if you can avoid it.                       Using Cassandra for DVR Scheduling at Comcast   4/9/2013   26 \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/nyc-2013-using-cassandra-for-dvr-scheduling-at-comcast-27-638.jpg?cb=1365534774\" title=\"Q&amp;AThanks for your attention!http://xfinity.comcast.net/x1J...\" target=\"_blank\">\n        27.\n      </a>\n    Q&amp;AThanks for your attention!http://xfinity.comcast.net/x1John McCann@dangermccann                                Using Cassandra for DVR Scheduling at Comcast   4/9/2013   27 \n  </li>\n              </ol></div>\n      </div>\n    </div>\n    <aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\"><dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\">\n    <ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"18505789\" title=\"Teacher Tips\" href=\"https://www.linkedin.com/learning/teacher-tips?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Teacher Tips\" data-ga-action=\"click\" readability=\"1\">\n    <div class=\"lynda-thumbnail\">\n      <img class=\"j-thumbnail j-lazy-thumb\" alt=\"Teacher Tips\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=mN807ffzPmE7p%2FNpTyJseUnrM8Q%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-gXySu_NCfYXPofcHaZLSiol8eeiUIlwE0feuvQjDoEo69LcLmY4Yx3A\"/>\n    </div>\n    <div class=\"lynda-content\" readability=\"37\">\n      <p>\n        Teacher Tips\n      </p>\n      <p>\n        Online Course - LinkedIn Learning\n      </p>\n    </div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"18505789\" title=\"Learning the Basics of Branding\" href=\"https://www.linkedin.com/learning/learning-the-basics-of-branding?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Learning the Basics of Branding\" data-ga-action=\"click\" readability=\"2\">\n    <div class=\"lynda-thumbnail\">\n      <img class=\"j-thumbnail j-lazy-thumb\" alt=\"Learning the Basics of Branding\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=8Kbe7Ed%2FeTQ%2BSU2%2BqS5hcINsN9Y%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-gWySj_9KfYXfocMLYZLSiol8QcS4BmQw3euaoQzbjEY69LcLmY4Yx3A\"/>\n    </div>\n    <div class=\"lynda-content\" readability=\"39\">\n      <p>\n        Learning the Basics of Branding\n      </p>\n      <p>\n        Online Course - LinkedIn Learning\n      </p>\n    </div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"18505789\" title=\"Creative Inspirations: Duarte Design, Presentation Design Studio\" href=\"https://www.linkedin.com/learning/creative-inspirations-duarte-design-presentation-design-studio?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Creative Inspirations: Duarte Design, Presentation Design Studio\" data-ga-action=\"click\" readability=\"3\">\n    <div class=\"lynda-thumbnail\">\n      <img class=\"j-thumbnail j-lazy-thumb\" alt=\"Creative Inspirations: Duarte Design, Presentation Design Studio\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=gN0XHJkdhuJ3Fk75H6mdK0VF9ag%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-kWiGr-MqFYXPoe9ref_qougdWLw\"/>\n    </div>\n    <div class=\"lynda-content\" readability=\"41\">\n      <p>\n        Creative Inspirations: Duarte Design, Presentation Design Studio\n      </p>\n      <p>\n        Online Course - LinkedIn Learning\n      </p>\n    </div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"7523306\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Cassandra &amp; puppet, scaling data at $15 per month\" href=\"https://www.slideshare.net/daveconnors/cassandra-puppet-scaling-data-at-15-per-month\" readability=\"-23\">\n    \n    <div class=\"related-content\" readability=\"14\">\n      <p>\n        Cassandra &amp; puppet, scaling data at $15 per month\n      </p>\n        <p>daveconnors</p>\n    </div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"13920376\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Cassandra at eBay - Cassandra Summit 2012\" href=\"https://www.slideshare.net/jaykumarpatel/cassandra-at-ebay-13920376\" readability=\"-24\">\n    \n    <div class=\"related-content\" readability=\"12\">\n      <p>\n        Cassandra at eBay - Cassandra Summit 2012\n      </p>\n        <p>Jay Patel</p>\n    </div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"8600029\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Migrating Netflix from Datacenter Oracle to Global Cassandra\" href=\"https://www.slideshare.net/adrianco/migrating-netflix-from-oracle-to-global-cassandra\" readability=\"-24\">\n    \n    <div class=\"related-content\" readability=\"12\">\n      <p>\n        Migrating Netflix from Datacenter Oracle to Global Cassandra\n      </p>\n        <p>Adrian Cockcroft</p>\n    </div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"23161202\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"C* Summit 2013: Time for a New Relationship - Intuit's Journey from RDBMS to Cassandra by Mohit Anchlia\" href=\"https://www.slideshare.net/planetcassandra/3-mohit-anchlia\" readability=\"-24\">\n    \n    <div class=\"related-content\" readability=\"12\">\n      <p>\n        C* Summit 2013: Time for a New Relationship - Intuit's Journey from RDBMS to ...\n      </p>\n        <p>DataStax Academy</p>\n    </div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"33064441\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Couchdb awesomeness - History, the basics and how we use it\" href=\"https://www.slideshare.net/BenDuncan/couchdb-awesomeness-the-history-basics-and-how-we-use-it\" readability=\"-23\">\n    \n    <div class=\"related-content\" readability=\"14\">\n      <p>\n        Couchdb awesomeness - History, the basics and how we use it\n      </p>\n        <p>Ben Duncan</p>\n    </div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"27300333\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Blueflood: Open Source Metrics Processing at CassandraEU 2013\" href=\"https://www.slideshare.net/gdusbabek/blueflood-open-source-metrics-processing-at-cassandraeu-2013\" readability=\"-24\">\n    \n    <div class=\"related-content\" readability=\"12\">\n      <p>\n        Blueflood: Open Source Metrics Processing at CassandraEU 2013\n      </p>\n        <p>gdusbabek</p>\n    </div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"33826376\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Best Practices for couchDB developers on Microsoft Azure\" href=\"https://www.slideshare.net/brianbenz/best-practices-for-couch-db-developers-on-windows-azure\" readability=\"-24\">\n    \n    <div class=\"related-content\" readability=\"12\">\n      <p>\n        Best Practices for couchDB developers on Microsoft Azure\n      </p>\n        <p>Brian Benz</p>\n    </div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div>\n</div>\n      \n      \n        <footer>\n          <div class=\"row\">\n            <div class=\"columns\">\n              <ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div>\n          </div>\n          \n          <div class=\"row\" readability=\"5\">\n            <div class=\"columns\" readability=\"11\">\n              <p class=\"copyright text-center\">LinkedIn Corporation © 2017</p>\n              \n            </div>\n          </div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\" readability=\"34\">\n          \n    <div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" readability=\"7\">\n  <h4 class=\"modal-title\">Public clipboards featuring this slide</h4>\n  <hr/>\n  \n  <p>\n    No public clipboards found for this slide\n  </p>\n  \n</div>\n    \n    <div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\" readability=\"7\">\n  <p>\n    <h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr/><a class=\"close-reveal-modal button-lrg\" href=\"https://www.slideshare.net/planetcassandra/nyc-tech-day-using-cassandra-for-dvr-scheduling-at-comcast#\" aria-label=\"Close\">×</a>\n  </p>\n  \n  <div class=\"modal-content\" readability=\"35\">\n    <div class=\"default-clipboard-panel radius\" readability=\"6\">\n      <p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"/> already.</p>\n    </div>\n    \n    <div class=\"clipboard-list-container\">\n      <div class=\"clipboard-create-new\">\n        \n        <p>Create a clipboard</p>\n      </div>\n    </div>\n  </div>\n</div>\n    <div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\" readability=\"11\">\n    <p>\n      <h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.\n      \n    </p>\n    <h4 class=\"modal-title\" id=\"modal-title\"/>\n    <hr aria-hidden=\"true\"/><form data-abide=\"\">\n    \n    <div class=\"row\">\n      <p>\n        <label>Description\n          \n        </label>\n      </p>\n    </div>\n    <div class=\"row\" readability=\"6\">\n      <label readability=\"2\">Visibility\n        <p>\n          <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          <label for=\"privacy-switch\"/>\n        </p>\n      </label>\n    </div>\n        \n    </form>\n    </div>\n    </div>\n    \n    \n      <meta content=\"{&quot;jsplayer_toolbar_clips.clip_count_info&quot;:&quot;{:count,choice,0#Be the first to clip this slide|singular#person clipped this slide|plural#people clipped this slide}&quot;}\" name=\"ss-i18n-translations\"/><noscript>\n    </noscript>\n  </body>",
        "created_at": "2017-09-14T19:30:55+0000",
        "updated_at": "2017-12-27T14:21:09+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 6,
        "domain_name": "www.slideshare.net",
        "preview_picture": "http://image.slidesharecdn.com/johnmccann-130409191105-phpapp01/95/slide-1-638.jpg?1365552774",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/5168"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 11,
            "label": "database",
            "slug": "database"
          },
          {
            "id": 36,
            "label": "solr",
            "slug": "solr"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 894,
            "label": "distributed",
            "slug": "distributed"
          }
        ],
        "is_public": false,
        "id": 5140,
        "uid": null,
        "title": "Distributed Graph Database",
        "url": "http://titan.thinkaurelius.com/",
        "content": "<img id=\"titanlogo\" src=\"http://titan.thinkaurelius.com/images/titan-logo2.png\"/><p>Titan is a scalable <a href=\"http://en.wikipedia.org/wiki/Graph_database\">graph database</a> optimized for storing and querying graphs containing hundreds of billions of vertices and edges distributed across a multi-machine cluster. \nTitan is a transactional database that can support <a href=\"http://thinkaurelius.com/2012/08/06/titan-provides-real-time-big-graph-data/\">thousands of concurrent users</a> executing <a href=\"http://thinkaurelius.com/2013/05/13/educating-the-planet-with-pearson/\">complex graph traversals</a> in real time.</p><p>In addition, Titan provides the following features:</p>\n<ul><li>Elastic and linear scalability for a growing data and user base.</li>\n<li>Data distribution and replication for performance and fault tolerance.</li>\n<li>Multi-datacenter high availability and hot backups.</li>\n<li>Support for <a href=\"http://en.wikipedia.org/wiki/ACID\">ACID</a> and <a href=\"http://en.wikipedia.org/wiki/Eventual_consistency\">eventual consistency</a>.</li>\n<li>Support for various <a href=\"http://s3.thinkaurelius.com/docs/titan/1.0.0/storage-backends.html\">storage backends</a>:\n<ul><li><a href=\"http://cassandra.apache.org/\">Apache Cassandra</a></li>\n<li><a href=\"http://hbase.apache.org/\">Apache HBase</a></li>\n<li><a href=\"http://www.oracle.com/technetwork/database/berkeleydb/overview/index-093405.html\">Oracle BerkeleyDB</a></li>\n</ul></li>\n<li>Support for global <a href=\"http://tinkerpop.incubator.apache.org/docs/3.0.1-incubating/#graphcomputer\">graph data analytics</a>, reporting, and ETL through integration with big data platforms:\n<ul><li><a href=\"http://spark.apache.org/\">Apache Spark</a></li>\n<li><a href=\"http://giraph.apache.org/\">Apache Giraph</a></li>\n<li><a href=\"http://hadoop.apache.org/\">Apache Hadoop</a></li>\n</ul></li>\n<li>Support for geo, numeric range, and full-text search via:\n<ul><li><a href=\"http://www.elasticsearch.org/\">ElasticSearch</a></li>\n<li><a href=\"http://lucene.apache.org/solr/\">Solr</a></li>\n<li><a href=\"http://lucene.apache.org/\">Lucene</a></li>\n</ul></li>\n<li>Native integration with the <a href=\"http://tinkerpop.incubator.apache.org/\">TinkerPop</a> graph stack:\n<ul><li><a href=\"http://tinkerpop.incubator.apache.org/docs/3.0.1-incubating/#traversal\">Gremlin graph query language</a></li>\n<li><a href=\"http://tinkerpop.incubator.apache.org/docs/3.0.1-incubating/#gremlin-server\">Gremlin graph server</a></li>\n<li><a href=\"http://tinkerpop.incubator.apache.org/docs/3.0.1-incubating/#gremlin-applications\">Gremlin applications</a></li>\n</ul></li>\n<li>Open source with the liberal <a href=\"http://www.apache.org/licenses/LICENSE-2.0.html\">Apache 2</a> license.</li>\n</ul><p><a href=\"https://github.com/thinkaurelius/titan/wiki/Downloads\">Download</a> Titan or <a href=\"https://github.com/thinkaurelius/titan\">clone</a> from GitHub.\nRead the <a href=\"http://s3.thinkaurelius.com/docs/titan/1.0.0/\">Titan documentation</a> and join the <a href=\"https://groups.google.com/forum/#!forum/aureliusgraphs\">mailing list</a>.<br/></p>\n<div class=\"highlight\" readability=\"13\"><pre>&lt;dependency&gt;\n   &lt;groupId&gt;com.thinkaurelius.titan&lt;/groupId&gt;\n   &lt;artifactId&gt;titan-core&lt;/artifactId&gt;\n   &lt;version&gt;1.0.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- core, all, cassandra, hbase, berkeleyje, es, lucene --&gt;\n</pre></div>\n<br/><div class=\"highlight\" readability=\"7\"><pre>// who is hercules' grandfather?\ng.V.has('name','hercules').out('father').out('father').name\n</pre></div>\n<p>Continue with the <a href=\"http://s3.thinkaurelius.com/docs/titan/1.0.0/getting-started.html\">Getting Started with Titan</a> guide for a step-by-step introduction.</p>",
        "created_at": "2017-10-06T00:47:52+0000",
        "updated_at": "2017-12-27T13:42:38+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "",
        "reading_time": 1,
        "domain_name": "titan.thinkaurelius.com",
        "preview_picture": "http://thinkaurelius.github.io/titan/images/titan-logo2.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/5140"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 11,
            "label": "database",
            "slug": "database"
          },
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 5121,
        "uid": null,
        "title": "Consider the Apache Cassandra database",
        "url": "https://www.ibm.com/developerworks/library/os-apache-cassandra/index.html",
        "content": "<p class=\"dw-article-subhead\">What are the pros and cons of this NoSQL database?</p>\n                                        <!-- Article Top Bar -->\n                                                <div class=\"ibm-columns dw-article-topbar\" readability=\"37\">\n                                                    <!-- Author and article info. -->\n                                                    <div class=\"ibm-col-6-2 ibm-col-medium-6-4 dw-article-metadata\" readability=\"34\">\n                                                        <div class=\"dw-article-avatar\"><img width=\"42\" height=\"42\" src=\"https://www.ibm.com/developerworks/i/p-sperera.jpg\" alt=\"Photo of Srinath Perera\"/></div><p>Srinath Perera<br/>Published on July 03,  2012</p>\n                                                    </div>\n                                                    <!-- Social -->\n                                                    <div class=\"ibm-col-6-2 ibm-col-medium-6-4 ibm-col-small-6-2 dw-article-social\">\n                                                        <!-- Sharing links -->\n                                                        \n                                                        <!-- Number of comments and link to comments -->\n                                                        <div id=\"dw-article-cmts\">\n                                                            <div class=\"dw-article-cmtslink\">\n                                                                <a onclick=\"tocLink('#icomments')\" href=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/index.html#icomments\" role=\"link\" tabindex=\"0\" aria-label=\"Comments\">\n                                                                    <img src=\"https://dw1.s81c.com/developerworks/i/v18/article/dw-article-cmt-icon.png\" width=\"29\" height=\"29\" alt=\"Comments\"/></a>\n                                                            </div>\n                                                            \n                                                        </div>\n                                                    </div>\n                                                </div>\n                                        <!-- Article Body -->\n                                        \n                                        <p>In the database history article \"What Goes Around Comes Around,\" (see <a href=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/index.html#artrelatedtopics\">Related topics</a>) Michal Stonebraker\n                describes in detail how storage techniques have evolved over time. Before\n                arriving at the relational model, developers tried other models such as\n                hierarchical and directed graph. It is worth noting that the SQL-based\n                relational model—which is the de facto standard even\n                    now—has prevailed for about 30 years. Given the short\n                history and fast pace of computer science, this is a remarkable\n                achievement. The relational model is so well-established that for many\n                years, selecting data storage for an application was an easy choice for\n                the solution architect. The choice was invariably a relational database. </p><p>Developments like increasing user bases of systems, mobile devices,\n                extended online presence of users, cloud computing, and multi-core systems\n                have led to increasingly large-scale systems. High-tech companies such as\n                Google and Amazon were among first to hit those problems of scale. They\n                soon found out that relational databases are not adequate to support\n                large-scale systems. </p><p>To circumvent those challenges, Google and Amazon came up with two\n                alternative solutions: Big Table and Dynamo (see <a href=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/index.html#artrelatedtopics\">Related topics</a>) where they relaxed the\n                guarantees provided by the relational data model to achieve higher\n                scalability. Eric Brewer's \"CAP Theorem\" (see <a href=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/index.html#artrelatedtopics\">Related topics</a>) later formalized those observations. It claims\n                that for scalable systems, consistency, availability, and partition\n                tolerance are trade-offs where it is impossible to build systems\n                containing all those properties. Soon, based on earlier work by Google and\n                Amazon, and understanding acquired about scalable systems, a new class of\n                storage systems was proposed. They were named \"NoSQL\" systems. The name\n                first meant \"do not use SQL if you want to scale\" and later it was\n                redefined to \"not only SQL\" to mean that there are other solutions in\n                addition to SQL-based solutions.</p><p>There are many NoSQL systems, and each relaxes or alters some aspect of the\n                relational model. It is worth noting that none of the NoSQL solutions work\n                for all scenarios. Each does better than relational models and scales for\n                some subsets of the use cases. My earlier article \"Finding the Right Data\n                Solution for Your Application in the Data Storage Haystack\" discusses how\n                to match application requirements to NoSQL solutions (see <a href=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/index.html#artrelatedtopics\">Related topics</a>). </p><p>Apache Cassandra (see <a href=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/index.html#artrelatedtopics\">Related topics</a>) is\n                one of the first and most widely used NoSQL solutions. This article takes\n                a detailed look at Cassandra and points out details and tricky points not\n                readily apparent when you look at Cassandra for the first time. </p><h2 id=\"2ApacheCassandra\" class=\"ibm-h2\">Apache\n                Cassandra</h2><p>Cassandra is a NoSQL Column family implementation supporting the Big Table\n                data model using the architectural aspects introduced by Amazon Dynamo.\n                Some of the strong points of Cassandra are: </p><ul class=\"ibm-bullet-list\"><li>Highly scalable and highly available with no single point of\n                    failure</li><li>NoSQL column family implementation</li><li>Very high write throughput and good read throughput</li><li>SQL-like query language (since 0.8) and support search through\n                    secondary indexes</li><li>Tunable consistency and support for replication</li><li>Flexible schema</li></ul><p>These positive points make it easy to recommend Cassandra, but it is\n                crucial for a developer to delve into the details and tricky points of\n                Cassandra to grasp the intricacies of this program. </p><p>Cassandra stores data according to the column family data model, depicted\n                in <a href=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/index.html#fig1\">Figure 1</a>.</p><h5 id=\"fig1\" class=\"ibm-h5\">Figure 1. Cassandra data model</h5><img src=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/figure001.gif\" class=\"ibm-downsize\" alt=\"Diagram showing column and row relationships in keyspaces\" height=\"507\" width=\"576\"/><div class=\"ibm-common-overlay ibm-overlay-alt-three\" data-widget=\"overlay\" id=\"N10096\"><img alt=\"Diagram showing column and row relationships in keyspaces\" src=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/figure001.gif\" width=\"576\"/></div><div class=\"dw-article-sidebar ibm-background-cool-white-20\" readability=\"9\"><h5>What is a Column?</h5><p><em>Column</em> is bit of a misnomer, and possibly the name\n                        <em>cell</em> would have been easier to understand. I will stick\n                    with <em>column</em> as that is the common usage.</p></div><p>Cassandra data model consists of columns, rows, column families, and\n                keyspace. Let's look at each part in detail. </p><ul class=\"ibm-bullet-list\" readability=\"11\"><li> Column – the most basic unit in the Cassandra data model, and each\n                    column consists of a name, a value, and a timestamp. For this\n                    discussion, ignore the timestamp, and then you can represent a column\n                    as a name value pair (such as author=\"Asimov\"). </li><li readability=\"15\"> Row – a collection of columns labeled with a name. For example, <a href=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/index.html#list1\">Listing 1</a> shows how a row might be represented:\n                        <h5 id=\"list1\" class=\"ibm-h5\">Listing 1. Example of a\n                        row</h5><pre data-widget=\"syntaxhighlighter\" class=\"brush: js; html-script: true; gutter: true;\">    \"Second Foundation\"-&gt; {\n    author=\"Asimov\", \n    publishedDate=\"..\",\n    tag1=\"sci-fi\", tag2=\"Asimov\"\n    }</pre><p>Cassandra\n                        consists of many storage nodes and stores each row within a single\n                        storage node. Within each row, Cassandra always stores columns\n                        sorted by their column names. Using this sort order, Cassandra\n                        supports slice queries where given a row, users can retrieve a\n                        subset of its columns falling within a given column name range.\n                        For example, a slice query with range tag0 to tag9999 will get all\n                        the columns whose names fall between tag0 and tag9999.</p></li><li readability=\"10\"> Column family – a collection of rows labeled with a name. <a href=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/index.html#list2\">Listing 2</a> shows how sample data might look:\n                        <h5 id=\"list2\" class=\"ibm-h5\">Listing 2. Example of a column\n                            family</h5><pre data-widget=\"syntaxhighlighter\" class=\"brush: js; html-script: true; gutter: true;\">    Books-&gt;{\n    \"Foundation\"-&gt;{author=\"Asimov\", publishedDate=\"..\"},\n    \"Second Foundation\"-&gt;{author=\"Asimov\", publishedDate=\"..\"},\n    …\n    }</pre><p>It\n                        is often said that a column family is like a table in a relational\n                        model. As shown in the following example, the similarities end\n                        there. </p></li><li> Keyspace – a group of many column families together. It is only a\n                    logical grouping of column families and provides an isolated scope for\n                    names. </li></ul><p>Finally, super columns reside within a column family that groups several\n                columns under a one key. As developers discourage the use of super\n                columns, I do not discuss them here. </p><h2 id=\"3CassandravsRDBMSdatamodels\" class=\"ibm-h2\">Cassandra\n                versus RDBMS data models</h2><p>From the above description of the Cassandra data model, data is placed in a\n                two dimensional (2D) space within each column family. To retrieve data in\n                a column family, users need two keys: row name and column name. In that\n                sense, both the relational model and Cassandra are similar, although there\n                are several crucial differences. </p><ul class=\"ibm-bullet-list\"><li> Relational columns are homogeneous across all rows in the table. A\n                    clear vertical relationship usually exists between data items, that is\n                    not the case with Cassandra columns. This is the reason Cassandra\n                    stores the column name with each data item (column). </li><li> With the relational model, 2D data space is complete. Each point in\n                    the 2D space should have at least the null value stored there. Again,\n                    this is not the case with Cassandra, and it can have rows containing\n                    only a few items, while other rows can have millions of items. </li><li> With a relational model, the schema is predefined and cannot be\n                    changed at runtime, while Cassandra lets users change the schema at\n                    runtime. </li><li> Cassandra always stores data such that columns are sorted based on\n                    their names. This makes it easier to search for data through a column\n                    using slice queries, but it is harder to search for data through a row\n                    unless you use an order-preserving partitioner. </li><li> Another crucial difference is that column names in RDMBS represent\n                    metadata about data, but never data. In Cassandra, however, the names\n                    of columns can include data. Consequently, Cassandra rows can have\n                    millions of columns, while a relational model usually has tens of\n                    columns. </li><li>Using a well-defined immutable schema, relational models support\n                    sophisticated queries that include JOINs, aggregations, and more. With\n                    a relational model, users can define the data schema without worrying\n                    about queries. Cassandra does not support JOINs and most SQL search\n                    methods. Therefore, schema has to be catered to the queries required\n                    by the application. </li></ul><p>To explore the above differences, consider a book rating site where users\n                can add books (author, rank, price, link), comments (text, time, name),\n                and tag them. The Application needs to support the following operations by\n                the users: </p><ul class=\"ibm-bullet-list\"><li> Adding books </li><li> Adding comments for books </li><li> Adding tags for books</li><li> Listing books sorted by rank</li><li> Listing books given a tag </li><li> Listing the comments given a book ID</li></ul><p>It is rather trivial to implement the above application with a relational\n                model. <a href=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/index.html#fig2\">Figure 2</a> shows the Entity–relationship (ER)\n                diagram for the database design. </p><h5 id=\"fig2\" class=\"ibm-h5\">Figure 2. ER Model for the Book rating\n                    site</h5><img src=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/figure002.gif\" class=\"ibm-downsize\" alt=\"Flow diagram of the book site data model\" height=\"289\" width=\"524\"/><div class=\"ibm-common-overlay ibm-overlay-alt-three\" data-widget=\"overlay\" id=\"N10100\"><img alt=\"Flow diagram of the book site data model\" src=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/figure002.gif\" width=\"524\"/></div><p> Let's see how this can be implemented using the Cassandra data model. <a href=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/index.html#list3\">Listing 3</a> shows a potential schema with Cassandra,\n                where the first line represents the \"Books\" column family which has\n                multiple rows, each having properties of the book as columns. &lt;TS1&gt;\n                and &lt;TS2&gt; denote timestamps. </p><h5 id=\"list3\" class=\"ibm-h5\">Listing 3. Cassandra schema for the book rating\n                sample</h5><pre data-widget=\"syntaxhighlighter\" class=\"brush: js; html-script: true; gutter: true;\">Books[BookID-&gt;(author, rank, price, link, tag&lt;TS1&gt;, tag&lt;TS2&gt; .., \n    cmt+&lt;TS1&gt;= text + \"-\" + author) …] \nTags2BooksIndex[TagID-&gt;(&lt;TS1&gt;=bookID1, &lt;TS2&gt;=bookID2, ..) ] \nTags2AuthorsIndex[TagID-&gt;(&lt;TS1&gt;=bookID1, &lt;TS2&gt;=bookID2, ..) ]\nRanksIndex[\"RANK\" -&gt; (rank&lt;TS1&gt;=bookID)]</pre><p><a href=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/index.html#table1\">Table 1</a> is a sample data set as per the schema. </p><h5 id=\"table1\" class=\"ibm-h5\">Table 1. Sample data for the book\n                    rating site</h5><table border=\"1\" cellpadding=\"0\" cellspacing=\"0\" class=\"ibm-data-table\" data-widget=\"datatable\" summary=\"Sample data for the book rating site\"><thead xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"><tr><th class=\"ibm-background-neutral-white-30\"> Column Family\n                            Name </th><th class=\"ibm-background-neutral-white-30\"> Sample Dataset\n                        </th></tr></thead><tbody xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" readability=\"10\"><tr readability=\"16\"><td> Books</td><td>\"Foundation\" -&gt;\n                            (\"author\"=\"Asimov\", \"rank\"=9, \"price\"=14, \"tag1\"=\"sci-fi\",\n                            \"tag2\"=\"future\", \"cmt1311031405922\"=\"best book-sanjiva\",\n                            \"cmt1311031405923\"=\"well I disagree-srinath\")<br/>\"I Robot\"\n                            -&gt; (\"author\"=\"Asimov\", \"rank\"=7, \"price\"=14,\n                            \"tag1\"=\"sci-fi\" \"tag2\"=\"robots\", \"cmt1311031405924\"=\"Asimov's\n                            best-srinath\", \"cmt1311031405928\"=\"I like foundation\n                            better-sanjiva\")<br/></td></tr><tr readability=\"2\"><td> RanksIndex </td><td> \"Rank\" -&gt; (9=\"Foundation\", 7=\"I\n                            Robot\") </td></tr><tr readability=\"2\"><td> Tags2BooksIndex <br/></td><td> \"sci-fi\" -&gt;\n                            (\"1311031405918\"=\"Foundation\", \"1311031405919\"=\"I Robot\"<br/>\"future\" -&gt; …<br/></td></tr><tr readability=\"1\"><td> Tags2AuthorsIndex </td><td> \"sci-fi\" -&gt;\n                            (1311031405920=\"Asimov\")<br/>\"future\" -&gt; … </td></tr></tbody></table><p>This example shows several design differences between the relational and\n                Cassandra models. The Cassandra model stores data about books in a single\n                column family called \"Books,\" and the other three Column Families are\n                indexes built to support queries. </p><p>Looking at the \"Books\" column family in detail, the model uses a row to\n                represent each book where a book name is the row ID. Details about the\n                book are represented as columns stored within the row. </p><p>Looking closely, you might notice that data items stored (like comments,\n                and tags that have 1:M relationship with books) are also within a single\n                row. To do that, append the time stamp to the column names for tags and\n                comments. This approach stores all data within the same column. This\n                action avoids having to do JOINs to retrieve data. Cassandra circumvents\n                the lack of support for JOINs through this approach.</p><p>This provides several advantages.</p><ul class=\"ibm-bullet-list\"><li>You can read all data about a book through a single query reading the\n                    complete row. </li><li>You can retrieve comments and tags without a JOIN by using slice\n                    queries that have cmt0-cmt9999 and tag0-tag9999 as starting and ending\n                    ranges. </li></ul><p>Because Cassandra stores columns sorted by their column names, making slice\n                queries is very fast. It is worth noting that storing all the details\n                about the data item in a single row and the use of sort orders are the\n                most crucial ideas behind the Cassandra data design. Most Cassandra data\n                model designs follow these ideas in some form. User can use the sort\n                orders while storing data and building indexes. For example, another side\n                effect of appending time stamps to column names is that as column names\n                are stored in the sorted order, comments having column names post-fixed by\n                the timestamps are stored in the order they are created, and search\n                results would have the same order. </p><p>Cassandra does not support any search methods from the basic design.\n                Although it supports secondary indexes, they are supported using indexes\n                that are built later, and secondary indexes have several limitations\n                including lack of support for range queries. </p><p>Consequently, the best results in a Cassandra data design needs users to\n                implement searches by building custom indexes and utilizing column and row\n                sort orders. Other three-column families (Tags2BooksIndex,\n                Tags2AuthorsIndex, and RankIndex) do exactly that. Since users need to\n                search for books given a tag, \"Tags2BooksIndex\" column family builds an\n                index by storing the tag name as the row ID and all books tagged by that\n                tag as columns under that row. As shown by the example, timestamps are\n                added as the column keys, but that is to provide a unique column ID. The\n                search implementation simply reads the index by looking up the row by tag\n                name and finding the matches by reading all columns stored within that\n                rowID. </p><p><a href=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/index.html#table2\">Table 2</a> discusses how each of the queries required by\n                the application is implemented using the above Cassandra indexes. </p><h5 id=\"table2\" class=\"ibm-h5\">Table 2. Comparison of query\n                    implementations</h5><table border=\"1\" cellpadding=\"0\" cellspacing=\"0\" class=\"ibm-data-table\" data-widget=\"datatable\" summary=\"Comparison of query implementations\"><thead xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"><tr><th class=\"ibm-background-neutral-white-30\"> Query\n                            description </th><th class=\"ibm-background-neutral-white-30\"> Query as SQL </th><th class=\"ibm-background-neutral-white-30\"> Cassandra\n                            implementation </th></tr></thead><tbody xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" readability=\"12\"><tr readability=\"8\"><td> List books sorted by the\n                            rank</td><td> Run the\n                            query <br/><code>\"Select * from Books order by rank\"</code>\n                            and then on each result do\n                            <code>\"Select tag from Tags where\n                            bookid=?\" and \"Select comment from Comments where\n                            bookid=?\"</code></td><td> Do a slice query on \"RankIndex\"\n                            column family to receive an ordered list of books, and for\n                            each book do a slice query on \"Books\" column family to read\n                            the details about the book. </td></tr><tr readability=\"5\"><td> Given a tag, find the authors\n                            whose books have the given tag. </td><td><code>Select distinct author from Tags, Books where\n                            Tags.bookid=Books.bookid and tag=?</code></td><td> Read all columns for the given tag\n                            from Tags2Authors using a slice query. </td></tr><tr readability=\"4\"><td> Given a tag, list books that have\n                            the given tag. </td><td><code>Select bookid from Tags where tag=? </code></td><td> Read all columns for the given tag\n                            from Tags2BooksIndex using a slice query. </td></tr><tr readability=\"10\"><td> Given a book, list the comments\n                            for that book in sorted order of the time when the comments\n                            were created. </td><td><code>Select text, time, user from Comments where bookid=? Order by\n                            time</code></td><td> In \"Books\" column family, do a\n                            slice query from the row corresponding to the given book. They\n                            are in sorted order due to timestamps used as the column name.\n                        </td></tr></tbody></table><p>Although the above design can efficiently support queries required by the\n                book-rating site, it can only support queries that it is designed for and\n                cannot support ad-hoc queries. For example, it cannot do the following\n                queries without building new indexes. </p><ul class=\"ibm-bullet-list\"><li><code>Select * from Books where price &gt; 50; </code></li><li><code>Select * from Books where author=\"Asimov\"</code></li></ul><p>It is possible to change the design to support those and other queries by\n                either building appropriate indexes or by writing code to walk through the\n                data. The need for custom code to support new queries, however, is a\n                limitation compared to relational models where adding new queries often\n                needs no changes to the schema. </p><p>From the 0.8 release, Cassandra supports secondary indexes where users can\n                specify a search by a given property, and Cassandra automatically builds\n                indexes for searching based on that property. That model, however,\n                provides less flexibility. For example, secondary indexes do not support\n                range queries and provide no guarantees on sort orders of results. </p><h2 id=\"4UsingCassandrafromJavaoutline\" class=\"ibm-h2\">Using Cassandra from the Java environment</h2><p>Cassandra has many clients written in different languages. This article\n                focuses on the Hector client (see <a href=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/index.html#artrelatedtopics\">Related\n                    topics</a>), which is the most widely used Java client for Cassandra.\n                Users can add to their application by adding the Hector JARs to the\n                application classpath. <a href=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/index.html#list4\">Listing 4</a> shows a sample\n                Hector client. </p><p>First, connect to a Cassandra cluster. Use the instructions in the\n                Cassandra Getting Started Page (see <a href=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/index.html#artrelatedtopics\">Related\n                    topics</a>) to set up a Cassandra node. Unless its configuration has\n                been changed, it typically runs on port 9160. Next, define a keyspace.\n                This can be done either through the client or through the\n                conf/cassandra.yaml configuration file.</p><h5 id=\"list4\" class=\"ibm-h5\">Listing 4. Sample Hector client code for\n                Cassandra</h5><pre data-widget=\"syntaxhighlighter\" class=\"brush: js; html-script: true; gutter: true;\">Cluster cluster = HFactory.createCluster('TestCluster', \n        new CassandraHostConfigurator(\"localhost:9160\"));\n//define a keyspace\nKeyspace keyspace = HFactory.createKeyspace(\"BooksRating\", cluster);\n//Now let's add a new column. \nString rowID = \"Foundation\"; \nString columnFamily = \"Books\";\nMutator&lt;String&gt;\n mutator = HFactory.createMutator(keyspace, user);\nmutator.insert(rowID, columnFamily, \n        HFactory.createStringColumn(\"author\", \"Asimov\"));\n//Now let's read the column back \nColumnQuery&lt;String, String, String&gt;\n        columnQuery = HFactory.createStringColumnQuery(keyspace);\ncolumnQuery.setColumnFamily(columnFamily).setKey(”wso2”).setName(\"address\");\nQueryResult&lt;HColumn&lt;String, String&gt;\n result = columnQuery.execute();\nSystem.out.println(\"received \"+ result.get().getName() + \"= \" \n        + result.get().getValue() + \" ts = \"+ result.get().getClock());</pre><p>Find the complete code for the book rating example in <a href=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/index.html#artdownload\">Download</a>. It includes samples for slice\n                queries and other complex operations. </p><h2 id=\"5CassandraArchitecture\" class=\"ibm-h2\">Cassandra\n                architecture</h2><p>Having looked at the data model of Cassandra, let's return to its\n                architecture to understand some of its strengths and weaknesses from a\n                distributed systems point of view. </p><p><a href=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/index.html#fig3\">Figure 3</a> shows the architecture of a Cassandra cluster.\n                The first observation is that Cassandra is a distributed system. Cassandra\n                consists of multiple nodes, and it distributes the data across those nodes\n                (or shards them, in the database terminology). </p><h5 id=\"fig3\" class=\"ibm-h5\">Figure 3. Cassandra cluster</h5><img src=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/figure003.gif\" class=\"ibm-downsize\" alt=\"Diagram for the cassandra cluster showing how each node is connected in a loop\" height=\"434\" width=\"471\"/><div class=\"ibm-common-overlay ibm-overlay-alt-three\" data-widget=\"overlay\" id=\"N101FA\"><img alt=\"Diagram for the cassandra cluster showing how each node is connected in a loop\" src=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/figure003.gif\" width=\"471\"/></div><p>Cassandra uses consistent hashing to assign data items to nodes. In simple\n                terms, Cassandra uses a hash algorithm to calculate the hash for keys of\n                each data item stored in Cassandra (for example, column name, row ID). The\n                hash range or all possible hash values (also known as keyspace) is divided\n                among the nodes in the Cassandra cluster. Then Cassandra assigns each data\n                item to the node, and that node is responsible for storing and managing\n                the data item. The paper \"Cassandra - A Decentralized Structured Storage\n                System\" (see <a href=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/index.html#artrelatedtopics\">Related topics</a>) provides a\n                detailed discussion about Cassandra architecture. </p><p>The resulting architecture provides the following properties: </p><ul class=\"ibm-bullet-list\"><li> Cassandra distributes data among its nodes transparently to the\n                    users. Any node can accept any request (read, write, or delete) and\n                    route it to the correct node even if the data is not stored in that\n                    node.</li><li> Users can define how many replicas are needed, and Cassandra handles\n                    replica creation and management transparently. </li><li> Tunable consistency: When storing and reading data, users can choose\n                    the expected consistency level per each operation. For example, if the\n                    \"quorum\" consistency level is used while writing or reading, data is\n                    written and read from more than half of the nodes in the cluster.\n                    Support for tunable consistency enables users to choose the\n                    consistency level best suited to the use case.</li><li> Cassandra provides very fast writes, and they are actually faster\n                    than reads where it can transfer data about 80-360MB/sec per node. It\n                    achieves this using two techniques. <ul class=\"ibm-bullet-list\"><li> Cassandra keeps most of the data within memory at the\n                            responsible node, and any updates are done in the memory and\n                            written to the persistent storage (file system) in a lazy\n                            fashion. To avoid losing data, however, Cassandra writes all\n                            transactions to a commit log in the disk. Unlike updating data\n                            items in the disk, writes to commit logs are append-only and,\n                            therefore, avoid rotational delay while writing to the disk.\n                            For more information on disk-drive performance\n                            characteristics, see <a href=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/index.html#artrelatedtopics\">Related\n                                topics</a>.</li><li> Unless writes have requested full consistency, Cassandra\n                            writes data to enough nodes without resolving any data\n                            inconsistencies where it resolves inconsistencies only at the\n                            first read. This process is called \"read repair.\" </li></ul></li></ul><p>The resulting architecture is highly scalable. You can build a Cassandra\n                cluster that has 10s of 100s of nodes that is capable of handling\n                terabytes to petabytes of data. There is a trade-off with distributed\n                systems, and scale almost never comes for free. As mentioned before, a\n                user might face many surprises moving from a relational database to\n                Cassandra. The next section discusses some of them. </p><h2 id=\"6SurpriseyoumightgetwithCassandra\" class=\"ibm-h2\">Possible surprises with Cassandra</h2><p>Be aware of these differences when you move from a relational database to\n                Cassandra.</p><h3 id=\"N10222\" class=\"ibm-h3\">No transactions, no JOINs</h3><p>It is well known that Cassandra does not support ACID transactions.\n                Although it has a batch operation, there is no guarantee that\n                sub-operations within the batch operation are carried out in an atomic\n                fashion. This will be discussed more under <a href=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/index.html#failedops\">Failed\n                    operations may leave changes</a>. </p><p>Furthermore, Cassandra does not support JOINs. If a user needs to join two\n                column families, you must retrieve and join data programmatically. This is\n                often expensive and time-consuming for large data sets. Cassandra\n                circumvents this limitation by storing as much data as possible in the\n                same row, as described in the example. </p><h3 id=\"N1022F\" class=\"ibm-h3\">No foreign keys and keys are\n                immutable</h3><p>Cassandra does not support foreign keys, so it is not possible for\n                Cassandra to manage the data consistency on a user's behalf. Therefore,\n                the application should handle the data consistency. Furthermore, users\n                cannot change the keys. It is recommended to use surrogate keys (generated\n                keys instead of the key, and managing the key as a property) with the use\n                cases that need changes to the keys. </p><h3 id=\"N10236\" class=\"ibm-h3\">Keys have to be unique</h3><p>Each key, for example row keys and column keys, has to be unique in its\n                scope, and if the same key has been used twice it will overwrite the data. </p><p>There are two solutions to this problem. First, you can use a composite\n                key. In other words, create the key by combining several fields together,\n                and this solution is often used with row keys. The second solution is when\n                there is a danger of the same key occurring twice, postfix the key with a\n                random value or a timestamp. This often happens with indexes when an index\n                stores a value as the column name. For example, in the book rating\n                application the rank was used as the column name. To avoid having two\n                entries having the same column name because both have the same rank, the\n                timestamp is added to the rank as a postfix. </p><h3 id=\"failedops\" class=\"ibm-h3\">Failed operations may leave\n                changes</h3><p>As explained before, Cassandra does not support atomic operations. Instead,\n                it supports idempotent operations. Idempotent operations leave the system\n                in the same state regardless of how many times the operations are carried\n                out. All Cassandra operations are idempotent. If an operation fails, you\n                can retry it without any problem. This provides a mechanism to recover\n                from transient failures. </p><p>Also Cassandra supports batch operations, but they do not have any\n                atomicity guarantees either. Since the operations are idempotent, the\n                client can keep retrying until all operations of the batch are successful. </p><p>Idempotent operations are not equal to atomic operations. If an operation\n                is successful, all is well and the outcome is identical to atomic\n                operations. If an operation fails, the client can retry, and if it is\n                successful, again all is well. If, however, the operations fails even\n                after retrying, unlike with atomic operations, it might leave side\n                effects. Unfortunately, with Cassandra, this is a complexity that\n                programmers have to deal with themselves. </p><h3 id=\"N1024A\" class=\"ibm-h3\">Searching is complicated</h3><p>Searching is not built into the core of the Cassandra architecture, and\n                search mechanisms are layered on top using sort orders as described\n                earlier. Cassandra supports secondary indexes where the system\n                automatically builds them, with some limited functionality. When secondary\n                indexes do not work, users have to learn the data model and build indexes\n                using sort orders and slices. </p><p>Three types of complexities area associated with building search\n                methods:</p><ol type=\"1\"><li> Building custom search methods require programmers to understand\n                    indexing and details about storage to a certain extent. Therefore,\n                    Cassandra needs higher skilled developers than with relational models. </li><li> Custom indexes heavily depend on sorted orders, and they are\n                    complicated. There are two types of sort orders: first, the columns\n                    are always sorted by name, and second, the row sort orders work only\n                    if an order-preserving partitioner (see <a href=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/index.html#artrelatedtopics\">Related topics</a>) is used. </li><li> Adding a new query often needs new indexes and code changes unlike\n                    with relational models. This requires developers to analyze queries\n                    before storing the data. </li></ol><h3 id=\"N1025F\" class=\"ibm-h3\">Super columns and order preserving\n                partitioners are discouraged</h3><p>Cassandra super columns can be useful when modeling multi-level data, where\n                it adds one more level to the hierarchy. Anything that can be modeled with\n                super columns, however, can also be supported through columns. Hence,\n                super columns do not provide additional power. Also, they do not support\n                secondary indexes. Therefore, the Cassandra developers discourage the use\n                of super columns. Although there is no firm date for discontinuing\n                support, it might happen in future releases. </p><p>A partitioner in Cassandra decides how to distribute (shard) data among\n                Cassandra nodes, and there are many implementations. If an\n                order-preserving partitioner is used, rowIDs are stored in a sorted order\n                and Cassandra can do slices (searches) across rowIDs as well. This\n                partitioner does not distribute the data uniformly among its nodes,\n                however, and with large datasets, some of the nodes might be hard-pressed\n                while others are lightly loaded. Therefore, developers also discourage the\n                use of order-preserving partitioners.</p><h3 id=\"N10268\" class=\"ibm-h3\">Healing from failure is\n                manual</h3><p>If a node in a Cassandra cluster has failed, the cluster will continue to\n                work if you have replicas. Full recovery, which is to redistribute data\n                and compensate for missing replicas, is a manual operation through a\n                command line tool called <em>node tool</em> (see <a href=\"https://www.ibm.com/developerworks/library/os-apache-cassandra/index.html#artrelatedtopics\">Related topics</a>). Also, while the manual\n                operation happens, the system will be unavailable. </p><h3 id=\"N10276\" class=\"ibm-h3\">It remembers deletes</h3><p>Cassandra is designed such that it continues to work without a problem even\n                if a node goes down (or gets disconnected) and comes back later. A\n                consequence is this complicates data deletions. For example, assume a node\n                is down. While down, a data item has been deleted in replicas. When the\n                unavailable node comes back on, it will reintroduce the deleted data item\n                at the syncing process unless Cassandra remembers that data item has been\n                deleted. </p><p>Therefore, Cassandra has to remember that the data item has been deleted.\n                In the 0.8 release, Cassandra was remembering all the data even if it is\n                deleted. This caused disk usage to keep growing for update-intensive\n                operations. Cassandra does not have to remember all the deleted data, but\n                just the fact that a data item has been deleted. This fix was done in\n                later releases of Cassandra. </p><h2 id=\"7Conclusionoutline\" class=\"ibm-h2\">Conclusion</h2><p>This article delves into some details that are not readily apparent when\n                you consider Cassandra. I described the Cassandra data model, comparing it\n                with the relational data model, and demonstrated a typical schema design\n                with Cassandra. A key observation is that unlike the relational model that\n                breaks data into many tables, Cassandra tends to keep as much as data as\n                possible within the same row to avoiding having to join that data for\n                retrieval.</p><p>You also looked at several limitations of the Cassandra-based approach.\n                These limitations, however, are common to most NoSQL solutions, and are\n                often conscious design trade-offs to enable high scalability. </p><!--CMA ID: 823612--><!--Site ID: 1--><!--XSLT stylesheet used to transform this file: dw-document-html-8.0.xsl-->\n                                        <!-- Article Quiz -->\n                                        \n                                        <!-- Article Resources -->\n                                        <h4 id=\"artdownload\" class=\"ibm-h4\">Downloadable resources</h4><h4 id=\"artrelatedtopics\" class=\"ibm-h4\">Related topics</h4><ul><li>Read <a href=\"http://idke.ruc.edu.cn/seminars/phd/2007/11.07/What%20Goes%20Around%20Comes%20Around.pdf\">What goes around comes around</a> (Michael Stonebraker and Joey\n                    Hellerstein, 2007), if you are interested about the history of storage\n                    technologies.</li><li>Read <a href=\"http://wiki.apache.org/cassandra/GettingStarted\">Getting Started Page</a> in the Cassandra Wiki to install\n                    Cassandra, run single node Cassandra, and find an overview of how to\n                    configure multinode clusters.</li><li>Read the paper, <a href=\"http://www.cs.cornell.edu/Projects/ladis2009/papers/Lakshman-ladis2009.PDF\">Cassandra - A Decentralized Structured Storage System</a>\n                    (Avinash Lakshman and Prashant Malik, 2009) to understand the\n                    Cassandra architecture in more detail.</li><li>Read more about Google's <a href=\"http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/archive/bigtable-osdi06.pdf\">Big Table</a> and Amazon's <a href=\"http://db.cs.pitt.edu/courses/cs3551/11-1/handouts/10-1.1.1.115.1568.pdf\">Dynamo</a>.</li><li>Read about Eric Brewer's <a href=\"http://www.julianbrowne.com/article/viewer/brewers-cap-theorem\">CAP theorem</a> (Julian Browne, January 2009).</li><li>See <a href=\"http://www.infoq.com/articles/perera-data-storage-haystack\">Finding the Right Data Solution for Your Application in the Data\n                        Storage Haystack</a> (Srinath Perera, InfoQ, October 2011) for an\n                    overview of NoSQL landscape and recommendations on how to choose the\n                    right NoSQL storage.</li><li>Explore <a href=\"http://cassandra.apache.org/\">Cassandra</a>\n                    on the project website.</li><li>Check out the <a href=\"https://github.com/rantav/hector\">Hector client</a> on the project website.</li><li>Learn more about <a href=\"http://en.wikipedia.org/wiki/Disk-drive_performance_characteristics\">Disk-drive performance characteristics</a> on\n                    Wikipedia.</li><li>Read about <a href=\"http://wiki.apache.org/cassandra/NodeTool\">node tool</a>, a simple command line interface to these exposed\n                    operations and attributes on the Cassandra Wiki.</li><li>Find more details about consistency levels in <a href=\"http://wiki.apache.org/cassandra/API\">the Casandra API\n                        wiki</a>. </li><li>Read more about <a href=\"http://wiki.apache.org/cassandra/StorageConfiguration\">storage configurations</a>.</li><li><a href=\"http://cassandra.apache.org/\">Download Cassandra</a>\n                    and find instructions on how to use it at\n                    cassandra.apache.org.</li><li><a href=\"https://www.ibm.com/developerworks/develop\">Start developing</a> with product trials, free downloads, and IBM\n                    Bluemix services.</li></ul><!-- Commenting --><!-- INLINE_COMMENTS_BEGIN: -->\n<div id=\"dw-article-cmts-top\" class=\"ibm-columns\" readability=\"35\">\n    <div class=\"ibm-col-6-2\" readability=\"4\">\n        \n        <div id=\"dw-article-cmts-login\" readability=\"26\">\n            <p><a onclick=\"window.location=userLinks[0].url;\" tabindex=\"0\" role=\"link\">Sign in</a> or \n                <a onclick=\"window.location=userLinks[1].url;\" tabindex=\"0\" role=\"link\">register</a> to add and subscribe to comments.</p>\n        </div>\n    </div>    \n    <p> \n        <label for=\"comment_notification\">Subscribe me to comment notifications</label>\t   \n    </p>\n</div>\n<!-- INLINE_COMMENTS_END -->                                        <!-- CENTER_6_4_CONTENT_COLUMN_END -->",
        "created_at": "2017-10-17T15:43:53+0000",
        "updated_at": "2017-12-27T13:35:20+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 22,
        "domain_name": "www.ibm.com",
        "preview_picture": "http://www.ibm.com/developerworks/library/os-apache-cassandra/figure001.gif",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/5121"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 23,
            "label": "elasticsearch",
            "slug": "elasticsearch"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 869,
            "label": "devops",
            "slug": "devops"
          }
        ],
        "is_public": false,
        "id": 5118,
        "uid": null,
        "title": "Multiple Data Center Deployment",
        "url": "https://docs.genesys.com/Documentation/HTCC/latest/Dep/multiDCDepExample",
        "content": "<div id=\"mw-content-text\" lang=\"en\" dir=\"ltr\" class=\"mw-content-ltr\" readability=\"159\"><div id=\"toc\" class=\"toc\"><p><h2>Contents</h2></p>\n<ul><li class=\"toclevel-1 tocsection-1\"><a href=\"https://docs.genesys.com/Documentation/HTCC/latest/Dep/multiDCDepExample#Multiple_Data_Center_Deployment\">1 Multiple Data Center Deployment</a>\n<ul><li class=\"toclevel-2 tocsection-2\"><a href=\"https://docs.genesys.com/Documentation/HTCC/latest/Dep/multiDCDepExample#Overview\">1.1 Overview</a></li>\n<li class=\"toclevel-2 tocsection-3\"><a href=\"https://docs.genesys.com/Documentation/HTCC/latest/Dep/multiDCDepExample#Architecture\">1.2 Architecture</a></li>\n<li class=\"toclevel-2 tocsection-4\"><a href=\"https://docs.genesys.com/Documentation/HTCC/latest/Dep/multiDCDepExample#Incoming_traffic_distribution\">1.3 Incoming traffic distribution</a></li>\n<li class=\"toclevel-2 tocsection-5\"><a href=\"https://docs.genesys.com/Documentation/HTCC/latest/Dep/multiDCDepExample#Configuration\">1.4 Configuration</a></li>\n<li class=\"toclevel-2 tocsection-10\"><a href=\"https://docs.genesys.com/Documentation/HTCC/latest/Dep/multiDCDepExample#GWS_Cluster_Management\">1.5 GWS Cluster Management</a></li>\n</ul></li>\n</ul></div>\n<p>Starting in release 8.5.2, GWS supports a deployment with multiple (two or more) data centers. This section describes this type of deployment.\n</p>\n<h2>Overview</h2>\n<p>A multiple data center deployment implies a logical partitioning of all GWS nodes into segregated groups that are using dedicated service resources, such as T-Server, StatServers, and so on.\n</p><p>The topology of a GWS Cluster can be considered as a standard directory tree where a leaf node is a GWS data center. The following diagram shows a GWS Cluster with 2 geographical regions (US and EU), and 3 GWS data centers (East and West in the US region, and EU as its own data center).\n</p>\n<div class=\"center\"><div class=\"floatnone\"><a href=\"https://docs.genesys.com/File:Gws_multidatacenters-topo_851.png\" class=\"image\"><img alt=\"Gws multidatacenters-topo 851.png\" src=\"https://docs.genesys.com/images/3/34/Gws_multidatacenters-topo_851.png\" width=\"200\" height=\"174\"/></a></div></div>\n<p>For data handling and distribution between GWS data centers, the following third-party applications are used:\n</p>\n<ul><li> Cassandra—a NoSQL database cluster with multiple data centers with data replication between each other.</li>\n<li> Elasticsearch—a search engine which provides fast and efficient solution for pattern searching across Cassandra data. Genesys recommends that each GWS data center have an independent, standalone Elasticsearch cluster.</li></ul><h2>Architecture</h2>\n<p>A typical GWS data center in a multiple data center deployment consists of the following components:\n</p>\n<ul><li> 2 GWS API nodes</li>\n<li> 2 GWS Stat nodes</li>\n<li> 3 Cassandra nodes</li>\n<li> 3 Elasticsearch nodes</li>\n<li> 1 GWS Sync node (only for Primary region)</li></ul><p>The following diagram illustrates the architecture of this sample multiple data center deployment.\n</p>\n<div class=\"center\"><div class=\"thumb tnone\"><div class=\"thumbinner\"><a href=\"https://docs.genesys.com/File:Gws-multidc-arch-852.png\" class=\"image\"><img alt=\"Gws-multidc-arch-852.png\" src=\"https://docs.genesys.com/images/a/a3/Gws-multidc-arch-852.png\" width=\"300\" height=\"170\" class=\"thumbimage\"/></a>  </div></div></div>\n<p>Note the following restrictions of this architecture:\n</p>\n<ul><li> Only 1 Sync node is deployed within a GWS Cluster</li>\n<li> Each data center must have a dedicated list of Genesys servers, such as Configuration Servers, Stat Servers, and T-Servers.</li>\n<li> The Cassandra Keyspace definition must comply with the number of GWS data centers.</li>\n<li> Each GWS data center must have its own standalone and dedicated Elasticsearch Cluster.</li>\n<li> The GWS node identity must be unique across the entire Cluster.</li></ul><h2>Incoming traffic distribution</h2>\n<p>GWS does not support traffic distribution between GWS nodes natively. To enable this, any third-party reverse proxy can be used that can provide a session stickiness based on association with sessions on the backend server; this rule is commonly referred to as <i>Application-Controlled Session Stickiness</i>. In other words, when a GWS node creates a new session and returns <tt>Set-Cookie</tt> in response, the load-balancer should issue its own stickiness cookie. GWS uses a <tt>JSESSIONID</tt> cookie by default, but this can be reconfigured by using the following option in the <b>application.yaml</b> file:\n</p>\n<div dir=\"ltr\" readability=\"5\"><div class=\"source-text\" readability=\"31\"><pre>jetty:\n  cookies:\n    name: &lt;HTTP Session Cookie Name&gt;</pre></div></div>\n<h2>Configuration</h2>\n<p>This section describes the additional configuration required to set up a multiple data center deployment.\n</p>\n<h3>Cassandra</h3>\n<p>Configure Cassandra in the same way as for a single data center deployment (described earlier in this document), making sure that the following conditions are met:\n</p>\n<ul><li> All Cassandra nodes must have the same cluster name in <b>application.yaml</b>.</li>\n<li> The same data center name must be assigned to all Cassandra nodes across the GWS data center (specified in <b>cassandra-network.properties</b> or  <b>cassandra-rackdc.properties</b>, depending on the Cassandra deployment).</li>\n<li> The Keyspace definition must be created based on <b>ks-schema-prod_HA.cql</b> from the Installation Package, changing only the following:\n<ul><li> The name and <tt>ReplicationFactor</tt> of each.</li>\n<li> The number of data centers between which the replication is enabled.</li>\n</ul><p>For example:\n</p>\n<div dir=\"ltr\" readability=\"7\"><div class=\"source-text\" readability=\"35\"><pre>CREATE KEYSPACE sipfs WITH replication = {'class': 'NetworkTopologyStrategy', 'USWest': '3', 'USEast': '3', 'EU': '3'} AND durable_writes = true;</pre></div></div></li>\n</ul><h3>Genesys Web Services and Applications</h3>\n<p>The position of each node inside the GWS Cluster is specified by the mandatory property <b>nodePath</b> provided in <b>application.yaml</b>. The value of this property is in the standard file path format, and uses the forward slash (<tt>/</tt>) symbol as a delimiter. This property has the following syntax:\n</p>\n<div dir=\"ltr\" readability=\"5\"><div class=\"source-text\" readability=\"31\"><pre>nodePath: &lt;path-to-node-in-cluster&gt;/&lt;node-identity&gt;</pre></div></div>\n<p>Where:\n</p>\n<ul><li> <tt>&lt;path-to-node-in-cluster&gt;</tt> is the path inside the cluster with all logical sub-groups.</li>\n<li> &lt;node-identity&gt; is the unique identity of the node. Genesys recommends that you use the name of the host on which this data center is running for this parameter.</li></ul><p>For example:\n</p>\n<div dir=\"ltr\" readability=\"5\"><div class=\"source-text\" readability=\"31\"><pre>nodePath: /US/West/api-node-1</pre></div></div>\n<p>In addition to the configuration options set in the standard deployment procedure, set the following configuration options in <b>application.yaml</b> for all GWS nodes to enable the multiple data center functionality:\n</p>\n<div dir=\"ltr\" readability=\"6\"><div class=\"source-text\" readability=\"33\"><pre>cassandraCluster:\n  write_consistency_level: CL_LOCAL_QUORUM\n  read_consistency_level: CL_LOCAL_QUORUM\n \nserverSettings:\n nodePath: &lt;path-to-node-in-cluster&gt;/&lt;node-identity&gt;\n \nstatistics:\n locationAwareMonitoringDistribution: true\n enableMultipleDataCenterMonitoring: true</pre></div></div>\n<div class=\"new-note Important\"><p>Important</p>\n<ul><li> If the replication factor is changed in the keyspace definition (that is, if additional Cassandra nodes are added) then <tt>replication_factor</tt> in <tt>cassandraCluster</tt> of <b>application.yaml</b> should be adjusted to agree with the keyspace definition.</li></ul>\n</div>\n<p>In addition, set the following options on all Stat nodes:\n</p>\n<div dir=\"ltr\" readability=\"6\"><div class=\"source-text\" readability=\"32\"><pre>serverSettings:\n   elasticSearchSettings:\n      enableScheduledIndexVerification: true\n      enableIndexVerificationAtStartUp: true</pre></div></div>\n<h3>GWS Sync Node</h3>\n<p>A Synchronization Node is a special node, and as indicated elsewhere in this Guide, in Deployment Guide, this node imports existing data from Configuration Server and keeps track of all changes. \n</p>\n<div class=\"new-note Warning\" readability=\"10\"><p>Warning</p>Only one Sync Node can be running at any point of time even if you have two sync nodes in your architecture. Genesys strongly recommends that you deploy Sync node in the same data center where the primary Configuration Server is located.</div>\n<p>If any disaster causes this node to terminate or become unavailable because of network issues or the whole data center goes down, provisioning of any object in Configuration Server will not be reflected in the GWS cluster until the Sync node is recovered. Other functionality related to Agent activity is not affected in this case.\n</p>\n<h3>Configuration Server</h3>\n<p>The GWS Cluster Application object (typically named <b>CloudCluster</b>) in the Configuration Database must be configured with a specified location for each connection to Genesys servers, like Configuration Server, Stat Server, T-Server, and so on. This setting defines which server instance is used by the GWS node based on its position in the GWS Cluster. The visibility resource rule is based on comparing the <b>nodePath</b> attribute and the specified specification in connections. \n</p><p>Set these locations as Application Parameters of each connection, as follows:\n</p>\n<div dir=\"ltr\" readability=\"5\"><div class=\"source-text\" readability=\"31\"><pre>locations=&lt;path-to-node-in-cluster&gt;</pre></div></div>\n<p>where <tt>&lt;path-to-node-in-cluster&gt;</tt> is the same path to the data center specified by the <b>nodePath</b> property in <b>application.yaml</b>.\n</p><p>For example:\n</p>\n<div dir=\"ltr\"><div class=\"source-text\"><pre>locations=/US/West</pre></div></div>\n<h2>GWS Cluster Management</h2>\n<h3>Add a New Data Center</h3>\n<p>Before deploying new GWS nodes, you must extend the Cassandra cluster by adding new nodes into the data ring and updating the keyspace definition with a replication strategy for this new data center. Using the CQLSH utility, run the following command to update the existing Cassandra keyspace:\n</p>\n<div dir=\"ltr\" readability=\"8\"><div class=\"source-text\" readability=\"36\"><pre>ALTER KEYSPACE sipfs WITH REPLICATION = {'class': 'NetworkTopologyStrategy', 'USWest': '3', 'USEast': '3', 'EU': '3', 'CA': '3'};</pre></div></div>\n<p>After you have deployed the new Cassandra data center, you can use the normal procedure to deploy additional GWS nodes.\n</p>\n<h3>Remove an Existing Data Center</h3>\n<p>Before removing a Cassandra data center, you must stop all GWS nodes in this data center to avoid writing data into Cassandra.\n</p>\n<ol><li> Stop all GWS nodes, and remove them if necessary.</li> \n<li> Update the keyspace definition by removing the appropriate data center from the replication strategy. Use the same CQL command as you used for adding a new data center.</li>\n<li> Run the following command on each Cassandra node in the data center being removed:\n<div dir=\"ltr\" readability=\"5\"><div class=\"source-text\" readability=\"31\"><pre>nodetool decommission</pre></div></div></li>\n<li> Stop all Cassandra nodes, and remove them if necessary.</li>\n</ol></div><p> This page was last modified on 30 November 2017, at 09:41.</p>",
        "created_at": "2017-10-19T03:27:18+0000",
        "updated_at": "2017-12-27T13:30:40+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 6,
        "domain_name": "docs.genesys.com",
        "preview_picture": "https://docs.genesys.com/images/3/34/Gws_multidatacenters-topo_851.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/5118"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 869,
            "label": "devops",
            "slug": "devops"
          },
          {
            "id": 907,
            "label": "mesos",
            "slug": "mesos"
          }
        ],
        "is_public": false,
        "id": 5117,
        "uid": null,
        "title": "Cassandra on Mesos Across Multiple Datacenters at Uber (Abhishek Verma) | C* Summit 2016",
        "url": "https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016",
        "content": "<head prefix=\"og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# 2490221586: http://ogp.me/ns/fb/2490221586#\"><meta charset=\"utf-8\"/><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, maximum-scale=1, user-scalable=no\"/><meta name=\"include_mode\" content=\"async\"/><!-- SL:start:notranslate --><title>Cassandra on Mesos Across Multiple Datacenters at Uber (Abhishek Verm…</title><meta name=\"description\" content=\"Traditionally, machines were statically partitioned across the different services at Uber. In an effort to increase the machine utilization, Uber has recently …\"/><!-- SL:end:notranslate --><meta name=\"robots\" content=\"index\"/><meta id=\"globalTrackingUrl\" content=\"https://www.linkedin.com/li/track\"/><!-- SL:start:notranslate --><meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"/><meta http-equiv=\"x-dns-prefetch-control\" content=\"on\"/><meta name=\"thumbnail\" content=\"https://cdn.slidesharecdn.com/ss_thumbnails/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137-thumbnail.jpg?cb=1474064812\"/><!-- SL:end:notranslate --><meta content=\"{&quot;lynda.domain&quot;:&quot;lynda&quot;,&quot;lynda.premium_video&quot;:&quot;PREMIUM VIDEO&quot;,&quot;lynda.tld&quot;:&quot;.com&quot;,&quot;right_rail.recommended&quot;:&quot;Recommended&quot;,&quot;views.other&quot;:&quot;views&quot;,&quot;share&quot;:&quot;Share&quot;,&quot;select&quot;:&quot;Select&quot;,&quot;selected&quot;:&quot;Selected&quot;,&quot;clipping.select_clipboard_modal.select_another_clipboard&quot;:&quot;Select another clipboard&quot;,&quot;clipping.select_clipboard_modal.select_new_clipboard&quot;:&quot;Select a new clipboard&quot;,&quot;clipping.toast.change_clipboard&quot;:&quot;Change clipboard&quot;,&quot;clipping.toast.share_clip&quot;:&quot;Share clip&quot;,&quot;li_connect.slideshare_added&quot;:&quot;Your SlideShare was successfully added to your LinkedIn profile.&quot;,&quot;li_connect.login.no_match&quot;:&quot;Login does not match, please try again.&quot;,&quot;ajax_signup.login.download&quot;:&quot;Login to SlideShare to download\\u2026&quot;,&quot;ajax_signup.login.addcontact&quot;:&quot;Login to SlideShare to follow\\u2026&quot;,&quot;ajax_signup.login.favorite&quot;:&quot;Login to SlideShare to like\\u2026&quot;,&quot;ajax_signup.login.comments&quot;:&quot;Login to SlideShare to post a comment\\u2026&quot;,&quot;ajax_signup.login.AddToCommunity&quot;:&quot;Login to SlideShare to add this document to a group/event\\u2026&quot;,&quot;ajax_signup.login.follow&quot;:&quot;Login to SlideShare to follow this user\\u2026&quot;,&quot;ajax_signup.login.business&quot;:&quot;Login to SlideShare to continue\\u2026&quot;,&quot;ajax_signup.login.upload&quot;:&quot;Login to SlideShare to start uploading\\u2026&quot;,&quot;ajax_signup.login.contest&quot;:&quot;Login to SlideShare to vote\\u2026&quot;,&quot;ajax_signup.login.rsvp&quot;:&quot;Login to SlideShare to join this meeting\\u2026&quot;,&quot;ajax_signup.login.user&quot;:&quot;Login to SlideShare to register the username\\u2026&quot;,&quot;ajax_signup.login.zipcast&quot;:&quot;Login to SlideShare to schedule a meeting\\u2026&quot;,&quot;ajax_signup.login.create&quot;:&quot;Login to SlideShare to start creating\\u2026&quot;,&quot;ajax_signup.login.clip&quot;:&quot;Login to SlideShare. Don\\u2019t lose your clips!&quot;,&quot;ajax_signup.signup.download&quot;:&quot;Signup for SlideShare to download\\u2026&quot;,&quot;ajax_signup.signup.addcontact&quot;:&quot;Signup for SlideShare to follow\\u2026&quot;,&quot;ajax_signup.signup.favorite&quot;:&quot;Signup for SlideShare to like\\u2026&quot;,&quot;ajax_signup.signup.comments&quot;:&quot;Signup for SlideShare to post a comment\\u2026&quot;,&quot;ajax_signup.signup.AddToCommunity&quot;:&quot;Signup for SlideShare to add this document to a group/event\\u2026&quot;,&quot;ajax_signup.signup.follow&quot;:&quot;Signup for SlideShare to follow this user\\u2026&quot;,&quot;ajax_signup.signup.business&quot;:&quot;Signup for SlideShare to continue\\u2026&quot;,&quot;ajax_signup.signup.upload&quot;:&quot;Signup for SlideShare to start uploading\\u2026&quot;,&quot;ajax_signup.signup.contest&quot;:&quot;Signup for SlideShare to vote\\u2026&quot;,&quot;ajax_signup.signup.rsvp&quot;:&quot;Signup for SlideShare to join this meeting\\u2026&quot;,&quot;ajax_signup.signup.user&quot;:&quot;Signup for SlideShare to register the username\\u2026&quot;,&quot;ajax_signup.signup.zipcast&quot;:&quot;Signup for SlideShare to schedule a meeting\\u2026&quot;,&quot;ajax_signup.signup.create&quot;:&quot;Signup for SlideShare to start creating\\u2026&quot;,&quot;ajax_signup.signup.clip&quot;:&quot;Signup for SlideShare. Don\\u2019t lose your clips!&quot;,&quot;ajax_signup.connect&quot;:&quot;connect&quot;,&quot;ajax_signup.signup_for_ss&quot;:&quot;Signup for SlideShare&quot;,&quot;ajax_signup.login_to_ss&quot;:&quot;Login to SlideShare&quot;,&quot;clipping.filmstrip.message&quot;:&quot;Top clipped slide&quot;,&quot;slideview.comments_loggedin.delete_comment.title&quot;:&quot;Are you sure you want to delete this comment?&quot;,&quot;slideview.comments_loggedin.spam_comment.title&quot;:&quot;Are you sure you want to mark this comment as spam?&quot;,&quot;slideview.comments_loggedin.block_user.title&quot;:&quot;Are you sure you want to block this user?&quot;,&quot;slideview.comments_loggedin.block_user.success&quot;:&quot;The user is blocked!&quot;,&quot;slideview.comments_loggedin.block_user.failure&quot;:&quot;Oops! Something went wrong. Please try again after sometime.&quot;,&quot;slideview.comments_loggedin.organization&quot;:&quot;at {:organization}&quot;,&quot;slideview.comments_loggedin.timestamp&quot;:&quot;less than a second ago&quot;,&quot;slideview.comments_loggedin.commenting.failure&quot;:&quot;Hey something went wrong :-(\\nCould you try again?&quot;,&quot;slideshow.foundation_slideview.likes_count&quot;:&quot;{:count,number} {:count,choice,singular#Like|plural#Likes}&quot;,&quot;action-bar.like.why_not_share&quot;:&quot;Like this? Why not share?&quot;,&quot;slideshow.foundation_slideview.like&quot;:&quot;Like&quot;,&quot;action-bar.unlike.text&quot;:&quot;Unlike&quot;}\" name=\"ss-i18n-translations\"/><meta content=\"{&quot;ssplayer.event_manager.playershare.share_slideshare&quot;:&quot;Share SlideShare&quot;,&quot;profile.edit_marketing_settings.saved&quot;:&quot;Saved&quot;,&quot;slideshow.foundation_slideview.loading&quot;:&quot;Loading\\u2026&quot;,&quot;clip.slideview.share_message&quot;:&quot;Share Slide {0,number} from {1,text}&quot;,&quot;clip.slideview.success_toast.message&quot;:&quot;Slide {0,number} clipped to: \\u003Cbr /\\u003E {1,anchor,text#{2,text}}&quot;,&quot;clip.clips.error_clipping&quot;:&quot;Error while clipping.&quot;,&quot;clip.clips.error_fetching_clipboards&quot;:&quot;Error fetching clipboards.&quot;,&quot;clip.clips.error_creating_clipboard&quot;:&quot;Error creating a clipboard.&quot;,&quot;clip.clips.error_adding_to_clipboard&quot;:&quot;Error adding to clipboard.&quot;}\" name=\"ss-i18n-translations\"/><!--[if IE 9]><link href=\"https://public.slidesharecdn.com/ss_foundation/stylesheets/ie9_nav_bar_fix.css?8fb8af5274\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\" /><![endif]--><meta content=\"index\" name=\"robots\"/><meta content=\"https://public.slidesharecdn.com/images/artdeco/icons.svg?43e81fd2ef\" name=\"ss-svg-icons\"/><!-- RUM javascript --><!-- RUM javascript ends --><meta name=\"apple-itunes-app\" content=\"app-id=917418728, affiliate-data=ct=smart_banner&amp;pt=10746, app-argument=slideshare-app://ss/66106372\"/><!-- fb open graph meta tags --><!-- SL:start:notranslate --><!-- SL:end:notranslate --><!-- SL:start:notranslate --><meta name=\"twitter:card\" value=\"player\"/><meta name=\"twitter:site\" value=\"@slideshare\"/><meta name=\"twitter:player:width\" value=\"342\"/><meta name=\"twitter:player:height\" value=\"291\"/><meta name=\"twitter:app:name:googleplay\" content=\"SlideShare Android\"/><meta name=\"twitter:app:id:googleplay\" content=\"net.slideshare.mobile\"/><meta name=\"twitter:app:url:googleplay\" content=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\"/><meta name=\"twitter:app:name:iphone\" content=\"SlideShare iOS\"/><meta name=\"twitter:app:id:iphone\" content=\"917418728\"/><meta name=\"twitter:app:url:iphone\" content=\"slideshare-app://ss/66106372\"/><meta name=\"twitter:app:name:ipad\" content=\"SlideShare iOS\"/><meta name=\"twitter:app:id:ipad\" content=\"917418728\"/><meta name=\"twitter:app:url:ipad\" content=\"slideshare-app://ss/66106372\"/><meta property=\"al:android:url\" content=\"slideshare-app://ss/66106372\"/><meta property=\"al:android:app_name\" content=\"SlideShare Android\"/><meta property=\"al:android:package\" content=\"net.slideshare.mobile\"/><meta property=\"al:ios:url\" content=\"slideshare-app://ss/66106372\"/><meta property=\"al:ios:app_store_id\" content=\"917418728\"/><meta property=\"al:ios:app_name\" content=\"SlideShare iOS\"/><!-- SL:end:notranslate --></head><body id=\"pagekey-slideshare_desktop_slideview_loggedout\" class=\"readabilityBody\" readability=\"48\">\n      <!-- TOS update banner -->\n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\" readability=\"0\">\n        <p>\n          <a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            <label id=\"home\">SlideShare</label>\n          </a>\n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\"/>\n            <label id=\"explore\">Explore</label>\n          </a>\n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\"/>\n              <label id=\"you\">You</label>\n            </a>\n        </p>\n        \n      </div>\n    <div class=\"wrapper\">\n        \n        \n      \n      \n<div id=\"slideview-container\" class=\"\">\n  \n  <div class=\"row\">\n    <div id=\"main-panel\" class=\"small-12 large-8 columns\">\n      <div class=\"sectionElements\">\n        <div class=\"playerWrapper\">\n            <div readability=\"5\">\n              <!-- For slideview page , combined js for player is now combined with slideview javascripts for logged out users-->\n<div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\" readability=\"6\">\n  \n    Cassandra on Mesos Across Multiple Datacenters at Uber (Abhishek Verma) | C* Summit 2016\n  \n  <div class=\"stage valign-first-slide\">\n    \n    \n    <div class=\"slide_container\">\n            <section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-1-638.jpg?cb=1474064812\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-1-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-1-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-1-1024.jpg?cb=1474064812\" alt=\"Running Cassandra on Apache Mesos&#10;across multiple datacenters at Uber&#10;Abhishek Verma (verma@uber.com)&#10; \"/></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-2-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-2-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-2-1024.jpg?cb=1474064812\" alt=\"About me&#10;● MS (2010) and PhD (2012) in Computer Science from University of Illinois&#10;at Urbana-Champaign&#10;● 2 years at Googl...\"/></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-3-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-3-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-3-1024.jpg?cb=1474064812\" alt=\"“Transportation as reliable as running water,&#10;everywhere, for everyone”&#10; \"/></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-4-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-4-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-4-1024.jpg?cb=1474064812\" alt=\"“Transportation as reliable as running&#10;water, everywhere, for everyone”&#10;99.99%&#10; \"/></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-5-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-5-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-5-1024.jpg?cb=1474064812\" alt=\"“Transportation as reliable as running water,&#10;everywhere, for everyone”&#10;efficient&#10; \"/></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-6-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-6-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-6-1024.jpg?cb=1474064812\" alt=\"Cluster Management @ Uber&#10;● Statically partitioned machines across different services&#10;● Move from custom deployment system...\"/></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-7-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-7-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-7-1024.jpg?cb=1474064812\" alt=\"Apache Mesos&#10;7&#10;● Mesos abstracts CPU, memory, storage away from machines&#10;○ program like it’s a single pool of resources&#10;● ...\"/></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-8-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-8-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-8-1024.jpg?cb=1474064812\" alt=\"Apache Cassandra&#10;8&#10;● Horizontal scalability&#10;○ Scales reads and writes linearly as new nodes are added&#10;● High availability&#10;...\"/></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-9-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-9-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-9-1024.jpg?cb=1474064812\" alt=\"Uber&#10;● Abhishek Verma&#10;● Karthik Gandhi&#10;● Matthias Eichstaedt&#10;● Varun Gupta&#10;● Zhitao Li&#10;DC/OS Cassandra Service&#10;9&#10;Mesospher...\"/></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-10-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-10-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-10-1024.jpg?cb=1474064812\" alt=\"Cassandra service architecture&#10;10&#10;Framework&#10;dcos-cassandra-service&#10;Mesos agent&#10;Mesos master&#10;(Leader)&#10;Web interface&#10;Control...\"/></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-11-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-11-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-11-1024.jpg?cb=1474064812\" alt=\"Cassandra Mesos primitives&#10;11&#10;● Mesos containerizer&#10;● Override 5 ports in configuration (storage_port,&#10;ssl_storage_port, n...\"/></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-12-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-12-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-12-1024.jpg?cb=1474064812\" alt=\"Custom seed provider&#10;12&#10;Node 1&#10;10.0.0.1&#10;http://scheduler/seeds&#10;{&#10;isSeed: true&#10;seeds: [ ]&#10;}&#10;Node 1&#10;10.0.0.1&#10;Node 2&#10;10.0.0.2...\"/></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-13-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-13-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-13-1024.jpg?cb=1474064812\" alt=\"Cassandra Service: Features&#10;13&#10;● Custom seed provider&#10;● Increasing cluster size&#10;● Changing Cassandra configuration&#10;● Repla...\"/></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-14-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-14-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-14-1024.jpg?cb=1474064812\" alt=\"Plan, Phases and Blocks&#10;14&#10;● Plan&#10;○ Phases&#10;■ Reconciliation&#10;■ Deployment&#10;■ Backup&#10;■ Restore&#10;■ Cleanup&#10;■ Repair&#10; \"/></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-15-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-15-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-15-1024.jpg?cb=1474064812\" alt=\"Spinning up a new Cassandra cluster&#10;15&#10;https://www.youtube.com/watch?v=gbYmjtDKSzs&#10; \"/></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-16-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-16-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-16-1024.jpg?cb=1474064812\" alt=\"Automate Cassandra operations&#10;16&#10;● Repair&#10;○ Synchronize all data across replicas&#10;■ Last write wins&#10;○ Anti-entropy mechanis...\"/></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-17-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-17-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-17-1024.jpg?cb=1474064812\" alt=\"Cleanup operation&#10;17&#10;https://www.youtube.com/watch?v=VxRLSl8MpYI&#10; \"/></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-18-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-18-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-18-1024.jpg?cb=1474064812\" alt=\"Failure scenarios&#10;18&#10;● Executor failure&#10;○ Restarted automatically&#10;● Cassandra daemon failure&#10;○ Restarted automatically&#10;● N...\"/></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-19-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-19-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-19-1024.jpg?cb=1474064812\" alt=\"Experiments&#10;19&#10; \"/></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-20-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-20-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-20-1024.jpg?cb=1474064812\" alt=\"Cluster startup&#10;20&#10;For each node in the cluster:&#10;1.Receive and accept offer&#10;2.Launch task&#10;3.Fetch executor, JRE, Cassandra...\"/></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-21-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-21-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-21-1024.jpg?cb=1474064812\" alt=\"Cluster startup time&#10;21&#10;Framework can start ~ one new node per minute&#10; \"/></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-22-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-22-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-22-1024.jpg?cb=1474064812\" alt=\"Tuning JVM Garbage collection&#10;22&#10;Changed from CMS to G1 garbage collector&#10;Left: https://github.com/apache/cassandra/blob/c...\"/></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-23-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-23-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-23-1024.jpg?cb=1474064812\" alt=\"Tuning JVM Garbage collection&#10;23&#10;Metric CMS G1&#10;G1 : CMS&#10;Factor&#10;op rate 1951 13765 7.06&#10;latency mean (ms) 3.6 0.4 9.00&#10;late...\"/></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-24-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-24-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-24-1024.jpg?cb=1474064812\" alt=\"Cluster Setup&#10;24&#10;● 3 nodes&#10;● Local DC&#10;● 24 cores, 128 GB RAM, 2TB SAS drives&#10;● Cassandra running on bare metal&#10;● Cassandra...\"/></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-25-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-25-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-25-1024.jpg?cb=1474064812\" alt=\"Bare metal Mesos&#10;Read Latency&#10;25&#10;Mean: 0.38 ms&#10;P95: 0.74 ms&#10;P99: 0.91 ms&#10;Mean: 0.44 ms&#10;P95: 0.76 ms&#10;P99: 0.98 ms&#10; \"/></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-26-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-26-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-26-1024.jpg?cb=1474064812\" alt=\"Bare metal Mesos&#10;Read Throughput&#10;26&#10; \"/></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-27-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-27-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-27-1024.jpg?cb=1474064812\" alt=\"Bare metal Mesos&#10;Write Latency&#10;27&#10;Mean: 0.43 ms&#10;P95: 0.94 ms&#10;P99: 1.05 ms&#10;Mean: 0.48 ms&#10;P95: 0.93 ms&#10;P99: 1.26 ms&#10; \"/></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-28-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-28-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-28-1024.jpg?cb=1474064812\" alt=\"Bare metal Mesos&#10;Write Throughput&#10;28&#10; \"/></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-29-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-29-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-29-1024.jpg?cb=1474064812\" alt=\"Running across datacenters&#10;29&#10;● Four datacenters&#10;○ Each running dcos-cassandra-service instance&#10;○ Sync datacenter phase&#10;■ ...\"/></section><section data-index=\"30\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-30-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-30-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-30-1024.jpg?cb=1474064812\" alt=\"Asynchronous cross-dc replication latency&#10;30&#10;● Write a row to dc1 using consistency level LOCAL_ONE&#10;○ Write timestamp to a...\"/></section><section data-index=\"31\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-31-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-31-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-31-1024.jpg?cb=1474064812\" alt=\"Cassandra on Mesos in Production&#10;31&#10;● ~20 clusters replicating across two datacenters (west and east coast)&#10;● ~300 machine...\"/></section><section data-index=\"32\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-32-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-32-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-32-1024.jpg?cb=1474064812\" alt=\"Questions?&#10;32&#10;verma@uber.com&#10; \"/></section><section data-index=\"33\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-33-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-33-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-33-1024.jpg?cb=1474064812\" alt=\"Cluster startup&#10;33&#10;For each node in the cluster:&#10;1.Receive and accept offer&#10;2.Launch task&#10;3.Fetch executor, JRE, Cassandra...\"/></section><section data-index=\"34\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-34-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-34-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-34-1024.jpg?cb=1474064812\" alt=\"Aurora hogs offers&#10;34&#10;● Aurora designed to be the only framework running on Mesos and&#10;controlling all the machines&#10;● Holds...\"/></section><section data-index=\"35\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-35-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-35-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-35-1024.jpg?cb=1474064812\" alt=\"Long term solution: dynamic reservations&#10;35&#10;● Dynamically reserve all the machines resources to the “cassandra”&#10;role&#10;● Res...\"/></section><section data-index=\"36\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"/>\n                <img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/85/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-36-320.jpg?cb=1474064812\" data-normal=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-36-638.jpg?cb=1474064812\" data-full=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-36-1024.jpg?cb=1474064812\" alt=\"Using the Cassandra cluster&#10;36&#10;https://www.youtube.com/watch?v=qgqO39DteHo&#10; \"/></section>\n        <div class=\"j-next-container next-container\">\n          <div class=\"content-container\">\n            <div class=\"next-slideshow-wrapper\">\n              <div class=\"j-next-slideshow next-slideshow\">\n                <p>\n                  Upcoming SlideShare\n                </p>\n                \n              </div>\n              <p>Loading in …5</p>\n              <p>×</p>\n            </div>\n          </div>\n        </div>\n    </div>\n  </div> <!-- end stage -->\n  \n  \n  \n</div>\n            </div>\n        </div>\n        \n      </div>\n      <div class=\"slideshow-info-container\" itemscope=\"\" itemtype=\"https://schema.org/MediaObject\" readability=\"6\">\n        \n        <div class=\"slideshow-tabs-container show-for-medium-up\">\n          <ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    0 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\"/>\n                \n                  19 Likes\n                \n              </a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\"/>\n                Statistics\n              </a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\"/>\n                Notes\n              </a>\n            </li>\n          </ul><div class=\"tabs-content\">\n              \n            <div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\">\n              <ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"\"><li itemtype=\"http://schema.org/Person\" itemscope=\"\">\n                      <div class=\"row\">\n                        \n                        <div class=\"small-11 columns\">\n                          <a class=\"favoriter notranslate\" title=\"MachhindraNale\" rel=\"nofollow\" href=\"https://www.slideshare.net/MachhindraNale?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Machhindra Nale\n                            \n                              \n                                , \n                                Principal Software Engineer at CA Technologies\n                              \n                              \n                                 at \n                                CA Technologies\n                              \n                            \n                            \n                            </a>\n                        </div>\n                      </div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"\">\n                      <div class=\"row\">\n                        \n                        <div class=\"small-11 columns\">\n                          <a class=\"favoriter notranslate\" title=\"gengmao\" rel=\"nofollow\" href=\"https://www.slideshare.net/gengmao?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Mao Geng\n                            \n                              \n                                , \n                                Site Reliability Engineer at Pinterest\n                              \n                              \n                                 at \n                                Pinterest\n                              \n                            \n                            \n                            </a>\n                        </div>\n                      </div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"\">\n                      <div class=\"row\">\n                        \n                        <div class=\"small-11 columns\">\n                          <a class=\"favoriter notranslate\" title=\"sheshech\" rel=\"nofollow\" href=\"https://www.slideshare.net/sheshech?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Lyunsik Hyun\n                            \n                              \n                                , \n                                Senior Software Engineer at Samsung Electronics | AWS Certified Solutions Architect - Professional\n                              \n                              \n                                 at \n                                Samsung Electronics\n                              \n                            \n                            \n                            </a>\n                        </div>\n                      </div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"\">\n                      <div class=\"row\">\n                        \n                        <div class=\"small-11 columns\">\n                          <a class=\"favoriter notranslate\" title=\"thinker0\" rel=\"nofollow\" href=\"https://www.slideshare.net/thinker0?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            se choi\n                            \n                              \n                                , \n                                Kakao Corp\n                              \n                              \n                                 at \n                                Kakao Corp\n                              \n                            \n                            \n                            </a>\n                        </div>\n                      </div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"\">\n                      <div class=\"row\">\n                        \n                        <div class=\"small-11 columns\">\n                          <a class=\"favoriter notranslate\" title=\"SendilKandhalu\" rel=\"nofollow\" href=\"https://www.slideshare.net/SendilKandhalu?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Sendil S\n                            \n                              \n                                , \n                                Technology Solutions Architect\n                              \n                              \n                                 at \n                                RenovITe Technologies Inc\n                              \n                            \n                            \n                            </a>\n                        </div>\n                      </div>\n                    </li>\n              </ul><div class=\"more-container text-center\">\n                  <a href=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016#\" class=\"j-more-favs\">\n                    Show More\n                    \n                  </a>\n                </div>\n            </div>\n            <div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\">\n                <p>No Downloads</p>\n            </div>\n            \n            <div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\">\n              <p>No notes for slide</p>\n            </div>\n          </div>\n        </div>\n            <div class=\"notranslate transcript add-padding-right j-transcript\">\n              \n              <ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    Running Cassandra on Apache Mesos\nacross multiple datacenters at Uber\nAbhishek Verma (verma@uber.com)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-2-638.jpg?cb=1474064812\" title=\"About me&#10;● MS (2010) and PhD (2012) in Computer Science fro...\" target=\"_blank\">\n        2.\n      </a>\n    About me\n● MS (2010) and PhD (2012) in Computer Science from University of Illinois\nat Urbana-Champaign\n● 2 years at Google, worked on Borg and Omega and first author of the\nBorg paper\n● ~ 1 year at TCS Research, Mumbai\n● Currently at Uber working on running Cassandra on Mesos\n© DataStax, All Rights Reserved. 2\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-3-638.jpg?cb=1474064812\" title=\"“Transportation as reliable as running water,&#10;everywhere, f...\" target=\"_blank\">\n        3.\n      </a>\n    “Transportation as reliable as running water,\neverywhere, for everyone”\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-4-638.jpg?cb=1474064812\" title=\"“Transportation as reliable as running&#10;water, everywhere, f...\" target=\"_blank\">\n        4.\n      </a>\n    “Transportation as reliable as running\nwater, everywhere, for everyone”\n99.99%\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-5-638.jpg?cb=1474064812\" title=\"“Transportation as reliable as running water,&#10;everywhere, f...\" target=\"_blank\">\n        5.\n      </a>\n    “Transportation as reliable as running water,\neverywhere, for everyone”\nefficient\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-6-638.jpg?cb=1474064812\" title=\"Cluster Management @ Uber&#10;● Statically partitioned machines...\" target=\"_blank\">\n        6.\n      </a>\n    Cluster Management @ Uber\n● Statically partitioned machines across different services\n● Move from custom deployment system to everything running on Mesos\n● Gain efficiency by increasing machine utilization\n○ Co-locate services on the same machine\n○ Can lead to 30% fewer machines1\n● Build stateful service frameworks to run on Mesos\n© DataStax, All Rights Reserved. 6\n“Large-scale cluster management at Google with Borg”, EuroSys 2015\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-7-638.jpg?cb=1474064812\" title=\"Apache Mesos&#10;7&#10;● Mesos abstracts CPU, memory, storage away ...\" target=\"_blank\">\n        7.\n      </a>\n    Apache Mesos\n7\n● Mesos abstracts CPU, memory, storage away from machines\n○ program like it’s a single pool of resources\n● Linear scalability\n● High availability\n● Native support for launching containers\n● Pluggable resource isolation\n● Two level scheduling\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-8-638.jpg?cb=1474064812\" title=\"Apache Cassandra&#10;8&#10;● Horizontal scalability&#10;○ Scales reads ...\" target=\"_blank\">\n        8.\n      </a>\n    Apache Cassandra\n8\n● Horizontal scalability\n○ Scales reads and writes linearly as new nodes are added\n● High availability\n○ Fault tolerant with tunable consistency levels\n● Low latency, solid performance\n● Operational simplicity\n○ Homogeneous cluster, no SPOF\n● Rich data model\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-9-638.jpg?cb=1474064812\" title=\"Uber&#10;● Abhishek Verma&#10;● Karthik Gandhi&#10;● Matthias Eichstaed...\" target=\"_blank\">\n        9.\n      </a>\n    Uber\n● Abhishek Verma\n● Karthik Gandhi\n● Matthias Eichstaedt\n● Varun Gupta\n● Zhitao Li\nDC/OS Cassandra Service\n9\nMesosphere\n● Chris Lambert\n● Gabriel Hartmann\n● Keith Chambers\n● Kenneth Owens\n● Mohit Soni\nhttps://github.com/mesosphere/dcos-cassandra-service\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-10-638.jpg?cb=1474064812\" title=\"Cassandra service architecture&#10;10&#10;Framework&#10;dcos-cassandra-...\" target=\"_blank\">\n        10.\n      </a>\n    Cassandra service architecture\n10\nFramework\ndcos-cassandra-service\nMesos agent\nMesos master\n(Leader)\nWeb interface\nControl plane API\nC*Cluster 1 C*Cluster 2\nAurora (DC1)\nMesos master\n(Standby)\nC*Node\n1a\nC*Node\n2a\nMesos agent\nC*Node\n1b\nC*Node\n2b\nMesos agent\nC*Node\n1c\nAurora (DC2)\nDeployment system\nDC2\nZK ZK\nZK\nZooKeeper\nquorum\nClient App\nuses CQL\ninterface\nCQL CQL CQL CQL CQL\n. . .\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-11-638.jpg?cb=1474064812\" title=\"Cassandra Mesos primitives&#10;11&#10;● Mesos containerizer&#10;● Overr...\" target=\"_blank\">\n        11.\n      </a>\n    Cassandra Mesos primitives\n11\n● Mesos containerizer\n● Override 5 ports in configuration (storage_port,\nssl_storage_port, native_transport_port, rpc_port, jmx_port)\n● Use persistent volumes\n○ Data stored outside of the sandbox directory\n○ Offered to the same task if it crashes and restarts\n● Use dynamic reservation\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-12-638.jpg?cb=1474064812\" title=\"Custom seed provider&#10;12&#10;Node 1&#10;10.0.0.1&#10;http://scheduler/se...\" target=\"_blank\">\n        12.\n      </a>\n    Custom seed provider\n12\nNode 1\n10.0.0.1\nhttp://scheduler/seeds\n{\nisSeed: true\nseeds: [ ]\n}\nNode 1\n10.0.0.1\nNode 2\n10.0.0.2\nNode 3\n10.0.0.3\nNode 2\n10.0.0.2\n{\nisSeed: true\nseeds: [ 10.0.0.1]\n}\n{\nisSeed: false\nseeds: [ 10.0.0.1,\n10.0.0.2]\n}\nNode 3\n10.0.0.3\nNumber of Nodes = 3\nNumber of Seeds = 2\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-13-638.jpg?cb=1474064812\" title=\"Cassandra Service: Features&#10;13&#10;● Custom seed provider&#10;● Inc...\" target=\"_blank\">\n        13.\n      </a>\n    Cassandra Service: Features\n13\n● Custom seed provider\n● Increasing cluster size\n● Changing Cassandra configuration\n● Replacing a dead node\n● Backup/Restore\n● Cleanup\n● Repair\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-14-638.jpg?cb=1474064812\" title=\"Plan, Phases and Blocks&#10;14&#10;● Plan&#10;○ Phases&#10;■ Reconciliation...\" target=\"_blank\">\n        14.\n      </a>\n    Plan, Phases and Blocks\n14\n● Plan\n○ Phases\n■ Reconciliation\n■ Deployment\n■ Backup\n■ Restore\n■ Cleanup\n■ Repair\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-15-638.jpg?cb=1474064812\" title=\"Spinning up a new Cassandra cluster&#10;15&#10;https://www.youtube....\" target=\"_blank\">\n        15.\n      </a>\n    Spinning up a new Cassandra cluster\n15\nhttps://www.youtube.com/watch?v=gbYmjtDKSzs\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-16-638.jpg?cb=1474064812\" title=\"Automate Cassandra operations&#10;16&#10;● Repair&#10;○ Synchronize all...\" target=\"_blank\">\n        16.\n      </a>\n    Automate Cassandra operations\n16\n● Repair\n○ Synchronize all data across replicas\n■ Last write wins\n○ Anti-entropy mechanism\n○ Repair primary key range node-by-node\n● Cleanup\n○ Remove data whose ownership has changed\n■ Because of addition or removal of nodes\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-17-638.jpg?cb=1474064812\" title=\"Cleanup operation&#10;17&#10;https://www.youtube.com/watch?v=VxRLSl...\" target=\"_blank\">\n        17.\n      </a>\n    Cleanup operation\n17\nhttps://www.youtube.com/watch?v=VxRLSl8MpYI\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-18-638.jpg?cb=1474064812\" title=\"Failure scenarios&#10;18&#10;● Executor failure&#10;○ Restarted automat...\" target=\"_blank\">\n        18.\n      </a>\n    Failure scenarios\n18\n● Executor failure\n○ Restarted automatically\n● Cassandra daemon failure\n○ Restarted automatically\n● Node failure\n○ Manual REST endpoint to replace node\n● Scheduling framework failure\n○ Existing nodes keep running, new nodes cannot be added\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-19-638.jpg?cb=1474064812\" title=\"Experiments&#10;19&#10; \" target=\"_blank\">\n        19.\n      </a>\n    Experiments\n19\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-20-638.jpg?cb=1474064812\" title=\"Cluster startup&#10;20&#10;For each node in the cluster:&#10;1.Receive ...\" target=\"_blank\">\n        20.\n      </a>\n    Cluster startup\n20\nFor each node in the cluster:\n1.Receive and accept offer\n2.Launch task\n3.Fetch executor, JRE, Cassandra binaries from S3/HDFS\n4.Launch executor\n5.Launch Cassandra daemon\n6.Wait for it’s mode to transition STARTING -&gt; JOINING -&gt; NORMAL\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-21-638.jpg?cb=1474064812\" title=\"Cluster startup time&#10;21&#10;Framework can start ~ one new node ...\" target=\"_blank\">\n        21.\n      </a>\n    Cluster startup time\n21\nFramework can start ~ one new node per minute\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-22-638.jpg?cb=1474064812\" title=\"Tuning JVM Garbage collection&#10;22&#10;Changed from CMS to G1 gar...\" target=\"_blank\">\n        22.\n      </a>\n    Tuning JVM Garbage collection\n22\nChanged from CMS to G1 garbage collector\nLeft: https://github.com/apache/cassandra/blob/cassandra-2.2/conf/cassandra-env.sh#L213\nRight: https://docs.datastax.com/en/cassandra/2.1/cassandra/operations/ops_tune_jvm_c.html?scroll=concept_ds_sv5_k4w_dk__tuning-java-garbage-collection\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-23-638.jpg?cb=1474064812\" title=\"Tuning JVM Garbage collection&#10;23&#10;Metric CMS G1&#10;G1 : CMS&#10;Fac...\" target=\"_blank\">\n        23.\n      </a>\n    Tuning JVM Garbage collection\n23\nMetric CMS G1\nG1 : CMS\nFactor\nop rate 1951 13765 7.06\nlatency mean (ms) 3.6 0.4 9.00\nlatency median (ms) 0.3 0.3 1.00\nlatency 95th percentile (ms) 0.6 0.4 1.50\nlatency 99th percentile (ms) 1 0.5 2.00\nlatency 99.9th percentile (ms) 11.6 0.7 16.57\nlatency max (ms) 13496.9 4626.9 2.92\nG1 garbage collector is much better without any tuning\nUsing cassandra-stress, 32 threads client\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-24-638.jpg?cb=1474064812\" title=\"Cluster Setup&#10;24&#10;● 3 nodes&#10;● Local DC&#10;● 24 cores, 128 GB RA...\" target=\"_blank\">\n        24.\n      </a>\n    Cluster Setup\n24\n● 3 nodes\n● Local DC\n● 24 cores, 128 GB RAM, 2TB SAS drives\n● Cassandra running on bare metal\n● Cassandra running in a Mesos container\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-25-638.jpg?cb=1474064812\" title=\"Bare metal Mesos&#10;Read Latency&#10;25&#10;Mean: 0.38 ms&#10;P95: 0.74 ms...\" target=\"_blank\">\n        25.\n      </a>\n    Bare metal Mesos\nRead Latency\n25\nMean: 0.38 ms\nP95: 0.74 ms\nP99: 0.91 ms\nMean: 0.44 ms\nP95: 0.76 ms\nP99: 0.98 ms\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-26-638.jpg?cb=1474064812\" title=\"Bare metal Mesos&#10;Read Throughput&#10;26&#10; \" target=\"_blank\">\n        26.\n      </a>\n    Bare metal Mesos\nRead Throughput\n26\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-27-638.jpg?cb=1474064812\" title=\"Bare metal Mesos&#10;Write Latency&#10;27&#10;Mean: 0.43 ms&#10;P95: 0.94 m...\" target=\"_blank\">\n        27.\n      </a>\n    Bare metal Mesos\nWrite Latency\n27\nMean: 0.43 ms\nP95: 0.94 ms\nP99: 1.05 ms\nMean: 0.48 ms\nP95: 0.93 ms\nP99: 1.26 ms\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-28-638.jpg?cb=1474064812\" title=\"Bare metal Mesos&#10;Write Throughput&#10;28&#10; \" target=\"_blank\">\n        28.\n      </a>\n    Bare metal Mesos\nWrite Throughput\n28\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-29-638.jpg?cb=1474064812\" title=\"Running across datacenters&#10;29&#10;● Four datacenters&#10;○ Each run...\" target=\"_blank\">\n        29.\n      </a>\n    Running across datacenters\n29\n● Four datacenters\n○ Each running dcos-cassandra-service instance\n○ Sync datacenter phase\n■ Periodically exchange seeds with external dcs\n● Cassandra nodes gossip topology\n○ Discover nodes in other datacenters\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-30-638.jpg?cb=1474064812\" title=\"Asynchronous cross-dc replication latency&#10;30&#10;● Write a row ...\" target=\"_blank\">\n        30.\n      </a>\n    Asynchronous cross-dc replication latency\n30\n● Write a row to dc1 using consistency level LOCAL_ONE\n○ Write timestamp to a file when operation completed\n● Spin in a loop to read the same row using consistency LOCAL_ONE in dc2\n○ Write timestamp to a file when operation completed\n● Difference between the two gives asynchronous replication latency\n○ p50 : 44.69ms, p95 : 46.38ms, p99:47.44ms\n● Round trip ping latency\n○ 77.8ms\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-31-638.jpg?cb=1474064812\" title=\"Cassandra on Mesos in Production&#10;31&#10;● ~20 clusters replicat...\" target=\"_blank\">\n        31.\n      </a>\n    Cassandra on Mesos in Production\n31\n● ~20 clusters replicating across two datacenters (west and east coast)\n● ~300 machines across two datacenters\n● Largest 2 clusters: more than a million writes/sec and ~100k reads/sec\n● Mean read latency: 13ms and write latency: 25ms\n● Mostly use LOCAL_QUORUM consistency level\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-32-638.jpg?cb=1474064812\" title=\"Questions?&#10;32&#10;verma@uber.com&#10; \" target=\"_blank\">\n        32.\n      </a>\n    Questions?\n32\nverma@uber.com\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-33-638.jpg?cb=1474064812\" title=\"Cluster startup&#10;33&#10;For each node in the cluster:&#10;1.Receive ...\" target=\"_blank\">\n        33.\n      </a>\n    Cluster startup\n33\nFor each node in the cluster:\n1.Receive and accept offer\n2.Launch task\n3.Fetch executor, JRE, Cassandra binaries from S3/HDFS\n4.Launch executor\n5.Launch Cassandra daemon\n6.Wait for it’s mode to transition STARTING -&gt; JOINING -&gt; NORMAL\nAurora hogging offers\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-34-638.jpg?cb=1474064812\" title=\"Aurora hogs offers&#10;34&#10;● Aurora designed to be the only fram...\" target=\"_blank\">\n        34.\n      </a>\n    Aurora hogs offers\n34\n● Aurora designed to be the only framework running on Mesos and\ncontrolling all the machines\n● Holds on to all received offers\n○ Does not accept or reject them\n● Mesos waits for --offer_timeout time duration and rescinds offer\n● --offer_timeout config\n○ Duration of time before an offer is rescinded from a framework. This helps fairness when\nrunning frameworks that hold on to offers, or frameworks that accidentally drop offers. If\nnot set, offers do not timeout.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-35-638.jpg?cb=1474064812\" title=\"Long term solution: dynamic reservations&#10;35&#10;● Dynamically r...\" target=\"_blank\">\n        35.\n      </a>\n    Long term solution: dynamic reservations\n35\n● Dynamically reserve all the machines resources to the “cassandra”\nrole\n● Resources are offered only to cassandra frameworks\n● Improves node startup time: 30s/node\n● Node failure replacement or updates are much faster\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-36-638.jpg?cb=1474064812\" title=\"Using the Cassandra cluster&#10;36&#10;https://www.youtube.com/watc...\" target=\"_blank\">\n        36.\n      </a>\n    Using the Cassandra cluster\n36\nhttps://www.youtube.com/watch?v=qgqO39DteHo\n \n  </li>\n              </ol></div>\n      </div>\n    </div>\n    <aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\"><dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\">\n    <ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"66106372\" title=\"Social Media in the Classroom\" href=\"https://www.linkedin.com/learning/social-media-in-the-classroom?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Social Media in the Classroom\" data-ga-action=\"click\" readability=\"2\">\n    <div class=\"lynda-thumbnail\">\n      <img class=\"j-thumbnail j-lazy-thumb\" alt=\"Social Media in the Classroom\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=n2BOLlN8y6aLPlRvdaS0okzSdaI%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-iXSWq-dGfY3HpecTbZLSiolkUfy0HkQI0f-eoSDHhFI69LcLmY4Yx3A\"/>\n    </div>\n    <div class=\"lynda-content\" readability=\"39\">\n      <p>\n        Social Media in the Classroom\n      </p>\n      <p>\n        Online Course - LinkedIn Learning\n      </p>\n    </div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"66106372\" title=\"Learning to Teach Online\" href=\"https://www.linkedin.com/learning/learning-to-teach-online?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Learning to Teach Online\" data-ga-action=\"click\" readability=\"2\">\n    <div class=\"lynda-thumbnail\">\n      <img class=\"j-thumbnail j-lazy-thumb\" alt=\"Learning to Teach Online\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=oaL29uUYS0ceLaNm0nQ7NB0R3wQ%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lXyGj_tyfZHPtcMPWZLSiol8eeywAmQIyfumvQjPgEo69LcLmY4Yx3A\"/>\n    </div>\n    <div class=\"lynda-content\" readability=\"39\">\n      <p>\n        Learning to Teach Online\n      </p>\n      <p>\n        Online Course - LinkedIn Learning\n      </p>\n    </div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"66106372\" title=\"College Prep: Writing a Strong Essay\" href=\"https://www.linkedin.com/learning/college-prep-writing-a-strong-essay?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_College Prep: Writing a Strong Essay\" data-ga-action=\"click\" readability=\"2\">\n    <div class=\"lynda-thumbnail\">\n      <img class=\"j-thumbnail j-lazy-thumb\" alt=\"College Prep: Writing a Strong Essay\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=UlSFF0nnPn2Agyz5a5T2WQKxkbA%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-iUyav_dSfY3_qfMDeZLSiol4TfSkFlAA0duqsQzfhGo69LcLmY4Yx3A\"/>\n    </div>\n    <div class=\"lynda-content\" readability=\"39\">\n      <p>\n        College Prep: Writing a Strong Essay\n      </p>\n      <p>\n        Online Course - LinkedIn Learning\n      </p>\n    </div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"28523876\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Building Real-Time Applications with Android and WebSockets\" href=\"https://www.slideshare.net/sergialmar/building-realtime-applications-with-android-and-websockets\" readability=\"-23\">\n    \n    <div class=\"related-content\" readability=\"14\">\n      <p>\n        Building Real-Time Applications with Android and WebSockets\n      </p>\n        <p>Sergi Almar i Graupera</p>\n    </div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"75400852\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Uber's new mobile architecture\" href=\"https://www.slideshare.net/dhaval2025/ubers-new-mobile-architecture\" readability=\"-24\">\n    \n    <div class=\"related-content\" readability=\"12\">\n      <p>\n        Uber's new mobile architecture\n      </p>\n        <p>Dhaval Patel</p>\n    </div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"78323537\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Open-source Infrastructure at Lyft\" href=\"https://www.slideshare.net/DanielHochman/open-source-infrastructure-at-lyft\" readability=\"-24\">\n    \n    <div class=\"related-content\" readability=\"12\">\n      <p>\n        Open-source Infrastructure at Lyft\n      </p>\n        <p>Daniel Hochman</p>\n    </div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"39188352\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Taxi Startup Presentation for Taxi Company\" href=\"https://www.slideshare.net/esuslo/taxi-startup-presentation-for-taxi-company\" readability=\"-24\">\n    \n    <div class=\"related-content\" readability=\"12\">\n      <p>\n        Taxi Startup Presentation for Taxi Company\n      </p>\n        <p>Eugene Suslo</p>\n    </div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"78959837\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"&quot;Building Data Foundations and Analytics Tools Across The Product&quot; by Crystal Widjaja (GO-JEK)\" href=\"https://www.slideshare.net/TechInAsiaID/building-data-foundations-and-analytics-tools-across-the-product-by-crystal-widjaja-gojek\" readability=\"-24\">\n    \n    <div class=\"related-content\" readability=\"12\">\n      <p>\n        \"Building Data Foundations and Analytics Tools Across The Product\" by Crystal...\n      </p>\n        <p>Tech in Asia ID</p>\n    </div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64631647\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Just Add Reality: Managing Logistics with the Uber Developer Platform\" href=\"https://www.slideshare.net/apigee/just-add-reality-managing-logistics-with-the-uber-developer-platform\" readability=\"-23\">\n    \n    <div class=\"related-content\" readability=\"14\">\n      <p>\n        Just Add Reality: Managing Logistics with the Uber Developer Platform\n      </p>\n        <p>Apigee | Google Cloud</p>\n    </div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"76662828\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Geospatial Indexing at Scale: The 15 Million QPS Redis Architecture Powering Lyft\" href=\"https://www.slideshare.net/DanielHochman/geospatial-indexing-at-scale-the-15-million-qps-redis-architecture-powering-lyft\" readability=\"-24\">\n    \n    <div class=\"related-content\" readability=\"12\">\n      <p>\n        Geospatial Indexing at Scale: The 15 Million QPS Redis Architecture Powering ...\n      </p>\n        <p>Daniel Hochman</p>\n    </div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div>\n</div>\n      \n      \n        <footer>\n          <div class=\"row\">\n            <div class=\"columns\">\n              <ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div>\n          </div>\n          \n          <div class=\"row\" readability=\"5\">\n            <div class=\"columns\" readability=\"11\">\n              <p class=\"copyright text-center\">LinkedIn Corporation © 2017</p>\n              \n            </div>\n          </div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\" readability=\"34\">\n          \n    <div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" readability=\"7\">\n  <h4 class=\"modal-title\">Public clipboards featuring this slide</h4>\n  <hr/>\n  \n  <p>\n    No public clipboards found for this slide\n  </p>\n  \n</div>\n    \n    <div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\" readability=\"7\">\n  <p>\n    <h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr/><a class=\"close-reveal-modal button-lrg\" href=\"https://www.slideshare.net/DataStax/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016#\" aria-label=\"Close\">×</a>\n  </p>\n  \n  <div class=\"modal-content\" readability=\"35\">\n    <div class=\"default-clipboard-panel radius\" readability=\"6\">\n      <p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"/> already.</p>\n    </div>\n    \n    <div class=\"clipboard-list-container\">\n      <div class=\"clipboard-create-new\">\n        \n        <p>Create a clipboard</p>\n      </div>\n    </div>\n  </div>\n</div>\n    <div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\" readability=\"11\">\n    <p>\n      <h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.\n      \n    </p>\n    <h4 class=\"modal-title\" id=\"modal-title\"/>\n    <hr aria-hidden=\"true\"/><form data-abide=\"\">\n    \n    <div class=\"row\">\n      <p>\n        <label>Description\n          \n        </label>\n      </p>\n    </div>\n    <div class=\"row\" readability=\"6\">\n      <label readability=\"2\">Visibility\n        <p>\n          <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          <label for=\"privacy-switch\"/>\n        </p>\n      </label>\n    </div>\n        \n    </form>\n    </div>\n    </div>\n    \n    \n    \n  \n    \n    \n  \n  \n  <noscript>\n    </noscript>\n  </body>",
        "created_at": "2017-10-19T03:27:24+0000",
        "updated_at": "2017-12-27T13:31:15+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 7,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://image.slidesharecdn.com/cassandrasummit2016-runningcassandraonapachemesosacrossmultipledatacentersatuber-160916191137/95/cassandra-on-mesos-across-multiple-datacenters-at-uber-abhishek-verma-c-summit-2016-1-638.jpg?cb=1474064812",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/5117"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 15,
            "label": "tutorial",
            "slug": "tutorial"
          },
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 869,
            "label": "devops",
            "slug": "devops"
          }
        ],
        "is_public": false,
        "id": 5116,
        "uid": null,
        "title": "Learning Apache Cassandra-Part-4-Adding Node To Cassandra Cluster",
        "url": "http://www.vstellar.com/2017/01/09/learning-apache-cassandra-part-4-adding-node-to-cassandra-cluster/",
        "content": "<p>In last post of this series we learnt how to install cassandra on Rhel 6. In this post we will look into additional configuration parameter that is needed to configure in order to facilitate other nodes to join the cassandra cluster.</p>\n<p>If you have missed earlier posts of this series then you can read them from below links:</p>\n<p>1: <a href=\"http://www.vstellar.com/2017/01/02/learning-apache-cassandra-part-1introduction/\">Introduction to Cassandra</a></p>\n<p>2: <a href=\"http://www.vstellar.com/2017/01/03/learning-apache-cassandra-part-2understanding-cassandra-readwrite-mechanism/\" target=\"_blank\" rel=\"noopener\">Understanding Cassandra Read/Write Mechanism</a></p>\n<p>3: <a href=\"http://www.vstellar.com/2017/01/08/learning-apache-cassandra-part-3-installing-cassandra-on-rhel6/\">Installing Cassandra on RHEL6</a></p>\n<p>At the moment we have one node cassandra cluster. Before going ahead and installing cassandra on other nodes, we will first perform following configuration changes in cassandra.yaml file.</p>\n<p>Navigate to cassandra.yaml file which is located in cassandra_install_dir/conf folder. Open the file in editor of your choice and look for following options:</p>\n<ul><li><strong>Listen_address</strong>: Address where gossip will be listening to. This address can’t be localhost or 0.0.0.0, because the rest of nodes will try to connect to this address.</li>\n<li><strong>RPC_address</strong>: This is the address where thrift will be listening. We must put a existing IP address (it may be localhost, if we want to), or 0.0.0.0 if we want to listen through all of them. This is the address to which client applications interact with cassandra DB.</li>\n<li><strong>Seeds</strong>: Seed nodes are the nodes which will provide cluster info to the new nodes which are bootstrapped and are ready to join the cluster. Seed nodes become a reference for any new nodes to join cluster in trustable way.</li>\n</ul><p>This settings we need to configure in <strong>cassandra.yaml</strong> file on each node which we want to put into the cluster.</p>\n<p><strong>Note:</strong> Be sure to install the same version of Cassandra as is installed on the other nodes in the cluster.</p>\n<p><strong>Procedure to add new nodes in cassandra cluster:</strong></p>\n<p>1: Install Cassandra on the new nodes, but do not start Cassandra.</p>\n<p>2: Set the following properties in the cassandra.yaml and, depending on the snitch, the cassandra-topology.properties or cassandra-rackdc.properties configuration files:</p>\n<ul><li><strong>auto_bootstrap</strong> – This property is not listed in the default cassandra.yaml configuration file, but it might have been added and set to false by other operations. If it is not defined in cassandra.yaml, Cassandra uses true as a default value. For this operation, search for this property in the cassandra.yaml file. If it is present, set it to true or delete it..</li>\n<li><strong>cluster_name</strong> – The name of the cluster the new node is joining. Ensure that cluster name is same for all nodes which will be part of cluster.</li>\n<li><strong>listen_address</strong> – Can usually be left blank. Otherwise, use IP address or hostname that other Cassandra nodes use to connect to the new node.</li>\n<li><strong>endpoint_snitch</strong> – The snitch Cassandra uses for locating nodes and routing requests. In my lab I am using simple snitch which is present as default in cassandra.yaml file and so I did not change or edit this.</li>\n<li><strong>num_tokens</strong> – The number of vnodes to assign to the node. If the hardware capabilities vary among the nodes in your cluster, you can assign a proportional number of vnodes to the larger machines.</li>\n<li><strong>seeds</strong> – Determines which nodes the new node contacts to learn about the cluster and establish the gossip process. Make sure that the -seeds list includes the address of at least one node in the existing cluster.</li>\n</ul><p>Post installing cassandra on your second node and making the configuration change as stated above, go ahead and start cassandra service on second node and do a watch on nodetool status on cassandra node 1 and you will see the new node joining the cluster.</p>\n<p><strong>Nodetool Status Output</strong></p>\n<p>Every 2.0s: /opt/apache-cassandra/bin/nodetool status Sun Jan 8 23:31:44 2017</p>\n<p>Datacenter: datacenter1<br/>\n =======================<br/>\n Status=Up/Down<br/>\n |/ State=Normal/Leaving/Joining/Moving<br/>\n — Address               Load           Tokens Owns (effective) Host ID Rack<br/>\n UN 192.168.109.70 214.99 KiB  256      100.0%                    14ba62c6-59e4-404b-a6a6-30c9503ef3a4       rack1<br/>\n UN 192.168.109.71 103.47 KiB  256       100.0%                    3b19bc83-f483-4a60-82e4-109c90c49a14      rack1</p>\n<p>we have to repeat the same steps for each node which we want to place in our cluster.</p>\n<p>I hope this post is informational to you. Feel free to share this on social media if it is worth sharing. Be sociable <img class=\"emoji\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/svg/1f642.svg\" alt=\"????\"/></p>\n\n<div id=\"jp-relatedposts\" class=\"jp-relatedposts\">\n\t<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<!-- Start Tags -->\n\t\t\t\t\t\t\t\t<div class=\"tags\"/>\n\t\t\t\t\t\t\t\t<!-- End Tags -->",
        "created_at": "2017-10-19T14:29:49+0000",
        "updated_at": "2017-12-27T13:33:31+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en-US",
        "reading_time": 3,
        "domain_name": "www.vstellar.com",
        "preview_picture": "https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/svg/1f642.svg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/5116"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 15,
            "label": "tutorial",
            "slug": "tutorial"
          },
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 5115,
        "uid": null,
        "title": "Bootstrapping Apache Cassandra Nodes",
        "url": "http://thelastpickle.com/blog/2017/05/23/auto-bootstrapping-part1.html",
        "content": "<p>Auto bootstrapping is a handy feature when it comes to growing an Apache Cassandra cluster. There are some unknowns about how this feature works which can lead to data inconsistencies in the cluster. In this post I will go through a bit about the history of the feature, the different knobs and levers available to operate it, and resolving some of the common issues that may arise.</p><p>Here are links to the various sections of the post to give you an idea of what I will cover.</p>\n<ul><li><a href=\"http://thelastpickle.com/blog/2017/05/23/auto-bootstrapping-part1.html#background\">Background</a></li>\n  <li><a href=\"http://thelastpickle.com/blog/2017/05/23/auto-bootstrapping-part1.html#back-to-basics\">Back to basics (how data is distributed)</a></li>\n  <li><a href=\"http://thelastpickle.com/blog/2017/05/23/auto-bootstrapping-part1.html#gotchas\">Gotchas (common mistakes and pitfalls)</a></li>\n  <li><a href=\"http://thelastpickle.com/blog/2017/05/23/auto-bootstrapping-part1.html#adding-a-replacement-node\">Adding a replacement node</a></li>\n  <li><a href=\"http://thelastpickle.com/blog/2017/05/23/auto-bootstrapping-part1.html#hang-on-what-about-adding-a-replacement-a-seed-node\">Hang on! What about adding a replacement a seed node?</a></li>\n  <li><a href=\"http://thelastpickle.com/blog/2017/05/23/auto-bootstrapping-part1.html#what-to-do-after-it-completes-successfully\">What to do after it completes successfully</a></li>\n  <li><a href=\"http://thelastpickle.com/blog/2017/05/23/auto-bootstrapping-part1.html#help-it-failed\">Help! It failed</a></li>\n  <li><a href=\"http://thelastpickle.com/blog/2017/05/23/auto-bootstrapping-part1.html#testing-the-theory\">Testing the theory</a></li>\n  <li><a href=\"http://thelastpickle.com/blog/2017/05/23/auto-bootstrapping-part1.html#conclusion\">Conclusion</a></li>\n</ul><p>The bootstrap feature in Apache Cassandra controls the ability for the data in cluster to be automatically redistributed when a new node is inserted. The new node joining the cluster is defined as an empty node without system tables or data.</p>\n<p>When a new node joins the cluster using the auto bootstrap feature, it will perform the following operations</p>\n<ul><li>Contact the seed nodes to learn about gossip state.</li>\n  <li>Transition to Up and Joining state (to indicate it is joining the cluster; represented by <strong><code class=\"highlighter-rouge\">UJ</code></strong> in the <code class=\"highlighter-rouge\">nodetool status</code>).</li>\n  <li>Contact the seed nodes to ensure schema agreement.</li>\n  <li>Calculate the tokens that it will become responsible for.</li>\n  <li>Stream replica data associated with the tokens it is responsible for from the former owners.</li>\n  <li>Transition to Up and Normal state once streaming is complete (to indicate it is now part of the cluster; represented by <strong><code class=\"highlighter-rouge\">UN</code></strong> in the <code class=\"highlighter-rouge\">nodetool status</code>).</li>\n</ul><p>The above operations can be seen in the logs.</p>\n<p><strong>Contact the seed nodes to learn about gossip state</strong></p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>INFO  [HANDSHAKE-/127.0.0.1] 2017-05-12 16:14:45,290 OutboundTcpConnection.java:487 - Handshaking version with /127.0.0.1\nINFO  [GossipStage:1] 2017-05-12 16:14:45,318 Gossiper.java:1029 - Node /127.0.0.1 is now part of the cluster\nINFO  [GossipStage:1] 2017-05-12 16:14:45,325 Gossiper.java:1029 - Node /127.0.0.2 is now part of the cluster\nINFO  [GossipStage:1] 2017-05-12 16:14:45,326 Gossiper.java:1029 - Node /127.0.0.3 is now part of the cluster\nINFO  [GossipStage:1] 2017-05-12 16:14:45,328 Gossiper.java:1029 - Node /127.0.0.4 is now part of the cluster\nINFO  [SharedPool-Worker-1] 2017-05-12 16:14:45,331 Gossiper.java:993 - InetAddress /127.0.0.1 is now UP\nINFO  [HANDSHAKE-/127.0.0.3] 2017-05-12 16:14:45,331 OutboundTcpConnection.java:487 - Handshaking version with /127.0.0.3\nINFO  [HANDSHAKE-/127.0.0.2] 2017-05-12 16:14:45,383 OutboundTcpConnection.java:487 - Handshaking version with /127.0.0.2\nINFO  [HANDSHAKE-/127.0.0.4] 2017-05-12 16:14:45,387 OutboundTcpConnection.java:487 - Handshaking version with /127.0.0.4\nINFO  [SharedPool-Worker-1] 2017-05-12 16:14:45,438 Gossiper.java:993 - InetAddress /127.0.0.3 is now UP\nINFO  [SharedPool-Worker-2] 2017-05-12 16:14:45,438 Gossiper.java:993 - InetAddress /127.0.0.4 is now UP\nINFO  [SharedPool-Worker-3] 2017-05-12 16:14:45,438 Gossiper.java:993 - InetAddress /127.0.0.2 is now UP\n...\nINFO  [main] 2017-05-12 16:14:46,289 StorageService.java:807 - Starting up server gossip\n</pre></div></div>\n<p><strong>Transition to Up and Joining state</strong></p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>INFO  [main] 2017-05-12 16:14:46,396 StorageService.java:1138 - JOINING: waiting for ring information\n</pre></div></div>\n<p><strong>Contact the seed nodes to ensure schema agreement</strong></p>\n<p>Take note of the last entry in this log snippet.</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>INFO  [GossipStage:1] 2017-05-12 16:14:49,081 Gossiper.java:1029 - Node /127.0.0.1 is now part of the cluster\nINFO  [SharedPool-Worker-1] 2017-05-12 16:14:49,082 Gossiper.java:993 - InetAddress /127.0.0.1 is now UP\nINFO  [GossipStage:1] 2017-05-12 16:14:49,095 TokenMetadata.java:414 - Updating topology for /127.0.0.1\nINFO  [GossipStage:1] 2017-05-12 16:14:49,096 TokenMetadata.java:414 - Updating topology for /127.0.0.1\nINFO  [HANDSHAKE-/127.0.0.1] 2017-05-12 16:14:49,096 OutboundTcpConnection.java:487 - Handshaking version with /127.0.0.1\nINFO  [GossipStage:1] 2017-05-12 16:14:49,098 Gossiper.java:1029 - Node /127.0.0.2 is now part of the cluster\nINFO  [SharedPool-Worker-1] 2017-05-12 16:14:49,102 Gossiper.java:993 - InetAddress /127.0.0.2 is now UP\nINFO  [GossipStage:1] 2017-05-12 16:14:49,103 TokenMetadata.java:414 - Updating topology for /127.0.0.2\nINFO  [HANDSHAKE-/127.0.0.2] 2017-05-12 16:14:49,104 OutboundTcpConnection.java:487 - Handshaking version with /127.0.0.2\nINFO  [GossipStage:1] 2017-05-12 16:14:49,104 TokenMetadata.java:414 - Updating topology for /127.0.0.2\nINFO  [GossipStage:1] 2017-05-12 16:14:49,106 Gossiper.java:1029 - Node /127.0.0.3 is now part of the cluster\nINFO  [SharedPool-Worker-1] 2017-05-12 16:14:49,111 Gossiper.java:993 - InetAddress /127.0.0.3 is now UP\nINFO  [GossipStage:1] 2017-05-12 16:14:49,112 TokenMetadata.java:414 - Updating topology for /127.0.0.3\nINFO  [HANDSHAKE-/127.0.0.3] 2017-05-12 16:14:49,195 OutboundTcpConnection.java:487 - Handshaking version with /127.0.0.3\nINFO  [GossipStage:1] 2017-05-12 16:14:49,236 TokenMetadata.java:414 - Updating topology for /127.0.0.3\nINFO  [GossipStage:1] 2017-05-12 16:14:49,247 Gossiper.java:1029 - Node /127.0.0.4 is now part of the cluster\nINFO  [SharedPool-Worker-1] 2017-05-12 16:14:49,248 Gossiper.java:993 - InetAddress /127.0.0.4 is now UP\nINFO  [InternalResponseStage:1] 2017-05-12 16:14:49,252 ColumnFamilyStore.java:905 - Enqueuing flush of schema_keyspaces: 1444 (0%) on-heap, 0 (0%) off-heap\nINFO  [MemtableFlushWriter:2] 2017-05-12 16:14:49,254 Memtable.java:347 - Writing Memtable-schema_keyspaces@1493033009(0.403KiB serialized bytes, 10 ops, 0%/0% of on/off-heap limit)\nINFO  [MemtableFlushWriter:2] 2017-05-12 16:14:49,256 Memtable.java:382 - Completed flushing .../node5/data0/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-tmp-ka-1-Data.db (0.000KiB) for commitlog position ReplayPosition(segmentId=1494569684606, position=119856)\nINFO  [InternalResponseStage:1] 2017-05-12 16:14:49,367 ColumnFamilyStore.java:905 - Enqueuing flush of schema_columnfamilies: 120419 (0%) on-heap, 0 (0%) off-heap\nINFO  [MemtableFlushWriter:1] 2017-05-12 16:14:49,368 Memtable.java:347 - Writing Memtable-schema_columnfamilies@1679976057(31.173KiB serialized bytes, 541 ops, 0%/0% of on/off-heap limit)\nINFO  [MemtableFlushWriter:1] 2017-05-12 16:14:49,396 Memtable.java:382 - Completed flushing .../node5/data0/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-tmp-ka-1-Data.db (0.000KiB) for commitlog position ReplayPosition(segmentId=1494569684606, position=119856)\n...\nINFO  [InternalResponseStage:5] 2017-05-12 16:14:50,824 ColumnFamilyStore.java:905 - Enqueuing flush of schema_usertypes: 160 (0%) on-heap, 0 (0%) off-heap\nINFO  [MemtableFlushWriter:2] 2017-05-12 16:14:50,824 Memtable.java:347 - Writing Memtable-schema_usertypes@1946148009(0.008KiB serialized bytes, 1 ops, 0%/0% of on/off-heap limit)\nINFO  [MemtableFlushWriter:2] 2017-05-12 16:14:50,826 Memtable.java:382 - Completed flushing .../node5/data0/system/schema_usertypes-3aa752254f82350b8d5c430fa221fa0a/system-schema_usertypes-tmp-ka-10-Data.db (0.000KiB) for commitlog position ReplayPosition(segmentId=1494569684606, position=252372)\nINFO  [main] 2017-05-12 16:14:50,404 StorageService.java:1138 - JOINING: schema complete, ready to bootstrap\n</pre></div></div>\n<p><strong>Calculate the tokens that it will become responsible for</strong></p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>INFO  [main] 2017-05-12 16:14:50,404 StorageService.java:1138 - JOINING: waiting for pending range calculation\nINFO  [main] 2017-05-12 16:14:50,404 StorageService.java:1138 - JOINING: calculation complete, ready to bootstrap\nINFO  [main] 2017-05-12 16:14:50,405 StorageService.java:1138 - JOINING: getting bootstrap token\n</pre></div></div>\n<p><strong>Stream replica data associated with the tokens it is responsible for from the former owners</strong></p>\n<p>Take note of the first and last entries in this log snippet.</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>INFO  [main] 2017-05-12 16:15:20,440 StorageService.java:1138 - JOINING: Starting to bootstrap...\nINFO  [main] 2017-05-12 16:15:20,461 StreamResultFuture.java:86 - [Stream #604b5690-36da-11e7-aeb6-9d89ad20c2d3] Executing streaming plan for Bootstrap\nINFO  [StreamConnectionEstablisher:1] 2017-05-12 16:15:20,462 StreamSession.java:220 - [Stream #604b5690-36da-11e7-aeb6-9d89ad20c2d3] Starting streaming to /127.0.0.1\nINFO  [StreamConnectionEstablisher:2] 2017-05-12 16:15:20,462 StreamSession.java:220 - [Stream #604b5690-36da-11e7-aeb6-9d89ad20c2d3] Starting streaming to /127.0.0.2\nINFO  [StreamConnectionEstablisher:3] 2017-05-12 16:15:20,462 StreamSession.java:220 - [Stream #604b5690-36da-11e7-aeb6-9d89ad20c2d3] Starting streaming to /127.0.0.3\nINFO  [StreamConnectionEstablisher:1] 2017-05-12 16:15:20,478 StreamCoordinator.java:209 - [Stream #604b5690-36da-11e7-aeb6-9d89ad20c2d3, ID#0] Beginning stream session with /127.0.0.1\nINFO  [StreamConnectionEstablisher:2] 2017-05-12 16:15:20,478 StreamCoordinator.java:209 - [Stream #604b5690-36da-11e7-aeb6-9d89ad20c2d3, ID#0] Beginning stream session with /127.0.0.2\nINFO  [StreamConnectionEstablisher:3] 2017-05-12 16:15:20,478 StreamCoordinator.java:209 - [Stream #604b5690-36da-11e7-aeb6-9d89ad20c2d3, ID#0] Beginning stream session with /127.0.0.3\nINFO  [STREAM-IN-/127.0.0.2] 2017-05-12 16:15:24,339 StreamResultFuture.java:166 - [Stream #604b5690-36da-11e7-aeb6-9d89ad20c2d3 ID#0] Prepare completed. Receiving 11 files(10176549820 bytes), sending 0 files(0 bytes)\nINFO  [STREAM-IN-/127.0.0.3] 2017-05-12 16:15:27,201 StreamResultFuture.java:180 - [Stream #604b5690-36da-11e7-aeb6-9d89ad20c2d3] Session with /127.0.0.3 is complete\nINFO  [STREAM-IN-/127.0.0.1] 2017-05-12 16:15:33,256 StreamResultFuture.java:180 - [Stream #604b5690-36da-11e7-aeb6-9d89ad20c2d3] Session with /127.0.0.1 is complete\nINFO  [StreamReceiveTask:1] 2017-05-12 16:36:31,249 StreamResultFuture.java:180 - [Stream #604b5690-36da-11e7-aeb6-9d89ad20c2d3] Session with /127.0.0.2 is complete\nINFO  [StreamReceiveTask:1] 2017-05-12 16:36:31,256 StreamResultFuture.java:212 - [Stream #604b5690-36da-11e7-aeb6-9d89ad20c2d3] All sessions completed\nINFO  [main] 2017-05-12 16:36:31,257 StorageService.java:1167 - Bootstrap completed! for the tokens [1577102245397509090, -713021257351906154, 5943548853755748481, -186427637333122985, 89474807595263595, -3872409873927530770, 269282297308186556, -2090619435347582830, -7442271648674805532, 1993467991047389706, 3250292341615557960, 3680244045045170206, -6121195565829299067, 2336819841643904893, 8366041580813128754, -1539294702421999531, 5559860204752248078, 4990559483982320587, -5978802488822380342, 7738662906313460122, -8543589077123834538, 8470022885937685086, 7921538168239180973, 5167628632246463806, -8217637230111416952, 7867074371397881074, -6728907721317936873, -5403440910106158938, 417632467923200524, -5024952230859509916, -2145251677903377866, 62038536271402824]\n</pre></div></div>\n<p><strong>Transition to Up and Normal state once streaming is complete</strong></p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>INFO  [main] 2017-05-12 16:36:31,348 StorageService.java:1715 - Node /127.0.0.5 state jump to NORMAL\n</pre></div></div>\n<p>During the bootstrapping process, the new node joining the cluster has no effect on the existing data in terms of Replication Factor (RF). However, the new node will accept new writes for the token ranges acquired while existing data from the other nodes is being streamed to it. This ensures that no new writes are missed while data changes hands. In addition, it ensures that Consistency Level (CL) is respected all the time during the streaming process and even in the case of bootstrap failure. Once the bootstrapping process for the new node completes, it will begin to serve read requests (and continue to receive writes). Like the pre-existing nodes in the cluster, it too will then have an effect on the data in terms of RF and CL.</p>\n<p>While the bootstrapping feature can be a time saver when expanding a cluster, there are some “gotchas” that are worth noting. But before we do, we need first revisit some basics.</p>\n<p>Cassandra uses a token system to work out which nodes will hold which partition keys for the primary replica of data. To work out where data is stored in the cluster, Cassandra will first apply a hashing function to the partition key. The generated hash is then used to calculate a token value using an algorithm; most commonly <a href=\"https://en.wikipedia.org/wiki/MurmurHash\">Murmur3</a> or RandomPartitioner.</p>\n<p>As seen from the log snippets, when a new node is added to the cluster it will calculate the tokens of the different data replicas that is to be responsible for. This process where tokens are calculated and acquired by the new node is often referred to as a range movement. i.e. token ranges are being moved between nodes. Once the range movement has completed, the node will by default begin the bootstrapping process where it streams data for the acquired tokens from other nodes.</p>\n<h2 id=\"range-movements\">Range movements</h2>\n<p>Whilst range movements may sound simple, the process can create implications with maintaining data consistency. A number of patches have been added over time to help maintain data consistency during range movements. A fairly well known issue was <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-2434\">CASSANDRA-2434</a> where it was highlighted that range movements violated consistency for Apache Cassandra versions below 2.1.x using vnodes.</p>\n<p>A fix was added for the issue <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-2434\">CASSANDRA-2434</a> to ensure range movements between nodes were consistent when using vnodes. Prior to this patch inconsistencies could be caused during bootstrapping as per the example Jeff Jirsa gave on the <a href=\"https://lists.apache.org/thread.html/8aad2e853fc0b722aaada382352bc2c187623fddf7ae4a7a0d2fa788@%3Cdev.cassandra.apache.org%3E\">dev mailing list</a>.</p>\n<p>Consider the case of a cluster containing three nodes A, B and D with a RF of 3. If node B was offline and a key ‘foo’ was written with CL of QUORUM, the value for key ‘foo’ would go to nodes A and D.</p>\n<p><img src=\"http://thelastpickle.com/files/auto-bootstrapping/CASSANDRA-2434-data-inconsistency-figure-01.png\" width=\"530px\"/></p>\n<p>At a later point in time node B is resurrected and added back into the cluster. Around the same time a node C is added to the cluster and begins bootstrapping.</p>\n<p><img src=\"http://thelastpickle.com/files/auto-bootstrapping/CASSANDRA-2434-data-inconsistency-figure-02.png\" width=\"530px\"/></p>\n<p>One of the tokens node C calculates and acquires during the bootstrap process is for key ‘foo’. Node B is the closest node with data for the newly acquired token and thus node C begins streaming from the neighbouring node B. This process violates the consistency guarantees of Cassandra. This is because the data on node C will be the same as node B, and both are missing the value for key ‘foo’.</p>\n<p><img src=\"http://thelastpickle.com/files/auto-bootstrapping/CASSANDRA-2434-data-inconsistency-figure-03.png\" width=\"530px\"/></p>\n<p>Thus, a query with a CL of QUORUM may query nodes B and C and return no data which is incorrect, despite there being data for ‘foo’ on node A. Node D previously had the correct data, but it stopped being a replica after C was inserted into the cluster.</p>\n<p><img src=\"http://thelastpickle.com/files/auto-bootstrapping/CASSANDRA-2434-data-inconsistency-figure-04.png\" width=\"530px\"/></p>\n<p>The above issue was solved in <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-2434\">CASSANDRA-2434</a> by changing the default behaviour to always trying to perform a consistent range movement. That is, when node C is added (in the previous example), data is streamed from the correct replica it is replacing, node D. In this case all queries with CL of QUORUM for the key ‘foo’ would always return the correct value.</p>\n<p>The JVM option <code class=\"highlighter-rouge\">cassandra.consistent.rangemovement</code> was added as part of this patch. The option allows consistent range movements during bootstrapping to be disabled should the user desire this behaviour. This fix is no silver bullet though, because it requires that the correct node be available for a consistent range moment during a bootstrap. This may not always be possible, and in such cases there are two options:</p>\n<ol><li>Get the required node back online (preferred option).</li>\n  <li>If the required node is unrecoverable, set <code class=\"highlighter-rouge\">JVM_OPTS=\"$JVM_OPTS -Dcassandra.consistent.rangemovement=false\"</code> in the <em>cassandra-env.sh</em> file to perform inconsistent range movements when auto bootstrapping. Once bootstrapping is complete, a repair will need to be run using the following command on the node. This is to ensure the data it streamed is consistent with the rest of the replicas.</li>\n</ol><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>nodetool repair -full\n</pre></div></div>\n<h2 id=\"adding-multiple-nodes\">Adding multiple nodes</h2>\n<p>Another common cause of grief for users was bootstrapping multiple node simultaneously; captured in <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-7069\">CASSANDRA-7069</a>. Adding two new nodes simultaneously to a cluster could potentially be harmful, given the operations performed by a new node when joining. Waiting two minutes for the gossip state to propagate before adding a new node is possible, however as noted in <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-9667\">CASSANDRA-9667</a>, there is no coordination between nodes during token selection. For example consider that case if Node A was bootstrapped, then two minutes later Node B was bootstrapped. Node B could potentially pick token ranges already selected by Node A.</p>\n<p>The above issue was solved in <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-7069\">CASSANDRA-7069</a> by changing the default behaviour such that adding a node would fail if another node was already bootstrapping in a cluster. Similar to <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-2434\">CASSANDRA-2434</a>, this behaviour could be disabled by setting the JVM option <code class=\"highlighter-rouge\">JVM_OPTS=\"$JVM_OPTS -Dcassandra.consistent.rangemovement=false\"</code> in the <em>cassandra-env.sh</em> file on the bootstrapping node. This means that if <code class=\"highlighter-rouge\">cassandra.consistent.rangemovement=false</code> is set to allow multiple nodes to bootstrap, the cluster runs the risk of violating consistency guarantees because of <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-2434\">CASSANDRA-2434</a>.</p>\n<p>Changes made by <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-7069\">CASSANDRA-7069</a> mean that the default behaviour forces a user to add a single node at a time to expand the cluster. This is the safest way of adding nodes to expand a cluster and ensure that the correct amount of data is streamed between nodes.</p>\n<h2 id=\"data-streaming\">Data streaming</h2>\n<p>To further add to the confusion there is a misconception about what the <code class=\"highlighter-rouge\">auto_bootstrap</code> property does in relation to a node being added to the cluster. Despite its name, this property controls the data streaming step only in the bootstrap process. The boolean property is by default set to <strong>true</strong>. When set to <strong>true</strong>, the data streaming step will be performed during the bootstrap process.</p>\n<p>Setting <code class=\"highlighter-rouge\">auto_bootstrap</code> to <strong>false</strong> when bootstrapping a new node exposes the cluster to huge inconsistencies. This is because all the other steps in the process are carried out but no data is streamed to the node. Hence, the node would be in the <code class=\"highlighter-rouge\">UN</code> state without having any data for the token ranges it has been allocated! Furthermore, the new node without data will be serving reads and nodes that previously owned the tokens will no longer be serving reads. Effectively, the token ranges for that replica would be replaced with no data.</p>\n<p>It is worth noting that the other danger to using <code class=\"highlighter-rouge\">auto_bootstrap</code> set to <strong>false</strong> is no IP address collision check occurs. As per <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-10134\">CASSANDRA-10134</a>, if a new node has <code class=\"highlighter-rouge\">auto_bootstrap</code> set to <strong>false</strong> and has the same address as an existing down node, the new node will take over the token range of the old node. No error is thrown, only a warning messages such as the following one below is written to the logs of the <em>other</em> nodes in the cluster. At the time of writing this post, the fix for this issue only appears in Apache Cassandra version 3.6 and above.</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>WARN  [GossipStage:1] 2017-05-19 17:35:10,994 TokenMetadata.java:237 - Changing /127.0.0.3's host ID from 1938db5d-5f23-46e8-921c-edde18e9c829 to c30fbbb8-07ae-412c-baea-90865856104e\n</pre></div></div>\n<p>The behaviour of <code class=\"highlighter-rouge\">auto_bootstrap: false</code> can lead to data inconsistencies in the following way. Consider the case of a cluster containing three nodes A, B and D with a RF of 3. If node B was offline and a key ‘foo’ was written with CL of QUORUM, the value for key ‘foo’ would go to nodes A and D. In this scenario Node D is the owner of the token relating to the key ‘foo’.</p>\n<p><img src=\"http://thelastpickle.com/files/auto-bootstrapping/auto-bootstrap-data-inconsistency-figure-01.png\" width=\"530px\"/></p>\n<p>At a later point in time node B is resurrected and added back into the cluster. Around the same time a node C is added to the cluster with <code class=\"highlighter-rouge\">auto_bootstrap</code> set to <strong>false</strong> and begins the joining process.</p>\n<p><img src=\"http://thelastpickle.com/files/auto-bootstrapping/auto-bootstrap-data-inconsistency-figure-02.png\" width=\"530px\"/></p>\n<p>One of the tokens node C calculates and acquires during the bootstrap process is for key ‘foo’. Now node D is no longer the owner and hence its data for the key ‘foo’ will no longer be used during reads/writes. This process causes inconsistencies in Cassandra because both nodes B and C contain no data for key ‘foo’.</p>\n<p><img src=\"http://thelastpickle.com/files/auto-bootstrapping/auto-bootstrap-data-inconsistency-figure-03.png\" width=\"530px\"/></p>\n<p>Thus, a query with a CL of QUORUM may query nodes B and C and return no data which is incorrect, despite there being data for ‘foo’ on node A. Node D previously had data, but it stopped being a replica after C was inserted.</p>\n<p><img src=\"http://thelastpickle.com/files/auto-bootstrapping/auto-bootstrap-data-inconsistency-figure-04.png\" width=\"530px\"/></p>\n<p>This confusing behaviour is one of the reasons why if you look into the <em>cassandra.yaml</em> file you will notice that the <code class=\"highlighter-rouge\">auto_bootstrap</code> configuration property is missing. Exposure of the property in the <em>cassandra.yaml</em> was short lived, as it was removed via <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-2447\">CASSANDRA-2447</a> in version 1.0.0. As a result, the property is hidden and its default value of <strong>true</strong> means that new nodes will stream data when they join the cluster.</p>\n<p>So far we have examined various options that control the bootstrapping default behaviour when a new node is added to a cluster. Adding a new node is just one case where bootstrapping is performed, what about the case of replacing a node in the cluster if one goes down?</p>\n<p>Should an existing node go down and needs to be replaced, the JVM option <code class=\"highlighter-rouge\">cassandra.replace_address</code> can be used. Note that this option is only available for Apache Cassandra versions 2.x.x and higher. This feature has been around for a while and <a href=\"https://blog.alteroot.org/articles/2014-03-12/replace-a-dead-node-in-cassandra.html\">blogged about</a> by other users in the past.</p>\n<p>As the name suggests, it effectively replaces a down or dead node in the cluster with a new node. It is because of this that replace address option should only be used if the node is in a Down and Normal state (represented by <strong><code class=\"highlighter-rouge\">DN</code></strong> in the <code class=\"highlighter-rouge\">nodetool status</code>). Furthermore, there are no range movements that occur when using this feature, the new replacement node will simply inherit the old dead node’s token ranges. This is simpler than decommissioning the dead node and bootstrapping a fresh one, which would involve two range movements and two streaming phases. Yuck! To use the option, simply add <code class=\"highlighter-rouge\">JVM_OPTS=\"$JVM_OPTS -Dcassandra.replace_address=&lt;IP_ADDRESS&gt;\"</code> to the <em>cassandra-env.sh</em> file of the new node that will be replacing the old node. Where <code class=\"highlighter-rouge\">&lt;IP_ADDRESS&gt;</code> is the IP address of the node to be replaced.</p>\n<p>Once the node completes bootstrapping and joins the cluster, the <code class=\"highlighter-rouge\">JVM_OPTS=\"$JVM_OPTS -Dcassandra.replace_address=&lt;IP_ADDRESS&gt;\"</code> option must be removed from the <em>cassandra-env.sh</em> file or the node will fail to start on a restart. This is a short coming of the <code class=\"highlighter-rouge\">cassandra.replace_address</code> feature. Many operators will typically be worried about a dead node being replaced and as a result forget to update the <em>cassandra-env.sh</em> file after the job is complete. It was for this reason that <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-7356\">CASSANDRA-7356</a> was raised and resulted in a new option being added; <code class=\"highlighter-rouge\">cassandra.replace_address_first_boot</code>. This option works once when Cassandra is first started and the replacement node inserted into the cluster. After that, the option is ignored for all subsequent restarts. It works in the same way as its predecessor; simply add <code class=\"highlighter-rouge\">JVM_OPTS=\"$JVM_OPTS -Dcassandra.replace_address_first_boot=&lt;IP_ADDRESS&gt;\"</code> to the <em>cassandra-env.sh</em> and the new node is ready to be inserted.</p>\n<p>Ok, so you need to replace a seed node. Seed nodes are just like every other node in the cluster. As per the <a href=\"https://cassandra.apache.org/doc/latest/faq/index.html?highlight=seed#what-are-seeds\">Apache Cassandra documentation</a>, the only difference being seed nodes are the go to node when a new node joins the cluster.</p>\n<p>There are a few extra steps to replace a seed node and bootstrap a new one in its place. Before adding the replacement seed node, the IP address of the seed node will need to be removed from the <code class=\"highlighter-rouge\">seed_provider</code> list in the <em>cassandra.yaml</em> file and replaced with another node in the cluster. This needs to be done for all the nodes in the cluster. Naturally, a rolling restart will need to be performed for the changes to take effect. Once the change is complete the replacement node can be inserted as described in the previous section of this post.</p>\n<p>Once your node has successfully completed the bootstrapping process, it will transition to Up and Normal state (represented by <strong><code class=\"highlighter-rouge\">UN</code></strong> in the <code class=\"highlighter-rouge\">nodetool status</code>) to indicate it is now part of the cluster. At this point it is time to cleanup the nodes on your cluster. Yes, your nodes are dirty and need to be cleaned. “Why?” you ask, well the reason is the data that has been acquired by the newly added node still remains on the nodes that previously owned it. Whilst the nodes that previously owned the data have streamed it to the new node and relinquished the associated tokens, the data that was streamed still remains on the original nodes. This “orphaned” data is consuming valuable disk space, and in the cases large data sets; probably consuming a significant amount.</p>\n<p>However, before running off to the console to remove the orphaned data from the nodes, make sure it is done as a last step in a cluster expansion. If the expansion of the cluster requires only one node to be added, perform the cleanup after the node has successfully completed bootstrapping and joined the cluster. If the expansion requires three nodes to be added, perform the cleanup after all three nodes have successfully completed bootstrapping and joined the cluster. This is because the cleanup will need to be executed on all nodes in the cluster, except for the last node that was added to the cluster. The last node added to the cluster will contain only the data it needed for the tokens acquired, where as other nodes may contain data for tokens they no longer have. It is still ok to run cleanup on the last node, it will likely return immediately after it is called.</p>\n<p>The cleanup can be executed on each node using the following command.</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>nodetool cleanup -j &lt;COMPACTION_SLOTS&gt;\n</pre></div></div>\n<p>Where\n <code class=\"highlighter-rouge\">&lt;COMPACTION_SLOTS&gt;</code> is the number of compaction slots to use for cleanup. By default this is <code class=\"highlighter-rouge\">2</code>. If set to <code class=\"highlighter-rouge\">0</code> it will use use all available compaction threads.</p>\n<p>It is probably worth limiting the number of compaction slots used by <code class=\"highlighter-rouge\">cleanup</code> otherwise it could potentially block compactions.</p>\n<p>The bootstrap process for a joining node can fail. Bootstrapping will put extra load on the network so should bootstrap fail, you could try tweaking the <code class=\"highlighter-rouge\">streaming_socket_timeout_in_ms</code>. Set <code class=\"highlighter-rouge\">streaming_socket_timeout_in_ms</code> in the <em>cassandra.yaml</em> file to 24 hours (60 * 60 * 24 * 1000 = 86,400,000ms). Having a socket timeout set is crucial for catching streams that hang and reporting them via an exception in the logs as per <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-11286\">CASSANDRA-11286</a>.</p>\n<p>If the bootstrap process fails in Cassandra version 2.1.x, the process will need to be restarted all over again. This can be done using the following steps.</p>\n<ol><li>Stop Cassandra on the node.</li>\n  <li>Delete all files and directories from the <em>data</em>, <em>commitlog</em> and <em>save_cache</em> directories but leave the directories there.</li>\n  <li>Wait about two minutes.</li>\n  <li>Start Cassandra on the node.</li>\n</ol><p>If the bootstrap process fails in Cassandra 2.2.x, the process can be easily be resumed using the following command thanks to <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-8942\">CASSANDRA-8942</a>.</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>nodetool bootstrap resume\n</pre></div></div>\n<p>We have gone through a lot of theory in this post, so I thought it would be good to test some of it out to demonstrate what can happen when bootstrapping multiple nodes at the same time.</p>\n<h2 id=\"setup\">Setup</h2>\n<p>In my test I used a three node local cluster running Apache Cassandra 2.1.14 which was created with the <a href=\"https://github.com/pcmanus/ccm\">ccm</a> tool. Each node was configured to use vnodes; specifically <code class=\"highlighter-rouge\">num_tokens</code> was set to <code class=\"highlighter-rouge\">32</code> in the <em>cassandra.yaml</em> file. The cluster was loaded with around 20 GB of data generated from the <a href=\"https://github.com/killrweather/killrweather\">killrweather</a> dataset. Data loading was performed in batches using <a href=\"https://github.com/rustyrazorblade/cdm\">cdm</a>. Prior to starting the test the cluster looked like this.</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>$ ccm node1 nodetool status\nDatacenter: datacenter1\n=======================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address    Load       Tokens  Owns (effective)  Host ID                               Rack\nUN  127.0.0.1  19.19 GB   32      29.1%             cfb50e13-52a4-4821-bca2-4dba6061d38a  rack1\nUN  127.0.0.2  9.55 GB    32      37.4%             5176598f-bbab-4165-8130-e33e39017f7e  rack1\nUN  127.0.0.3  19.22 GB   32      33.5%             d261faaf-628f-4b86-b60b-3825ed552aba  rack1\n</pre></div></div>\n<p>It was not the most well balanced cluster, however it was good enough for testing. It should be noted that the node with IP address <code class=\"highlighter-rouge\">127.0.0.1</code> was set to be the only seed node in the cluster. Taking a quick peak at the keyspace configuration in using CQLSH and we can see that it was using <code class=\"highlighter-rouge\">replication_factor: 1</code> i.e. <code class=\"highlighter-rouge\">RF = 1</code>.</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>cqlsh&gt; describe killrweather\nCREATE KEYSPACE killrweather WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}  AND durable_writes = true;\n</pre></div></div>\n<h2 id=\"adding-a-new-node\">Adding a new node</h2>\n<p>A new node (node4) was added to the cluster.</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>$ ccm node4 start\n</pre></div></div>\n<p>After a minute or so node4 was in the <code class=\"highlighter-rouge\">UJ</code> state and began the bootstrap process.</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>$ ccm node1 nodetool status\nDatacenter: datacenter1\n=======================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address    Load       Tokens  Owns (effective)  Host ID                               Rack\nUN  127.0.0.1  19.19 GB   32      29.1%             cfb50e13-52a4-4821-bca2-4dba6061d38a  rack1\nUN  127.0.0.2  9.55 GB    32      37.4%             5176598f-bbab-4165-8130-e33e39017f7e  rack1\nUN  127.0.0.3  19.22 GB   32      33.5%             d261faaf-628f-4b86-b60b-3825ed552aba  rack1\nUJ  127.0.0.4  14.44 KB   32      ?                 ae0a26a6-fab5-4cab-a189-697818be3c95  rack1\n</pre></div></div>\n<p>It was observed that node4 had started streaming data from node1 (IP address <code class=\"highlighter-rouge\">127.0.0.1</code>) and node2 (IP address <code class=\"highlighter-rouge\">127.0.0.2</code>).</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>$ ccm node4 nodetool netstats\nMode: JOINING\nBootstrap f4e54a00-36d9-11e7-b18e-9d89ad20c2d3\n    /127.0.0.1\n        Receiving 9 files, 10258729018 bytes total. Already received 2 files, 459059994 bytes total\n            .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-3-Data.db 452316846/452316846 bytes(100%) received from idx:0/127.0.0.1\n            .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-2-Data.db 6743148/6743148 bytes(100%) received from idx:0/127.0.0.1\n    /127.0.0.3\n    /127.0.0.2\n        Receiving 11 files, 10176549820 bytes total. Already received 1 files, 55948069 bytes total\n            .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-1-Data.db 55948069/55948069 bytes(100%) received from idx:0/127.0.0.2\nRead Repair Statistics:\nAttempted: 0\nMismatch (Blocking): 0\nMismatch (Background): 0\nPool Name                    Active   Pending      Completed\nCommands                        n/a         0              6\nResponses                       n/a         0            471\n</pre></div></div>\n<h2 id=\"adding-another-new-node\">Adding another new node</h2>\n<p>A few minutes later another new node (node5) was added to the cluster. To add this node to the cluster while node4 was bootstrapping the JVM option <code class=\"highlighter-rouge\">JVM_OPTS=\"$JVM_OPTS -Dcassandra.consistent.rangemovement=false\"</code> was added to the node’s <em>cassandra-env.sh</em> file. The node was then started.</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>$ ccm node5 start\n</pre></div></div>\n<p>After about a minute node5 was in the <code class=\"highlighter-rouge\">UJ</code> state and it too began the bootstrap process.</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>$ ccm node1 nodetool status\nDatacenter: datacenter1\n=======================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address    Load       Tokens  Owns (effective)  Host ID                               Rack\nUN  127.0.0.1  19.19 GB   32      29.1%             cfb50e13-52a4-4821-bca2-4dba6061d38a  rack1\nUN  127.0.0.2  9.55 GB    32      37.4%             5176598f-bbab-4165-8130-e33e39017f7e  rack1\nUN  127.0.0.3  19.22 GB   32      33.5%             d261faaf-628f-4b86-b60b-3825ed552aba  rack1\nUJ  127.0.0.4  106.52 KB  32      ?                 ae0a26a6-fab5-4cab-a189-697818be3c95  rack1\nUJ  127.0.0.5  14.43 KB   32      ?                 a71ed178-f353-42ec-82c8-d2b03967753a  rack1\n</pre></div></div>\n<p>It was observed that node5 had started streaming data from node2 as well; the same node that node4 was streaming data from.</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>$ ccm node5 nodetool netstats\nMode: JOINING\nBootstrap 604b5690-36da-11e7-aeb6-9d89ad20c2d3\n    /127.0.0.3\n    /127.0.0.2\n        Receiving 11 files, 10176549820 bytes total. Already received 1 files, 55948069 bytes total\n            .../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-1-Data.db 55948069/55948069 bytes(100%) received from idx:0/127.0.0.2\n    /127.0.0.1\nRead Repair Statistics:\nAttempted: 0\nMismatch (Blocking): 0\nMismatch (Background): 0\nPool Name                    Active   Pending      Completed\nCommands                        n/a         0              8\nResponses                       n/a         0            255\n</pre></div></div>\n<p>The interesting point to note when looking at the <code class=\"highlighter-rouge\">netstats</code> was that both node4 and node5 were each streaming a <em>Data.db</em> file exactly <code class=\"highlighter-rouge\">55948069</code> bytes from node2.</p>\n<h2 id=\"data-streaming-much\">Data streaming much</h2>\n<p>It had appeared that both node4 and node5 were streaming the same data from node2. This continued as the bootstrapping process progressed; the size of the files being streamed from node2 were the same for both node4 and node5. Checking the <code class=\"highlighter-rouge\">netstats</code> on node4 produced the following.</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>$ ccm node4 nodetool netstats\nBootstrap f4e54a00-36d9-11e7-b18e-9d89ad20c2d3\n    /127.0.0.1\n        Receiving 9 files, 10258729018 bytes total. Already received 6 files, 10112487796 bytes total\n            .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-13-Data.db 1788940555/1788940555 bytes(100%) received from idx:0/127.0.0.1\n            .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-5-Data.db 7384377358/7384377358 bytes(100%) received from idx:0/127.0.0.1\n            .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-12-Data.db 27960312/27960312 bytes(100%) received from idx:0/127.0.0.1\n            .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-3-Data.db 452316846/452316846 bytes(100%) received from idx:0/127.0.0.1\n            .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-11-Data.db 452149577/452149577 bytes(100%) received from idx:0/127.0.0.1\n            .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-2-Data.db 6743148/6743148 bytes(100%) received from idx:0/127.0.0.1\n    /127.0.0.3\n    /127.0.0.2\n        Receiving 11 files, 10176549820 bytes total. Already received 10 files, 10162463079 bytes total\n            .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-1-Data.db 55948069/55948069 bytes(100%) received from idx:0/127.0.0.2\n            .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-9-Data.db 55590043/55590043 bytes(100%) received from idx:0/127.0.0.2\n            .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-6-Data.db 901588743/901588743 bytes(100%) received from idx:0/127.0.0.2\n            .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-15-Data.db 14081154/14081154 bytes(100%) received from idx:0/127.0.0.2\n            .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-16-Data.db 1450179/1450179 bytes(100%) received from idx:0/127.0.0.2\n            .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-8-Data.db 901334951/901334951 bytes(100%) received from idx:0/127.0.0.2\n            .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-10-Data.db 3622476547/3622476547 bytes(100%) received from idx:0/127.0.0.2\n            .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-17-Data.db 56277615/56277615 bytes(100%) received from idx:0/127.0.0.2\n            .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-4-Data.db 3651310715/3651310715 bytes(100%) received from idx:0/127.0.0.2\n            .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-7-Data.db 902405063/902405063 bytes(100%) received from idx:0/127.0.0.2\nRead Repair Statistics:\nAttempted: 0\nMismatch (Blocking): 0\nMismatch (Background): 0\nPool Name                    Active   Pending      Completed\nCommands                        n/a         0              6\nResponses                       n/a         0           4536\n</pre></div></div>\n<p>Then checking <code class=\"highlighter-rouge\">netstats</code> on node5 produced the following.</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>$ ccm node5 nodetool netstats\nMode: JOINING\nBootstrap 604b5690-36da-11e7-aeb6-9d89ad20c2d3\n    /127.0.0.1\n    /127.0.0.3\n    /127.0.0.2\n        Receiving 11 files, 10176549820 bytes total. Already received 9 files, 10106185464 bytes total\n            .../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-2-Data.db 3651310715/3651310715 bytes(100%) received from idx:0/127.0.0.2\n            .../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-1-Data.db 55948069/55948069 bytes(100%) received from idx:0/127.0.0.2\n            .../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-9-Data.db 1450179/1450179 bytes(100%) received from idx:0/127.0.0.2\n            .../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-3-Data.db 901588743/901588743 bytes(100%) received from idx:0/127.0.0.2\n            .../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-6-Data.db 55590043/55590043 bytes(100%) received from idx:0/127.0.0.2\n            .../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-4-Data.db 902405063/902405063 bytes(100%) received from idx:0/127.0.0.2\n            .../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-8-Data.db 14081154/14081154 bytes(100%) received from idx:0/127.0.0.2\n            .../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-5-Data.db 901334951/901334951 bytes(100%) received from idx:0/127.0.0.2\n            .../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-7-Data.db 3622476547/3622476547 bytes(100%) received from idx:0/127.0.0.2\nRead Repair Statistics:\nAttempted: 0\nMismatch (Blocking): 0\nMismatch (Background): 0\nPool Name                    Active   Pending      Completed\nCommands                        n/a         0              8\nResponses                       n/a         0           4383\n</pre></div></div>\n<p>To be absolutely sure about what was being observed, I ran a command to order the <code class=\"highlighter-rouge\">netstats</code> output by file size for both node4 and node5.</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>$ for file_size in $(ccm node4 nodetool netstats  | grep '(100%)\\ received' | grep '127.0.0.2' | tr -s ' ' | cut -d' ' -f3 | cut -d'/' -f1 | sort -g); do ccm node4 nodetool netstats | grep ${file_size} | tr -s ' '; done\n .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-16-Data.db 1450179/1450179 bytes(100%) received from idx:0/127.0.0.2\n .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-15-Data.db 14081154/14081154 bytes(100%) received from idx:0/127.0.0.2\n .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-9-Data.db 55590043/55590043 bytes(100%) received from idx:0/127.0.0.2\n .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-1-Data.db 55948069/55948069 bytes(100%) received from idx:0/127.0.0.2\n .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-17-Data.db 56277615/56277615 bytes(100%) received from idx:0/127.0.0.2\n .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-8-Data.db 901334951/901334951 bytes(100%) received from idx:0/127.0.0.2\n .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-6-Data.db 901588743/901588743 bytes(100%) received from idx:0/127.0.0.2\n .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-7-Data.db 902405063/902405063 bytes(100%) received from idx:0/127.0.0.2\n .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-10-Data.db 3622476547/3622476547 bytes(100%) received from idx:0/127.0.0.2\n .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-4-Data.db 3651310715/3651310715 bytes(100%) received from idx:0/127.0.0.2\n$ for file_size in $(ccm node5 nodetool netstats  | grep '(100%)\\ received' | grep '127.0.0.2' | tr -s ' ' | cut -d' ' -f3 | cut -d'/' -f1 | sort -g); do ccm node5 nodetool netstats | grep ${file_size} | tr -s ' '; done\n .../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-9-Data.db 1450179/1450179 bytes(100%) received from idx:0/127.0.0.2\n .../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-8-Data.db 14081154/14081154 bytes(100%) received from idx:0/127.0.0.2\n .../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-6-Data.db 55590043/55590043 bytes(100%) received from idx:0/127.0.0.2\n .../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-1-Data.db 55948069/55948069 bytes(100%) received from idx:0/127.0.0.2\n .../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-5-Data.db 901334951/901334951 bytes(100%) received from idx:0/127.0.0.2\n .../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-3-Data.db 901588743/901588743 bytes(100%) received from idx:0/127.0.0.2\n .../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-4-Data.db 902405063/902405063 bytes(100%) received from idx:0/127.0.0.2\n .../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-7-Data.db 3622476547/3622476547 bytes(100%) received from idx:0/127.0.0.2\n .../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-tmp-ka-2-Data.db 3651310715/3651310715 bytes(100%) received from idx:0/127.0.0.2\n</pre></div></div>\n<p>With the exception of one file being streamed by node4, <em>killrweather-raw_weather_data-tmp-ka-17-Data.db</em> (size 56277615 bytes), node4 and node5 looked to be streaming the same data from node2. This was the first confirmation that node5 had stolen the tokens that where originally calculated by node4. Furthermore, it looked like node 4 was performing unnecessary streaming from node2. I noted down the file sizes displayed by node5’s <code class=\"highlighter-rouge\">netstats</code> output to help track down data files on each node.</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>$ ccm node5 nodetool netstats | grep '(100%)\\ received' | grep '127.0.0.2' | tr -s ' ' | cut -d' ' -f3 | cut -d'/' -f1 | sort -g &gt; file_sizes.txt; cat file_sizes.txt\n1450179\n14081154\n55590043\n55948069\n901334951\n901588743\n902405063\n3622476547\n3651310715\n</pre></div></div>\n<h2 id=\"token-and-the-thief\">Token and the thief</h2>\n<p>Once both nodes had finished bootstrapping and had successfully joined the cluster it looked like this.</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>$ ccm node1 nodetool status\nDatacenter: datacenter1\n=======================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address    Load       Tokens  Owns (effective)  Host ID                               Rack\nUN  127.0.0.1  19.19 GB   32      14.8%             cfb50e13-52a4-4821-bca2-4dba6061d38a  rack1\nUN  127.0.0.2  9.55 GB    32      22.0%             5176598f-bbab-4165-8130-e33e39017f7e  rack1\nUN  127.0.0.3  19.22 GB   32      23.6%             d261faaf-628f-4b86-b60b-3825ed552aba  rack1\nUN  127.0.0.4  19.17 GB   32      17.5%             ae0a26a6-fab5-4cab-a189-697818be3c95  rack1\nUN  127.0.0.5  9.55 GB    32      22.1%             a71ed178-f353-42ec-82c8-d2b03967753a  rack1\n</pre></div></div>\n<p>Using the file sizes I captured earlier from node5 <code class=\"highlighter-rouge\">netstats</code>, I checked the data directories of node4 and node5 to confirm both nodes contained files of those sizes.</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>$ for file_size in $(cat file_sizes.txt); do ls -al .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/ | grep ${file_size}; done\n-rw-r--r--    1 anthony  staff     1450179 12 May 16:33 killrweather-raw_weather_data-ka-16-Data.db\n-rw-r--r--    1 anthony  staff    14081154 12 May 16:33 killrweather-raw_weather_data-ka-15-Data.db\n-rw-r--r--    1 anthony  staff    55590043 12 May 16:33 killrweather-raw_weather_data-ka-9-Data.db\n-rw-r--r--    1 anthony  staff    55948069 12 May 16:33 killrweather-raw_weather_data-ka-1-Data.db\n-rw-r--r--    1 anthony  staff   901334951 12 May 16:33 killrweather-raw_weather_data-ka-8-Data.db\n-rw-r--r--    1 anthony  staff   901588743 12 May 16:33 killrweather-raw_weather_data-ka-6-Data.db\n-rw-r--r--    1 anthony  staff   902405063 12 May 16:33 killrweather-raw_weather_data-ka-7-Data.db\n-rw-r--r--    1 anthony  staff  3622476547 12 May 16:33 killrweather-raw_weather_data-ka-10-Data.db\n-rw-r--r--    1 anthony  staff  3651310715 12 May 16:33 killrweather-raw_weather_data-ka-4-Data.db\n$ for file_size in $(cat file_sizes.txt); do ls -al  .../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/ | grep ${file_size}; done\n-rw-r--r--    1 anthony  staff     1450179 12 May 16:36 killrweather-raw_weather_data-ka-9-Data.db\n-rw-r--r--    1 anthony  staff    14081154 12 May 16:36 killrweather-raw_weather_data-ka-8-Data.db\n-rw-r--r--    1 anthony  staff    55590043 12 May 16:36 killrweather-raw_weather_data-ka-6-Data.db\n-rw-r--r--    1 anthony  staff    55948069 12 May 16:36 killrweather-raw_weather_data-ka-1-Data.db\n-rw-r--r--    1 anthony  staff   901334951 12 May 16:36 killrweather-raw_weather_data-ka-5-Data.db\n-rw-r--r--    1 anthony  staff   901588743 12 May 16:36 killrweather-raw_weather_data-ka-3-Data.db\n-rw-r--r--    1 anthony  staff   902405063 12 May 16:36 killrweather-raw_weather_data-ka-4-Data.db\n-rw-r--r--    1 anthony  staff  3622476547 12 May 16:36 killrweather-raw_weather_data-ka-7-Data.db\n-rw-r--r--    1 anthony  staff  3651310715 12 May 16:36 killrweather-raw_weather_data-ka-2-Data.db\n</pre></div></div>\n<p>So both nodes contained files of the same size. I then decided to check if the files on each node that were the same size had the same data content. This check was done by performing an MD5 check of file pairs that were the same size.</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>$ BASE_DIR=...; DATA_DIR=data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d; for file_size in $(cat file_sizes.txt); do node_4_file=$(ls -al ${BASE_DIR}/node4/${DATA_DIR}/ | grep ${file_size} | tr -s ' ' | cut -d' ' -f9); node_5_file=$(ls -al ${BASE_DIR}/node5/${DATA_DIR}/ | grep ${file_size} | tr -s ' ' | cut -d' ' -f9); md5 ${BASE_DIR}/node4/${DATA_DIR}/${node_4_file} ${BASE_DIR}/node5/${DATA_DIR}/${node_5_file}; echo; done\nMD5 (.../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-16-Data.db) = a9edb85f70197c7f37aa021c817de2a2\nMD5 (.../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-9-Data.db) = a9edb85f70197c7f37aa021c817de2a2\nMD5 (.../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-15-Data.db) = 975f184ae36cbab07a9c28b032532f88\nMD5 (.../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-8-Data.db) = 975f184ae36cbab07a9c28b032532f88\nMD5 (.../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-9-Data.db) = f0160cf8e7555031b6e0835951e1896a\nMD5 (.../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-6-Data.db) = f0160cf8e7555031b6e0835951e1896a\nMD5 (.../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-1-Data.db) = 7789b794bb3ef24338282d4a1a960903\nMD5 (.../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-1-Data.db) = 7789b794bb3ef24338282d4a1a960903\nMD5 (.../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-8-Data.db) = 1738695bb6b4bd237b3592e80eb785f2\nMD5 (.../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-5-Data.db) = 1738695bb6b4bd237b3592e80eb785f2\nMD5 (.../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-6-Data.db) = f7d1faa5c59a26a260038d61e4983022\nMD5 (.../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-3-Data.db) = f7d1faa5c59a26a260038d61e4983022\nMD5 (.../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-7-Data.db) = d791179432dcdbaf9a9b315178fb04c7\nMD5 (.../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-4-Data.db) = d791179432dcdbaf9a9b315178fb04c7\nMD5 (.../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-10-Data.db) = 3e6623c2f06bcd3f5caeacee1917898b\nMD5 (.../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-7-Data.db) = 3e6623c2f06bcd3f5caeacee1917898b\nMD5 (.../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-4-Data.db) = 8775f5df08882df353427753f946bf10\nMD5 (.../node5/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-2-Data.db) = 8775f5df08882df353427753f946bf10\n</pre></div></div>\n<p>Now I had absolute proof that both nodes did in fact stream the same data from node2. It did look as though that when node5 joined the cluster it had taken tokens calculated by node4. If this were the case, it would mean that the data files on node4 that are the same on node5 would no longer be needed. One way to prove that there is “orphaned” data on node4 i.e. data not associated to any of node4’s tokens, would be to run <code class=\"highlighter-rouge\">cleanup</code> on the cluster. If there is orphaned data on node4 the cleanup would technically delete all or some of those files. Before running cleanup on the cluster, I took note of the files on node4 which were the same as the ones on node5.</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>$ for file_size in $(cat file_sizes.txt); do ls -al .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/ | grep ${file_size}; | tr -s ' ' | cut -d' '  -f9; done &gt; node4_orphaned_files.txt; cat node4_orphaned_files.txt\nkillrweather-raw_weather_data-ka-16-Data.db\nkillrweather-raw_weather_data-ka-15-Data.db\nkillrweather-raw_weather_data-ka-9-Data.db\nkillrweather-raw_weather_data-ka-1-Data.db\nkillrweather-raw_weather_data-ka-8-Data.db\nkillrweather-raw_weather_data-ka-6-Data.db\nkillrweather-raw_weather_data-ka-7-Data.db\nkillrweather-raw_weather_data-ka-10-Data.db\nkillrweather-raw_weather_data-ka-4-Data.db\n</pre></div></div>\n<p>I then ran a <code class=\"highlighter-rouge\">cleanup</code> on all the nodes in the cluster.</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>$ ccm node1 nodetool cleanup\n$ ccm node2 nodetool cleanup\n$ ccm node3 nodetool cleanup\n$ ccm node4 nodetool cleanup\n$ ccm node5 nodetool cleanup\n$ ccm node1 nodetool status\nDatacenter: datacenter1\n=======================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address    Load       Tokens  Owns (effective)  Host ID                               Rack\nUN  127.0.0.1  9.57 GB    32      14.8%             cfb50e13-52a4-4821-bca2-4dba6061d38a  rack1\nUN  127.0.0.2  138.92 KB  32      22.0%             5176598f-bbab-4165-8130-e33e39017f7e  rack1\nUN  127.0.0.3  19.22 GB   32      23.6%             d261faaf-628f-4b86-b60b-3825ed552aba  rack1\nUN  127.0.0.4  9.62 GB    32      17.5%             ae0a26a6-fab5-4cab-a189-697818be3c95  rack1\nUN  127.0.0.5  9.55 GB    32      22.1%             a71ed178-f353-42ec-82c8-d2b03967753a  rack1\n</pre></div></div>\n<p>From this output it was obvious that node4 contained orphaned data. Earlier I had run a <code class=\"highlighter-rouge\">nodetool status</code> which was just after both nodes completed bootstrapping and moved to the <code class=\"highlighter-rouge\">UN</code> state, and prior to running <code class=\"highlighter-rouge\">cleanup</code>. The output produced at that point showed that node4 had a <em>Load</em> of <code class=\"highlighter-rouge\">19.17 GB</code>. Now after cleanup it was showing to have a load of <code class=\"highlighter-rouge\">9.62 GB</code>. As a final verification, I iterated through the list of files on node4 which were the same as the ones on node5 (<em>node4_orphaned_files.txt</em>) and checked if they still were present on node4.</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre>$ for file_name in $(cat node4_orphaned_files.txt); do ls .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/${file_name}; done\nls: .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-16-Data.db: No such file or directory\nls: .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-15-Data.db: No such file or directory\nls: .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-9-Data.db: No such file or directory\nls: .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-1-Data.db: No such file or directory\nls: .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-8-Data.db: No such file or directory\nls: .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-6-Data.db: No such file or directory\nls: .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-7-Data.db: No such file or directory\nls: .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-10-Data.db: No such file or directory\nls: .../node4/data0/killrweather/raw_weather_data-32f23d1015cb11e79d0fa90042a0802d/killrweather-raw_weather_data-ka-4-Data.db: No such file or directory\n</pre></div></div>\n<p>As it can be seen the files were deleted as part of the cleanup on node4. Which means that during bootstrap node4 originally calculated tokens for that data. It then asked for a list of files that related to those tokens from node2 and began streaming them. A little while later node5 was added to the cluster while node4 was still bootstrapping. It then calculated tokens that overlapped with node4’s tokens. Node5 then asked for a list of files that related to those tokens from node2 and started streaming data for them as well. The issue here is node4 was never notified that it no longer required to stream files from node2. Hence, unnecessary resources were being consumed as a result of bootstrapping two nodes at the same time.</p>\n<p>Auto bootstrapping combined with vnodes is probably one of the most handy features in Cassandra. It takes the pain out of manually having to move data around ensure a continuous availability while expanding the cluster in a reliable and efficient way. There a number of knobs and levers for controlling the default behaviour of bootstrapping.</p>\n<h2 id=\"configuration-properties\">Configuration properties</h2>\n<ul><li><code class=\"highlighter-rouge\">auto_bootstrap</code> - controls whether data is streamed to the new node when inserted.</li>\n  <li><code class=\"highlighter-rouge\">streaming_socket_timeout_in_ms</code> - sets socket timeout for streaming operations.</li>\n</ul><h2 id=\"jvm-options\">JVM options</h2>\n<ul><li><code class=\"highlighter-rouge\">cassandra.consistent.rangemovement</code> - controls consistent range movements and multiple node bootstrapping.</li>\n  <li><code class=\"highlighter-rouge\">cassandra.replace_address_first_boot=&lt;IP_ADDRESS&gt;</code> - allows a down node to be replaced with a new node.</li>\n</ul><p>As demonstrated by setting the JVM option <code class=\"highlighter-rouge\">cassandra.consistent.rangemovement=false</code> the cluster runs the risk of over streaming of data and worse still, it can violate consistency. For new users to Cassandra, the safest way to add multiple nodes into a cluster is to add them one at a time. Stay tuned as I will be following up with another post on bootstrapping.</p>",
        "created_at": "2017-10-19T14:36:28+0000",
        "updated_at": "2017-12-27T13:32:20+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "",
        "reading_time": 45,
        "domain_name": "thelastpickle.com",
        "preview_picture": "http://thelastpickle.com/files/auto-bootstrapping/CASSANDRA-2434-data-inconsistency-figure-01.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/5115"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 869,
            "label": "devops",
            "slug": "devops"
          }
        ],
        "is_public": false,
        "id": 5093,
        "uid": null,
        "title": "Everyday I’m scaling... Cassandra",
        "url": "https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra",
        "content": "Everyday I’m scaling... Cassandra\n      \n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            </a><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div>\n    <div class=\"wrapper\"><p>Successfully reported this slideshow.</p><div id=\"slideview-container\" class=\"\"><div class=\"row\"><div id=\"main-panel\" class=\"small-12 large-8 columns\"><div class=\"sectionElements\"><div class=\"playerWrapper\"><div><div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\">Everyday I’m scaling... Cassandra<div class=\"stage valign-first-slide\"><div class=\"slide_container\"><section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-1-638.jpg?cb=1473726554\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-1-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-1-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-1-1024.jpg?cb=1473726554\" alt=\"Ben Bromhead&#10;Cassandra… Every day I’m scaling&#10;\" /></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-2-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-2-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-2-1024.jpg?cb=1473726554\" alt=\"2© DataStax, All Rights Reserved.&#10;\" /></i></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-3-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-3-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-3-1024.jpg?cb=1473726554\" alt=\"Who am I and What do I do?&#10;• Co-founder and CTO of Instaclustr -&gt; www.instaclustr.com&#10;• Instaclustr provides Cassandra-as-...\" /></i></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-4-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-4-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-4-1024.jpg?cb=1473726554\" alt=\"1 Why scaling sucks in Cassandra&#10;2 It gets better&#10;3 Then it gets really awesome&#10;4© DataStax, All Rights Reserved.&#10;\" /></i></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-5-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-5-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-5-1024.jpg?cb=1473726554\" alt=\"Linear Scalability – In theory&#10;© DataStax, All Rights Reserved. 5&#10;\" /></i></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-6-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-6-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-6-1024.jpg?cb=1473726554\" alt=\"Linear Scalability – In practice&#10;© DataStax, All Rights Reserved. 6&#10;\" /></i></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-7-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-7-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-7-1024.jpg?cb=1473726554\" alt=\"What’s supposed to happen&#10;• Scaling Cassandra is just “bootstrap new nodes”&#10;• That works if your cluster is under provisio...\" /></i></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-8-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-8-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-8-1024.jpg?cb=1473726554\" alt=\"What actually happens&#10;• Add 1 node&#10;• Bootstrapping node fails (1 day)&#10;• WTF - Full disk on bootstrapping node? (5 minutes)...\" /></i></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-9-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-9-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-9-1024.jpg?cb=1473726554\" alt=\"What actually happens&#10;• Restart bootstrapping process&#10;• Disk alert 70% (2 days later)&#10;• Throttle streaming throughput to b...\" /></i></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-10-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-10-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-10-1024.jpg?cb=1473726554\" alt=\"What actually happens&#10;© DataStax, All Rights Reserved. 10&#10;\" /></i></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-11-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-11-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-11-1024.jpg?cb=1473726554\" alt=\"Scalability in Cassandra sucks&#10;• Soo much over streaming&#10;• LCS and Bootstrap – Over stream then compact all the data!&#10;• ST...\" /></i></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-12-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-12-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-12-1024.jpg?cb=1473726554\" alt=\"Scalability in Cassandra sucks&#10;• No vnodes? – Can only double your cluster&#10;• Vnodes? – Can only add one node at a time&#10;• B...\" /></i></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-13-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-13-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-13-1024.jpg?cb=1473726554\" alt=\"Why does it suck for you&#10;Your database never meets your business requirements from a capacity perspective&#10;(bad) and if you...\" /></i></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-14-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-14-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-14-1024.jpg?cb=1473726554\" alt=\"How did it get this way?&#10;It’s actually a hard problem:&#10;• Moving large amounts of data between nodes requires just as much ...\" /></i></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-15-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-15-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-15-1024.jpg?cb=1473726554\" alt=\"Does it get better?&#10;© DataStax, All Rights Reserved. 15&#10;Yes!&#10;\" /></i></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-16-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-16-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-16-1024.jpg?cb=1473726554\" alt=\"Does it get better? Consistent bootstrap&#10;Strongly consistent membership and ownership – CASSANDRA-9667&#10;• Using LWT to prop...\" /></i></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-17-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-17-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-17-1024.jpg?cb=1473726554\" alt=\"Does it get better? Bootstrap stability&#10;Keep-alives for all streaming operations – CASSANDRA-11841&#10;• Currently implements ...\" /></i></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-18-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-18-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-18-1024.jpg?cb=1473726554\" alt=\"Can we make it even better?&#10;© DataStax, All Rights Reserved. 18&#10;Yes!&#10;\" /></i></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-19-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-19-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-19-1024.jpg?cb=1473726554\" alt=\"Can we make it even better?&#10;© DataStax, All Rights Reserved. 19&#10;• Let’s try scaling without data ownership changes&#10;• Take ...\" /></i></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-20-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-20-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-20-1024.jpg?cb=1473726554\" alt=\"Introducing token pinned scaling&#10;© DataStax, All Rights Reserved. 20&#10;• Probably needs a better name&#10;• Here is how it works&#10;\" /></i></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-21-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-21-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-21-1024.jpg?cb=1473726554\" alt=\"Introducing token pinned scaling&#10;© DataStax, All Rights Reserved. 21&#10;With the introduction of:&#10;• Partitioning SSTables by ...\" /></i></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-22-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-22-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-22-1024.jpg?cb=1473726554\" alt=\"Introducing token pinned scaling&#10;© DataStax, All Rights Reserved. 22&#10;Before Partitioning SSTables by Range and Range Aware...\" /></i></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-23-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-23-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-23-1024.jpg?cb=1473726554\" alt=\"Introducing token pinned scaling&#10;© DataStax, All Rights Reserved. 23&#10;After Partitioning SSTables by Range and Range Aware ...\" /></i></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-24-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-24-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-24-1024.jpg?cb=1473726554\" alt=\"Introducing token pinned scaling&#10;© DataStax, All Rights Reserved. 24&#10;Your SSTables will converge to contain a single vnode...\" /></i></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-25-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-25-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-25-1024.jpg?cb=1473726554\" alt=\"Leveraging EBS to separate I/O from CPU&#10;© DataStax, All Rights Reserved. 25&#10;• Amazon Web Services provides a networked att...\" /></i></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-26-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-26-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-26-1024.jpg?cb=1473726554\" alt=\"Adding it all together&#10;© DataStax, All Rights Reserved. 26&#10;• Make each EBS disk a data directory in Cassandra&#10;• Cassandra ...\" /></i></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-27-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-27-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-27-1024.jpg?cb=1473726554\" alt=\"Adding it all together&#10;© DataStax, All Rights Reserved. 27&#10;• Make each EBS disk a data directory in Cassandra&#10;sda&#10;sdd&#10;sdb&#10;...\" /></i></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-28-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-28-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-28-1024.jpg?cb=1473726554\" alt=\"Adding it all together&#10;© DataStax, All Rights Reserved. 28&#10;• Cassandra guarantees only data from a specific token range wi...\" /></i></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-29-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-29-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-29-1024.jpg?cb=1473726554\" alt=\"Adding it all together&#10;© DataStax, All Rights Reserved. 29&#10;• When throughput is low attach all disks in a single AZ to a s...\" /></i></section><section data-index=\"30\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-30-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-30-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-30-1024.jpg?cb=1473726554\" alt=\"Adding it all together&#10;© DataStax, All Rights Reserved. 30&#10;• When load is high, launch more instances and spread the disks...\" /></i></section><section data-index=\"31\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-31-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-31-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-31-1024.jpg?cb=1473726554\" alt=\"How it works - Scaling&#10;© DataStax, All Rights Reserved. 31&#10;• Normally you have to provision your cluster at your maximum o...\" /></i></section><section data-index=\"32\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-32-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-32-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-32-1024.jpg?cb=1473726554\" alt=\"© DataStax, All Rights Reserved. 32&#10;\" /></i></section><section data-index=\"33\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-33-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-33-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-33-1024.jpg?cb=1473726554\" alt=\"How it works - Scaling&#10;© DataStax, All Rights Reserved. 33&#10;• Let’s make our resources match our workload&#10;Provisioned IOPS ...\" /></i></section><section data-index=\"34\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-34-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-34-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-34-1024.jpg?cb=1473726554\" alt=\"How it works - Scaling&#10;© DataStax, All Rights Reserved. 34&#10;• Let’s make our resources match our workload&#10;Provisioned IOPS ...\" /></i></section><section data-index=\"35\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-35-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-35-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-35-1024.jpg?cb=1473726554\" alt=\"How it works - Consistency&#10;© DataStax, All Rights Reserved. 35&#10;• No range movements! You don’t need a Jepsen test to see h...\" /></i></section><section data-index=\"36\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-36-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-36-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-36-1024.jpg?cb=1473726554\" alt=\"How it works - Consistency&#10;© DataStax, All Rights Reserved. 36&#10;• Treats Racks as a giant “meta-node”, network topology str...\" /></i></section><section data-index=\"37\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-37-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-37-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-37-1024.jpg?cb=1473726554\" alt=\"How it works - Consistency&#10;© DataStax, All Rights Reserved. 37&#10;1,5,10&#10;2,6,11&#10;3, 22, 44&#10;4,23,45&#10;102,134,167&#10;101,122,155&#10;1,2...\" /></i></section><section data-index=\"38\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-38-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-38-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-38-1024.jpg?cb=1473726554\" alt=\"How it works - Consistency&#10;© DataStax, All Rights Reserved. 38&#10;1,5,10&#10;2,6,11&#10;3, 22, 44&#10;4,23,45&#10;102,134,167&#10;101,122,155&#10;1,5...\" /></i></section><section data-index=\"39\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-39-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-39-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-39-1024.jpg?cb=1473726554\" alt=\"How it works - TODO&#10;© DataStax, All Rights Reserved. 39&#10;Some issues remain:&#10;• Hinted handoff breaks (handoff is based on e...\" /></i></section><section data-index=\"40\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-40-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-40-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-40-1024.jpg?cb=1473726554\" alt=\"How it works – Real world&#10;© DataStax, All Rights Reserved. 40&#10;• No production tests yet &#10;• Have gone from a 3 node cluste...\" /></i></section><section data-index=\"41\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-41-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-41-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-41-1024.jpg?cb=1473726554\" alt=\"How it works – Real world&#10;© DataStax, All Rights Reserved. 41&#10;• Really this is bending some new and impending changes to d...\" /></i></section><section data-index=\"42\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-42-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-42-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-42-1024.jpg?cb=1473726554\" alt=\"Questions?&#10;\" /></i></section><section data-index=\"43\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/Instaclustr/everyday-im-scaling-cassandra\" data-small=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/85/everyday-im-scaling-cassandra-43-320.jpg?cb=1473726554\" data-normal=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-43-638.jpg?cb=1473726554\" data-full=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-43-1024.jpg?cb=1473726554\" alt=\"Questions?&#10;\" /></i></section><div class=\"j-next-container next-container\"><div class=\"content-container\"><div class=\"next-slideshow-wrapper\"><div class=\"j-next-slideshow next-slideshow\"><p>Upcoming SlideShare</p></div><p>Loading in …5</p><p>×</p></div></div></div></div></div></div></div></div></div><div class=\"slideshow-info-container\" itemscope=\"itemscope\" itemtype=\"https://schema.org/MediaObject\"><div class=\"slideshow-tabs-container show-for-medium-up\"><ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    0 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\">\n                \n                  0 Likes\n                \n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\">\n                Statistics\n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\">\n                Notes\n              </i></a>\n            </li>\n          </ul><div class=\"tabs-content\"><div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\"><ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"itemscope\"><li>\n                    <p class=\"empty-stat-box text-center\">\n                      <em>Be the first to like this</em>\n                    </p>\n                  </li>\n              </ul></div><div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No Downloads</p></div><div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No notes for slide</p></div></div></div><div class=\"notranslate transcript add-padding-right j-transcript\"><ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    Ben Bromhead\nCassandra… Every day I’m scaling\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-2-638.jpg?cb=1473726554\" title=\"2© DataStax, All Rights Reserved.&#10;\" target=\"_blank\">\n        2.\n      </a>\n    2© DataStax, All Rights Reserved.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-3-638.jpg?cb=1473726554\" title=\"Who am I and What do I do?&#10;• Co-founder and CTO of Instaclu...\" target=\"_blank\">\n        3.\n      </a>\n    Who am I and What do I do?\n• Co-founder and CTO of Instaclustr -&gt; www.instaclustr.com\n• Instaclustr provides Cassandra-as-a-Service in the cloud.\n• Currently support AWS, Azure, Heroku, Softlayer and Private DCs with more to come.\n• Approaching 1000 nodes under management\n• Yes… we are hiring! Come live in Australia!\n© DataStax, All Rights Reserved. 3\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-4-638.jpg?cb=1473726554\" title=\"1 Why scaling sucks in Cassandra&#10;2 It gets better&#10;3 Then it...\" target=\"_blank\">\n        4.\n      </a>\n    1 Why scaling sucks in Cassandra\n2 It gets better\n3 Then it gets really awesome\n4© DataStax, All Rights Reserved.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-5-638.jpg?cb=1473726554\" title=\"Linear Scalability – In theory&#10;© DataStax, All Rights Reser...\" target=\"_blank\">\n        5.\n      </a>\n    Linear Scalability – In theory\n© DataStax, All Rights Reserved. 5\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-6-638.jpg?cb=1473726554\" title=\"Linear Scalability – In practice&#10;© DataStax, All Rights Res...\" target=\"_blank\">\n        6.\n      </a>\n    Linear Scalability – In practice\n© DataStax, All Rights Reserved. 6\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-7-638.jpg?cb=1473726554\" title=\"What’s supposed to happen&#10;• Scaling Cassandra is just “boot...\" target=\"_blank\">\n        7.\n      </a>\n    What’s supposed to happen\n• Scaling Cassandra is just “bootstrap new nodes”\n• That works if your cluster is under provisioned and has 30% disk usage\n© DataStax, All Rights Reserved. 7\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-8-638.jpg?cb=1473726554\" title=\"What actually happens&#10;• Add 1 node&#10;• Bootstrapping node fai...\" target=\"_blank\">\n        8.\n      </a>\n    What actually happens\n• Add 1 node\n• Bootstrapping node fails (1 day)\n• WTF - Full disk on bootstrapping node? (5 minutes)\n• If STCS run SSTableSplit on large SSTables on original nodes (2 days)\n• Attach super sized network storage (EBS) and bind mount to bootstrapping node.\n© DataStax, All Rights Reserved. 8\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-9-638.jpg?cb=1473726554\" title=\"What actually happens&#10;• Restart bootstrapping process&#10;• Dis...\" target=\"_blank\">\n        9.\n      </a>\n    What actually happens\n• Restart bootstrapping process\n• Disk alert 70% (2 days later)\n• Throttle streaming throughput to below compaction throughput\n• Bootstrapping finishes (5 days later)\n• Cluster latency spikes cause bootstrap finished but their was a million compactions\nremaining\n• Take node offline and let compaction finish\n• Run repair on node (10 years)\n• Add next node.\n© DataStax, All Rights Reserved. 9\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-10-638.jpg?cb=1473726554\" title=\"What actually happens&#10;© DataStax, All Rights Reserved. 10&#10;\" target=\"_blank\">\n        10.\n      </a>\n    What actually happens\n© DataStax, All Rights Reserved. 10\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-11-638.jpg?cb=1473726554\" title=\"Scalability in Cassandra sucks&#10;• Soo much over streaming&#10;• ...\" target=\"_blank\">\n        11.\n      </a>\n    Scalability in Cassandra sucks\n• Soo much over streaming\n• LCS and Bootstrap – Over stream then compact all the data!\n• STCS and bootstrap – Over stream all the data and run out of disk space\n© DataStax, All Rights Reserved. 11\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-12-638.jpg?cb=1473726554\" title=\"Scalability in Cassandra sucks&#10;• No vnodes? – Can only doub...\" target=\"_blank\">\n        12.\n      </a>\n    Scalability in Cassandra sucks\n• No vnodes? – Can only double your cluster\n• Vnodes? – Can only add one node at a time\n• Bootstrap – Fragile and not guaranteed to be consistent\n© DataStax, All Rights Reserved. 12\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-13-638.jpg?cb=1473726554\" title=\"Why does it suck for you&#10;Your database never meets your bus...\" target=\"_blank\">\n        13.\n      </a>\n    Why does it suck for you\nYour database never meets your business requirements from a capacity perspective\n(bad) and if you try…\n• You could interrupt availability and performance (really bad)\n• You could loose data (really really bad)\n© DataStax, All Rights Reserved. 13\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-14-638.jpg?cb=1473726554\" title=\"How did it get this way?&#10;It’s actually a hard problem:&#10;• Mo...\" target=\"_blank\">\n        14.\n      </a>\n    How did it get this way?\nIt’s actually a hard problem:\n• Moving large amounts of data between nodes requires just as much attention to it\nfrom a CAP perspective as client facing stuff.\n• New features don’t tend to consider impact on scaling operations\n• Features that help ops tends to be less sexy\n© DataStax, All Rights Reserved. 14\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-15-638.jpg?cb=1473726554\" title=\"Does it get better?&#10;© DataStax, All Rights Reserved. 15&#10;Yes!&#10;\" target=\"_blank\">\n        15.\n      </a>\n    Does it get better?\n© DataStax, All Rights Reserved. 15\nYes!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-16-638.jpg?cb=1473726554\" title=\"Does it get better? Consistent bootstrap&#10;Strongly consisten...\" target=\"_blank\">\n        16.\n      </a>\n    Does it get better? Consistent bootstrap\nStrongly consistent membership and ownership – CASSANDRA-9667\n• Using LWT to propose and claim ownership of new token allocations in a consistent\nmanner\n• Work in progress\n• You can do this today by pre-assigning non-overlapping (inc replicas) vnode tokens\nand using cassandra.consistent.simultaneousmoves.allow=true as a JVM property\nbefore bootstrapping your nodes\n© DataStax, All Rights Reserved. 16\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-17-638.jpg?cb=1473726554\" title=\"Does it get better? Bootstrap stability&#10;Keep-alives for all...\" target=\"_blank\">\n        17.\n      </a>\n    Does it get better? Bootstrap stability\nKeep-alives for all streaming operations – CASSANDRA-11841\n• Currently implements a timeout, you can reduce this to be more aggressive, but large\nSSTables will then never stream\nResummable bootstrap – CASSANDRA-8942 &amp; CASSANDRA-8838\n• You can do this in 2.2+\nIncremental bootstrap – CASSANDRA-8494\n• Being worked on, hard to do with vnodes right now (try it… the error message uses\nthe word “thusly”), instead throttle streaming and uncap compaction to ensure the\nnode doesn’t get overloaded during bootstrap\n© DataStax, All Rights Reserved. 17\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-18-638.jpg?cb=1473726554\" title=\"Can we make it even better?&#10;© DataStax, All Rights Reserved...\" target=\"_blank\">\n        18.\n      </a>\n    Can we make it even better?\n© DataStax, All Rights Reserved. 18\nYes!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-19-638.jpg?cb=1473726554\" title=\"Can we make it even better?&#10;© DataStax, All Rights Reserved...\" target=\"_blank\">\n        19.\n      </a>\n    Can we make it even better?\n© DataStax, All Rights Reserved. 19\n• Let’s try scaling without data ownership changes\n• Take advantage of Cassandras normal partition and availability mechanisms\n• With a little help from our cloud providers!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-20-638.jpg?cb=1473726554\" title=\"Introducing token pinned scaling&#10;© DataStax, All Rights Res...\" target=\"_blank\">\n        20.\n      </a>\n    Introducing token pinned scaling\n© DataStax, All Rights Reserved. 20\n• Probably needs a better name\n• Here is how it works\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-21-638.jpg?cb=1473726554\" title=\"Introducing token pinned scaling&#10;© DataStax, All Rights Res...\" target=\"_blank\">\n        21.\n      </a>\n    Introducing token pinned scaling\n© DataStax, All Rights Reserved. 21\nWith the introduction of:\n• Partitioning SSTables by Range (CASSANDRA-6696)\n• Range Aware Compaction (CASSANDRA-10540)\n• A few extra lines of code to save/load a map of token to disks (coming soon)\nCassandra will now keep data associated with specific tokens in a single data directory,\nthis could let us treat a disk as a unit in which to scale around!\nBut first what do these two features actually let us do?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-22-638.jpg?cb=1473726554\" title=\"Introducing token pinned scaling&#10;© DataStax, All Rights Res...\" target=\"_blank\">\n        22.\n      </a>\n    Introducing token pinned scaling\n© DataStax, All Rights Reserved. 22\nBefore Partitioning SSTables by Range and Range Aware Compaction:\n1 - 100\n901 - 1000\n1401-1500\nDisk0 Disk1\nSSTables\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-23-638.jpg?cb=1473726554\" title=\"Introducing token pinned scaling&#10;© DataStax, All Rights Res...\" target=\"_blank\">\n        23.\n      </a>\n    Introducing token pinned scaling\n© DataStax, All Rights Reserved. 23\nAfter Partitioning SSTables by Range and Range Aware Compaction:\n1 - 100\n901 - 1000\n1401-1500\nDisk0 Disk1\nSSTables\nData within a token range is now kept on a specific disk\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-24-638.jpg?cb=1473726554\" title=\"Introducing token pinned scaling&#10;© DataStax, All Rights Res...\" target=\"_blank\">\n        24.\n      </a>\n    Introducing token pinned scaling\n© DataStax, All Rights Reserved. 24\nYour SSTables will converge to contain a single vnode range when things get big enough\n1 - 100\n901 - 1000\n1401-1500\nDisk0 Disk1\nSSTables\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-25-638.jpg?cb=1473726554\" title=\"Leveraging EBS to separate I/O from CPU&#10;© DataStax, All Rig...\" target=\"_blank\">\n        25.\n      </a>\n    Leveraging EBS to separate I/O from CPU\n© DataStax, All Rights Reserved. 25\n• Amazon Web Services provides a networked attached block store called EBS (Elastic\nBlock Store).\n• Isolated to each availability zone\n• We can attach and reattach EBS disk ad-hoc and in seconds/minutes\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-26-638.jpg?cb=1473726554\" title=\"Adding it all together&#10;© DataStax, All Rights Reserved. 26&#10;...\" target=\"_blank\">\n        26.\n      </a>\n    Adding it all together\n© DataStax, All Rights Reserved. 26\n• Make each EBS disk a data directory in Cassandra\n• Cassandra guarantees only data from a specific token range will exist on a given disk\n• When throughput is low attach all disks in a single AZ to a single node, specify all the\nranges from each disk via a comma separated list of tokens.\n• Up to 40 disks per instance!\n• When load is high, launch more instances and spread the disks across the new\ninstances.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-27-638.jpg?cb=1473726554\" title=\"Adding it all together&#10;© DataStax, All Rights Reserved. 27&#10;...\" target=\"_blank\">\n        27.\n      </a>\n    Adding it all together\n© DataStax, All Rights Reserved. 27\n• Make each EBS disk a data directory in Cassandra\nsda\nsdd\nsdb\nsde\nsdc\nsdf\nAmazon EBS\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-28-638.jpg?cb=1473726554\" title=\"Adding it all together&#10;© DataStax, All Rights Reserved. 28&#10;...\" target=\"_blank\">\n        28.\n      </a>\n    Adding it all together\n© DataStax, All Rights Reserved. 28\n• Cassandra guarantees only data from a specific token range will exist on a given disk\nAmazon EBS\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-29-638.jpg?cb=1473726554\" title=\"Adding it all together&#10;© DataStax, All Rights Reserved. 29&#10;...\" target=\"_blank\">\n        29.\n      </a>\n    Adding it all together\n© DataStax, All Rights Reserved. 29\n• When throughput is low attach all disks in a single AZ to a single node\nAmazon EBS\n200 op/s\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-30-638.jpg?cb=1473726554\" title=\"Adding it all together&#10;© DataStax, All Rights Reserved. 30&#10;...\" target=\"_blank\">\n        30.\n      </a>\n    Adding it all together\n© DataStax, All Rights Reserved. 30\n• When load is high, launch more instances and spread the disks across the new instances.\nAmazon EBS\n10,000 op/s\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-31-638.jpg?cb=1473726554\" title=\"How it works - Scaling&#10;© DataStax, All Rights Reserved. 31&#10;...\" target=\"_blank\">\n        31.\n      </a>\n    How it works - Scaling\n© DataStax, All Rights Reserved. 31\n• Normally you have to provision your cluster at your maximum operations per second +\n30% (headroom in case your get it wrong).\n• Provision enough IOPS, CPU, RAM etc\n• Makes Cassandra an $$$ solution\nProvisioned workload\nActual workload\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-32-638.jpg?cb=1473726554\" title=\"© DataStax, All Rights Reserved. 32&#10;\" target=\"_blank\">\n        32.\n      </a>\n    © DataStax, All Rights Reserved. 32\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-33-638.jpg?cb=1473726554\" title=\"How it works - Scaling&#10;© DataStax, All Rights Reserved. 33&#10;...\" target=\"_blank\">\n        33.\n      </a>\n    How it works - Scaling\n© DataStax, All Rights Reserved. 33\n• Let’s make our resources match our workload\nProvisioned IOPS workload\nActual workload\nProvisioned CPU &amp; RAM workload\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-34-638.jpg?cb=1473726554\" title=\"How it works - Scaling&#10;© DataStax, All Rights Reserved. 34&#10;...\" target=\"_blank\">\n        34.\n      </a>\n    How it works - Scaling\n© DataStax, All Rights Reserved. 34\n• Let’s make our resources match our workload\nProvisioned IOPS workload\nActual workload\nProvisioned CPU &amp; RAM workload\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-35-638.jpg?cb=1473726554\" title=\"How it works - Consistency&#10;© DataStax, All Rights Reserved....\" target=\"_blank\">\n        35.\n      </a>\n    How it works - Consistency\n© DataStax, All Rights Reserved. 35\n• No range movements! You don’t need a Jepsen test to see how bad range\nmovements are for consistency.\n• Tokens and ranges are fixed during all scaling operations\n• Range movements are where you see most consistency badness in Cassandra\n(bootstrap, node replacement, decommission) and need to rely on repair.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-36-638.jpg?cb=1473726554\" title=\"How it works - Consistency&#10;© DataStax, All Rights Reserved....\" target=\"_blank\">\n        36.\n      </a>\n    How it works - Consistency\n© DataStax, All Rights Reserved. 36\n• Treats Racks as a giant “meta-node”, network topology strategy ensures replicas are\non different racks.\n• AWS Rack == AZ\n• As tokens for a node change based on the disk they have, replica topology stays the\nsame\n• You can only swap disks between instances within the same AZ\n• Scale one rack at a time… scale your cluster in constant time!\n• If you want to do this with a single rack, you will have a bad time\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-37-638.jpg?cb=1473726554\" title=\"How it works - Consistency&#10;© DataStax, All Rights Reserved....\" target=\"_blank\">\n        37.\n      </a>\n    How it works - Consistency\n© DataStax, All Rights Reserved. 37\n1,5,10\n2,6,11\n3, 22, 44\n4,23,45\n102,134,167\n101,122,155\n1,2,3,4,5,6,10,11,22,23,44,45,101 …\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-38-638.jpg?cb=1473726554\" title=\"How it works - Consistency&#10;© DataStax, All Rights Reserved....\" target=\"_blank\">\n        38.\n      </a>\n    How it works - Consistency\n© DataStax, All Rights Reserved. 38\n1,5,10\n2,6,11\n3, 22, 44\n4,23,45\n102,134,167\n101,122,155\n1,5,10\n2,6,11\n3,22,44\n4,23,45\n102,134,167\n101,122,155\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-39-638.jpg?cb=1473726554\" title=\"How it works - TODO&#10;© DataStax, All Rights Reserved. 39&#10;Som...\" target=\"_blank\">\n        39.\n      </a>\n    How it works - TODO\n© DataStax, All Rights Reserved. 39\nSome issues remain:\n• Hinted handoff breaks (handoff is based on endpoint rather than token)\n• Time for gossip to settle on any decent sized cluster\n• Currently just clearing out the system.local folder to allow booting\n• Can’t do this while repair is running… for some people this is all the time\n• You’ll need to run repair more often as scaling intentionally introduces outages\n• Breaks consistency and everything where RF &gt; number of racks (usually the\nsystem_auth keyspace).\n• More work needed!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-40-638.jpg?cb=1473726554\" title=\"How it works – Real world&#10;© DataStax, All Rights Reserved. ...\" target=\"_blank\">\n        40.\n      </a>\n    How it works – Real world\n© DataStax, All Rights Reserved. 40\n• No production tests yet \n• Have gone from a 3 node cluster to a 36 node cluster in around 50 minutes.\n• Plenty left to optimize (e.g. bake everything into an AMI to reduce startup time)\n• Could get this down to 10 minutes per rack depending on how responsive AWS is!\n• No performance overhead compared to Cassandra on EBS.\n• Check out the code here: https://github.com/benbromhead/Cassandra/tree/ic-token-\npinning\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-41-638.jpg?cb=1473726554\" title=\"How it works – Real world&#10;© DataStax, All Rights Reserved. ...\" target=\"_blank\">\n        41.\n      </a>\n    How it works – Real world\n© DataStax, All Rights Reserved. 41\n• Really this is bending some new and impending changes to do funky stuff \n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-42-638.jpg?cb=1473726554\" title=\"Questions?&#10;\" target=\"_blank\">\n        42.\n      </a>\n    Questions?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/summit2016presentation-160912235725/95/everyday-im-scaling-cassandra-43-638.jpg?cb=1473726554\" title=\"Questions?&#10;\" target=\"_blank\">\n        43.\n      </a>\n    Questions?\n \n  </li>\n              </ol></div></div></div><aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\">\n<dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\"><ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"65954721\" title=\"PowerPoint Tips Weekly\" href=\"https://www.linkedin.com/learning/powerpoint-tips-weekly?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_PowerPoint Tips Weekly\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"PowerPoint Tips Weekly\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=6cG1Zal7XCcFk5gV4KXBYpFAzJA%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-kWCGs_tGfZXTtf8PbZLSiol4Sey4GlgA6f-yoSTXpE469LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>PowerPoint Tips Weekly</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"65954721\" title=\"Information Literacy\" href=\"https://www.linkedin.com/learning/information-literacy?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Information Literacy\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Information Literacy\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=MblVxiytNLshOk%2FjzlsgLNOuz08%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-iXS2q_tOfY3HhecPZZLSiol4TfCsClQUzeuarRzXkEI69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Information Literacy</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"65954721\" title=\"PowerPoint 2016: Tips and Tricks\" href=\"https://www.linkedin.com/learning/powerpoint-2016-tips-and-tricks?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_PowerPoint 2016: Tips and Tricks\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"PowerPoint 2016: Tips and Tricks\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=nIcokYvYRZ365i4sqNHaREMZ6bI%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lUyKj_tWfZH_ucMPfZLSiol8eeywDlAE0e-moQTPjFI69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>PowerPoint 2016: Tips and Tricks</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"66407043\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Getting Started with Apache Cassandra and Apache Zeppelin (DuyHai DOAN, DataStax) | C* Summit 2016\" href=\"https://www.slideshare.net/DataStax/getting-started-with-apache-cassandra-and-apache-zeppelin-duyhai-doan-datastax-c-summit-2016\">\n    \n    <div class=\"related-content\"><p>Getting Started with Apache Cassandra and Apache Zeppelin (DuyHai DOAN, DataS...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"61926447\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Time Series Processing with Apache Spark\" href=\"https://www.slideshare.net/adersberger/time-series-processing-with-apache-spark\">\n    \n    <div class=\"related-content\"><p>Time Series Processing with Apache Spark</p><p>Josef Adersberger</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"57936244\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Frustration-Reduced Spark: DataFrames and the Spark Time-Series Library\" href=\"https://www.slideshare.net/ilganeli/frustrationreduced-spark-dataframes-and-the-spark-timeseries-library\">\n    \n    <div class=\"related-content\"><p>Frustration-Reduced Spark: DataFrames and the Spark Time-Series Library</p><p>Ilya Ganelin</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"50052363\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Analyzing Time Series Data with Apache Spark and Cassandra\" href=\"https://www.slideshare.net/patrickmcfadin/analyzing-time-series-data-with-apache-spark-and-cassandra\">\n    \n    <div class=\"related-content\"><p>Analyzing Time Series Data with Apache Spark and Cassandra</p><p>Patrick McFadin</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"80965555\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Apache Cassandra Community Health\" href=\"https://www.slideshare.net/Instaclustr/apache-cassandra-community-health\">\n    \n    <div class=\"related-content\"><p>Apache Cassandra Community Health</p><p>Instaclustr</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"75486973\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Instaclustr introduction to managing cassandra\" href=\"https://www.slideshare.net/Instaclustr/instaclustr-introduction-to-managing-cassandra\">\n    \n    <div class=\"related-content\"><p>Instaclustr introduction to managing cassandra</p><p>Instaclustr</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"73692909\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Instaclustr webinar 50,000 transactions per second with Apache Spark on Apache Cassandra\" href=\"https://www.slideshare.net/Instaclustr/instaclustr-webinar-50000-transactions-per-second-with-apache-spark-on-apache-cassandra-73692909\">\n    \n    <div class=\"related-content\"><p>Instaclustr webinar 50,000 transactions per second with Apache Spark on Apach...</p><p>Instaclustr</p></div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div></div><footer>\n          <div class=\"row\"><div class=\"columns\"><ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div></div>\n          \n          <div class=\"row\"><div class=\"columns\"><p class=\"copyright text-center\">LinkedIn Corporation © 2017</p></div></div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div>\n    <div class=\"row\"><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></div>\n        \n    </div>\n    \n    \n      <noscript>\n    </noscript>",
        "created_at": "2017-10-30T02:58:24+0000",
        "updated_at": "2017-12-26T23:35:04+0000",
        "published_at": null,
        "published_by": [
          "Instaclustr"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 9,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/summit2016presentation-160912235725-thumbnail-4.jpg?cb=1473726554",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/5093"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          }
        ],
        "is_public": false,
        "id": 5079,
        "uid": null,
        "title": "RussellSpitzer/spark-cassandra-csv · GitHub",
        "url": "https://github.com/RussellSpitzer/spark-cassandra-csv",
        "content": "<p>An Example Tool for Using Spark to load a CSV file into Cassandra using spark\nPull Requests and Issues Welcome!</p><pre>Spark CSV Loader 1.0\nUsage: sparkcsvexample [options] filename keyspace table mapping [master] [cassandraIp]\n  filename\n        Filename to read, csv, ex.(file:///temp/file.csv). If no locator uri it provided will look in Hadoop DefaultFS (CFS on DSE)\n  keyspace\n        Keyspace to save to\n  table\n        Table to save to\n  mapping\n        A file containing the names of the Cassandra columns that the csv columns should map to, comma-delimited\n  master\n        Spark Address of Master Node, Default runs `dsetool sparkmaster` to find master\n  cassandraIp\n        Ip Address of Cassandra Server, Default uses Spark Master IP address\n  -m &lt;value&gt; | --maxcores &lt;value&gt;\n        Number of cores to use by this application\n  -x &lt;value&gt; | --executormemory &lt;value&gt;\n        Amount of memory for each executor (JVM Style Strings)\n  -v | --verify\n        Run verification checks after inserting data\n  --help\n        CLI Help\n</pre><p>This tool is designed to work with both standalone Apache Spark and Cassandra Clusters as well as DataStax\nCassandra/Spark Clusters.</p><h2><a href=\"https://github.com/RussellSpitzer/spark-cassandra-csv#requirements\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-requirements\"></a>Requirements</h2><p>(DSE &gt; 4.5.2 or Apache C* &gt; 2.0.5 ) and Spark &gt; 0.9.1</p><h2><a href=\"https://github.com/RussellSpitzer/spark-cassandra-csv#building-the-project\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-building-the-project\"></a>Building the project</h2><p>To build go to the home directory of the project and run</p><pre>./sbt/sbt assembly\n</pre><p>This will produce a fat-jar in <code>target/scala-2.10/spark-csv-assembly-1.0.jar</code>. Which needs to be included in any running\nSpark job. It contains the references to the anonymous functions which Spark will use when running.</p><h2><a href=\"https://github.com/RussellSpitzer/spark-cassandra-csv#creating-the-example-keyspace-and-table\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-creating-the-example-keyspace-and-table\"></a>Creating the Example Keyspace and Table</h2><p>This application assumes that the keyspace and table to be inserted to already exist. To create\nthe table used in the example used below run the following commands in cqlsh.</p><pre>CREATE KEYSPACE ks WITH replication = {\n  'class': 'SimpleStrategy',\n  'replication_factor': '1'\n};\nUSE ks;\nCREATE TABLE tab (\n  key int,\n  data1 int,\n  data2 int,\n  data3 int,\n  PRIMARY KEY ((key))\n)\n</pre><h2><a href=\"https://github.com/RussellSpitzer/spark-cassandra-csv#running-with-datastax-enterprise\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-running-with-datastax-enterprise\"></a>Running with Datastax Enterprise</h2><p>When running on a Datstax Enterprise Cluster with Spark Enabled the app can be run with the included\nrun.sh script. This will include the fat-jar referenced above on the classpath for the dse spark-class call\nand run the application. Running with this method will pickup your spark-env.sh file and correctly place the logs\nin your predefined locations.</p><pre>##example\n./run.sh -m 4 file://`pwd`/exampleCsv ks tab exampleMapping\n</pre><h2><a href=\"https://github.com/RussellSpitzer/spark-cassandra-csv#running-with-apache-cassandra\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-running-with-apache-cassandra\"></a>Running with Apache Cassandra</h2><p>We can run directly from sbt using</p><pre>#Note that here we need to specify the spark master uri and cassandra ip, otherwise\n#the program will try to use DataStax Enterprise to pick up these values\n./sbt/sbt \"run -m 4 file://`pwd`/exampleCsv ks tab exampleMapping spark://127.0.0.1:7077 127.0.0.1\"    \n</pre>",
        "created_at": "2017-11-03T14:40:25+0000",
        "updated_at": "2018-01-15T19:24:05+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/413025?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/5079"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 217,
            "label": "tool",
            "slug": "tool"
          }
        ],
        "is_public": false,
        "id": 5077,
        "uid": null,
        "title": "tbarbugli/cassandra_snapshotter",
        "url": "https://github.com/tbarbugli/cassandra_snapshotter",
        "content": "<p>A tool to backup cassandra nodes using snapshots and incremental backups on S3</p><p>The scope of this project is to make it easier to backup a cluster to S3 and to combine\nsnapshots and incremental backups.</p><p><a href=\"https://travis-ci.org/tbarbugli/cassandra_snapshotter\"><img src=\"https://camo.githubusercontent.com/2371daaafb8623168cc5a53f10146329d022023c/68747470733a2f2f7472617669732d63692e6f72672f746261726275676c692f63617373616e6472615f736e617073686f747465722e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/tbarbugli/cassandra_snapshotter.svg?branch=master\" /></a></p><h2><a href=\"https://github.com/tbarbugli/cassandra_snapshotter#how-to-install\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-how-to-install\"></a>How to install</h2><p>Both the machine that runs the backup and the Cassandra nodes need to install the tool</p><div class=\"highlight highlight-source-shell\"><pre>pip install cassandra_snapshotter</pre></div><p>Nodes in the cluster also need to have lzop installed so that backups on S3 can be archived compressed</p><p>You can install it on Debian/Ubuntu via apt-get</p><div class=\"highlight highlight-source-shell\"><pre>apt-get install lzop</pre></div><p>Make sure you have JNA enabled and (if you want to use them) that incremental backups are enabled in your cassandra config file.</p><h2><a href=\"https://github.com/tbarbugli/cassandra_snapshotter#usage\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-usage\"></a>Usage</h2><p>You can see the list of parameters available via <code>cassandra-snapshotter --help</code></p><h4><a href=\"https://github.com/tbarbugli/cassandra_snapshotter#create-a-new-backup-for-mycluster\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-create-a-new-backup-for-mycluster\"></a>Create a new backup for <em>mycluster</em>:</h4><div class=\"highlight highlight-source-shell\"><pre>cassandra-snapshotter --s3-bucket-name=Z \\\n                      --s3-bucket-region=eu-west-1 \\\n                      --s3-base-path=mycluster \\\n                      --aws-access-key-id=X \\ # optional\n                      --aws-secret-access-key=Y \\ # optional\n                      --s3-ssenc \\ # optional\n                      backup \\\n                      --hosts=h1,h2,h3,h4 \\\n                      --user=cassandra # optional</pre></div><ul><li>connects via ssh to hosts h1,h2,h3,h4 using user cassandra</li>\n<li>backups up (using snapshots or incremental backups) on the S3 bucket Z</li>\n<li>backups are stored in /mycluster/</li>\n<li>if your bucket is not on us-east-1 region (the default region), you should really specify the region in the command line; otherwise weird 'connection reset by peer' errors can appear as you'll be transferring files amongst regions</li>\n<li><code>--aws-access-key-id</code> and <code>--aws-secret-access-key</code> are optional. Omitting them will use the instance IAM profile. See <a href=\"http://docs.pythonboto.org/en/latest/boto_config_tut.html\">http://docs.pythonboto.org/en/latest/boto_config_tut.html</a> for more details.</li>\n<li>if you wish to use AWS S3 server-side encryption specify <code>--s3-ssenc</code></li>\n</ul><h4><a href=\"https://github.com/tbarbugli/cassandra_snapshotter#list-existing-backups-for-mycluster\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-list-existing-backups-for-mycluster\"></a>List existing backups for <em>mycluster</em>:</h4><div class=\"highlight highlight-source-shell\"><pre>cassandra-snapshotter --s3-bucket-name=Z \\\n                      --s3-bucket-region=eu-west-1 \\\n                      --s3-base-path=mycluster \\\n                      --aws-access-key-id=X \\ # optional\n                      --aws-secret-access-key=Y \\ # optional\n                      --s3-ssenc \\ # optional\n                      list</pre></div><h3><a href=\"https://github.com/tbarbugli/cassandra_snapshotter#how-it-works\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-how-it-works\"></a>How it works</h3><p>cassandra_snapshotter connects to your cassandra nodes using ssh and uses nodetool to generate\nthe backups for keyspaces / table you want to backup.</p><p>Backups are stored on S3 using this convention:</p><h4><a href=\"https://github.com/tbarbugli/cassandra_snapshotter#snapshots\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-snapshots\"></a>Snapshots:</h4><pre>/s3_base_path/snapshot_creation_time/hostname/cassandra/data/path/keyspace/table/snapshots\n</pre><h4><a href=\"https://github.com/tbarbugli/cassandra_snapshotter#incremental-backups\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-incremental-backups\"></a>Incremental Backups:</h4><pre>/s3_base_path/snapshot_creation_time/hostname/cassandra/data/path/keyspace/table/backups\n</pre><h3><a href=\"https://github.com/tbarbugli/cassandra_snapshotter#s3_base_path\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-s3_base_path\"></a>S3_BASE_PATH</h3><p>This parameter is used to make it possible to use for a single S3 bucket to store multiple cassandra backups.</p><p>This parameter can be also seen as a backup profile identifier; the snapshotter uses the s3_base_path to search for existing snapshots on your S3 bucket.</p><h3><a href=\"https://github.com/tbarbugli/cassandra_snapshotter#incremental-backups-1\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-incremental-backups-1\"></a>INCREMENTAL BACKUPS</h3><p>Incremental backups are created only when a snapshot already exists, incremental backups are stored in their parent snapshot path.</p><p>incremental_backups are only used when all this conditions are met:</p><ul><li>there is a snapshot in the same base_path</li>\n<li>the existing snapshot was created for the same list of nodes</li>\n<li>the existing snapshot was created with the same keyspace / table parameters</li>\n</ul><p>if one of this condition is not met a new snapshot will be created.</p><p>In order to take advantage of incremental backups you need to configure your cassandra cluster for it (see cassandra.yaml config file).</p><p><strong>NOTE:</strong> Incremental backups are not enabled by default on cassandra.</p><h3><a href=\"https://github.com/tbarbugli/cassandra_snapshotter#create-new-snapshot\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-create-new-snapshot\"></a>CREATE NEW SNAPSHOT</h3><p>If you dont want to use incremental backups, or if for some reason you want to create a new snapshot for your data, run the cassandra_snapshotter with the <code>--new-snapshot</code> argument.</p><h3><a href=\"https://github.com/tbarbugli/cassandra_snapshotter#data-retention--cleanup-old-snapshots\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-data-retention--cleanup-old-snapshots\"></a>Data retention / Cleanup old snapshots</h3><p>Its not in the scope of this project to clean up your S3 buckets.\nS3 Lifecycle rules allows you do drop or archive to Glacier object stored based on their age.</p><h3><a href=\"https://github.com/tbarbugli/cassandra_snapshotter#restore-your-data\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-restore-your-data\"></a>Restore your data</h3><p>cassandra_snaphotter tries to store data and metadata in a way to make restores less painful; There is not (yet) a feature complete restore command; every patch / pull request about this is more than welcome (hint hint).</p><p>In case you need, cassandra_snapshotter stores the ring token description every time a backup is done ( you can find it the ring file in the snapshot base path )</p><p>The way data is stored on S3 should makes it really easy to use the Node Restart Method (<a href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/operations/ops_backup_snapshot_restore_t.html#task_ds_vf4_z1r_gk\">https://docs.datastax.com/en/cassandra/2.1/cassandra/operations/ops_backup_snapshot_restore_t.html#task_ds_vf4_z1r_gk</a>)</p>",
        "created_at": "2017-11-03T14:46:14+0000",
        "updated_at": "2019-01-08T23:57:22+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "github.com",
        "preview_picture": "https://camo.githubusercontent.com/2371daaafb8623168cc5a53f10146329d022023c/68747470733a2f2f7472617669732d63692e6f72672f746261726275676c692f63617373616e6472615f736e617073686f747465722e7376673f6272616e63683d6d6173746572",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/5077"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          }
        ],
        "is_public": false,
        "id": 5076,
        "uid": null,
        "title": "waldmark/spark-cassandra-batch-s3-examples",
        "url": "https://github.com/waldmark/spark-cassandra-batch-s3-examples",
        "content": "<h3>\n      <svg aria-hidden=\"true\" class=\"octicon octicon-book\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"/>\n      README.md\n    </h3>\n      <article class=\"markdown-body entry-content\" itemprop=\"text\"><h2><a href=\"https://github.com/waldmark/spark-cassandra-batch-s3-examples#spark-casaandra-example\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-spark-casaandra-example\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"/></a>Spark Casaandra Example</h2>\n<p><a href=\"https://github.com/waldmark/spark-cassandra-batch-s3-examples/blob/master/images/spark-logo-trademark.png\" target=\"_blank\"><img src=\"https://github.com/waldmark/spark-cassandra-batch-s3-examples/raw/master/images/spark-logo-trademark.png\" alt=\"Alt text\"/></a></p>\n<p>\nJava example of Apache Spark consuming and processing 911 calls stored in Cassandra. \n</p>\n<p><i>Requirements:</i></p>\n<ul><li>Java 8 installed</li>\n<li>Cassandra</li>\n<li>Scality S3 server</li>\n</ul><p>This demo was developed using docker images running locally for Cassandra and Scality S3. Other instances of Cassandra and S3 should work as well.</p>\n<p>The example can be run from an IDE (like IntelliJ), or from a runnable jar. See instructions below on building the runnable <i>uber-jar</i>.</p>\n<h3><a href=\"https://github.com/waldmark/spark-cassandra-batch-s3-examples#stand-alone-processing-from-a-file\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-stand-alone-processing-from-a-file\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"/></a>Stand alone processing from a file</h3>\n<p>The class com.objectpartners.spark.rt911.standalone.MainApplication has a runnable main. It loads data into Cassandra;\nonce loaded, it uses the Spark Cassandra Connector to read and then analyze data from Cassandra, and then store the results into S3.</p>\n<h2><a href=\"https://github.com/waldmark/spark-cassandra-batch-s3-examples#building-a-runnable-jar\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-building-a-runnable-jar\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"/></a>Building a runnable jar</h2>\n<p>A standalone jar can be created using Gradle. In the project root directory, in a terminal run gradle:</p>\n<ol><li>gradle clean build</li>\n<li>gradle shadowjar</li>\n</ol><p>The uber-jar will be built and placed in the {$project.dir}/build/libs directory.</p>\n<h2><a href=\"https://github.com/waldmark/spark-cassandra-batch-s3-examples#resources\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-resources\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"/></a>Resources</h2>\n<p>In src/main/resources are two gzips containing 911 call data in csv format:</p>\n<ol><li>Seattle_Real_Time_Fire_911_Calls_10_Test.csv.gz contains 10 911 calls (10 lines) and can be used for simple testing.\nNote that the application assumes the first line contains header data, so only 9 calls are actually processed.</li>\n<li>Seattle_Real_Time_Fire_911_Calls_Chrono.csv.gz\nA chronologically ordered set of (lots of) calls.</li>\n</ol></article>",
        "created_at": "2017-11-03T14:48:27+0000",
        "updated_at": "2017-12-15T08:44:23+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://github.com/waldmark/spark-cassandra-batch-s3-examples/blob/master/images/spark-logo-trademark.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/5076"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 5038,
        "uid": null,
        "title": "lbruand/cql2plantuml",
        "url": "https://github.com/lbruand/cql2plantuml",
        "content": "<h3>\n      <svg aria-hidden=\"true\" class=\"octicon octicon-book\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"/>\n      README.md\n    </h3>\n      <article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>Turn any CQL ( <a href=\"http://cassandra.apache.org/\" rel=\"nofollow\">Cassandra</a>) schema into a graphical representation.</p>\n<p>A small scala tool to transform a schema to a <a href=\"http://plantuml.com/\" rel=\"nofollow\">plantuml</a> puml file that can be in turn transformed into a graphical representation of your CQL schema.</p>\n<p><a href=\"https://github.com/lbruand/cql2plantuml/blob/master/examples/system_auth/system_auth.png\" target=\"_blank\"><img src=\"https://github.com/lbruand/cql2plantuml/raw/master/examples/system_auth/system_auth.png\" alt=\"system auth\"/></a></p>\n<p>NB : Because the CQL Schema does not contain any relation between tables, it is up to you to define the relations in the plantuml file.</p>\n<p>Download a jar file from <a href=\"https://github.com/lbruand/cql2plantuml/releases\">release</a>.</p>\n<p>get a CQL schema from C* keyspace <em>test</em> :</p>\n<div class=\"highlight highlight-source-shell\"><pre>cqlsh -e 'describe keyspace test;' &gt; test.cql</pre></div>\n<p>run the jar file using :</p>\n<div class=\"highlight highlight-source-shell\"><pre>java -jar cql2plantuml-1.1-jar-with-dependencies.jar test.cql</pre></div>\n<p>This creates a <em>test.puml</em> file from your input <em>test.cql</em> file.\nThis .puml file does not contain any link.\nYou can edit it to add links.\nYou can then run <a href=\"http://plantuml.com/\" rel=\"nofollow\">plantuml</a> to obtain a .png or .svg file:</p>\n<div class=\"highlight highlight-source-shell\"><pre>plantuml -Tpng test.puml\nplantuml -Tsvg test.puml</pre></div>\n<p>You can use cql2plantuml as a maven plugin :</p>\n<div class=\"highlight highlight-text-xml\"><pre>    &lt;build&gt;\n        &lt;plugins&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;cql2plantuml&lt;/groupId&gt;\n                &lt;artifactId&gt;cql2plantuml&lt;/artifactId&gt;\n                &lt;version&gt;1.1&lt;/version&gt;\n                &lt;executions&gt;\n                    &lt;execution&gt;\n                        &lt;phase&gt;compile&lt;/phase&gt;\n                        &lt;goals&gt;\n                            &lt;goal&gt;touch&lt;/goal&gt;\n                        &lt;/goals&gt;\n                        &lt;configuration&gt;\n                            &lt;format&gt;svg&lt;/format&gt;\n                            &lt;outputPuml&gt;true&lt;/outputPuml&gt;\n                            &lt;sourceFiles&gt;\n                                &lt;directory&gt;src/main/resources&lt;/directory&gt;\n                                &lt;includes&gt;&lt;include&gt;**/*.cql&lt;/include&gt;\n                                &lt;/includes&gt;\n                            &lt;/sourceFiles&gt;\n                        &lt;/configuration&gt;\n                    &lt;/execution&gt;\n                &lt;/executions&gt;\n            &lt;/plugin&gt;\n        &lt;/plugins&gt;\n    &lt;/build&gt;</pre></div>\n<p>NB : Because cql2plantuml uses plantuml, you need to install graphviz on the machine.</p>\n<p>This work includes classes (CQL AST Parser) originally written by Tamer AbdulRadi distributed under the Apache Licence 2.0 at (<a href=\"https://github.com/schemasafe/troy\">https://github.com/schemasafe/troy</a>). These classes were then modified.</p>\n<p>This work includes classes originally written by Julien Eluard distributed under the Apache Licence 2.0 at (<a href=\"https://github.com/jeluard/maven-plantuml-plugin\">https://github.com/jeluard/maven-plantuml-plugin</a>). These classes were then modified.</p>\n</article>",
        "created_at": "2017-11-11T16:02:05+0000",
        "updated_at": "2017-12-15T06:17:15+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/680234?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/5038"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 15,
            "label": "tutorial",
            "slug": "tutorial"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 5037,
        "uid": null,
        "title": "Consulting Cassandra: Second Contact with the Monolith",
        "url": "https://www.instaclustr.com/consulting-cassandra-second-contact-monolith/",
        "content": "<p><img class=\"aligncenter wp-image-6309 size-full\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/09/Consulting-Cassandra-Second-Contact-with-the-monolith-blog.jpg\" alt=\"Consulting Cassandra - Second contact with the monolith Instaclustr blog\" width=\"640\" height=\"356\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Consulting-Cassandra-Second-Contact-with-the-monolith-blog.jpg 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Consulting-Cassandra-Second-Contact-with-the-monolith-blog-300x167.jpg 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Consulting-Cassandra-Second-Contact-with-the-monolith-blog-86x48.jpg 86w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Consulting-Cassandra-Second-Contact-with-the-monolith-blog-194x108.jpg 194w\" />In the first part of this blog (<a href=\"https://www.instaclustr.com/cassandra-cluster-creation-10-minutes/\">Cluster Creation in Under Ten Minutes</a>), I created a Cassandra cluster. In this part, we blast off to the Moon for 2nd contact.</p> <h2>Consulting the Oracles</h2> <h6><b>Croesus</b>: Hi Oracle.  How will my war with Cyrus the Persian go?<br /><b>Oracle</b>: If you proceed, a great empire will be destroyed.<br /><b>Croesus</b>: Great – Attack!</h6> <p>What happened? Croesus was defeated as he had incorrectly assumed it would be the Persian empire that would be destroyed.</p> <h6><b>King Priam of Troy:</b> Hi Cassandra. The Trojans have left an offering of a wooden horse at the gate. Is it safe to bring it inside?<br /><b>Cassandra:</b>  No! Leave it alone, it’s a trick! The Trojan soldiers are inside and Troy will be destroyed!<br /><b>King Priam</b>: That’s a Yes then. Bring the horse inside the gates!</h6> <p>What happened? Troy was sacked. Cassandra was a prophetess but had been cursed by Apollo so that no-one believed her.</p> <p><img class=\"aligncenter wp-image-6316 size-full\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/09/Consulting-Cassandra-Oracle-Instaclustr-blog.jpg\" alt=\"Consulting Cassandra oracle Instaclustr Blog\" width=\"904\" height=\"732\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Consulting-Cassandra-Oracle-Instaclustr-blog.jpg 904w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Consulting-Cassandra-Oracle-Instaclustr-blog-300x243.jpg 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Consulting-Cassandra-Oracle-Instaclustr-blog-768x622.jpg 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Consulting-Cassandra-Oracle-Instaclustr-blog-761x616.jpg 761w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Consulting-Cassandra-Oracle-Instaclustr-blog-640x518.jpg 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Consulting-Cassandra-Oracle-Instaclustr-blog-59x48.jpg 59w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Consulting-Cassandra-Oracle-Instaclustr-blog-133x108.jpg 133w\" /><em>A wall painting from Pompeii, showing the Trojan princess Cassandra, fainting, as she possesses the power of prophecy and knows that the horse means the doom of Troy (How did soldiers fit in a horse that small? It’s no surprise that Cassandra’s warning wasn’t taken seriously ????</em></p> <p>Let’s explore one way to connect to and consult Cassandra…<br /></p> <h2>Consulting Cassandra (CQLSH)</h2> <p>Now that my Cassandra cluster is up and running, what can I do with it? How do I talk to it? What sort of clients are supported? What’s the protocol? How does the client find the cluster? Is the client just another node in the gossip protocol? (No). Is there a default REST API? (No).</p> <p><a href=\"http://cassandra.apache.org/doc/latest/getting_started/querying.html#client-drivers\">The Apache docs say</a> that the API to Cassandra is <a href=\"http://cassandra.apache.org/doc/latest/cql/index.html#cql\">CQL</a>, the Cassandra Query Language. To use CQL, you will need to connect to the cluster, which can be done:</p> <ul><li>either using cqlsh,</li> <li>or through a client driver for Cassandra.</li> </ul><p>Here’s the general Instaclustr documentation on <a href=\"https://support.instaclustr.com/hc/en-us/articles/203759250-Connecting-to-a-Cluster\">Connecting to a Cluster</a>, and the specific documentation for <a href=\"https://support.instaclustr.com/hc/en-us/articles/216238308-Connecting-to-Instaclustr-Using-Cqlsh\">Connecting to Instaclustr Using Cqlsh</a>.</p> <p>The 1st thing you need to do is to obtain the Apache Cassandra installation including the cqlsh shell. You don’t need to install or run Cassandra on your machine to run cqlsh. Using cqlsh allows you to check that you can connect to your newly created Instaclustr Cassandra Cluster and is also useful for debugging even after you have a Java etc client working.</p> <p>Downloading Cassandra – using a browser click on the link to the Latest Version <a href=\"http://cassandra.apache.org/download/\">on this page</a> and it will take you to mirror options for you to download from. Once downloaded and you’ve extracted/unzipped it (reminder, no need to start it), start a command line on your preferred OS, go to the Cassandra bin directory and type the magic incantation to connect:</p> <p>Cassandra drivers must be provided with the address of at least one node in the cluster and will auto-discover the remaining topology (all the IP addresses) of the cluster via the CQL binary protocol. The driver maintains a connection to the cluster and will keep this internal topology info up-to-date (Cassandra uses the gossip protocol internally to build this topology information). Subsequently the client can connect to any node (depending on the load balancing policy).</p>\n<p>To connect to the cluster we need a public ip address of at least one node, and username and password. Log into <a href=\"https://console.instaclustr.com\">https://console.instaclustr.com</a> and click on the name of the trial cluster created previously.  Under details, you will see the 3 nodes and their Public IP addresses. Under Settings (Firewall rules) you can check (or add) client IP addresses that are allowed to connect to the Cassandra cluster. Click on the Connection Information Tab for detailed connection information. This page shows all the public IP addresses of the nodes, and the username/password.  For cqlsh connection just pick any one public IP address. </p>\n<p>I have a confession. I’ve heard that CQL is “like” SQL which makes it confusing for people with a SQL background. I may be lucky! The only experience I have with SQL is a group project at university to design and implement a SQL database. Once we built it we didn’t actually use it for anything useful. Oh, and a I built a system for distributed Oracle instances (with an eventual time to consistency of 30 minutes). So I’m not (yet) confused by CQL, I just have no preconceptions at all.</p>\n<p>Once the cqlsh is up and running you can consult Cassandra. I tried typing “Hi Cassandra;” but only got “SyntaxException: line 1:0 no viable alternative at input ‘hi’ ([hi]…)”. Time to read the <a href=\"http://cassandra.apache.org/doc/latest/cql/index.html\">CQL commands docs</a> or type “help;”.</p>\n<p>DESCRIBE is useful. E.g. “DESCRIBE keyspaces;”, “DESCRIBE tables;”</p>\n<p>DESCRIBE keyspace instaclustr;</p>\n<p>will show information about all the tables in the Instaclustr keyspace (one that our management system creates in every cluster we manage for synthetic transaction monitoring and other uses). <br /></p>\n<p>SELECT * from keyspace.table;</p>\n<p>is useful for looking at things (assuming there are not too many!) Once you’ve found a keyspace and table then try it out.</p>\n<p>SELECT peer from system.peers;</p>\n<p>gives you the IP addresses of the other 2 nodes (that you didn’t connect to via the client) in the cluster.</p>\n<p>Note that we recommend that you immediately change the superuser password, and create a non-superuser account.  Here’s how to change the password.</p>\n<p>LIST users;</p>\n<p>And then:</p>\n<p>ALTER USER iccassandra WITH PASSWORD ‘newpassword’;</p>\n<p>And while we’re at it let’s make sure HAL can’t take over by creating a non super user:</p>\n<p>CREATE user hal9000 with password ‘ImsorryDave’ NOSUPERUSER;</p>\n<p>And allow hal900 to do things:</p>\n<p>GRANT ALL ON ALL KEYSPACES TO hal9000;</p>\n<p>EXIT;</p>\n<p>And reconnect again as hal9000.</p>\n<p>Note that roles (allowing more sophisticated security) are supported in the current versions of Cassandra:</p>\n<p>LIST roles;</p>\n<p>What else can you do? Well, you can try creating a keyspace, a table, inserting some rows, selecting rows, etc.  However, this requires some understanding of Cassandra concepts including keyspaces, tables, primary keys, partition keys, clustering columns, etc. </p>\n<p>Here is the relevant documentation and some definitions:</p>\n<p><a href=\"http://cassandra.apache.org/doc/latest/cql/ddl.html#create-keyspace\">http://cassandra.apache.org/doc/latest/cql/ddl.html#create-keyspace</a></p>\n<p><a href=\"http://cassandra.apache.org/doc/latest/cql/dml.html\">http://cassandra.apache.org/doc/latest/cql/dml.html</a></p>\n<p><b>Keyspace</b></p>\n<p>A namespace container that defines how data is replicated on nodes. Similar to a “database” in a RDBMS.</p>\n<p><b>Table (Column Family)</b></p>\n<p>A container for rows, similar to the table in a relational system. Called table in CQL 3.</p>\n<p><b>Primary Key</b></p>\n<p>The partition key. One or more columns that uniquely identify a row in a table.   </p>\n<p><b>Clustering Column</b></p>\n<p>In the table definition, a clustering column is a column that is part of the compound primary key definition, but not part of the  partition key. Columns are clustered in multiple rows within a single partition. The clustering order is determined by the position of columns in the compound primary key definition.</p>\n<p><b>Compound Partition Key</b></p>\n<p>A partition key consisting of multiple columns.</p>\n<p><b>Compound Primary Key</b></p>\n<p>A primary key consisting of the partition key, which determines on which node data is stored, and one or more additional columns that determine clustering.</p>\n<p>Here are some examples of possible primary keys:</p>\n<p><b><i>Simple Primary Key, Simple Partition Key</i></b></p>\n<p>PRIMARY KEY(partition1)</p>\n<p><b><i>Simple Primary Key, Compound Partition Key</i></b></p>\n<p>PRIMARY KEY((partition1, partition2))<br /><b><i></i></b></p>\n<p><b><i>Compound Primary Key,  Simple Partition Key</i></b></p>\n<p>PRIMARY KEY(partition1, clustering1)</p>\n<p>PRIMARY KEY(partition1, clustering1, clustering2)</p>\n<p><b><i>Compound Primary Key, Compound Partition Key</i></b></p>\n<p>PRIMARY KEY((partition1, partition2), clustering1)</p>\n<p>PRIMARY KEY((partition1, partition2), clustering1, clustering2)</p>\n<p>Let’s create a table and some data. First, create a keyspace with some probably naive defaults. I called it dev.</p>\n<p>CREATE KEYSPACE dev WITH replication = {‘class’: ‘SimpleStrategy’, ‘replication_factor’: ‘3’}  AND durable_writes = true;</p>\n<p>Have a look at it with:</p>\n<p>DESCRIBE KEYSPACE dev;</p>\n<p>Now create a table for fake sensor data to store data of the form:</p>\n<p>&lt;’host1’, ‘temp’, ‘sometime’, 22.2&gt;</p>\n<p>CREATE TABLE dev.sensordata(     host text,     metric text,     time timestamp,     value double,   PRIMARY KEY ((host, metric), time) ) WITH CLUSTERING ORDER BY (time ASC);</p>\n<p>What does this do? Create a new table in the dev keyspace with the name sensordata with the columns: host, metric, time, value. The partition key is compound and is made up of host AND metric.  All queries will need to specify values for both of these columns. The time column is a clustering column (order) with ascending order (past time to now).</p>\n<p>Now type:</p>\n<p>DESCRIBE TABLE dev.sensordata;</p>\n<p>And you get all this back:</p>\n<p>To insert some data type:</p>\n<p>And then:</p>\n<p>SELECT * from dev.sensordata;</p>\n<p><b>host</b>  | <b>metric</b> | <b>time</b>                                                                   | <b>value<br /></b>——-+———-+————————————————-+——–<br /><b>host1</b> |   <b>temp</b> | <b>2017-08-24 06:01:14.504000+0000</b> |  <b>22.2</b></p>\n<p>If you repeat the identical insert and then select again you will get 2 rows because the time is different:</p>\n<p><b>host</b>  | <b>metric</b> | <b>time</b>                                                                   | <b>value<br /></b>——–+——–+————————————————–+——–<br /><b>host1</b> |   <b>temp</b> | <b>2017-08-24 06:01:14.504000+0000</b> |  <b>22.2<br /></b><b>host1</b> |   <b>temp</b> | <b>2017-08-24 06:02:35.412000+0000</b> |  <b>22.2</b></p>\n<p>Notice that the results are returned in increasing time order.</p>\n<p>Now let’s try out the WHERE clause. I’ve added some more rows and now have 2 hosts and 2 metrics:</p>\n<p><b> host</b>    | <b>metric</b>        | <b>time</b>                                                                  | <b>value<br /></b>——–+————-+————————————————–+——-<br /><b>host2</b> |     <b>temp</b>     | <b>2017-08-24 06:05:34.893000+0000</b> |  <b>20.4<br /></b><b>host2</b> |     <b>temp </b>    | <b>2017-08-24 06:05:43.805000+0000</b> |  <b>22.1<br /></b><b>host2</b> |     <b>temp</b>     | <b>2017-08-24 06:05:49.333000+0000</b> |  <b>20.1<br /></b><b>host1</b> | <b>pressure</b> | <b>2017-08-24 06:06:21.136000+0000</b> |    <b>44<br /></b><b>host1</b> | <b>pressure</b> | <b>2017-08-24 06:06:29.276000+0000</b> |    <b>42<br /></b><b>host2</b> | <b>pressure</b> | <b>2017-08-24 06:06:11.406000+0000</b> |    <b>33<br /></b><b>host2</b> | <b>pressure</b> | <b>2017-08-24 06:06:14.758000+0000</b> |    <b>44<br /></b><b>host1</b> |     <b>temp    </b> | <b>2017-08-24 06:01:14.504000+0000</b> |  <b>22.2<br /></b><b>host1</b> |     <b>temp</b>     | <b>2017-08-24 06:02:35.412000+0000</b> |  <b>22.2<br /></b><b>host1</b> |     <b>temp     </b>| <b>2017-08-24 06:03:44.405000+0000</b> |  <b>10.9<br /></b><b>host1</b> |     <b>temp</b>     | <b>2017-08-24 06:03:51.538000+0000</b> |  <b>11.9</b></p>\n<p>To get rows for host1 AND temp only type:</p>\n<p>SELECT * from dev.sensordata where host=’host1′ and metric=’temp’;</p>\n<p><b>host</b>   | <b>metric</b> | <b>time</b>                                                                   | <b>value<br /></b>——-+———-+—————————————————+——-<br /><b>host1</b> |   <b>temp  </b>| <b>2017-08-24 06:01:14.504000+0000</b> |  <b>22.2<br /></b><b>host1</b> |   <b>temp </b> | <b>2017-08-24 06:02:35.412000+0000</b> |  <b>22.2<br /></b><b>host1</b> |   <b>temp</b>  | <b>2017-08-24 06:03:44.405000+0000</b> |  <b>10.9<br /></b><b>host1</b> |   <b>temp</b>  | <b>2017-08-24 06:03:51.538000+0000</b> |  <b>11.9</b></p>\n<p>Notice that we had to specify <b>both</b> of the compound partition key columns in the where clauses.  You can also add a where clause over the clustering key (with inequalities to). </p>\n<p>SELECT * from dev.sensordata where host=’host1′ and metric=’temp’ and time&gt;toTimestamp(now());</p>\n<p>Returns nothing (predictably, as we are asking for future events). Now try some other select queries with missing where clauses. For example the following won’t work as the partition keys are missing (only the clustering key is provided):</p>\n<p>SELECT * from dev.sensordata where time&gt;toTimestamp(now());</p>\n<p>InvalidRequest: Error from server: code=2200 [Invalid query] message=”Cannot execute this query as it might involve data filtering and thus may have unpredictable performance. If you want to execute this query despite the performance unpredictability, use ALLOW FILTERING”</p>\n<p>What does ALLOW FILTERING do?</p>\n<p>By default, CQL only allows select queries that don’t involve “filtering” server side, i.e. queries where we know that all (live) record read will be returned (maybe partly) in the result set. The reasoning is that those “non filtering” queries have predictable performance in the sense that they will execute in a time that is proportional to the amount of data <b>returned</b> by the query (which can be controlled through LIMIT).</p>\n<p>The ALLOW FILTERING option allows to explicitly allow (some) queries that require filtering. Please note that a query using ALLOW FILTERING may thus have unpredictable performance (for the definition above), i.e. even a query that selects a handful of records <b>may</b> exhibit performance that depends on the total amount of data stored in the cluster.  … However, if you “know what you are doing”, you can force the execution of this query by using ALLOW FILTERING (I don’t so I won’t).</p>\n<p>Aggregate functions (min, max, avg, count, sum) will also work on this table (it would be more sensible to have a time range where clause):</p>\n<p>SELECT max(value) from dev.sensordata where host=’host1′ and metric=’temp’  and time&lt;toTimestamp(now());</p>\n<p><b>system.max(value)<br /></b>————————–<br /><b>                            22.2</b></p>\n<h2>NOTES</h2>\n<p>1 In this simple example there is no bucket (time) column which is common in Cassandra time series data tables (but is it really needed?)</p>\n<p>2 I made time order ascending. Descending is more common for time series data when you are interested in the most recent data first.</p>\n<p>3 How do you know what the possible host and metric names are? Try this:</p>\n<p>SELECT distinct host,metric from dev.sensordata;</p>\n<p><b>host</b>   | <b>metric<br /></b>——-+———-<br /><b>host2</b> |     <b>temp<br /></b><b>host1</b> | <b>pressure<br /></b><b>host2</b> | <b>pressure<br /></b><b>host1</b> |     <b>temp</b></p>",
        "created_at": "2017-11-13T16:20:39+0000",
        "updated_at": "2018-09-13T14:49:03+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 10,
        "domain_name": "www.instaclustr.com",
        "preview_picture": "https://www.instaclustr.com/wp-content/uploads/2017/09/Consulting-Cassandra-Second-Contact-with-the-monolith-blog.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/5037"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 15,
            "label": "tutorial",
            "slug": "tutorial"
          },
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 883,
            "label": "java",
            "slug": "java"
          }
        ],
        "is_public": false,
        "id": 5036,
        "uid": null,
        "title": "Hello Cassandra! A Java Client Example",
        "url": "https://www.instaclustr.com/hello-cassandra-java-client-example/",
        "content": "<p>package test1;</p><p>import java.util.Date;</p><p>import com.datastax.driver.core.*;</p><p>import com.datastax.driver.core.policies.DCAwareRoundRobinPolicy;</p><p>/*</p><p> * Simple Java client test to connect to trial cluster, create a time series data table, fill it, query it, and save it as csv for graphing.</p><p> */</p><p>public class CassTest1 {</p><p> // the 3 node trial Cassandra test cluster Public IPs. These are dummy values.</p><p> static String n1PubIP = \"01.23.45.678\";</p><p> static String n2PubIP = \"01.234.56.78\";</p><p> static String n3PubIP = \"01.23.456.78\";</p><p> static String dcName = \"hal_sydney\"; // this is the DC name you used when created</p><p> static String user = \"user\";</p><p> static String password = \"password\";</p><p> public static void main(String[] args) {</p><p> long t1 = 0; // time each CQL operation, t1 is start time t2 is end time, time is t2-t1</p><p> long t2 = 0;</p><p> long time = 0; </p><p> Cluster.Builder clusterBuilder = Cluster.builder()</p><p>     .addContactPoints(</p><p>      n1PubIP, n2PubIP, n3PubIP // provide all 3 public IPs</p><p>     )</p><p>     .withLoadBalancingPolicy(DCAwareRoundRobinPolicy.builder().withLocalDc(dcName).build()) // your local data centre</p><p>     .withPort(9042)</p><p>     .withAuthProvider(new PlainTextAuthProvider(user, password));</p><p> Cluster cluster = null;</p><p> try {</p><p> cluster = clusterBuilder.build();</p><p>     Metadata metadata = cluster.getMetadata();</p><p>     System.out.printf(\"Connected to cluster: %s\\n\", metadata.getClusterName());</p><p>     for (Host host: metadata.getAllHosts()) {</p><p>         System.out.printf(\"Datacenter: %s; Host: %s; Rack: %s\\n\", host.getDatacenter(), host.getAddress(), host.getRack());</p><p>     }</p><p>     Session session = cluster.connect();                                           </p><p>     ResultSet rs;</p><p>     boolean createTable = true;</p><p>     if (createTable) {</p><p>      rs = session.execute(\"CREATE KEYSPACE IF NOT EXISTS hals WITH replication = {'class': 'SimpleStrategy', 'replication_factor' : 3}\");</p><p>      rs = session.execute(\"DROP TABLE IF EXISTS hals.sensordata\");</p><p>      rs = session.execute(\"CREATE TABLE hals.sensordata(host text, metric text, time timestamp, value double, PRIMARY KEY ((host, metric), time) ) WITH CLUSTERING ORDER BY (time ASC)\");</p><p>      System.out.println(\"Table hals.sensordata created!\");</p><p>     }</p><p>     // Fill the table with some realistic sensor data. if createTable=false we just ADD data to the table</p><p>     double startValue = 100; // start value for random walk</p><p>     double nextValue = startValue; // next value in random walk, initially startValue</p><p>     int numHosts = 100; // how many host names to generate</p><p>     int toCreate = 1000; // how many times to pick a host name and create all metrics for it</p><p>     boolean usePrepared = false;</p><p>     PreparedStatement prepared = null;</p><p>     // prepare a prepared statement</p><p>     if (usePrepared) </p><p>      {</p><p>      System.out.println(\"Using PREPARED statements for INSERT\");</p><p>      prepared = session.prepare(\"insert into hals.sensordata (host, metric, time, value) values (?, ?, ?, ?)\");</p><p>      }</p><p>     t1 = System.currentTimeMillis();</p><p>     System.out.println(\"Creating data... iterations = \" + toCreate);</p><p>     for (int r=1; r &amp;lt;= toCreate; r++) {</p><p>      long now = System.currentTimeMillis();</p><p>      Date date = new Date(now);</p><p>      // generate a random host name</p><p>      String hostname = \"host\" + (long)Math.round((Math.random() * numHosts));</p><p>      // do a random walk to produce realistic data</p><p>      double rand = Math.random();</p><p>      if (rand &amp;lt; 0.5)</p><p>      // 50% chance that value doesn't change</p><p>      ;</p><p>      else if (rand &amp;lt; 0.75)</p><p>      // 25% chance that value increases by 1</p><p>      nextValue++;</p><p>      else</p><p>      // 25% chance that value decreases by 1</p><p>      nextValue--;</p><p>      // never go negative</p><p>      if (nextValue &amp;lt; 0)</p><p>      nextValue = 0;</p><p>      // comparison of prepared vs. non-prepared statements</p><p>      if (usePrepared)  {</p><p>      session.execute(prepared.bind(\"'\" + hostname + \"'\", \"'m1'\", date, nextValue));</p><p>      session.execute(prepared.bind(\"'\" + hostname + \"'\", \"'m2'\", date, nextValue * 10));</p><p>      session.execute(prepared.bind(\"'\" + hostname + \"'\", \"'m3'\", date, nextValue * 100));</p><p>      }</p><p>      else {</p><p>      // fake three metrics (m1, m2, m3) which are somehow related.</p><p>      rs = session.execute(\"insert into hals.sensordata (host, metric, time, value) values (\" + \"'\" + hostname + \"'\" + \", \" + \"'m1'\" + \", \" + now + \",\" + (nextValue) + \");\" );</p><p>      rs = session.execute(\"insert into hals.sensordata (host, metric, time, value) values (\" + \"'\" + hostname + \"'\" + \", \" + \"'m2'\" + \", \" + now + \",\" + (nextValue * 10) + \");\" );</p><p>      rs = session.execute(\"insert into hals.sensordata (host, metric, time, value) values (\" + \"'\" + hostname + \"'\" + \", \" + \"'m3'\" + \", \" + now + \",\" + (nextValue * 100) + \");\" );</p><p>      }</p><p>      }</p><p>     t2 = System.currentTimeMillis();</p><p>     System.out.println(\"Created rows = \" + toCreate*3 + \" in time = \" + (t2-t1)); </p><p>     // find the max value for a sample</p><p>     System.out.println(\"Getting max value for sample...\");</p><p>     t1 = System.currentTimeMillis();</p><p>     rs = session.execute(\"select max(value) from hals.sensordata where host='host1' and metric='m1'\");   </p><p>     t2 = System.currentTimeMillis();</p><p>     time = t2-t1;</p><p>     Row row = rs.one();</p><p>     System.out.println(\"Max value = \" + row.toString() + \" in time = \" + time);   </p><p>     // get all the values for a sample</p><p>     System.out.println(\"Getting all rows for sample...\");</p><p>     t1 = System.currentTimeMillis();</p><p>     rs = session.execute(\"select * from hals.sensordata where host='host1' and metric='m1'\"); </p><p>     for (Row rowN : rs) {</p><p>      System.out.println(rowN.toString());</p><p>     }</p><p>     t2 = System.currentTimeMillis();</p><p>     time = t2-t1;</p><p>     System.out.println(\"time = \" + time);   </p><p>     // get all host/metric permutations</p><p>     System.out.println(\"Getting all host/metric permutations\");</p><p>     t1 = System.currentTimeMillis();</p><p>     rs = session.execute(\"select distinct host, metric from hals.sensordata\"); </p><p>     for (Row rowN : rs) {</p><p>      System.out.println(rowN.toString());</p><p>     }</p><p>     t2 = System.currentTimeMillis();</p><p>     time = t2-t1;</p><p>     System.out.println(\"time = \" + time);   </p><p>     // Note that SELECT * will return all results without limit (even though the driver might use multiple queries in the background).</p><p>     // To handle large result sets, you use a LIMIT clause in your CQL query, or use one of the techniques described in the paging documentation.</p><p>     System.out.println(\"Select ALL...\");</p><p>     t1 = System.currentTimeMillis();</p><p>     rs = session.execute(\"select * from hals.sensordata\");   </p><p>     System.out.println(\"Got rows (without fetching) = \" + rs.getAvailableWithoutFetching());</p><p>     int i = 0;</p><p>     long numBytes = 0;</p><p>     // example use of the data: count rows and total bytes returned.</p><p>     for (Row rowN : rs)</p><p>     {</p><p>      i++;</p><p>      numBytes += rowN.toString().length();</p><p>     }</p><p>     t2 = System.currentTimeMillis();</p><p>     time = t2-t1;</p><p>     System.out.println(\"Returned rows = \" + i + \", total bytes = \" + numBytes + \", in time = \" + time);</p><p> } finally {</p><p> if (cluster != null) cluster.close();     </p><p> }</p><p> }</p><p>}</p>",
        "created_at": "2017-11-13T16:20:47+0000",
        "updated_at": "2018-09-13T14:49:10+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 4,
        "domain_name": "www.instaclustr.com",
        "preview_picture": "https://www.instaclustr.com/wp-content/uploads/2017/09/sample-sensor-data-values-over-time-for-1-host-and-3-metrics-Instaclustr.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/5036"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 24,
            "label": "node",
            "slug": "node-js"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 568,
            "label": "azure",
            "slug": "azure"
          }
        ],
        "is_public": false,
        "id": 4950,
        "uid": null,
        "title": "Running Cassandra with Linux on Azure and Accessing it from Node.js",
        "url": "https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/cassandra-nodejs",
        "content": "<div class=\"IMPORTANT\" readability=\"8\"><p>Important</p><p>Azure has two different deployment models for creating and working with resources: <a href=\"https://docs.microsoft.com/en-us/azure/resource-manager-deployment-model\" data-linktype=\"relative-path\">Resource Manager and Classic</a>. This article covers using the Classic deployment model. Microsoft recommends that most new deployments use the Resource Manager model. See Resource Manager templates for <a href=\"https://azure.microsoft.com/documentation/templates/datastax\" data-linktype=\"external\">Datastax Enterprise</a> and <a href=\"https://azure.microsoft.com/documentation/templates/spark-and-cassandra-on-centos/\" data-linktype=\"external\">Spark cluster and Cassandra on CentOS</a>.</p>\n</div>\n<h2 id=\"overview\">Overview</h2>\n<p>Microsoft Azure is an open cloud platform that runs both Microsoft as well as non-Microsoft software which  includes operating systems, application servers, messaging middleware as well as SQL and NoSQL databases from both commercial and open source models. Building resilient services on public clouds including Azure requires careful planning and deliberate architecture for both applications servers as well storage layers. Cassandra’s distributed storage architecture naturally helps in building highly available systems that are fault tolerant for cluster failures. Cassandra is a cloud scale NoSQL database maintained by Apache Software Foundation at cassandra.apache.org; Cassandra is written in Java and hence runs on both on Windows as well as Linux platforms.</p>\n<p>The focus of this article is to show Cassandra deployment on Ubuntu as a single and multi-data center cluster leveraging Microsoft Azure Virtual Machines and Virtual Networks. The cluster deployment for production optimized workloads is out of scope of this article as it requires multi-disk node configuration, appropriate ring topology design and data modeling to support the needed replication, data consistency, throughput and high availability requirements.</p>\n<p>This article takes a fundamental approach to show what is involved in building the Cassandra cluster compared Docker, Chef or Puppet which can make the infrastructure deployment a lot easier.  </p>\n<h2 id=\"the-deployment-models\">The Deployment Models</h2>\n<p>Microsoft Azure networking allows the deployment of isolated private clusters, the access of which can be restricted to attain fine grained network security.  Since this article is about showing the Cassandra deployment at a fundamental level, we will not focus on the consistency level and the optimal storage design for throughput. Here is the list of networking requirements for our hypothetical cluster:</p>\n<ul><li>External systems can’t access Cassandra database from within or outside Azure</li>\n<li>Cassandra cluster has to be behind a load balancer for thrift traffic</li>\n<li>Deploy Cassandra nodes in two groups in each data center for an enhanced cluster availability</li>\n<li>Lock down the cluster so that only application server farm has access to the database directly</li>\n<li>No public networking endpoints other than SSH</li>\n<li>Each Cassandra node needs a fixed internal IP address</li>\n</ul><p>Cassandra can be deployed to a single Azure region or to multiple regions based on the distributed nature of the workload. Multi-region deployment model can be leveraged to serve end users closer to a particular geography through the same Cassandra infrastructure. Cassandra’s built-in node replication takes care of the synchronization of multi-master writes originating from multiple data centers and presents a consistent view of the data to applications. Multi-region deployment can also help with the risk mitigation of the broader Azure service outages. Cassandra’s tunable consistency and replication topology will help in meeting diverse RPO needs of applications.</p>\n<h3 id=\"single-region-deployment\">Single Region Deployment</h3>\n<p>We will start with a single region deployment and harvest the learnings in creating a multi-region model. Azure virtual networking will be used to create isolated subnets so that the network security requirements mentioned above can be met.  The process described in creating the single region deployment uses Ubuntu 14.04 LTS and Cassandra 2.08; however, the process can easily be adopted to the other Linux variants. The following are some of the systemic characteristics of the single region deployment.  </p>\n<p><strong>High Availability:</strong> The Cassandra nodes shown in the Figure 1 are deployed to two availability sets so that the nodes are spread between multiple fault domains for high availability. VMs annotated with each availability set is mapped to 2 fault domains.  Microsoft Azure uses the concept of fault domain to manage unplanned down time (e.g. hardware or software failures) while the concept of upgrade domain (e.g. host or guest OS patching/upgrades, application upgrades) is used for managing scheduled down time. Please see <a href=\"http://msdn.microsoft.com/library/dn251004.aspx\" data-linktype=\"external\">Disaster Recovery and High Availability for Azure Applications</a> for the role of fault and upgrade domains in attaining high availability.</p>\n<p><img src=\"https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/media/cassandra-nodejs/cassandra-linux1.png\" alt=\"Single region deployment\" data-linktype=\"relative-path\"/></p>\n<p>Figure 1: Single region deployment</p>\n<p>Note that at the time of this writing, Azure doesn’t allow the explicit mapping of a group of VMs to a specific fault domain; consequently, even with the deployment model shown in Figure 1, it is statistically probable that all the virtual machines may be mapped to two fault domains instead of four.</p>\n<p><strong>Load Balancing Thrift Traffic:</strong> Thrift client libraries inside the web server connect to the cluster through an internal load balancer. This requires the process of adding the internal load balancer to the “data” subnet (refer Figure 1) in the context of the cloud service hosting the Cassandra cluster. Once the internal load balancer is defined, each node requires the load balanced endpoint to be added with the annotations of a load balanced set with previously defined load balancer name. See <a href=\"https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-internal-overview\" data-linktype=\"relative-path\">Azure Internal Load Balancing </a>for more details.</p>\n<p><strong>Cluster Seeds:</strong> It is important to select the most highly available nodes for seeds as the new nodes will communicate with seed nodes to discover the topology of the cluster. One node from each availability set is designated as seed nodes to avoid single point of failure.</p>\n<p><strong>Replication Factor and Consistency Level:</strong> Cassandra’s build-in high-availability and data durability is characterized by the Replication Factor (RF - number of copies of each row stored on the cluster) and Consistency Level (number of replicas to be read/written before returning the result to the caller). Replication factor is specified during the KEYSPACE (similar to a relational database) creation whereas the consistency level is specified while issuing the CRUD query. See Cassandra documentation at <a href=\"http://www.datastax.com/documentation/cassandra/2.0/cassandra/dml/dml_config_consistency_c.html\" data-linktype=\"external\">Configuring for Consistency</a> for consistency details and the formula for quorum computation.</p>\n<p>Cassandra supports two types of data integrity models – Consistency and Eventual Consistency; the Replication Factor and Consistency Level will together determine if the data will be consistent as soon as a write operation is complete or it will be eventually consistent. For example, specifying QUORUM as the Consistency Level will always ensures data Consistency while any consistency level, below the number of replicas to be written as needed to attain QUORUM (e.g. ONE) results in data being eventually consistent.</p>\n<p>The 8-node cluster shown above, with a replication factor of 3 and QUORUM (2 nodes are read or written for consistency) read/write consistency level, can survive the theoretical loss of at the most 1 node per replication group before the application start noticing the failure. This assumes that all the key spaces have well balanced read/write requests.  The following are the parameters we will use for the deployed cluster:</p>\n<p>Single region Cassandra cluster configuration:</p>\n<table><thead><tr><th>Cluster Parameter</th>\n<th>Value</th>\n<th>Remarks</th>\n</tr></thead><tbody readability=\"10\"><tr readability=\"1\"><td>Number of Nodes (N)</td>\n<td>8</td>\n<td>Total number of nodes in the cluster</td>\n</tr><tr readability=\"2\"><td>Replication Factor (RF)</td>\n<td>3</td>\n<td>Number of replicas of a given row</td>\n</tr><tr readability=\"5\"><td>Consistency Level (Write)</td>\n<td>QUORUM[(RF/2) +1) = 2] The result of the formula is rounded down</td>\n<td>Writes at the most 2 replicas before the response is sent to the caller; 3rd replica is written in an eventually consistent manner.</td>\n</tr><tr readability=\"3\"><td>Consistency Level (Read)</td>\n<td>QUORUM [(RF/2) +1= 2] The result of the formula is rounded down</td>\n<td>Reads 2 replicas before sending response to the caller.</td>\n</tr><tr readability=\"5\"><td>Replication Strategy</td>\n<td>NetworkTopologyStrategy see <a href=\"http://www.datastax.com/documentation/cassandra/2.0/cassandra/architecture/architectureDataDistributeReplication_c.html\" data-linktype=\"external\">Data Replication</a> in Cassandra documentation for more information</td>\n<td>Understands the deployment topology and places replicas on nodes so that all the replicas don’t end up on the same rack</td>\n</tr><tr readability=\"7\"><td>Snitch</td>\n<td>GossipingPropertyFileSnitch see <a href=\"http://www.datastax.com/documentation/cassandra/2.0/cassandra/architecture/architectureSnitchesAbout_c.html\" data-linktype=\"external\">Snitches</a> in Cassandra documentation for more information</td>\n<td>NetworkTopologyStrategy uses a concept of snitch to understand the topology. GossipingPropertyFileSnitch gives better control in mapping each node to data center and rack. The cluster then uses gossip to propagate this information. This is much simpler in dynamic IP setting relative to PropertyFileSnitch</td>\n</tr></tbody></table><p><strong>Azure Considerations for Cassandra Cluster:</strong> Microsoft Azure Virtual Machines capability uses Azure Blob storage for disk persistence; Azure Storage saves 3 replicas of each disk for high durability. That means each row of data inserted into a Cassandra table is already stored in 3 replicas and hence data consistency is already taken care of even if the Replication Factor (RF) is 1. The main problem with Replication Factor being 1 is that the application will experience downtime even if a single Cassandra node fails. However, if a node is down for the problems (e.g. hardware, system software failures) recognized by Azure Fabric Controller, it will provision a new node in its place using the same storage drives. Provisioning a new node to replace the old one may take a few minutes.  Similarly for planned maintenance activities like guest OS changes, Cassandra upgrades and application changes Azure Fabric Controller performs rolling upgrades of the nodes in the cluster.  Rolling upgrades also may take down a few nodes at a time and hence the cluster may experience brief downtime for a few partitions. However, the data will not be lost due to the built-in Azure Storage redundancy.  </p>\n<p>For systems deployed to Azure that doesn’t require high availability (e.g. around 99.9 which is equivalent to 8.76 hrs/year; see <a href=\"http://en.wikipedia.org/wiki/High_availability\" data-linktype=\"external\">High Availability</a> for details) you may be able to run with RF=1 and Consistency Level=ONE.  For applications with high availability requirements, RF=3 and Consistency Level=QUORUM will tolerate the down time of one of the nodes one of the replicas. RF=1 in traditional deployments (e.g. on-premises) can’t be used due to the possible data loss resulting from problems like disk failures.   </p>\n<h2 id=\"multi-region-deployment\">Multi-region Deployment</h2>\n<p>Cassandra’s data-center-aware replication and consistency model described above helps with the multi-region deployment out of the box without the need for any external tooling. This is quite different from the traditional relational databases where the setup for database mirroring for multi-master writes can be quite complex. Cassandra in a multi-region set up can help with the usage scenarios including the following:</p>\n<p><strong>Proximity based deployment:</strong> Multi-tenant applications, with clear mapping of tenant users -to-region, can be benefited by the multi-region cluster’s low latencies. For example a learning management systems for educational institutions can deploy a distributed cluster in East US and West US regions to serve the respective campuses for transactional as well as analytics. The data can be locally consistent at the time reads and writes and can be eventually consistent across both the regions. There are other examples like media distribution, e-commerce and anything and everything that serves geo concentrated user base is a good use case for this deployment model.</p>\n<p><strong>High Availability:</strong> Redundancy is a key factor in attaining high availability of software and hardware; see Building Reliable Cloud Systems on Microsoft Azure for details. On Microsoft Azure, the only reliable way of achieving true redundancy is by deploying a multi-region cluster. Applications can be deployed in an active-active or active-passive mode and if one of the regions is down, Azure Traffic Manager can redirect traffic to the active region.  With the single region deployment, if the availability is 99.9, a two-region deployment can attain an availability of 99.9999 computed by the formula: (1-(1-0.999) * (1-0.999))*100); see the above paper for details.</p>\n<p><strong>Disaster Recovery:</strong> Multi-region Cassandra cluster, if properly designed, can withstand catastrophic data center outages. If one region is down, the application deployed to other regions can start serving the end users. Like any other business continuity implementations, the application has to be tolerant for some data loss resulting from the data in the asynchronous pipeline. However, Cassandra makes the recovery much swifter than the time taken by traditional database recovery processes. Figure 2 shows the typical multi-region deployment model with eight nodes in each region. Both regions are mirror images of each other for the same of symmetry; real world designs depend on the workload type (e.g. transactional or analytical), RPO, RTO, data consistency and availability requirements.</p>\n<p><img src=\"https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/media/cassandra-nodejs/cassandra-linux2.png\" alt=\"Multi region deployment\" data-linktype=\"relative-path\"/></p>\n<p>Figure 2: Multi-region Cassandra deployment</p>\n<h3 id=\"network-integration\">Network Integration</h3>\n<p>Sets of virtual machines deployed to private networks located on two regions communicates with each other using a VPN tunnel. The VPN tunnel connects two software gateways provisioned during the network deployment process. Both regions have similar network architecture in terms of “web” and “data” subnets; Azure networking allows the creation of as many subnets as needed and apply ACLs as needed by network security. While designing the cluster topology inter data center communication latency and the economic impact of the network traffic need to be considered.</p>\n<h3 id=\"data-consistency-for-multi-data-center-deployment\">Data Consistency for Multi-Data Center Deployment</h3>\n<p>Distributed deployments need to be aware of the cluster topology impact on throughput and high availability. The RF and Consistency Level need to be selected in such way that the quorum doesn’t depend on the availability of all the data centers.\nFor a system that needs high consistency, a LOCAL_QUORUM for consistency level (for reads and writes) will make sure that the local reads and writes are satisfied from the local nodes while data is replicated asynchronously to the remote data centers.  Table 2 summarizes the configuration details for the multi-region cluster outlined later in the write up.</p>\n<p><strong>Two-region Cassandra cluster configuration</strong></p>\n<table><thead><tr><th>Cluster Parameter</th>\n<th>Value</th>\n<th>Remarks</th>\n</tr></thead><tbody readability=\"11\"><tr readability=\"1\"><td>Number of Nodes (N)</td>\n<td>8 + 8</td>\n<td>Total number of nodes in the cluster</td>\n</tr><tr readability=\"2\"><td>Replication Factor (RF)</td>\n<td>3</td>\n<td>Number of replicas of a given row</td>\n</tr><tr readability=\"5\"><td>Consistency Level (Write)</td>\n<td>LOCAL_QUORUM [(sum(RF)/2) +1) = 4] The result of the formula is rounded down</td>\n<td>2 nodes will be written to the first data center synchronously; the additional 2 nodes needed for quorum will be written asynchronously to the 2nd data center.</td>\n</tr><tr readability=\"5\"><td>Consistency Level (Read)</td>\n<td>LOCAL_QUORUM ((RF/2) +1) = 2 The result of the formula is rounded down</td>\n<td>Read requests are satisfied from only one region; 2 nodes are read before the response is sent back to the client.</td>\n</tr><tr readability=\"5\"><td>Replication Strategy</td>\n<td>NetworkTopologyStrategy see <a href=\"http://www.datastax.com/documentation/cassandra/2.0/cassandra/architecture/architectureDataDistributeReplication_c.html\" data-linktype=\"external\">Data Replication</a> in Cassandra documentation for more information</td>\n<td>Understands the deployment topology and places replicas on nodes so that all the replicas don’t end up on the same rack</td>\n</tr><tr readability=\"7\"><td>Snitch</td>\n<td>GossipingPropertyFileSnitch see <a href=\"http://www.datastax.com/documentation/cassandra/2.0/cassandra/architecture/architectureSnitchesAbout_c.html\" data-linktype=\"external\">Snitches</a> in Cassandra documentation for more information</td>\n<td>NetworkTopologyStrategy uses a concept of snitch to understand the topology. GossipingPropertyFileSnitch gives better control in mapping each node to data center and rack. The cluster then uses gossip to propagate this information. This is much simpler in dynamic IP setting relative to PropertyFileSnitch</td>\n</tr></tbody></table><h2 id=\"the-software-configuration\">THE SOFTWARE CONFIGURATION</h2>\n<p>The following software versions are used during the deployment:</p>\n<table readability=\"0\"><tr><th>Software</th><th>Source</th><th>Version</th></tr><tr><td>JRE    </td><td><a href=\"http://www.oracle.com/technetwork/java/javase/downloads/server-jre8-downloads-2133154.html\" data-linktype=\"external\">JRE 8</a> </td><td>8U5</td></tr><tr><td>JNA    </td><td><a href=\"https://github.com/twall/jna\" data-linktype=\"external\">JNA</a> </td><td> 3.2.7</td></tr><tr readability=\"0\"><td>Cassandra</td><td><a href=\"http://www.apache.org/dist/cassandra/2.0.8/apache-cassandra-2.0.8-bin.tar.gz\" data-linktype=\"external\">Apache Cassandra 2.0.8</a></td><td> 2.0.8</td></tr><tr><td>Ubuntu    </td><td><a href=\"https://azure.microsoft.com/\" data-linktype=\"external\">Microsoft Azure</a> </td><td>14.04 LTS</td></tr></table><p>Since downloading of JRE requires manual acceptance of Oracle license, to simplify the deployment, download all the required software to the desktop for later uploading into the Ubuntu template image we will be creating as a precursor to the cluster deployment.</p>\n<p>Download the above software into a well-known download directory (e.g. %TEMP%/downloads on Windows or ~/Downloads on most Linux distributions or Mac) on the local computer.</p>\n<h3 id=\"create-ubuntu-vm\">CREATE UBUNTU VM</h3>\n<p>In this step of the process we will create Ubuntu image with the pre-requisite software so that the image can be reused for provisioning several Cassandra nodes.  </p>\n<h4 id=\"step-1-generate-ssh-key-pair\">STEP 1: Generate SSH key pair</h4>\n<p>Azure needs an X509 public key that is either PEM or DER encoded at the provisioning time. Generate a public/private key pair using the instructions located at How to Use SSH with Linux on Azure. If you plan to use putty.exe as an SSH client either on Windows or Linux, you have to convert the PEM encoded RSA private key to PPK format using puttygen.exe; the instructions for this can be found in the above web page.</p>\n<h4 id=\"step-2-create-ubuntu-template-vm\">STEP 2: Create Ubuntu template VM</h4>\n<p>To create the template VM, log into the Azure portal and use the following sequence: Click NEW, COMPUTE, VIRTUAL MACHINE, FROM GALLERY, UBUNTU, Ubuntu Server 14.04 LTS, and then click the right arrow. For a tutorial that describes how to create a Linux VM, see Create a Virtual Machine Running Linux.</p>\n<p>Enter the following information on the “Virtual machine configuration” screen #1:</p>\n<table readability=\"4\"><tr><th>FIELD NAME              </th><td>       FIELD VALUE               </td><td>         REMARKS                </td></tr><tr/><tr readability=\"2\"><td>VERSION RELEASE DATE    </td><td> Select a date from the drop down</td><td/></tr><tr/><tr readability=\"2\"><td>VIRTUAL MACHINE NAME    </td><td> cass-template                   </td><td> This is the hostname of the VM </td></tr><tr/><tr><td>TIER                     </td><td> STANDARD                           </td><td> Leave the default              </td></tr><tr/><tr readability=\"1\"><td>SIZE                     </td><td> A1                              </td><td>Select the VM based on the IO needs; for this purpose leave the default </td></tr><tr/><tr readability=\"1\"><td> NEW USER NAME             </td><td> localadmin                       </td><td> \"admin\" is a reserved user name in Ubuntu 12.xx and after</td></tr><tr/><tr readability=\"1\"><td> AUTHENTICATION         </td><td> Click check box                 </td><td>Check if you want to secure with an SSH key </td></tr><tr/><tr readability=\"2\"><td> CERTIFICATE             </td><td> file name of the public key certificate </td><td> Use the public key generated previously</td></tr><tr/><tr><td> New Password    </td><td> strong password </td><td> </td></tr><tr/><tr><td> Confirm Password    </td><td> strong password </td><td/></tr><tr/></table><p>Enter the following information on the “Virtual machine configuration” screen #2:</p>\n<table readability=\"4\"><tr><th>FIELD NAME             </th><th> FIELD VALUE                       </th><th> REMARKS                                 </th></tr><tr readability=\"2\"><td> CLOUD SERVICE    </td><td> Create a new cloud service    </td><td>Cloud service is a container compute resources like virtual machines</td></tr><tr readability=\"3\"><td> CLOUD SERVICE DNS NAME    </td><td>ubuntu-template.cloudapp.net    </td><td>Give a machine agnostic load balancer name</td></tr><tr readability=\"2\"><td> REGION/AFFINITY GROUP/VIRTUAL NETWORK </td><td>    West US    </td><td> Select a region from which your web applications access the Cassandra cluster</td></tr><tr readability=\"1\"><td>STORAGE ACCOUNT </td><td>    Use default    </td><td>Use the default storage account  or a pre-created storage account in a particular region</td></tr><tr><td>AVAILABILITY SET </td><td>    None </td><td>    Leave it blank</td></tr><tr readability=\"1\"><td>ENDPOINTS    </td><td>Use default </td><td>    Use the default SSH configuration </td></tr></table><p>Click right arrow, leave the defaults on the screen #3 and click the “check” button to complete the VM provisioning process. After a few minutes, the VM with the name “ubuntu-template” should be in a “running” status.</p>\n<h3 id=\"install-the-necessary-software\">INSTALL THE NECESSARY SOFTWARE</h3>\n<p>Using scp or pscp, copy the previously downloaded software to ~/downloads directory using the following command format:</p>\n<h5 id=\"pscp-server-jre-8u5-linux-x64targz-localadminhk-cas-templatecloudappnethomelocaladmindownloadsserver-jre-8u5-linux-x64targz\">pscp server-jre-8u5-linux-x64.tar.gz localadmin@hk-cas-template.cloudapp.net:/home/localadmin/downloads/server-jre-8u5-linux-x64.tar.gz</h5>\n<p>Repeat the above command for JRE as well as for the Cassandra bits.</p>\n<p>Log into the VM and create the directory structure and extract software as a super user using the bash script below:</p>\n<pre>#!/bin/bash\nCASS_INSTALL_DIR=\"/opt/cassandra\"\nJRE_INSTALL_DIR=\"/opt/java\"\nCASS_DATA_DIR=\"/var/lib/cassandra\"\nCASS_LOG_DIR=\"/var/log/cassandra\"\nDOWNLOADS_DIR=\"~/downloads\"\nJRE_TARBALL=\"server-jre-8u5-linux-x64.tar.gz\"\nCASS_TARBALL=\"apache-cassandra-2.0.8-bin.tar.gz\"\nSVC_USER=\"localadmin\"\nRESET_ERROR=1\nMKDIR_ERROR=2\nreset_installation ()\n{\n   rm -rf $CASS_INSTALL_DIR 2&gt; /dev/null\n   rm -rf $JRE_INSTALL_DIR 2&gt; /dev/null\n   rm -rf $CASS_DATA_DIR 2&gt; /dev/null\n   rm -rf $CASS_LOG_DIR 2&gt; /dev/null\n}\nmake_dir ()\n{\n   if [ -z \"$1\" ]\n   then\n      echo \"make_dir: invalid directory name\"\n      exit $MKDIR_ERROR\n   fi\n   if [ -d \"$1\" ]\n   then\n      echo \"make_dir: directory already exists\"\n      exit $MKDIR_ERROR\n   fi\n   mkdir $1 2&gt;/dev/null\n   if [ $? != 0 ]\n   then\n      echo \"directory creation failed\"\n      exit $MKDIR_ERROR\n   fi\n}\nunzip()\n{\n   if [ $# == 2 ]\n   then\n      tar xzf $1 -C $2\n   else\n      echo \"archive error\"\n   fi\n}\nif [ -n \"$1\" ]\nthen\n   SVC_USER=$1\nfi\nreset_installation\nmake_dir $CASS_INSTALL_DIR\nmake_dir $JRE_INSTALL_DIR\nmake_dir $CASS_DATA_DIR\nmake_dir $CASS_LOG_DIR\n#unzip JRE and Cassandra\nunzip $HOME/downloads/$JRE_TARBALL $JRE_INSTALL_DIR\nunzip $HOME/downloads/$CASS_TARBALL $CASS_INSTALL_DIR\n#Change the ownership to the service credentials\nchown -R $SVC_USER:$GROUP $CASS_DATA_DIR\nchown -R $SVC_USER:$GROUP $CASS_LOG_DIR\necho \"edit /etc/profile to add JRE to the PATH\"\necho \"installation is complete\"\n</pre><p>If you paste this script into vim window, make sure to remove the carriage return (‘\\r”) using the following command:</p>\n<pre>tr -d '\\r' &lt;infile.sh &gt;outfile.sh\n</pre><h4 id=\"step-3-edit-etcprofile\">Step 3: Edit etc/profile</h4>\n<p>Append the following at the end:</p>\n<pre>JAVA_HOME=/opt/java/jdk1.8.0_05\nCASS_HOME= /opt/cassandra/apache-cassandra-2.0.8\nPATH=$PATH:$HOME/bin:$JAVA_HOME/bin:$CASS_HOME/bin\nexport JAVA_HOME\nexport CASS_HOME\nexport PATH\n</pre><h4 id=\"step-4-install-jna-for-production-systems\">Step 4: Install JNA for production systems</h4>\n<p>Use the following command sequence:\nThe following command will install jna-3.2.7.jar and jna-platform-3.2.7.jar to /usr/share.java directory\nsudo apt-get install libjna-java</p>\n<p>Create symbolic links in $CASS_HOME/lib directory so that Cassandra startup script can find these jars:</p>\n<pre>ln -s /usr/share/java/jna-3.2.7.jar $CASS_HOME/lib/jna.jar\nln -s /usr/share/java/jna-platform-3.2.7.jar $CASS_HOME/lib/jna-platform.jar\n</pre><h4 id=\"step-5-configure-cassandrayaml\">Step 5: Configure cassandra.yaml</h4>\n<p>Edit cassandra.yaml on each VM to reflect configuration needed by all the virtual machines [we will tweak this during the actual provisioning]:</p>\n<table readability=\"3\"><tr><th>Field Name   </th><th> Value  </th><th>    Remarks </th></tr><tr readability=\"1\"><td>cluster_name </td><td>    “CustomerService”    </td><td> Use the name that reflects your deployment</td></tr><tr><td>listen_address    </td><td>[leave it blank]    </td><td> Delete “localhost” </td></tr><tr><td>rpc_addres   </td><td>[leave it blank]    </td><td> Delete “localhost” </td></tr><tr readability=\"4\"><td>seeds    </td><td>\"10.1.2.4, 10.1.2.6, 10.1.2.8\"    </td><td>List of  all the IP addresses which are designated as seeds.</td></tr><tr readability=\"2\"><td>endpoint_snitch </td><td> org.apache.cassandra.locator.GossipingPropertyFileSnitch </td><td> This is used by the NetworkTopologyStrateg for inferring the data center and the rack of the VM</td></tr></table><h4 id=\"step-6-capture-the-vm-image\">Step 6: Capture the VM image</h4>\n<p>Log into the virtual machine using the hostname (hk-cas-template.cloudapp.net) and the SSH private key previously created. See How to Use SSH with Linux on Azure for details on how to log in using the command ssh or putty.exe.</p>\n<p>Execute the following sequence of actions to capture the image:</p>\n<h5 id=\"1-deprovision\">1. Deprovision</h5>\n<p>Use the command “sudo waagent –deprovision+user” to remove Virtual Machine instance specific information. See for <a href=\"https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/capture-image\" data-linktype=\"relative-path\">How to Capture a Linux Virtual Machine</a> to Use as a Template more details on the image capture process.</p>\n<h5 id=\"2-shutdown-the-vm\">2: Shutdown the VM</h5>\n<p>Make sure that the virtual machine is highlighted and click the SHUTDOWN link from the bottom command bar.</p>\n<h5 id=\"3-capture-the-image\">3: Capture the image</h5>\n<p>Make sure that the virtual machine is highlighted and click the CAPTURE link from the bottom command bar. In the next screen, give an IMAGE NAME (e.g. hk-cas-2-08-ub-14-04-2014071), appropriate IMAGE DESCRIPTION, and click the “check” mark to finish the CAPTURE process.</p>\n<p>This will take a few seconds and the image should be available in MY IMAGES section of the image gallery. The source VM will be automatically deleted after the image is successfully captured. </p>\n<p><strong>Step 1: Create the Virtual Network</strong>\nLog into the Azure portal and create a virtual network (classic) with the attributes shown in the following table. See <a href=\"https://docs.microsoft.com/en-us/azure/virtual-network/virtual-networks-create-vnet-classic-pportal\" data-linktype=\"relative-path\">Create a virtual network (classic) using the Azure portal</a> for detailed steps of the process.      </p>\n<table readability=\"0\"><tr><th>VM Attribute Name</th><th>Value</th><th>Remarks</th></tr><tr><td>Name</td><td>vnet-cass-west-us</td><td/></tr><tr><td>Region</td><td>West US</td><td/></tr><tr readability=\"1\"><td>DNS Servers</td><td>None</td><td>Ignore this as we are not using a DNS Server</td></tr><tr><td>Address Space</td><td>10.1.0.0/16</td><td/></tr><br/><tr><td>Starting IP</td><td>10.1.0.0</td><td/></tr><br/><tr><td>CIDR </td><td>/16 (65531)</td><td/></tr></table><p>Add the following subnets:</p>\n<table readability=\"1\"><tr><th>Name</th><th>Starting IP</th><th>CIDR</th><th>Remarks</th></tr><tr readability=\"1\"><td>web</td><td>10.1.1.0</td><td>/24 (251)</td><td>Subnet for the web farm</td></tr><tr readability=\"1\"><td>data</td><td>10.1.2.0</td><td>/24 (251)</td><td>Subnet for the database nodes</td></tr></table><p>Data and Web subnets can be protected through network security groups the coverage of which is out of scope for this article.  </p>\n<p><strong>Step 2: Provision Virtual Machines</strong>\nUsing the image created previously, we will create the following virtual machines in the cloud server “hk-c-svc-west” and bind them to the respective subnets as shown below:</p>\n<table readability=\"4\"><tr><th>Machine Name    </th><th>Subnet    </th><th>IP Address    </th><th>Availability set</th><th>DC/Rack</th><th>Seed?</th></tr><tr readability=\"1\"><td>hk-c1-west-us    </td><td>data    </td><td>10.1.2.4    </td><td>hk-c-aset-1    </td><td>dc =WESTUS rack =rack1 </td><td>Yes</td></tr><tr readability=\"1\"><td>hk-c2-west-us    </td><td>data    </td><td>10.1.2.5    </td><td>hk-c-aset-1    </td><td>dc =WESTUS rack =rack1    </td><td>No </td></tr><tr readability=\"1\"><td>hk-c3-west-us    </td><td>data    </td><td>10.1.2.6    </td><td>hk-c-aset-1    </td><td>dc =WESTUS rack =rack2    </td><td>Yes</td></tr><tr readability=\"1\"><td>hk-c4-west-us    </td><td>data    </td><td>10.1.2.7    </td><td>hk-c-aset-1    </td><td>dc =WESTUS rack =rack2    </td><td>No </td></tr><tr readability=\"1\"><td>hk-c5-west-us    </td><td>data    </td><td>10.1.2.8    </td><td>hk-c-aset-2    </td><td>dc =WESTUS rack =rack3    </td><td>Yes</td></tr><tr readability=\"1\"><td>hk-c6-west-us    </td><td>data    </td><td>10.1.2.9    </td><td>hk-c-aset-2    </td><td>dc =WESTUS rack =rack3    </td><td>No </td></tr><tr readability=\"1\"><td>hk-c7-west-us    </td><td>data    </td><td>10.1.2.10    </td><td>hk-c-aset-2    </td><td>dc =WESTUS rack =rack4    </td><td>Yes</td></tr><tr readability=\"1\"><td>hk-c8-west-us    </td><td>data    </td><td>10.1.2.11    </td><td>hk-c-aset-2    </td><td>dc =WESTUS rack =rack4    </td><td>No </td></tr><tr><td>hk-w1-west-us    </td><td>web    </td><td>10.1.1.4    </td><td>hk-w-aset-1    </td><td>                       </td><td>N/A</td></tr><tr><td>hk-w2-west-us    </td><td>web    </td><td>10.1.1.5    </td><td>hk-w-aset-1    </td><td>                       </td><td>N/A</td></tr></table><p>Creation of the above list of VMs requires the following process:</p>\n<ol><li>Create an empty cloud service in a particular region</li>\n<li>Create a VM from the previously captured image and attach it to the virtual network created previously; repeat this for all the VMs</li>\n<li>Add an internal load balancer to the cloud service and attach it to the “data” subnet</li>\n<li>For each VM created previously, add a load balanced endpoint for thrift traffic through a load balanced set connected to the previously created internal load balancer</li>\n</ol><p>The above process can be executed using Azure classic portal; use a Windows machine (use a VM on Azure if you don't have access to a Windows machine), use the following PowerShell script to provision all 8 VMs automatically.</p>\n<p><strong>List 1: PowerShell script for provisioning virtual machines</strong></p>\n<pre>    #Tested with Azure Powershell - November 2014\n    #This powershell script deployes a number of VMs from an existing image inside an Azure region\n    #Import your Azure subscription into the current Powershell session before proceeding\n    #The process: 1. create Azure Storage account, 2. create virtual network, 3.create the VM template, 2. crate a list of VMs from the template\n    #fundamental variables - change these to reflect your subscription\n    $country=\"us\"; $region=\"west\"; $vnetName = \"your_vnet_name\";$storageAccount=\"your_storage_account\"\n    $numVMs=8;$prefix = \"hk-cass\";$ilbIP=\"your_ilb_ip\"\n    $subscriptionName = \"Azure_subscription_name\";\n    $vmSize=\"ExtraSmall\"; $imageName=\"your_linux_image_name\"\n    $ilbName=\"ThriftInternalLB\"; $thriftEndPoint=\"ThriftEndPoint\"\n    #generated variables\n    $serviceName = \"$prefix-svc-$region-$country\"; $azureRegion = \"$region $country\"\n    $vmNames = @()\n    for ($i=0; $i -lt $numVMs; $i++)\n    {\n       $vmNames+=(\"$prefix-vm\"+($i+1) + \"-$region-$country\" );\n    }\n    #select an Azure subscription already imported into Powershell session\n    Select-AzureSubscription -SubscriptionName $subscriptionName -Current\n    Set-AzureSubscription -SubscriptionName $subscriptionName -CurrentStorageAccountName $storageAccount\n    #create an empty cloud service\n    New-AzureService -ServiceName $serviceName -Label \"hkcass$region\" -Location $azureRegion\n    Write-Host \"Created $serviceName\"\n    $VMList= @()   # stores the list of azure vm configuration objects\n    #create the list of VMs\n    foreach($vmName in $vmNames)\n    {\n       $VMList += New-AzureVMConfig -Name $vmName -InstanceSize ExtraSmall -ImageName $imageName |\n       Add-AzureProvisioningConfig -Linux -LinuxUser \"localadmin\" -Password \"Local123\" |\n       Set-AzureSubnet \"data\"\n    }\n    New-AzureVM -ServiceName $serviceName -VNetName $vnetName -VMs $VMList\n    #Create internal load balancer\n    Add-AzureInternalLoadBalancer -ServiceName $serviceName -InternalLoadBalancerName $ilbName -SubnetName \"data\" -StaticVNetIPAddress \"$ilbIP\"\n    Write-Host \"Created $ilbName\"\n    #Add add the thrift endpoint to the internal load balancer for all the VMs\n    foreach($vmName in $vmNames)\n    {\n        Get-AzureVM -ServiceName $serviceName -Name $vmName |\n            Add-AzureEndpoint -Name $thriftEndPoint -LBSetName \"ThriftLBSet\" -Protocol tcp -LocalPort 9160 -PublicPort 9160 -ProbePort 9160 -ProbeProtocol tcp -ProbeIntervalInSeconds 10 -InternalLoadBalancerName $ilbName |\n            Update-AzureVM\n        Write-Host \"created $vmName\"     \n    }\n</pre><p><strong>Step 3: Configure Cassandra on each VM</strong></p>\n<p>Log into the VM and perform the following:</p>\n<ul readability=\"4\"><li readability=\"3\"><p>Edit $CASS_HOME/conf/cassandra-rackdc.properties to specify the data center and rack properties:</p>\n<pre> dc =EASTUS, rack =rack1\n</pre></li>\n<li readability=\"5\"><p>Edit cassandra.yaml to configure seed nodes as below:</p>\n<pre> Seeds: \"10.1.2.4,10.1.2.6,10.1.2.8,10.1.2.10\"\n</pre></li>\n</ul><p><strong>Step 4: Start the VMs and test the cluster</strong></p>\n<p>Log into one of the nodes (e.g. hk-c1-west-us) and run the following command to see the status of the cluster:</p>\n<pre>   nodetool –h 10.1.2.4 –p 7199 status\n</pre><p>You should see the display similar to the one below for an 8-node cluster:</p>\n<table><tr><th>Status</th><th>Address    </th><th>Load    </th><th>Tokens    </th><th>Owns </th><th>Host ID    </th><th>Rack</th></tr><tr><th>UN    </th><td>10.1.2.4     </td><td>87.81 KB    </td><td>256    </td><td>38.0%    </td><td>Guid (removed)</td><td>rack1</td></tr><tr><th>UN    </th><td>10.1.2.5     </td><td>41.08 KB    </td><td>256    </td><td>68.9%    </td><td>Guid (removed)</td><td>rack1</td></tr><tr><th>UN    </th><td>10.1.2.6     </td><td>55.29 KB    </td><td>256    </td><td>68.8%    </td><td>Guid (removed)</td><td>rack2</td></tr><tr><th>UN    </th><td>10.1.2.7     </td><td>55.29 KB    </td><td>256    </td><td>68.8%    </td><td>Guid (removed)</td><td>rack2</td></tr><tr><th>UN    </th><td>10.1.2.8     </td><td>55.29 KB    </td><td>256    </td><td>68.8%    </td><td>Guid (removed)</td><td>rack3</td></tr><tr><th>UN    </th><td>10.1.2.9     </td><td>55.29 KB    </td><td>256    </td><td>68.8%    </td><td>Guid (removed)</td><td>rack3</td></tr><tr><th>UN    </th><td>10.1.2.10     </td><td>55.29 KB    </td><td>256    </td><td>68.8%    </td><td>Guid (removed)</td><td>rack4</td></tr><tr><th>UN    </th><td>10.1.2.11     </td><td>55.29 KB    </td><td>256    </td><td>68.8%    </td><td>Guid (removed)</td><td>rack4</td></tr></table><p>Use the following steps to test the cluster:</p>\n<ol readability=\"9\"><li>Using the Powershell command Get-AzureInternalLoadbalancer commandlet, obtain the IP address of the internal load balancer (e.g.  10.1.2.101). The syntax of the command is shown below: Get-AzureLoadbalancer –ServiceName \"hk-c-svc-west-us” [displays the details of the internal load balancer along with its IP address]</li>\n<li>Log into the web farm VM (e.g. hk-w1-west-us) using Putty or ssh</li>\n<li>Execute $CASS_HOME/bin/cqlsh 10.1.2.101 9160</li>\n<li readability=\"19\"><p>Use the following CQL commands to verify if the cluster is working:</p>\n<p>  CREATE KEYSPACE customers_ks WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 3 };\n  USE customers_ks;\n  CREATE TABLE Customers(customer_id int PRIMARY KEY, firstname text, lastname text);\n  INSERT INTO Customers(customer_id, firstname, lastname) VALUES(1, 'John', 'Doe');\n  INSERT INTO Customers(customer_id, firstname, lastname) VALUES (2, 'Jane', 'Doe');</p>\n<p>  SELECT * FROM Customers;</p>\n</li>\n</ol><p>You should see a display like the one below:</p>\n<table><tr><th> customer_id </th><th> firstname </th><th> lastname </th></tr><tr><td> 1 </td><td> John </td><td> Doe </td></tr><tr><td> 2 </td><td> Jane </td><td> Doe </td></tr></table><p>Please note that the keyspace created in step 4 uses SimpleStrategy with a  replication_factor of 3. SimpleStrategy is recommended for single data center deployments whereas NetworkTopologyStrategy for multi-data center deployments. A replication_factor of 3 will give tolerance for node failures.</p>\n<h2 id=\"a-idtworegion-amulti-region-deployment-process\"><a id=\"tworegion\"> </a>Multi-Region Deployment Process</h2>\n<p>Will leverage the single region deployment completed and repeat the same process for installing the second region. The key difference between the single and multiple region deployment is the VPN tunnel setup for inter-region communication; we will start with the network installation, provision the VMs and configure Cassandra.</p>\n<h3 id=\"step-1-create-the-virtual-network-at-the-2nd-region\">Step 1: Create the Virtual Network at the 2nd Region</h3>\n<p>Log into the Azure classic portal and create a Virtual Network with the attributes show in the table. See <a href=\"https://docs.microsoft.com/en-us/azure/virtual-network/virtual-networks-create-vnet-classic-pportal\" data-linktype=\"relative-path\">Configure a Cloud-Only Virtual Network in the Azure classic portal</a> for detailed steps of the process.      </p>\n<table readability=\"1\"><tr><th>Attribute Name    </th><th>Value    </th><th>Remarks</th></tr><tr><td>Name    </td><td>vnet-cass-east-us</td><td/></tr><tr><td>Region    </td><td>East US</td><td/></tr><tr readability=\"1\"><td>DNS Servers        </td><td/><td>Ignore this as we are not using a DNS Server</td></tr><tr readability=\"1\"><td>Configure a point-to-site VPN</td><td/><td>        Ignore this</td></tr><tr readability=\"1\"><td>Configure a site-to-site VPN</td><td/><td>        Ignore this</td></tr><tr><td>Address Space    </td><td>10.2.0.0/16</td><td/></tr><tr><td>Starting IP    </td><td>10.2.0.0    </td><td/></tr><tr><td>CIDR    </td><td>/16 (65531)</td><td/></tr></table><p>Add the following subnets:</p>\n<table readability=\"1\"><tr><th>Name    </th><th>Starting IP    </th><th>CIDR    </th><th>Remarks</th></tr><tr readability=\"1\"><td>web    </td><td>10.2.1.0    </td><td>/24 (251)    </td><td>Subnet for the web farm</td></tr><tr readability=\"1\"><td>data    </td><td>10.2.2.0    </td><td>/24 (251)    </td><td>Subnet for the database nodes</td></tr></table><h3 id=\"step-2-create-local-networks\">Step 2: Create Local Networks</h3>\n<p>A Local Network in Azure virtual networking is a proxy address space that maps to a remote site including a private cloud or another Azure region. This proxy address space is bound to a remote gateway for routing network to the right networking destinations. See <a href=\"https://docs.microsoft.com/en-us/azure/vpn-gateway/virtual-networks-configure-vnet-to-vnet-connection\" data-linktype=\"relative-path\">Configure a VNet to VNet Connection</a> for the instructions on establishing VNET-to-VNET connection.</p>\n<p>Create two local networks per the following details:</p>\n<table><thead><tr><th>Network Name</th>\n<th>VPN Gateway Address</th>\n<th>Address Space</th>\n<th>Remarks</th>\n</tr></thead><tbody readability=\"5\"><tr readability=\"6\"><td>hk-lnet-map-to-east-us</td>\n<td>23.1.1.1</td>\n<td>10.2.0.0/16</td>\n<td>While creating the Local Network give a placeholder gateway address. The real gateway address is filled once the gateway is created. Make sure the address space exactly matches the respective remote VNET; in this case the VNET created in the East US region.</td>\n</tr><tr readability=\"6\"><td>hk-lnet-map-to-west-us</td>\n<td>23.2.2.2</td>\n<td>10.1.0.0/16</td>\n<td>While creating the Local Network give a placeholder gateway address. The real gateway address is filled once the gateway is created. Make sure the address space exactly matches the respective remote VNET; in this case the VNET created in the West US region.</td>\n</tr></tbody></table><h3 id=\"step-3-map-local-network-to-the-respective-vnets\">Step 3: Map “Local” network to the respective VNETs</h3>\n<p>From the Azure classic portal, select each vnet, click “Configure”, check “Connect to the local network”, and select the Local Networks per the following details:</p>\n<table><thead><tr><th>Virtual Network</th>\n<th>Local Network</th>\n</tr></thead><tbody readability=\"1\"><tr readability=\"1\"><td>hk-vnet-west-us</td>\n<td>hk-lnet-map-to-east-us</td>\n</tr><tr readability=\"1\"><td>hk-vnet-east-us</td>\n<td>hk-lnet-map-to-west-us</td>\n</tr></tbody></table><h3 id=\"step-4-create-gateways-on-vnet1-and-vnet2\">Step 4: Create Gateways on VNET1 and VNET2</h3>\n<p>From the dashboard of both the virtual networks, click CREATE GATEWAY which will trigger the VPN gateway provisioning process. After a few minutes the dashboard of each virtual network should display the actual gateway address.</p>\n<p>Edit both the local networks to replace the placeholder gateway IP address with the real IP address of the just provisioned gateways. Use the following mapping:</p>\n<table readability=\"2\"><tr><th>Local Network    </th><th>Virtual Network Gateway</th></tr><tr readability=\"2\"><td>hk-lnet-map-to-east-us </td><td>Gateway of hk-vnet-west-us</td></tr><tr readability=\"2\"><td>hk-lnet-map-to-west-us </td><td>Gateway of hk-vnet-east-us</td></tr></table><p>Use the following Powershell script to update the IPSec key of each VPN gateway [use the sake key for both the gateways]:\nSet-AzureVNetGatewayKey -VNetName hk-vnet-east-us -LocalNetworkSiteName hk-lnet-map-to-west-us -SharedKey D9E76BKK\nSet-AzureVNetGatewayKey -VNetName hk-vnet-west-us -LocalNetworkSiteName hk-lnet-map-to-east-us -SharedKey D9E76BKK</p>\n<h3 id=\"step-7-establish-the-vnet-to-vnet-connection\">Step 7: Establish the VNET-to-VNET connection</h3>\n<p>From the Azure classic portal, use the “DASHBOARD” menu of both the virtual networks to establish gateway-to-gateway connection. Use the “CONNECT” menu items in the bottom toolbar. After a few minutes the dashboard should display the connection details graphically.</p>\n<h3 id=\"step-8-create-the-virtual-machines-in-region-2\">Step 8: Create the virtual machines in region #2</h3>\n<p>Create the Ubuntu image as described in region #1 deployment by following the same steps or copy the image VHD file to the Azure storage account located in region #2 and create the image. Use this image and create the following list of virtual machines into a new cloud service hk-c-svc-east-us:</p>\n<table><thead><tr><th>Machine Name</th>\n<th>Subnet</th>\n<th>IP Address</th>\n<th>Availability set</th>\n<th>DC/Rack</th>\n<th>Seed?</th>\n</tr></thead><tbody readability=\"3\"><tr readability=\"1\"><td>hk-c1-east-us</td>\n<td>data</td>\n<td>10.2.2.4</td>\n<td>hk-c-aset-1</td>\n<td>dc =EASTUS rack =rack1</td>\n<td>Yes</td>\n</tr><tr readability=\"1\"><td>hk-c2-east-us</td>\n<td>data</td>\n<td>10.2.2.5</td>\n<td>hk-c-aset-1</td>\n<td>dc =EASTUS rack =rack1</td>\n<td>No</td>\n</tr><tr readability=\"1\"><td>hk-c3-east-us</td>\n<td>data</td>\n<td>10.2.2.6</td>\n<td>hk-c-aset-1</td>\n<td>dc =EASTUS rack =rack2</td>\n<td>Yes</td>\n</tr><tr readability=\"1\"><td>hk-c5-east-us</td>\n<td>data</td>\n<td>10.2.2.8</td>\n<td>hk-c-aset-2</td>\n<td>dc =EASTUS rack =rack3</td>\n<td>Yes</td>\n</tr><tr readability=\"1\"><td>hk-c6-east-us</td>\n<td>data</td>\n<td>10.2.2.9</td>\n<td>hk-c-aset-2</td>\n<td>dc =EASTUS rack =rack3</td>\n<td>No</td>\n</tr><tr readability=\"1\"><td>hk-c7-east-us</td>\n<td>data</td>\n<td>10.2.2.10</td>\n<td>hk-c-aset-2</td>\n<td>dc =EASTUS rack =rack4</td>\n<td>Yes</td>\n</tr><tr readability=\"1\"><td>hk-c8-east-us</td>\n<td>data</td>\n<td>10.2.2.11</td>\n<td>hk-c-aset-2</td>\n<td>dc =EASTUS rack =rack4</td>\n<td>No</td>\n</tr><tr><td>hk-w1-east-us</td>\n<td>web</td>\n<td>10.2.1.4</td>\n<td>hk-w-aset-1</td>\n<td>N/A</td>\n<td>N/A</td>\n</tr><tr><td>hk-w2-east-us</td>\n<td>web</td>\n<td>10.2.1.5</td>\n<td>hk-w-aset-1</td>\n<td>N/A</td>\n<td>N/A</td>\n</tr></tbody></table><p>Follow the same instructions as region #1 but use 10.2.xxx.xxx address space.</p>\n<h3 id=\"step-9-configure-cassandra-on-each-vm\">Step 9: Configure Cassandra on each VM</h3>\n<p>Log into the VM and perform the following:</p>\n<ol><li>Edit $CASS_HOME/conf/cassandra-rackdc.properties to specify the data center and rack properties in the format:\n dc =EASTUS\n rack =rack1</li>\n<li>Edit cassandra.yaml to configure seed nodes:\n Seeds: \"10.1.2.4,10.1.2.6,10.1.2.8,10.1.2.10,10.2.2.4,10.2.2.6,10.2.2.8,10.2.2.10\"</li>\n</ol><h3 id=\"step-10-start-cassandra\">Step 10: Start Cassandra</h3>\n<p>Log into each VM and start Cassandra in the background by running the following command:\n$CASS_HOME/bin/cassandra</p>\n<h2 id=\"test-the-multi-region-cluster\">Test the Multi-Region Cluster</h2>\n<p>By now Cassandra has been deployed to 16 nodes with 8 nodes in each Azure region. These nodes are in the same cluster by virtue of the common cluster name and the seed node configuration. Use the following process to test the cluster:</p>\n<ul readability=\"1\"><li>Get-AzureInternalLoadbalancer -ServiceName \"hk-c-svc-west-us\"</li>\n<li readability=\"3\"><p>Get-AzureInternalLoadbalancer -ServiceName \"hk-c-svc-east-us\"  </p>\n<p>  Note the IP addresses (e.g. west - 10.1.2.101, east - 10.2.2.101) displayed.</p>\n</li>\n</ul><h3 id=\"step-2-execute-the-following-in-the-west-region-after-logging-into-hk-w1-west-us\">Step 2: Execute the following in the west region after logging into hk-w1-west-us</h3>\n<ol readability=\"9\"><li>Execute $CASS_HOME/bin/cqlsh 10.1.2.101 9160</li>\n<li readability=\"19\"><p>Execute the following CQL commands:</p>\n<p>  CREATE KEYSPACE customers_ks\n  WITH REPLICATION = { 'class' : 'NetworkToplogyStrategy', 'WESTUS' : 3, 'EASTUS' : 3};\n  USE customers_ks;\n  CREATE TABLE Customers(customer_id int PRIMARY KEY, firstname text, lastname text);\n  INSERT INTO Customers(customer_id, firstname, lastname) VALUES(1, 'John', 'Doe');\n  INSERT INTO Customers(customer_id, firstname, lastname) VALUES (2, 'Jane', 'Doe');\n  SELECT * FROM Customers;</p>\n</li>\n</ol><p>You should see a display like the one below:</p>\n<table><thead><tr><th>customer_id</th>\n<th>firstname</th>\n<th>Lastname</th>\n</tr></thead><tbody><tr><td>1</td>\n<td>John</td>\n<td>Doe</td>\n</tr><tr><td>2</td>\n<td>Jane</td>\n<td>Doe</td>\n</tr></tbody></table><h3 id=\"step-3-execute-the-following-in-the-east-region-after-logging-into-hk-w1-east-us\">Step 3: Execute the following in the east region after logging into hk-w1-east-us:</h3>\n<ol readability=\"7\"><li>Execute $CASS_HOME/bin/cqlsh 10.2.2.101 9160</li>\n<li readability=\"15\"><p>Execute the following CQL commands:</p>\n<p>  USE customers_ks;\n  CREATE TABLE Customers(customer_id int PRIMARY KEY, firstname text, lastname text);\n  INSERT INTO Customers(customer_id, firstname, lastname) VALUES(1, 'John', 'Doe');\n  INSERT INTO Customers(customer_id, firstname, lastname) VALUES (2, 'Jane', 'Doe');\n  SELECT * FROM Customers;</p>\n</li>\n</ol><p>You should see the same display as seen for the West region:</p>\n<table><thead><tr><th>customer_id</th>\n<th>firstname</th>\n<th>Lastname</th>\n</tr></thead><tbody><tr><td>1</td>\n<td>John</td>\n<td>Doe</td>\n</tr><tr><td>2</td>\n<td>Jane</td>\n<td>Doe</td>\n</tr></tbody></table><p>Execute a few more inserts and see that those get replicated to west-us part of the cluster.</p>\n<h2 id=\"test-cassandra-cluster-from-nodejs\">Test Cassandra Cluster from Node.js</h2>\n<p>Using one of the Linux VMs crated in the \"web\" tier previously, we will execute a simple Node.js script to read the previously inserted data</p>\n<p><strong>Step 1: Install Node.js and Cassandra Client</strong></p>\n<ol readability=\"20\"><li>Install Node.js and npm</li>\n<li>Install node package \"cassandra-client\" using npm</li>\n<li readability=\"43\"><p>Execute the following script at the shell prompt which displays the json string of the retrieved data:</p>\n<pre> var pooledCon = require('cassandra-client').PooledConnection;\n var ksName = \"custsupport_ks\";\n var cfName = \"customers_cf\";\n var hostList = ['internal_loadbalancer_ip:9160'];\n var ksConOptions = { hosts: hostList,\n                      keyspace: ksName, use_bigints: false };\n function createKeyspace(callback){\n    var cql = 'CREATE KEYSPACE ' + ksName + ' WITH strategy_class=SimpleStrategy AND strategy_options:replication_factor=1';\n    var sysConOptions = { hosts: hostList,  \n                          keyspace: 'system', use_bigints: false };\n    var con = new pooledCon(sysConOptions);\n    con.execute(cql,[],function(err) {\n    if (err) {\n      console.log(\"Failed to create Keyspace: \" + ksName);\n      console.log(err);\n    }\n    else {\n      console.log(\"Created Keyspace: \" + ksName);\n      callback(ksConOptions, populateCustomerData);\n    }\n    });\n    con.shutdown();\n }\n function createColumnFamily(ksConOptions, callback){\n   var params = ['customers_cf','custid','varint','custname',\n                 'text','custaddress','text'];\n   var cql = 'CREATE COLUMNFAMILY ? (? ? PRIMARY KEY,? ?, ? ?)';\n var con =  new pooledCon(ksConOptions);\n   con.execute(cql,params,function(err) {\n       if (err) {\n          console.log(\"Failed to create column family: \" + params[0]);\n          console.log(err);\n       }\n       else {\n          console.log(\"Created column family: \" + params[0]);\n          callback();\n       }\n   });\n   con.shutdown();\n }\n //populate Data\n function populateCustomerData() {\n    var params = ['John','Infinity Dr, TX', 1];\n    updateCustomer(ksConOptions,params);\n    params = ['Tom','Fermat Ln, WA', 2];\n    updateCustomer(ksConOptions,params);\n }\n //update will also insert the record if none exists\n function updateCustomer(ksConOptions,params)\n {\n   var cql = 'UPDATE customers_cf SET custname=?,custaddress=? where custid=?';\n   var con = new pooledCon(ksConOptions);\n   con.execute(cql,params,function(err) {\n       if (err) console.log(err);\n       else console.log(\"Inserted customer : \" + params[0]);\n   });\n   con.shutdown();\n }\n //read the two rows inserted above\n function readCustomer(ksConOptions)\n {\n   var cql = 'SELECT * FROM customers_cf WHERE custid IN (1,2)';\n   var con = new pooledCon(ksConOptions);\n   con.execute(cql,[],function(err,rows) {\n       if (err)\n          console.log(err);\n       else\n          for (var i=0; i&lt;rows.length; i++)\n             console.log(JSON.stringify(rows[i]));\n     });\n    con.shutdown();\n }\n //exectue the code\n createKeyspace(createColumnFamily);\n readCustomer(ksConOptions)\n</pre></li>\n</ol><h2 id=\"conclusion\">Conclusion</h2>\n<p>Microsoft Azure is a flexible platform that allows the running of both Microsoft as well as open source software as demonstrated by this exercise. Highly available Cassandra clusters can be deployed on a single data center through the spreading of the cluster nodes across multiple fault domains. Cassandra clusters can also be deployed across multiple geographically distant Azure regions for disaster proof systems. Azure and Cassandra together enables the construction of highly scalable, highly available and disaster recoverable cloud services needed by today's internet scale services.  </p>\n<h2 id=\"references\">References</h2>\n<ul><li><a href=\"http://cassandra.apache.org\" data-linktype=\"external\">http://cassandra.apache.org</a></li>\n<li><a href=\"http://www.datastax.com\" data-linktype=\"external\">http://www.datastax.com</a></li>\n<li><a href=\"http://www.nodejs.org\" data-linktype=\"external\">http://www.nodejs.org</a></li>\n</ul>",
        "created_at": "2017-12-12T13:31:33+0000",
        "updated_at": "2017-12-24T11:20:25+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en-us",
        "reading_time": 33,
        "domain_name": "docs.microsoft.com",
        "preview_picture": "https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/media/cassandra-nodejs/cassandra-linux1.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/4950"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 35,
            "label": "docker",
            "slug": "docker"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 4947,
        "uid": null,
        "title": "calvinlfer/compose-cassandra-cluster",
        "url": "https://github.com/calvinlfer/compose-cassandra-cluster",
        "content": "<h3>\n      <svg aria-hidden=\"true\" class=\"octicon octicon-book\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"/>\n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\"><p>A <code>docker-compose</code> blueprint that describes a 3 node Cassandra cluster.\nIt only exposes important Cassandra ports on the seed node to the host\nmachine. Internally, all of the nodes will be a part of the same Docker\nnetwork and will form a cluster using that Docker network.</p>\n<h2><a href=\"https://github.com/calvinlfer/compose-cassandra-cluster#instructions\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-instructions\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"/></a>Instructions</h2>\n<p>In order to bring up the cluster:</p>\n<ul><li>Use <code>docker-compose up</code> to see the logs of all the containers</li>\n<li>Use <code>docker-compose up -d</code> if you want it to run in the foreground</li>\n</ul><p>In order to clean up the cluster, use <code>docker-compose down</code></p>\n<h2><a href=\"https://github.com/calvinlfer/compose-cassandra-cluster#notes\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-notes\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"/></a>Notes</h2>\n<p>You need to make sure that the Docker daemon has enough of resources\notherwise you will encounter exit code 137 (Out of Memory Killer) on\nyour containers.</p>\n<p>When you create a single node cluster and you try to add more nodes to\nthe cluster, you must add them one by one. This means that you cannot\nhave multiple nodes join the cluster (by pointing to the seed node) at\nthe same time. You must add a cluster, wait for it to join the ring and\nstabilize before you can begin to add another cluster. This is why you\nwill see an additional delay on start up between the non-seed nodes.\nIf you attempt to join a new node whilst stabilization has not yet been\nachieved, you will see an error like this:</p>\n<pre>ERROR [main] 2017-08-22 23:19:11,055 CassandraDaemon.java:706 - Exception encountered during startup\njava.lang.UnsupportedOperationException: Other bootstrapping/leaving/moving nodes detected, cannot bootstrap while cassandra.consistent.rangemovement is true\n</pre>\n<p>This <a href=\"http://thelastpickle.com/blog/2017/05/23/auto-bootstrapping-part1.html\" rel=\"nofollow\">article</a>\ndiscusses the implications of turning <code>cassandra.consistent.rangemovement</code>\noff.</p>\n<h3><a href=\"https://github.com/calvinlfer/compose-cassandra-cluster#credits\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-credits\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"/></a>Credits</h3>\n<ul><li><a href=\"http://thelastpickle.com/blog\" rel=\"nofollow\">The Last Pickle</a></li>\n</ul></article>",
        "created_at": "2017-12-12T16:47:35+0000",
        "updated_at": "2017-12-15T06:16:50+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://avatars3.githubusercontent.com/u/14280155?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/4947"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 35,
            "label": "docker",
            "slug": "docker"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 4946,
        "uid": null,
        "title": "how to create cassandra cluster on docker",
        "url": "https://www.google.com/search?q=how%20to%20create%20cassandra%20cluster%20on%20docker&oq=how%20to%20create%20cassandra%20cluster%20on%20docker&aqs=chrome..69i57j0l4.5377j0j4&sourceid=chrome&ie=UTF-8",
        "content": "how to create cassandra cluster on docker - Google Search<noscript><noscript><p>Please click <a href=\"https://www.google.com/search?q=how+to+create+cassandra+cluster+on+docker&amp;gbv=1&amp;sei=II0_WsKsMIKEmQGKqYnYBg\">here</a> if you are not redirected within a few seconds.</p></noscript><div><div class=\"gb_ud gb_Tc\" aria-hidden=\"true\"><p>We've detected you're using an older version of Chrome.<a class=\"gb_xd gb_ed\" role=\"button\" tabindex=\"0\">Reinstall to stay secure</a></p><p>×</p></div></div></noscript>",
        "created_at": "2017-12-12T16:48:05+0000",
        "updated_at": "2017-12-24T11:19:14+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "www.google.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/4946"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          }
        ],
        "is_public": false,
        "id": 4936,
        "uid": null,
        "title": "How to integrate Spark and Cassandra",
        "url": "http://www.youtube.com/oembed?url=https://www.youtube.com/watch?v=jpEABn80OCU&format=xml",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/jpEABn80OCU?feature=oembed\" frameborder=\"0\" gesture=\"media\" allow=\"encrypted-media\" allowfullscreen=\"\">[embedded content]</iframe>",
        "created_at": "2017-12-09T16:33:38+0000",
        "updated_at": "2017-12-15T12:41:47+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": "",
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/jpEABn80OCU/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/4936"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 4911,
        "uid": null,
        "title": "Relational Database to NoSQL",
        "url": "https://www.datastax.com/relational-database-to-nosql",
        "content": "<!--HID72901 NSH--><!-- Google Tag Manager (noscript)  - PART 2 of 2  -->\n<noscript/>\n<!-- End Google Tag Manager (noscript) -->\n<div id=\"DXDIV0\">\n    <!-- #Header -->\n\t<div id=\"Wrapper\"><!-- WrpprID1 -->\n\t\t<div class=\"wrapperContainer\">\n             \n    <div id=\"MainBody\">\n\t\t\t\t\t\t\t\t\n        <div id=\"MainChannel\" class=\"width-100 clearfix\">\n                                <div id=\"Content\" class=\" \">\n            \t\t\t\t\t\n\t\t\t\t\t\t<div id=\"ContentChannel\">\n\t\t\t<!-- https://www.datastax.com/relational-database-to-nosql -->\n<!--  cloud  -->\n<div class=\"dx_stlfntmobmod_pge\" id=\"dx_thispage_div1\">\n<!-- DXGS_NoSqlBanner_v3 START -->\n<!-- DXGS_NoSqlBanner_v3 END -->\n<div class=\"thismb2content1width1\" readability=\"6.4545454545455\">\n       <div class=\"dx_hnltstd_lt_div dx_marginvertical_approx68pxless\" readability=\"18.454545454546\">\n            \n            <div class=\"dxfnts_ds_f20l30 \" readability=\"29\">\nTechnology that can scale, perform and deliver continuous availability is the difference between today’s successful online applications and those that fail. Relational databases (RDBMS) have struggled to keep up with the wave of modernization, leading to the rise of NoSQL as the most viable database option for online Web and mobile applications.\n<p>The path to understanding whether a NoSQL technology like DataStax Enterprise is right for your business as either a complementary technology to an RDBMS or as a complete replacement is a three step approach.&#13;\n            </p></div>\n</div></div><!-- thismb2content1width1 -->\n<!-- dxcs_graybgdiv -->\n</div>\n\t\t\t\n</div> <!-- #ContentChannel -->\n</div> <!-- #MainChannel -->\n<div class=\"DXpgA2AclassV1\"><p>SHARE THIS PAGE</p></div>\n    \n    </div>\n\t <!-- #MainChannel -->\n\t\t\t</div> <!-- TBD IF - #MainBody -->\n\t\t</div> <!-- .wrapperContainer -->\n\t</div> <!-- #Wrapper -->\n              \n<!-- DXcom_footerKv2_div1 -->\n\t\n  <!-- Included JS Files (Compressed) -->\n    \n\t\n\t\n\t\n\t\n\t\n\t\n</div><!-- /DXDIV0 -->",
        "created_at": "2017-10-31T20:25:11+0000",
        "updated_at": "2017-12-15T13:03:05+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en-US",
        "reading_time": 0,
        "domain_name": "www.datastax.com",
        "preview_picture": "https://www.datastax.com/wp-content/themes/datastax-2014-08/images/common/DataStax_Web_Social_DefaultGenericV2_1024x351_wide.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/4911"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 4910,
        "uid": null,
        "title": "MySQL to Cassandra Migrations",
        "url": "https://academy.datastax.com/planet-cassandra/mysql-to-cassandra-migration",
        "content": "<p>For 15+ years, Oracle’s MySQL has been a de facto infrastructure piece in web applications, enjoying wide adoption. This is for good reason: MySQL provides a solid relational database that enables companies to build systems that perform well in many use cases. Yet, even its strongest supporters admit that it is not architected to tackle the new wave of big data applications. Modern businesses that need to manage big data use cases are turning to Apache Cassandra to replace MySQL.</p>\n<p>Migrating from MySQL to Cassandra: General Info</p>\n<h3>Is Cassandra Right for Your Application?</h3>\n<p>A new class of databases (sometimes referred to as “NoSQL”) have been developed and designed with 18+ years worth of lessons learned from traditional relational databases such as MySQL. Cassandra (and other distributed or “NoSQL” databases) aim to make the “right” tradeoffs to<br />ultimately deliver a database that provides the scalability, redundancy, and performance needed in todays applications. Although MySQL may have performed well for you in the past, new business requirements and/or the need to both scale and improve the reliability of your application might mean that MySQL is no longer the correct fit.</p>\n<p>Before committing any further time towards a MySQL to Cassandra migration, ask yourself:<br />“Is MySQL currently preventing development of new features or providing acceptable uptime, reliability, and scalability for my users?”</p>\n<p>“No”: Not only should you not migrate to Cassandra, but also you most likely should not be considering a migration to any alternative database. Migrating an application to a new database is a very difficult, time consuming, and error-prone process.</p>\n<p>“Yes”: Then hopefully you’ve found a helpful resource to help guide and plan your migration from MySQL to Cassandra. There are many databases<br />available, all with their various advantages, disadvantages and tradeoffs. This article is not an attempt to portray Cassandra as a perfect solution; in fact, Cassandra’s tradeoffs, advantages, and disadvantages will be highlighted. Hopefully this will help you make a decision that is both informed and educated; not one motivated by marketing hype or change for the sake of change.</p>\n<p>Don’t try to shove a square peg in a round hole!</p>\n<ul><li>Cassandra is not a relational database.</li>\n<li>Cassandra is not a 100%/“drop-in” replacement for MySQL.</li>\n<li>Simply migrating existing code to Cassandra without modifying and rethinking your existing data model will not result in perfect uptime or fix performance bottlenecks for your application. In fact, it might make things worse.</li>\n</ul><h3>Key Terminology</h3>\n<p>The following overview of Cassandra terminology provides descriptions and their MySQL equivalent. The goal is to introduce the most basic terms and concepts required to get a basic understanding of Cassandra. To read more on the key terms and architecture of Cassandra you can find more detail in the <a href=\"http://www.datastax.com/documentation/cassandra/2.0/cassandra/architecture/architectureIntro_c.html\">Cassandra architecture documentation</a> or for a higher level overview visit the “<a href=\"http://planetcassandra.org/what-is-apache-cassandra/\">What is Cassandra</a>” page on Planet Cassandra.</p>\n<p><img alt=\"\" src=\"https://academy.datastax.com/sites/default/files/keyterms-1.png\" /></p>\n<p><img alt=\"\" src=\"https://academy.datastax.com/sites/default/files/keyterms-2.png\" /></p>\n<p><img alt=\"\" src=\"https://academy.datastax.com/sites/default/files/keyterms-3.png\" /></p>\n<h3>How is Data Handled?</h3>\n<p>At a very high level, Cassandra operates by dividing all data evenly around a cluster of nodes, which can be visualized as aring. Nodes generally run on commodity hardware. Each Cassandra node in the cluster is responsible for and assigned a token range (which is essentially a range of hashes defined by a partitioner, which defaults to Murmur3Partitioner in Cassandra v1.2+). By default this hash range is defined with a maximum number of possible hash values ranging from 0 to 2^127-1.</p>\n<p>Each update or addition of data contains a unique row key (also known as a primary key). The primary key is hashed to determine a replica (or node) responsible for a token range inclusive of a given row key. The data is then stored in the cluster<strong>n</strong> times (where <strong>n</strong> is defined by the keyspace’s replication factor), or once on each replica responsible a given query’s row key. All nodes in Cassandra are peers and a client’s read or write request can be sent to any node in the cluster, regardless of whether or not that node actually contains and is responsible for the requested data. There is no concept of a master or slave, and nodes dynamically learn about each other and the state and health of other nodes thru the gossip protocol. A node that receives a client query is referred to as the coordinator for the client operation; it facilitates communication between all replica nodes responsible for the query (contacting at least n replica nodes to satisfy the query’s consistency level) and prepares and returns a result to the client.</p>\n<h3>Reads and Writes</h3>\n<p>Clients may interface with Cassandra for reads and writes via either the native binary protocol or Thrift. CQL queries can be made over both transports. As a general recommendation, if you are just getting started with Cassandra you should stick to the native binary protocol and CQL and ignore Thrift.</p>\n<p>When a client performs a read or write request, the coordinator node contacts the number of required replicas to satisfy the consistency level included with each request. For example, if a read request is processed using QUORUM consistency, and the keyspace was created with a “replication factor” of 3, 2 of the 3 replicas for the requested data would be contacted, their results merged, and a single result returned to the client. With write requests, the coordinator node will send a write requests with all mutated columns to all replica nodes for a given row key.</p>\n<h3>Processing a Local Update</h3>\n<p>When an update is processed – also known as a mutation — an entry is first added to the commit log, which ensures durability of the transaction. Next, it is also added to the memtable. A memtable is a bounded in memory write-back cache that contains recent writes which have not yet been flushed to an SSTable (a permanent, immutable, and serialized on disk copy of the tables data).</p>\n<p>When updates cause a memtable to reach it’s configured maximum in-memory size, the memtable is flushed to an immutable SSTable, persisting the data from the memtable permanently on disk while making room for future updates. In the event of a crash or node failure, events are replayed from the commit log, which prevents the loss of any data from memtables that had not been flushed to disk prior to an unexpected event such as a power outage or crash.</p>\n<h3>Distributed Computing</h3>\n<p>Distributed logic and designs will inevitably cause an increase in complexity in application logic. When done right however, the rewards are obvious and easy to appreciate. Operationally, while it might be possible to get away with a single non-sharded MySQL instance installed via apt-get/emerge/yum/etc., operations with Cassandra need to be taken seriously to achieve desired performance and uptime of the cluster. Or, if you currently shard data across multiple MySQL instances, knowing that Cassandra deals with sharding and replication for you might be a huge benefit and upsell for Cassandra. But, unfortunately there is no such thing as a free lunch. For example, although Cassandra will remove all of your homegrown database abstraction and sharding code, you ultimately ended simply moving that logic from your code to Cassandra. Luckily, given the number of people and corporations of all sizes using Cassandra in production combined with an engaged and involved community, it’s fair to assume and argue that Cassandra’s equivalent of your MySQL sharding code will be better than your old homegrown solution.</p>\n<h2>Development Considerations</h2>\n<h3>Be Thoughtful About Your Data Model</h3>\n<p>Creating a thoughtful and conscious data model in Cassandra from the very beginning is very important. A bad data model can easily ruin and erase any of the benefits you want by migrating to Cassandra in the first place. With MySQL, the lack of a thoughtful or poor data model can frequently be worked around and accommodated thanks to the various relational database features (for example, the use of complex JOINS).<br />While these MySQL queries might be slow and expensive, given enough time and resources it’s possible to get the exact desired result from the dataset. With Cassandra, it is much harder to retroactively “fix” a poor data model. First, the lack of JOINS in Cassandra removes complex reads as a hacked solution to a bad data model. Additionally, thanks to the power and architecture of Cassandra, it becomes very easy to store more rows and data than imaginable with MySQL. With increased amounts of data stored, comes an increased complexity in successfully getting the exact data needed within the given performance boundaries required by your application. A SELECT query containing only 30 rows will return quickly and predictably. Performing a query over 5 million rows requires processing significantly more IO. Just as more data in MySQL made complex JOINS more difficult, accommodating for a Cassandra data model that requires the iteration over multiple nodes and rows will be slow, inefficient, and most likely not work at all. Obviously, faster database responses are always better in any application; so don’t let your data model be the cause of slow database latency in your application!</p>\n<h3>Denormalization</h3>\n<p>Denormalization is the concept that a data model should be designed so that a given query can be served from the results from one row and query. Instead of doing multiple reads from multiple tables and rows to gather all the required data for a response, instead modify your application logic to insert the required data multiple times into every row that might need it in the future. This way, all required data can be available in just one read which prevents multiple lookups.</p>\n<h2>Operational Considerations</h2>\n<h3>Optimization and Tuning Cassandra</h3>\n<p>There are lots of options to tweak in Cassandra. Much like turning the treble, bass, and volume nobs of your car’s sound system all to 11 won’t sound very good to your ears, it’s easy to do more harm than good when “optimizing” Cassandra and it’s many nobs and dials.</p>\n<p>Options such as key cache and row cache are two great examples. In a MySQL world, much of the configuration tuning is spent on optimizing the various amounts of cache allocated. In the Cassandra world, these settings actually tend to decrease node and cluster stability. Cassandra is written in Java, and thus it must operate within the limitations of Java. One of the biggest considerations is Garbage Collection and the maximum size of the heap possible without running into large garbage collection related issues, which will crater the performance of Cassandra. As of JDK7 with CMS (the default in Cassandra 1.2.x and 2.0.x) the maximum recommended size of the heap is 8GB. This 8GB must be shared between all of the various Cassandra components. 2GB allocated to the key cache will (obviously) put another 2GB of pressure on the heap. Caches are an optimization not a requirement, so allocating more memory to caches should be considered as part of the big picture. If you can allocate the full 8GB to Cassandra, a suggestion would be to start with allocating no more than 768MB to the key cache (key_cache_size_in_mb) and 0MB to the row cache (row_cache_size_in_mb).</p>\n<p>Another example is multithreaded_compaction. While this might seem like an obvious option to enable, in most cases leaving this option disabled can actually improve overall cluster stability and performance. In many cases, less is more.</p>\n<h2>Migration Plan Considerations</h2>\n<h3>Maintaining Data Integrity</h3>\n<p>Sometimes the most difficult component of a migration is not in writing a set of reliable scripts to read from MySQL and insert into Cassandra, but trivial coding mistakes that can cause significant data discrepancies between the MySQL and Cassandra versions of the data.</p>\n<p>Because migrating from MySQL to Cassandra will most likely require a change in your data model, the logic required to “convert” your relational MySQL data to it’s de-normalized form is the hardest part of the migration and certainly has the biggest risk.</p>\n<p>Treat your migration scripts and logic not as one-off instances, but production quality code that can be run in any order, at any time. Mistakes in migration logic that result in an inconsistent version of the migrated data in Cassandra most likely will have a much greater impact than other dataset migration related bugs.</p>\n<h3>Get to Know Bulk Loading</h3>\n<p>Regardless of your migration strategy, in almost all cases you will have to perform an initial bulk import of your existing MySQL data into Cassandra. While it might be tempting to simply iterate over every MySQL result and then insert that result one mutation at a time into Cassandra, a more efficient way is to use the Cassandra Bulk Loader. At a high level, the Bulk Loader requires you to create a CSV file containing all of the rows and columns that need to be loaded into Cassandra. Using the Java class SSTableSimpleUnsortedWriter, you can create an SSTable from your CSV file, which can then be loaded directly into Cassandra using SSTableloader.</p>\n<p>For more details and code samples reference the article at <a href=\"http://www.datastax.com/dev/blog/bulk-loading\">http://www.datastax.com/dev/blog/bulk-loading</a></p>\n<h3>Migration Methods</h3>\n<p>Sync Data Method:<br />When migrating to Cassandra and choosing a new data model might significantly increase your database workload. Alternatively, you might still need a live dataset in MySQL after the initial migration for legacy scripts that have not yet been migrated to use Cassandra.</p>\n<p>Syncing from MySQL to Cassandra<br />In some cases it might not be practicable to add Cassandra to a legacy application. In this case it might be necessary to have an external process sync data from MySQL to Cassandra while running both new and old logic in parallel.</p>\n<p>Suggestion:<br />Add a timestamp column to the MySQL table to be synced. With each update to MySQL also update the timestamp with the last updated time. At a scheduled interval then do a SELECT query from all MySQL shards where the last updated timestamp is greater than or equal to the time your last sync started.</p>\n<p>Syncing from Cassandra back to MySQL<br />Some data models will be hard to sync from Cassandra back to MySQL (for example time series data). However, rows containing more de-normalized<br />“metadata”-like information can be synced.</p>\n<p>What won’t work: Creating a sync script that executes via cron every n minutes and attempts to do a SELECT * FROM TABLE from Cassandra (and<br />then update and insert all of those records into MySQL) is a recipe for failure. Inherent to Cassandra’s design is that data is sharded across multiple nodes by a hash of it’s key. Performing a SELECT * query is a Cassandra anti-pattern and should be avoided. Iterating through every key across all nodes and returning a single paged dataset is both inefficient and impractical.</p>\n<p>1st Suggestion:<br />Implement a queue that your application additionally writes to when it modifies a row in Cassandra. Have a script consume from this queue and de-duplicate the modified keys on a time interval and then bulk insert updates into MySQL.</p>\n<p>2nd Suggestion:<br />If the data can be updated less frequently into MySQL, you could write a Hadoop Map/Reduce job that iterates over the column families that you need to sync. This solution gives a practicable and reproducible way to iterate through all keys in a column family. Using this approach as an additional sanity option to resolve missed updates from other incremental sync options.</p>\n<p>3rd Suggestion:<br />Another option if you can afford a greater delay in the delta between updates from Cassandra back to MySQL is to use a tool such as SSTable2JSON to dump a column families SSTables into a JSON format, which can then be parsed and then used to update MySQL. This is a pretty heavy-handed method. Additionally, you’ll have to write logic to ensure you dump the SSTables from all nodes to get the entire column family.</p>\n<p>Write Twice and Forget Method:<br />If you are able to modify your existing application to also interface with Cassandra, you can initially start your migration by writing database updates twice, once to MySQL and an additional time to Cassandra. Once you have all new updates being written to both MySQL and Cassandra, you can run a migration script that pages through all your existing MySQL data and inserts those records into Cassandra.</p>\n<p>Initially, you might want to implement this second write to Cassandra as a completely non-blocking, write and forget, operation. If you experience initial issues during your Cassandra deployment, make sure not to impact your existing application when Cassandra is down.</p>\n<p>Once you are satisfied with the fire-and-forget writes, you can slowly modify your application logic to start performing reads from Cassandra instead of MySQL. Thanks to the dual writes, if you run into issues, simply revert back to doing reads from MySQL.</p>\n<h2>Use Cases and Migration Resources</h2>\n<h3>Use Cases</h3>\n<p><a href=\"http://planetcassandra.org/blog/post/youve-got-scale-aol-migrates-from-mysql-to-apache-cassandra-for-8x-improvement/\">AOL</a><br />AOL migrated their article index, in use for several AOL technologies form MySQL. The result was an 8X increase in writes, and considering the move to Cassandra as a “big win”.</p>\n<p><a href=\"http://planetcassandra.org/blog/interview/coursera-migrates-to-the-top-of-the-class-moves-to-cassandra-for-an-always-on-on-demand-classroom/\">Coursera</a></p>\n<p>Coursera was experiencing unexpected downtime, due to the RDBMS’ single point of failure.  In addition, Cassandra has enabled Coursera to become more dynamic; introducing their over 9 million users to an always available, on-demand course system.</p>\n<p><a href=\"http://www.datastax.com/wp-content/uploads/2011/06/DataStax-CaseStudy-Mahalo.pdf\">Mahalo</a><br />Mahalo’s search technology was forced to move off of MySQL to Cassandra as their primary data store in order to realize lower costs and higher performance and scalability.</p>\n<p><a href=\"http://planetcassandra.org/blog/post/scaling-in-the-cloud-with-cassandra-at-pantheon\">Pantheon Systems</a><br />Pantheon Systems, offering a platform for Drupal websites in the cloud, migrated to Cassandra primarily for greater scalability and ease of use.</p>\n<p><a href=\"http://planetcassandra.org/blog/post/scoopit-turns-to-apache-cassandra-the-latest-and-best-technology-when-mysql-fails-to-keep-up\">Scoop.it</a><br />Scoop.it’s content curation publishing platform experienced the limitations of MySQL for handling their data growth and moved to Apache Cassandra for scalability and requirement of no downtime.</p>\n<p><a href=\"http://planetcassandra.org/blog/post/ampushs-migration-from-mysql-to-cassandra-for-data-volume-high-availability-and-performance\">Ampush</a><br />Ampush’s migration from MySQL to Cassandra due to their increase in data volume, high availability and performance requirements which only Cassandra could satisfy.</p>\n<p><a href=\"http://planetcassandra.org/blog/post/barracuda-networks-and-cassandra---battling-the-zombies\">Barracuda Networks</a><br />Barracudna Networks were not able to monitor customer threats in real-time with MySQL and went to Cassandra for the scalability and availability benefits.</p>\n<p><a href=\"http://planetcassandra.org/blog/post/cassandra-summit-2013-cabs-cassandra-and-hailo-mysql-to-cassandra-by-dave-gardner\">Hailo</a><br />Hailo has leveraged Cassandra to build one of the most successful startups in European history. This presentation looks at how Hailo grew from a simple MySQL-backed infrastructure to a resilient Cassandra-backed system running in three data centers globally.</p>\n<p><a href=\"http://www.datastax.com/wp-content/uploads/2011/04/DataStax-CS-Ooyala.pdf\">Ooyala</a><br />Ooyala chose Apache Cassandra for its elastic scalability and high performance – especially when their MySQL environment was not meeting customer service levels – to help their customers take a more strategic approach when delivering a digital video experience.</p>\n<p><a href=\"http://planetcassandra.org/blog/post/appssavvy-fixes-mysql-scalability-by-switching-to-apache-cassandra\">AppsSavvy</a><br />AppsSavvy’s targeted advertising delivery solution moved from MySQL to Cassandra for increased scalability and performance under load.</p>\n<p><a href=\"http://planetcassandra.org/blog/post/dating-site-zoosk-breaks-up-with-mysql-migrates-to-apache-cassandra-for-persistent-notifications\">Zoosk</a><br />Zoosk’s persistent notification system was moved off of MySQL and onto Apache Cassandra because it is a superior database for their high volume of writes of time series data.</p>\n<p><a href=\"http://planetcassandra.org/blog/post/agentis-energy-stores-over-15-billion-records-of-time-series-usage-data-in-apache-cassandra\">Agentis</a><br />Agentis Energy had to move to Cassandra once the scale of their data became unmanageable on MySQL as they now store over 15 billion records of time series usage energy usage data.</p>\n<h3>Migration Resources</h3>\n<p>Whitepaper: <a href=\"http://www.datastax.com/wp-content/uploads/2012/08/WP-DataStax-MySQLtoCassandra.pdf\">Why Migrate From MySQL to Cassandra?</a> By Robin Schumacher<br />This whitepaper discusses the ‘why’ and ‘how’ to migrate from MySQL to Cassandra as well as what a good migration candidate looks like.</p>\n<p>Hindsight is 20/20: <a href=\"http://www.youtube.com/watch?v=gW4jEOKRB04\" target=\"_blank\">MySQL to Cassandra</a>. This webinar offers a brief intro to how Barracuda Networks uses Cassandra and the ways in which they are replacing their MySQL infrastructure, with Cassandra including lessons learned. A slideshare from this presentation is available as well: <a href=\"http://www.slideshare.net/planetcassandra/c-summit-2013-hindsight-is-2020-mysql-to-cassandra-by-michael-kjellman\">Hindsight is 20/20: MySQL to Cassandra</a></p>\n<p>5 lessons learned by Zoosk for <a href=\"https://about.zoosk.com/en/engineering-blog/moving-persistent-notifications-from-mysql-to-cassandra/\" target=\"_blank\">moving persistent notifications from MySQL to Apache Cassandra</a> in order to support very high volumes of write while minimizing write latency.</p>\n<h3>About the Author</h3>\n<p>Michael Kjellman is a San Francisco based Software Engineer. Michael works across multiple products, technologies, and languages. He primarily works on Barracuda’s spam infrastructure and web filter classification data. Follow him on Twitter at<a href=\"https://twitter.com/mkjellman\">@mkjellman</a>.</p>",
        "created_at": "2017-10-31T20:25:15+0000",
        "updated_at": "2019-01-25T04:24:28+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "",
        "reading_time": 16,
        "domain_name": "academy.datastax.com",
        "preview_picture": "https://academy.datastax.com/sites/default/files/keyterms-1.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/4910"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1135,
            "label": "packages",
            "slug": "packages"
          },
          {
            "id": 1163,
            "label": "libraries",
            "slug": "libraries"
          }
        ],
        "is_public": true,
        "id": 4909,
        "uid": "5b57831b36c6b3.90061531",
        "title": "DataStax Java Driver",
        "url": "https://github.com/datastax/java-driver",
        "content": "<p><a href=\"https://travis-ci.org/datastax/java-driver\"><img src=\"https://camo.githubusercontent.com/8c0a0df4c133ce530d31b6639c743655b23f2fd2/68747470733a2f2f7472617669732d63692e6f72672f64617461737461782f6a6176612d6472697665722e7376673f6272616e63683d332e78\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/datastax/java-driver.svg?branch=3.x\" /></a></p><p><em>If you're reading this on github.com, please note that this is the readme\nfor the development version and that some features described here might\nnot yet have been released. You can find the documentation for latest\nversion through <a href=\"http://datastax.github.io/java-driver/\">Java driver\ndocs</a> or via the release tags,\n<a href=\"https://github.com/datastax/java-driver/tree/3.3.0\">e.g.\n3.3.0</a>.</em></p><p>A modern, <a href=\"https://github.com/datastax/java-driver/blob/3.x/manual\">feature-rich</a> and highly tunable Java client\nlibrary for Apache Cassandra (1.2+) and DataStax Enterprise (3.1+) using\nexclusively Cassandra's binary protocol and Cassandra Query Language v3.</p><p><strong>Features:</strong></p><p>The driver architecture is based on layers. At the bottom lies the driver core.\nThis core handles everything related to the connections to a Cassandra\ncluster (for example, connection pool, discovering new nodes, etc.) and exposes a simple,\nrelatively low-level API on top of which higher level layers can be built.</p><p>The driver contains the following modules:</p><ul><li>driver-core: the core layer.</li>\n<li>driver-mapping: the object mapper.</li>\n<li>driver-extras: optional features for the Java driver.</li>\n<li>driver-examples: example applications using the other modules which are\nonly meant for demonstration purposes.</li>\n<li>driver-tests: tests for the java-driver.</li>\n</ul><p><strong>Useful links:</strong></p><ul><li>JIRA (bug tracking): <a href=\"https://datastax-oss.atlassian.net/browse/JAVA\">https://datastax-oss.atlassian.net/browse/JAVA</a></li>\n<li>MAILING LIST: <a href=\"https://groups.google.com/a/lists.datastax.com/forum/#!forum/java-driver-user\">https://groups.google.com/a/lists.datastax.com/forum/#!forum/java-driver-user</a></li>\n<li>IRC: #datastax-drivers on <a href=\"http://freenode.net\">irc.freenode.net</a></li>\n<li>TWITTER: <a href=\"https://twitter.com/dsJavaDriver\">@dsJavaDriver</a> tweets Java\ndriver releases and important announcements (low frequency).\n<a href=\"https://twitter.com/datastaxeng\">@DataStaxEng</a> has more news including\nother drivers, Cassandra, and DSE.</li>\n<li>DOCS: the <a href=\"http://docs.datastax.com/en/developer/java-driver/3.2/manual/\">manual</a> has quick\nstart material and technical details about the driver and its features.</li>\n<li>API: <a href=\"http://www.datastax.com/drivers/java/3.2\">http://www.datastax.com/drivers/java/3.2</a></li>\n<li><a href=\"https://github.com/datastax/java-driver/blob/3.x/changelog\">changelog</a></li>\n<li><a href=\"http://downloads.datastax.com/java-driver/cassandra-java-driver-3.3.0.tar.gz\">binary tarball</a></li>\n</ul><p><strong>Feeback requested:</strong> help us focus our efforts, provide your input on the <a href=\"http://goo.gl/forms/qwUE6qnL7U\">Platform and Runtime Survey</a> (we kept it short).</p><h2><a href=\"https://github.com/datastax/java-driver#getting-the-driver\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-getting-the-driver\"></a>Getting the driver</h2><p>The last release of the driver is available on Maven Central. You can install\nit in your application using the following Maven dependency:</p><div class=\"highlight highlight-text-xml\"><pre>&lt;dependency&gt;\n  &lt;groupId&gt;com.datastax.cassandra&lt;/groupId&gt;\n  &lt;artifactId&gt;cassandra-driver-core&lt;/artifactId&gt;\n  &lt;version&gt;3.3.0&lt;/version&gt;\n&lt;/dependency&gt;</pre></div><p>Note that the object mapper is published as a separate artifact:</p><div class=\"highlight highlight-text-xml\"><pre>&lt;dependency&gt;\n  &lt;groupId&gt;com.datastax.cassandra&lt;/groupId&gt;\n  &lt;artifactId&gt;cassandra-driver-mapping&lt;/artifactId&gt;\n  &lt;version&gt;3.3.0&lt;/version&gt;\n&lt;/dependency&gt;</pre></div><p>The 'extras' module is also published as a separate artifact:</p><div class=\"highlight highlight-text-xml\"><pre>&lt;dependency&gt;\n  &lt;groupId&gt;com.datastax.cassandra&lt;/groupId&gt;\n  &lt;artifactId&gt;cassandra-driver-extras&lt;/artifactId&gt;\n  &lt;version&gt;3.3.0&lt;/version&gt;\n&lt;/dependency&gt;</pre></div><p>We also provide a <a href=\"https://github.com/datastax/java-driver/blob/3.x/manual/shaded_jar\">shaded JAR</a>\nto avoid the explicit dependency to Netty.</p><p>If you can't use a dependency management tool, a\n<a href=\"http://downloads.datastax.com/java-driver/cassandra-java-driver-3.3.0.tar.gz\">binary tarball</a>\nis available for download.</p><h2><a href=\"https://github.com/datastax/java-driver#compatibility\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-compatibility\"></a>Compatibility</h2><p>The Java client driver 3.3.0 (<a href=\"https://github.com/datastax/java-driver/tree/3.x\">branch 3.x</a>) is compatible with Apache\nCassandra 1.2, 2.0, 2.1, 2.2 and 3.0 (see <a href=\"http://datastax.github.io/java-driver/manual/native_protocol\">this page</a> for\nthe most up-to-date compatibility information).</p><p>UDT and tuple support is available only when using Apache Cassandra 2.1 or higher (see <a href=\"http://www.datastax.com/dev/blog/cql-in-2-1\">CQL improvements in Cassandra 2.1</a>).</p><p>Other features are available only when using Apache Cassandra 2.0 or higher (e.g. result set paging,\n<a href=\"https://github.com/datastax/java-driver/blob/3.x/driver-core/src/main/java/com/datastax/driver/core/BatchStatement.java\">BatchStatement</a>,\n<a href=\"http://www.datastax.com/documentation/cql/3.1/cql/cql_using/use_ltweight_transaction_t.html\">lightweight transactions</a>\n-- see <a href=\"http://www.datastax.com/documentation/cassandra/2.0/cassandra/features/features_key_c.html\">What's new in Cassandra 2.0</a>).\nTrying to use these with a cluster running Cassandra 1.2 will result in\nan <a href=\"https://github.com/datastax/java-driver/blob/3.x/driver-core/src/main/java/com/datastax/driver/core/exceptions/UnsupportedFeatureException.java\">UnsupportedFeatureException</a> being thrown.</p><p><strong>Note</strong>: DataStax products do not support big-endian systems.</p><h2><a href=\"https://github.com/datastax/java-driver#upgrading-from-previous-versions\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-upgrading-from-previous-versions\"></a>Upgrading from previous versions</h2><p>If you are upgrading from a previous version of the driver, be sure to have a look at\nthe <a href=\"https://github.com/datastax/java-driver/blob/3.x/upgrade_guide\">upgrade guide</a>.</p><h3><a href=\"https://github.com/datastax/java-driver#troubleshooting\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-troubleshooting\"></a>Troubleshooting</h3><p>If you are having issues connecting to the cluster (seeing <code>NoHostAvailableConnection</code> exceptions) please check the\n<a href=\"https://github.com/datastax/java-driver/wiki/Connection-requirements\">connection requirements</a>.</p><h2><a href=\"https://github.com/datastax/java-driver#license\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-license\"></a>License</h2><p>Copyright 2012-2015, DataStax</p><p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p><p><a href=\"http://www.apache.org/licenses/LICENSE-2.0\">http://www.apache.org/licenses/LICENSE-2.0</a></p><p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>",
        "created_at": "2017-10-31T20:25:20+0000",
        "updated_at": "2018-07-24T19:50:51+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "github.com",
        "preview_picture": "https://camo.githubusercontent.com/8c0a0df4c133ce530d31b6639c743655b23f2fd2/68747470733a2f2f7472617669732d63692e6f72672f64617461737461782f6a6176612d6472697665722e7376673f6272616e63683d332e78",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/4909"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1135,
            "label": "packages",
            "slug": "packages"
          },
          {
            "id": 1163,
            "label": "libraries",
            "slug": "libraries"
          }
        ],
        "is_public": true,
        "id": 4908,
        "uid": "5b57831dd364f2.38156398",
        "title": "DataStax C++ Driver",
        "url": "https://github.com/datastax/cpp-driver",
        "content": "<p><a href=\"https://travis-ci.org/datastax/cpp-driver\"><img src=\"https://camo.githubusercontent.com/57fca1f7ce88e7a0e7110393ac2ef16d0304a6e9/68747470733a2f2f7472617669732d63692e6f72672f64617461737461782f6370702d6472697665722e7376673f6272616e63683d6d6173746572\" alt=\"Build Status: Linux\" data-canonical-src=\"https://travis-ci.org/datastax/cpp-driver.svg?branch=master\" /></a>\n<a href=\"https://ci.appveyor.com/project/DataStax/cpp-driver\"><img src=\"https://camo.githubusercontent.com/1200623e2f8525a13948f2cdb42f09cce61e386c/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f656330783076756b35396173323872362f6272616e63682f6d61737465723f7376673d74727565\" alt=\"Build Status: Windows\" data-canonical-src=\"https://ci.appveyor.com/api/projects/status/ec0x0vuk59as28r6/branch/master?svg=true\" /></a></p><p>A modern, <a href=\"https://github.com/datastax/cpp-driver#features\">feature-rich</a>, and highly tunable C/C++ client library for\nApache Cassandra (1.2+) and DataStax Enterprise (3.1+) using exclusively\nCassandra's native protocol and Cassandra Query Language v3.</p><ul><li>Code: <a href=\"https://github.com/datastax/cpp-driver\">https://github.com/datastax/cpp-driver</a></li>\n<li>Binaries: <a href=\"http://downloads.datastax.com/cpp-driver/\">http://downloads.datastax.com/cpp-driver/</a></li>\n<li>Docs: <a href=\"http://datastax.github.io/cpp-driver\">http://datastax.github.io/cpp-driver</a></li>\n<li>JIRA: <a href=\"https://datastax-oss.atlassian.net/browse/CPP\">https://datastax-oss.atlassian.net/browse/CPP</a></li>\n<li>Mailing List: <a href=\"https://groups.google.com/a/lists.datastax.com/forum/#!forum/cpp-driver-user\">https://groups.google.com/a/lists.datastax.com/forum/#!forum/cpp-driver-user</a></li>\n<li>IRC: <a href=\"http://webchat.freenode.net/?channels=datastax-drivers\">#datastax-drivers on <code>irc.freenode.net &lt;http://freenode.net&gt;</code></a></li>\n</ul><p><strong>Note</strong>: DataStax products do not support big-endian systems.</p><h2><a href=\"https://github.com/datastax/cpp-driver#whats-new-in-2526\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-whats-new-in-2526\"></a>What's New in 2.5/2.6</h2><ul><li>Support for <a href=\"https://github.com/datastax/cpp-driver/tree/2.6.0/examples/duration/duration.c\"><code>duration</code></a></li>\n<li><a href=\"http://datastax.github.io/cpp-driver/topics/configuration/#speculative-execution\">Speculative execution</a></li>\n<li><a href=\"http://datastax.github.io/cpp-driver/topics/configuration/#query-idempotence\">Idempotent statements</a></li>\n<li><a href=\"http://datastax.github.io/cpp-driver/topics/security/ssl/\">SSL</a> can be enabled without re-initializing the underlying library (e.g. OpenSSL)</li>\n</ul><p>More information about features included in 2.3 can be found in this <a href=\"http://www.datastax.com/dev/blog/datastax-c-driver-2-3-ga-released\">blog\npost</a>.</p><h2><a href=\"https://github.com/datastax/cpp-driver#upgrading-from-20-or-21-to-22\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-upgrading-from-20-or-21-to-22\"></a>Upgrading from 2.0 or 2.1 to 2.2+</h2><p>The new schema metadata API in 2.2 required some breaking API changes.\nApplications that used the previous schema metadata API from 2.0 and 2.1 will\nrequire some small modifications to use the new API. More information about the\nnew schema metadata API can be found in this\n<a href=\"http://www.datastax.com/dev/blog/datastax-c-driver-2-2-ga-released\">blog post</a>.</p><h2><a href=\"https://github.com/datastax/cpp-driver#upgrading-from-10-to-20\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-upgrading-from-10-to-20\"></a>Upgrading from 1.0 to 2.0+</h2><p>There were a couple breaking API changes between 1.0 and 2.0 that are documented\n<a href=\"http://www.datastax.com/dev/blog/datastax-c-driver-2-0-released\">here</a>.</p><h2><a href=\"https://github.com/datastax/cpp-driver#features\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-features\"></a>Features</h2><ul><li><a href=\"http://datastax.github.io/cpp-driver/topics/#futures\">Asynchronous API</a></li>\n<li><a href=\"http://datastax.github.io/cpp-driver/topics/#executing-queries\">Simple</a>, <a href=\"http://datastax.github.io/cpp-driver/topics/basics/prepared_statements/\">Prepared</a>, and <a href=\"http://datastax.github.io/cpp-driver/topics/basics/batches/\">Batch</a> statements</li>\n<li><a href=\"http://datastax.github.io/cpp-driver/topics/#asynchronous-i-o\">Asynchronous I/O</a>, <a href=\"http://datastax.github.io/cpp-driver/topics/#thread-safety\">parallel execution</a>, and request pipelining</li>\n<li>Connection pooling</li>\n<li>Automatic node discovery</li>\n<li>Automatic reconnection</li>\n<li>Configurable <a href=\"http://datastax.github.io/cpp-driver/topics/configuration/#load-balancing\">load balancing</a></li>\n<li>Works with any cluster size</li>\n<li><a href=\"http://datastax.github.io/cpp-driver/topics/security/#authentication\">Authentication</a></li>\n<li><a href=\"http://datastax.github.io/cpp-driver/topics/security/ssl/\">SSL</a></li>\n<li><a href=\"http://datastax.github.io/cpp-driver/topics/configuration/#latency-aware-routing\">Latency-aware routing</a></li>\n<li><a href=\"http://datastax.github.io/cpp-driver/topics/metrics/\">Performance metrics</a></li>\n<li><a href=\"http://datastax.github.io/cpp-driver/topics/basics/tuples/\">Tuples</a> and <a href=\"http://datastax.github.io/cpp-driver/topics/basics/user_defined_types/\">UDTs</a></li>\n<li><a href=\"http://datastax.github.io/cpp-driver/topics/basics/binding_parameters/#nested-collections\">Nested collections</a></li>\n<li><a href=\"http://datastax.github.io/cpp-driver/topics/configuration/retry_policies/\">Retry policies</a></li>\n<li><a href=\"http://datastax.github.io/cpp-driver/topics/basics/client_side_timestamps/\">Client-side timestamps</a></li>\n<li><a href=\"http://datastax.github.io/cpp-driver/topics/basics/data_types/\">Data types</a></li>\n<li><a href=\"http://datastax.github.io/cpp-driver/topics/configuration/#connection-heartbeats\">Idle connection heartbeats</a></li>\n<li>Support for materialized view and secondary index metadata</li>\n<li>Support for clustering key order, <code>frozen&lt;&gt;</code> and Cassandra version metadata</li>\n<li><a href=\"http://datastax.github.io/cpp-driver/topics/configuration/#blacklist\">Blacklist</a>, <a href=\"http://datastax.github.io/cpp-driver/topics/configuration/#datacenter\">whitelist DC</a>, and <a href=\"http://datastax.github.io/cpp-driver/topics/configuration/#datacenter\">blacklist DC</a> load balancing policies</li>\n<li><a href=\"http://datastax.github.io/cpp-driver/topics/security/#custom\">Custom</a> authenticators</li>\n<li><a href=\"http://datastax.github.io/cpp-driver/topics/security/ssl/#enabling-cassandra-identity-verification\">Reverse DNS</a> with SSL peer identity verification support</li>\n<li>Randomized contact points</li>\n</ul><h2><a href=\"https://github.com/datastax/cpp-driver#compatibility\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-compatibility\"></a>Compatibility</h2><p>This release is compatible with Apache Cassandra 1.2, 2.0, 2.1, 2.2 and 3.0.</p><p>A complete compatibility matrix for both Apache Cassandra and DataStax\nEnterprise can be found\n<a href=\"https://docs.datastax.com/en/developer/driver-matrix/doc/common/driverMatrix.html?scroll=driverMatrix__cpp-driver-matrix\">here</a>.</p><h2><a href=\"https://github.com/datastax/cpp-driver#installation\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-installation\"></a>Installation</h2><p>Binary packages are <a href=\"http://downloads.datastax.com/cpp-driver/\">available</a> for\nCentOS, Ubuntu and Windows. Packages for the driver's dependencies, libuv (1.x)\nand OpenSSL, are also provided under the <code>dependencies</code> directory for each\nplatform e.g. <a href=\"http://downloads.datastax.com/cpp-driver/centos/7/dependencies/\">CentOS 7</a>,\n<a href=\"http://downloads.datastax.com/cpp-driver/ubuntu/14.04/dependencies/\">Ubuntu 14.04</a>,\n<a href=\"http://downloads.datastax.com/cpp-driver/windows/dependencies/\">Windows</a>.</p><p><em>Note</em>: CentOS and Ubuntu use the version of OpenSSL provided with the\ndistribution.</p><p>The driver can also be <a href=\"http://datastax.github.io/cpp-driver/topics/building/\">built from\nsource</a>.</p><h2><a href=\"https://github.com/datastax/cpp-driver#feedback-requested\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-feedback-requested\"></a>Feedback Requested</h2><p><strong>Help us focus our efforts!</strong> <a href=\"http://goo.gl/forms/ihKC5uEQr6\">Provide your input</a> on the C/C++ Driver Platform and Runtime Survey (we kept it short).</p><h2><a href=\"https://github.com/datastax/cpp-driver#examples\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-examples\"></a>Examples</h2><p>There are several examples provided here: <a href=\"https://github.com/datastax/cpp-driver/tree/1.0/examples\">examples</a>.</p><h2><a href=\"https://github.com/datastax/cpp-driver#a-simple-example\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-a-simple-example\"></a>A Simple Example</h2><div class=\"highlight highlight-source-c\"><pre>#include &lt;cassandra.h&gt;\n#include &lt;stdio.h&gt;\nint main(int argc, char* argv[]) {\n  /* Setup and connect to cluster */\n  CassFuture* connect_future = NULL;\n  CassCluster* cluster = cass_cluster_new();\n  CassSession* session = cass_session_new();\n  char* hosts = \"127.0.0.1\";\n  if (argc &gt; 1) {\n    hosts = argv[1];\n  }\n  /* Add contact points */\n  cass_cluster_set_contact_points(cluster, hosts);\n  /* Provide the cluster object as configuration to connect the session */\n  connect_future = cass_session_connect(session, cluster);\n  if (cass_future_error_code(connect_future) == CASS_OK) {\n    CassFuture* close_future = NULL;\n    /* Build statement and execute query */\n    const char* query = \"SELECT release_version FROM system.local\";\n    CassStatement* statement = cass_statement_new(query, 0);\n    CassFuture* result_future = cass_session_execute(session, statement);\n    if (cass_future_error_code(result_future) == CASS_OK) {\n      /* Retrieve result set and get the first row */\n      const CassResult* result = cass_future_get_result(result_future);\n      const CassRow* row = cass_result_first_row(result);\n      if (row) {\n        const CassValue* value = cass_row_get_column_by_name(row, \"release_version\");\n        const char* release_version;\n        size_t release_version_length;\n        cass_value_get_string(value, &amp;release_version, &amp;release_version_length);\n        printf(\"release_version: '%.*s'\\n\", (int)release_version_length, release_version);\n      }\n      cass_result_free(result);\n    } else {\n      /* Handle error */\n      const char* message;\n      size_t message_length;\n      cass_future_error_message(result_future, &amp;message, &amp;message_length);\n      fprintf(stderr, \"Unable to run query: '%.*s'\\n\", (int)message_length, message);\n    }\n    cass_statement_free(statement);\n    cass_future_free(result_future);\n    /* Close the session */\n    close_future = cass_session_close(session);\n    cass_future_wait(close_future);\n    cass_future_free(close_future);\n  } else {\n    /* Handle error */\n    const char* message;\n    size_t message_length;\n    cass_future_error_message(connect_future, &amp;message, &amp;message_length);\n    fprintf(stderr, \"Unable to connect: '%.*s'\\n\", (int)message_length, message);\n  }\n  cass_future_free(connect_future);\n  cass_cluster_free(cluster);\n  cass_session_free(session);\n  return 0;\n}</pre></div><h2><a href=\"https://github.com/datastax/cpp-driver#license\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-license\"></a>License</h2><p>Copyright (c) 2014-2016 DataStax</p><p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p><p><a href=\"http://www.apache.org/licenses/LICENSE-2.0\">http://www.apache.org/licenses/LICENSE-2.0</a></p><p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>",
        "created_at": "2017-10-31T20:25:24+0000",
        "updated_at": "2018-07-24T19:50:53+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 4,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/573369?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/4908"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 200,
            "label": "python",
            "slug": "python"
          },
          {
            "id": 1135,
            "label": "packages",
            "slug": "packages"
          },
          {
            "id": 1163,
            "label": "libraries",
            "slug": "libraries"
          }
        ],
        "is_public": true,
        "id": 4907,
        "uid": "5b578320ae1110.61939383",
        "title": "DataStax Python Driver",
        "url": "https://github.com/datastax/python-driver",
        "content": "<a href=\"https://travis-ci.org/datastax/python-driver\"><img alt=\"https://travis-ci.org/datastax/python-driver.png?branch=master\" src=\"https://camo.githubusercontent.com/f127f299b7b441d923b99ecfef34bdfc72d7fb63/68747470733a2f2f7472617669732d63692e6f72672f64617461737461782f707974686f6e2d6472697665722e706e673f6272616e63683d6d6173746572\" data-canonical-src=\"https://travis-ci.org/datastax/python-driver.png?branch=master\" /></a><p>A modern, <a href=\"https://github.com/datastax/python-driver#features\">feature-rich</a> and highly-tunable Python client library for Apache Cassandra (2.1+) using exclusively Cassandra's binary protocol and Cassandra Query Language v3.</p><p>The driver supports Python 2.7, 3.3, 3.4, 3.5, and 3.6.</p><p>If you require compatibility with DataStax Enterprise, use the <a href=\"http://docs.datastax.com/en/developer/python-dse-driver/\">DataStax Enterprise Python Driver</a>.</p><p><strong>Note:</strong> DataStax products do not support big-endian systems.</p><h2><a href=\"https://github.com/datastax/python-driver#feedback-requested\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-feedback-requested\"></a>Feedback Requested</h2><p><strong>Help us focus our efforts!</strong> Provide your input on the <a href=\"https://docs.google.com/a/datastax.com/forms/d/10wkbKLqmqs91gvhFW5u43y60pg_geZDolVNrxfO5_48/viewform\">Platform and Runtime Survey</a> (we kept it short).</p><h2><a href=\"https://github.com/datastax/python-driver#features\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-features\"></a>Features</h2><ul><li><a href=\"http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Session.execute\">Synchronous</a> and <a href=\"http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Session.execute_async\">Asynchronous</a> APIs</li>\n<li><a href=\"http://datastax.github.io/python-driver/api/cassandra/query.html#cassandra.query.Statement\">Simple, Prepared, and Batch statements</a></li>\n<li>Asynchronous IO, parallel execution, request pipelining</li>\n<li><a href=\"http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.get_core_connections_per_host\">Connection pooling</a></li>\n<li>Automatic node discovery</li>\n<li><a href=\"http://datastax.github.io/python-driver/api/cassandra/policies.html#reconnecting-to-dead-hosts\">Automatic reconnection</a></li>\n<li>Configurable <a href=\"http://datastax.github.io/python-driver/api/cassandra/policies.html#load-balancing\">load balancing</a> and <a href=\"http://datastax.github.io/python-driver/api/cassandra/policies.html#retrying-failed-operations\">retry policies</a></li>\n<li><a href=\"http://datastax.github.io/python-driver/api/cassandra/concurrent.html\">Concurrent execution utilities</a></li>\n<li><a href=\"http://datastax.github.io/python-driver/object_mapper.html\">Object mapper</a></li>\n</ul><h2><a href=\"https://github.com/datastax/python-driver#installation\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-installation\"></a>Installation</h2><p>Installation through pip is recommended:</p><pre>$ pip install cassandra-driver\n</pre><p>For more complete installation instructions, see the\n<a href=\"http://datastax.github.io/python-driver/installation.html\">installation guide</a>.</p><h2><a href=\"https://github.com/datastax/python-driver#documentation\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-documentation\"></a>Documentation</h2><p>The documentation can be found online <a href=\"http://datastax.github.io/python-driver/index.html\">here</a>.</p><p>A couple of links for getting up to speed:</p><ul><li><a href=\"http://datastax.github.io/python-driver/installation.html\">Installation</a></li>\n<li><a href=\"http://datastax.github.io/python-driver/getting_started.html\">Getting started guide</a></li>\n<li><a href=\"http://datastax.github.io/python-driver/api/index.html\">API docs</a></li>\n<li><a href=\"http://datastax.github.io/python-driver/performance.html\">Performance tips</a></li>\n</ul><h2><a href=\"https://github.com/datastax/python-driver#object-mapper\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-object-mapper\"></a>Object Mapper</h2><p>cqlengine (originally developed by Blake Eggleston and Jon Haddad, with contributions from the\ncommunity) is now maintained as an integral part of this package. Refer to\n<a href=\"http://datastax.github.io/python-driver/object_mapper.html\">documentation here</a>.</p><h2><a href=\"https://github.com/datastax/python-driver#contributing\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-contributing\"></a>Contributing</h2><p>See <a href=\"https://github.com/datastax/python-driver/blob/master/CONTRIBUTING.rst\">CONTRIBUTING.md</a>.</p><h2><a href=\"https://github.com/datastax/python-driver#reporting-problems\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-reporting-problems\"></a>Reporting Problems</h2><p>Please report any bugs and make any feature requests on the\n<a href=\"https://datastax-oss.atlassian.net/browse/PYTHON\">JIRA</a> issue tracker.</p><p>If you would like to contribute, please feel free to open a pull request.</p><h2><a href=\"https://github.com/datastax/python-driver#getting-help\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-getting-help\"></a>Getting Help</h2><p>Your best options for getting help with the driver are the\n<a href=\"https://groups.google.com/a/lists.datastax.com/forum/#!forum/python-driver-user\">mailing list</a>\nand the <code>#datastax-drivers</code> channel in the <a href=\"https://academy.datastax.com/slack\">DataStax Academy Slack</a>.</p><h2><a href=\"https://github.com/datastax/python-driver#license\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-license\"></a>License</h2><p>Copyright 2013-2017 DataStax</p><p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p><p><a href=\"http://www.apache.org/licenses/LICENSE-2.0\">http://www.apache.org/licenses/LICENSE-2.0</a></p><p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>",
        "created_at": "2017-10-31T20:25:30+0000",
        "updated_at": "2018-07-24T19:50:56+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://camo.githubusercontent.com/f127f299b7b441d923b99ecfef34bdfc72d7fb63/68747470733a2f2f7472617669732d63692e6f72672f64617461737461782f707974686f6e2d6472697665722e706e673f6272616e63683d6d6173746572",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/4907"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1135,
            "label": "packages",
            "slug": "packages"
          },
          {
            "id": 1163,
            "label": "libraries",
            "slug": "libraries"
          }
        ],
        "is_public": true,
        "id": 4906,
        "uid": "5b578323d2be44.68477335",
        "title": "DataStax Ruby Driver",
        "url": "https://github.com/datastax/ruby-driver",
        "content": "<p><em>If you're reading this on GitHub, please note that this is the readme for the development version and that some\nfeatures described here might not yet have been released. You can view the documentation for the latest released\nversion <a href=\"http://docs.datastax.com/en/developer/ruby-driver/latest\">here</a>.</em></p><p><a href=\"https://travis-ci.org/datastax/ruby-driver\"><img src=\"https://camo.githubusercontent.com/5f27a8ea4298c2484f420bd170bd7a41f65bc570/68747470733a2f2f7472617669732d63692e6f72672f64617461737461782f727562792d6472697665722e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/datastax/ruby-driver.svg?branch=master\" /></a></p><p>A Ruby client driver for Apache Cassandra. This driver works exclusively with\nthe Cassandra Query Language version 3 (CQL3) and Cassandra's native protocol.</p><p>Use the <a href=\"https://github.com/datastax/ruby-dse-driver.git\">Ruby DSE driver</a> for\nbetter compatibility and support for DataStax Enterprise.</p><ul><li>Code: <a href=\"https://github.com/datastax/ruby-driver\">https://github.com/datastax/ruby-driver</a></li>\n<li>Docs: <a href=\"http://docs.datastax.com/en/developer/ruby-driver\">http://docs.datastax.com/en/developer/ruby-driver</a></li>\n<li>Jira: <a href=\"https://datastax-oss.atlassian.net/browse/RUBY\">https://datastax-oss.atlassian.net/browse/RUBY</a></li>\n<li>Mailing List: <a href=\"https://groups.google.com/a/lists.datastax.com/forum/#!forum/ruby-driver-user\">https://groups.google.com/a/lists.datastax.com/forum/#!forum/ruby-driver-user</a></li>\n<li>Slack: <code>#datastax-drivers</code> channel at <a href=\"https://academy.datastax.com/slack\">https://academy.datastax.com/slack</a></li>\n<li>Twitter: Follow the latest news about DataStax Drivers - <a href=\"http://twitter.com/stamhankar999\">@stamhankar999</a>, <a href=\"http://twitter.com/avalanche123\">@avalanche123</a>, <a href=\"https://twitter.com/al3xandru\">@al3xandru</a></li>\n</ul><p>This driver is based on <a href=\"https://github.com/iconara/cql-rb\">the cql-rb gem</a> by <a href=\"https://github.com/iconara\">Theo Hultberg</a> and we added support for:</p><ul><li><a href=\"http://docs.datastax.com/en/developer/ruby-driver/3.0/features/asynchronous_io/\">Asynchronous execution</a></li>\n<li>One-off, <a href=\"http://docs.datastax.com/en/developer/ruby-driver/3.0/features/basics/prepared_statements/\">prepared</a> and <a href=\"http://docs.datastax.com/en/developer/ruby-driver/3.0/features/basics/batch_statements/\">batch statements</a></li>\n<li>Automatic peer discovery and cluster metadata with <a href=\"http://docs.datastax.com/en/developer/ruby-driver/3.0/features/state_listeners/\">support for change notifications</a></li>\n<li>Various <a href=\"http://docs.datastax.com/en/developer/ruby-driver/3.0/features/load_balancing/\">load-balancing</a>, <a href=\"http://docs.datastax.com/en/developer/ruby-driver/3.0/features/retry_policies/\">retry</a> and <a href=\"http://docs.datastax.com/en/developer/ruby-driver/3.0/features/reconnection/\">reconnection</a> policies with <a href=\"http://docs.datastax.com/en/developer/ruby-driver/3.0/features/load_balancing/implementing_a_policy/\">ability to write your own</a></li>\n<li><a href=\"http://docs.datastax.com/en/developer/ruby-driver/3.0/features/security/ssl_encryption/\">SSL encryption</a></li>\n<li><a href=\"http://docs.datastax.com/en/developer/ruby-driver/3.0/features/error_handling/\">Flexible and robust error handling</a></li>\n<li><a href=\"http://docs.datastax.com/en/developer/ruby-driver/3.0/features/debugging/\">Per-request execution information and tracing</a></li>\n<li><a href=\"http://docs.datastax.com/en/developer/ruby-driver/3.0/features/address_resolution/\">Configurable address resolution</a></li>\n</ul><p><a href=\"https://speakerdeck.com/avalanche123/ruby-driver-explained\">Check out the slides from Ruby Driver Explained</a> for a detailed overview of the Ruby Driver architecture.</p><h2><a href=\"https://github.com/datastax/ruby-driver#compatibility\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-compatibility\"></a>Compatibility</h2><p>This driver works exclusively with the Cassandra Query Language v3 (CQL3) and Cassandra's native protocol. The current version works with:</p><ul><li>Apache Cassandra versions 2.1, 2.2, and 3.x</li>\n<li>DataStax Enterprise 4.8 and above. However, the <a href=\"https://github.com/datastax/ruby-dse-driver.git\">Ruby DSE driver</a> provides more features and is recommended for use with DataStax Enterprise.</li>\n<li>Ruby (MRI) 2.2, 2.3, 2.4</li>\n<li>JRuby 1.7, 9k</li>\n</ul><p><strong>Note</strong>: Rubinius is not supported. MRI 2.0, and 2.1 are not officially supported, but they should work. MRI 1.9.3 is deprecated\nand may break in any release after 3.0.</p><p><strong>Note</strong>: Big-endian systems are not supported.</p><h2><a href=\"https://github.com/datastax/ruby-driver#feedback-requested\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-feedback-requested\"></a>Feedback Requested</h2><p><em>Help us focus our efforts!</em> <a href=\"http://goo.gl/forms/pCs8PTpHLf\">Provide your input</a> on the Ruby Driver Platform and Runtime Survey (we kept it short).</p><h2><a href=\"https://github.com/datastax/ruby-driver#quick-start\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-quick-start\"></a>Quick start</h2><div class=\"highlight highlight-source-ruby\"><pre>require 'cassandra'\ncluster = Cassandra.cluster # connects to localhost by default\ncluster.each_host do |host| # automatically discovers all peers\n  puts \"Host #{host.ip}: id=#{host.id} datacenter=#{host.datacenter} rack=#{host.rack}\"\nend\nkeyspace = 'system_schema'\nsession  = cluster.connect(keyspace) # create session, optionally scoped to a keyspace, to execute queries\nfuture = session.execute_async('SELECT keyspace_name, table_name FROM tables') # fully asynchronous api\nfuture.on_success do |rows|\n  rows.each do |row|\n    puts \"The keyspace #{row['keyspace_name']} has a table called #{row['table_name']}\"\n  end\nend\nfuture.join</pre></div><p><strong>Note</strong>: The host you specify is just a seed node, the driver will automatically discover all peers in the cluster.</p><p>Read more:</p><ul><li><a href=\"http://docs.datastax.com/en/developer/ruby-driver/3.0/api/cassandra/#cluster-class_method\"><code>Cassandra.cluster</code> options</a></li>\n<li><a href=\"http://docs.datastax.com/en/developer/ruby-driver/3.0/api/cassandra/session/#execute_async-instance_method\"><code>Session#execute_async</code> options</a></li>\n<li><a href=\"http://docs.datastax.com/en/developer/ruby-driver/3.0/features\">Usage documentation</a></li>\n</ul><h2><a href=\"https://github.com/datastax/ruby-driver#installation\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-installation\"></a>Installation</h2><p>Install via rubygems</p><div class=\"highlight highlight-source-shell\"><pre>gem install cassandra-driver</pre></div><p>Install via Gemfile</p><div class=\"highlight highlight-source-ruby\"><pre>gem 'cassandra-driver'</pre></div><p><strong>Note</strong>: if you want to use compression you should also install <a href=\"http://rubygems.org/gems/snappy\">snappy</a> or <a href=\"http://rubygems.org/gems/lz4-ruby\">lz4-ruby</a>. <a href=\"http://docs.datastax.com/en/developer/ruby-driver/3.0/features/#compression\">Read more about compression.</a></p><h2><a href=\"https://github.com/datastax/ruby-driver#upgrading-from-cql-rb\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-upgrading-from-cql-rb\"></a>Upgrading from cql-rb</h2><p>Some of the new features added to the driver have unfortunately led to changes in the original cql-rb API.\nIn the examples directory, you can find <a href=\"https://github.com/datastax/ruby-driver/blob/master/examples/cql-rb-wrapper.rb\">an example of how to wrap the ruby driver to achieve almost complete\ninterface parity with cql-rb</a>\nto assist you with gradual upgrade.</p><p>If you are upgrading to DataStax Enterprise, use the <a href=\"https://github.com/datastax/ruby-dse-driver.git\">Ruby DSE driver</a>\nfor more features and better compatibility.</p><h2><a href=\"https://github.com/datastax/ruby-driver#whats-new-in-v32\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-whats-new-in-v32\"></a>What's new in v3.2</h2><p>This minor release adds support for MRI 2.4.x and also contains a few miscellaneous defect fixes.</p><p>See the <a href=\"https://github.com/datastax/ruby-driver/blob/master/CHANGELOG.md\">changelog</a> for more information on all\nchanges in this version and past versions.</p><h2><a href=\"https://github.com/datastax/ruby-driver#whats-new-in-v31\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-whats-new-in-v31\"></a>What's new in v3.1</h2><p>This minor release introduces features and fixes around resiliency, schema metadata, usability, and performance. One\nof the most user-impacting of these is the introduction of\n<a href=\"http://docs.datastax.com/en/developer/ruby-driver/3.1/features/basics/execution_profiles\">execution profiles</a>.\nExecution profiles allow you to group various execution options into a 'profile' and you reference the desired\nprofile at execution time. Get the scoop\n<a href=\"http://docs.datastax.com/en/developer/ruby-driver/3.1/features/basics/execution_profiles\">here</a>.</p><h2><a href=\"https://github.com/datastax/ruby-driver#whats-new-in-v30\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-whats-new-in-v30\"></a>What's new in v3.0</h2><h3><a href=\"https://github.com/datastax/ruby-driver#features\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-features\"></a>Features:</h3><ul><li>Add support for Apache Cassandra native protocol v4</li>\n<li>Add support for smallint, tinyint, date (<code>Cassandra::Date</code>) and time (<code>Cassandra::Time</code>) data types.</li>\n<li>Include schema metadata for User Defined Functions and User Defined Aggregates.</li>\n<li>Augment the <code>Cassandra::Table</code> object to expose many more attributes: <code>id</code>, <code>options</code>, <code>keyspace</code>, <code>partition_key</code>, <code>clustering_columns</code>, and <code>clustering_order</code>. This makes it significantly easier to write administration scripts that report various attributes of your schema, which may help to highlight areas for improvement.</li>\n<li>Include client ip addresses in request traces, only on Cassandra 3.x.</li>\n<li>Add new retry policy decision <code>Cassandra::Retry::Policy#try_next_host</code>.</li>\n<li>Support specifying statement idempotence with the new <code>idempotent</code> option when executing.</li>\n<li>Support sending custom payloads when preparing or executing statements using the new <code>payload</code> option.</li>\n<li>Expose custom payloads received with responses on server exceptions and <code>Cassandra::Execution::Info</code> instances.</li>\n<li>Expose server warnings on server exceptions and <code>Cassandra::Execution::Info</code> instances.</li>\n<li>Add <code>connections_per_local_node</code>, <code>connections_per_remote_node</code>, <code>requests_per_connection</code> cluster configuration options to tune parallel query execution and resource usage.</li>\n<li>Add <code>Cassandra::Logger</code> class to make it easy for users to enable debug logging in the client.</li>\n<li>Add <code>protocol_version</code> configuration option to allow the user to force the protocol version to use for communication with nodes.</li>\n<li>Add support for materialized views and indexes in the schema metadata.</li>\n<li>Support the <code>ReadError</code>, <code>WriteError</code>, and <code>FunctionCallError</code> Cassandra error responses introduced in Cassandra 2.2.</li>\n<li>Add support for unset variables in bound statements.</li>\n<li>Support DSE security (<code>DseAuthenticator</code>, configured for LDAP).</li>\n<li>Add a timeout option to <code>Cassandra::Future#get</code>.</li>\n</ul><h3><a href=\"https://github.com/datastax/ruby-driver#breaking-changes-from-2x\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-breaking-changes-from-2x\"></a>Breaking Changes from 2.x:</h3><ul><li><code>Cassandra::Future#join</code> is now an alias to Cassandra::Future#get and will raise an error if the future is resolved with one.</li>\n<li>Default consistency level is now LOCAL_ONE.</li>\n<li>Enable tcp no-delay by default.</li>\n<li>Unavailable errors are retried on the next host in the load balancing plan by default.</li>\n<li>Statement execution no longer retried on timeouts, unless the statement is marked as idempotent in the call to <code>Cassandra::Session#execute*</code> or when creating a <code>Cassandra::Statement</code> object.</li>\n<li><code>Cassandra::Statements::Batch#add</code> and <code>Cassandra::Session#execute*</code> signatures have changed in how one specifies query parameters. Specify the query parameters array as the value of the arguments key:</li>\n</ul><div class=\"highlight highlight-source-ruby\"><pre>batch.add(query, ['val1', 'val2'])\n# becomes\nbatch.add(query, arguments: ['val1', 'val2'])\nbatch.add(query, {p1: 'val1'})\n# becomes\nbatch.add(query, arguments: {p1: 'val1'})</pre></div><ul><li>The Datacenter-aware load balancing policy (<code>Cassandra::LoadBalancing::Policies::DCAwareRoundRobin</code>) defaults to using\nnodes in the local DC only. In prior releases, the policy would fall back to remote nodes after exhausting local nodes.\nSpecify a positive value (or nil for unlimited) for <code>max_remote_hosts_to_use</code> when initializing the policy to allow remote node use.</li>\n<li>Unspecified variables in statements previously resulted in an exception. Now they are essentially ignored or treated as null.</li>\n</ul><h2><a href=\"https://github.com/datastax/ruby-driver#code-examples\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-code-examples\"></a>Code examples</h2><p>The DataStax Ruby Driver uses the awesome <a href=\"http://cukes.info/\">Cucumber Framework</a> for\nboth end-to-end, or acceptance, testing and constructing documentation. All of the\nfeatures supported by the driver have appropriate acceptance tests with easy-to-copy code\nexamples in the <code>features/</code> directory.</p><h2><a href=\"https://github.com/datastax/ruby-driver#running-tests\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-running-tests\"></a>Running tests</h2><p>If you don't feel like reading through the following instructions on how to run\nruby-driver tests, feel free to <a href=\"https://github.com/datastax/ruby-driver/blob/master/.travis.yml\">check out .travis.yml for the entire build code</a>.</p><ul><li>Check out the driver codebase and install test dependencies:</li>\n</ul><div class=\"highlight highlight-source-shell\"><pre>git clone https://github.com/datastax/ruby-driver.git\ncd ruby-driver\nbundle install --without docs</pre></div><ul><li>\n<p><a href=\"http://www.datastax.com/dev/blog/ccm-a-development-tool-for-creating-local-cassandra-clusters\">Install ccm</a></p>\n</li>\n<li>\n<p>Run tests against different versions of Cassandra:</p>\n</li>\n</ul><div class=\"highlight highlight-source-shell\"><pre>CASSANDRA_VERSION=3.1.1 bundle exec cucumber # runs end-to-end tests (or bundle exec rake cucumber)\nCASSANDRA_VERSION=3.0.0 bundle exec rspec # runs unit tests (or bundle exec rake rspec)\nCASSANDRA_VERSION=2.1.12 bundle exec rake integration # run integration tests\nCASSANDRA_VERSION=2.1.12 bundle exec rake test # run both as well as integration tests</pre></div><h2><a href=\"https://github.com/datastax/ruby-driver#changelog--versioning\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-changelog--versioning\"></a>Changelog &amp; versioning</h2><p>Check out the <a href=\"https://github.com/datastax/ruby-driver/releases\">releases on GitHub</a> and\n<a href=\"https://github.com/datastax/ruby-driver/blob/master/CHANGELOG.md\">changelog</a>. Version\nnumbering follows the <a href=\"http://semver.org/\">semantic versioning</a> scheme.</p><p>Private and experimental APIs, defined as whatever is not in the\n<a href=\"http://docs.datastax.com/en/developer/ruby-driver/3.0/api\">public API documentation</a>, i.e. classes and methods marked as <code>@private</code>, will change\nwithout warning. If you've been recommended to try an experimental API by the maintainers,\nplease let them know if you depend on that API. Experimental APIs will eventually become\npublic, and knowing how they are used helps in determining their maturity.</p><p>Prereleases will be stable, in the sense that they will have finished and properly tested\nfeatures only, but may introduce APIs that will change before the final release. Please\nuse the prereleases and report bugs, but don't deploy them to production without\nconsulting the maintainers, or doing extensive testing yourself. If you do deploy to\nproduction please let the maintainers know as this helps in determining the maturity of\nthe release.</p><h2><a href=\"https://github.com/datastax/ruby-driver#known-bugs--limitations\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-known-bugs--limitations\"></a>Known bugs &amp; limitations</h2><ul><li>Specifying a <code>protocol_version</code> option of 1 or 2 in cluster options will fail with a\n<code>NoHostsAvailable</code> error rather than a <code>ProtocolError</code> against Cassandra node versions 3.0-3.4.</li>\n<li>Because the driver reactor is using <code>IO.select</code>, the maximum number of tcp connections allowed is 1024.</li>\n<li>Because the driver uses <code>IO#write_nonblock</code>, Windows is not supported.</li>\n</ul><p>Please <a href=\"http://docs.datastax.com/en/developer/ruby-driver/3.0/features/\">refer to the usage documentation for more information on common pitfalls</a></p><h2><a href=\"https://github.com/datastax/ruby-driver#contributing\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-contributing\"></a>Contributing</h2><p>For contributing read <a href=\"https://github.com/datastax/ruby-driver/blob/master/CONTRIBUTING.md\">CONTRIBUTING.md</a></p><h2><a href=\"https://github.com/datastax/ruby-driver#credits\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-credits\"></a>Credits</h2><p>This driver is based on the original work of <a href=\"https://github.com/iconara\">Theo Hultberg</a>\non <a href=\"https://github.com/iconara/cql-rb/\">cql-rb</a> and adds a series of advanced features\nthat are common across all other DataStax drivers for Apache Cassandra.</p><p>The development effort to provide an up to date, high performance, fully featured Ruby\nDriver for Apache Cassandra will continue on this project, while\n<a href=\"https://github.com/iconara/cql-rb/\">cql-rb</a> has been discontinued.</p><h2><a href=\"https://github.com/datastax/ruby-driver#copyright\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-copyright\"></a>Copyright</h2><p>Copyright 2013-2017 DataStax, Inc.</p><p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file\nexcept in compliance with the License. You may obtain a copy of the License at</p><p><a href=\"http://www.apache.org/licenses/LICENSE-2.0\">http://www.apache.org/licenses/LICENSE-2.0</a></p><p>Unless required by applicable law or agreed to in writing, software distributed under the\nLicense is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,\neither express or implied. See the License for the specific language governing permissions\nand limitations under the License.</p>",
        "created_at": "2017-10-31T20:25:34+0000",
        "updated_at": "2018-07-24T19:50:59+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 8,
        "domain_name": "github.com",
        "preview_picture": "https://camo.githubusercontent.com/5f27a8ea4298c2484f420bd170bd7a41f65bc570/68747470733a2f2f7472617669732d63692e6f72672f64617461737461782f727562792d6472697665722e7376673f6272616e63683d6d6173746572",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/4906"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 24,
            "label": "node",
            "slug": "node-js"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1135,
            "label": "packages",
            "slug": "packages"
          },
          {
            "id": 1163,
            "label": "libraries",
            "slug": "libraries"
          }
        ],
        "is_public": true,
        "id": 4905,
        "uid": "5b5783267f2ff8.47737723",
        "title": "DataStax NodeJS Driver",
        "url": "https://github.com/datastax/nodejs-driver",
        "content": "<p>A modern, <a href=\"https://github.com/datastax/nodejs-driver#features\">feature-rich</a> and highly tunable Node.js client library for Apache Cassandra (1.2+) using exclusively Cassandra's binary protocol and Cassandra Query Language v3. <em>Use the <a href=\"https://github.com/datastax/nodejs-dse-driver\">DSE Node.js driver</a> for better compatibility and support for DataStax Enterprise</em>.</p><h2><a href=\"https://github.com/datastax/nodejs-driver#installation\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-installation\"></a>Installation</h2><div class=\"highlight highlight-source-shell\"><pre>$ npm install cassandra-driver</pre></div><p><a href=\"https://travis-ci.org/datastax/nodejs-driver\"><img src=\"https://camo.githubusercontent.com/eb00101df16c48e8c49bad714207b207be5feac1/68747470733a2f2f7472617669732d63692e6f72672f64617461737461782f6e6f64656a732d6472697665722e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/datastax/nodejs-driver.svg?branch=master\" /></a> <a href=\"https://ci.appveyor.com/project/datastax/nodejs-driver/branch/master\"><img src=\"https://camo.githubusercontent.com/964cf2b4f7f404c95ca883916f62576cd8915f30/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f6d32317432746664706d6b6a6578316c2f6272616e63682f6d61737465723f7376673d74727565\" alt=\"Build status\" data-canonical-src=\"https://ci.appveyor.com/api/projects/status/m21t2tfdpmkjex1l/branch/master?svg=true\" /></a></p><h2><a href=\"https://github.com/datastax/nodejs-driver#features\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-features\"></a>Features</h2><ul><li>Simple, Prepared, and <a href=\"http://docs.datastax.com/en/developer/nodejs-driver/latest/features/batch/\">Batch</a> statements</li>\n<li>Asynchronous IO, parallel execution, request pipelining</li>\n<li><a href=\"http://docs.datastax.com/en/developer/nodejs-driver/latest/features/connection-pooling/\">Connection pooling</a></li>\n<li>Auto node discovery</li>\n<li>Automatic reconnection</li>\n<li>Configurable <a href=\"http://docs.datastax.com/en/developer/nodejs-driver/latest/features/tuning-policies/#load-balancing-policy\">load balancing</a> and <a href=\"http://docs.datastax.com/en/developer/nodejs-driver/latest/features/tuning-policies/#retry-policy\">retry policies</a></li>\n<li>Works with any cluster size</li>\n<li>Both <a href=\"http://docs.datastax.com/en/developer/nodejs-driver/latest/features/promise-callback/\">promise and callback-based API</a></li>\n<li><a href=\"https://github.com/datastax/nodejs-driver#row-streaming-and-pipes\">Row streaming and pipes</a></li>\n</ul><h2><a href=\"https://github.com/datastax/nodejs-driver#documentation\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-documentation\"></a>Documentation</h2><ul><li><a href=\"http://docs.datastax.com/en/developer/nodejs-driver/latest/\">Documentation index</a></li>\n<li><a href=\"http://docs.datastax.com/en/developer/nodejs-driver/latest/features/datatypes/\">CQL types to JavaScript types</a></li>\n<li><a href=\"http://docs.datastax.com/en/developer/nodejs-driver/latest/api/\">API docs</a></li>\n<li><a href=\"http://docs.datastax.com/en/developer/nodejs-driver/latest/faq/\">FAQ</a></li>\n</ul><h2><a href=\"https://github.com/datastax/nodejs-driver#getting-help\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-getting-help\"></a>Getting Help</h2><p>You can use the <a href=\"https://groups.google.com/a/lists.datastax.com/forum/#!forum/nodejs-driver-user\">project mailing list</a> or create a ticket on the <a href=\"https://datastax-oss.atlassian.net/projects/NODEJS/issues\">Jira issue tracker</a>. Additionally, you can use the <code>#datastax-drivers</code> channel in the <a href=\"https://academy.datastax.com/slack\">DataStax Academy Slack</a>.</p><h2><a href=\"https://github.com/datastax/nodejs-driver#basic-usage\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-basic-usage\"></a>Basic usage</h2><div class=\"highlight highlight-source-js\"><pre>const cassandra = require('cassandra-driver');\nconst client = new cassandra.Client({ contactPoints: ['h1', 'h2'], keyspace: 'ks1' });\nconst query = 'SELECT name, email FROM users WHERE key = ?';\nclient.execute(query, [ 'someone' ])\n  .then(result =&gt; console.log('User with email %s', result.rows[0].email));</pre></div><p>Alternatively, you can use the callback-based execution for all asynchronous methods of the API.</p><div class=\"highlight highlight-source-js\"><pre>client.execute(query, [ 'someone' ], function(err, result) {\n  assert.ifError(err);\n  console.log('User with email %s', result.rows[0].email);\n});</pre></div><h3><a href=\"https://github.com/datastax/nodejs-driver#prepare-your-queries\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-prepare-your-queries\"></a>Prepare your queries</h3><p>Using prepared statements provides multiple benefits.\nPrepared statements are parsed and prepared on the Cassandra nodes and are ready for future execution.\nAlso, when preparing, the driver retrieves information about the parameter types which\n<strong>allows an accurate mapping between a JavaScript type and a Cassandra type</strong>.</p><p>The driver will prepare the query once on each host and execute the statement with the bound parameters.</p><div class=\"highlight highlight-source-js\"><pre>// Use query markers (?) and parameters\nconst query = 'UPDATE users SET birth = ? WHERE key=?'; \nconst params = [ new Date(1942, 10, 1), 'jimi-hendrix' ];\n// Set the prepare flag in the query options\nclient.execute(query, params, { prepare: true })\n  .then(result =&gt; console.log('Row updated on the cluster'));</pre></div><h3><a href=\"https://github.com/datastax/nodejs-driver#row-streaming-and-pipes\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-row-streaming-and-pipes\"></a>Row streaming and pipes</h3><p>When using <code>#eachRow()</code> and <code>#stream()</code> methods, the driver parses each row as soon as it is received,\nyielding rows without buffering them.</p><div class=\"highlight highlight-source-js\"><pre>// Reducing a large result\nclient.eachRow('SELECT time, val FROM temperature WHERE station_id=', ['abc'],\n  function(n, row) {\n    // The callback will be invoked per each row as soon as they are received\n    minTemperature = Math.min(row.val, minTemperature);\n  },\n  function (err) {\n    assert.ifError(err);\n  }\n);</pre></div><p>The <code>#stream()</code> method works in the same way but instead of callback it returns a <a href=\"http://nodejs.org/api/stream.html#stream_class_stream_readable\">Readable Streams2</a> object\nin <code>objectMode</code> that emits instances of <code>Row</code>.\nIt can be <strong>piped</strong> downstream and provides automatic pause/resume logic (it buffers when not read).</p><div class=\"highlight highlight-source-js\"><pre>client.stream('SELECT time, val FROM temperature WHERE station_id=', [ 'abc' ])\n  .on('readable', function () {\n    // 'readable' is emitted as soon a row is received and parsed\n    var row;\n    while (row = this.read()) {\n      console.log('time %s and value %s', row.time, row.val);\n    }\n  })\n  .on('end', function () {\n    // Stream ended, there aren't any more rows\n  })\n  .on('error', function (err) {\n    // Something went wrong: err is a response error from Cassandra\n  });</pre></div><h3><a href=\"https://github.com/datastax/nodejs-driver#user-defined-types\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-user-defined-types\"></a>User defined types</h3><p><a href=\"http://cassandra.apache.org/doc/latest/cql/types.html#udts\">User defined types (UDT)</a> are represented as JavaScript objects.</p><p>For example:\nConsider the following UDT and table</p><div class=\"highlight highlight-source-sql\"><pre>CREATE TYPE address (\n  street text,\n  city text,\n  state text,\n  zip int,\n  phones set&lt;text&gt;\n);\nCREATE TABLE users (\n  name text PRIMARY KEY,\n  email text,\n  address frozen&lt;address&gt;\n);</pre></div><p>You can retrieve the user address details as a regular JavaScript object.</p><div class=\"highlight highlight-source-js\"><pre>const query = 'SELECT name, address FROM users WHERE key = ?';\nclient.execute(query, [ key ], { prepare: true })\n  .then(result =&gt; {\n    const row = result.first();\n    const address = row.address;\n    console.log('User lives in %s, %s - %s', address.street, address.city, address.state); \n  });</pre></div><p>Read more information  about using <a href=\"http://docs.datastax.com/en/developer/nodejs-driver/latest/features/datatypes/udts/\">UDTs with the Node.js Driver</a>.</p><h3><a href=\"https://github.com/datastax/nodejs-driver#paging\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-paging\"></a>Paging</h3><p>All driver methods use a default <code>fetchSize</code> of 5000 rows, retrieving only first page of results up to a\nmaximum of 5000 rows to shield an application against accidentally large result sets. To retrieve the following\nrecords you can use the <code>autoPage</code> flag in the query options of <code>#eachRow()</code> and <code>#stream()</code> methods.</p><div class=\"highlight highlight-source-js\"><pre>//Imagine a column family with millions of rows\nconst query = 'SELECT * FROM largetable';\nclient.eachRow(query, [], { autoPage: true }, function (n, row) {\n  // This function will be invoked per each of the rows in all the table\n}, endCallback);</pre></div><h3><a href=\"https://github.com/datastax/nodejs-driver#batch-multiple-statements\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-batch-multiple-statements\"></a>Batch multiple statements</h3><p>You can execute multiple statements in a batch to update/insert several rows atomically even in different column families.</p><div class=\"highlight highlight-source-js\"><pre>const queries = [\n  {\n    query: 'UPDATE user_profiles SET email=? WHERE key=?',\n    params: [ emailAddress, 'hendrix' ]\n  },\n  {\n    query: 'INSERT INTO user_track (key, text, date) VALUES (?, ?, ?)',\n    params: [ 'hendrix', 'Changed email', new Date() ]\n  }\n];\nclient.batch(queries, { prepare: true })\n  .then(result =&gt; console.log('Data updated on cluster'));</pre></div><hr /><h2><a href=\"https://github.com/datastax/nodejs-driver#data-types\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-data-types\"></a>Data types</h2><p>There are few data types defined in the ECMAScript specification, this usually represents a problem when you are trying\nto deal with data types that come from other systems in JavaScript.</p><p>The driver supports all the CQL data types in Apache Cassandra (3.0 and below) even for types with no built-in\nJavaScript representation, like decimal, varint and bigint. Check the documentation on working with\n<a href=\"http://docs.datastax.com/en/developer/nodejs-driver/latest/features/datatypes/numerical/\">numerical values</a>, <a href=\"http://docs.datastax.com/en/developer/nodejs-driver/latest/features/datatypes/uuids/\">uuids</a> and <a href=\"http://docs.datastax.com/en/developer/nodejs-driver/latest/features/datatypes/collections/\">collections</a>.</p><h2><a href=\"https://github.com/datastax/nodejs-driver#logging\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-logging\"></a>Logging</h2><p>Instances of <code>Client()</code> are <code>EventEmitter</code> and emit <code>log</code> events:</p><div class=\"highlight highlight-source-js\"><pre>client.on('log', function(level, className, message, furtherInfo) {\n  console.log('log event: %s -- %s', level, message);\n});</pre></div><p>The <code>level</code> being passed to the listener can be <code>verbose</code>, <code>info</code>, <code>warning</code> or <code>error</code>.</p><h2><a href=\"https://github.com/datastax/nodejs-driver#compatibility\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-compatibility\"></a>Compatibility</h2><ul><li>Apache Cassandra versions 2.0 and above.</li>\n<li>DataStax Enterprise versions 4.5 and above.</li>\n<li>Node.js versions 0.10 and above.</li>\n</ul><p>Note: DataStax products do not support big-endian systems.</p><h2><a href=\"https://github.com/datastax/nodejs-driver#feedback-requested\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-feedback-requested\"></a>Feedback Requested</h2><p><strong>Help us focus our efforts!</strong> Provide your input on the <a href=\"http://goo.gl/forms/f216tY3Ebr\">Platform and Runtime Survey</a> (we kept it short).</p><h2><a href=\"https://github.com/datastax/nodejs-driver#credits\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-credits\"></a>Credits</h2><p>This driver is based on the original work of <a href=\"https://github.com/jorgebay\">Jorge Bay</a> on <a href=\"https://github.com/jorgebay/node-cassandra-cql\">node-cassandra-cql</a> and adds a series of advanced features that are common across all other <a href=\"https://github.com/datastax\">DataStax drivers</a> for Apache Cassandra.</p><p>The development effort to provide an up to date, high performance, fully featured Node.js Driver for Apache Cassandra will continue on this project, while <a href=\"https://github.com/jorgebay/node-cassandra-cql\">node-cassandra-cql</a> will be discontinued.</p><h2><a href=\"https://github.com/datastax/nodejs-driver#license\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-license\"></a>License</h2><p>Copyright 2014-2017 DataStax</p><p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p><p><a href=\"http://www.apache.org/licenses/LICENSE-2.0\">http://www.apache.org/licenses/LICENSE-2.0</a></p><p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>",
        "created_at": "2017-10-31T20:25:39+0000",
        "updated_at": "2018-07-24T19:51:02+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 5,
        "domain_name": "github.com",
        "preview_picture": "https://camo.githubusercontent.com/eb00101df16c48e8c49bad714207b207be5feac1/68747470733a2f2f7472617669732d63692e6f72672f64617461737461782f6e6f64656a732d6472697665722e7376673f6272616e63683d6d6173746572",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/4905"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1135,
            "label": "packages",
            "slug": "packages"
          },
          {
            "id": 1163,
            "label": "libraries",
            "slug": "libraries"
          }
        ],
        "is_public": true,
        "id": 4904,
        "uid": "5b57832a1dd002.21843717",
        "title": "DataStax C# Driver",
        "url": "https://github.com/datastax/csharp-driver",
        "content": "<p>A modern, <a href=\"http://docs.datastax.com/en/developer/csharp-driver/latest/features/\">feature-rich</a> and highly tunable C# client library for Apache Cassandra (1.2+) using exclusively Cassandra's binary protocol and Cassandra Query Language v3. <em>Use the <a href=\"https://github.com/datastax/csharp-dse-driver\">DSE C# driver</a> for better compatibility and support for DataStax Enterprise</em>.</p><p>The driver supports .NET Framework 4.5+ and .NET Core 1+.</p><h2><a href=\"https://github.com/datastax/csharp-driver#installation\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-installation\"></a>Installation</h2><p><a href=\"https://nuget.org/packages/CassandraCSharpDriver/\">Get it on Nuget</a></p><div class=\"highlight highlight-source-shell\"><pre>PM&gt; Install-Package CassandraCSharpDriver</pre></div><p><a href=\"https://travis-ci.org/datastax/csharp-driver\"><img src=\"https://camo.githubusercontent.com/65a02e83e7515dc3b1ef568bd7317d0dac6a1874/68747470733a2f2f7472617669732d63692e6f72672f64617461737461782f6373686172702d6472697665722e7376673f6272616e63683d6d6173746572\" alt=\"Build status\" data-canonical-src=\"https://travis-ci.org/datastax/csharp-driver.svg?branch=master\" /></a>\n<a href=\"https://ci.appveyor.com/project/DataStax/csharp-driver/branch/master\"><img src=\"https://camo.githubusercontent.com/5735947c513653cfe29b994bb8826e9cd99d797f/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f7269316f6c7638626c376237796b37792f6272616e63682f6d61737465723f7376673d74727565\" alt=\"Windows Build status\" data-canonical-src=\"https://ci.appveyor.com/api/projects/status/ri1olv8bl7b7yk7y/branch/master?svg=true\" /></a>\n<a href=\"https://www.nuget.org/packages/CassandraCSharpDriver\"><img src=\"https://camo.githubusercontent.com/5d098659e24e8ec35c2a495433a10248901d2036/68747470733a2f2f696d672e736869656c64732e696f2f6e756765742f762f43617373616e6472614353686172704472697665722e737667\" alt=\"Latest stable\" data-canonical-src=\"https://img.shields.io/nuget/v/CassandraCSharpDriver.svg\" /></a></p><h2><a href=\"https://github.com/datastax/csharp-driver#features\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-features\"></a>Features</h2><ul><li>Sync and <a href=\"https://github.com/datastax/csharp-driver#asynchronous-api\">Async</a> API</li>\n<li>Simple, <a href=\"https://github.com/datastax/csharp-driver#prepared-statements\">Prepared</a>, and <a href=\"https://github.com/datastax/csharp-driver#batching-statements\">Batch</a> statements</li>\n<li>Asynchronous IO, parallel execution, request pipelining</li>\n<li>Connection pooling</li>\n<li>Auto node discovery</li>\n<li>Automatic reconnection</li>\n<li>Configurable <a href=\"http://docs.datastax.com/en/developer/csharp-driver/latest/features/tuning-policies/\">load balancing</a> and <a href=\"http://docs.datastax.com/en/developer/csharp-driver/latest/features/tuning-policies/\">retry policies</a></li>\n<li>Works with any cluster size</li>\n<li><a href=\"http://docs.datastax.com/en/developer/csharp-driver/latest/features/components/linq/\">Linq2Cql</a> and Ado.Net support</li>\n</ul><h2><a href=\"https://github.com/datastax/csharp-driver#documentation\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-documentation\"></a>Documentation</h2><ul><li><a href=\"http://docs.datastax.com/en/developer/csharp-driver/latest/\">Documentation index</a></li>\n<li><a href=\"https://academy.datastax.com/resources/getting-started-apache-cassandra-and-c-net\">Getting started guide</a></li>\n<li><a href=\"http://docs.datastax.com/en/drivers/csharp/3.3/\">API docs</a></li>\n</ul><h2><a href=\"https://github.com/datastax/csharp-driver#getting-help\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-getting-help\"></a>Getting Help</h2><p>You can use the project <a href=\"https://groups.google.com/a/lists.datastax.com/forum/#!forum/csharp-driver-user\">Mailing list</a> or create a ticket on the <a href=\"https://datastax-oss.atlassian.net/projects/CSHARP/issues\">Jira issue tracker</a>. Additionally, you can use the <code>#datastax-drivers</code> channel in the <a href=\"https://academy.datastax.com/slack\">DataStax Academy Slack</a>.</p><h2><a href=\"https://github.com/datastax/csharp-driver#upgrading-from-previous-versions\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-upgrading-from-previous-versions\"></a>Upgrading from previous versions</h2><p>If you are upgrading to <a href=\"https://www.datastax.com/products/datastax-enterprise\">DataStax Enterprise</a>, use the <a href=\"https://github.com/datastax/csharp-dse-driver\">DSE C# driver</a> for more features and better compatibility.</p><p>If you are upgrading from the 2.1 branch of the driver, be sure to have a look at the <a href=\"https://github.com/datastax/csharp-driver/blob/master/doc/upgrade-guide-2.5.md\">upgrade guide</a>.</p><p>If you are upgrading from the 1.x branch of the driver, follow the <a href=\"https://github.com/datastax/csharp-driver/blob/master/doc/upgrade-guide-2.0.md\">upgrade guide to 2.0</a>, and then the above document.</p><h2><a href=\"https://github.com/datastax/csharp-driver#basic-usage\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-basic-usage\"></a>Basic Usage</h2><div class=\"highlight highlight-source-cs\"><pre>var cluster = Cluster.Builder()\n                     .AddContactPoints(\"host1\")\n                     .Build();\n// Connect to the nodes using a keyspace\nvar session = cluster.Connect(\"sample_keyspace\");\n// Execute a query on a connection synchronously\nvar rs = session.Execute(\"SELECT * FROM sample_table\");\n// Iterate through the RowSet\nforeach (var row in rs)\n{\n  var value = row.GetValue&lt;int&gt;(\"sample_int_column\");\n  // Do something with the value\n}</pre></div><h3><a href=\"https://github.com/datastax/csharp-driver#prepared-statements\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-prepared-statements\"></a>Prepared statements</h3><p>Prepare your query <strong>once</strong> and bind different parameters to obtain best performance.</p><div class=\"highlight highlight-source-cs\"><pre>// Prepare a statement once\nvar ps = session.Prepare(\"UPDATE user_profiles SET birth=? WHERE key=?\");\n// ...bind different parameters every time you need to execute\nvar statement = ps.Bind(new DateTime(1942, 11, 27), \"hendrix\");\n// Execute the bound statement with the provided parameters\nsession.Execute(statement);</pre></div><h3><a href=\"https://github.com/datastax/csharp-driver#batching-statements\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-batching-statements\"></a>Batching statements</h3><p>You can execute multiple statements (prepared or unprepared) in a batch to update/insert several rows atomically even in different column families.</p><div class=\"highlight highlight-source-cs\"><pre>// Prepare the statements involved in a profile update once\nvar profileStmt = session.Prepare(\"UPDATE user_profiles SET email=? WHERE key=?\");\nvar userTrackStmt = session.Prepare(\"INSERT INTO user_track (key, text, date) VALUES (?, ?, ?)\");\n// ...you should reuse the prepared statement\n// Bind the parameters and add the statement to the batch batch\nvar batch = new BatchStatement()\n  .Add(profileStmt.Bind(emailAddress, \"hendrix\"))\n  .Add(userTrackStmt.Bind(\"hendrix\", \"You changed your email\", DateTime.Now));\n// Execute the batch\nsession.Execute(batch);</pre></div><h3><a href=\"https://github.com/datastax/csharp-driver#asynchronous-api\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-asynchronous-api\"></a>Asynchronous API</h3><p>Session allows asynchronous execution of statements (for any type of statement: simple, bound or batch) by exposing the <code>ExecuteAsync</code> method.</p><div class=\"highlight highlight-source-cs\"><pre>// Execute a statement asynchronously using await\nvar rs = await session.ExecuteAsync(statement);</pre></div><h3><a href=\"https://github.com/datastax/csharp-driver#avoid-boilerplate-mapping-code\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-avoid-boilerplate-mapping-code\"></a>Avoid boilerplate mapping code</h3><p>The driver features a built-in <a href=\"http://docs.datastax.com/en/developer/csharp-driver/latest/features/components/mapper/\">Mapper</a> and <a href=\"http://docs.datastax.com/en/developer/csharp-driver/latest/features/components/linq/\">Linq</a> components that can use to avoid boilerplate mapping code between cql rows and your application entities.</p><div class=\"highlight highlight-source-cs\"><pre>User user = mapper.Single&lt;User&gt;(\"SELECT name, email FROM users WHERE id = ?\", userId);</pre></div><p>See the <a href=\"http://docs.datastax.com/en/developer/csharp-driver/latest/features/components/\">driver components documentation</a> for more information.</p><h3><a href=\"https://github.com/datastax/csharp-driver#automatic-pagination-of-results\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-automatic-pagination-of-results\"></a>Automatic pagination of results</h3><p>You can iterate indefinitely over the <code>RowSet</code>, having the rows fetched block by block until the rows available on the client side are exhausted.</p><div class=\"highlight highlight-source-cs\"><pre>var statement = new SimpleStatement(\"SELECT * from large_table\");\n// Set the page size, in this case the RowSet will not contain more than 1000 at any time\nstatement.SetPageSize(1000);\nvar rs = session.Execute(statement);\nforeach (var row in rs)\n{\n  // The enumerator will yield all the rows from Cassandra\n  // Retrieving them in the back in blocks of 1000.\n}</pre></div><h3><a href=\"https://github.com/datastax/csharp-driver#user-defined-types-mapping\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-user-defined-types-mapping\"></a>User defined types mapping</h3><p>You can map your <a href=\"http://docs.datastax.com/en/cql/3.1/cql/cql_reference/cqlRefUDType.html\">Cassandra User Defined Types</a> to your application entities.</p><p>For a given udt</p><div class=\"highlight highlight-source-sql\"><pre>CREATE TYPE address (\n  street text,\n  city text,\n  zip_code int,\n  phones set&lt;text&gt;\n);</pre></div><p>For a given class</p><div class=\"highlight highlight-source-cs\"><pre>public class Address\n{\n  public string Street { get; set; }\n  public string City { get; set; }\n  public int ZipCode { get; set; }\n  public IEnumerable&lt;string&gt; Phones { get; set;}\n}</pre></div><p>You can either map the properties by name</p><div class=\"highlight highlight-source-cs\"><pre>//Map the properties by name automatically\nsession.UserDefinedTypes.Define(\n  UdtMap.For&lt;Address&gt;()\n);</pre></div><p>Or you can define the properties manually</p><div class=\"highlight highlight-source-cs\"><pre>session.UserDefinedTypes.Define(\n  UdtMap.For&lt;Address&gt;()\n    .Map(a =&gt; a.Street, \"street\")\n    .Map(a =&gt; a.City, \"city\")\n    .Map(a =&gt; a.ZipCode, \"zip_code\")\n    .Map(a =&gt; a.Phones, \"phones\")\n);</pre></div><p>You should <strong>map your <a href=\"http://docs.datastax.com/en/cql/3.1/cql/cql_reference/cqlRefUDType.html\">UDT</a> to your entity once</strong> and you will be able to use that mapping during all your application lifetime.</p><div class=\"highlight highlight-source-cs\"><pre>var rs = session.Execute(\"SELECT id, name, address FROM users where id = x\");\nvar row = rs.First();\n// You can retrieve the field as a value of type Address\nvar userAddress = row.GetValue&lt;Address&gt;(\"address\");\nConsole.WriteLine(\"user lives on {0} Street\", userAddress.Street);</pre></div><h3><a href=\"https://github.com/datastax/csharp-driver#setting-cluster-and-statement-execution-options\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-setting-cluster-and-statement-execution-options\"></a>Setting cluster and statement execution options</h3><p>You can set the options on how the driver connects to the nodes and the execution options.</p><div class=\"highlight highlight-source-cs\"><pre>// Example at cluster level\nvar cluster = Cluster\n  .Builder()\n  .AddContactPoints(hosts)\n  .WithCompression(CompressionType.LZ4)\n  .WithLoadBalancingPolicy(new DCAwareRoundRobinPolicy(\"west\"));\n// Example at statement (simple, bound, batch) level\nvar statement = new SimpleStatement(query)\n  .SetConsistencyLevel(ConsistencyLevel.Quorum)\n  .SetRetryPolicy(DowngradingConsistencyRetryPolicy.Instance)\n  .SetPageSize(1000);</pre></div><h2><a href=\"https://github.com/datastax/csharp-driver#compatibility\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-compatibility\"></a>Compatibility</h2><ul><li>Apache Cassandra versions 2.0 and above.</li>\n<li>DataStax Enterprise versions 4.5 and above.</li>\n<li>.NET Framework versions 4.5 and above and .NET Core versions 1.0 and above.</li>\n</ul><p>Note: DataStax products do not support big-endian systems.</p><h2><a href=\"https://github.com/datastax/csharp-driver#feedback-requested\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-feedback-requested\"></a>Feedback Requested</h2><p><strong>Help us focus our efforts!</strong> Provide your input on the <a href=\"http://goo.gl/forms/3BxcP8nKs6\">Platform and Runtime Survey</a> (we kept it short).</p><h2><a href=\"https://github.com/datastax/csharp-driver#building-and-running-the-tests\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-building-and-running-the-tests\"></a>Building and running the tests</h2><p>You can use Visual Studio or msbuild to build the solution.</p><p><a href=\"https://github.com/datastax/csharp-driver/wiki/Building-and-running-tests\">Check the documentation for building the driver from source and running the tests</a>.</p><h2><a href=\"https://github.com/datastax/csharp-driver#license\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-license\"></a>License</h2><p>Copyright 2012-2017, DataStax.</p><p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p><p><a href=\"http://www.apache.org/licenses/LICENSE-2.0\">http://www.apache.org/licenses/LICENSE-2.0</a></p><p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>",
        "created_at": "2017-10-31T20:25:43+0000",
        "updated_at": "2018-07-24T19:51:06+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 5,
        "domain_name": "github.com",
        "preview_picture": "https://camo.githubusercontent.com/65a02e83e7515dc3b1ef568bd7317d0dac6a1874/68747470733a2f2f7472617669732d63692e6f72672f64617461737461782f6373686172702d6472697665722e7376673f6272616e63683d6d6173746572",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/4904"
          }
        }
      }
    ]
  }
}